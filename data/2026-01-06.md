<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 6]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 140]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Analog Weight Update Rule in Ferroelectric Hafnia, using pico-Joule Programming Pulses](https://arxiv.org/abs/2601.01186)
*Alexandre Baigol,Nikhil Garg,Matteo Mazza,Yanming Zhang,Elisa Zaccaria,Wooseok Choi,Bert Jan Offrein,Laura Bégon-Lours*

Main category: cs.ET

TL;DR: 该研究通过横向缩小铁电电阻器件面积至100μm²以下，实现了20纳秒编程脉冲，将每个脉冲能耗降至3皮焦耳，并发现最终权重仅由脉冲幅度决定，与初始状态无关。


<details>
  <summary>Details</summary>
Motivation: 为了与大脑处理信息的高效性竞争，神经形态硬件需要降低训练阶段的能耗。缩短编程脉冲持续时间是降低能耗的策略之一，但受到电阻器件自加载时间的限制。

Method: 采用与CMOS后端工艺兼容的铪/锆氧化物纳米层叠结构制造铁电电阻器件，通过横向缩小器件面积至100μm²以下来减少自加载时间，实现20纳秒编程脉冲，并实验测量不同幅度和初始电导状态下的权重更新规则。

Result: 器件面积缩小后自加载时间显著缩短，实现了20纳秒编程脉冲，每个脉冲最大能耗仅为3皮焦耳。实验发现最终权重值仅取决于编程脉冲的幅度，与初始权重状态无关。

Conclusion: 通过横向缩小铁电电阻器件面积可以有效降低自加载时间，实现超低能耗的快速编程，这为开发更高效的神经形态硬件提供了重要途径。

Abstract: In an effort to compete with the brain's efficiency at processing information, neuromorphic hardware combines artificial synapses and neurons using mixed-signal circuits and emerging memories. In ferroelectric resistive weights, the strength of the synaptic connection between two neurons is stored in the device conductance. During learning, programming pulses are applied to the synaptic weight, which reconfigures the ferroelectric domains and adjusts the conductance. One strategy to lower the energy cost during the training phase is to lower the duration of the programming pulses. However, the latter cannot be shorter than the self-loading time of the resistive weights, limited by intrinsic parasitics in the circuits. In this work, ferroelectric resistive weights are fabricated using a process compatible with CMOS Back-End-Of-Line integration, based on hafnia/zirconia nanolaminates. By laterally scaling the device area under 100 $μ$m$^2$, the self-loading time becomes sufficiently short to enable 20 ns programming, which corresponds to a maximum of 3 picoJoules per pulse. Further, in this work, the weight update rule with 20 ns pulses is experimentally measured not only for different amplitudes but also for different initial conductance states. We find that the final weight is determined by the pulse amplitude, independent of the initial weight value.

</details>


### [2] [Bridging Language Gaps: Utilizing Interactive Robots to Teach Cantonese in Real-Life Contexts for Newly-Arrived Children](https://arxiv.org/abs/2601.01234)
*Ka-Yan Fung,Yuxing Tao,Tze-Leung,Rick Lui,Kuen-Fung Sin*

Main category: cs.ET

TL;DR: 香港多元文化教育中，新來港學生面臨語言和文化障礙，研究探索互動機器人Boon Boon通過情境教學提升學生粵語學習參與度和動機的效果。


<details>
  <summary>Details</summary>
Motivation: 香港教育系統具有多元文化特點，新來港學生（普通話使用者）面臨粵語語言障礙和文化差異，現有資源主要關注英語學習而忽略粵語和文化適應，導致學生學業困難、社會融入問題和情感困擾。

Method: 研究邀請14名兒童參與為期四天的學習項目，使用互動機器人Boon Boon通過真實生活情境進行粵語教學，探討機器人賦能的情境學習對學習參與度、動機和語言能力的影響。

Result: 初步結果顯示Boon Boon能夠吸引學生的學習注意力並促進學業成就，表明互動機器人在提升新來港學生粵語學習參與度和動機方面具有積極效果。

Conclusion: 互動機器人情境學習對新來港學生的粵語學習具有潛在積極影響，未來研究將關注長期效果評估以及在不同教育環境和文化背景中的可擴展性。

Abstract: Hong Kong's education system is notably multicultural, including local, non-Chinese-speaking, and newly arrived students (NAS) (Mandarine Chinese-speaking). NAS can guess the meaning of vocabulary but cannot speak out, presenting unique challenges for them, particularly language barriers and cultural differences. These challenges hinder their academic success and social integration, leading to feelings of isolation and demotivation. Current resources often fail to address the emotional well-being of these students and predominantly focus on English language acquisition, leaving a gap in support for learning Cantonese and navigating the local cultural landscape. This study explores the effectiveness of an interactive robot, Boon Boon, in teaching Cantonese through real-life contexts to enhance NAS children learning engagement and motivation. The research questions are: (1) How does interactive robot-empowered scenario learning influence the learning engagement and motivation of NAS in learning Cantonese? and (2) What is the impact of a robot-empowered scenario learning system on the Cantonese language proficiency of NAS? Fourteen children are invited to participate in a four-day learning program with Boon Boon. The preliminary result indicated that Boon Boon drove students' attention to learning and academic achievement. Future research will focus on long-term assessments of robot-empowered learning's effectiveness and explore the scalability of this approach across diverse educational settings and cultural backgrounds.

</details>


### [3] [Adaptive Tuning of the Unscented Kalman Filter using Particle Swarm Optimization for Inertial-GPS Sensor Fusion Systems](https://arxiv.org/abs/2601.01578)
*Psyche T. Malabo,Bobby D. Gerardo*

Main category: cs.ET

TL;DR: PSO优化UKF参数，在CARLA仿真中提升车辆定位精度82.14%，减少IMU漂移，满足实时性要求


<details>
  <summary>Details</summary>
Motivation: 现有IMU-GPS融合方法（EKF、UKF、ML、GA、DE）存在非线性、不稳定或计算成本高的问题，需要更有效的参数优化方法

Method: 提出基于PSO的自适应调优框架，优化UKF参数（α、β、κ、Q、R），在CARLA 0.9.14中使用Tesla Model 3进行多场景测试

Result: 在参数边界内15代收敛，相比手动调优精度提升82.14%，IMU漂移减少高达21,606.59米，更新时间低于10ms实时阈值

Conclusion: PSO调优的UKF在动态、GPS受限条件下表现出实用的定位性能，通过统计验证确认了稳定增益

Abstract: Accurate vehicle positioning requires effective IMU-GPS fusion, yet prior methods-EKF, UKF, ML, GA, and DE-suffer from nonlinearity, instability, or high computational cost. This study introduces a PSO-based adaptive tuning framework for optimizing UKF parameters (α, \b{eta}, \k{appa}, Q, R), evaluated in CARLA 0.9.14 using a Tesla Model 3 under diverse maneuvers and environmental conditions. Within defined parameter bounds, convergence stabilized within 15 generations, achieving an 82.14% accuracy improvement over manual tuning and reducing IMU drift by up to 21,606.59m. Multi-trial statistical validation confirmed consistent gains with low confidence intervals. With update times remaining below the 10 ms real-time threshold, the PSO-tuned UKF demonstrates practical localization performance for dynamic, GPS-challenged conditions.

</details>


### [4] [Physics-Informed Deep Recurrent Back-Projection Network for Tunnel Propagation Modeling](https://arxiv.org/abs/2601.02007)
*Kunyu Wu,Qiushi Zhao,Jingyi Zhou,Junqiao Wang,Hao Qin,Xinyue Zhang,Xingqi Zhang*

Main category: cs.ET

TL;DR: 提出PRBPN网络，从粗网格PWE切片重建精细RSS场，减少对计算密集型精细网格求解器的依赖，同时保持高保真度隧道传播预测


<details>
  <summary>Details</summary>
Motivation: 铁路隧道中无线电波传播的精确高效建模对CBTC系统可靠性至关重要。精细网格PWE求解器计算成本高，粗网格模型会丢失重要模态和几何细节，需要平衡精度与效率

Method: 提出物理信息循环反投影传播网络(PRBPN)，集成多切片时间融合与迭代投影/反投影机制，强制物理一致性，无需预上采样阶段

Result: 在四种隧道截面几何和四种频率下的仿真显示，PRBPN能紧密跟踪精细网格PWE参考。在法国Massif Central隧道的工程级验证进一步证实了数据稀缺场景下的鲁棒性

Conclusion: PRBPN能显著减少对计算密集型精细网格求解器的依赖，同时保持高保真度隧道传播预测，在数据稀缺场景下表现出色

Abstract: Accurate and efficient modeling of radio wave propagation in railway tunnels is is critical for ensuring reliable communication-based train control (CBTC) systems. Fine-grid parabolic wave equation (PWE) solvers provide high-fidelity field predictions but are computationally expensive for large-scale tunnels, whereas coarse-grid models lose essential modal and geometric details. To address this challenge, we propose a physics-informed recurrent back-projection propagation network (PRBPN) that reconstructs fine-resolution received-signal-strength (RSS) fields from coarse PWE slices. The network integrates multi-slice temporal fusion with an iterative projection/back-projection mechanism that enforces physical consistency and avoids any pre-upsampling stage, resulting in strong data efficiency and improved generalization. Simulations across four tunnel cross-section geometries and four frequencies show that the proposed PRBPN closely tracks fine-mesh PWE references. Engineering-level validation on the Massif Central tunnel in France further confirms robustness in data-scarce scenarios, trained with only a few paired coarse/fine RSS. These results indicate that the proposed PRBPN can substantially reduce reliance on computationally intensive fine-grid solvers while maintaining high-fidelity tunnel propagation predictions.

</details>


### [5] [Impact of Spatial Proximity on Drone Services](https://arxiv.org/abs/2601.02210)
*Vejaykarthy Srithar,Syeda Amna Rizvi,Amani Abusafia,Athman Bouguettaya,Balsam Alkouz*

Main category: cs.ET

TL;DR: 研究无人机在近距离飞行时的相互影响，通过实验分析位置、间距和风况对能耗的影响，并开发GUI可视化分析工具


<details>
  <summary>Details</summary>
Motivation: 理解无人机在近距离飞行时的相互影响对于规划高效的无人机配送服务至关重要，需要研究位置、间距和风况等因素对能耗的影响

Method: 在不同风况下，让无人机在3D空间中不同位置进行实验，收集在航段飞行的能耗数据，开发GUI可视化无人机轨迹并分析相互影响

Result: 开发了能够绘制无人机轨迹的GUI工具，便于分析无人机之间的相互影响，包括位置、间距和风况对能耗的影响

Conclusion: 无人机在近距离飞行时存在相互影响，通过实验和可视化工具可以分析这些影响，为优化无人机配送服务提供依据

Abstract: We demonstrate the peer-to-peer impact of drones flying in close proximity. Understanding these impacts is crucial for planning efficient drone delivery services. In this regard, we conducted a set of experiments using drones at varying positions in a 3D space under different wind conditions. We collected data on drone energy consumption traveling in a skyway segment. We developed a Graphical User Interface (GUI) that plots drone trajectories within a segment. The GUI facilitates analyzing the peer-to-peer influence of drones on their energy consumption. The analysis includes drones' positions, distance of separation, and wind impact.

</details>


### [6] [Modeling Inter-drone Interference as a Service in Skyway Networks](https://arxiv.org/abs/2601.02270)
*Gabriel Timothy,Syeda Amna Rizvi,Muhammad Umair,Athman Bouguettaya,Balsam Alkouz*

Main category: cs.ET

TL;DR: 研究无人机群在天空网络中的相互干扰对配送效率的影响，通过实验分析干扰机制并建立预测模型。


<details>
  <summary>Details</summary>
Motivation: 随着多无人机配送网络的发展，无人机之间的相互干扰成为影响配送效率的关键问题，需要系统研究其影响机制。

Method: 在室内测试环境中进行控制实验，对比单机飞行与多机并发飞行的性能，分析飞行中和充电站的干扰，建立预测模型。

Result: 实验生成了全面的干扰数据集，验证了预测模型能准确预测无人机间干扰，为优化网络性能提供依据。

Conclusion: 研究成功建立了无人机干扰预测模型，为多无人机天空网络的效率优化提供了理论基础和实用工具。

Abstract: We present a novel investigation into the impact of inter-drone interference on delivery efficiencies within multi-drone skyway networks. We conduct controlled experiments to analyze the behavior of drones in an indoor testbed environment. Our study compares performance between solo flights and concurrent multi-drone operations along predefined routes. This analysis captures interference occurring during both flight and at charging stations, providing a comprehensive evaluation of its effects on overall network performance. We conduct a comprehensive series of experiments across diverse scenarios to systematically understand and model the dynamics of inter-drone interference. Key metrics, such as power consumption and delivery times, are considered. This generates a comprehensive dataset for in-depth analysis of interference at both the node and segment levels. These findings are then formalized into a predictive model. The results validate the effectiveness of the developed model, demonstrating its potential to accurately forecast inter-drone interferences.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations](https://arxiv.org/abs/2601.01031)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: 提出了首个用于分布式卫星系统的多端口并发通信可分负载理论（MPCC-DLT）框架，量化计算速度、链路带宽和结果大小开销对任务完成时间的联合影响，并扩展了实时准入控制机制。


<details>
  <summary>Details</summary>
Motivation: 分布式卫星系统（DSS）需要处理并发数据分发、并行计算和结果返回的复杂任务，现有理论缺乏对异构机载处理和星间链路条件的联合分析框架。

Method: 开发了MPCC-DLT框架，推导出最优负载分配和完成时间的闭式表达式，建立了截止时间可行性条件，并扩展了处理随机任务到达和截止时间约束的实时准入控制机制。

Result: 高度可分任务显著降低延迟，通信密集型任务因结果传输开销而收益递减；实时模拟显示任务结构和系统参数共同决定截止时间满足情况和运行状态。

Conclusion: 该工作为分布式卫星系统提供了首个分析可处理的MPCC-DLT模型，为应用感知调度和未来卫星星座系统级设计提供了可操作的见解。

Abstract: We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.

</details>


### [8] [Performance and Security Aware Distributed Service Placement in Fog Computing](https://arxiv.org/abs/2601.01125)
*Mohammad Goudarzi,Arash Shaghaghi,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出SPA-DDRL框架，使用分布式深度强化学习联合优化雾计算中的服务响应时间和安全合规性，通过三层安全评分体系和分布式架构实现性能与安全平衡。


<details>
  <summary>Details</summary>
Motivation: 物联网应用快速增长导致雾计算中服务放置需求增加，但异构资源、动态工作负载和多样化安全需求使得最优服务放置极具挑战。现有解决方案大多只关注性能指标，忽视了部署决策的安全影响。

Method: 提出SPA-DDRL框架，将问题建模为加权多目标优化任务，最小化延迟同时最大化基于雾节点安全能力的安全评分。采用三层安全评分体系（配置级检查、能力级评估、控制级评估），使用分布式代理-学习者架构，集成LSTM网络、优先经验回放和离策略校正机制。

Result: 基于真实物联网工作负载的实验显示，SPA-DDRL相比现有方法显著改善了服务响应时间和放置安全性，响应时间提升16.3%，收敛速度加快33%，在所有系统规模下都能保持一致、可行、安全合规的解决方案。

Conclusion: SPA-DDRL框架成功解决了雾计算中服务放置的性能与安全联合优化问题，通过分布式深度强化学习和三层安全评估体系，在保证安全合规的同时实现了性能优化，为物联网应用提供了有效的服务放置解决方案。

Abstract: The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.

</details>


### [9] [OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL](https://arxiv.org/abs/2601.01209)
*Xin Tan,Yicheng Feng,Yu Zhou,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.DC

TL;DR: OrchestrRL是一个用于解耦强化学习训练与生成阶段的编排框架，通过自适应计算调度和可重构混合光电网来解决动态负载和网络瓶颈问题，实现1.40倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 将强化学习的生成和训练阶段解耦为并行异步流水线虽能提高扩展性和吞吐量，但面临两大挑战：生成阶段因动态负载变化和执行不均衡成为瓶颈；解耦阶段产生多样动态网络流量模式压垮传统网络架构。

Method: 提出OrchestrRL编排框架，包含：1) 自适应计算调度器，动态调整并行度以匹配负载特征，加速执行并重新平衡请求；2) RFabric可重构混合光电网，利用光电路开关实时重构拓扑，支持不同并行配置下的生成、训练迭代中的层间集体通信和周期性权重同步。

Result: 在48个H800 GPU物理测试平台上，OrchestrRL实现最高1.40倍吞吐量提升。通过RLSim高保真模拟器评估RFabric，显示相比静态Fat-Tree网络具有更优的性能成本效率。

Conclusion: OrchestrRL通过动态管理计算和网络节奏，有效解决了解耦强化学习中的瓶颈问题，RFabric混合光电网为大规模RL工作负载提供了高效解决方案。

Abstract: Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.

</details>


### [10] [Making MoE based LLM inference resilient with Tarragon](https://arxiv.org/abs/2601.01310)
*Songyu Zhang,Aaron Tam,Myungjin Lee,Shixiong Qi,K. K. Ramakrishnan*

Main category: cs.DC

TL;DR: Tarragon是一个具有故障恢复能力的MoE推理框架，通过可重构数据路径和自愈机制，将故障影响限制在单个工作节点，显著减少故障导致的停顿时间。


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理系统故障恢复能力差，单个工作节点故障就会触发整个服务的粗粒度重启，丢弃已累积的进度并暂停整个推理流水线，这不适合对延迟敏感的LLM服务。

Method: 1. 利用MoE中注意力计算和专家计算的自然分离，将注意力工作节点和专家工作节点视为独立的故障域；2. 引入可重构数据路径，通过重定向请求到健康节点来屏蔽故障；3. 实现自愈机制，放松现有MoE框架的紧密同步执行；4. 对有状态的注意力工作节点采用异步增量KV缓存检查点与按请求恢复；5. 对无状态的专家工作节点利用剩余GPU内存部署影子专家。

Result: 与最先进的MegaScale-Infer相比，Tarragon将故障导致的停顿时间减少了160-213倍（从约64秒降至0.3-0.4秒），同时在无故障发生时保持性能不变。

Conclusion: Tarragon通过细粒度的故障隔离和高效的自愈机制，为大规模MoE推理提供了高弹性的解决方案，显著提高了服务的可用性和可靠性。

Abstract: Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.
  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.

</details>


### [11] [DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster](https://arxiv.org/abs/2601.01500)
*Jinxiao Zhang,Yunpu Xu,Xiyong Wu,Runmin Dong,Shenggan Cheng,Yi Zhao,Mengxuan Chen,Qinrui Zheng,Jianting Liu,Haohuan Fu*

Main category: cs.DC

TL;DR: DiT-HC：首个在下一代HPC CPU集群上训练和扩展生成模型DiT的系统，通过通信自由张量并行、优化算子内核和自定义MPI后端实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 随着生成基础模型在科学计算中数据重建和模拟的重要性日益增长，以及新硬件特性（如矩阵加速单元和高带宽内存）的发展，CPU集群为加速和扩展此类模型提供了有前景的机会，促进人工智能与科学计算的融合。

Method: DiT-HC系统包含三项关键技术：1) 通信自由张量并行(CFTP)与AutoMem实现自动内存感知数据流；2) HCOps优化算子套件，利用向量和矩阵加速单元优化GEMM和算子内核；3) 自定义MPI后端，重叠计算、通信和内存移动。

Result: 实验显示比原生或公共CPU库加速8.2到87.7倍，在256个节点上达到90.6%的弱扩展效率，证明了在CPU集群上进行大规模生成模型训练的可行性。

Conclusion: DiT-HC展示了在CPU集群上高效训练大规模生成模型的可行性，为未来HPC-AI协同设计提供了新的见解，促进了人工智能与科学计算的统一。

Abstract: Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.

</details>


### [12] [FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data](https://arxiv.org/abs/2601.01596)
*Congrong Ren,Robert Underwood,Sheng Di,Emrecan Kutay,Zarija Lukic,Aylin Yener,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出一种基于快速傅里叶校正算法的谱特征保持压缩技术，通过在空间域和频率域同时约束误差来保护科学数据的关键频域特征。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法仅保证空间域精度，忽视了频率域特征，而许多科学应用（如宇宙学、燃烧湍流、X射线衍射）需要同时保留空间和频率表示以获得互补的科学洞察。

Method: 提出快速傅里叶校正算法，将频率域误差表示为空间域误差的线性组合，推导出同时约束两个域误差的区域。通过迭代投影将基础压缩器的空间误差向量投影到空间和频率约束区域的交集内，并使用GPU并行加速。

Result: 在宇宙学模拟、X射线衍射、燃烧模拟和脑电图等多个数据集上验证了方法的有效性，成功保留了空间和频率域的关键科学信息。

Conclusion: 该方法解决了现有压缩技术忽视频率域特征的问题，通过联合约束空间和频率误差，为需要同时分析两个域的科学应用提供了有效的压缩解决方案。

Abstract: This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.

</details>


### [13] [RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference](https://arxiv.org/abs/2601.01712)
*Jiarui Wang,Huichao Chai,Yuanhang Zhang,Zongjin Zhou,Wei Guo,Xingkun Yang,Qiang Tang,Bo Pan,Jiawei Zhu,Ke Cheng,Yuting Yan,Shulan Wang,Yingjie Zhu,Zhengfan Yuan,Jiaqi Huang,Yuhan Zhang,Xiaosong Sun,Zhinan Zhang,Hong Zhu,Yongsheng Zhang,Tiantian Dong,Zhong Xiao,Deliang Liu,Chengzhou Lu,Yuan Sun,Zhiyuan Chen,Xinming Han,Zaizhu Liu,Yaoyuan Wang,Ziyang Zhang,Yong Liu,Jinxin Xu,Yajing Sun,Zhoujun Yu,Wenting Zhou,Qidong Zhang,Zhengyong Zhang,Zhonghai Gu,Yibo Jin,Yongxiang Feng,Pengfei Zuo*

Main category: cs.DC

TL;DR: RelayGR系统通过选择性预推断用户行为前缀、保持KV缓存在HBM中，并确保后续排序能直接使用，解决了生成式推荐模型在实时系统中的序列长度限制问题。


<details>
  <summary>Details</summary>
Motivation: 实时推荐系统在严格尾部延迟SLO下，生成式推荐模型的在线序列长度受到限制。大多数GR token编码的用户行为与候选物品无关，这为预推断和复用用户行为前缀提供了机会。

Method: RelayGR采用三种技术：1) 序列感知触发器，在有限缓存占用和预推断负载下仅处理有风险的请求；2) 亲和感知路由器，将缓存生产和消费路由到同一实例；3) 内存感知扩展器，利用服务器本地DRAM捕获短期跨请求复用。

Result: 在固定P99 SLO下，RelayGR支持长达1.5倍的序列长度，并将SLO兼容吞吐量提高至3.6倍。

Conclusion: RelayGR通过创新的中继式推理架构，成功解决了工业规模生成式推荐模型的序列长度限制问题，显著提升了系统性能。

Abstract: Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

</details>


### [14] [pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression](https://arxiv.org/abs/2601.01787)
*Yuxiao Li,Mingze Xia,Xin Liang,Bei Wang,Robert Underwood,Sheng Di,Hemant Sharma,Dishant Beniwal,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出分布式并行算法，用于修正压缩后数据的拓扑特征（PLMSS），在128个GPU上实现超过90%的并行效率


<details>
  <summary>Details</summary>
Motivation: 有损压缩会扭曲数据的关键特征，可能影响下游分析和科学结论。现有单GPU算法无法扩展到极端规模数据，并行计算积分路径是主要瓶颈

Method: 简化MSz算法，通过在所有位置保留最陡上升和下降方向来避免显式计算和修正积分路径，减少进程间通信，引入可忽略的额外存储开销

Result: 在Perlmutter超级计算机上，对真实世界数据集在128个GPU上实现了超过90%的并行效率

Conclusion: 提出的分布式并行算法成功解决了PLMSS修正的扩展瓶颈，能够在极端规模数据上高效运行，为科学数据压缩后的拓扑特征修正提供了可扩展解决方案

Abstract: Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.

</details>


### [15] [Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet](https://arxiv.org/abs/2601.01980)
*Manuel Parra-Royón,Álvaro Rodríguez-Gallardo,Susana Sánchez-Expósito,Laura Darriba-Pol,Jesús Sánchez-Castañeda,M. Ángeles Mendoza,Julián Garrido,Javier Moldón,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: 该论文提出在SKA天文台区域中心网络中采用分布式和原位计算，结合FaaS和进化算法优化数据密集型工作流，解决传统数据集中处理面临的网络和存储瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: SKA天文台将产生前所未有的海量数据，传统的数据中心化计算模型由于网络和存储瓶颈已不可行。区域中心网络需要在近exascale环境中运行，需要新的计算范式来解决数据移动带来的效率问题。

Method: 提出向分布式和原位计算转变，将计算移动到数据附近。集成函数即服务（FaaS）与基于进化算法的智能决策实体，FaaS实现轻量级模块化函数执行，进化算法用于多目标优化（执行时间和能耗），考虑数据位置和传输成本约束。

Result: 建立了一个用于SRCNet架构中高效、成本感知的计算到数据策略的基准框架，通过MOEAs探索考虑执行时间、能耗、数据位置和传输成本的近最优执行计划。

Conclusion: 该工作为SKA区域中心网络中的分布式计算提供了创新解决方案，通过FaaS和进化算法的结合，实现了计算到数据的优化策略，为解决大规模天文数据处理挑战奠定了基础。

Abstract: The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.
  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.
  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.

</details>


### [16] [SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks](https://arxiv.org/abs/2601.02092)
*Abdullah Al Asif,Sixing Yu,Juan Pablo Munoz,Arya Mazaheri,Ali Jannesari*

Main category: cs.DC

TL;DR: SuperSFL提出了一种结合联邦学习和分割学习的框架，通过权重共享超网络动态生成资源感知的客户端子网络，有效应对设备异构性挑战，并引入三阶段梯度融合优化机制加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统SplitFed Learning在异构边缘计算环境中面临显著挑战，包括设备计算和通信能力差异大、收敛速度慢、通信成本高、训练时间长等问题，需要一种能够动态适应设备资源差异的解决方案。

Method: 1) 使用权重共享超网络动态生成资源感知的客户端子网络；2) 引入三阶段梯度融合(TPGF)优化机制协调本地更新、服务器端计算和梯度融合；3) 采用容错客户端分类器和协作式客户端-服务器聚合机制应对通信中断。

Result: 在CIFAR-10和CIFAR-100数据集上，与100个异构客户端测试显示：通信轮数收敛速度提升2-5倍，准确率更高，总通信成本降低高达20倍，训练时间缩短13倍，同时表现出更好的能效。

Conclusion: SuperSFL通过动态资源感知子网络生成和优化的梯度融合机制，有效解决了异构边缘环境中的联邦学习挑战，实现了更快的收敛速度、更低的通信成本和更高的能效，是实用的异构边缘联邦学习解决方案。

Abstract: SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\times$ lower total communication cost and $13\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.

</details>


### [17] [BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation](https://arxiv.org/abs/2601.02286)
*Rahul Sengupta,Nooshin Yousefzadeh,Manav Sanghvi,Yash Ranjan,Anand Rangarajan,Sanjay Ranka,Yashaswi Karnati,Jeremy Dilmore,Tushar Patel,Ryan Casburn*

Main category: cs.DC

TL;DR: BigSUMO是一个端到端、可扩展的开源交通分析框架，用于交通数据分析、中断检测和并行仿真，帮助城市交通管理优化。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市化进程加快，交通基础设施的高效管理对交通机构和城市规划者至关重要。需要能够分析大量交通数据并制定有效干预措施的工具。

Method: 开发了BigSUMO框架，该系统摄入高分辨率环形检测器和信号状态数据，以及稀疏的轨迹数据。首先进行描述性分析和中断检测，然后使用SUMO微观仿真器进行预测性分析，测试数百种假设场景以优化交通性能。模块化设计允许集成不同的数据处理和异常检测算法。

Result: BigSUMO是一个基于开源软件和库构建的成本效益高、可扩展且易于部署的管道系统，能够有效支持智能城市交通解决方案的开发。

Conclusion: BigSUMO有望成为开发智能城市交通解决方案的有价值工具，帮助交通机构和城市规划者更好地管理交通基础设施。

Abstract: With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.

</details>


### [18] [Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies](https://arxiv.org/abs/2601.02311)
*Deep Pankajbhai Mehta*

Main category: cs.DC

TL;DR: 提出placement semantics框架，仅通过四种训练状态在设备上的五种放置模式，就能推导出内存消耗和通信量，统一了各种并行策略。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型需要在多个加速器上分配计算，但实践者通过试错选择并行策略（数据、张量、流水线、ZeRO），缺乏统一的系统框架来预测它们的行为。

Method: 引入placement semantics：每个策略通过四种训练状态（参数、优化器、梯度、激活）在设备上使用五种模式（复制、分片、分片-聚集、物化、卸载）来指定。仅从放置方式就能推导内存消耗和通信量。

Result: 预测结果与已发表结果完全匹配：ZeRO-3比数据并行少用8倍内存，通信成本增加1.5倍。证明了梯度完整性和状态一致性是分布式训练匹配单设备结果的必要充分条件，并提供了安全组合策略的规则。

Conclusion: 该框架统一了ZeRO Stages 1-3、FSDP、张量并行和流水线并行，将它们视为具有不同放置选择的实例，为系统化选择并行策略提供了理论基础。

Abstract: Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: 论文证明在离线强化学习中，horizon reduction（视野缩减）可能导致不可恢复的信息损失，使最优策略在统计上无法与次优策略区分，即使有无限数据和完美函数逼近。


<details>
  <summary>Details</summary>
Motivation: 尽管经验证据表明视野缩减能改善离线强化学习的扩展性，但其理论影响尚未充分发展。本文旨在揭示视野缩减可能导致的基本信息损失问题。

Method: 将视野缩减形式化为从固定长度轨迹片段中学习，并证明在这种范式下，最优策略可能统计上无法与次优策略区分。通过构建最小反例马尔可夫决策过程，识别三种结构失效模式。

Result: 证明了视野缩减可能导致不可恢复的信息损失，识别了三种失效模式：前缀不可区分性导致可识别性失败、截断回报引起的目标错误指定、离线数据集支持和表示混叠。

Conclusion: 视野缩减在离线强化学习中存在固有局限性，无法仅通过算法改进克服。研究为视野缩减的安全使用建立了必要条件，补充了关于保守目标和分布偏移的算法工作。

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [20] [ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI](https://arxiv.org/abs/2601.00832)
*Israk Hasan Jone,D. M. Rafiun Bin Masud,Promit Sarker,Sayed Fuad Al Labib,Nazmul Islam,Farhad Billah*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的虾病自动分类方法，使用六种预训练模型在包含1,149张图像的四个疾病类别数据集上进行评估，其中ConvNeXt-Tiny模型在测试集上取得了96.88%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 虾是全球消费最广泛的水产品种之一，具有重要的营养和经济价值。虾养殖是许多地区的重要收入来源，但与其他形式的水产养殖一样，虾养殖受到疾病爆发的严重影响。这些疾病对可持续虾生产构成重大挑战。为解决这一问题，自动化疾病分类方法可以提供及时准确的检测。

Method: 研究采用深度学习方法来分类虾病。使用包含1,149张图像、覆盖四个疾病类别的数据集。部署并评估了六种预训练深度学习模型：ResNet50、EfficientNet、DenseNet201、MobileNet、ConvNeXt-Tiny和Xception。进行了背景移除和Keras图像管道的标准化预处理。使用快速梯度符号方法（FGSM）进行对抗训练以增强模型鲁棒性。实施了CutMix和MixUp等高级数据增强策略以减少过拟合并提高泛化能力。应用了Grad-CAM、Grad-CAM++和XGrad-CAM等事后解释方法来支持可解释性和可视化模型关注区域。

Result: 探索性结果表明，ConvNeXt-Tiny模型取得了最佳性能，在测试数据集上达到了96.88%的准确率。经过1000次迭代后，模型的99%置信区间为[0.953,0.971]。

Conclusion: 该研究成功开发了一种基于深度学习的虾病自动分类系统，ConvNeXt-Tiny模型表现出最佳性能，为虾养殖中的疾病检测提供了有效的自动化解决方案，有助于提高虾养殖的可持续性和生产力。

Abstract: Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

</details>


### [21] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: IM-PINN：一种无网格几何深度学习框架，通过将黎曼度量张量嵌入自动微分图，在连续参数域中直接求解复杂流形上的非线性反应扩散方程，解决了传统方法的高保真网格生成成本和辛漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂非欧几里得流形上模拟非线性反应扩散动力学面临两大挑战：高保真网格生成的高昂成本，以及离散时间步进方案中的辛漂移问题。传统自适应细化方法在极端高斯曲率波动下无法解析各向异性的图灵不稳定性。

Method: 提出内在度量物理信息神经网络（IM-PINN），将黎曼度量张量嵌入自动微分图，解析重构拉普拉斯-贝尔特拉米算子，使解复杂度与几何离散化解耦。采用双流架构和傅里叶特征嵌入来缓解谱偏差，在具有极端高斯曲率波动的"随机布料"流形上验证框架。

Result: IM-PINN成功恢复了Gray-Scott模型的"分裂斑点"和"迷宫"状态。与表面有限元法（SFEM）相比，IM-PINN展现出更优的物理严谨性：全局质量守恒误差为0.157（SFEM为0.258），作为热力学一致的全局求解器消除了半隐式积分固有的质量漂移。

Conclusion: IM-PINN为在演化表面上模拟生物模式形成提供了内存高效、分辨率无关的范式，桥接了微分几何和物理信息机器学习，解决了传统方法在复杂流形上的计算挑战。

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [22] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: 该论文研究了RAG系统中的查询级控制问题，通过选择检索深度和生成模式来满足服务级别目标，发现固定基线策略表现良好，学习策略主要在质量导向SLO下提供额外成本节省


<details>
  <summary>Details</summary>
Motivation: RAG系统面临实际控制问题：需要针对每个查询选择检索深度和生成行为，以满足成本、拒绝率和幻觉风险等服务级别目标

Method: 将查询级控制建模为离散动作选择（检索深度、生成模式或拒绝），使用SQuAD 2.0构建离线日志数据集，评估两种简单策略学习目标：最佳动作监督分类和奖励加权变体

Result: 固定基线策略（低k值、防护提示）表现具有竞争力；学习策略主要在质量导向SLO下提供额外成本节省，在廉价SLO下当拒绝被高度奖励时可能出现拒绝崩溃

Conclusion: 本文提供了RAG管道SLO感知控制的可重复案例研究，强调失败模式和报告规范，而非提出新的检索器或语言模型

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [23] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: 提出一种增强JEPA世界模型规划能力的方法，通过塑造表示空间使状态嵌入间的距离近似负目标条件值函数，从而显著提升规划性能


<details>
  <summary>Details</summary>
Motivation: JEPA框架能通过自监督预测目标学习环境动态表示，但其支持有效行动规划的能力有限，需要增强规划能力

Method: 塑造JEPA表示空间，使状态嵌入间的距离（或准距离）近似给定环境中到达成本的负目标条件值函数，并在训练中强制执行此约束

Result: 相比标准JEPA模型，在简单控制任务上显著提升了规划性能

Conclusion: 通过约束表示空间使距离近似值函数的方法能有效增强JEPA世界模型的规划能力

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [24] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex是一个异步多智能体LLM框架，通过解耦智能体逻辑与物理内存，实现百万级智能体认知扩展，将内存复杂度从O(N*L)降至O(1)权重和O(N*k)上下文。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架存在线性内存扩展问题，使得"系统2"并行推理在消费级硬件上不切实际，需要解决内存瓶颈以实现大规模智能体部署。

Method: 采用异步架构、单例权重共享和拓扑突触技术，将KV缓存视为潜在空间中的点云，应用见证复杂稀疏化技术，并引入引用注入机制实现非侵入式KV缓存更新。

Result: 在单张NVIDIA RTX 4090上实现100个并发智能体仅需2.2GB显存，理论容量超过1000个智能体，计算延迟成为主要瓶颈。

Conclusion: Warp Cortex通过创新的内存优化架构，显著提升了多智能体LLM系统的可扩展性，为大规模并行推理系统在消费级硬件上的部署提供了可行方案。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [25] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: MFEE框架将推理重构为控制平面决策问题，通过语义分析选择性执行transformer，实现78.1%的执行减少同时保持100%准确率


<details>
  <summary>Details</summary>
Motivation: 当前AI推理系统将transformer执行视为强制性的，混淆了模型能力与执行必要性。需要区分何时必须执行transformer，何时可以通过替代路径保持正确性

Method: 提出Meaning-First Execution (MFEE)控制平面架构，作为现有堆栈之上的门控层，不修改模型、权重或参数。通过语义分析选择性调用transformer推理

Result: 在1000个多样化提示下，MFEE实现78.1%执行减少，同时保持100%精确匹配等价性。相比模式路由器最多53.3%避免率且有正确性失败，MFEE达到100%避免率且零失败

Conclusion: 通过定理1证明仅基于有限特征映射的路由器无法同时保证零假跳过和正避免率。执行治理应成为ML系统基础设施的基础层，与模型级优化技术正交

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [26] [Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission](https://arxiv.org/abs/2601.02253)
*Emrah Mete,Emin Erkan Korkmaz*

Main category: cs.LG

TL;DR: 提出Neuro-Channel Networks (NCN)，一种无需乘法运算的神经网络架构，通过模拟生物神经系统的离子通道机制，用通道宽度和神经递质参数替代传统权重，仅使用加法、减法和位运算实现前向传播。


<details>
  <summary>Details</summary>
Motivation: 深度学习严重依赖昂贵、高能耗且供应紧张的GPU硬件，限制了AI在边缘设备的普及。传统人工感知器依赖密集矩阵乘法，而生物神经系统通过物理离子通道限制和化学神经递质调节信号传输，无需算术乘法，效率更高。

Method: 提出Neuro-Channel Networks (NCN)：1) 用通道宽度(Channel Widths)物理限制信号幅度；2) 用神经递质(Neurotransmitter)参数基于符号逻辑调节信号传输；3) 前向传播仅使用加法、减法和位运算(最小值、符号)，完全消除浮点乘法；4) 使用标准反向传播进行训练。

Result: NCN能够以100%准确率解决非线性可分问题(XOR和多数函数)，证明其无需乘法权重即可形成复杂决策边界的能力。

Conclusion: NCN架构为下一代神经形态硬件提供了高效替代方案，使复杂模型能够在商用CPU或超低功耗芯片上运行，无需依赖昂贵的GPU集群，有望实现AI在边缘设备的广泛部署。

Abstract: The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.

</details>


### [27] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: EdgeJury：轻量级集成框架，使用小型指令调优语言模型（3B-8B）通过并行角色生成、匿名交叉评审、主席合成和一致性标注四阶段流程，显著提升问答真实性，在边缘部署中实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题阻碍了可靠问答，特别是在资源受限的边缘部署场景中，前沿规模模型或检索管道可能不切实际。需要一种轻量级解决方案，仅使用小型模型就能提升真实性和鲁棒性。

Method: EdgeJury采用四阶段集成框架：1）并行角色专业化生成；2）匿名交叉评审，包含结构化批评和排名；3）主席合成，整合最强内容并解决标记问题；4）基于模型间一致性的声明级一致性标注。

Result: 在TruthfulQA（MC1）上达到76.2%准确率，相比单个8B基线（62.8%）提升21.4%；在200个对抗性EdgeCases问题上获得48.2%相对增益；人工分析显示事实性幻觉错误减少约55%；在Cloudflare Workers AI上实现8.4秒中位端到端延迟。

Conclusion: 协调的小型模型集成可以在不依赖外部检索或专有大模型API的情况下，显著提升在误解密集型问答基准上的真实性，适合边缘服务器推理部署。

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [28] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: FedSCAM是一种联邦学习算法，通过基于客户端异质性动态调整SAM扰动半径和聚合权重，解决非IID数据下的收敛问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的统计异质性（特别是非IID标签分布）对收敛和泛化构成重大挑战。现有SAM方法对所有客户端使用统一的扰动半径，忽略了客户端特定的异质性。

Method: 提出FedSCAM算法：1) 计算每个客户端的异质性指标；2) 根据异质性分数反向调制SAM扰动半径，防止高方差客户端破坏全局模型；3) 引入异质性感知的加权聚合机制，优先考虑与全局优化方向一致的客户端更新。

Result: 在CIFAR-10和Fashion-MNIST数据集上，使用不同程度的狄利克雷标签偏斜进行实验，FedSCAM在收敛速度和最终测试准确率方面与FedSAM、FedLESAM等先进基线相比具有竞争力。

Conclusion: FedSCAM通过动态调整扰动半径和聚合权重来适应客户端异质性，有效解决了联邦学习中非IID数据带来的挑战，提升了模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [29] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: 评估AlphaEarth Foundation地理空间基础模型在农业监测任务中的表现，发现其在作物产量预测和耕作制图方面有竞争力，但存在空间可迁移性、可解释性和时间敏感性等限制


<details>
  <summary>Details</summary>
Motivation: 尽管AlphaEarth Foundation等地理空间基础模型在土地覆盖分类任务中表现出色，但在农业监测关键下游任务中的应用缺乏深入评估，且与传统遥感模型缺乏全面比较

Method: 在美国的三个农业下游任务（作物产量预测、耕作制图、覆盖作物制图）中评估AEF嵌入，使用公开和私有数据源，训练传统遥感模型作为对比基准

Result: AEF模型在所有任务中表现良好，在产量预测和县级耕作制图方面与专门构建的遥感模型竞争力相当，但存在空间可迁移性有限、可解释性低、时间敏感性不足等限制

Conclusion: AEF嵌入在农业应用中需谨慎使用，特别是在时间敏感性、泛化能力和可解释性要求高的场景中，当前版本存在明显限制

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [30] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: 该论文证明纯机械系统通过耗散量子动力学和非局域上下文聚合可以生成智能语言，而守恒定律会导致根本性失败。语言生成被确立为耗散量子场论。


<details>
  <summary>Details</summary>
Motivation: 探索纯机械系统是否能够生成智能语言，研究量子动力学在语言生成中的作用，特别是耗散与非局域性对智能涌现的影响。

Method: 使用具有封闭形式路径积分传播子的Koopman算子，分析耗散量子动力学中的非局域上下文聚合。通过谱分析揭示特征值结构，包括衰减模式（遗忘）、增长模式（放大）和中性模式（保持）。

Result: 证明不可逆计算需要受控信息耗散和因果上下文聚合。哈密顿约束会消除耗散模式，导致性能下降，尽管模型容量不变。耗散量子动力学能够产生连贯的文本生成。

Conclusion: 语言生成是耗散量子场论，机械系统通过耗散和非局域性的结合获得智能，而不是通过守恒定律。这为理解智能涌现提供了新的理论框架。

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [31] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: 提出统一的电池容量衰减预测框架，使用时间序列基础模型和参数高效微调，在包含1704个电池的大规模数据集上实现跨化学体系、容量尺度和工况的稳定预测性能。


<details>
  <summary>Details</summary>
Motivation: 电池容量衰减预测对储能系统的安全、可靠和长期效率至关重要，但不同化学体系、形态和工况的强异质性使得单一模型难以泛化到训练域之外。

Method: 收集20个公开老化数据集构建大规模语料库（1704个电池，396万充放电循环段），采用时间序列基础模型（TSFM）作为骨干，结合参数高效的低秩适应（LoRA）和物理引导的对比表示学习来捕捉共享的退化模式。

Result: 在已见和刻意保留的未见数据集上，单一统一模型相比每个数据集的强基线模型达到竞争性或更优的准确度，同时在训练中排除的化学体系、容量尺度和工况上保持稳定性能。

Conclusion: 基于TSFM的架构展示了作为电池管理系统容量衰减预测的可扩展和可迁移解决方案的潜力，能够实现跨多样电池类型和工况的稳健预测。

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [32] [Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery](https://arxiv.org/abs/2601.00863)
*Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出materiomusic生成框架，将物质层次结构与音乐创作逻辑连接，通过可逆映射使声音成为科学探针，揭示科学与艺术在约束下的创造性本质。


<details>
  <summary>Details</summary>
Motivation: 探索物质结构与音乐创作之间的深层联系，建立声音作为科学探针的方法论，揭示科学与艺术在约束条件下创造性过程的共同本质。

Method: 使用可逆映射方法：1) 分子光谱映射到音调；2) 三维网络映射到可演奏乐器；3) 枚举所有2^12音乐音阶进行定量分析；4) 基于群体的AI模型进行音乐创作。

Result: 发现文化重要音乐系统聚集在中熵、中缺陷走廊，与材料科学中的Hall-Petch最优缺陷密度直接对应；AI创作音乐展现人类结构特征；振动成为跨尺度结构组织的共享语法。

Conclusion: 科学与艺术都是在约束下的生成性世界构建行为，振动是跨尺度结构组织的共享语法，选择性不完美是恢复连贯性与适应性平衡的机制，创造性源于约束与自由度扩展的碰撞。

Abstract: We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.

</details>


### [33] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 将KDEy量化学习方法扩展到图数据，通过结构重要性采样处理训练和测试数据之间的结构偏移问题


<details>
  <summary>Details</summary>
Motivation: 在图数据中，传统量化学习方法的先验概率偏移假设在结构偏移情况下不成立，需要专门处理图结构变化带来的分布变化

Method: 将结构重要性采样思想扩展到KDEy量化学习方法，通过调整权重来适应训练和测试数据之间的结构差异

Result: 提出的方法能够适应结构偏移，在性能上优于标准量化学习方法

Conclusion: 通过将结构重要性采样与KDEy结合，有效解决了图数据中结构偏移下的量化学习问题

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [34] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: 本文提出了针对循环客户端参与的联邦AUC最大化算法，在平方替代损失下达到$\widetilde{O}(1/ε^{1/2})$通信复杂度和$\widetilde{O}(1/ε)$迭代复杂度，在一般成对损失下达到$O(1/ε^3)$通信复杂度，在PL条件下可提升至$\widetilde{O}(1/ε^{1/2})$。


<details>
  <summary>Details</summary>
Motivation: 现有联邦AUC最大化方法通常假设客户端完全可用，但实际联邦学习系统中客户端通常以循环方式参与训练，这给不可分解的AUC目标带来了独特的优化挑战。

Method: 针对循环客户端参与的联邦AUC最大化问题，研究两种设置：1）使用平方替代损失，将问题重构为非凸-强凹极小极大优化，利用Polyak-Łojasiewicz条件；2）考虑一般成对AUC损失，建立通信和迭代复杂度界限。

Result: 在平方替代损失下达到$\widetilde{O}(1/ε^{1/2})$通信复杂度和$\widetilde{O}(1/ε)$迭代复杂度；在一般成对损失下达到$O(1/ε^3)$通信复杂度和$O(1/ε^4)$迭代复杂度，PL条件下可提升至$\widetilde{O}(1/ε^{1/2})$通信复杂度和$\widetilde{O}(1/ε)$迭代复杂度。

Conclusion: 本文提出的方法在图像分类、医学成像和欺诈检测等基准任务上表现出优越的效率和有效性，解决了循环客户端参与下的联邦AUC最大化问题。

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [35] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: 提出了一种改进的辅助物理信息神经网络(A-PINN)框架，结合平衡自适应优化器，用于结构振动分析，相比基线方法性能提升至少40%。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络(PINNs)及其变体在求解微分方程控制的正反问题方面表现出色，但需要改进以准确捕捉结构振动现象并确保可靠的预测分析。

Method: 提出改进的辅助物理信息神经网络(A-PINN)框架，采用平衡自适应优化器，通过数值模拟近似欧拉-伯努利梁方程来评估性能。

Result: 数值结果表明，该模型在数值稳定性和预测准确性方面均有增强，相比基线方法性能提升至少40%。

Conclusion: 该研究为深入理解科学机器学习模型在解决振动问题方面的鲁棒性提供了重要见解，改进的A-PINN框架在结构振动分析中表现出优越性能。

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [36] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: FedCSPACK：一种基于余弦稀疏化参数打包和双权重聚合的个性化联邦学习方法，旨在解决数据异构性和客户端资源有限的问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据异构性会降低模型性能，现有方法虽然通过模型分割和知识蒸馏增强模型兼容性，但忽视了客户端通信带宽和计算能力不足的问题，未能有效平衡数据异构性处理和有限客户端资源之间的矛盾

Method: 提出FedCSPACK方法：1）客户端基于余弦相似性打包模型参数并选择贡献最大的参数包进行共享，减少带宽需求；2）客户端生成基于共享参数包的掩码矩阵，提高服务器上稀疏更新的对齐和聚合效率；3）在掩码中嵌入方向和分布距离权重，实现加权引导聚合机制

Result: 在四个数据集上使用十种最先进方法的广泛实验表明，FedCSPACK在保持高模型准确率的同时，有效提高了通信和计算效率

Conclusion: FedCSPACK通过余弦稀疏化参数打包和双权重聚合机制，有效利用有限的客户端资源，减少数据异构性对模型性能的影响，在通信和计算效率与模型准确性之间取得了良好平衡

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [37] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow是一个多层框架，结合强化学习和Agentic AI解决城市共享单车动态再平衡问题，通过战略DQN学习、战术优化和基于LLM的通信层，显著减少网络不平衡并提高运营效率。


<details>
  <summary>Details</summary>
Motivation: 解决城市共享单车服务中的动态再平衡问题，减少空闲时间、提高单车可用性、降低运营成本，并弥合机器智能与人工操作之间的鸿沟。

Method: 采用多层架构：战略层使用DQN在纽约Citi Bike网络的高保真模拟中学习再平衡策略；战术层优化多段行程和调度；通信层使用基于LLM的Agentic AI将计划转化为可执行指令。

Result: 在多次种子运行评估中，SmartFlow将网络不平衡减少超过95%，同时需要最小的行驶距离，并实现强大的卡车利用率。

Conclusion: SmartFlow为复杂城市移动网络提供了一个可扩展、可解释的AI驱动物流解决方案蓝图，成功整合了机器智能与人工操作。

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [38] [Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems](https://arxiv.org/abs/2601.00873)
*Osasumwen Cedric Ogiesoba-Eguakun,Suman Rath*

Main category: cs.LG

TL;DR: 量子机器学习用于检测微电网中的协调隐蔽攻击，混合量子经典模型在低维数据集上表现最佳，相比经典SVM基线有适度提升。


<details>
  <summary>Details</summary>
Motivation: 协调隐蔽攻击是分布式发电系统的严重网络安全威胁，它们修改控制和测量信号但保持接近正常行为，使得传统入侵检测方法难以发现。需要探索量子机器学习方法来提高检测能力。

Method: 使用高质量模拟测量数据创建平衡的二元分类数据集，包含三个特征：DG1的无功功率、相对于标称值的频率偏差和端电压幅值。评估了经典机器学习基线、完全量子变分分类器和混合量子经典模型。

Result: 混合量子经典模型（量子特征嵌入与经典RBF支持向量机结合）在低维数据集上获得最佳整体性能，相比强大的经典SVM基线在准确率和F1分数上有适度提升。完全量子模型由于训练不稳定性和当前NISQ硬件的限制表现较差。

Conclusion: 混合模型训练更可靠，表明量子特征映射即使在完全量子学习尚不实用的情况下也能增强入侵检测能力。这为量子机器学习在网络安全应用中的实际部署提供了有希望的途径。

Abstract: Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.

</details>


### [39] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize是一个开源Python框架，利用大语言模型通过迭代提示和上下文学习进行优化，将优化问题转化为自然语言生成候选解、外部评估和反馈改进的黑盒过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出超越传统语言任务的推理能力，这启发了将其应用于数值优化的可能性。传统优化方法需要数学编程或元启发式设计专业知识，而LLMize旨在通过自然语言描述让从业者能够定义复杂优化问题。

Method: LLMize将优化构建为黑盒过程：在自然语言中生成候选解，通过外部目标函数评估，利用解-分数反馈在连续迭代中改进。支持多种优化策略，包括优化提示（OPRO）和受进化算法、模拟退火启发的混合LLM方法。

Result: 在凸优化、线性规划、旅行商问题、神经网络超参数调优和核燃料晶格优化等任务上评估。结果显示，对于简单问题，LLM优化不如经典求解器有竞争力，但对于约束和启发式难以形式化的复杂领域特定任务，提供了一种实用且易用的方法。

Conclusion: LLMize为复杂、领域特定的优化问题提供了一个实用且易用的框架，通过自然语言注入约束、规则和领域知识，使从业者无需数学编程或元启发式设计专业知识即可定义优化问题。

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [40] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD是一种神经符号方法，用于从脑部MRI数据预测阿尔茨海默病，学习完全可解释的规则。该方法结合统计模型/决策树/随机森林/GNN识别相关脑连接，然后使用FastLAS学习全局规则，在保持完全可解释性的同时达到与SVM相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 在临床神经科学中，虽然深度学习模型（如GNN）在脑部疾病预测方面表现出色，但缺乏可解释性限制了其在临床决策中的应用。需要一种既能保持高预测性能又能提供完全可解释规则的方法，以帮助医生理解疾病机制和模型决策依据。

Method: LearnAD采用神经符号混合方法：首先使用统计模型、决策树、随机森林或GNN识别相关的脑连接特征，然后应用FastLAS（一种归纳逻辑编程系统）从这些特征中学习全局可解释规则。这种两阶段方法结合了神经网络的表示学习能力和符号系统的逻辑推理能力。

Result: 最佳实例性能优于决策树，与支持向量机准确率相当，仅略低于使用所有特征的随机森林和GNN，同时保持完全可解释性。消融研究表明，神经符号方法在保持可比性能的同时显著提高了可解释性。

Conclusion: LearnAD展示了符号学习如何能够深化我们对GNN在临床神经科学中行为的理解，为开发既准确又可解释的医疗AI系统提供了有前景的方向，有助于弥合高性能黑盒模型与临床可解释性需求之间的差距。

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [41] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: 提出基于向量余弦相似度的多维异常检测方法，通过添加零值维度构建新数据集，利用观测点与测量点之间的向量相似性识别异常数据


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在多维数据中可能效果有限，需要一种基于向量相似度的新方法来更有效地识别异常点

Method: 1. 向原始数据添加零值维度构建新数据集；2. 选择测量点并创建观测点（原点，仅在新维度有非零值）；3. 构建从观测点到测量点及其他点的向量；4. 比较这些向量的余弦相似度来识别异常

Result: 开发了优化实现MDOD并发布在PyPI上，提供了一种有效的多维异常检测工具

Conclusion: 该方法通过创新的向量相似度比较机制，为多维数据异常检测提供了新的有效解决方案

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [42] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: FANoS是一种受物理启发的优化器，结合了二阶动力系统、Nosé-Hoover恒温器和半隐式积分器，在Rosenbrock基准测试中表现优于AdamW和SGD+momentum，但在其他任务中不稳定，不是通用优化器的替代方案。


<details>
  <summary>Details</summary>
Motivation: 受分子动力学中结构保持积分和恒温器思想的启发，开发一种物理启发的优化器，旨在处理非凸优化中的刚性谷地问题。

Method: FANoS结合了：(1) 作为离散二阶动力系统的动量更新，(2) 使用动能反馈自适应调整标量摩擦系数的Nosé-Hoover型恒温器变量，(3) 半隐式（辛欧拉）积分器，可选配对角RMS预处理器。

Result: 在Rosenbrock-100D基准测试中，FANoS-RMS达到1.74×10⁻²，优于未裁剪的AdamW(48.50)和SGD+momentum(90.76)，但不如梯度裁剪的AdamW(1.87×10⁻³)和L-BFGS(≈4.4×10⁻¹⁰)。在病态凸二次问题和小型PINN任务中表现不稳定。

Conclusion: FANoS是现有思想的可解释性综合，在某些刚性非凸谷地中有帮助，但不是现代基线的通用替代品，其行为对温度调度和超参数选择敏感。

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [43] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: 提出一种层次拓扑聚类算法，可使用任意距离度量，通过持久性分析识别任意形状的聚类和异常值


<details>
  <summary>Details</summary>
Motivation: 拓扑方法能够在不假设数据结构的情况下探索数据云，但需要一种能够处理任意形状聚类和异常值的通用聚类算法

Method: 提出层次拓扑聚类算法，支持任意距离选择，通过构建层次结构推断异常值和任意形状聚类的持久性

Result: 在图像、医疗和经济数据等包含相关异常值的数据集上展示了算法潜力，能够在其他技术失败的情况下提供有意义的聚类

Conclusion: 该层次拓扑聚类算法为处理复杂数据结构和异常值提供了一种有效的通用方法，在多种应用场景中表现出优越性

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [44] [When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training](https://arxiv.org/abs/2601.00894)
*Gihyeon Sim*

Main category: cs.LG

TL;DR: PonderTTT：基于自监督重建损失的训练无关门控策略，选择性触发测试时训练更新，在代码语言建模中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对所有输入采用统一计算，不考虑难度差异。需要一种能根据输入难度选择性触发测试时训练更新的方法，以提高计算效率

Method: 提出PonderTTT门控策略，使用TTT层的自监督重建损失作为门控信号。门控决策完全训练无关，无需学习分类器或辅助网络，仅需在未标记数据上校准单个标量阈值，并通过EMA持续调整以维持目标更新率

Result: 在GPT-2模型（124M到1.5B参数）的代码语言建模实验中，该方法在OOD语言上比随机跳过基线降低16%损失，达到82-89%的Oracle恢复率，且完全训练无关

Conclusion: 自监督重建损失是推理兼容的有效门控信号，PonderTTT能够高效选择性地触发测试时训练更新，显著提升模型在分布外数据上的性能

Abstract: Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).

</details>


### [45] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: DIPOLE是一种新颖的强化学习算法，通过将最优策略分解为一对稳定学习的二分策略（一个最大化奖励，一个最小化奖励），实现了稳定可控的扩散策略优化。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略训练方法存在两个主要问题：1）直接最大化价值目标导致训练不稳定；2）依赖粗糙的高斯似然近似需要大量小步去噪，计算成本高。需要一种稳定且可控的扩散策略优化方法。

Method: 提出DIPOLE算法：重新审视RL中的KL正则化目标，设计贪婪化策略正则化方案，将最优策略分解为一对二分策略（奖励最大化和最小化策略）。在推理时通过线性组合这两个策略的分数来生成优化动作，从而灵活控制贪婪程度。

Result: 在ExORL和OGBench的离线和离线到在线RL设置中验证了方法的有效性。还成功训练了用于端到端自动驾驶的大型视觉-语言-动作模型，并在大规模真实世界AD基准NAVSIM上进行了评估。

Conclusion: DIPOLE提供了一种稳定可控的扩散策略优化方法，能够有效训练大型扩散策略，并在复杂真实世界应用中展现出潜力。

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [46] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: 研究显示，在COVID-19引发的分布偏移下，保形预测的覆盖率下降程度差异巨大（0%-86.7%），单特征依赖是灾难性失效的关键因素。通过SHAP分析发现，特征重要性集中度与失效程度高度相关，季度重训练可部分修复灾难性任务，但稳健任务无需重训练。


<details>
  <summary>Details</summary>
Motivation: 保形预测在分布偏移下的性能保证会下降，但不同任务下降程度差异巨大。本研究旨在理解这种差异的原因，并开发应对策略，特别是在COVID-19引发的供应链任务分布偏移背景下。

Method: 使用COVID-19作为自然实验，分析8个供应链任务在严重特征变动（Jaccard约0）下的表现。通过SHAP分析特征重要性分布，计算特征集中度与覆盖率下降的相关性。测试季度重训练的效果，并探索4个中等特征稳定性任务以验证假设。

Result: 覆盖率下降从0%到86.7%不等，差异达两个数量级。灾难性失效与单特征依赖高度相关（rho=0.714, p=0.047）。灾难性任务特征重要性集中在单一特征（增加4.5倍），稳健任务则分布在多个特征（10-20倍）。季度重训练将灾难性任务覆盖率从22%提升至41%（+19pp, p=0.04），但对稳健任务无益（99.8%覆盖率）。

Conclusion: 特征稳定性而非集中度决定模型稳健性，但集中度效应在严重偏移下特别重要。提出决策框架：部署前监控SHAP集中度；若集中度>40%则季度重训练；若稳健则无需重训练。这为分布偏移下的保形预测应用提供了实用指南。

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [47] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: 提出LC-CVAE方法，通过强制潜在空间在锚点位置的一致性，从有限气候模型运行中生成统计一致的新实现，解决传统CVAE在跨实现泛化上的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模气候模型集合计算成本高昂，但许多下游分析需要额外的统计一致的气候变量实现。需要从有限运行中生成新实现的方法。

Method: 提出潜在约束条件变分自编码器(LC-CVAE)，在共享地理"锚点"位置强制跨实现潜在嵌入的同质性，然后使用多输出高斯过程回归预测新实现中未采样位置的潜在坐标，最后解码生成完整时间序列场。

Result: 实验显示：(1)单实现训练不稳定；(2)纳入约5个实现后收益递减；(3)空间覆盖与重建质量之间存在权衡，这与潜在空间中的平均邻近距离密切相关。

Conclusion: LC-CVAE方法能有效从有限气候模型运行中生成统计一致的新实现，解决了传统CVAE的泛化问题，为气候建模提供了一种计算高效的替代方案。

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [48] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 本文提出Lazy Attention机制，通过位置区分和弹性Softmax解决注意力过载和欠载问题，缓解表示崩溃和注意力沉没，在多个基准测试中实现竞争性性能和高达59.58%的注意力稀疏性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的标准注意力机制存在表示崩溃和注意力沉没两个已知问题，先前研究通常孤立地解决这些问题，缺乏对它们深层联系的统一理解。本文认为这两个问题都源于不恰当的注意力分配。

Method: 提出Lazy Attention机制：1) 针对注意力过载问题，采用跨头和跨维度的位置区分来增强token区分度；2) 针对注意力欠载问题，引入弹性Softmax归一化函数，放松标准softmax约束以抑制对无关token的关注。

Result: 在FineWeb-Edu语料库上的实验表明，Lazy Attention成功缓解了注意力沉没问题，在九个多样化基准测试中与标准注意力和现代架构相比达到竞争性性能，同时实现了高达59.58%的注意力稀疏性。

Conclusion: Lazy Attention通过统一的视角解决了注意力过载和欠载问题，为注意力机制提供了更集中的注意力分布，有效缓解了表示崩溃和注意力沉没，同时保持了模型性能。

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [49] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: MODE：一个结合低秩神经ODE与增强Mamba架构的统一时间序列预测框架，通过低秩近似和动态选择性扫描机制，在保持表达力的同时提升计算效率和长程依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在效率、可扩展性和准确性之间难以平衡，特别是在处理长程依赖和不规则采样数据时存在挑战。需要一种能够统一处理这些问题的框架。

Method: 提出MODE框架：1）线性标记化层处理输入序列；2）多个增强Mamba编码器块，每个包含因果卷积、SiLU激活和低秩神经ODE增强；3）基于伪ODE动态的分段选择性扫描机制，自适应聚焦重要子序列；4）低秩公式减少计算开销。

Result: 在基准数据集上的大量实验表明，MODE在预测准确性和计算效率方面均超越现有基线方法。

Conclusion: MODE通过统一架构、Mamba选择性扫描与低秩神经ODE的集成、以及低秩近似和动态选择性扫描带来的效率提升，为长期时间序列建模提供了有效的解决方案。

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [50] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: 该研究使用量子核方法和几何感知SPD描述符，从血液和支气管肺泡灌洗液生物标志物预测COPD小鼠模型的肌肉功能指标，在低数据、低特征场景下取得了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 慢性阻塞性肺病（COPD）常伴随骨骼肌功能障碍，与全身和气道炎症密切相关。研究旨在通过微创生物标志物纵向预测肌肉功能指标，为临床提供预测工具。

Method: 使用213只动物（Sham组与香烟烟雾暴露组）的临床前数据集，包含血液和支气管肺泡灌洗液测量值。比较了调优的经典基线方法、几何感知对称正定（SPD）描述符（使用Stein散度）以及为低维表格数据设计的量子核模型。

Result: 在肌肉重量预测中，使用四个可解释输入（血液C反应蛋白、中性粒细胞计数、支气管肺泡灌洗细胞数和条件）的量子核岭回归获得测试RMSE为4.41mg，R²为0.605，优于相同特征集的岭回归基线（4.70mg，0.553）。几何感知Stein散度原型距离在仅使用生物标志物的设置中也获得一致改进（4.55mg vs 4.79mg）。筛查式评估（以训练组Sham均值的0.8倍为阈值）检测低肌肉重量的ROC-AUC最高达0.90。

Conclusion: 几何和量子核提升方法在低数据、低特征的生物医学预测问题中能提供可测量的性能优势，同时保持可解释性和透明的模型选择，为COPD肌肉功能障碍的预测建模提供了新思路。

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [51] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出一种将算法源代码转换为数值嵌入的通用方法，通过动态分析程序在不同输入下的行为，并为分析指标定制复杂度函数，基于r-Complexity构建嵌入，使用XGBoost在Codeforces代码片段数据集上实现多标签分类


<details>
  <summary>Details</summary>
Motivation: 需要一种通用方法将算法源代码转换为数值表示（嵌入），以便进行机器学习分析，特别是针对编程竞赛中的代码片段分类问题

Method: 动态分析程序在不同输入下的行为，为分析指标定制多个通用复杂度函数，基于r-Complexity构建代码嵌入，使用XGBoost算法进行分类

Result: 在包含11个类别的多标签数据集（基于Codeforces平台真实代码片段）上，实现了平均F1分数（具体分数未在摘要中给出）

Conclusion: 提出的代码嵌入方法能够有效将算法源代码转换为数值表示，支持机器学习模型在代码分析任务中的应用

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [52] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的产品开发框架，使用联合神经网络优化多个相关属性，并引入ConfMC方法提供不确定性估计和覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 传统产品开发需要大量实验来探索设计参数与性能的关系，而多属性优化中属性间的相关性增加了复杂性。需要一种能够同时优化多个相关属性并提供可靠不确定性估计的方法。

Method: 1) 使用神经网络学习设计规格与产品属性之间的关系；2) 应用投影梯度下降寻找最优设计特征；3) 使用联合神经网络捕获多目标间的相互依赖关系；4) 提出ConfMC方法（结合嵌套共形预测和蒙特卡洛dropout）进行不确定性估计。

Result: 在五个真实世界数据集上的实验表明，该方法达到了最先进的性能水平，同时提供了自适应、非均匀的预测区间，并且在调整覆盖水平时无需重新训练模型。

Conclusion: 该框架为数据驱动的产品开发提供了一种有效的多属性优化方法，通过ConfMC技术提供了模型无关的有限样本覆盖保证，具有实际应用价值。

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [53] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: 提出LOFA算法用于在线影响力最大化问题，在完全bandit反馈下实现更低经验遗憾


<details>
  <summary>Details</summary>
Motivation: 在线影响力最大化问题中，现有算法虽然利用子模性实现低遗憾，但仍有改进空间，需要更高效的算法来降低经验遗憾

Method: 提出Lazy Online Forward Algorithm (LOFA)，进一步利用影响力函数的子模性质，在完全bandit反馈模型下工作

Result: 在真实社交网络上的实验表明，LOFA在累积遗憾和瞬时奖励方面优于现有bandit算法

Conclusion: LOFA算法通过更充分地利用子模性质，在在线影响力最大化问题上实现了更优的性能表现

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [54] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: 稀疏MoE架构在随机解码下的可靠性研究：指令微调而非架构稀疏性是决定模型在确定性任务中鲁棒性的主要因素


<details>
  <summary>Details</summary>
Motivation: 随着稀疏MoE架构在大型语言模型中的普及，需要研究其在随机解码下的可靠性。虽然条件计算带来了计算效率的提升，但稀疏路由与基于温度的采样之间的相互作用是否会损害输出稳定性尚不清楚。

Method: 评估三个代表性模型：OLMoE-7B（稀疏基础模型）、Mixtral-8x7B（稀疏指令微调模型）和Qwen2.5-3B（密集指令微调模型）。在具有客观可验证答案的确定性算术推理任务上进行测试，涵盖四种解码配置（从贪婪解码到T=1.0），评估准确性、格式合规性、重复生成输出一致性和置信度指标，总计9,360次模型生成。

Result: 稀疏指令微调模型在所有解码温度下表现出与密集指令微调模型相当的稳定性，而稀疏基础模型随着温度升高出现系统性性能下降。这表明指令微调而非架构稀疏性是决定模型在确定性任务中对解码随机性鲁棒性的主要因素。

Conclusion: 指令微调是确保稀疏语言模型在可靠性关键应用中稳定性的关键，在某些场景下稀疏架构可以安全采用而不牺牲输出稳定性。

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [55] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在开放集识别（OSR）中的表现，将计算机视觉中的特征衰减假设移植到文本处理，评估了COSTARR等方法的性能，发现在高类别数设置下效果有限。


<details>
  <summary>Details</summary>
Motivation: Transformer分类器（如BERT）在封闭集上表现优异，但在面对未见类别输入时脆弱，这是部署NLP系统常见场景。需要研究文本的开放集识别问题，探索计算机视觉中的OSR方法在NLP中的适用性。

Method: 将计算机视觉中的COSTARR框架适配到两个语言模型（BERT base和GPT-2），训练它们标注176个arXiv学科领域。同时评估了最大softmax概率（MSP）、MaxLogit和温度缩放自由能分数，使用OOSA和AUOSCR指标进行评测。

Result: COSTARR可以扩展到NLP而无需重新训练，但在统计上没有显著优于MaxLogit或MSP；自由能分数在高类别数设置下落后于所有其他分数。

Conclusion: 研究显示了将视觉中心OSR思想移植到语言模型的潜力和当前局限性，指出需要更大的骨干网络和任务定制的衰减策略。

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [56] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: 提出一种基于LIME解释的对抗鲁棒性训练框架，通过抑制虚假特征提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全关键领域应用日益广泛，需要既对抗扰动鲁棒又决策透明的防御方法。研究发现可解释性方法识别的虚假、不稳定或语义无关特征会显著增加对抗脆弱性。

Method: 提出基于属性引导的精炼框架，将LIME从被动诊断工具转变为主动训练信号。通过特征掩码、敏感感知正则化和对抗增强构建闭环精炼流程，无需额外数据集或模型架构。

Result: 在CIFAR-10、CIFAR-10-C和CIFAR-100数据集上评估，在对抗鲁棒性和分布外泛化方面取得显著提升。

Conclusion: 建立了可解释性与鲁棒性之间的直接联系，提出了一种将解释对齐作为主动训练信号的有效框架，为构建更可靠、透明的深度学习系统提供了新途径。

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [57] [Zero-shot Forecasting by Simulation Alone](https://arxiv.org/abs/2601.00970)
*Boris N. Oreshkin,Mayank Jauhari,Ravi Kiran Selvam,Malcolm Wolff,Wenhao Pan,Shankar Ramasubramanian,Kin G. Olivares,Tatiana Konstantinova,Andres Potapczynski,Mengfei Cao,Dmitry Efimov,Michael W. Mahoney,Andrew G. Wilson*

Main category: cs.LG

TL;DR: SarSim0：首个实用的单变量时间序列模拟器，基于SARIMA模型，通过三步法生成稳定、多季节性、间歇性的时间序列，支持零样本预测，在M-Series和GiftEval基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 零样本时间序列预测面临数据有限、评估易泄露、隐私和许可限制等挑战，需要一种既能快速生成数据又能实现良好零样本预测性能的模拟器。

Method: 基于SARIMA模型的三步模拟流程：1) 从特征多项式稳定区域采样稳定轨迹；2) 通过叠加方案组合多个路径生成多季节性序列；3) 添加基于速率的重尾噪声模型捕捉突发性和间歇性。

Result: SarSim0比基于核的生成器快几个数量级，可训练约10亿个模拟序列；神经网络骨干在零样本协议下表现出强泛化能力，超越统计预测器和近期基础模型基线，在GiftEval上甚至超过生成过程AutoARIMA的准确性。

Conclusion: SarSim0是首个实用的单变量时间序列模拟器，解决了零样本预测中的数据限制问题，实现了优异的零样本预测性能，为工业预测应用提供了有效解决方案。

Abstract: Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.

</details>


### [58] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 论文提出收缩扩散策略（CDPs），通过在扩散采样动力学中引入收缩性来增强离线策略学习的鲁棒性，减少求解器和分数匹配误差的影响，在数据稀缺时表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然作为离线策略学习的强大生成模型，但其基于分数的SDE建模会引入求解器和分数匹配误差、需要大量数据、且动作生成存在不一致性。这些在图像生成中不太关键的问题在连续控制环境中会累积并导致失败。

Method: 提出收缩扩散策略（CDPs），在扩散采样动力学中诱导收缩行为。收缩性将附近的流拉近，增强对求解器和分数匹配误差的鲁棒性，同时减少不必要的动作方差。开发了理论分析框架和实际实现方法，可以最小化修改和计算成本地集成到现有扩散策略架构中。

Result: 在仿真和真实世界环境中进行了广泛实验评估。在多个基准测试中，CDPs通常优于基线策略，在数据稀缺情况下表现出更明显的优势。

Conclusion: 收缩扩散策略通过引入收缩性有效解决了扩散策略在连续控制中的误差累积问题，提高了鲁棒性和数据效率，为离线策略学习提供了更可靠的解决方案。

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [59] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: 使用多种机器学习算法分析混凝土配合比对氯离子渗透时间演化的影响，为基础设施寿命评估提供数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 评估受侵蚀环境中混凝土结构的服役寿命，需要理解氯离子渗透的时间演化规律，而传统方法难以捕捉混凝土配合比与氯离子渗透之间的复杂非线性关系。

Method: 采用简单和复杂的机器学习算法：线性回归、KNN回归、核岭回归、支持向量回归、高斯过程回归、多层感知机和门控循环单元，基于综合数据集预测氯离子渗透。

Result: 核岭回归、高斯过程回归和多层感知机表现出高精度，GRU因数据多样性未能准确预测。多数配合比组分与氯离子含量呈负相关，少数呈正相关。

Conclusion: 机器学习方法能有效揭示混凝土配合比与氯离子渗透的潜在关联，为描述氯离子渗透物理过程提供替代方法，有助于延长基础设施服役寿命。

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [60] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: 深度Transformer中的表示崩溃本质上是几何问题，而非优化问题，作者提出通过流形约束和深度增量学习来避免表示退化


<details>
  <summary>Details</summary>
Motivation: 现有解释将Transformer深度扩展时的表示崩溃归因于优化不稳定或梯度消失，但这无法解释为什么在现代归一化和初始化方案下崩溃仍然存在。作者认为这是一个根本的几何问题。

Method: 提出统一的几何框架：1) 流形约束超连接限制残差更新到有效的局部切向方向，防止不受控制的流形漂移；2) 深度增量学习引入数据依赖的非单调更新，能够反射和擦除冗余特征而非无条件累积。

Result: 这些机制解耦了特征更新的方向和符号，实现了跨深度的稳定几何演化。由此产生的架构称为流形几何Transformer(MGT)。

Conclusion: 强制几何有效性同时允许动态擦除对于避免超深度网络中的秩崩溃至关重要。几何而非深度本身是深度表示学习的关键限制因素。

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [61] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: 该研究探讨了在自编码器和变分自编码器中应用随机傅里叶变换对训练过程和推理性能的影响，通过频率原理分析发现RFT模型能同时学习低频和高频特征，而传统DNN只能从低频开始逐步学习高频。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索随机傅里叶变换如何改进深度神经网络的训练过程和推理性能，特别是在自编码器和变分自编码器中进行基于重构的异常检测任务。

Method: 采用随机傅里叶变换和可训练的傅里叶变换变体，通过频率原理分析模型训练行为，使用两个低维合成数据集进行数据表示分析，以及航空安全数据集进行高维重构异常检测。

Result: 结果显示傅里叶变换模型优于传统模型，但可训练傅里叶变换相对于随机变体的优势尚不明确。RFT模型能同时学习低频和高频特征，而传统DNN只能从低频开始逐步学习。

Conclusion: 傅里叶变换能显著改进自编码器和变分自编码器的性能，特别是在异常检测任务中。虽然可训练傅里叶变换的优势尚不明确，但随机傅里叶变换已显示出明显优势。

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [62] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: 基于Wiener混沌展开的神经算子架构，用于学习SDE/SPDE的解算子，通过正交Hermite特征投影噪声路径，用神经算子参数化确定性混沌系数，实现从噪声到完整解轨迹的单次前向计算。


<details>
  <summary>Details</summary>
Motivation: SDE和SPDE是建模随机动力学的基本工具，开发深度学习模型逼近其解算子不仅能提供快速实用的求解器，还能为经典学习任务提供新视角。传统方法在处理复杂随机系统时存在计算效率低或灵活性不足的问题。

Method: 基于经典Wiener混沌展开(WCE)，将驱动噪声路径投影到正交Wick Hermite特征上，用神经算子参数化得到的确定性混沌系数，从而可以从噪声直接重构完整解轨迹。理论方面，为多维SDE和半线性SPDE明确写出混沌系数的耦合ODE/PDE系统，使随机强迫与确定性动力学分离。

Result: 在多个问题上验证模型：经典SPDE基准测试、图像上的扩散单步采样、图上的拓扑插值、金融外推、参数估计以及洪水预测的流形SDE，展示了竞争性精度和广泛适用性。

Conclusion: WCE基神经算子为学习SDE/SPDE解算子提供了实用且可扩展的方法，在多个领域表现出良好性能，表明该框架具有广泛的适用潜力。

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [63] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: 提出了一个任务和模型感知的无线数据集相似性度量框架，用于预测跨数据集可迁移性，在CSI压缩和波束预测任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 无线通信中需要比较不同数据集的相似性来支持数据集选择/增强、仿真到真实数据比较、任务特定合成数据生成以及模型训练/适应决策，但缺乏系统化的度量方法。

Method: 提出了任务和模型感知的框架，使用基于UMAP嵌入的度量结合Wasserstein和欧几里得距离，对于监督任务还集成了监督UMAP和数据集不平衡惩罚。

Result: 在无监督CSI压缩任务中，数据集距离与训练-测试性能的Pearson相关系数超过0.85；在监督波束预测任务中，提出的标签感知距离优于传统基线方法。

Conclusion: 该框架能够有效度量无线数据集之间的相似性，准确预测模型跨数据集的可迁移性，支持任务相关的数据集比较，为无线通信中的数据集管理提供了实用工具。

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [64] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: 提出一种基于信息论Lyapunov函数的投影反向扩散方法，用于在生成模型中控制粗粒度量（如图像块强度）的演化


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏理论描述粗粒度量（如图像块强度）在反向扩散过程中的演化规律，需要开发能显式控制这些量的方法

Method: 将信息论Lyapunov函数V移植到反向扩散过程，提出V-delta投影反向扩散方案，扩展单调性到非齐次块保持马尔可夫核

Result: 数值实验表明，该方法能将块质量误差和泄漏容忍势保持在预设容差内，同时保持与非投影动态相当的像素级精度和视觉质量

Conclusion: 将生成采样重新解释为从噪声到数据的信息势减少过程，为具有显式粗粒度量控制的反向扩散过程提供了设计原则

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [65] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: 提出ML-UCB算法，将任意机器学习模型集成到多臂老虎机框架中，通过建模学习曲线行为解决传统方法缺乏可处理浓度不等式的问题。


<details>
  <summary>Details</summary>
Motivation: 在序列决策中部署复杂ML模型面临的主要挑战是缺乏可处理的浓度不等式来进行原则性探索。传统方法需要针对每个模型进行特定理论分析，限制了先进ML模型在多臂老虎机中的应用。

Method: 假设均方误差随训练样本数呈幂律下降，推导出广义浓度不等式，提出ML-UCB算法。该算法直接建模底层估计器的学习曲线行为，无需模型特定理论分析，只要学习曲线可以经验性表征即可。

Result: 理论上证明ML-UCB能够实现次线性遗憾。实验验证中，在协同过滤推荐系统上使用在线矩阵分解（模拟简化双塔模型）进行测试，相比LinUCB有显著改进。

Conclusion: ML-UCB为任意机器学习模型提供了原则性集成到多臂老虎机框架的通用方法，通过建模学习曲线行为克服了传统浓度不等式的限制，实现了更灵活的序列决策系统设计。

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [66] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 该研究提出了一种端到端的视觉播客生成管道，通过微调Qwen3-VL-32B模型，使用合成到真实的训练策略，在视觉叙事任务中显著提升了对话自然性和叙事深度。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在描述性任务上表现优异，但在生成引人入胜的长篇叙事（特别是多说话者播客对话）方面仍未被充分探索且难以评估。现有标准指标无法捕捉对话自然性、个性和叙事流程的细微差别，往往奖励安全、重复的输出而非引人入胜的叙事。

Method: 1. 提出端到端视觉播客生成管道；2. 在4000个图像-对话对的数据集上微调Qwen3-VL-32B模型；3. 采用合成到真实的训练策略：在结构化播客研究语料库的高质量播客对话与合成生成图像上进行训练，在视觉叙事数据集的真实世界照片序列上进行评估；4. 提出超越文本重叠的综合评估框架，使用AI作为评判和新型风格指标。

Result: 微调的32B模型在对话自然性上显著优于235B基础模型（胜率>80%），叙事深度提升50%（平均对话轮次长度增加），同时保持相同的视觉基础能力（CLIPScore: 20.39）。

Conclusion: 该研究展示了通过精心设计的训练策略和评估框架，相对较小的模型可以在视觉叙事任务中超越大型基础模型，为视觉语言模型在创造性叙事生成领域的发展提供了新方向。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [67] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: 本文提出将基于TinyML的低功耗边缘设备集成到水产养殖系统中，实现实时自动化监测和控制，解决传统人工监测效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 水产养殖业面临水质波动、疾病爆发和饲料管理效率低等挑战，传统人工监测方法耗时且可能导致问题处理延迟，需要更高效的自动化解决方案。

Method: 集成基于TinyML的低功耗边缘设备，通过传感器实时收集pH值、温度、溶解氧、氨氮水平等参数数据，实现自动化监测、异常报警和控制功能。

Result: 系统能够提供实时水质监测数据，实现异常检测报警，收集的数据可用于优化水处理过程、饲料分配和模式分析，提高饲料效率并降低运营成本。

Conclusion: 研究表明TinyML在水产养殖监测中具有可行性，通过考虑传感器选择、算法设计、硬件约束和伦理因素，可为开发更可持续、高效的养殖实践做出贡献。

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [68] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种改进的分析框架，简化了非平稳参数化赌博机中加权策略的理论分析，产生了更简单的算法设计，并在线性赌博机、广义线性赌博机和自协调赌博机中取得了更好的遗憾界，同时还将框架扩展到具有函数近似的非平稳马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 非平稳参数化赌博机中，加权策略在处理渐变漂移模式时常用，但先前理论分析复杂，导致算法计算效率低或统计次优。本文旨在解决加权策略分析复杂、算法设计繁琐的问题。

Method: 提出了一种精炼的分析框架，简化了加权策略的推导过程。在线性赌博机中，该框架产生了更简单的基于权重的算法。框架还可扩展到广义线性赌博机、自协调赌博机，以及具有函数近似的非平稳马尔可夫决策过程（线性混合MDP和多项Logit混合MDP）。

Result: 在线性赌博机中，新算法与窗口/重启算法同样高效，且保持相同遗憾界。在广义线性赌博机中，获得了 $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ 的遗憾界，优于先前的 $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$。框架还成功扩展到非平稳MDPs，为线性混合MDP和多项Logit混合MDP建立了动态遗憾保证。

Conclusion: 本文提出的精炼分析框架解决了非平稳参数化赌博机中加权策略分析复杂的问题，简化了算法设计，提高了理论性能，并成功扩展到更广泛的模型，为非平稳环境下的强化学习提供了更有效的理论工具。

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [69] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: 提出Flow Equivariant World Models框架，将自运动和外部物体运动统一为单参数李群"流"，利用群等变性实现稳定潜在世界表示，在2D/3D部分观测视频世界建模基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 具身系统体验世界为"流动的交响曲"：多个连续感官输入流与自运动耦合，交织着外部物体动态。这些流遵循平滑的时间参数对称性，但大多数神经网络世界模型忽略这种结构，反复从数据中重新学习相同变换。

Method: 引入Flow Equivariant World Models框架，将自运动和外部物体运动统一为单参数李群"流"，利用这种统一实现对变换的群等变性，提供数百时间步的稳定潜在世界表示。

Result: 在2D和3D部分观测视频世界建模基准上，Flow Equivariant World Models显著优于可比较的最先进扩散基和记忆增强世界建模架构，特别是在智能体当前视野外存在可预测世界动态时。流等变性对长序列外推特别有益，能泛化远超训练范围。

Conclusion: 通过将世界模型表示结构与内部和外部运动对齐，流等变性为数据高效、对称引导的具身智能开辟了可扩展路径。

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [70] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: DMS算法通过使用连续折扣模型解决高维度量空间中QD优化的失真问题，优于CMA-MAE等现有方法


<details>
  <summary>Details</summary>
Motivation: 现有QD算法在高维度量空间中存在失真问题，即许多解映射到相似的度量值，导致探索停滞。CMA-MAE使用直方图记录折扣值，但在高维空间中相似度量的解落入同一单元格，获得相同折扣值而无法继续探索。

Method: 提出折扣模型搜索(DMS)，使用提供平滑连续折扣值表示的模型来指导探索。该模型能够区分高维度量空间中相似度量的解，从而持续探索。

Result: DMS在高维基准测试和两个新应用领域（度量空间为图像高维空间）中表现优于CMA-MAE和其他黑盒QD算法。新应用允许用户通过提供图像数据集而非手动设计度量函数来指定所需度量。

Conclusion: DMS通过连续折扣模型有效解决了高维度量空间中的失真问题，推动了QD在图像等高维度量空间中的应用，为QD算法在高维场景中的探索提供了新方法。

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [71] [Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding](https://arxiv.org/abs/2601.01089)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT是一个整合DNA、RNA和蛋白质预训练语言模型的架构，通过中心法则逻辑实现多模态整合，在CRISPRi增强子扰动数据上取得良好预测性能，并提供机制解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然DNA、RNA和蛋白质的领域特定基础模型各自取得成功，但它们仍然孤立，限制了我们对整合细胞过程的建模能力。需要一种能够遵循中心法则信息流方向的整合架构。

Method: 提出中心法则变换器(CDT)，整合DNA、RNA和蛋白质的预训练语言模型，采用方向性交叉注意力机制：DNA-to-RNA注意力建模转录调控，RNA-to-Protein注意力建模翻译关系，产生统一虚拟细胞嵌入。

Result: 在K562细胞的CRISPRi增强子扰动数据上，CDT v1实现Pearson相关系数0.503，达到交叉实验变异性理论上限(r=0.797)的63%。注意力和梯度分析提供互补的解释窗口，梯度分析识别出CTCF结合位点。

Conclusion: 与生物信息流对齐的AI架构既能实现预测准确性，又能提供机制解释性，为整合多模态细胞生物学建模提供了有前景的方向。

Abstract: Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.

</details>


### [72] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: 开发了一个可解释的机器学习框架，用于孟加拉国和南亚人群的早期慢性肾病社区筛查，相比现有工具显著提高了准确性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有CKD筛查工具主要基于高收入国家人群开发，在孟加拉国和南亚地区表现不佳，且大多依赖简单的加性评分函数，无法捕捉风险因素间的复杂交互作用，对早期CKD预测能力有限。

Method: 使用孟加拉国社区数据集（南亚首个此类数据集），评估12种ML分类器，应用10种互补特征选择技术识别稳健预测因子，采用10折交叉验证，并在印度、阿联酋和孟加拉国的三个独立数据集上进行外部验证，使用SHAP提供模型可解释性。

Result: RFECV选择特征子集的ML模型平衡准确率达90.40%，最小非病理测试特征集平衡准确率达89.23%，通常优于更大或完整特征集。相比现有筛查工具，准确性和敏感性显著提高且输入更少。外部验证显示78%至98%的敏感性，SHAP识别出与已知CKD风险因素一致的临床意义预测因子。

Conclusion: 该研究开发了一个针对孟加拉国和南亚人群的可解释ML框架，能够有效进行早期CKD社区筛查，相比现有工具表现更优，具有良好泛化能力，为低资源环境提供了实用筛查解决方案。

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [73] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: HISTOGRAPH提出了一种基于注意力的两阶段图池化方法，利用中间层激活历史来改进图分类性能，特别是在深层GNN中表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图池化方法通常只使用最后一层GNN特征，未能充分利用前向传播过程中产生的中间层激活（历史图激活）。这在节点表示可能随层数显著变化的情况下尤其明显，且图特有的挑战（如深层架构中的过平滑问题）加剧了这一问题。

Method: HISTOGRAPH是一种新颖的两阶段基于注意力的最终聚合层：首先在中间激活上应用统一的层间注意力，然后应用节点间注意力。通过建模节点表示在层间的演化，该方法利用节点的激活历史和图结构来精炼用于最终预测的特征。

Result: 在多个图分类基准测试上的实证结果表明，HISTOGRAPH提供了强大的性能，持续改进了传统技术，在深层GNN中表现出特别强的鲁棒性。

Conclusion: 通过利用历史图激活，HISTOGRAPH能够更有效地聚合图特征，特别是在深层架构中，为解决图池化中的信息利用不足问题提供了有效方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [74] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 论文提出了一种基于维特根斯坦家族相似性概念的聚类算法WFR及其核变体，无需预设聚类数量或形状假设


<details>
  <summary>Details</summary>
Motivation: 将维特根斯坦的家族相似性哲学概念应用于机器学习聚类问题，解决传统聚类方法需要预设聚类数量和形状假设的限制

Method: 提出WFR算法：计算相邻数据实例间的相似度得分，通过阈值处理后构建相似图，图的连通分量即为聚类结果；还开发了核变体kernel WFR

Result: 在基准数据集上的模拟实验表明，WFR是一种有效的非线性聚类算法，不需要预先知道聚类数量或对其形状做出假设

Conclusion: 成功将哲学概念转化为实用的机器学习算法，为聚类问题提供了新的解决方案，展示了跨学科研究的价值

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [75] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经混沌学习与自训练的混合半监督学习方法，在标签数据稀缺的情况下显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，获取大量标注数据既困难又昂贵，而无标签数据却容易获得。传统的监督学习方法在标签数据稀少或数据集不平衡的情况下表现不佳。

Method: 提出了一种混合半监督学习架构，将神经混沌学习与基于阈值的自训练方法相结合。神经混沌学习将输入特征转换为混沌发放率表示，捕捉数据中的非线性关系；自训练则利用高置信度的伪标签样本逐步扩展标签集。

Result: 在10个基准数据集和5个机器学习分类器上评估，使用85%无标签和15%有标签数据。提出的NL+ST架构相比独立自训练模型获得显著性能提升，特别是在Iris(188.66%)、Wine(158.58%)和Glass Identification(110.48%)等有限、非线性和不平衡数据集上。

Conclusion: 混沌特征提取与半监督学习结合，在低数据环境下提高了泛化能力、鲁棒性和分类准确性，为解决标签稀缺问题提供了有效方案。

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [76] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: Evo-TFS：一种结合时频域特征的进化过采样方法，用于不平衡时间序列分类，通过强类型遗传编程生成多样化的高质量时间序列样本。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设数据分布平衡，在不平衡时间序列数据中容易忽略少数类；传统过采样方法依赖线性插值，难以保持时间动态特性和生成多样化样本。

Method: 提出Evo-TFS方法，使用强类型遗传编程同时考虑时域和频域特征，通过包含时频域特征的适应度函数指导进化过程，生成多样化的高质量时间序列样本。

Result: 在不平衡时间序列数据集上的实验表明，Evo-TFS优于现有过采样方法，显著提升了时域和频域分类器的性能。

Conclusion: Evo-TFS通过结合时频域特征的进化过采样方法，有效解决了不平衡时间序列分类问题，生成了多样化的高质量少数类样本。

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [77] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: ARISE利用LLM获取分类数据的语义嵌入，结合原始数据提升聚类质量，在8个基准数据集上相比7个对比方法提升19-27%


<details>
  <summary>Details</summary>
Motivation: 分类数据聚类面临相似性度量挑战，传统方法将无序属性值视为等距，存在语义鸿沟。现有基于共现模式的方法在样本有限时不可靠，未能充分利用数据的语义上下文。

Method: 提出ARISE框架，利用大语言模型获取属性值的语义描述，构建语义感知表示，将LLM增强的嵌入与原始数据结合，通过注意力加权机制探索语义显著的聚类结构。

Result: 在8个基准数据集上的实验表明，ARISE相比7个代表性对比方法取得一致改进，性能提升19-27%，代码已开源。

Conclusion: ARISE通过整合外部语义知识有效弥补了分类数据聚类的语义鸿沟，LLM提供的语义嵌入显著提升了聚类质量，特别是在样本有限的情况下。

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [78] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 使用多类型严肃游戏和机器学习预测软件开发岗位适应性，通过游戏行为特征达到97%精度和94%准确率


<details>
  <summary>Details</summary>
Motivation: 传统职业评估中的自我报告问卷存在反应偏差、疲劳和故意扭曲等问题，需要更客观、可扩展且无偏见的替代方案

Method: 通过文献综述和实证研究确定开发者相关特质，设计定制移动游戏捕捉问题解决、规划、适应性等行为，采用两阶段建模策略从游戏行为特征预测适应性

Result: 模型达到97%精度和94%准确率，合适候选人展现出独特的游戏行为模式（更多解谜胜利、更多侧挑战、更频繁菜单导航、较少暂停/重试/放弃）

Conclusion: 游戏过程中捕获的隐式行为痕迹能有效预测软件开发适应性，支持严肃游戏作为职业评估的可扩展、吸引人且偏见较少的替代方案

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [79] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯框架的稀疏符号图神经网络，通过建模带符号邻接矩阵的后验分布来处理异质性和边噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中的半监督学习常面临异质性挑战，现有图神经网络要么依赖固定邻接结构，要么通过正则化处理结构噪声，缺乏对结构不确定性的显式建模。

Method: 建模带符号邻接矩阵的后验分布（边可为正、负或缺失），提出稀疏符号消息传递网络，结合后验边缘化和稀疏符号消息聚合，从贝叶斯角度提供理论解释。

Result: 在合成和真实世界结构噪声下的异质性基准测试中，该方法优于强基线模型。

Conclusion: 通过显式建模结构不确定性，该方法为处理边噪声和异质性提供了原则性方法，在异质图学习任务上表现优异。

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [80] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: 提出混合贝叶斯-保形框架，结合贝叶斯层次随机森林和组感知保形校准，为临床决策提供分布自由的覆盖保证和风险自适应精度。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要不确定性量化，既要分布自由的覆盖保证，又要风险自适应精度，现有方法无法同时满足这两个要求。

Method: 集成贝叶斯层次随机森林与组感知保形校准，利用后验不确定性对保形分数进行加权，同时保持严格的覆盖有效性。

Result: 在61,538例入院患者、3,793家美国医院和4个地区的评估中，方法达到目标覆盖（94.3% vs 95%目标），低不确定性病例区间宽度减少21%，高风险预测适当加宽。纯贝叶斯不确定性严重欠覆盖（14.1%）。

Conclusion: 该框架支持风险分层临床协议、高效资源规划和高置信度预测，为不确定病例提供增强监督的保守分配，为多样化医疗环境提供不确定性感知决策支持。

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [81] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 研究提出"依赖鸿沟"框架，发现在基础设施脆弱环境中，高度投入的学生反而对网络故障更敏感，挑战了"投入越多越好"的传统假设。


<details>
  <summary>Details</summary>
Motivation: 传统数字鸿沟框架无法解释在相同网络条件下学生满意度差异的问题。研究旨在揭示在资源受限环境中，数字学习参与度与基础设施可靠性之间的复杂关系。

Method: 对孟加拉国396名大学生进行横断面研究，采用三阶段分析方法：K-prototypes聚类识别学生类型、随机森林模型分析满意度驱动因素、倾向得分匹配检验依赖鸿沟假设。

Result: 识别出三类学生：偶尔参与(58%)、高效学习者(35%)、高度投入(7%)。发现教育设备使用时间与网络可靠性的显著交互作用，证实依赖鸿沟存在。高度投入学生对基础设施故障最脆弱。

Conclusion: 在脆弱基础设施环境中，数字能力可能成为负担。政策应优先保障高依赖用户的可靠性，建立应急系统，并教育学生认识依赖风险，而非一味鼓励参与。

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [82] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: 该研究对Mamba SSM和LLaMA Transformer在长上下文序列建模上进行全面基准测试，使用治疗会话作为测试案例，比较计算效率和表示效率，为实践者提供何时选择SSM的指导。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)作为Transformer的替代方案在长上下文序列建模中展现出潜力，具有线性计算复杂度(O(N))，而Transformer是二次复杂度(O(N^2))。需要系统比较两者在实际长上下文应用中的表现。

Method: 使用治疗会话作为代表性测试案例，从两个维度评估Mamba SSM和LLaMA Transformer：1)计算效率(内存使用和推理速度，测试512到8,192个token)；2)表示效率(分析隐藏状态动态和注意力模式)。

Result: 研究结果为长上下文应用实践者提供了可操作的见解，明确了SSM相对于Transformer具有优势的具体条件。

Conclusion: 该基准研究为选择长上下文序列建模架构提供了实证依据，帮助实践者根据具体条件在SSM和Transformer之间做出明智选择。

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [83] [Accelerated Full Waveform Inversion by Deep Compressed Learning](https://arxiv.org/abs/2601.01268)
*Maayan Gelboim,Amir Adler,Mauricio Araya-Polo*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的全波形反演数据降维方法，通过压缩学习选择关键地震数据，结合自编码器表示学习和K-means聚类，实现分层数据选择，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 工业级全波形反演需要太字节级别数据存储，计算成本极高，限制了复杂地下情况分析和多场景探索。需要一种有效的数据降维方法来降低计算负担。

Method: 1. 使用带二值化感知层的深度神经网络，通过压缩学习从大量地下模型中学习简洁但关键的地震采集布局；2. 训练好的网络从大数据集中选择较小子集；3. 通过自编码器计算数据的潜在表示；4. 对潜在表示进行K-means聚类，进一步选择最相关的FWI数据。

Result: 该方法在仅使用10%数据的情况下，在2D FWI中始终优于随机数据采样，为大规模3D反演加速FWI铺平了道路。

Conclusion: 提出的分层选择方法能有效减少FWI输入维度，降低计算成本，为处理大规模地震数据提供了可行的解决方案。

Abstract: We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.

</details>


### [84] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: 本文通过比较LLM的上下文学习与监督分类器的行为，发现当演示相关性高时，LLM行为类似于kNN分类器；当相关性低时，LLM能利用参数记忆获得更好表现。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习在实践中被证明有效，但其工作机制仍不明确。本文旨在探究LLM在上下文学习中的行为机制，特别是与监督分类器的相似性和差异。

Method: 使用文本分类作为用例，在6个数据集和3个LLM上，将上下文学习与基于相同演示训练的监督分类器（梯度下降和k近邻）进行行为比较。

Result: 当演示相关性高时，LLM行为与监督分类器相似，且更接近kNN而非逻辑回归；当演示相关性低时，LLM表现优于这些分类器，因其能利用参数记忆。

Conclusion: 上下文学习机制在演示相关时类似于kNN分类，在演示不相关时能利用LLM的参数记忆优势，这为理解上下文学习的工作机制提供了实证依据。

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [85] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: 本文引入对数加权Barron空间，证明深度ReLU网络在该空间中具有更低的规律性要求，为深度架构在高维问题中的成功提供理论解释。


<details>
  <summary>Details</summary>
Motivation: 经典Barron空间理论虽然解释了神经网络近似能力，但对目标函数的规律性要求仍然过高（比Sobolev空间更强），且现有深度敏感结果通常假设约束条件（如sL ≤ 1/2），限制了理论对实际深度模型成功的解释力。

Method: 1. 引入对数加权Barron空间ℬ^log，其假设条件比任何s>0的ℬ^s空间都弱；2. 研究该空间的嵌入性质并通过Rademacher复杂度进行统计分析；3. 证明ℬ^log中函数可由深度ReLU网络近似，并给出显式的深度依赖关系；4. 定义ℬ^{s,log}空间族，建立H^1范数下的近似界，并确定保持这些速率的最大深度尺度。

Result: 1. 对数加权Barron空间ℬ^log提供了比经典Barron空间更弱的规律性要求；2. 深度ReLU网络在该空间中具有明确的深度依赖近似能力；3. 建立了H^1范数下的近似界，并确定了保持速率的最大深度尺度。

Conclusion: 深度减少了高效表示所需的规律性要求，为深度架构超越经典Barron设置的实际性能提供了更精确的理论解释，并解释了其在当今高维问题中稳定使用的原因。

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


### [86] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: Argus框架将高维数据流中的分布漂移检测重新定义为在数据流形固定空间划分上跟踪局部统计量，通过Voronoi划分实现正交变换不变的漂移度量，具有线性复杂度并能定位漂移空间位置。


<details>
  <summary>Details</summary>
Motivation: 高维数据流中的分布漂移检测面临三大挑战：全局比较方法扩展性差、基于投影的方法丢失几何结构、重新聚类方法存在身份不稳定性。需要一种既能保持高维结构又计算高效的方法。

Method: Argus框架将漂移检测重新定义为在数据流形固定空间划分上跟踪局部统计量。使用Voronoi划分在规范正交基上创建空间分区，证明这种划分产生的漂移度量对正交变换不变。引入图论方法区分连贯分布漂移与孤立扰动，使用乘积量化划分扩展到超高维度。

Result: 框架实现O(N)复杂度，提供单元级空间定位能力，能正确识别坐标旋转下的漂移而现有方法会产生误报。图论方法能有效区分漂移传播模式，乘积量化划分可扩展到500+维度。

Conclusion: Argus为分布监测提供了有原则的几何基础，在保持高维结构的同时避免了成对比较的计算负担，通过固定空间划分实现了高效、可解释的漂移检测。

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [87] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: 本文提出Muon++，一种改进的矩阵优化器，能够在整个训练过程中可靠地满足μP的谱条件，无需对权重进行显式谱归一化，降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: μP为大语言模型训练提供了理论基础，但现有矩阵优化器（如Muon）在μP框架下存在局限性：要么无法在整个训练过程中保证谱条件，要么需要频繁的谱归一化操作，计算开销大且不实用。

Method: 提出Muon++，核心洞察是对于中等规模模型，仅需在优化器更新层面维持谱控制即可保持μP兼容的缩放特性，无需对权重进行显式谱归一化。同时首次引入数据依赖的自适应谱条件。

Result: Muon++能够在整个训练过程中可靠地满足μP的谱条件，填补了μP理论与矩阵优化器实际部署之间的差距，降低了计算开销，更适合长时程LLM训练。

Conclusion: 本文提出的Muon++方法通过仅控制优化器更新的谱特性，可靠地保证了μP的谱条件，为矩阵优化器在μP框架下的实际应用提供了实用解决方案，并首次引入了自适应谱条件概念。

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [88] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: SWH是一种并行双流架构，通过全局分支（基于卷积定理）和局部分支（滑动窗口注意力）解耦序列建模，实现线性扩展至长序列，同时保持局部精度。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次方复杂度限制了其在长序列任务中的应用，需要在计算效率和表示表达能力之间取得平衡。

Method: 提出Spectral-Window Hybrid (SWH)架构，将序列建模解耦为两个并行流：全局分支利用卷积定理建模长程衰减动态（O(T log T)时间），局部分支采用滑动窗口注意力处理有界上下文内的token交互。

Result: SWH在短上下文上达到标准Transformer的困惑度，同时能够高效线性扩展到长序列。

Conclusion: SWH通过并行双流设计避免了全局注意力的计算瓶颈，同时保留了局部精度，为长序列建模提供了高效解决方案。

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [89] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: GM-MLG：基于图-基序特征融合和多标签生成的开放式药物不良反应预测新范式，将ADR预测从多标签分类转化为Transformer解码器驱动的多标签生成，显著提升预测性能并扩展预测空间


<details>
  <summary>Details</summary>
Motivation: 当前药物不良反应预测方法面临三大挑战：药物数据稀缺导致的冷启动问题、封闭标签集限制、以及标签依赖关系建模不足。这些限制阻碍了计算生物学在降低新药开发成本和时间方面的潜力。

Method: 提出GM-MLG方法：1）构建原子级、局部分子级（通过BRICS算法动态提取细粒度基序）和全局分子级的双图表示架构；2）将ADR预测从多标签分类转化为基于Transformer解码器的多标签生成，将ADR标签视为离散标记序列；3）使用位置嵌入显式捕获大规模标签空间中的依赖和共现关系，通过自回归解码动态扩展预测空间。

Result: 实验显示GM-MLG实现了最高38%的性能提升，平均增益达20%，将预测空间从200种扩展到超过10,000种。通过逆合成基序分析阐明了ADR与基序之间的非线性构效关系。

Conclusion: GM-MLG开创了开放式ADR预测新范式，通过多标签生成方法有效解决了冷启动、标签集封闭和依赖关系建模问题，为药物安全系统性风险降低提供了可解释的创新支持。

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [90] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot是一个LLM智能体，专门用于燃烧建模研究，通过自动化、自校正的CFD工作流程，将科学文献知识与CFD工具执行能力相结合，显著提升了模拟的成功率和可执行性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂科学领域（如燃烧建模）存在关键缺口，需要将领域文献知识与专业工具（如CFD代码）的执行能力无缝集成，以实现实用的AI辅助研究。

Method: FlamePilot采用基于原子工具的架构，确保在OpenFOAM和DeepFlame等框架中稳健设置和执行复杂模拟；系统能够从科学文献中学习，提取关键信息指导从初始设置到优化结果的整个模拟过程。

Result: 在公共基准测试中，FlamePilot获得了完美的1.0可执行性分数和0.438的成功率，超过了先前最佳代理的0.625和0.250分数；在MILD燃烧模拟案例研究中，系统能够自主将研究论文转化为配置模拟、执行模拟、后处理结果、提出基于证据的改进建议，并在最少人工干预下管理多步参数研究直至收敛。

Conclusion: FlamePilot通过透明可解释的范式，为AI赋能的燃烧建模建立了基础框架，促进了研究人员与智能体之间的协作伙伴关系，智能体负责工作流程编排，研究人员则专注于高层次分析。

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [91] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: 提出基于f-GAN框架的因果发现方法，从存在未测量混杂因素的数据中学习二元因果结构，将结构学习问题转化为最小化贝叶斯自由能量，并证明其等价于最小化真实数据分布与模型生成分布之间的f-散度。


<details>
  <summary>Details</summary>
Motivation: 从存在未测量混杂因素的数据中进行因果发现是一个具有挑战性的问题，需要开发能够独立于特定权重值学习因果结构的方法。

Method: 基于f-GAN框架，将结构学习问题重新表述为最小化贝叶斯自由能量，并证明其等价于最小化f-散度。通过f-GAN框架将目标转化为min-max对抗优化问题，使用Gumbel-Softmax松弛在离散图空间中进行梯度搜索。

Result: 该方法能够从存在未测量混杂因素的数据中学习二元因果结构，通过对抗优化框架实现有效的结构学习。

Conclusion: 提出的基于f-GAN的因果发现方法为解决存在未测量混杂因素的因果结构学习问题提供了有效的框架，通过对抗优化和离散空间梯度搜索实现了鲁棒的因果发现。

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [92] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: 提出一个轻量级的两阶段框架，能在训练前根据数据集特性和模型结构预测模型性能，无需实际训练或复杂模拟。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构选择通常依赖试错法，耗时耗资源且难以自动化。现有性能预测方法要么需要部分训练，要么计算开销大，缺乏通用性。

Method: 两阶段框架：第一阶段基于数据集的可测量属性预测基线性能；第二阶段结合模型架构和超参数细节调整估计。框架可跨数据集和模型类型泛化。

Result: 框架不仅能预测模型性能，还能指导架构选择、预处理流程，并在训练前检测潜在问题数据集。发现数据集方差等特征可作为数据质量的早期指标。

Conclusion: 提出的轻量级框架提供了一种高效、通用的模型性能预测方法，减少试错成本，同时为模型选择和数据处理提供实用指导。

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [93] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: 提出SaMPFA框架，通过局部拓扑切片采样和多任务图学习模型，提升电力系统潮流分析在不同系统规模下的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发对拓扑变化具有强适应性的深度学习模型对潮流分析具有重要实际意义，需要增强模型在可变系统规模下的性能，并提高支路功率预测的鲁棒性。

Method: 提出SaMPFA框架，包含：1) 局部拓扑切片采样技术，从完整电网中提取不同规模的子图以增强跨尺度学习能力；2) 无参考多任务图学习模型，预测母线电压和支路功率而非相角，避免误差放大并学习相角差的物理关系；3) 损失函数中加入额外项，鼓励模型捕捉角差和功率传输的物理模式。

Result: 在IEEE 39节点系统和实际省级电网上的仿真表明，该模型在可变系统规模下实现了优越的适应性和泛化能力，准确率分别提高了4.47%和36.82%。

Conclusion: SaMPFA框架通过局部拓扑切片和多任务学习，有效提升了深度学习模型在电力系统潮流分析中的跨尺度适应性和物理一致性，为拓扑变化的潮流分析提供了有效解决方案。

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [94] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: GDME是一个基于图的无监督在线时间序列异常检测框架，通过动态模型池和图结构进行模型集成，能有效处理异构流数据并检测概念漂移。


<details>
  <summary>Details</summary>
Motivation: 工业系统中流数据量不断增加，在线异常检测变得至关重要。现有方法多为离线设计或难以有效处理异构流数据，且数据模式多样且快速演变带来重大挑战。

Method: 提出GDME框架：维护动态模型池（持续修剪性能不佳模型并引入新模型）；使用动态图结构表示模型间关系；通过图上的社区检测选择适当的集成子集；利用图结构变化监测概念漂移。

Result: 在七个异构时间序列数据集上的实验表明，GDME优于现有在线异常检测方法，提升高达24%；其集成策略相比单个模型和平均集成具有更优检测性能，且计算效率具有竞争力。

Conclusion: GDME通过动态模型池和图结构集成，有效解决了在线时间序列异常检测中处理异构流数据和适应概念漂移的挑战，在性能和效率方面均表现优异。

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [95] [A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory](https://arxiv.org/abs/2601.01417)
*Itay Safran*

Main category: cs.LG

TL;DR: 本文证明了ReLU神经网络计算d个实数最大值函数需要超线性宽度，当深度3≤k≤log₂(log₂(d))时，宽度至少为Ω(d^{1+1/(2^{k-2}-1)})，这是首个针对深度k≥3的无条件超线性下界。


<details>
  <summary>Details</summary>
Motivation: 最大值函数是神经网络中的基本算子，但对其计算复杂度缺乏深入理解。现有研究主要关注浅层网络，对于深层网络计算最大值所需的最小宽度和深度关系尚不清楚。本文旨在填补这一空白，揭示最大值函数的内在复杂性。

Method: 采用组合论证方法，将最大值函数的不可微分脊线与计算网络第一隐藏层诱导的图中的团相关联。利用极值图论中的Turán定理，证明足够窄的网络无法捕捉最大值函数的非线性特性。

Result: 证明了深度层次定理：对于任意深度3≤k≤log₂(log₂(d))，表示最大值函数需要宽度Ω(d^{1+1/(2^{k-2}-1)})。这是首个针对深度k≥3的无条件超线性下界，即使深度随d变化也成立。

Conclusion: 尽管最大值函数看似简单，但其不可微分超平面的几何结构赋予了固有的复杂性。该证明技术为深度神经网络下界证明提供了新方法，揭示了深度与宽度在表示能力上的权衡关系。

Abstract: We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ is necessary to represent the maximum for any depth $3\le k\le \log_2(\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.

</details>


### [96] [Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance](https://arxiv.org/abs/2601.01424)
*Akshay Sasi,Malavika Pradeep,Nusaibah Farrukh,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 该研究提出使用ECG信号替代EEG进行认知负荷监测，通过跨模态XGBoost框架将ECG特征映射到EEG认知空间，实现仅用ECG即可准确分类认知状态。


<details>
  <summary>Details</summary>
Motivation: 虽然EEG是评估心理工作负荷的金标准，但其便携性有限限制了实际应用。广泛可用的可穿戴设备ECG提供了一种实用的替代方案，需要验证ECG是否能可靠反映认知负荷并作为EEG指标的代理。

Method: 收集工作记忆和被动听力任务的多模态数据，提取ECG时域HRV指标和Catch22描述符，对应EEG频谱和Catch22特征。提出跨模态XGBoost框架，将ECG特征投影到EEG代表的认知空间，实现仅用ECG进行工作负荷推断。

Result: ECG衍生的投影能显著捕捉认知状态的变化，为准确分类提供良好支持。ECG特征能有效映射到EEG认知空间，实现可靠的认知负荷监测。

Conclusion: ECG可作为可解释、实时、可穿戴的日常认知监测解决方案，为生理计算提供实用替代方案。

Abstract: Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.

</details>


### [97] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: BSZO是一种贝叶斯子空间零阶优化方法，通过卡尔曼滤波结合多个扰动方向的信息，相比传统ZO方法提高了收敛速度，在保持低内存消耗的同时实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法依赖单步随机扰动的梯度估计，信息利用效率低，收敛速度慢。需要一种能有效结合多个扰动方向信息的方法来提升优化效率。

Method: 提出贝叶斯子空间零阶优化(BSZO)，将每个有限差分测量视为噪声观测，通过卡尔曼滤波构建投影梯度的后验分布，并使用基于残差的自适应机制调整扰动尺度。

Result: 理论分析显示BSZO将收敛速度提高了k/γ倍。在RoBERTa、Mistral和OPT模型上的实验表明，BSZO优于MeZO、MeZO-Adam和HiZOO，在OPT-13B上实现了最高6.67%的绝对平均改进，同时内存使用仅为基础推理的1.00-1.08倍。

Conclusion: BSZO通过贝叶斯方法有效整合多个扰动方向的信息，显著提升了零阶优化的收敛速度和性能，同时保持了低内存消耗的优势，为大规模语言模型的高效微调提供了新方法。

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [98] [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://arxiv.org/abs/2601.01465)
*Ze Peng,Jian Zhang,Yisen Wang,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种新的信息论泛化界，能更好地利用SGD的平坦性偏好，在数值上更紧且能正确反映平坦性改善时的泛化提升。


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界虽然理论上数据依赖和算法依赖，但未能有效捕捉SGD平坦性偏好对泛化的改善，且在数值上较松。需要开发能更好利用平坦性偏好的泛化界。

Method: 提出"全知轨迹"技术，推导出更充分利用平坦性的信息论泛化界。该界表明当最终权重协方差的大方差方向在损失景观中具有小局部曲率时，模型泛化更好。

Result: 在深度神经网络上的实验表明，新界不仅正确反映了平坦性改善时的泛化提升，且数值上更紧。应用于凸-Lipschitz-有界问题的梯度下降极小化超额风险时，将代表性信息论界的Ω(1)率改进为O(1/√n)。

Conclusion: 通过"全知轨迹"技术，成功开发出能更好利用SGD平坦性偏好的信息论泛化界，解决了现有界未能捕捉平坦性改善的问题，并在理论和实验上均表现出优越性。

Abstract: Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called "omniscient trajectory". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $Ω(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.

</details>


### [99] [Accelerating Storage-Based Training for Graph Neural Networks](https://arxiv.org/abs/2601.01473)
*Myung-Hwan Jang,Jeong-Min Park,Yunyong Ko,Sang-Wook Kim*

Main category: cs.LG

TL;DR: AGNES是一个基于存储的GNN训练框架，通过块状存储I/O处理和超批次处理来解决大规模图神经网络训练中的I/O瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于存储的GNN训练方法在处理大规模图时面临严重的数据准备瓶颈，主要问题是无法有效处理大量小型存储I/O操作，这限制了高性能存储设备的带宽利用率。

Method: 提出AGNES框架，采用块状存储I/O处理方法来充分利用高性能存储设备的I/O带宽，并结合超批次处理策略，基于真实世界图的特性进一步优化每个存储I/O的效率。

Result: 在五个真实世界图数据集上的综合实验表明，AGNES始终优于四种最先进的方法，比最佳竞争对手快达4.1倍。

Conclusion: AGNES通过创新的存储I/O优化策略，有效解决了大规模GNN训练中的存储瓶颈问题，显著提升了训练效率，为处理web规模图数据提供了有效的单机解决方案。

Abstract: Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.

</details>


### [100] [Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts](https://arxiv.org/abs/2601.01475)
*Ruofeng Yang,Yongcan Li,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: 本文提出MoLR-MoG建模方法，将扩散模型的数据建模为低秩高斯混合的子空间联合，解决了传统方法无法捕捉潜在流形多模态特性的问题，实现了维度诅咒的突破。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型虽然在小数据集上表现良好，但估计误差受维度诅咒影响（n^{-1/D}）。现有方法将数据建模为高斯潜在变量的线性子空间联合，虽然反映了多流形特性，但无法捕捉潜在流形的多模态特性。

Method: 提出MoLR-MoG（低秩高斯混合的子空间混合）建模方法：将目标数据建模为K个线性子空间的联合，每个子空间采用高斯混合潜在变量（n_k个模态，维度d_k）。对应的得分函数具有混合专家结构，能捕捉多模态信息并包含非线性特性。

Result: 1. 实验表明MoE-latent MoG NN的生成结果远优于MoE-latent Gaussian score；2. MoE-latent MoG NN与参数多10倍的MoE-latent Unet性能相当；3. 理论分析得到R^4√(∑n_k)√(∑n_kd_k)/√n的估计误差，突破了维度诅咒；4. 证明了MoLR-MoG建模下的优化收敛保证。

Conclusion: MoLR-MoG建模方法合理且适用于真实世界数据，解释了为什么扩散模型只需小训练样本和快速优化过程就能获得优异性能。该方法通过利用数据结构突破了维度诅咒，为扩散模型的理论基础提供了新视角。

Abstract: Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{Σ_{k=1}^Kn_k}\sqrt{Σ_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.

</details>


### [101] [SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines](https://arxiv.org/abs/2601.01484)
*Itai Morad,Nir Shlezinger,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 本文从贝叶斯视角分析知识蒸馏，证明从贝叶斯分类概率学习能降低方差、提升收敛稳定性，并实验验证贝叶斯教师模型能让学生获得更高准确率和更稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）虽然在实践中表现出色，但其理论基础尚未完全理解。本文旨在从贝叶斯角度严格分析KD的收敛行为，特别是当教师提供贝叶斯分类概率（BCPs）时，与传统的one-hot监督相比有何理论优势。

Method: 采用贝叶斯视角分析知识蒸馏，研究两种监督方式：1）教师提供精确的贝叶斯分类概率（BCPs）；2）教师提供带噪声的BCPs近似。理论分析使用随机梯度下降（SGD）的收敛行为分析，并实验验证贝叶斯深度学习模型作为教师的效果。

Result: 理论分析表明：从BCPs学习能实现方差降低，并在收敛边界中去除邻域项。噪声水平影响泛化能力和准确性。实验结果显示：从贝叶斯教师蒸馏的学生不仅准确率更高（最高提升4.27%），而且收敛更稳定（噪声减少达30%）。

Conclusion: 贝叶斯视角为知识蒸馏提供了理论依据，证明贝叶斯分类概率作为监督信号能带来方差降低和收敛稳定性提升。贝叶斯深度学习模型作为教师能提供更好的BCPs估计，从而在知识蒸馏中实现更优性能。

Abstract: Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.

</details>


### [102] [Accelerating Decentralized Optimization via Overlapping Local Steps](https://arxiv.org/abs/2601.01493)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: 提出OLDSGD方法，通过计算-通信重叠加速去中心化训练，减少网络空闲时间，保持与Local SGD相同的平均更新但避免通信阻塞


<details>
  <summary>Details</summary>
Motivation: 现有去中心化优化方法存在通信瓶颈，频繁的节点间同步导致效率低下，需要一种能减少网络空闲时间的加速方法

Method: 设计重叠局部去中心化SGD（OLDSGD），通过精心设计的更新机制实现计算与通信的重叠，保持与Local SGD相同的平均更新但避免通信引起的停顿

Result: 理论上建立了光滑非凸目标的非渐近收敛率，OLDSGD保持与标准Local Decentralized SGD相同的迭代复杂度但改善每次迭代运行时间；实证结果显示在不同通信延迟下都能改善实际时间收敛

Conclusion: OLDSGD通过对现有框架的最小修改，提供了不牺牲理论保证的更快去中心化学习的实用解决方案

Abstract: Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.

</details>


### [103] [Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE](https://arxiv.org/abs/2601.01501)
*Fan Xu,Wei Gong,Hao Wu,Lilan Peng,Nan Wang,Qingsong Wen,Xian Wu,Kun Wang,Xibin Zhao*

Main category: cs.LG

TL;DR: HiGO：一种用于全球野火预测的多尺度图神经网络ODE框架，通过层次化图结构和连续时间建模显著提升长期预测性能


<details>
  <summary>Details</summary>
Motivation: 野火作为地球系统的重要组成部分，受到大气、海洋和陆地过程在广泛时空尺度上的复杂相互作用影响。虽然深度学习在全球天气预报方面取得突破，但在全球野火行为预测方面的潜力尚未充分探索，而这是一个关键且具有挑战性的任务。

Method: 提出层次化图ODE（HiGO）框架：1）将地球系统表示为多层图层次结构；2）设计自适应滤波消息传递机制，用于层内和层间信息流，实现更有效的特征提取和融合；3）在多个层次上集成GNN参数化的神经ODE模块，显式学习每个尺度的连续动态。

Result: 在SeasFire Cube数据集上的广泛实验表明，HiGO在长期野火预测方面显著优于最先进的基线方法。其连续时间预测表现出强烈的观测一致性，突显了其在现实世界应用中的潜力。

Conclusion: HiGO框架成功地将多尺度建模和连续时间动态学习相结合，为全球野火行为预测提供了一种新颖有效的解决方案，具有重要的实际应用价值。

Abstract: Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.

</details>


### [104] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: 卫星图像嵌入比传统流域属性更能有效预测无观测记录河流的流量，通过选择相似流域可提高预测精度


<details>
  <summary>Details</summary>
Motivation: 传统流域属性无法完全描述自然环境的复杂性，需要更有效的方法来预测无流量记录河流的径流

Method: 使用AlphaEarth Foundation嵌入（从卫星图像学习的环境表征）代替传统流域属性，并研究如何选择相似流域作为"供体流域"来改进预测

Result: 基于卫星嵌入的模型在未参与训练的流域上预测精度更高，通过嵌入相似性选择相似流域可提高性能，而添加不相似流域会降低精度

Conclusion: 卫星图像学习的环境表征能更有效地捕捉流域物理差异，增强水文预测能力，支持开发适应性更强的水文模型

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [105] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: 该论文通过梯度归因属性和两阶段决策-采样假设，从理论上解释了RL后训练为何能产生自我反思能力，并实证验证了RL的泛化优势主要来自决策能力的提升而非采样能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在RL后训练后出现自我反思能力，但统一的优化目标如何产生生成解决方案和评估何时修订这两种不同功能仍不清楚。需要从机制上解释RL为何比SFT更成功。

Method: 引入梯度归因属性来表征奖励梯度在策略组件中的分布，形式化为两阶段决策-采样假设，将策略分解为用于生成的采样策略和用于验证的决策策略。理论分析不同优化目标的梯度归因特性，并在算术推理任务上进行实证验证。

Result: 理论证明：代理奖励表现出平衡梯度归因，而SFT和KL惩罚表现出不平衡梯度归因，长度加权创建了不对称正则化，约束采样策略而让决策策略优化不足。实证验证：RL的泛化优势主要来自决策能力的提升而非采样能力。

Conclusion: 该研究从第一性原理机制上解释了思维模型中的自我修正能力，为RL在产生自我反思能力方面的成功提供了理论解释，揭示了决策能力在泛化中的关键作用。

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [106] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: 提出REE-TTT模型，通过时空测试时训练机制提升雷达回波外推的泛化能力，解决传统方法对高质量本地训练数据和静态参数的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的雷达回波外推方法在降水临近预报中占主导地位，但存在泛化能力差的问题，因为它依赖高质量本地训练数据和静态模型参数，限制了在不同区域和极端事件中的适用性。

Method: 提出REE-TTT模型，引入自适应测试时训练机制，核心是设计的时空测试时训练块，用任务特定的注意力机制替代标准线性投影，使模型能够适应非平稳气象分布，增强降水特征表示。

Result: 在跨区域极端降水场景下的实验表明，REE-TTT在预测精度和泛化能力上显著优于现有基线模型，对数据分布偏移表现出卓越的适应性。

Conclusion: REE-TTT通过测试时训练机制有效解决了雷达回波外推的泛化问题，为降水临近预报提供了更可靠和适应性强的解决方案。

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [107] [Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry](https://arxiv.org/abs/2601.01616)
*Md Istiauk Hossain Rifat,Moin Khan,Mohammad Zunaed*

Main category: cs.LG

TL;DR: 本文为孟加拉国纺织行业开发了基于非侵入式负载监测(NILM)的实时能耗监控框架，重点关注纺织切割机等相同电机负载，通过硬件采集和云端处理实现远程监控，但相同设备同时运行时负载分解面临挑战。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国纺织行业作为高能耗产业，其能耗监控方法落后，导致能源使用效率低下和运营成本高昂。需要开发适合工业应用的实时监控解决方案来改善这一问题。

Method: 开发了包含电压电流传感器、Arduino Mega和ESP8266的硬件系统，采集总负载和单个负载数据并上传至云端处理。创建了包含三个相同感应电机和辅助负载的新数据集（超过18万个样本），使用先进的MATNILM模型在具有挑战性的工业条件下进行评估。

Result: 总体能耗估计相对准确，但单个设备的负载分解面临困难，特别是在多个相同机器同时运行时。尽管如此，集成系统通过Blynk应用实现了实用的实时远程监控。

Conclusion: 该研究展示了NILM在工业应用中的潜力和局限性，为未来改进提供了方向，包括更高频率的数据采集、更大规模的数据集以及处理相同负载的先进深度学习方法。

Abstract: The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.

</details>


### [108] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络和对抗训练的投票规则学习方法，通过将选举表示为二分图来提升投票规则的表达能力和抗策略性投票能力。


<details>
  <summary>Details</summary>
Motivation: 选举在现代民主、市场调节和推荐系统中至关重要，但设计一个满足所有场景的通用投票规则仍然具有挑战性。现有的基于集合不变架构的自动化机制设计方法虽然适合建模选举系统，但在实际应用中面临抗策略性投票等鲁棒性问题。

Method: 将选举表示为二分图，使用图神经网络学习投票规则，并结合对抗训练来提升投票规则在最大化社会福利的同时对策略性投票的鲁棒性。

Result: 在合成和真实世界数据集上的评估表明，该方法解决了先前工作在学习投票规则方面的关键限制，提升了投票规则的表达能力和鲁棒性。

Conclusion: 该方法为将机器学习应用于真实世界选举开辟了新前沿，通过图神经网络和对抗训练的结合，能够学习出更具表达能力和鲁棒性的投票规则。

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [109] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: 提出长度感知采样(LAS)方法，通过按轨迹长度分组采样来减少批次内长度异质性，改善轨迹生成模型的分布匹配效果


<details>
  <summary>Details</summary>
Motivation: 在可变长度轨迹生成建模中，标准小批量训练在轨迹长度高度异质时不稳定，这会降低轨迹派生统计量的分布匹配质量

Method: 提出长度感知采样(LAS)：按轨迹长度分组，从单一长度桶中采样批次，减少批次内长度异质性；集成到带辅助时间对齐损失的轨迹GAN中

Result: 理论上提供分布级保证和IPM/Wasserstein机制解释；实证上在多商场购物轨迹和多种公共序列数据集上一致改善派生变量分布匹配

Conclusion: LAS是一种简单有效的批处理策略，无需改变模型结构就能改善轨迹生成模型的分布匹配性能，在多种实际应用中表现优于随机采样

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [110] [Who is the Winning Algorithm? Rank Aggregation for Comparative Studies](https://arxiv.org/abs/2601.01664)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 提出新框架，利用算法在基准数据集上的完整排名信息（不仅是获胜次数）来估计各算法在未见数据集上的获胜概率，相比现有方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只统计算法在基准数据集上的获胜次数来评估性能，但忽略了完整排名信息（如第二、第三名等），这些信息可能包含更多关于算法相对性能的有价值信息。

Method: 引入新的概念框架，利用算法在基准数据集上的完整排名信息（不仅仅是获胜次数），通过更全面的统计方法来估计每个算法在未见数据集上的获胜概率。

Result: 在合成数据和真实世界示例中，所提出的框架显著优于当前已知的方法，能够更准确地预测算法在未见数据集上的表现。

Conclusion: 利用算法的完整排名信息而非仅获胜次数，可以更有效地评估机器学习算法的相对性能，为算法选择提供更可靠的依据。

Abstract: Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.

</details>


### [111] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 提出一个面向鲁棒性的偏好条件深度强化学习求解器框架，包含偏好对抗攻击生成困难实例，以及通过硬度感知偏好选择的防御策略增强求解器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在多目标组合优化问题中表现出潜力，但其鲁棒性在多样复杂问题分布中尚未充分探索，需要系统评估和增强求解器的鲁棒性。

Method: 提出统一鲁棒性框架：1) 偏好对抗攻击生成暴露求解器弱点的困难实例；2) 通过帕累托前沿质量退化量化攻击影响；3) 防御策略整合硬度感知偏好选择到对抗训练中，减少对受限偏好区域的过拟合。

Result: 在多目标旅行商问题、多目标容量车辆路径问题和多目标背包问题上验证：攻击方法成功为不同求解器生成困难实例；防御方法显著增强神经求解器的鲁棒性和泛化能力，在困难或分布外实例上表现优异。

Conclusion: 提出的鲁棒性框架有效评估和增强偏好条件深度强化学习求解器，攻击方法能暴露弱点，防御策略能提升鲁棒性和泛化性能，为多目标组合优化问题的学习型求解器提供鲁棒性保障。

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [112] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: HeurekaBench是一个用于评估科学智能体的基准框架，通过半自动化流程从真实科学研究和代码库中创建开放式研究问题，并在单细胞生物学领域实例化验证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的科学智能体系统缺乏现实的端到端评估场景，需要能够整合数据分析、解释和从实验数据生成新见解的基准测试。

Method: 开发半自动化流水线，利用多个LLM从科学研究和代码库中提取见解并生成候选工作流，然后与报告结果进行验证，创建基于真实科学研究的开放式研究问题基准。

Result: 在单细胞生物学领域实例化为sc-HeurekaBench基准，发现批评模块可将开源LLM智能体的不良响应改善达22%，缩小与闭源模型的差距。

Conclusion: HeurekaBench为科学智能体提供了严格的端到端评估路径，将基准构建扎根于真实的科学工作流程中。

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [113] [DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors](https://arxiv.org/abs/2601.01688)
*Yash Thesia,Meera Suthar*

Main category: cs.LG

TL;DR: DiMEx利用预训练潜在扩散模型和贝叶斯优化实现高效无数据模型窃取，HSE防御通过检测优化轨迹来对抗此类攻击


<details>
  <summary>Details</summary>
Motivation: 现有无数据模型提取(DFME)方法存在"冷启动"问题，GAN方法需要大量查询从随机噪声收敛到有意义数据，效率低下。需要更高效的模型窃取方法，同时也需要相应的防御机制。

Method: DiMEx：利用预训练潜在扩散模型的语义先验，在生成器潜在空间中使用随机嵌入贝叶斯优化(REMBO)，立即合成高质量查询。HSE防御：混合状态集成防御，通过检测潜在空间攻击的独特"优化轨迹"来识别攻击。

Result: DiMEx在SVHN数据集上仅用2000次查询就达到52.1%的协议率，比最先进的GAN基线高出16%以上。HSE防御能将攻击成功率抑制到21.6%，且延迟可忽略不计。

Conclusion: DiMEx展示了利用预训练扩散模型实现高效模型窃取的可行性，而HSE防御则提供了对抗此类语义攻击的有效方法，强调了检测攻击优化轨迹的重要性。

Abstract: Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the "Cold Start" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique "optimization trajectory" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.

</details>


### [114] [Enhanced Multi-model Online Conformal Prediction](https://arxiv.org/abs/2601.01692)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出一种新颖的多模型在线共形预测算法，通过二分图选择有效模型子集，降低计算复杂度并提高预测效率


<details>
  <summary>Details</summary>
Motivation: 传统共形预测依赖单一固定模型，在在线环境中可能表现不稳定；现有多模型方法计算成本高且可能受性能差的模型影响

Method: 开发多模型在线共形预测算法，在每个时间步生成二分图识别有效模型子集，从中选择模型构建预测集

Result: 实验表明该方法在预测集大小和计算效率方面优于现有多模型共形预测技术

Conclusion: 提出的算法能有效降低计算复杂度，提高预测效率，解决了多模型共形预测中的计算和性能问题

Abstract: Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.

</details>


### [115] [Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT](https://arxiv.org/abs/2601.01701)
*Mohammed Ayalew Belay,Adil Rasheed,Pierluigi Salvo Rossi*

Main category: cs.LG

TL;DR: 提出数字孪生集成联邦学习(DTFL)方法，通过五种新颖方法解决工业异常检测中的数据隐私、通信效率和有限标注数据问题，显著提升收敛速度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临依赖真实传感器数据、标注数据有限、高误报率和隐私问题等挑战，需要既能保护数据隐私又能高效通信的解决方案。

Method: 提出五种数字孪生集成联邦学习方法：DTML（元学习）、FPF（参数融合）、LPE（分层参数交换）、CWA（循环权重适应）和DTKD（知识蒸馏），结合合成与真实知识，平衡泛化与通信开销。

Result: 在80%目标准确率下，CWA仅需33轮，FPF需41轮，LPE需48轮，DTML需87轮，而标准FedAvg和DTKD在100轮内未达标。CWA比DTML减少62%轮次，比LPE减少31%。

Conclusion: 数字孪生知识集成到联邦学习中能显著加速收敛，提升通信效率，为工业物联网异常检测提供实用解决方案。

Abstract: Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.

</details>


### [116] [Entropy-Aligned Decoding of LMs for Better Writing and Reasoning](https://arxiv.org/abs/2601.01714)
*Kareem Ahmed,Sameer Singh*

Main category: cs.LG

TL;DR: EPIC是一种无需超参数的解码方法，通过将未来轨迹的熵纳入语言模型解码，在每一步生成时显式调节不确定性，使采样分布的熵与数据不确定性对齐，从而提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型解码算法依赖贪婪启发式方法，导致生成文本同质化、重复且不连贯。现有方法在限制语言模型分布到高概率延续时引入了短视扭曲，需要更好的解码策略来提升生成质量。

Method: EPIC通过熵感知懒惰Gumbel-Max采样，将未来轨迹的熵纳入解码过程。该方法显式调节每一步生成时表达的不确定性，使采样分布的熵与数据不确定性对齐。算法是精确的且高效，每步仅需亚线性次数的熵评估。

Result: 在创意写作和摘要任务中，EPIC在LM-as-judge偏好胜率上持续优于广泛使用的解码策略。自动指标显示EPIC产生更多样化的生成和更忠实的摘要。在数学推理任务中，EPIC也优于所有基线方法。

Conclusion: EPIC通过将熵不确定性纳入解码过程，解决了传统解码方法的局限性，在多个任务上实现了更好的生成质量、多样性和忠实度，为语言模型解码提供了有效的超参数自由解决方案。

Abstract: Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.

</details>


### [117] [Context-Free Recognition with Transformers](https://arxiv.org/abs/2601.01754)
*Selim Jerad,Anej Svete,Sophie Hao,Ryan Cotterell,William Merrill*

Main category: cs.LG

TL;DR: 循环Transformer通过O(log n)循环层和O(n⁶)填充token可以识别所有上下文无关语言，但实际中可能不实用；对于无歧义CFL，仅需O(n³)填充token，更高效。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理自然语言和代码等语法结构良好的输入时表现出色，但理论上标准Transformer无法识别上下文无关语言（CFL）。虽然已有研究证明循环层能帮助识别正则语言，但CFL识别问题仍未解决。

Method: 提出循环Transformer架构，使用O(log n)循环层和O(n⁶)填充token来实现CFL识别。对于无歧义CFL子类，将填充token需求降低到O(n³)。通过实验验证循环机制在需要对数深度的语言上的有效性。

Result: 理论上证明循环Transformer能识别所有CFL，但需要大量填充token可能不实用。对于无歧义CFL，识别问题变得可处理，仅需O(n³)填充。实验表明循环机制确实有助于需要对数深度的语言识别。

Conclusion: CFL识别对Transformer具有复杂性：通用识别可能需要大量填充token而不实用，但自然约束（如无歧义性）能产生高效的识别算法。循环机制为Transformer处理语法结构提供了理论支持。

Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.

</details>


### [118] [UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk](https://arxiv.org/abs/2601.01786)
*Intae Jeon,Yujeong Kwon,Hyungjoon Koo*

Main category: cs.LG

TL;DR: UnPII：首个基于PII风险的机器学习遗忘方法，通过PII风险指数（PRI）量化不同敏感属性的隐私风险，实现差异化遗忘策略，提升模型准确率、实用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融、医疗、政府等关键领域的广泛应用，处理训练数据中的个人身份信息（PII）引发隐私担忧。GDPR等法规要求应请求删除PII，需要可靠且经济高效的数据删除方案。现有遗忘技术采用统一策略，未考虑不同PII属性的隐私风险和业务风险差异。

Method: 提出UnPII方法：1）引入PII风险指数（PRI），综合考虑可识别性、敏感性、可用性、可链接性、持久性、可暴露性和合规性七个风险维度；2）构建合成PII数据集（1700个实例）模拟真实暴露场景；3）与现有遗忘算法（梯度上升、负偏好优化、直接偏好优化）无缝集成，无需修改算法原理。

Result: 实验结果显示，UnPII相比基线方法：准确率提升最高11.8%，实用性提升最高6.3%，泛化能力提升最高12.4%。遗忘过程中平均产生27.5%的微调开销。

Conclusion: UnPII是首个基于PII风险的机器学习遗忘方法，通过PRI实现差异化遗忘策略，显著提升模型性能，同时保持与现有算法的兼容性，为合规性数据删除提供了有效解决方案。

Abstract: The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.

</details>


### [119] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: HyperCLOVA X 8B Omni是首个支持文本、音频、视觉任意输入输出的全模态模型，通过统一的多模态序列处理实现理解和生成，在韩语和英语上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的任意到任意全模态助手，避免传统多模态系统中分离的模态特定流水线，实现更实用的多模态交互。

Method: 通过共享的下一个token预测接口统一处理交错的多模态序列，使用视觉和音频编码器注入连续嵌入以实现细粒度理解和基础。

Result: 在文本、音频、视觉的多种输入输出组合上，与类似规模模型相比表现出竞争力，支持韩语和英语。

Conclusion: HyperCLOVA X 8B Omni作为8B规模的全模态路径点，其开源权重将支持广泛的研究和部署场景。

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [120] [Distributed Federated Learning by Alternating Periods of Training](https://arxiv.org/abs/2601.01793)
*Shamik Bhattacharyya,Rachel Kalpana Kalaimani*

Main category: cs.LG

TL;DR: 提出分布式联邦学习框架，通过多服务器架构解决传统联邦学习的可扩展性和容错性问题


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖单一中央服务器，在大规模客户端场景下面临可扩展性限制和单点故障风险，需要更健壮的分布式解决方案

Method: 设计分布式联邦学习框架，包含多个具有服务器间通信能力的服务器，每个服务器关联一组不相交的客户端；提出DFL算法，交替进行客户端本地训练和服务器间全局训练

Result: 在适当参数选择下，DFL算法确保所有服务器收敛到理想模型的微小容忍范围内，有效整合本地和全局训练模型；通过数值模拟验证理论主张

Conclusion: 分布式联邦学习框架成功解决了传统联邦学习的可扩展性和容错性限制，为大规模联邦学习应用提供了更健壮的解决方案

Abstract: Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.

</details>


### [121] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: CARRL提出了一种针对自动驾驶稀疏安全风险的对抗训练方法，通过风险暴露对手和风险目标鲁棒代理的博弈，在约束预算下识别关键安全时刻，减少碰撞率至少22.66%。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法将自动驾驶代理与对手的交互建模为零和博弈，忽略了双方的不对称性，且未能反映安全关键风险的稀疏性，导致实际部署中的鲁棒性不足。

Method: CARRL包含两个交互组件：风险暴露对手（REA）和风险目标鲁棒代理（RTRA）。将交互建模为一般和博弈，REA采用解耦优化机制在约束预算下识别稀疏安全关键时刻，RTRA通过双回放缓冲池联合利用正常和对抗经验，并强制扰动下的策略一致性。

Result: 实验结果表明，与最先进的基线方法相比，该方法在所有情况下至少减少22.66%的碰撞率。

Conclusion: CARRL通过建模不对称交互和稀疏安全风险，显著提升了自动驾驶强化学习在实际场景中的鲁棒性和安全性。

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [122] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出一种基于分布评论家高阶矩（偏度和峰度）的PPO改进方法，通过惩罚极端尾部行为来减少策略更新的不稳定性，在Walker2D环境中将稳定性提升达75%。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习智能体常学习到相同累积回报但行为差异很大的策略，这是由于环境和算法因素导致的。在连续控制任务中，即使小的参数变化也会产生不稳定的步态，这既影响算法比较也影响实际应用。虽然约束策略保持窄的回报分布可以改善稳定性，但直接估计该分布计算成本高。

Method: 通过分布评论家建模状态-动作回报分布，然后使用该分布的高阶矩（偏度和峰度）来偏置PPO的优势函数。通过惩罚极端尾部行为，该方法阻止策略进入容易不稳定的参数区域。

Result: 在Walker2D环境中，该方法将稳定性提升达75%，同时保持可比较的评估回报。当更新后评论家值与更新后回报对齐不佳时，标准PPO难以产生窄的回报分布，而基于矩的修正能有效缩小该分布。

Conclusion: 利用环境随机性通过分布评论家的高阶矩来偏置优势函数，可以有效减少策略更新的不稳定性，改善强化学习在连续控制任务中的稳定性和可转移性。

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [123] [RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data](https://arxiv.org/abs/2601.01829)
*Peiyan Hu,Haodong Feng,Hongyuan Liu,Tongtong Yan,Wenhao Deng,Tianrun Gao,Rong Zheng,Haoren Zheng,Chenglei Yu,Chuanrui Wang,Kaiwen Li,Zhi-Ming Ma,Dezhi Zhou,Xingcai Lu,Dixia Fan,Tailin Wu*

Main category: cs.LG

TL;DR: RealPDEBench：首个结合真实测量数据与配对数值模拟的科学机器学习基准，包含5个数据集、3个任务、8个指标和10个基线模型，旨在解决sim-to-real鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前科学机器学习模型主要依赖模拟数据进行训练和验证，缺乏真实世界数据，这限制了模型发展、评估以及sim-to-real迁移研究。需要建立连接真实测量与模拟数据的基准来推动科学ML的实际部署。

Method: 构建包含5个真实世界测量数据集及其配对数值模拟的基准；定义3个任务（真实数据预测、模拟数据预测、sim-to-real迁移）；设计8个评估指标（数据导向和物理导向）；评估10个代表性基线模型（SOTA模型、预训练PDE基础模型、传统方法）。

Result: 实验显示模拟数据与真实数据存在显著差异，但使用模拟数据进行预训练能持续提升模型在真实数据上的准确性和收敛性。基准揭示了sim-to-real鸿沟的具体表现。

Conclusion: RealPDEBench为科学机器学习提供了首个连接真实与模拟数据的基准，揭示了sim-to-real差距，并证明模拟数据预训练的有效性，推动科学ML向实际部署发展。

Abstract: Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.

</details>


### [124] [FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks](https://arxiv.org/abs/2601.01833)
*Chenyu Hu,Qiming Hu,Sinan Chen,Nianyu Li,Mingyue Zhang,Jialong Li*

Main category: cs.LG

TL;DR: FAROS：一个增强的联邦学习框架，通过自适应差分缩放和鲁棒核心集计算来防御后门攻击，相比现有防御方法在攻击成功率和主任务准确率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临后门攻击威胁，现有防御方法依赖固定参数，存在单点故障风险，难以应对复杂的攻击策略。

Method: 提出FAROS框架，包含自适应差分缩放（ADS）和鲁棒核心集计算（RCC）。ADS根据客户端上传梯度的离散度动态调整防御敏感度；RCC通过计算高置信度客户端核心集的质心来降低单点故障风险。

Result: 在多种数据集、模型和攻击场景下的实验表明，该方法在攻击成功率和主任务准确率方面优于现有防御方法。

Conclusion: FAROS通过动态自适应防御机制有效应对联邦学习中的后门攻击，解决了现有防御方法的固有限制。

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.

</details>


### [125] [High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation](https://arxiv.org/abs/2601.01860)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出基于因子分解机和二次优化退火(FMQA)的高阶上位性检测方法，将上位性检测建模为黑盒优化问题，使用MDR计算的分类错误率作为目标函数，在有限迭代次数内有效识别高阶上位性相互作用。


<details>
  <summary>Details</summary>
Motivation: 高阶上位性检测在遗传关联研究中面临组合爆炸的计算挑战。虽然多因子降维(MDR)是常用的上位性评估方法，但随着位点数量或相互作用阶数增加，基于MDR的穷举搜索变得计算不可行。

Method: 将上位性检测问题定义为黑盒优化问题，使用因子分解机结合二次优化退火(FMQA)求解。提出基于FMQA的高效上位性检测方法，其中MDR计算的分类错误率(CER)作为黑盒目标函数。

Result: 使用具有预定义高阶上位性的模拟病例对照数据集进行实验评估。结果表明，所提方法在各种相互作用阶数和遗传位点数量下，在有限迭代次数内成功识别出真实的上位性相互作用。

Conclusion: 所提方法对于高阶上位性检测是有效且计算高效的，能够解决传统MDR方法面临的组合爆炸问题。

Abstract: Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.

</details>


### [126] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 仅需一个安全样本即可完全恢复安全对齐大语言模型，无需牺牲性能且成本极低


<details>
  <summary>Details</summary>
Motivation: 微调安全对齐的大语言模型会显著损害其安全性，现有方法需要大量安全样本或校准集，导致计算开销大且模型性能下降

Method: 提出仅需单个安全示例即可恢复安全对齐的方法，发现安全梯度具有低秩结构，解释了这种高效修正的可能性

Result: 该方法在5个安全对齐LLM和多个数据集上验证有效，无论有害示例数量或模型大小如何，都能在几个epoch内收敛

Conclusion: 安全对齐可以极低成本高效恢复，无需牺牲模型性能，为LLM安全修正提供了新思路

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [127] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross：一种个性化单次联邦学习框架，通过聚类、双层跨集群优化和个性化蒸馏解决非IID数据下预测冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有的单次联邦学习方法在非IID数据下，所有客户端的预测在平均时会相互抵消，产生接近均匀的软标签，为蒸馏提供弱监督。这限制了在隐私敏感的医疗应用中的性能。

Method: 提出FedBiCross框架，包含三个阶段：1）根据模型输出相似性对客户端进行聚类，形成一致的子集成；2）双层跨集群优化，学习自适应权重，选择性地利用有益的跨集群知识，同时抑制负迁移；3）个性化蒸馏，进行客户端特定适应。

Result: 在四个医学图像数据集上的实验表明，FedBiCross在不同非IID程度下始终优于最先进的基线方法。

Conclusion: FedBiCross通过聚类和选择性知识转移有效解决了非IID数据下单次联邦学习中的预测冲突问题，为隐私敏感的医疗应用提供了有效的个性化解决方案。

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [128] [TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train](https://arxiv.org/abs/2601.01903)
*Ungsik Kim,Suwon Lee*

Main category: cs.LG

TL;DR: TT-FSI：利用矩阵乘积算子（MPO）高效计算忠实Shapley交互指数，将内存需求从O(4^d)降至O(ℓd²)，时间从O(d^ℓ·2^d)降至O(ℓ²d³·2^d)，在d=20时实现280倍加速和290倍内存减少。


<details>
  <summary>Details</summary>
Motivation: 忠实Shapley交互指数（FSI）是唯一满足忠实性公理的Shapley交互指数，但现有计算方法需要O(d^ℓ·2^d)时间和O(4^d)内存，计算成本极高，限制了其在实际高维问题中的应用。

Method: 提出TT-FSI方法，利用FSI的代数结构，证明线性算子v↦FSI(v)具有TT秩为O(ℓd)的矩阵乘积算子表示，从而设计出高效的扫描算法，仅需O(ℓ²d³·2^d)时间和O(ℓd²)核心存储。

Result: 在6个数据集（d=8到d=20）上的实验表明：相比基线实现最高280倍加速，相比SHAP-IQ最高85倍加速，内存减少290倍。TT-FSI可扩展到d=20（100万个联盟），而所有竞争方法均失败。

Conclusion: TT-FSI通过张量网络表示显著降低了FSI的计算复杂度，实现了指数级的内存和时间改进，使忠实Shapley交互指数能够应用于实际高维机器学习问题，为特征交互分析提供了高效工具。

Abstract: The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\ell \cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \mapsto \text{FSI}(v)$ admits an MPO representation with TT-rank $O(\ell d)$, enabling an efficient sweep algorithm with $O(\ell^2 d^3 \cdot 2^d)$ time and $O(\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\times$ speedup over baseline, 85$\times$ over SHAP-IQ, and 290$\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.

</details>


### [129] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: 该论文研究了强化学习中的偏好学习（PbRL），重点关注特征依赖性噪声问题，发现现有噪声鲁棒方法在某些特征依赖性噪声场景下表现不佳，而普通PbRL方法反而表现更好。


<details>
  <summary>Details</summary>
Motivation: 偏好学习在复杂任务中很有用，但偏好数据常包含不确定性和噪声。现有研究大多关注均匀分布的噪声，而忽略了与观测特征相关的噪声，这在实际应用中更为常见。

Method: 提出了特征依赖性噪声的形式化概念，包括轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声和语言模型噪声等多种变体。在DMControl和Meta-world的连续控制任务中评估这些噪声。

Result: 实验显示，在某些特征依赖性噪声设置下，最先进的噪声鲁棒PbRL方法学习性能显著下降，而没有显式去噪的PbRL方法在多数情况下反而表现更好。语言模型噪声表现出与特征依赖性噪声相似的特征。

Conclusion: 特征依赖性噪声对现有噪声鲁棒PbRL方法构成挑战，需要进一步研究如何鲁棒地学习处理这类噪声。语言模型噪声模拟了真实人类的噪声特征，值得深入研究。

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [130] [Distorted Distributional Policy Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2601.01917)
*Ryo Iwaki,Takayuki Osogami*

Main category: cs.LG

TL;DR: 提出了一种基于分位数扭曲的非均匀悲观主义方法，用于改进离线分布强化学习，通过根据数据可用性调整保守程度来克服均匀悲观主义导致的过度保守问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线分布强化学习方法采用均匀悲观主义，即对所有分位数进行均匀低估，这会导致过度保守的价值估计，从而限制泛化能力和性能表现。

Method: 引入了分位数扭曲的概念，通过根据支持数据的可用性调整保守程度，实现非均匀悲观主义。该方法基于理论分析，能够针对不同分位数应用不同程度的悲观主义。

Result: 该方法在实证验证中表现出优于均匀悲观主义的性能改进，证明了非均匀悲观主义在离线分布强化学习中的有效性。

Conclusion: 分位数扭曲提供了一种有前景的方法来改进离线分布强化学习，通过非均匀悲观主义克服了均匀悲观主义的局限性，实现了更好的泛化和性能。

Abstract: While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.

</details>


### [131] [Theoretical Convergence of SMOTE-Generated Samples](https://arxiv.org/abs/2601.01927)
*Firuz Kamalov,Hana Sulieman,Witold Pedrycz*

Main category: cs.LG

TL;DR: 本文对SMOTE过采样方法进行了严格的理论分析，证明了其合成变量在概率上收敛于原始变量，在紧致条件下有更强的均值收敛，并发现较小的最近邻秩能带来更快收敛。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据广泛影响机器学习应用，SMOTE作为最流行的解决方法之一，需要从理论和实证两方面进行验证。当前缺乏对SMOTE收敛性质的严格理论分析。

Method: 对SMOTE方法进行严格的理论分析，证明合成随机变量Z在概率上收敛于基础随机变量X，在X紧致条件下证明更强的均值收敛，并通过数值实验验证理论结果。

Result: 证明了SMOTE合成变量Z以概率收敛于X，在紧致条件下有均值收敛，发现较小的最近邻秩能带来更快收敛，数值实验支持理论结果。

Conclusion: 本文为SMOTE提供了理论基础，增强了数据增强技术的理解，超越了不平衡数据场景，为实践者提供了可操作的指导。

Abstract: Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.

</details>


### [132] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: DéjàQ是一个通过联合演化合成数学问题与模型训练来增强数学推理能力的框架，使用LLM驱动的突变策略动态调整训练数据


<details>
  <summary>Details</summary>
Motivation: 当前推理模型主要依赖静态数据集，这可能导致记忆而非泛化。需要动态适应模型能力的数据生成方法来提升数学推理的泛化能力

Method: 提出DéjàQ框架，在模型训练过程中联合演化多样化的合成数学问题。使用两种LLM驱动的突变策略：1）改变上下文细节；2）直接修改问题结构。模型自身参与训练数据的突变生成

Result: 模型能够生成新颖且有意义的数学问题，LLM驱动的突变策略改善了强化学习训练效果。分析了生成问题的有效性和计算开销，证明了动态演化训练数据的潜力

Conclusion: 动态演化训练数据能够有效增强数学推理能力，该方法具有更广泛的适用性，作者将开源代码以支持进一步研究

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [133] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN是一个用于计算机辅助合成规划的统一基准框架和开放数据资源，将端到端合成规划分解为五个任务系列，提供标准化数据集和评估工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划领域缺乏统一的基准框架，存在数据集异质性、评估标准不一致、数据污染等问题，阻碍了方法的公平比较和实际应用。

Method: 将合成规划分解为五个任务系列：反应平衡、原子映射、反应分类、反应性质预测和合成路线设计；从异构公共源收集反应数据，进行统一表示和版本控制；提供透明分割函数、标准化评估流程和指标套件；采用防泄漏的数据划分策略。

Result: 创建了一个包含版本化数据集、机器可读清单、校验和、行数统计的开放资源；提供了可重复生成的脚本化构建方案；支持敏感任务的独立评估集，防止数据污染。

Conclusion: SynRXN通过消除数据集异质性并提供透明可复用的评估框架，实现了CASP方法的公平纵向比较，支持全反应信息学管道的严格消融和压力测试，降低了实际合成规划工作负载的评估门槛。

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [134] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: 论文提出Refinement Provenance Inference (RPI)任务，开发RePro框架通过教师强制token分布和logit排序信号来推断训练数据中prompt是否经过LLM精炼


<details>
  <summary>Details</summary>
Motivation: 指令调优越来越多地依赖LLM-based prompt refinement，这引发了一个实例级审计问题：对于微调模型和训练prompt-response对，能否推断模型是在原始prompt还是其LLM精炼版本上训练的？这对数据集治理和训练数据争议解决很重要。

Method: 提出RePro框架，利用prompt refinement导致的教师强制token分布稳定可检测变化，融合教师强制似然特征和logit排序信号。通过shadow fine-tuning学习可迁移表示，使用轻量级线性头在未见过的受害者模型上推断来源，无需训练数据访问。

Result: RePro在实验中获得强大性能，并能很好地跨不同refiner迁移，表明它利用了refiner无关的分布变化而非重写风格伪影。

Conclusion: 论文形式化了Refinement Provenance Inference (RPI)任务，证明了prompt refinement会产生可检测的分布变化，提出的RePro框架能有效推断训练数据的来源，为数据集治理提供了实用工具。

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [135] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: SerpentFlow：一种无配对域对齐生成框架，通过潜在空间分解将数据分为共享结构和域特定成分，利用合成训练对实现条件生成


<details>
  <summary>Details</summary>
Motivation: 解决无配对观测情况下的域对齐问题，特别是在域间共享底层结构模式但具体实现不同的场景中，传统方法因缺乏跨域直接监督而面临挑战

Method: 在潜在空间中将数据分解为共享成分和域特定成分，通过隔离共享结构并用随机噪声替换域特定成分，构建共享表示与目标域样本之间的合成训练对，从而支持条件生成模型

Result: 在合成图像、物理过程模拟和气候降尺度任务上的实验表明，该方法能有效重建与底层低频模式一致的高频结构，支持共享结构分解作为无配对域对齐的有效策略

Conclusion: SerpentFlow框架通过共享结构分解成功解决了无配对域对齐问题，为条件生成模型在无监督跨域学习中的应用提供了新途径，特别适用于超分辨率等任务

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [136] [Prior Diffusiveness and Regret in the Linear-Gaussian Bandit](https://arxiv.org/abs/2601.02022)
*Yifan Zhu,John C. Duchi,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 本文证明了Thompson采样在线性高斯赌博机中具有$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾上界，其中先验依赖的"预热"项与极小极大遗憾项呈加性而非乘性关系。


<details>
  <summary>Details</summary>
Motivation: 现有Thompson采样在线性高斯赌博机中的遗憾界通常包含先验分布参数与极小极大遗憾项的乘积关系，这可能导致理论分析过于悲观。本文旨在证明这两个项实际上可以解耦为加性关系，从而提供更精确的遗憾分析。

Method: 通过新的"椭圆势能"引理来分析Thompson采样的性能，该引理能够更精细地处理先验协方差矩阵的影响。同时提供了下界证明来表明预热项是不可避免的。

Result: 证明了Thompson采样具有$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾上界，其中$σd \sqrt{T}$是极小极大遗憾项，$d r \sqrt{\mathrm{Tr}(Σ_0)}$是先验依赖的预热项，两者呈加性关系。下界分析表明预热项是必要的。

Conclusion: Thompson采样在线性高斯赌博机中的遗憾可以分解为加性的极小极大项和先验依赖项，这比现有的乘积关系更精确地描述了算法的性能。新的椭圆势能引理为分析提供了关键技术工具。

Abstract: We prove that Thompson sampling exhibits $\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \sqrt{\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.

</details>


### [137] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: 提出输出嵌入中心化(OEC)方法解决大语言模型预训练中的输出logit发散问题，包括确定性μ-centering和正则化μ-loss两种实现，相比现有z-loss方法在训练稳定性和学习率敏感性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练不仅昂贵，而且容易出现训练不稳定性。在训练后期使用大学习率时经常出现的输出logit发散问题，现有最常用的缓解策略z-loss只是治标不治本，没有解决根本原因。

Method: 从输出嵌入几何的角度分析不稳定性，识别其根本原因。提出输出嵌入中心化(OEC)作为新的缓解策略，证明其能抑制输出logit发散。OEC有两种实现方式：确定性操作μ-centering和正则化方法μ-loss。

Result: 两种OEC变体在训练稳定性和学习率敏感性方面都优于z-loss。特别是，即使在z-loss失败的大学习率情况下，OEC也能确保训练收敛。此外，μ-loss对正则化超参数调整的敏感性显著低于z-loss。

Conclusion: 通过分析输出嵌入几何识别了输出logit发散的根本原因，提出的OEC方法（μ-centering和μ-loss）能有效解决这一问题，相比现有z-loss方法在稳定性和超参数敏感性方面都有显著改进。

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [138] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 提出GDRO方法，通过群体级离线优化解决文本到图像整流流扩散模型的奖励对齐问题，提高效率并避免奖励黑客陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习方法在文本到图像整流流扩散模型的奖励对齐中存在效率低、依赖随机采样器和奖励黑客等问题，需要针对整流流模型的特性设计更有效的对齐方法。

Method: 提出群体级直接奖励优化(GDRO)，这是一种结合整流流模型特性的后训练范式，支持完全离线训练，无需图像采样，且独立于扩散采样器，避免了ODE到SDE的近似需求。

Result: GDRO在OCR和GenEval任务中有效提高了扩散模型的奖励分数，同时展现出强大的稳定性和鲁棒性，能够缓解奖励黑客问题。

Conclusion: GDRO为整流流扩散模型的群体级奖励对齐提供了一种高效、稳定且理论完备的解决方案，解决了现有在线强化学习方法的关键局限性。

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [139] [Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling](https://arxiv.org/abs/2601.02037)
*Wei Hu,Zewei Yu,Jianqiu Xu*

Main category: cs.LG

TL;DR: DMPEAD：一种用于多元时间序列异常检测的动态模型池与集成框架，通过构建多样化模型池、动态更新和集成排名靠前模型来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常检测方法存在局限性：选择方法依赖单一模型且对策略敏感；集成方法要么组合所有模型要么仅限于单变量数据；大多数方法依赖固定数据维度，限制了可扩展性。

Method: 提出DMPEAD框架：1) 通过参数迁移和多样性度量构建多样化模型池；2) 使用元模型和基于相似性的策略动态更新模型池，实现自适应扩展、子集选择和池合并；3) 通过代理度量排名和top-k聚合在选定子集中集成排名靠前的模型。

Result: 在8个真实世界数据集上的广泛实验表明，该模型优于所有基线方法，展现出卓越的适应性和可扩展性。

Conclusion: DMPEAD通过动态模型池构建和集成策略，有效解决了现有多元时间序列异常检测方法的局限性，在多个真实数据集上取得了最先进的性能。

Abstract: Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.

</details>


### [140] [Explore the Ideology of Deep Learning in ENSO Forecasts](https://arxiv.org/abs/2601.02050)
*Yanhai Gan,Yipeng Chen,Ning Li,Xingguo Liu,Junyu Dong,Xianyao Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于有界变差函数的可解释性框架，通过激活函数饱和区"拯救"死亡神经元来增强模型表达能力，揭示了ENSO可预测性主要来自热带太平洋，并探讨了春季可预测性障碍的成因。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习显著提高了ENSO预测技能，但模型的不透明性阻碍了科学信任和业务应用，需要开发数学基础扎实的可解释性框架来增强模型可信度。

Method: 引入基于有界变差函数的数学可解释性框架，通过从激活函数饱和区"拯救"死亡神经元来增强模型表达能力，并进行控制实验验证方法的稳健性。

Result: 分析显示ENSO可预测性主要来自热带太平洋，印度洋和大西洋也有贡献，这与物理理解一致。研究发现尽管春季敏感性扩大，但预测性能下降，这可能是由于变量选择不优造成的。

Conclusion: 春季可预测性障碍可能源于次优的变量选择，建议纳入更多海洋-大气变量可能有助于突破SPB限制，推进长期ENSO预测。

Abstract: The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the "dead" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.

</details>


### [141] [The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks](https://arxiv.org/abs/2601.02080)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 本文揭示了双随机矩阵约束在深度架构中的谱退化现象（同质性陷阱），表明高熵约束会抑制次主导奇异值，限制特征变换的有效深度，导致几何结构在噪声主导时不可逆丢失。


<details>
  <summary>Details</summary>
Motivation: 双随机矩阵在结构保持的深度架构中广泛应用，但存在潜在的谱退化问题。研究者旨在揭示这些约束如何影响网络的谱特性和表达能力，特别是在高熵约束下可能导致的特征退化现象。

Method: 通过理论分析双随机矩阵的谱特性，推导了次主导奇异值σ_2与网络有效深度的谱界，分析了最大熵偏置如何驱动混合算子趋向均匀重心，并形式化证明了层归一化在噪声主导机制下的失效条件。

Result: 发现高熵约束会抑制次主导奇异值，限制特征变换的有效感受野；当信噪比低于临界阈值时，几何结构会不可逆地丢失到噪声诱导的正交崩溃中；层归一化无法缓解这种崩溃。

Conclusion: 双随机矩阵约束网络存在熵稳定性与谱表达能力之间的基本权衡，高熵约束虽然提供数值稳定性和概率可解释性，但会牺牲网络的谱表达能力和深层特征变换能力。

Abstract: Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.

</details>


### [142] [A Differentiable Adversarial Framework for Task-Aware Data Subsampling](https://arxiv.org/abs/2601.02081)
*Jiacheng Lyu,Bihua Bao*

Main category: cs.LG

TL;DR: 提出对抗性软选择下采样框架，将数据缩减重构为可微分的端到端学习问题，通过选择器网络和任务网络的对抗博弈学习样本重要性权重，实现任务感知的智能数据下采样。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集对模型训练带来计算挑战，传统数据下采样方法作为静态、任务无关的预处理步骤通常会丢弃对下游预测关键的信息。

Method: 提出对抗性软选择下采样框架，通过选择器网络和任务网络的对抗博弈，选择器网络学习为样本分配连续重要性权重，使用Gumbel-Softmax松弛实现直接优化，在平衡预测保真度和稀疏性的损失函数指导下识别和保留对特定任务目标信息量最大的样本。

Result: 在四个大规模真实世界数据集上的综合实验表明，ASSS始终优于聚类和最近邻细化等启发式下采样基线，不仅能匹配有时甚至超过使用整个数据集的训练性能，展示了智能去噪的效果。

Conclusion: 该工作将任务感知数据下采样确立为可学习组件，为有效的大规模数据学习提供了原则性解决方案。

Abstract: The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.

</details>


### [143] [Horizon Activation Mapping for Neural Networks in Time Series Forecasting](https://arxiv.org/abs/2601.02094)
*Hans Krupakar,V A Kandappan*

Main category: cs.LG

TL;DR: 本文提出HAM（Horizon Activation Mapping），一种用于时间序列预测模型的可视化解释技术，通过梯度范数平均分析不同时间子序列的重要性，适用于跨模型家族的模型选择和比较。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型依赖误差指标和特定架构的解释方法，这些方法无法跨不同模型家族应用。需要一种模型无关的可视化解释技术来理解预测模型在不同时间子序列上的关注模式。

Method: 提出HAM技术，受grad-CAM启发，使用梯度范数平均来研究时间序列预测中不同时间子序列的重要性。引入因果和反因果模式计算每个时间步的梯度更新范数平均，以及表示范数平均均匀分布的比例线。在ETTm2数据集上测试了多种模型，包括MLP-based、自注意力、SSM和扩散模型。

Result: HAM能够可视化不同模型在训练、验证和测试集上的激活模式。发现批次大小差异的活动可能表明存在指数近似关系。NHITS的神经近似定理和SpaceTime的指数自回归活动在HAM图中得到体现。HAM可用于细粒度模型选择、验证集选择和跨模型家族比较。

Conclusion: HAM是一种有效的跨模型家族可视化解释技术，能够帮助研究人员理解不同时间序列预测模型的行为模式，为模型选择、验证集设计和跨模型比较提供新的视角。

Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.

</details>


### [144] [LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training](https://arxiv.org/abs/2601.02105)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 提出LION-DG初始化方法，针对深度监督架构的辅助分类器进行层感知初始化，通过梯度唤醒机制实现无超参数隐式预热，加速收敛


<details>
  <summary>Details</summary>
Motivation: 现有权重初始化方法大多是层无关的，而深度监督架构中的未训练辅助分类器头会通过梯度干扰破坏早期训练稳定性

Method: 提出LION-DG层感知初始化：对辅助分类器头进行零初始化，对主干网络应用标准He初始化，实现梯度唤醒机制

Result: 在CIFAR-10/100上测试DenseNet-DS和ResNet-DS：DenseNet-DS收敛速度提升8.3%；LSUV与LION-DG结合达到81.92%最佳准确率；ResNet-DS在CIFAR-100上加速11.3%

Conclusion: LION-DG方法简单、无需超参数、无计算开销，为深度监督架构提供了有效的初始化解决方案和实用指南

Abstract: Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.
  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.
  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.
  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.

</details>


### [145] [Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI](https://arxiv.org/abs/2601.02106)
*Ashish Rana,Ammar Shaker,Sascha Saralajew,Takashi Suzuki,Kosuke Yasuda,Shintaro Kato,Toshikazu Wada,Toshiyuki Fujikawa,Toru Kikutsuji*

Main category: cs.LG

TL;DR: ProtoPal框架通过原型学习实现个性化预防医疗，提供可理解和可验证的预测与干预，具有前后端模式，性能优越且展示直观。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在个性化预防医疗中存在不足：预测、干预和推荐需要对所有医疗利益相关者来说都是可理解和可验证的。

Method: 提出ProtoPal框架，采用原型学习方法，具有前端和后端两种模式，提供直观的干预展示和模拟结果。

Result: 实现了优越的定量性能，同时提供了干预措施及其模拟结果的直观呈现。

Conclusion: 原型学习能够满足个性化预防医疗中对可理解性和可验证性的需求，ProtoPal框架为此提供了有效解决方案。

Abstract: Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.

</details>


### [146] [Edge-aware GAT-based protein binding site prediction](https://arxiv.org/abs/2601.02138)
*Weisen Yang,Hanqing Zhang,Wangren Qiu,Xuan Xiao,Weizhong Lin*

Main category: cs.LG

TL;DR: 提出Edge-aware GAT模型，通过原子级图结构和多维特征集成，实现蛋白质结合位点的细粒度预测，在基准数据集上达到0.93 ROC-AUC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结合位点的准确识别对理解生物分子相互作用机制和药物靶点理性设计至关重要。传统预测方法在捕捉复杂空间构象时难以平衡预测精度与计算效率。

Method: 提出Edge-aware Graph Attention Network模型，构建原子级图并集成几何描述符、DSSP二级结构和相对溶剂可及性等多维结构特征。通过将原子间距离和方向向量作为注意力机制中的边特征，增强模型表示能力。采用方向张量传播和残基级注意力池化技术。

Result: 在基准数据集上，模型在蛋白质-蛋白质结合位点预测中达到0.93 ROC-AUC，优于多种最先进方法。PyMOL可视化证实了模型的实用性和可解释性。已部署公开可访问的Web服务器。

Conclusion: 该方法为蛋白质功能位点识别提供了新颖高效的解决方案，在预测精度、泛化能力和可解释性之间取得了良好平衡。

Abstract: Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.

</details>


### [147] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: 提出EAFT方法，利用token级熵作为门控机制，区分认知不确定性和知识冲突，在保持下游性能的同时显著缓解通用能力退化


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)常导致灾难性遗忘，而在线强化学习(RL)能有效保留通用能力。研究发现根本原因在于分布差异：RL与模型内部信念对齐，而SFT强制模型拟合外部监督，导致"自信冲突"token（低概率但低熵）引发破坏性梯度更新

Method: 提出熵自适应微调(EAFT)，不同于仅依赖预测概率的方法，利用token级熵作为门控机制来区分认知不确定性和知识冲突。模型从不确信样本中学习，同时抑制冲突数据的梯度

Result: 在Qwen和GLM系列模型（4B到32B参数）上，在数学、医疗和智能体领域进行广泛实验。EAFT在保持标准SFT下游性能的同时，显著减轻了通用能力的退化

Conclusion: EAFT通过熵自适应机制有效解决了SFT中的灾难性遗忘问题，在保持下游任务性能的同时保护了模型的通用能力，验证了"自信冲突"是导致遗忘的关键因素

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [148] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: 研究显示，即使面对看似有益的单调对抗性数据污染，标准机器学习算法也会因过度依赖数据的可交换性和独立性而表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探究标准机器学习算法对数据可交换性和独立性的依赖程度，通过引入单调对抗性污染模型来测试算法在非理想数据条件下的鲁棒性。

Method: 引入单调对抗性污染模型：攻击者在查看"干净"的i.i.d.数据集后，插入自己选择的"污染"点，这些点被约束为单调污染（即按照真实目标函数进行标记）。

Result: 所有已知的二元分类最优学习算法在这种设置下都会在新独立测试点上获得次优的期望误差；而基于一致收敛的算法则不会降低其性能保证。

Conclusion: 最优学习算法在面对看似有益的单调污染时会崩溃，暴露了它们对可交换性的过度依赖，而基于一致收敛的算法更具鲁棒性。

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [149] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于蒙特卡洛树搜索和图神经网络的自动化网络防御方法，在CAGE-4挑战中实现了比现有强化学习方法更高效和鲁棒的防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有自动化网络防御方法（如深度强化学习）在复杂网络环境中面临探索困难、样本效率低下的问题，需要大量昂贵的训练样本。为了解决这一挑战，研究者寻求开发样本效率更高的防御策略。

Method: 将自动化网络防御建模为基于上下文的部分可观察马尔可夫决策问题，提出基于蒙特卡洛树搜索的规划中心防御策略。使用图神经网络将网络观测嵌入为属性图，实现主机及其关系的置换不变推理。结合学习到的图嵌入和图编辑动作先验来指导MCTS，融合无模型泛化、策略蒸馏和前向规划。

Result: 在CAGE-4挑战的各种网络结构和对手行为场景中，基于搜索引导和图嵌入的规划方法相比最先进的强化学习基线，显著提高了防御奖励和鲁棒性。

Conclusion: 该方法通过结合图神经网络和蒙特卡洛树搜索，有效解决了自动化网络防御中的探索-利用权衡问题，实现了样本效率更高、性能更优的防御策略，为复杂网络环境下的自动化防御提供了实用解决方案。

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [150] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: CORE提出了一种基于代码的逆自训练框架，通过图扩展连接模仿学习和强化学习，自动从专家演示中推断奖励函数，增强行为多样性，无需人工设计奖励。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚拟代理训练存在两种主流范式冲突：行为克隆简单有效但行为多样性低；强化学习能发现新策略但严重依赖人工设计的奖励函数。需要解决这两种方法的冲突。

Method: 1. 语义代码抽象：自动从专家演示中推断奖励函数（标签函数），无需人工设计；2. 策略图扩展：构建多路径策略图，增强领域内行为多样性；3. 轨迹引导外推：利用成功和失败轨迹扩展任务空间，丰富领域外行为多样性。

Result: 在Web和Android平台上的实验表明，CORE显著提高了整体性能和泛化能力，展示了作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。

Conclusion: CORE通过连接模仿学习和探索学习，提供了一种新颖的训练框架，既促进了行为多样性，又消除了对人工奖励设计的依赖，为构建强大的多模态虚拟代理提供了有效的解决方案。

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


### [151] [Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.02213)
*Haoyu Zhou,Ping Xue,Tianfan Fu,Hao Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种用于压缩和加速SO(3)-等变图神经网络的低比特量化技术，通过解耦量化方案、分支分离训练策略和注意力归一化机制，在保持精度和等变性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署对3D旋转等变的图神经网络面临高计算成本挑战，需要压缩和加速这些模型以在实际化学应用中实现高效部署。

Method: 提出三种创新技术：1) 幅度-方向解耦量化方案，分别量化等变特征的范数和方向；2) 分支分离量化感知训练策略，在注意力基SO(3)-GNN中区别处理不变和等变特征通道；3) 鲁棒性增强的注意力归一化机制，稳定低精度注意力计算。

Result: 在QM9和rMD17分子基准测试中，8位模型在能量和力预测方面达到与全精度基线相当的精度，推理速度提升2.37-2.73倍，模型大小减小4倍，同时保持等变性。

Conclusion: 提出的量化技术能够在保持精度和物理对称性的前提下，显著提升SO(3)-等变GNN的效率，使其能够在实际化学应用中部署，为对称感知图神经网络在边缘设备上的应用提供了可行方案。

Abstract: Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.

</details>


### [152] [ELLA: Efficient Lifelong Learning for Adapters in Large Language Models](https://arxiv.org/abs/2601.02232)
*Shristi Das Biswas,Yue Zhang,Anwesan Pal,Radhika Bhargava,Kaushik Roy*

Main category: cs.LG

TL;DR: ELLA：一种基于选择性子空间去相关的持续学习框架，通过惩罚任务特定方向的对齐来减少灾难性遗忘，同时保留低能量子空间的自由度以实现正向迁移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在持续学习设置中面临严重的灾难性遗忘问题。现有方法存在根本性限制：基于重放的方法不切实际且侵犯隐私，而严格正交性方法在规模扩展时会崩溃，因为每个新任务都被投影到正交补空间，逐渐减少剩余自由度并禁止共享表示的重叠，从而消除正向迁移。

Method: ELLA基于选择性子空间去相关原则，通过明确表征过去更新的结构，惩罚沿高能量、任务特定方向的对齐，同时保留低能量残差子空间的自由度以实现迁移。这通过一个轻量级正则化器在单个聚合更新矩阵上实现，对应一个各向异性收缩算子来限制干扰。

Result: 在三个流行基准测试中达到最先进的持续学习性能，相对准确率提升高达9.6%，内存占用减少35倍。无需数据重放、架构扩展和可忽略的存储开销，并能跨架构稳健扩展，主动增强模型在未见任务上的零样本泛化性能。

Conclusion: ELLA为构建性终身LLM适应提供了一个原则性且可扩展的解决方案，通过选择性子空间去相关有效平衡了任务特定学习和正向迁移，解决了现有持续学习方法的根本限制。

Abstract: Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\%$ and a $35\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.

</details>


### [153] [POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network](https://arxiv.org/abs/2601.02264)
*Boris Kriuk,Fedor Kriuk*

Main category: cs.LG

TL;DR: POSEIDON是一个物理信息能量模型，用于统一的多任务地震事件预测，结合了Gutenberg-Richter定律和Omori-Utsu余震衰减定律作为可学习的物理约束，在三个任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 地震预测和地震危险性评估是地球物理学的基本挑战，现有机器学习方法通常作为黑盒运行，忽略了已建立的物理定律。需要将物理原理整合到机器学习模型中以提高预测性能和可解释性。

Method: 提出POSEIDON（物理优化地震能量推断和检测操作网络），一个物理信息能量模型，将Gutenberg-Richter震级-频率关系和Omori-Utsu余震衰减定律作为可学习约束嵌入到能量建模框架中。同时处理三个相互关联的预测任务：余震序列识别、海啸生成潜力和前震检测。

Result: POSEIDON在所有任务上实现了最先进的性能，优于梯度提升、随机森林和CNN基线，在所有比较方法中获得了最高的平均F1分数。学习到的物理参数收敛到科学可解释的值：Gutenberg-Richter b值为0.752，Omori-Utsu参数p=0.835，c=0.1948天，这些值落在已建立的地震学范围内，同时增强了预测准确性。

Conclusion: POSEIDON成功地将物理定律整合到机器学习模型中，实现了高性能和可解释性的统一。同时发布了Poseidon数据集——最大的开源全球地震目录，包含280万次事件，跨越30年，为物理信息地震研究提供了标准化资源。

Abstract: Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.

</details>


### [154] [Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck](https://arxiv.org/abs/2601.02307)
*Dina El Zein,James Henderson*

Main category: cs.LG

TL;DR: 提出一种基于非参数变分差分隐私的文本数据共享方法，通过在Transformer嵌入中注入噪声来保护隐私，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: Transformer隐藏表示可能编码敏感信息，使攻击者能够恢复输入数据，特别是因为Transformer嵌入包含每个token的多个向量，加剧了隐私风险。

Method: 提出非参数变分差分隐私方法，在Transformer架构中集成非参数变分信息瓶颈层，向多向量嵌入注入噪声，使用Rényi散度衡量隐私保护，并通过贝叶斯差分隐私提供保证。

Result: 在GLUE基准测试中，通过调整噪声水平实现了隐私与准确性的有效权衡，较低噪声水平下模型保持高准确性同时提供强隐私保证。

Conclusion: NVDP方法能够平衡隐私保护和数据实用性，为文本数据的安全共享提供了有效解决方案。

Abstract: We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.

</details>


### [155] [Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay](https://arxiv.org/abs/2601.02310)
*Ahmad Makinde*

Main category: cs.LG

TL;DR: 使用T-KAN网络替代传统LSTM的固定线性权重，通过可学习的B样条激活函数学习市场信号的"形状"，在高频交易环境中显著提升了预测性能


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中的限价订单簿数据噪声大、非线性强，传统模型如DeepLOB随着时间跨度增加预测能力衰减严重（alpha衰减问题），需要更有效的模型来处理市场信号的复杂模式

Method: 提出Temporal Kolmogorov-Arnold Networks (T-KAN)，用可学习的B样条激活函数替代标准LSTM的固定线性权重，使模型能够学习市场信号的"形状"而非仅幅度，并针对低延迟FPGA实现进行优化

Result: 在k=100时间跨度上相对F1分数提升19.1%，在1.0bps交易成本下获得132.48%回报（相比DeepLOB的-82.76%回撤），模型具有可解释性（样条中的"死区"清晰可见）

Conclusion: T-KAN网络在高频限价订单簿预测中显著优于传统方法，不仅提升预测性能，还提供更好的可解释性和硬件优化潜力，为高频交易环境提供了有效的解决方案

Abstract: High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.

</details>


### [156] [Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning](https://arxiv.org/abs/2601.02313)
*Hanzaleh Akbari Nodehi,Viveck R. Cadambe,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 论文提出了一个新颖的博弈论框架"编码博弈"，将编码理论扩展到理性对手场景，在诚实节点不占多数的情况下仍能实现非零数据恢复概率，并具有Sybil抗性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化机器学习等新兴应用中，参与节点因贡献而获得奖励，这催生了理性对手而非纯粹恶意对手。传统编码理论假设最坏情况对抗模型，要求诚实节点数量超过对手，但在去中心化系统中诚实节点可能不占多数，需要新的编码方法。

Method: 引入"编码博弈"这一博弈论框架，扩展编码理论到信任最小化设置。重点关注重复编码，分析理性对手的战略行为，研究在对手占多数情况下仍能实现数据恢复的机制，并证明该框架具有Sybil抗性。

Result: 该框架能够实现：1）即使对手节点占多数，仍能获得非零的数据恢复概率；2）Sybil抗性，即均衡状态不随对手节点数量增加而改变。为理性对手场景下的编码提供了理论基础。

Conclusion: 编码博弈框架为去中心化系统中的理性对手问题提供了新的解决方案，突破了传统编码理论的限制。论文还探讨了对手策略未知的情况，并提出了未来研究的若干开放问题。

Abstract: Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.

</details>


### [157] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 论文提出评估视觉语言模型的三个标准：忠实性、可区分性和效率，并发现现有评估存在多项缺陷，通过转换和过滤方法改进评估质量，发布清理后的评估套件。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型训练工作众多，但其评估方法仍不成熟。现有评估存在多种缺陷，无法准确反映模型真实能力，且计算成本过高，需要建立更严谨和可持续的评估实践。

Method: 提出评估应满足的三个标准：忠实性、可区分性和效率。识别现有评估的失败模式，包括选择题格式问题、盲目可解问题和错误标注样本。通过将选择题转换为生成任务，过滤盲目可解和错误标注样本来改进评估。

Result: 发现选择题转换为生成任务后模型能力下降高达35%。过滤盲目可解和错误标注样本提高了区分能力同时降低计算成本。发布DatBench-Full（33个数据集）和DatBench（高效子集），后者实现13倍平均加速（最高50倍）。

Conclusion: 为视觉语言模型评估提供了更严谨和可持续的路径，通过转换和过滤现有基准来提高评估的忠实性和区分能力，同时显著降低计算成本。

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


### [158] [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
*Yazan Obeidi,Amir Sarfi,Joel Lidin,Paul Janson,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo（低通信数据并行）与低带宽流水线模型并行相结合，通过激活和激活梯度压缩，实现异构分布式训练，提高LLM预训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要分布式计算，但带宽限制使得在数据中心之外难以扩展，特别是当模型并行需要频繁的大规模设备间通信时。需要研究如何将低通信数据并行方法与低带宽流水线模型并行相结合。

Method: 提出异构分布式训练框架：高带宽参与者托管完整副本，资源受限参与者分组使用流水线并行，通过子空间投影进行阶段间通信压缩。将子空间流水线压缩与SparseLoCo结合，研究多种适配方案。

Result: 在大规模语言建模实验（1.78亿-10亿参数）中，激活压缩与SparseLoCo结合成本适中，选择性（异构）压缩相比压缩所有副本能持续改善损失-通信权衡，特别是在高压缩比下。

Conclusion: 这些结果表明了一条将低带宽模型并行和异构参与者纳入LLM预训练的实用路径，为资源受限环境下的分布式训练提供了可行方案。

Abstract: Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [159] [A System Architecture for Low Latency Multiprogramming Quantum Computing](https://arxiv.org/abs/2601.01158)
*Yilun Zhao,Yu Chen,Kaiyan Chang,He Li,Bing Li,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: FLAMENCO是一个保真度感知的多版本编译系统，通过离线编译和运行时动态区域选择，消除了量子多程序计算中的在线编译开销，实现低延迟、高保真度的多程序执行。


<details>
  <summary>Details</summary>
Motivation: 随着量子系统规模扩大，多程序量子计算（MPQC）对提高设备利用率和吞吐量至关重要。然而，当前的MPQC流水线依赖昂贵的在线编译来协同优化并发运行的程序，因为量子可执行文件是设备相关的、跨量子比特区域不可移植的，并且对噪声和串扰高度敏感。这种在线步骤主导了运行时，阻碍了未来实际工作负载（如重复调用的量子神经网络服务）的低延迟部署。

Method: 1. 架构层面：将设备抽象为计算单元，大幅缩小区域分配的搜索空间
2. 编译时：为每个程序生成绑定到不同量子比特区域的多样化可执行版本，实现运行时动态区域选择
3. 运行时：采用简化的编排器，利用编译后保真度指标避免冲突和减轻串扰，无需在线协同优化

Result: 与最先进的MPQC基线相比，FLAMENCO消除了在线编译开销，实现了超过5倍的运行时加速，提高了执行保真度，并在并发增加时保持高利用率。

Conclusion: FLAMENCO通过保真度感知的多版本编译系统，实现了独立的离线编译和低延迟、高保真度的多程序运行时执行，解决了量子多程序计算中的关键瓶颈问题。

Abstract: As quantum systems scale, Multiprogramming Quantum Computing (MPQC) becomes essential to improve device utilization and throughput. However, current MPQC pipelines rely on expensive online compilation to co-optimize concurrently running programs, because quantum executables are device-dependent, non-portable across qubit regions, and highly susceptible to noise and crosstalk. This online step dominates runtime and impedes low-latency deployments for practical, real-world workloads in the future, such as repeatedly invoked Quantum Neural Network (QNN) services.
  We present FLAMENCO, a fidelity-aware multi-version compilation system that enables independent offline compilation and low-latency, high-fidelity multiprogramming at runtime. At the architecture level, FLAMENCO abstracts devices into compute units to drastically shrink the search space of region allocation. At compile time, it generates diverse executable versions for each program -- each bound to a distinct qubit region -- allowing dynamic region selection at runtime and overcoming non-portability. At runtime, FLAMENCO employs a streamlined orchestrator that leverages post-compilation fidelity metrics to avoid conflicts and mitigate crosstalk, achieving reliable co-execution without online co-optimization. Comprehensive evaluations against state-of-the-art MPQC baselines show that FLAMENCO removes online compilation overhead, achieves over 5$\times$ runtime speedup, improves execution fidelity, and maintains high utilization as concurrency increases.

</details>


### [160] [CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)](https://arxiv.org/abs/2601.01265)
*Nick Lindsay,Caroline Trippel,Anurag Khandelwal,Abhishek Bhattacharjee*

Main category: cs.AR

TL;DR: CounterPoint是一个测试微架构模型与性能计数器数据一致性的框架，通过μ路径决策图表达模型，利用多维计数器置信区域减少复用噪声，帮助专家发现未文档化的硬件特性。


<details>
  <summary>Details</summary>
Motivation: 硬件事件计数器虽然能揭示微架构行为，但由于规格模糊、设计不透明和复用噪声，导致数据难以解释。现有方法无法有效验证微架构模型与计数器数据的一致性。

Method: 提出CounterPoint框架：1）使用μ路径决策图表达用户指定的微架构模型；2）测试模型与性能计数器数据的一致性；3）当出现不匹配时，精确定位可能的微架构特征；4）使用多维计数器置信区域来减轻复用噪声的影响。

Result: 将CounterPoint应用于Haswell内存管理单元案例研究，揭示了多个未文档化和文档不完整的微架构行为，包括：负载存储队列侧TLB预取器、合并页表遍历器、可中止页表遍历等。

Conclusion: CounterPoint帮助专家将噪声硬件性能计数器测量与微架构心理模型相协调，在此过程中发现先前隐藏的微妙硬件特性，为微架构分析提供了系统化方法。

Abstract: Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.
  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $μ$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.
  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.

</details>


### [161] [Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows](https://arxiv.org/abs/2601.02053)
*Leandro Lanzieri,Jiri Kral,Goerschwin Fey,Holger Schlarb,Thomas C. Schmidt*

Main category: cs.AR

TL;DR: 提出一种基于软件自测试的微控制器硬件老化监控方法，通过可变长度时间窗口确定设备最大工作频率，可现场部署检测温度引起的性能退化


<details>
  <summary>Details</summary>
Motivation: 微控制器在嵌入式系统和可靠性应用中日益普及，硬件老化导致的故障可能产生严重影响。目前缺乏可部署的老化监控技术，通常采用静态保护带防止时序错误，但这会限制性能并在设备老化时导致突然故障

Method: 采用基于软件的自测试方法设计微控制器硬件退化监控技术，利用可变长度时间窗口确定设备的最大工作频率，可在现场部署

Result: 在真实硬件上经验验证，该方法能一致检测到温度引起的最大工作频率退化，在温度升高60°C时，不同设备的频率退化最高可达13.79%

Conclusion: 提出的软件自测试方法为微控制器提供了一种可部署的硬件老化监控解决方案，能有效检测温度引起的性能退化，优于传统的静态保护带方法

Abstract: Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase.

</details>


### [162] [HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV](https://arxiv.org/abs/2601.02135)
*Liu Shijie,Zeng Zhenghao,Jiao Han,Huang Yihua*

Main category: cs.AR

TL;DR: HFRWKV：针对RWKV模型的FPGA硬件加速器，通过混合精度量化、可复用架构和全片上计算系统，显著提升吞吐量和能效


<details>
  <summary>Details</summary>
Motivation: RWKV作为现代RNN架构在处理长上下文时具有线性内存优势，但其顺序计算模式难以充分利用GPU并行性，且频繁的片外权重访问造成内存瓶颈

Method: 1. 硬件友好的混合精度量化策略；2. 复杂运算（指数、除法）采用可复用架构结合查找表或分段线性逼近；3. 全片上计算系统集成并行矩阵向量处理阵列和高效流水线架构；4. 计算重排序和分块双缓冲技术消除数据传输瓶颈

Result: 在Alveo U50和U280平台上实现，相比CPU实现63.48倍吞吐量提升和139.17倍能效提升；相比GPU实现32.33倍吞吐量提升和171.36倍能效提升

Conclusion: HFRWKV通过定制化硬件设计有效解决了RWKV在GPU上的并行性不足和内存瓶颈问题，在吞吐量和能效方面均取得显著提升，为RNN类模型的高效推理提供了有前景的硬件解决方案

Abstract: RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\times$ and an energy efficiency improvement of 139.17$\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\times$ and an energy efficiency improvement of 171.36$\times$.

</details>
