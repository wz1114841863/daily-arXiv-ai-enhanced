<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Portable Targeted Sampling Framework Using LLVM](https://arxiv.org/abs/2509.02873)
*Zhantong Qiu,Mahyar Samani,Jason Lowe-Power*

Main category: cs.AR

TL;DR: Nugget是一个灵活的采样框架，可在LLVM IR级别进行二进制无关的区间分析，生成轻量级可执行文件，大幅降低仿真成本并支持跨平台验证。


<details>
  <summary>Details</summary>
Motivation: 全面的架构评估受到仿真速度慢和每二进制采样流程的限制，需要一种更快速、可移植的采样方法。

Method: 在LLVM IR级别进行二进制无关的区间分析，生成轻量级、跨平台的可执行文件（nuggets），可在真实机器上验证后再驱动仿真。

Result: 在SPEC CPU2017、NPB和LSMS基准测试中，相比功能仿真将区间分析成本降低了数个数量级（多线程NPB最高达578倍），单线程开销低，支持原生速度的样本验证。

Conclusion: Nugget使采样方法学研究更快、更可移植，支持系统性能和模型准确性的评估。

Abstract: Comprehensive architectural evaluation of full workloads is throttled by slow
simulation and per-binary sampling pipelines. We present Nugget, a flexible
framework for portable sampling across simulators and real hardware, ISAs, and
libraries. Nugget operates at the LLVM IR level to perform binary-agnostic
interval analysis, then emits lightweight, cross-platform
executables--nuggets--that can be validated on real machines before driving
simulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis
cost by orders of magnitude relative to functional simulation (up to ~578X on
multithreaded NPB), keeps single-thread overhead low, and enables native-speed
validation of selected samples. Case studies with gem5 show that nuggets
support evaluation of system performance and model accuracy. Nugget makes
sampling methodology research faster and more portable.

</details>


### [2] [FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays](https://arxiv.org/abs/2509.03103)
*Abdul Rahoof,Vivek Chaturvedi,Muhammad Shafique*

Main category: cs.AR

TL;DR: 该论文提出了一种在FPGA上加速完整胶囊网络(CapsNet)的两步方法：首先使用前瞻核剪枝(LAKP)技术压缩网络，然后简化路由算法的非线性操作和并行化处理，在MNIST和F-MNIST数据集上实现了99.26%和98.84%的压缩率，以及1351 FPS和934 FPS的高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 胶囊网络相比传统CNN在图像理解方面有显著改进，但由于胶囊结构和路由机制的复杂性，在FPGA上加速完整CapsNet具有挑战性。现有工作仅实现动态路由算法，无法满足实际应用需求。

Method: 提出两步法：1) 使用前瞻核剪枝(LAKP)方法剪枝网络；2) 简化路由算法的非线性操作、重排序循环和并行化操作来降低硬件复杂度。

Result: 在MNIST和F-MNIST数据集上，LAKP方法分别实现99.26%和98.84%的压缩率，在PYNQ-Z1 FPGA上分别达到82 FPS和48 FPS的吞吐量。路由算法优化后吞吐量提升至1351 FPS和934 FPS。

Conclusion: 该工作首次实现了完整CapsNet在FPGA上的高效加速，为现代边缘设备中低成本FPGA的高性能部署提供了可行方案。

Abstract: Capsule Network (CapsNet) has shown significant improvement in understanding
the variation in images along with better generalization ability compared to
traditional Convolutional Neural Network (CNN). CapsNet preserves spatial
relationship among extracted features and apply dynamic routing to efficiently
learn the internal connections between capsules. However, due to the capsule
structure and the complexity of the routing mechanism, it is non-trivial to
accelerate CapsNet performance in its original form on Field Programmable Gate
Array (FPGA). Most of the existing works on CapsNet have achieved limited
acceleration as they implement only the dynamic routing algorithm on FPGA,
while considering all the processing steps synergistically is important for
real-world applications of Capsule Networks. Towards this, we propose a novel
two-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune
the network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that
uses the sum of look-ahead scores of the model parameters. Next, we simplify
the nonlinear operations, reorder loops, and parallelize operations of the
routing algorithm to reduce CapsNet hardware complexity. To the best of our
knowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.
Experimental results on the MNIST and F-MNIST datasets (typical in Capsule
Network community) show that the proposed LAKP approach achieves an effective
compression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and
48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware
complexity of the routing algorithm increases the throughput to 1351 FPS and
934 FPS respectively. As corroborated by our results, this work enables highly
performance-efficient deployment of CapsNets on low-cost FPGA that are popular
in modern edge devices.

</details>


### [3] [CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array](https://arxiv.org/abs/2509.03201)
*Abdul Rahoof,Vivek Chaturvedi,Mahesh Raveendranatha Panicker,Muhammad Shafique*

Main category: cs.AR

TL;DR: 提出CapsBeam胶囊网络波束成形器，通过模型剪枝和硬件优化，在FPGA上实现高效部署，相比传统DAS方法显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习在超声成像波束成形中面临边缘设备资源受限的部署挑战，需要开发轻量化高效模型。

Method: 基于胶囊网络的CapsBeam波束成形器，采用多层LookAhead核剪枝(LAKP-ML)方法压缩模型85%，结合量化、非线性操作简化和并行化优化硬件复杂度。

Result: 相比DAS方法：体外数据对比度提升32.31%，轴向和侧向分辨率分别提升16.54%和6.7%；仿真数据对比度提升26%，轴向和侧向分辨率分别提升13.6%和21.5%。FPGA加速器实现卷积操作30 GOPS和动态路由17.4 GOPS吞吐量。

Conclusion: CapsBeam在保持图像质量的同时显著降低计算复杂度，通过专用硬件加速器实现了高效的边缘部署，为资源受限设备的实时超声成像提供了可行解决方案。

Abstract: In recent years, there has been a growing trend in accelerating
computationally complex non-real-time beamforming algorithms in ultrasound
imaging using deep learning models. However, due to the large size and
complexity these state-of-the-art deep learning techniques poses significant
challenges when deploying on resource-constrained edge devices. In this work,
we propose a novel capsule network based beamformer called CapsBeam, designed
to operate on raw radio-frequency data and provide an envelope of beamformed
data through non-steered plane wave insonification. Experiments on in-vivo
data, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)
beamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in
contrast, along with gains of 16.54% and 6.7% in axial and lateral resolution
compared to the DAS. Similarly, in-silico data showed a 26% enhancement in
contrast, along with improvements of 13.6% and 21.5% in axial and lateral
resolution, respectively, compared to the DAS. To reduce the parameter
redundancy and enhance the computational efficiency, we pruned the model using
our multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a
compression ratio of 85% without affecting the image quality. Additionally, the
hardware complexity of the proposed model is reduced by applying quantization,
simplification of non-linear operations, and parallelizing operations. Finally,
we proposed a specialized accelerator architecture for the pruned and optimized
CapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator
achieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS
for the dynamic routing operation.

</details>


### [4] [Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing](https://arxiv.org/abs/2509.03377)
*Rui Xie,Asad Ul Haq,Linsen Ma,Yunhua Fang,Zirak Burzin Engineer,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 通过在CXL设备中集成动态采用低精度位平面布局和透明无损压缩技术，CXL-NDP架构在不改变CXL.mem接口或AI模型的情况下提高了CXL带宽效析，从而大幅提升大语言模型推理性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理遍历于CXL基于内存容量扩展的带宽限制，需要一种透明的近数据处理方案来提高带宽效析而无需改变现有硬件接口或AI模型

Method: 设计CXL-NDP架构，集成精度可缩放位平面布局支持动态量化，并在CXL设备中直接实现对权重咈KV缓存的透明无损压缩

Result: 在终端到终端服务中，CXL-NDP提高吞吐量43%，扩展最大上下文长度87%，减少KV缓存占用46.9%，且无准确性损失，硬件合成确认其实用性

Conclusion: CXL-NDP通过透明的近数据处理技术，使用高效、可扩展的CXL基于内存在生成式AI基础设施中的采用门槛大大降低

Abstract: Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 分析MLPerf Training v4.1在BERT、Llama2 LoRA、RetinaNet和Stable Diffusion四个工作负载上的性能数据，发现存在优化性能、GPU使用率和效率平衡的配置方案


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型训练面临效率挑战，虽然大量使用GPU可以加速训练，但会对效率产生负面影响，需要找到性能与效率的最佳平衡点

Method: 对MLPerf Training v4.1基准测试中四个工作负载（BERT、Llama2 LoRA、RetinaNet、Stable Diffusion）的报告时间进行详细分析

Result: 发现了可以优化性能、GPU使用率和效率之间关系的配置方案，存在一个平衡点可以在减少训练时间的同时最大化效率

Conclusion: 通过合理配置GPU使用策略，可以在保持训练性能的同时显著提升训练效率，为大规模深度学习模型训练提供了实用的优化指导

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [6] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 通过地震信号检测大象脚步的分析框架，采用CCW事件检测技术和SVM分类器，在自然环境中达到有140米检测范围和70-99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决人象冲突(HEC)问题，充分利用地震信号检测大象脚步，克服现有方案依赖人工分类、无法实时应用于自然环境的限制。

Method: 提出了一个优先考虑准确性和计算效率的分析框架，包括新的CCW事件检测技术(专门为大象脚步设计)，并与STA/LTA方法进行对比。使用SVM分类器(RBF核)进行大象脚步分类，采用可解释性AI进行特征影响分析。

Result: 最大验证检测范围在受控条件下为155.6米，在自然环境中为140米。SVM分类器表现优异：受控环境准确率99%，自然大象树林环境73%，HEC人类生境最具70%。零点交叉数和DTW对齐成本是最重要特征。

Conclusion: 该框架能够在资源受限环境中实现高效的大象脚步检测和分类，为解决人象冲突提供了可靠的技术支持。CCW技术和SVM分类器在各种环境下都表现出艹出性能。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [7] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: 提出Lifecycle原则，通过状态记忆机制解决长期神经元失活导致的训练不稳定问题，提升泛化能力和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法如Dropout只是临时改变神经元，本文研究更强大的正则化形式——长期停用神经元，但面临神经元复活时随机权重导致的严重训练不稳定问题

Method: 提出Lifecycle原则，核心创新是状态记忆机制：当神经元复活时，不是重新初始化，而是恢复到上次有效的参数状态，从而保留已学知识并避免破坏性优化冲击

Result: 理论分析表明LC原则平滑了损失景观，引导优化趋向更平坦的最小值（与更好泛化相关）；图像分类基准实验证明方法提升了泛化能力和鲁棒性；消融研究确认状态记忆是实现这些增益的关键

Conclusion: Lifecycle原则通过状态记忆机制有效解决了长期神经元动态性带来的训练不稳定问题，在保持正则化效果的同时显著提升了模型性能

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [8] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 提出基于期望最大化(EM)的隐变量建模方法，结合多智能体强化学习(MARL)用于无人机协同野生动物保护，在伊朗豹栖息地巡逻任务中表现优于PPO和DDPG算法


<details>
  <summary>Details</summary>
Motivation: 解决濒危野生动物非法盗猎问题，特别是在广阔且部分可观测环境中需要实时响应的挑战

Method: 使用EM算法建模隐藏环境因素和智能体间动态的隐变量，增强不确定性下的探索和协调能力，采用10架无人机进行保护区巡逻模拟

Result: 实验结果显示在检测精度、适应性和策略收敛性方面优于PPO和DDPG等标准算法

Conclusion: EM推理与MARL结合能显著改善复杂高风险保护场景中的分散决策能力，具有重要应用潜力

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [9] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: 该论文提出了组感知阈值检验方法，通过为不同人口统计组设置不同的决策阈值，在类别不平衡问题上比合成数据生成方法更有效。


<details>
  <summary>Details</summary>
Motivation: 传统的类别不平衡解决方案常带来新问题，需要找到更简单、可解释性强且效果更好的方法。

Method: 采用组感知阈值检验技术，为不同人口统计组设置不同的决策阈值，优化平衡准确率和最差组平衡准确率之间的底特雷敌优化问题。

Result: 组特定阈值方法比SMOTE和CT-GAN增广模型获得1.5-4%更高的平衡准确率，同时改善了最差组平衡准确率。在7种模型家族中都得到了验证。

Conclusion: 组感知阈值检验提供了一种更简单、可解释性强且效果更好的类别不平衡解决方案，与合成数据生成方法存在基本重复性。

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [10] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: DP-SGD中的量化会导致比普通SGD更严重的精度下降，QPQuant通过动态层选择和损失感知优先化来减少量化方差，在保持隐私保护的同时显著提升性能


<details>
  <summary>Details</summary>
Motivation: DP-SGD中的噪声注入会放大量化方差，导致比常规SGD更严重的精度下降，需要解决量化在差分隐私训练中的性能退化问题

Method: 提出QPQuant动态量化框架，包含：(1)概率性层采样轮换量化层；(2)差分隐私损失敏感度估计器进行损失感知层优先化

Result: 在ResNet18、ResNet50和DenseNet121上验证，QPQuant优于静态量化基线，达到接近帕累托最优的精度-计算权衡，理论吞吐量提升2.21倍，验证精度下降小于2%

Conclusion: QPQuant有效解决了DP-SGD中量化导致的精度下降问题，通过动态量化策略在保持差分隐私保证的同时实现了显著的性能提升

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


### [11] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: 提出了DPO-PRO算法，一种基于直接偏好优化的鲁棒微调方法，用于公共卫生领域的序列资源分配问题奖励函数设计，在噪声偏好信号下表现更稳健且推理成本更低


<details>
  <summary>Details</summary>
Motivation: 解决公共卫生领域中复杂模糊目标和有限数据可用性带来的对齐挑战，需要设计能够处理偏好分布不确定性的鲁棒奖励函数

Method: 基于直接偏好优化(DPO)的DPO-PRO算法，采用轻量级分布式鲁棒优化(DRO)公式来考虑偏好分布的不确定性，相比之前的方法更加不保守

Result: 在真实世界母婴移动健康项目和标准对齐基准测试中，DPO-PRO相比现有DPO变体在噪声偏好信号下鲁棒性显著提升，且与基于自反思的基线方法性能相当但推理成本大幅降低

Conclusion: DPO-PRO为公共卫生领域的序列资源分配问题提供了一种高效且鲁棒的奖励函数设计方法，在保持性能的同时显著降低了计算成本

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [12] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 论文发现策略梯度方法中深度神经网络在特定约束下会出现类似神经坍缩的"动作坍缩"现象，并提出使用固定等角紧框架作为动作选择层目标的ACPG方法，实验证明该方法能加速训练并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注策略网络的收敛性和全局最优性，但很少分析底层网络的表征结构。作者观察到在训练最优策略DNN时会出现动作坍缩现象，这启发他们思考是否可以使用固定的ETF结构作为动作选择层的目标配置来学习最优策略。

Method: 提出Action Collapse Policy Gradient (ACPG)方法，在动作选择层固定一个合成的等角紧框架(ETF)作为目标配置，诱导策略DNN产生理想的配置结构同时保持最优性。

Result: 在各种OpenAI Gym环境中的实验表明，ACPG可以集成到任何离散策略梯度方法中，能够更快、更鲁棒地获得更好的奖励改进。

Conclusion: 通过固定ETF作为动作选择层目标，ACPG方法能够有效诱导策略网络产生理想的动作坍缩结构，从而加速训练过程并提高策略性能，为策略梯度方法提供了新的优化思路。

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


### [13] [Mentality: A Mamba-based Approach towards Foundation Models for EEG](https://arxiv.org/abs/2509.02746)
*Saarang Panchavati,Corey Arnold,William Speier*

Main category: cs.LG

TL;DR: 使用Mamba选择性状态空间模型作为基础模型，通过自监督重建和癫痫检测任务训练，在EEG数据分析中取得0.72 AUROC，为神经疾病诊断提供新方法


<details>
  <summary>Details</summary>
Motivation: EEG信号噪声大、高维且非线性，传统机器学习方法难以捕捉其复杂的时空动态特性，需要更强大的基础模型来提升神经疾病诊断效果

Method: 采用Mamba选择性状态空间模型，在大规模癫痫和非癫痫EEG数据集上进行训练，先进行自监督重建任务，再进行癫痫检测任务

Result: 在保留测试集上达到0.72的AUROC，证明了该方法的有效性

Conclusion: 这种方法为开发大规模、临床适用的EEG数据分析基础模型迈出了重要一步

Abstract: This work explores the potential of foundation models, specifically a
Mamba-based selective state space model, for enhancing EEG analysis in
neurological disorder diagnosis. EEG, crucial for diagnosing conditions like
epilepsy, presents significant challenges due to its noisy, high-dimensional,
and nonlinear nature. Traditional machine learning methods have made advances
in automating EEG analysis but often fail to capture its complex
spatio-temporal dynamics. Recent advances in deep learning, particularly in
sequence modeling, offer new avenues for creating more generalized and
expressive models capable of handling such complexities. By training a
Mamba-based model on a large dataset containing seizure and non-seizure EEG
recordings through a self-supervised reconstruction task followed by a seizure
detection task, we demonstrate the model's effectiveness, achieving an AUROC of
0.72 on a held-out test set. This approach marks a significant step toward
developing large-scale, clinically applicable foundation models for EEG data
analysis.

</details>


### [14] [LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference](https://arxiv.org/abs/2509.02753)
*Krishna Teja Chitty-Venkata,Sandeep Madireddy,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: LExI是一种数据无关的MoE模型优化技术，通过分析模型权重自适应确定每层最优激活专家数量，显著提升推理效率且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 现有MoE剪枝方法主要减少内存占用但无法显著提升GPU推理性能，且传统MoE架构在所有层均匀激活固定数量专家导致计算冗余和次优性能

Method: LExI利用仅模型权重估计每层相对重要性，自适应分配每层激活专家数量，无需训练数据

Result: 在语言和视觉MoE基准测试中，LExI显著优于传统MoE剪枝方法，Qwen1.5-MoE在H100 GPU上达到相同吞吐量时精度提升10%

Conclusion: LExI通过层间专家数量自适应分配有效解决了MoE模型推理效率问题，为稀疏架构优化提供了新方向

Abstract: Mixture-of-Experts (MoE) models scale efficiently by activating only a subset
of experts per token, offering a computationally sparse alternative to dense
architectures. While prior post-training optimizations, such as inter- and
intra-expert pruning, reduce memory usage they provide limited gains in
inference-time compute efficiency. Moreover, existing MoE architectures
typically activate a fixed number of experts uniformly across all layers,
resulting in redundant computation and suboptimal performance. In this work, we
first demonstrate that MoE pruning strategies improve only the memory footprint
but do not significantly improve inference performance on GPU using optimized
frameworks such as vLLM. To address this, we introduce LExI, a data-free
optimization technique that determines the optimal number of active experts per
layer in a pretrained MoE model. LExI leverages only the model weights to
estimate the relative importance of each layer and adaptively assigns the
number of active experts accordingly per layer. Experiments on state-of-the-art
language and vision MoE benchmarks demonstrate that LExI significantly
outperforms traditional MoE pruning approaches in terms of inference efficiency
with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves
the same throughput on Nvidia H100 GPU with 10% better accuracy than
traditional expert pruning.

</details>


### [15] [The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface](https://arxiv.org/abs/2509.02783)
*Arnab Mazumder,Javier E. Santos,Noah Hobbs,Mohamed Mehana,Daniel O'Malley*

Main category: cs.LG

TL;DR: Transparent Earth是一个基于transformer的架构，用于从稀疏度、分辨率和模态各异的异构数据集中重建地下属性，支持任意模态数量的扩展和上下文学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决从不同类型观测数据（如应力角度、地幔温度、板块类型等）中重建地下属性的挑战，这些数据在稀疏性、分辨率和模态上存在差异，需要一种能够灵活处理多模态数据并支持新模态扩展的模型。

Method: 采用transformer架构，结合位置编码和模态编码（通过文本嵌入模型对每种模态的描述生成），支持任意数量的模态，目前包含八种模态（方向角度、分类类别、连续属性如温度和厚度），支持上下文学习（无输入或任意数量观测的预测）。

Result: 在验证数据上，预测应力角度的误差减少了三倍以上，模型具有可扩展性，参数增加时性能提升。

Conclusion: Transparent Earth作为地球地下属性的基础模型，旨在预测地球上任何地方的地下属性，展示了在处理多模态地下数据方面的先进能力和扩展性。

Abstract: We present the Transparent Earth, a transformer-based architecture for
reconstructing subsurface properties from heterogeneous datasets that vary in
sparsity, resolution, and modality, where each modality represents a distinct
type of observation (e.g., stress angle, mantle temperature, tectonic plate
type). The model incorporates positional encodings of observations together
with modality encodings, derived from a text embedding model applied to a
description of each modality. This design enables the model to scale to an
arbitrary number of modalities, making it straightforward to add new ones not
considered in the initial design. We currently include eight modalities
spanning directional angles, categorical classes, and continuous properties
such as temperature and thickness. These capabilities support in-context
learning, enabling the model to generate predictions either with no inputs or
with an arbitrary number of additional observations from any subset of
modalities. On validation data, this reduces errors in predicting stress angle
by more than a factor of three. The proposed architecture is scalable and
demonstrates improved performance with increased parameters. Together, these
advances make the Transparent Earth an initial foundation model for the Earth's
subsurface that ultimately aims to predict any subsurface property anywhere on
Earth.

</details>


### [16] [Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity](https://arxiv.org/abs/2509.02792)
*Alejandro Rodriguez Dominguez,Muhammad Shahzad,Xia Hong*

Main category: cs.LG

TL;DR: 提出结构化基函数网络，通过Bregman散度将多假设预测和集成学习统一起来，提供可调节的多样性机制来控制偏差-方差-多样性权衡


<details>
  <summary>Details</summary>
Motivation: 现有预测不确定性方法存在局限：多假设预测缺乏原则性聚合，集成学习难以捕捉结构化模糊性，缺乏与损失几何一致的统一框架

Method: 使用Bregman散度诱导的质心聚合连接多假设预测和集成学习，支持闭式最小二乘估计器和基于梯度的通用目标优化，提供可调节多样性机制

Result: 实验验证了该方法在多假设泛化和损失感知集成聚合之间的关系，并在深度学习预测器上研究了复杂度-容量-多样性权衡

Conclusion: 该框架为预测不确定性提供了统一的几何一致性方法，能够有效控制偏差-方差-多样性权衡，适用于回归和分类任务

Abstract: Existing approaches to predictive uncertainty rely either on multi-hypothesis
prediction, which promotes diversity but lacks principled aggregation, or on
ensemble learning, which improves accuracy but rarely captures the structured
ambiguity. This implicitly means that a unified framework consistent with the
loss geometry remains absent. The Structured Basis Function Network addresses
this gap by linking multi-hypothesis prediction and ensembling through
centroidal aggregation induced by Bregman divergences. The formulation applies
across regression and classification by aligning predictions with the geometry
of the loss, and supports both a closed-form least-squares estimator and a
gradient-based procedure for general objectives. A tunable diversity mechanism
provides parametric control of the bias-variance-diversity trade-off,
connecting multi-hypothesis generalisation with loss-aware ensemble
aggregation. Experiments validate this relation and use the mechanism to study
the complexity-capacity-diversity trade-off across datasets of increasing
difficulty with deep-learning predictors.

</details>


### [17] [Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks](https://arxiv.org/abs/2509.02803)
*Howard Dai,Nyambura Njenga,Benjamin Whitsett,Catherine Ma,Darwin Deng,Sara de Ángel,Alexandre Van Tassel,Siddharth Viswanath,Ryan Pellico,Ian Adelstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出基于拉普拉斯特征向量学习的图神经网络预训练框架，通过预测低频率特征向量来捕获全局图结构信息，解决传统MPNN的过平滑问题


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络(MPNN)在增加网络深度时容易出现过平滑问题，难以捕获全局和区域图结构信息。图拉普拉斯矩阵的低频特征向量编码了全局信息，因此通过预训练GNN来预测这些特征向量可以促使网络自然学习大规模结构模式

Method: 提出自监督预训练框架，通过归纳学习拉普拉斯特征向量来预训练GNN。该方法可以应用于所有基于图的数据集，在任务特定数据稀疏时还可以使用合成特征

Result: 实验表明，通过该框架预训练的模型在各种基于图结构的任务上优于基线模型

Conclusion: 该结构化的自监督预训练框架具有高度灵活性，与大多数关注领域特定任务（如节点或边特征重建）的现有预训练方法不同，该方法专注于图结构学习

Abstract: We propose a novel framework for pre-training Graph Neural Networks (GNNs) by
inductively learning Laplacian eigenvectors. Traditional Message Passing Neural
Networks (MPNNs) often struggle to capture global and regional graph structure
due to over-smoothing risk as network depth increases. Because the
low-frequency eigenvectors of the graph Laplacian matrix encode global
information, pre-training GNNs to predict these eigenvectors encourages the
network to naturally learn large-scale structural patterns over each graph.
Empirically, we show that models pre-trained via our framework outperform
baseline models on a variety of graph structure-based tasks. While most
existing pre-training methods focus on domain-specific tasks like node or edge
feature reconstruction, our self-supervised pre-training framework is
structure-based and highly flexible. Eigenvector-learning can be applied to all
graph-based datasets, and can be used with synthetic features when
task-specific data is sparse.

</details>


### [18] [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
*Trang Nguyen,Jackson Michaels,Madalina Fiterau,David Jensen*

Main category: cs.LG

TL;DR: 论文研究了视觉语言模型中冲突检测与冲突解决的分离机制，发现在LLaVA-OV-7B模型的中间层存在线性可解码的冲突信号，注意力模式在不同网络阶段出现分化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型中冲突检测与冲突解决机制难以分离的挑战，以提升模型在冲突性多模态输入下的可解释性和鲁棒性。

Method: 使用线性探针进行监督度量分析和基于组的注意力模式分析，对LLaVA-OV-7B模型进行机制性研究。

Result: 发现在模型中间层出现线性可解码的冲突信号，冲突检测和解决的注意力模式在网络不同阶段出现分化，证实了二者是功能不同的机制。

Conclusion: 这种分解机制为实现更可操作的可解释性和针对性干预提供了可能，有助于提升模型在挑战性多模态环境中的鲁棒性。

Abstract: This paper highlights the challenge of decomposing conflict detection from
conflict resolution in Vision-Language Models (VLMs) and presents potential
approaches, including using a supervised metric via linear probes and
group-based attention pattern analysis. We conduct a mechanistic investigation
of LLaVA-OV-7B, a state-of-the-art VLM that exhibits diverse resolution
behaviors when faced with conflicting multimodal inputs. Our results show that
a linearly decodable conflict signal emerges in the model's intermediate layers
and that attention patterns associated with conflict detection and resolution
diverge at different stages of the network. These findings support the
hypothesis that detection and resolution are functionally distinct mechanisms.
We discuss how such decomposition enables more actionable interpretability and
targeted interventions for improving model robustness in challenging multimodal
settings.

</details>


### [19] [Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs](https://arxiv.org/abs/2509.02820)
*Naman Deep Singh,Maximilian Müller,Francesco Croce,Matthias Hein*

Main category: cs.LG

TL;DR: JensUn是一种新的大语言模型遗忘方法，使用Jensen-Shannon散度作为训练目标，在遗忘和保留集上实现更稳定的遗忘动态，相比现有方法具有更好的遗忘-效用平衡和抗再学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型遗忘方法在严格评估下效果不佳，需要更有效的遗忘技术来确保模型安全，删除预训练中获得的私有数据或有害知识。

Method: 提出JensUn方法，利用Jensen-Shannon散度作为训练目标，同时优化遗忘集和保留集；引入LKF数据集用于精确评估；使用LLM作为语义评判器替代ROUGE分数；采用最坏情况评估框架。

Result: JensUn在广泛实验中实现了比竞争方法更好的遗忘-效用平衡，并表现出对良性再学习的强大韧性。改进的评估框架显示许多现有方法效果不如之前认为的好。

Conclusion: JensUn提供了一种更有效的大语言模型遗忘解决方案，新的评估方法揭示了现有方法的局限性，为未来研究提供了更严格的评估标准。

Abstract: Unlearning in large language models (LLMs) involves precisely removing
specific information from a pre-trained model. This is crucial to ensure safety
of LLMs by deleting private data or harmful knowledge acquired during
pre-training. However, existing unlearning methods often fall short when
subjected to thorough evaluation. To overcome this, we introduce JensUn, where
we leverage the Jensen-Shannon Divergence as the training objective for both
forget and retain sets for more stable and effective unlearning dynamics
compared to commonly used loss functions. In extensive experiments, JensUn
achieves better forget-utility trade-off than competing methods, and even
demonstrates strong resilience to benign relearning. Additionally, for a
precise unlearning evaluation, we introduce LKF, a curated dataset of
lesser-known facts that provides a realistic unlearning scenario. Finally, to
comprehensively test unlearning methods, we propose (i) employing an LLM as
semantic judge instead of the standard ROUGE score, and (ii) using worst-case
unlearning evaluation over various paraphrases and input formats. Our improved
evaluation framework reveals that many existing methods are less effective than
previously thought.

</details>


### [20] [Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction](https://arxiv.org/abs/2509.02826)
*Towhidul Islam,Md Sumon Ali*

Main category: cs.LG

TL;DR: 这篇论文比较了混合多数投票咈集成堆叠两种集成学习方法在肥胖风险预测中的性能，发现集成堆叠方法在复杂数据分布下表现更优。


<details>
  <summary>Details</summary>
Motivation: 肥胖是一个全球急需关注的健康问题，与多种慢性疾病相关。机器学习在早期肥胖风险预测中显示出潜力，但对集成技术（特别是混合多数投票咈集成堆叠）的比较评估仍有限。

Method: 使用两个数据集评估三种集成模型：多数硬投票、权重硬投票咈堆叠（以多层感知机作为元分类器）。分析9种机器学习算法的50个超参数配置，选择前三名作为基学习器。预处理步骤包括数据集平衡咈异常值检测，以准确率咈F1分数评估模型性能。

Result: 在数据集1上，权重硬投票咈堆叠表现几乎相同（准确率0.920304，F1 0.920070），超过多数硬投票。在数据集2上，堆叠方法表现最优（准确率0.989837，F1 0.989825），多数硬投票次之（准确率0.981707，F1 0.981675），权重硬投票表现最差。

Conclusion: 集成堆叠方法在复杂数据分布下提供了更强的预测能力，而混合多数投票仍是一个健壮的备选方案，为健康预测模型选择提供了指导。

Abstract: Obesity is a critical global health issue driven by dietary, physiological,
and environmental factors, and is strongly associated with chronic diseases
such as diabetes, cardiovascular disorders, and cancer. Machine learning has
emerged as a promising approach for early obesity risk prediction, yet a
comparative evaluation of ensemble techniques -- particularly hybrid majority
voting and ensemble stacking -- remains limited. This study aims to compare
hybrid majority voting and ensemble stacking methods for obesity risk
prediction, identifying which approach delivers higher accuracy and efficiency.
The analysis seeks to highlight the complementary strengths of these ensemble
techniques in guiding better predictive model selection for healthcare
applications. Two datasets were utilized to evaluate three ensemble models:
Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer
Perceptron as meta-classifier). A pool of nine Machine Learning (ML)
algorithms, evaluated across a total of 50 hyperparameter configurations, was
analyzed to identify the top three models to serve as base learners for the
ensemble methods. Preprocessing steps involved dataset balancing, and outlier
detection, and model performance was evaluated using Accuracy and F1-Score. On
Dataset-1, weighted hard voting and stacking achieved nearly identical
performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard
voting. On Dataset-2, stacking demonstrated superior results (Accuracy:
0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,
F1: 0.981675) and weighted hard voting, which showed the lowest performance.
The findings confirm that ensemble stacking provides stronger predictive
capability, particularly for complex data distributions, while hybrid majority
voting remains a robust alternative.

</details>


### [21] [Conformal Prediction for Time-series Forecasting with Change Points](https://arxiv.org/abs/2509.02844)
*Sophia Sun,Rose Yu*

Main category: cs.LG

TL;DR: 提出CPTC算法，通过整合状态预测模型和在线保形预测来处理带变点的时间序列不确定性量化问题


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法难以处理具有变点（数据生成过程突然变化）的时间序列数据

Method: 结合状态预测模型和在线保形预测，处理非平稳时间序列中的不确定性

Result: 在6个合成和真实数据集上验证了CPTC的有效性，相比现有方法具有更好的有效性和适应性

Conclusion: CPTC算法在最小假设下证明了有效性，能够有效处理带变点的时间序列不确定性量化问题

Abstract: Conformal prediction has been explored as a general and efficient way to
provide uncertainty quantification for time series. However, current methods
struggle to handle time series data with change points - sudden shifts in the
underlying data-generating process. In this paper, we propose a novel Conformal
Prediction for Time-series with Change points (CPTC) algorithm, addressing this
gap by integrating a model to predict the underlying state with online
conformal prediction to model uncertainties in non-stationary time series. We
prove CPTC's validity and improved adaptivity in the time series setting under
minimum assumptions, and demonstrate CPTC's practical effectiveness on 6
synthetic and real-world datasets, showing improved validity and adaptivity
compared to state-of-the-art baselines.

</details>


### [22] [Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm](https://arxiv.org/abs/2509.02846)
*Siddharth Mansingh,James Amarel,Ragib Arnab,Arvind Mohan,Kamaljeet Singh,Gerd J. Kunde,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Nathan A. Debarledeben,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.LG

TL;DR: 提出了首个针对偏微分方程(PDE)的测试时计算策略，利用推理时的计算资源，通过奖励模型评估时空一致性，用更少训练数据和更小模型实现更准确预测


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型受限于预训练数据集，在自回归展开性能特别是分布外情况下表现不佳，且计算和训练数据需求大，限制了关键应用

Method: 基于大语言模型的"思考"策略，使用两种奖励模型评估随机基础模型的预测结果，检查时空一致性

Result: 在PDEGym基准的可压缩欧拉方程模拟中，测试时计算相比标准非自适应自回归推理获得了改进的预测效果

Conclusion: 该TTC框架是向更先进PDE建模推理算法的基础步骤，包括构建基于强化学习的方法，可能改变物理和工程领域的计算工作流程

Abstract: Partial Differential Equations (PDEs) are the bedrock for modern
computational sciences and engineering, and inherently computationally
expensive. While PDE foundation models have shown much promise for simulating
such complex spatio-temporal phenomena, existing models remain constrained by
the pretraining datasets and struggle with auto-regressive rollout performance,
especially in out-of-distribution (OOD) cases. Furthermore, they have
significant compute and training data requirements which hamper their use in
many critical applications. Inspired by recent advances in ``thinking"
strategies used in large language models (LLMs), we introduce the first
test-time computing (TTC) strategy for PDEs that utilizes computational
resources during inference to achieve more accurate predictions with fewer
training samples and smaller models. We accomplish this with two types of
reward models that evaluate predictions of a stochastic based model for
spatio-temporal consistency. We demonstrate this method on compressible
Euler-equation simulations from the PDEGym benchmark and show that TTC captures
improved predictions relative to standard non-adaptive auto-regressive
inference. This TTC framework marks a foundational step towards more advanced
reasoning algorithms or PDE modeling, inluding building
reinforcement-learning-based approaches, potentially transforming computational
workflows in physics and engineering.

</details>


### [23] [Power Grid Control with Graph-Based Distributed Reinforcement Learning](https://arxiv.org/abs/2509.02861)
*Carlo Fabrizio,Gianvito Losapio,Marco Mussi,Alberto Maria Metelli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出基于图神经网络的分布式强化学习框架，用于实时可扩展的电网管理，通过分层智能体架构和局部观测分解，在Grid2Op环境中表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 可再生能源集成和电网规模扩大对传统控制系统提出挑战，需要更动态分布式的控制策略来适应不断变化的电网环境。

Method: 采用分层分布式强化学习架构：底层智能体控制单个电力线路，高层管理器协调；使用GNN编码网络拓扑信息；结合模仿学习和基于势能的奖励塑形来加速收敛。

Result: 在Grid2Op仿真环境中，该方法 consistently outperforms 标准基线，且计算效率远高于基于仿真的专家方法。

Conclusion: 提出的图神经网络分布式强化学习框架为大规模电网实时控制提供了有效解决方案，在性能和计算效率方面均表现出优势。

Abstract: The necessary integration of renewable energy sources, combined with the
expanding scale of power networks, presents significant challenges in
controlling modern power grids. Traditional control systems, which are human
and optimization-based, struggle to adapt and to scale in such an evolving
context, motivating the exploration of more dynamic and distributed control
strategies. This work advances a graph-based distributed reinforcement learning
framework for real-time, scalable grid management. The proposed architecture
consists of a network of distributed low-level agents acting on individual
power lines and coordinated by a high-level manager agent. A Graph Neural
Network (GNN) is employed to encode the network's topological information
within the single low-level agent's observation. To accelerate convergence and
enhance learning stability, the framework integrates imitation learning and
potential-based reward shaping. In contrast to conventional decentralized
approaches that decompose only the action space while relying on global
observations, this method also decomposes the observation space. Each low-level
agent acts based on a structured and informative local view of the environment
constructed through the GNN. Experiments on the Grid2Op simulation environment
show the effectiveness of the approach, which consistently outperforms the
standard baseline commonly adopted in the field. Additionally, the proposed
model proves to be much more computationally efficient than the
simulation-based Expert method.

</details>


### [24] [Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)](https://arxiv.org/abs/2509.02863)
*Vikas Kashtriya,Pardeep Singh*

Main category: cs.LG

TL;DR: QI-SMOTE是一种基于量子原理的新型数据增强技术，用于解决医学机器学习中的类别不平衡问题，相比传统过采样方法能显著提升多种分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 医学机器学习中存在严重的类别不平衡问题，少数类样本不足导致模型偏差和预测性能下降，需要开发更有效的数据增强方法来改善模型泛化能力。

Method: 提出量子启发的SMOTE方法(QI-SMOTE)，利用量子演化和分层纠缠等量子原理生成合成样本，保持复杂数据结构，与传统过采样方法进行对比验证。

Result: 在MIMIC-III和MIMIC-IV数据集上的实验表明，QI-SMOTE显著提升了集成方法、核模型和深度学习方法的效果，在准确率、F1分数、G-Mean和AUC-ROC等指标上优于传统方法。

Conclusion: QI-SMOTE通过将量子启发变换集成到机器学习流程中，不仅缓解了类别不平衡问题，还增强了医学诊断和决策中预测模型的鲁棒性和可靠性，展示了量子启发重采样技术在推进最先进机器学习方法中的潜力。

Abstract: Class imbalance remains a critical challenge in machine learning (ML),
particularly in the medical domain, where underrepresented minority classes
lead to biased models and reduced predictive performance. This study introduces
Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that
enhances the performance of ML classifiers, including Random Forest (RF),
Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors
(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum
principles such as quantum evolution and layered entanglement. Unlike
conventional oversampling methods, QI-SMOTE generates synthetic instances that
preserve complex data structures, improving model generalization and
classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV
datasets, using mortality detection as a benchmark task due to their clinical
significance and inherent class imbalance. We compare our method against
traditional oversampling techniques, including Borderline-SMOTE, ADASYN,
SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as
Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE
significantly improves the effectiveness of ensemble methods (RF, GB, ADA),
kernel-based models (SVM), and deep learning approaches by producing more
informative and balanced training data. By integrating quantum-inspired
transformations into the ML pipeline, QI-SMOTE not only mitigates class
imbalance but also enhances the robustness and reliability of predictive models
in medical diagnostics and decision-making. This study highlights the potential
of quantum-inspired resampling techniques in advancing state-of-the-art ML
methodologies.

</details>


### [25] [Improving Generative Methods for Causal Evaluation via Simulation-Based Inference](https://arxiv.org/abs/2509.02892)
*Pracheta Amaranath,Vinitra Muralikrishnan,Amit Sharma,David D. Jensen*

Main category: cs.LG

TL;DR: SBICE是一个基于模拟推理的因果评估框架，通过将生成参数建模为不确定并推断其后验分布，生成与源数据分布更一致的真实合成数据集，提高因果估计器评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法需要用户提供参数的点估计而非分布，且不允许参考源数据改进估计，这限制了用户表达参数不确定性的能力，可能导致不可靠的估计器比较。

Method: 采用模拟推理技术，将生成参数建模为不确定，并基于源数据集推断参数的后验分布，识别能够产生与源数据分布紧密对齐的合成数据集的参数配置。

Result: 实证结果表明SBICE通过生成更真实的数据集，提高了估计器评估的可靠性，支持在不确定性下进行稳健和数据一致的因果基准测试。

Conclusion: SBICE框架为因果评估提供了一种更可靠的方法，通过考虑参数不确定性并利用模拟推理技术，能够生成更符合真实数据分布的合成数据集，从而改善因果估计器的比较和评估。

Abstract: Generating synthetic datasets that accurately reflect real-world
observational data is critical for evaluating causal estimators, but remains a
challenging task. Existing generative methods offer a solution by producing
synthetic datasets anchored in the observed data (source data) while allowing
variation in key parameters such as the treatment effect and amount of
confounding bias. However, existing methods typically require users to provide
point estimates of such parameters (rather than distributions) and fixed
estimates (rather than estimates that can be improved with reference to the
source data). This denies users the ability to express uncertainty over
parameter values and removes the potential for posterior inference, potentially
leading to unreliable estimator comparisons. We introduce simulation-based
inference for causal evaluation (SBICE), a framework that models generative
parameters as uncertain and infers their posterior distribution given a source
dataset. Leveraging techniques in simulation-based inference, SBICE identifies
parameter configurations that produce synthetic datasets closely aligned with
the source data distribution. Empirical results demonstrate that SBICE improves
the reliability of estimator evaluations by generating more realistic datasets,
which supports a robust and data-consistent approach to causal benchmarking
under uncertainty.

</details>


### [26] [A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers](https://arxiv.org/abs/2509.02923)
*Kunal Kumar,Muhammad Ashad Kabir,Luke Donnan,Sayed Ahmed*

Main category: cs.LG

TL;DR: 本文对45篇关于糖尿病足溃疡（DFU）减压鞋具处方决策的研究进行了系统性综述，分析了现有指南、知识系统和机器学习应用的局限性，并提出了一个五部分的临床决策支持系统（CDSS）框架。


<details>
  <summary>Details</summary>
Motivation: 目前DFU减压鞋具的处方决策存在碎片化问题：特征选择不一致、个性化有限、评估方法差异大。需要建立一个统一的框架来改善临床决策支持系统的可扩展性和患者中心性。

Method: 对截至2025年8月的45项研究进行叙述性综述，包括12个指南/协议、25个知识系统和8个机器学习应用。通过主题分析评估知识类型、决策逻辑、评估方法和使能技术。

Result: 发现指南强调足底压力阈值但缺乏可操作输出；知识系统使用规则和传感器驱动逻辑；ML模型计算精度高但可解释性和临床验证有限；评估方法碎片化严重。

Conclusion: 提出了一个五部分CDSS框架，包括最小可行数据集、混合架构、结构化特征输出、持续验证评估以及临床工作流集成，旨在实现可扩展、以患者为中心的DFU护理决策支持。

Abstract: Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by
lowering plantar pressure (PP), yet prescription decisions remain fragmented:
feature selection varies, personalization is limited, and evaluation practices
differ. We performed a narrative review of 45 studies (12 guidelines/protocols,
25 knowledge-based systems, 8 machine-learning applications) published to Aug
2025. We thematically analyzed knowledge type, decision logic, evaluation
methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200
kPa or >=25--30\% reduction) but rarely yield actionable, feature-level
outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating
PP monitoring, adherence tracking, and usability testing. ML work introduces
predictive, optimization, and generative models with high computational
accuracy but limited explainability and clinical validation. Evaluation remains
fragmented: protocols prioritize biomechanical tests; knowledge-based systems
assess usability/adherence; ML studies focus on technical accuracy with weak
linkage to long-term outcomes. From this synthesis we propose a five-part CDSS
framework: (1) a minimum viable dataset; (2) a hybrid architecture combining
rules, optimization, and explainable ML; (3) structured feature-level outputs;
(4) continuous validation and evaluation; and (5) integration with clinical and
telehealth workflows. This framework aims to enable scalable, patient-centered
CDSSs for DFU care; prioritizing interoperable datasets, explainable models,
and outcome-focused evaluation will be key to clinical adoption.

</details>


### [27] [PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials](https://arxiv.org/abs/2509.02927)
*Shih-Peng Huang,Nontawat Charoenphakdee,Yuta Tsuboi,Yong-Bin Zhuang,Wenwen Li*

Main category: cs.LG

TL;DR: 提出PDRL方法，一种基于描述符的后处理残差学习框架，用于机器学习原子间势能的不确定性量化，相比集成方法计算成本更低且不影响预测精度


<details>
  <summary>Details</summary>
Motivation: 集成方法虽然是不确定性量化的金标准，但计算成本高限制了其实用性。现有替代方法如蒙特卡洛dropout和深度核学习要么无法应用于已训练模型，要么可能影响预测精度

Method: 利用已训练图神经网络势能的描述符来估计残差误差，通过建模MLIP预测与真实值之间的差异，将残差作为预测不确定性的代理指标

Result: 开发了PDRL的多个变体，并与现有UQ方法进行了基准测试，评估了其有效性和局限性

Conclusion: PDRL提供了一个简单高效的后处理不确定性量化框架，能够在不影响模型性能的前提下降低计算成本

Abstract: Ensemble method is considered the gold standard for uncertainty
quantification (UQ) for machine learning interatomic potentials (MLIPs).
However, their high computational cost can limit its practicality. Alternative
techniques, such as Monte Carlo dropout and deep kernel learning, have been
proposed to improve computational efficiency; however, some of these methods
cannot be applied to already trained models and may affect the prediction
accuracy. In this paper, we propose a simple and efficient post-hoc framework
for UQ that leverages the descriptor of a trained graph neural network
potential to estimate residual errors. We refer to this method as post-hoc
descriptor-based residual-based learning (PDRL). PDRL models the discrepancy
between MLIP predictions and ground truth values, allowing these residuals to
act as proxies for prediction uncertainty. We explore multiple variants of PDRL
and benchmark them against established UQ methods, evaluating both their
effectiveness and limitations.

</details>


### [28] [VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills](https://arxiv.org/abs/2509.02930)
*Erik M. Lintunen*

Main category: cs.LG

TL;DR: 本文提出VendiRL框架，使用生态学中的Vendi Score度量技能多样性，解决了自监督RL中技能多样性评估和学习的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 自监督强化学习中，学习多样化技能以应对未知任务是一个关键挑战。现有方法在可扩展性和评估方面存在问题：高维特征空间中相关特征难以识别，且多样性定义通常需要硬性承诺特定概念，导致结果难以比较和多样性探索不足。

Method: 采用生态学中的Vendi Score作为样本多样性度量，允许用户指定和评估任何期望的多样性形式。提出VendiRL统一框架，通过不同的相似性函数来激励不同形式的多样性。

Result: VendiRL能够促进技能评估，并在新的丰富交互环境中支持各种多样性形式的技能多样性预训练。

Conclusion: Vendi Score提供了一个灵活的多样性度量框架，VendiRL能够学习多样化技能集，为自监督RL中的技能多样性问题提供了统一解决方案。

Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.

</details>


### [29] [AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting](https://arxiv.org/abs/2509.02967)
*Chen Zeng,Tiehang Xu,Qiao Wang*

Main category: cs.LG

TL;DR: 提出了AR-KAN混合模型，结合KAN网络和自回归组件，解决了传统神经网络在非周期信号频谱分析中的局限性，在72%的真实数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在频谱分析中存在局限，特别是对于非周期信号（如频率不可公度的准周期函数）的预测，传统ARIMA模型往往优于神经网络和大语言模型

Method: 提出Autoregressive-Weight-Enhanced AR-KAN混合模型，利用通用近视映射定理，使用KAN网络处理静态非线性部分，并通过预训练的自回归组件引入记忆机制，保留有用信息并消除冗余

Result: 实验数据显示AR-KAN在72%的真实世界数据集上提供了更优越的结果

Conclusion: AR-KAN模型成功解决了传统神经网络在非周期信号分析中的挑战，通过结合KAN网络和自回归方法的优势，在真实数据集上取得了显著改进

Abstract: Conventional neural networks frequently face challenges in spectral analysis
of signals. To address this challenge, Fourier neural networks (FNNs) and
similar approaches integrate components of Fourier series into the structure of
neural networks. Nonetheless, a significant hurdle is often overlooked: the
superposition of periodic signals does not necessarily result in a periodic
signal. For example, when forecasting almost periodic functions composed of
signals with incommensurate frequencies, traditional models such as
Autoregressive Integrated Moving Average (ARIMA) frequently outperform most
neural networks including large language models (LLMs). To tackle this goal, we
propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the
benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply
a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include
memory through a pre-trained AR component, which can be explained to retain the
most useful information while eliminating redundancy. Experimental data
indicates that AR-KAN delivers superior results on $72\%$ of real-world
datasets.

</details>


### [30] [Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation](https://arxiv.org/abs/2509.02970)
*Kaoru Otsuka,Yuki Takezawa,Makoto Yamada*

Main category: cs.LG

TL;DR: 提出了D-Byz-SGDM方法，通过延迟动量聚合机制解决联邦学习中部分参与场景下的拜占庭攻击问题，在保证隐私的同时提供强收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒联邦学习方法假设全客户端参与，但在实际通信约束和客户端可用性下，部分参与是常态。当采样客户端包含拜占庭多数时，现有方法会立即失效。

Method: 引入延迟动量聚合原则：服务器聚合来自非参与客户端的最新接收梯度，并结合活跃客户端的即时动量。开发D-Byz-SGDM优化器实现这一原则。

Result: 建立了收敛性保证，恢复了全参与结果并匹配部分参与设置的理论下界。深度学习实验验证了在各种拜占庭攻击下的稳定鲁棒训练。

Conclusion: D-Byz-SGDM方法有效解决了部分参与联邦学习中的拜占庭攻击问题，提供了理论保证和实际性能验证，为稀疏通信场景下的安全联邦学习提供了解决方案。

Abstract: Federated Learning (FL) allows distributed model training across multiple
clients while preserving data privacy, but it remains vulnerable to Byzantine
clients that exhibit malicious behavior. While existing Byzantine-robust FL
methods provide strong convergence guarantees (e.g., to a stationary point in
expectation) under Byzantine attacks, they typically assume full client
participation, which is unrealistic due to communication constraints and client
availability. Under partial participation, existing methods fail immediately
after the sampled clients contain a Byzantine majority, creating a fundamental
challenge for sparse communication. First, we introduce delayed momentum
aggregation, a novel principle where the server aggregates the most recently
received gradients from non-participating clients alongside fresh momentum from
active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with
Momentum) implements this delayed momentum aggregation principle for
Byzantine-robust FL with partial participation. Then, we establish convergence
guarantees that recover previous full participation results and match the
fundamental lower bounds we prove for the partial participation setting.
Experiments on deep learning tasks validated our theoretical findings, showing
stable and robust training under various Byzantine attacks.

</details>


### [31] [AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates](https://arxiv.org/abs/2509.02981)
*Minxin Zhang,Yuxuan Liu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: AdaGO结合了Muon优化器的正交化动量更新和AdaGrad的自适应学习率调整，通过累积梯度范数来缩放正交更新方向，在保持正交性的同时实现自适应步长调整。


<details>
  <summary>Details</summary>
Motivation: Muon优化器通过正交化动量更新在大型语言模型训练中表现出色，但缺乏自适应学习率机制；而AdaGrad虽然自适应但更新方向不正交。需要结合两者的优势。

Method: 在Muon的基础上增加一个标量变量来累积过去梯度的平方范数，用这个累积值来缩放正交更新方向，实现自适应步长调整同时保持更新方向的正交性。

Result: 理论分析证明了在非凸函数上的最优收敛速率，实验在CIFAR-10分类和函数回归任务上显示AdaGO优于Muon和Adam。

Conclusion: AdaGO成功结合了正交化更新和自适应学习率的优势，计算和内存效率高，在理论和实验上都表现出色。

Abstract: The recently proposed Muon optimizer updates weight matrices via
orthogonalized momentum and has demonstrated strong empirical success in large
language model training. However, it remains unclear how to determine the
learning rates for such orthogonalized updates. AdaGrad, by contrast, is a
widely used adaptive method that scales stochastic gradients by accumulated
past gradients. We propose a new algorithm, AdaGO, which combines a norm-based
AdaGrad-type stepsize with an orthogonalized update direction, bringing
together the benefits of both approaches. Unlike other adaptive variants of
Muon, AdaGO preserves the orthogonality of the update direction, which can be
interpreted as a spectral descent direction, while adapting the stepsizes to
the optimization landscape by scaling the direction with accumulated past
gradient norms. The implementation of AdaGO requires only minimal modification
to Muon, with a single additional scalar variable, the accumulated squared
gradient norms, to be computed, making it computationally and memory efficient.
Optimal theoretical convergence rates are established for nonconvex functions
in both stochastic and deterministic settings under standard smoothness and
unbiased bounded-variance noise assumptions. Empirical results on CIFAR-10
classification and function regression demonstrate that AdaGO outperforms Muon
and Adam.

</details>


### [32] [StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails](https://arxiv.org/abs/2509.02982)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 提出一种流式、无源测试时自适应方法，结合熵最小化、批归一化统计更新和安全机制，用于睡眠分期模型在未见患者数据上的适应性提升


<details>
  <summary>Details</summary>
Motivation: 睡眠分期模型在面对未见过的生理特征或记录条件时性能会下降，需要一种无需源数据或患者校准的实用自适应方法

Method: 流式无源测试时自适应(TTA)方法，结合熵最小化(Tent)、批归一化统计刷新，以及两个安全机制：熵门控(暂停不确定窗口的适应)和EMA重置(防止漂移)

Result: 在Sleep-EDF Expanded数据集上使用单导联EEG，相比冻结基线模型在秒级延迟和最小内存消耗下获得一致性能提升，报告了各阶段指标和Cohen's k

Conclusion: 该方法模型无关，无需源数据或患者校准，适用于设备端或床旁使用，具有实际应用价值

Abstract: Sleep staging models often degrade when deployed on patients with unseen
physiology or recording conditions. We propose a streaming, source-free
test-time adaptation (TTA) recipe that combines entropy minimization (Tent)
with Batch-Norm statistic refresh and two safety rails: an entropy gate to
pause adaptation on uncertain windows and an EMA-based reset to reel back
drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s
epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline
at seconds-level latency and minimal memory, reporting per-stage metrics and
Cohen's k. The method is model-agnostic, requires no source data or patient
calibration, and is practical for on-device or bedside use.

</details>


### [33] [Multimodal learning of melt pool dynamics in laser powder bed fusion](https://arxiv.org/abs/2509.03029)
*Satyajit Mojumder,Pallock Halder,Tiana Tonge*

Main category: cs.LG

TL;DR: 提出多模态数据融合方法，结合高成本X射线和低成本吸收率数据，通过CNN-RNN架构预测熔池动力学，实现仅用吸收率数据的高精度实时监测


<details>
  <summary>Details</summary>
Motivation: 现有监测方法存在局限性：X射线成像成本高不实用，光电二极管吸收率数据噪声大精度低，需要找到经济有效的实时监测方案

Method: 多模态学习框架，CNN提取X射线空间特征，RNN提取吸收率时序特征，采用早期融合策略，并用作迁移学习模型微调RNN

Result: 多模态训练显著提升预测精度，训练后模型仅需吸收率数据即可推断熔池特性，无需昂贵X射线成像

Conclusion: 该方法实现了经济高效的实时监测，在增材制造领域具有广泛适用性，解决了工业应用中成本与精度的平衡问题

Abstract: While multiple sensors are used for real-time monitoring in additive
manufacturing, not all provide practical or reliable process insights. For
example, high-speed X-ray imaging offers valuable spatial information about
subsurface melt pool behavior but is costly and impractical for most industrial
settings. In contrast, absorptivity data from low-cost photodiodes correlate
with melt pool dynamics but is often too noisy for accurate prediction when
used alone. In this paper, we propose a multimodal data fusion approach for
predicting melt pool dynamics by combining high-fidelity X-ray data with
low-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.
Our multimodal learning framework integrates convolutional neural networks
(CNNs) for spatial feature extraction from X-ray data with recurrent neural
networks (RNNs) for temporal feature extraction from absorptivity signals,
using an early fusion strategy. The multimodal model is further used as a
transfer learning model to fine-tune the RNN model that can predict melt pool
dynamics only with absorptivity, with greater accuracy compared to the
multimodal model. Results show that training with both modalities significantly
improves prediction accuracy compared to using either modality alone.
Furthermore, once trained, the model can infer melt pool characteristics using
only absorptivity data, eliminating the need for expensive X-ray imaging. This
multimodal fusion approach enables cost-effective, real-time monitoring and has
broad applicability in additive manufacturing.

</details>


### [34] [Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning](https://arxiv.org/abs/2509.03030)
*Zida Wu,Mathieu Lauriere,Matthieu Geist,Olivier Pietquin,Ankur Mehta*

Main category: cs.LG

TL;DR: 提出一种高效的深度强化学习算法，用于在均值场游戏中寻找纳什均衡，无需依赖历史采样或平均，适用于未知初始分布和共同噪声场景。


<details>
  <summary>Details</summary>
Motivation: 均值场游戏是研究大规模多智能体系统的强大框架，但在未知初始分布或存在共同噪声的情况下，学习纳什均衡仍然具有挑战性。现有方法依赖平均或历史采样，限制了算法的适应性和鲁棒性。

Method: 基于Munchausen RL和在线镜像下降的深度强化学习算法，直接学习种群依赖的纳什均衡策略，不依赖平均或历史采样。

Result: 在七个经典示例上的数值实验表明，该算法相比最先进的算法（特别是基于虚拟游戏的DRL方法）具有更优越的收敛性能，在共同噪声存在时表现出良好的鲁棒性和适应性。

Conclusion: 该算法为均值场游戏中的纳什均衡学习提供了一种高效、适应性强的解决方案，特别适用于存在共同噪声和未知初始分布的复杂场景。

Abstract: Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.

</details>


### [35] [Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models](https://arxiv.org/abs/2509.03036)
*Bilge Taskin,Wenxiong Xie,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 利用预训练大语言模型自动化牢物理知识融入符号回归，提升方程发现的准确性和健壃性


<details>
  <summary>Details</summary>
Motivation: 解决传统牢物理知识符号回归方法需要专业知识和手工特征工程的问题，让预训练的LLM自动融入领域知识

Method: 将LLM集成到SR损失函数中，添加LLM对生成方程的评估项，使用三种SR算法和三种LLM模型在三种牢物理动力学问题上进行验证

Result: LLM集成一致提升了从数据重建牢物理动力学的性能，增强了SR模型对噪声和复杂性的健壃性，更信息丰富的提示语显著提高性能

Conclusion: 预训练LLM可以有效地自动融入领域知识，使牢物理知识符号回归更加可访和通用，为自动化科学发玱提供了新的解决方案

Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated
scientific discovery, enabling the derivation of governing equations from
experimental data. A growing body of work illustrates the promise of
integrating domain knowledge into the SR to improve the discovered equation's
generality and usefulness. Physics-informed SR (PiSR) addresses this by
incorporating domain knowledge, but current methods often require specialized
formulations and manual feature engineering, limiting their adaptability only
to domain experts. In this study, we leverage pre-trained Large Language Models
(LLMs) to facilitate knowledge integration in PiSR. By harnessing the
contextual understanding of LLMs trained on vast scientific literature, we aim
to automate the incorporation of domain knowledge, reducing the need for manual
intervention and making the process more accessible to a broader range of
scientific problems. Namely, the LLM is integrated into the SR's loss function,
adding a term of the LLM's evaluation of the SR's produced equation. We
extensively evaluate our method using three SR algorithms (DEAP, gplearn, and
PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three
physical dynamics (dropping ball, simple harmonic motion, and electromagnetic
wave). The results demonstrate that LLM integration consistently improves the
reconstruction of physical dynamics from data, enhancing the robustness of SR
models to noise and complexity. We further explore the impact of prompt
engineering, finding that more informative prompts significantly improve
performance.

</details>


### [36] [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)
*Xinzhe Zheng,Zhen-Qun Yang,Haoran Xie,S. Joe Qin,Arlene Chen,Fangzhen Lin*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的二进制量化方法，通过动态选择最优子矩阵和适应性分组策略，在平均1.007比特的压缩比下保持了高模型质量，性能跟4比特量化方法相当，而压缩效率极高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要大量内存和计算资源，传统二进制量化虽然能大幅减少存储和推理成本，但导致显著的性能下降。需要找到一种方法在极端压缩的同时保持模型质量。

Method: 提出了一种专门为二进制量化设计的新题优化目标，以及三种实现该目标的算法。通过动态识别最优的非结构化子矩阵，采用适应性分组策略来改进块量化。

Result: 平均比特长度仅1.007比特，LLaMA 3.2 3B模型语言混淆度从原始的7.81降至8.23，显著超越之前的BiLLM（语言混淆度123.90），与GPTQ等SOTA 4比特方法相当。压缩效率极高，单CPU核仅需14秒完成全部量化，整个过程不超100分钟。

Conclusion: 该方法在极端的二进制量化下实现了超越以往方法的性能，通过动态选择最优子矩阵和适应性分组，在平均1.007比特的压缩比下保持了高模型质量，且压缩效率极高，具有良好的并行性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of Natural Language Processing (NLP) tasks, but require
substantial memory and computational resources. Binary quantization, which
compresses model weights from 16-bit Brain Float to 1-bit representations in
{-1, 1}, offers significant reductions in storage and inference costs. However,
such aggressive quantization often leads to notable performance degradation
compared to more conservative 4-bit quantization methods. In this research, we
propose a novel optimization objective tailored for binary quantization, along
with three algorithms designed to realize it effectively. Our method enhances
blocked quantization by dynamically identifying optimal unstructured
sub-matrices through adaptive grouping strategies. Experimental results
demonstrate that our approach achieves an average bit length of just 1.007
bits, while maintaining high model quality. Specifically, our quantized LLaMA
3.2 3B model attains a perplexity of 8.23, remarkably close to the original
7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.
Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ
in both performance and efficiency. The compression process is highly
efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights
on a single CPU core, with the entire process completing in under 100 minutes
and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan

</details>


### [37] [Discrete Functional Geometry of ReLU Networks via ReLU Transition Graphs](https://arxiv.org/abs/2509.03056)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 通过扩展ReLU过渡图框架，提出了一种图论模型来理解深度ReLU网络，证明了随机初始化时的强扩展性质和正分布度分布，并通过谜园定理给出了新的容量和泛化界。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地理解深度ReLU网络的功能行为，通过构建离散几何结构来分析网络的线性激活区域和ReLU激活翻转关系。

Method: 扩展ReLU过渡图(RTG)框架，将每个节点表示一个线性激活区域，边连接相差仅一个ReLU激活翻转的区域。证明随机初始化时RTG的扩展性质和度分布，通过谜园定理推导容量和泛化界。

Result: 区域盛在过参数化时饱和，谜园间隔与泛化相关，相邻区域间的KL散度反映功能平滑性。实验验证了理论预测，构建了小网络的RTG并测量其平滑性和连通性。

Conclusion: 该工作提供了通过离散功能几何视角分析ReLU网络的统一框架，为理解、诊断和改善泛化提供了新工具。

Abstract: We extend the ReLU Transition Graph (RTG) framework into a comprehensive
graph-theoretic model for understanding deep ReLU networks. In this model, each
node represents a linear activation region, and edges connect regions that
differ by a single ReLU activation flip, forming a discrete geometric structure
over the network's functional behavior. We prove that RTGs at random
initialization exhibit strong expansion, binomial degree distributions, and
spectral properties that tightly govern generalization. These structural
insights enable new bounds on capacity via region entropy and on generalization
via spectral gap and edge-wise KL divergence. Empirically, we construct RTGs
for small networks, measure their smoothness and connectivity properties, and
validate theoretical predictions. Our results show that region entropy
saturates under overparameterization, spectral gap correlates with
generalization, and KL divergence across adjacent regions reflects functional
smoothness. This work provides a unified framework for analyzing ReLU networks
through the lens of discrete functional geometry, offering new tools to
understand, diagnose, and improve generalization.

</details>


### [38] [Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers](https://arxiv.org/abs/2509.03059)
*Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li*

Main category: cs.LG

TL;DR: Loong项目是一个开源框架，通过合成数据生成和验证来解决推理密集型领域的数据稀缺问题，包含LoongBench基准数据集和LoongEnv合成数据生成环境。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学和编程等可验证领域的推理能力通过RLVR得到显著提升，但其他推理密集型领域因缺乏高质量可验证数据集和人工监督成本高而难以取得类似成功。

Method: 框架包含两个核心组件：1) LoongBench - 包含8,729个人工审核样本的种子数据集，覆盖12个领域；2) LoongEnv - 模块化合成数据生成环境，支持多种提示策略生成新的问题-答案-代码三元组，形成智能体-环境循环支持强化学习。

Result: 对开源和专有LLM进行了广泛基准测试，评估领域覆盖范围并揭示性能瓶颈；对合成数据的正确性、难度和多样性进行了全面分析。

Conclusion: Loong项目提供了一个可扩展的合成数据生成和验证框架，能够有效支持LLM在多种推理密集型领域的强化学习训练，解决了数据稀缺和验证成本高的问题。

Abstract: Recent advances in Large Language Models (LLMs) have shown that their
reasoning capabilities can be significantly improved through Reinforcement
Learning with Verifiable Reward (RLVR), particularly in domains like
mathematics and programming, where ground-truth correctness can be
automatically evaluated. However, extending this success to other
reasoning-intensive domains remains challenging due to the scarcity of
high-quality, verifiable datasets and the high cost of human supervision. In
this work, we introduce the Loong Project: an open-source framework for
scalable synthetic data generation and verification across a diverse range of
reasoning-intensive domains. The framework consists of two key components: (1)
LoongBench, a curated seed dataset containing 8,729 human-vetted examples
across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired
with executable code and rich metadata; and (2) LoongEnv, a modular synthetic
data generation environment that supports multiple prompting strategies to
produce new question-answer-code triples. Together, these components form an
agent-environment loop that enables reinforcement learning, where an LLM-based
agent is rewarded for generating Chain-of-Thought (CoT) solutions that align
with code-executed answers. Empirically, we benchmark LoongBench on a broad
suite of both open-source and proprietary LLMs to evaluate domain coverage and
reveal performance bottlenecks. In addition, we conduct a comprehensive
analysis of synthetic data generated by LoongEnv, examining correctness,
difficulty, and diversity. Code and documentation are available at
https://github.com/camel-ai/loong.

</details>


### [39] [LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization](https://arxiv.org/abs/2509.03110)
*Yunfei Teng,Sixin Zhang*

Main category: cs.LG

TL;DR: LSAM是一种新颖的优化器，在保持SAM泛化优势的同时提供更高的分布式大批次训练效率


<details>
  <summary>Details</summary>
Motivation: 虽然SAM通过最小化损失和锐度来改善深度神经网络的泛化能力，但在分布式大批次训练中存在效率低下的问题

Method: LSAM将SAM的对抗性步骤与异步分布式采样策略相结合，生成异步分布式采样方案，产生平滑的锐度感知损失景观进行优化

Result: 该设计消除了同步瓶颈，加速了大批次收敛，相比数据并行SAM提供了更高的最终准确率

Conclusion: LSAM在保持SAM泛化优势的同时，显著提高了分布式大批次训练的效率

Abstract: While Sharpness-Aware Minimization (SAM) improves generalization in deep
neural networks by minimizing both loss and sharpness, it suffers from
inefficiency in distributed large-batch training. We present Landscape-Smoothed
SAM (LSAM), a novel optimizer that preserves SAM's generalization advantages
while offering superior efficiency. LSAM integrates SAM's adversarial steps
with an asynchronous distributed sampling strategy, generating an asynchronous
distributed sampling scheme, producing a smoothed sharpness-aware loss
landscape for optimization. This design eliminates synchronization bottlenecks,
accelerates large-batch convergence, and delivers higher final accuracy
compared to data-parallel SAM.

</details>


### [40] [A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning](https://arxiv.org/abs/2509.03118)
*Hankang Gu,Yuli Zhang,Chengming Wang,Ruiyuan Jiang,Ziheng Qiao,Pengfei Fan,Dongyao Jia*

Main category: cs.LG

TL;DR: 提出了一种名为DHCP的深度强化学习模型，通过分层分配交通信号周期时长来解决传统相位选择策略的安全性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的DRL交通信号控制方法存在两种主要范式："选择相位"策略可能导致不可预测的相位序列影响安全性，"切换"策略则可能导致不公平和低效的相位分配。需要一种既能保持可预测性又能灵活分配时间的解决方案。

Method: 采用分层控制架构：高层代理根据整体交通状态确定南北方向和东西方向的总周期时间分配，低层代理进一步在每个主要方向内为直行和左转运动分配更灵活的持续时间。

Result: 在真实和合成的道路网络以及多种交通流数据集上进行测试，结果显示该模型在所有数据集上都优于基线方法，取得了最佳性能。

Conclusion: DHCP模型通过分层周期规划方法，在保持相位序列可预测性的同时实现了更灵活和高效的交通信号控制，有效解决了传统方法的局限性。

Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic
signal control (TSC) due to its ability to learn adaptive policies from complex
traffic environments. Within DRL-based TSC methods, two primary control
paradigms are ``choose phase" and ``switch" strategies. Although the agent in
the choose phase paradigm selects the next active phase adaptively, this
paradigm may result in unexpected phase sequences for drivers, disrupting their
anticipation and potentially compromising safety at intersections. Meanwhile,
the switch paradigm allows the agent to decide whether to switch to the next
predefined phase or extend the current phase. While this structure maintains a
more predictable order, it can lead to unfair and inefficient phase
allocations, as certain movements may be extended disproportionately while
others are neglected. In this paper, we propose a DRL model, named Deep
Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle
duration hierarchically. A high-level agent first determines the split of the
total cycle time between the North-South (NS) and East-West (EW) directions
based on the overall traffic state. Then, a low-level agent further divides the
allocated duration within each major direction between straight and left-turn
movements, enabling more flexible durations for the two movements. We test our
model on both real and synthetic road networks, along with multiple sets of
real and synthetic traffic flows. Empirical results show our model achieves the
best performance over all datasets against baselines.

</details>


### [41] [A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy](https://arxiv.org/abs/2509.03137)
*Li Yi,Qian Yang*

Main category: cs.LG

TL;DR: 使用人工智能框架结合数值谱模拟和深度学习，实现了无标准的自动化多核素TDCR谱分析方法


<details>
  <summary>Details</summary>
Motivation: 解决传统TDCR多核素分析中自动化程度低、依赖混合物标准的问题，特别是当标准源不易获得时

Method: 使用Geant4模拟生成β谱，结合统计检测器响应采样，设计特定神经网络结构进行训练，覆盖不同核素混合比和渒氧场景

Result: 在活度比例(均绝对误差=0.009)、检测效率(均绝对误差=0.002)和谱图重建(结构相似性指数=0.9998)任务中均获得高精度

Conclusion: 该AI驱动方法具有强大的自动化潜力、健壮的普适性、实时处理能力和工程可行性，特别适用于无标准物质或需要快速现场分析的场景

Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is
widely adopted as a standard method for radionuclide quantification because of
its inherent advantages such as high precision, self-calibrating capability,
and independence from radioactive reference sources. However, multiradionuclide
analysis via TDCR faces the challenges of limited automation and reliance on
mixture-specific standards, which may not be easily available. Here, we present
an Artificial Intelligence (AI) framework that combines numerical spectral
simulation and deep learning for standard-free automated analysis. $\beta$
spectra for model training were generated using Geant4 simulations coupled with
statistically modeled detector response sampling. A tailored neural network
architecture, trained on this dataset covering various nuclei mix ratio and
quenching scenarios, enables autonomous resolution of individual radionuclide
activities and detecting efficiency through end-to-end learning paradigms. The
model delivers consistent high accuracy across tasks: activity proportions
(mean absolute error = 0.009), detection efficiencies (mean absolute error =
0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),
validating its physical plausibility for quenched $\beta$ spectroscopy. This
AI-driven methodology exhibits significant potential for automated
safety-compliant multiradionuclide analysis with robust generalization,
real-time processing capabilities, and engineering feasibility, particularly in
scenarios where reference materials are unavailable or rapid field analysis is
required.

</details>


### [42] [Rashomon in the Streets: Explanation Ambiguity in Scene Understanding](https://arxiv.org/abs/2509.03169)
*Helge Spieker,Jørn Eirik Betten,Arnaud Gotlieb,Nadjib Lazaar,Nassim Belmecheri*

Main category: cs.LG

TL;DR: 本文首次实证量化了自动驾驶场景中动作预测任务的Rashomon效应，发现不同模型对同一预测提供显著不同的解释，说明解释歧义是问题的固有属性而非建模伪影。


<details>
  <summary>Details</summary>
Motivation: 可解释AI在安全关键应用中至关重要，但Rashomon效应（多个同等准确模型对同一预测提供不同解释）挑战了XAI的可靠性，需要实证研究其在真实驾驶场景中的影响。

Method: 使用定性可解释图(QXGs)作为符号场景表示，训练两类Rashomon集合模型：可解释的基于对的梯度提升模型和复杂的基于图的图神经网络(GNNs)，通过特征归因方法测量模型内部和模型间的解释一致性。

Result: 结果显示存在显著的解释分歧，不同模型对同一预测提供的解释差异很大。

Conclusion: 解释歧义是动作预测问题的固有属性，而不仅仅是建模方法的产物，这对可解释AI在安全关键应用中的可靠性提出了重要挑战。

Abstract: Explainable AI (XAI) is essential for validating and trusting models in
safety-critical applications like autonomous driving. However, the reliability
of XAI is challenged by the Rashomon effect, where multiple, equally accurate
models can offer divergent explanations for the same prediction. This paper
provides the first empirical quantification of this effect for the task of
action prediction in real-world driving scenes. Using Qualitative Explainable
Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two
distinct model classes: interpretable, pair-based gradient boosting models and
complex, graph-based Graph Neural Networks (GNNs). Using feature attribution
methods, we measure the agreement of explanations both within and between these
classes. Our results reveal significant explanation disagreement. Our findings
suggest that explanation ambiguity is an inherent property of the problem, not
just a modeling artifact.

</details>


### [43] [Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns](https://arxiv.org/abs/2509.03176)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 这篇论文提出了一种免阈值的评价框架，解决了现有归因方法评估中的阈值选择偏差问题，通过AUC-IoU指标在医学图像领域实现了更稳健的方法比较。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法评估存在阈值选择偏差，单一阈值的选择可能导致方法排名反转（超过200%变化），影响研究结论的可靠性。

Method: 提出免阈值框架，通过计算交并比面积曲线下面积（AUC-IoU）来评估归因质量，考察全阈值范围内的表现。在皮肤病学图像数据上对七种归因方法进行评测。

Result: 结果显示单一阈值指标产生矛盾结果，而免阈值评估能够提供可靠的方法区分。XRAI方法比LIME提高31%，比原生积分梯度提高204%。根据症状大小分层分析显示表现差异达269%。

Conclusion: 该研究建立了消除评估偏差的方法论标准，为实证基础的方法选择提供指导。免阈值框架既提供了归因行为的理论见解，也为医学图像及其他领域的稳健比较提供了实践指南。

Abstract: Attribution methods explain neural network predictions by identifying
influential input features, but their evaluation suffers from threshold
selection bias that can reverse method rankings and undermine conclusions.
Current protocols binarize attribution maps at single thresholds, where
threshold choice alone can alter rankings by over 200 percentage points. We
address this flaw with a threshold-free framework that computes Area Under the
Curve for Intersection over Union (AUC-IoU), capturing attribution quality
across the full threshold spectrum. Evaluating seven attribution methods on
dermatological imaging, we show single-threshold metrics yield contradictory
results, while threshold-free evaluation provides reliable differentiation.
XRAI achieves 31% improvement over LIME and 204% over vanilla Integrated
Gradients, with size-stratified analysis revealing performance variations up to
269% across lesion scales. These findings establish methodological standards
that eliminate evaluation artifacts and enable evidence-based method selection.
The threshold-free framework provides both theoretical insight into attribution
behavior and practical guidance for robust comparison in medical imaging and
beyond.

</details>


### [44] [Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025](https://arxiv.org/abs/2509.03191)
*Taiga Saito,Yu Otake,Stephen Wu*

Main category: cs.LG

TL;DR: 本文应用TabPFN基础模型于地质工程场地特征化问题，在零训练几样学习设置下较传统汇减贝叶斯模型实现更高准确性和更快的推理速度，标志着表格数据基础模型在地质工程预测中的首次成功应用。


<details>
  <summary>Details</summary>
Motivation: 探索TabPFN这种基于Transformer的表格数据基础模型在地质工程场地特征化问题中的应用潜力，解决传统模型在准确性、效率和不确定性量化方面的挑战。

Method: 采用TabPFN模型进行零训练、几样学习的上下文学习方式，无需调优超参数，并给模型提供来自大垂直间接数据库的额外上下文信息。对比传统分层贝叶斯模型基线。

Result: 在空间剩余剪切强度预测任务中，TabPFN准确性更高且运行时间缩短一个数量级；在缺失机械参数插值任务中，所有目标参数的RMSE更低且不确定性量化良好，但累计计算成本更高。

Conclusion: TabPFN作为通用基础模型，在地质工程预测中实现了更高准确性、良好的预测分布检验和显著的推理效率提升，标志着表格数据基础模型在地质工程预测中的首次成功应用，有望带来概率性场地特征化方法的范式转变。

Abstract: This paper presents a novel application of the Tabular Prior-Data Fitted
Network (TabPFN) - a transformer-based foundation model for tabular data - to
geotechnical site characterization problems defined in the GEOAI benchmark
BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the
spatial variation of undrained shear strength (su) across borehole depth
profiles, and (2) imputing missing mechanical parameters in a dense-site
dataset. We apply TabPFN in a zero-training, few-shot, in-context learning
setting - without hyper-parameter tuning - and provide it with additional
context from the big indirect database (BID). The study demonstrates that
TabPFN, as a general-purpose foundation model, achieved superior accuracy and
well-calibrated predictive distributions compared to a conventional
hierarchical Bayesian model (HBM) baseline, while also offering significant
gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),
TabPFN outperformed the HBM in prediction accuracy and delivered an
order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical
parameter imputation), TabPFN likewise achieved lower RMSE for all target
parameters with well-quantified uncertainties, though its cumulative
computation cost was higher than HBM's due to its one-variable-at-a-time
inference. These results mark the first successful use of a tabular foundation
model in geotechnical modeling, suggesting a potential paradigm shift in
probabilistic site characterization.

</details>


### [45] [Exploring the Design Space of Fair Tree Learning Algorithms](https://arxiv.org/abs/2509.03204)
*Kiara Stempel,Mattia Cerrato,Stefan Kramer*

Main category: cs.LG

TL;DR: 本文探讨决策树公平性的三种设计方案，重点研究了之前异常被忽视的第二种约束方案和第三种双树方案。


<details>
  <summary>Details</summary>
Motivation: 虽然决策树在公平性领域已有广泛研究，但现有技术主要集中在第一种目标函数方案和第二种的贪心变体。对于整个设计空间中的其他可能性异常被忽视。

Method: 提出了两种新的设计方案：(1)使用目标函数优化y和约束s的非贪心方法；(2)构建独立的双树模型Ty和Ts，让y和s的信息使用完全解耦。在多个数据集上进行实验性评估。

Result: 实验结果展示了新提出的两种设计方案在决策树公平性问题上的效果，为该领域提供了更多的技术选择。

Conclusion: 本文扩展了决策树公平性的设计空间，证明了之前异常被忽视的设计方案的可行性和效果，为该领域的研究提供了新的视角。

Abstract: Decision trees have been studied extensively in the context of fairness,
aiming to maximize prediction performance while ensuring non-discrimination
against different groups. Techniques in this space usually focus on imposing
constraints at training time, constraining the search space so that solutions
which display unacceptable values of relevant metrics are not considered,
discarded, or discouraged. If we assume one target variable y and one sensitive
attribute s, the design space of tree learning algorithms can be spanned as
follows: (i) One can have one tree T that is built using an objective function
that is a function of y, s, and T. For instance, one can build a tree based on
the weighted information gain regarding y (maximizing) and s (minimizing). (ii)
The second option is to have one tree model T that uses an objective function
in y and T and a constraint on s and T. Here, s is no longer part of the
objective, but part of a constraint. This can be achieved greedily by aborting
a further split as soon as the condition that optimizes the objective in y
fails to satisfy the constraint on s. A simple way to explore other splits is
to backtrack during tree construction once a fairness constraint is violated.
(iii) The third option is to have two trees T_y and T_s, one for y and one for
s, such that the tree structure for y and s does not have to be shared. In this
way, information regarding y and regarding s can be used independently, without
having to constrain the choices in tree construction by the mutual information
between the two variables. Quite surprisingly, of the three options, only the
first one and the greedy variant of the second have been studied in the
literature so far. In this paper, we introduce the above two additional options
from that design space and characterize them experimentally on multiple
datasets.

</details>


### [46] [Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback](https://arxiv.org/abs/2509.03206)
*Zeqiang Zhang,Fabian Wurzberger,Gerrit Schmid,Sebastian Gottwald,Daniel A. Braun*

Main category: cs.LG

TL;DR: 提出了一种将对比学习整合到GCSL框架中的新模型，从成功和失败中学习，克服智能体初始偏见限制，实现更好的探索行为和性能表现


<details>
  <summary>Details</summary>
Motivation: 解决GCSL框架的两个局限性：仅从自身经验学习会加剧智能体偏见；重标注策略只关注成功结果，无法从错误中学习

Method: 将对比学习原理整合到GCSL框架中，使智能体能够同时从成功和失败的经验中学习

Result: 算法克服了智能体初始偏见的限制，促进了更多的探索行为，有助于识别和采用有效策略，在各种挑战性环境中表现出优越性能

Conclusion: 通过整合对比学习，新模型有效解决了GCSL的局限性，实现了从成功和失败中同时学习的能力，提升了强化学习在稀疏奖励任务中的表现

Abstract: Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

</details>


### [47] [TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2509.03234)
*Yuxuan Gu,Wuyang Zhou,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: TeRA是一种新颖的参数高效微调方法，通过张量网络实现高秩权重更新，同时保持向量化方法的参数效率


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA风格适配器在高秩表达性和参数效率之间的权衡问题，既要实现高秩权重更新的表达能力，又要保持向量化方法的参数效率

Method: 使用类Tucker张量网络参数化权重更新矩阵，冻结大型随机初始化因子并在层间共享，仅训练由对角因子矩阵条目组成的小型层特定缩放向量

Result: TeRA匹配甚至超越高秩适配器的性能，同时所需可训练参数数量与向量化方法相当

Conclusion: TeRA成功实现了高秩权重更新与参数效率的平衡，理论分析和消融研究验证了方法的有效性

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have significantly reduced the number of trainable parameters needed in
fine-tuning large language models (LLMs). Subsequent developments of LoRA-style
adapters have diverged into two main directions: (1) enhancing model
expressivity with high-rank adapters, and (2) pushing for further parameter
reduction, as exemplified by vector-based methods. However, these approaches
present a trade-off, as achieving the expressivity of high-rank weight updates
typically comes at the cost of sacrificing the extreme parameter efficiency
offered by vector-based techniques. To address this issue, we propose a
vector-based random \underline{\textbf{Te}}nsor network for
high-\underline{\textbf{R}}ank \underline{\textbf{A}}daptation (TeRA), a novel
PEFT method that achieves high-rank weight updates while retaining the
parameter efficiency of vector-based PEFT adapters. This is achieved by
parameterizing the tensorized weight update matrix as a Tucker-like tensor
network (TN), in which large randomly initialized factors are frozen and shared
across layers, while only small layer-specific scaling vectors, formed by
entries in diagonal factor matrices, are trained. This design effectively
decouples the rank of the weight update matrix from the number of trainable
parameters. Comprehensive experiments demonstrate that TeRA matches or even
outperforms high-rank adapters, while requiring a trainable parameter count
similar to vector-based methods. Theoretical analysis and ablation studies
further validate the effectiveness of our approach.

</details>


### [48] [Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric](https://arxiv.org/abs/2509.03240)
*Harald Vilhelm Skat-Rørdam,Sneha Das,Kathrine Sofie Rasmussen,Nicole Nadine Lønfeldt,Line Clemmensen*

Main category: cs.LG

TL;DR: 时间序列事件检测评估存在问题，标准F1指标在实际应用中容易误导。研究提出窗口基于F1指标(F1$_w$)，给予时间宽容度，能更准确评估渐进式事件检测性能。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备压力监测等应用中，真实标签通常是单点事件，但实际现象是渐进和时间散射的。标准评估指标在这种实际数据集中容易导致模型性能评估偏差。

Method: 提出窗口基于F1指标(F1$_w$)，包含时间宽容度，允许在不完美对齐的情况下进行更健壮的事件检测评估。在三个生理数据集中进行实验分析。

Result: 窗口基于F1指标能够显示传统指标无法发现的有意义模型性能模式。在两个野外应用场景中，只有时间宽容指标能够显示TimesFM预测在统计上显著优于随机和空基准的收益。

Conclusion: 评估指标的选择强烈影响对模型性能的解释。这项工作补充了时间序列评估的关键空白，为健康养护应用提供了实用指南，因为不同情况下对时间精度的要求不同。

Abstract: Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.

</details>


### [49] [Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network](https://arxiv.org/abs/2509.03241)
*Pujitha Mamillapalli,Yoghitha Ramamoorthi,Abhinav Kumar,Tomoki Murakami,Tomoaki Ogawa,Yasushi Takatori*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于神经网络的方法，通过预处理技术降低输入维度，优化RIS元素分配和资源分配，在降低计算复杂度的同时提高系统吞吐量6.8%


<details>
  <summary>Details</summary>
Motivation: 解决传统迭代优化方法在RIS元素数量增加时计算复杂度指数增长的问题，以及监督学习中训练标签生成的复杂性

Method: 使用五层全连接神经网络(FNN)结合预处理技术，显著降低输入维度，降低计算复杂度，提高可扩展性

Result: 计算开销降低，系统吞吐量提高6.8%，较现有RIS元素分配方案有显著改善

Conclusion: 提出的NN基于方案在降低计算复杂度的同时获得更好性能，显著提高了可扩展性

Abstract: The increasing demand for high data rates and seamless connectivity in
wireless systems has sparked significant interest in reconfigurable intelligent
surfaces (RIS) and artificial intelligence-based wireless applications. RIS
typically comprises passive reflective antenna elements that control the
wireless propagation environment by adequately tuning the phase of the
reflective elements. The allocation of RIS elements to multipleuser equipment
(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a
joint optimization problem that optimizes the RIS phase configuration and
resource allocation under an $\alpha$-fair scheduling framework and propose an
efficient way of allocating RIS elements. Conventional iterative optimization
methods, however, suffer from exponentially increasing computational complexity
as the number of RIS elements increases and also complicate the generation of
training labels for supervised learning. To overcome these challenges, we
propose a five-layer fully connected neural network (FNN) combined with a
preprocessing technique to significantly reduce input dimensionality, lower
computational complexity, and enhance scalability. The simulation results show
that our proposed NN-based solution reduces computational overhead while
significantly improving system throughput by 6.8% compared to existing RIS
element allocation schemes. Furthermore, the proposed system achieves better
performance while reducing computational complexity, making it significantly
more scalable than the iterative optimization algorithms.

</details>


### [50] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: TopoMap是一种黑盒、模型无关的方法，通过降维和聚类创建输入特征空间的拓扑地图，自动选择最优配置来识别DL模型的可区分故障特征区域。


<details>
  <summary>Details</summary>
Motivation: 现有DL测试方法可能只关注特定故障特征而忽略其他特征区域，需要一种系统方法来分组识别导致DL模型失败的输入特征。

Method: 使用降维获得输入嵌入，然后进行聚类，通过DNN近似人类评估者自动选择最优的嵌入/聚类配置来生成拓扑地图。

Result: TopoMap生成的地图包含可区分且有意义的区域，在变异分析中比随机选择平均高出35%（可杀死变异体）和61%（不可杀死变异体）。

Conclusion: TopoMap能有效识别DL模型输入特征空间中的故障区域，为DL测试提供了系统化的特征分组方法。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [51] [FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization](https://arxiv.org/abs/2509.03244)
*Yiming Yao,Fei Liu,Liang Zhao,Xi Lin,Qingfu Zhang*

Main category: cs.LG

TL;DR: FoMEMO是一种基于基础模型的昂贵多目标优化新范式，通过海量合成数据预训练，无需针对新问题重新训练模型，实现了优秀的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要为每个新问题重新构建高斯过程代理模型或依赖大量领域实验数据进行预训练的问题，提高多目标优化的泛化能力和实用性。

Method: 提出FoMEMO框架，使用数百亿合成数据预训练基础模型，基于领域轨迹和用户偏好建立条件化基础模型，通过预测的偏好聚合后验进行快速上下文优化。

Result: 在多种合成基准和实际应用中表现出优异的泛化能力和竞争力，相比现有方法具有更好的适应性。

Conclusion: FoMEMO通过合成数据预训练的基础模型范式，为昂贵多目标优化问题提供了高效、通用的解决方案，无需后续模型更新即可适应未知问题。

Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in
many real-world scenarios, where sample-efficiency is vital due to the limited
evaluations to recover the true Pareto front for decision making. Existing
works either involve rebuilding Gaussian process surrogates from scratch for
each objective in each new problem encountered, or rely on extensive past
domain experiments for pre-training deep learning models, making them hard to
generalize and impractical to cope with various emerging applications in the
real world. To address this issue, we propose a new paradigm named FoMEMO
(Foundation Models for Expensive Multi-objective Optimization), which enables
the establishment of a foundation model conditioned on any domain trajectory
and user preference, and facilitates fast in-context optimization based on the
predicted preference-wise aggregation posteriors. Rather than accessing
extensive domain experiments in the real world, we demonstrate that
pre-training the foundation model with a diverse set of hundreds of millions of
synthetic data can lead to superior adaptability to unknown problems, without
necessitating any subsequent model training or updates in the optimization
process. We evaluate our method across a variety of synthetic benchmarks and
real-word applications, and demonstrate its superior generality and competitive
performance compared to existing methods.

</details>


### [52] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 提出了一种称为"结构转移"的新颖演算方法，用于实现不同表示系统间的表示转换，确保源表示和目标表示满足任意指定关系（如语义等价）。


<details>
  <summary>Details</summary>
Motivation: 解决表示系统无关的技术问题，推动表示转换和选择，提高沟通和推理的有效性。

Method: 利用模式（schemas）编码表示系统知识，通过结构转移演算规则在源表示系统的基础上生成目标表示系统的表示，确保满足指定的关系。

Result: 结构转移演算具有通用性，能够处理形式语言、几何图形、图表以及非正式符号等多种表示系统。

Conclusion: 结构转移是一种系统无关的演算方法，可在广泛的实际场景中识别替代表示。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [53] [HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling](https://arxiv.org/abs/2509.03260)
*Minjung Park,Gyuyeon Na,Soyoun Kim,Sunyoung Moon,HyeonJeong Cha,Sangmi Chai*

Main category: cs.LG

TL;DR: HyPV-LEAD是一个用于加密货币异常交易检测的早期预警框架，通过窗口-时间建模、峰谷采样和双曲嵌入技术，在比特币交易数据上实现了0.9624的PR-AUC性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币异常交易（如混币服务、欺诈转账、拉盘砸盘）对金融完整性构成日益增长的风险，但现有方法主要是模型中心化和事后检测，无法提供预防性价值。

Method: HyPV-LEAD框架包含三个创新：(1)窗口-时间建模确保可操作的提前预警；(2)峰谷采样缓解类别不平衡同时保持时间连续性；(3)双曲嵌入捕捉区块链交易网络的层次和无标度特性。

Result: 在大规模比特币交易数据上的实证评估显示，HyPV-LEAD持续优于最先进的基线方法，PR-AUC达到0.9624，在精确率和召回率方面均有显著提升。消融研究证实各组件均提供互补效益。

Conclusion: HyPV-LEAD将异常检测从被动分类转向主动预警，为实时风险管理、反洗钱合规和动态区块链环境中的金融安全建立了坚实基础。

Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent
transfers, and pump-and-dump operations -- pose escalating risks to financial
integrity but remain notoriously difficult to detect due to class imbalance,
temporal volatility, and complex network dependencies. Existing approaches are
predominantly model-centric and post hoc, flagging anomalies only after they
occur and thus offering limited preventive value. This paper introduces
HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a
data-driven early-warning framework that explicitly incorporates lead time into
anomaly detection. Unlike prior methods, HyPV-LEAD integrates three
innovations: (1) window-horizon modeling to guarantee actionable lead-time
alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while
preserving temporal continuity, and (3) hyperbolic embedding to capture the
hierarchical and scale-free properties of blockchain transaction networks.
Empirical evaluation on large-scale Bitcoin transaction data demonstrates that
HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a
PR-AUC of 0.9624 with significant gains in precision and recall. Ablation
studies further confirm that each component - PV sampling, hyperbolic
embedding, and structural-temporal modeling - provides complementary benefits,
with the full framework delivering the highest performance. By shifting anomaly
detection from reactive classification to proactive early-warning, HyPV-LEAD
establishes a robust foundation for real-time risk management, anti-money
laundering (AML) compliance, and financial security in dynamic blockchain
environments.

</details>


### [54] [Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning](https://arxiv.org/abs/2509.03316)
*Fatemeh Azad,Zoran Bosnić,Matjaž Kukar*

Main category: cs.LG

TL;DR: 本文提出一种新的元填补方法Meta-Imputation，通过综合多个基础填补器的输出来更准确预测缺失值


<details>
  <summary>Details</summary>
Motivation: 机器学习中缺失数据是一个根本挑战，特别是在生物信息学和临床机器学习领域。现有的单一填补方法无法在不同数据集和缺失机制下都表现良好

Method: 提出Meta-Imputation Balanced (MIB)方法，通过在知道真实值的合成遮盖数据上训练，学习预测每种方法的最适合填补值

Result: 该方法能够更准确地预测缺失值，展现了集成学习在填补中的潜力

Conclusion: 这项工作为实际机器学习系统开启了更稳健、模块化和可解释性的预处理流程

Abstract: Missing data represents a fundamental challenge in machine learning
applications, often reducing model performance and reliability. This problem is
particularly acute in fields like bioinformatics and clinical machine learning,
where datasets are frequently incomplete due to the nature of both data
generation and data collection. While numerous imputation methods exist, from
simple statistical techniques to advanced deep learning models, no single
method consistently performs well across diverse datasets and missingness
mechanisms. This paper proposes a novel Meta-Imputation approach that learns to
combine the outputs of multiple base imputers to predict missing values more
accurately. By training the proposed method called Meta-Imputation Balanced
(MIB) on synthetically masked data with known ground truth, the system learns
to predict the most suitable imputed value based on the behavior of each
method. Our work highlights the potential of ensemble learning in imputation
and paves the way for more robust, modular, and interpretable preprocessing
pipelines in real-world machine learning systems.

</details>


### [55] [EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms](https://arxiv.org/abs/2509.03335)
*Leizhen Wang,Peibo Duan,Hao Wang,Yue Wang,Jian Xu,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: EvolveSignal使用大语言模型自动发现新的交通信号控制算法，通过程序合成和进化搜索优化，在信号交叉口实验中比Webster基准减少20.1%的平均延迟和47.1%的平均停车次数。


<details>
  <summary>Details</summary>
Motivation: 固定时间交通信号控制依赖手工公式和人工调整，在异构或拥堵条件下效果不佳且劳动密集，需要自动化算法设计方法。

Method: 将问题表述为程序合成，候选算法表示为具有固定输入输出结构的Python函数，通过外部评估（交通模拟器）和进化搜索进行迭代优化。

Result: 实验显示发现的算法优于Webster基准，平均延迟减少20.1%，平均停车次数减少47.1%，并提供对交通工程师有实际意义的见解。

Conclusion: 这项工作通过利用AI进行交通信号控制算法设计，开辟了新的研究方向，将程序合成与交通工程相结合。

Abstract: In traffic engineering, the fixed-time traffic signal control remains widely
used for its low cost, stability, and interpretability. However, its design
depends on hand-crafted formulas (e.g., Webster) and manual re-timing by
engineers to adapt to demand changes, which is labor-intensive and often yields
suboptimal results under heterogeneous or congested conditions. This paper
introduces the EvolveSignal, a large language models (LLMs) powered coding
agent to automatically discover new traffic signal control algorithms. We
formulate the problem as program synthesis, where candidate algorithms are
represented as Python functions with fixed input-output structures, and
iteratively optimized through external evaluations (e.g., a traffic simulator)
and evolutionary search. Experiments on a signalized intersection demonstrate
that the discovered algorithms outperform Webster's baseline, reducing average
delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and
incremental analyses reveal that EvolveSignal modifications-such as adjusting
cycle length bounds, incorporating right-turn demand, and rescaling green
allocations-can offer practically meaningful insights for traffic engineers.
This work opens a new research direction by leveraging AI for algorithm design
in traffic signal control, bridging program synthesis with transportation
engineering.

</details>


### [56] [Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems](https://arxiv.org/abs/2509.03340)
*Fleur Hendriks,Ondřej Rokoš,Martin Doškář,Marc G. D. Geers,Vlado Menkovski*

Main category: cs.LG

TL;DR: 提出基于流匹配的生成框架，用于建模分岔现象中的多模态概率分布，通过等变建模保持系统对称性，在多种系统中验证了优于非概率和变分方法的性能


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统中的分岔现象会产生多个共存稳定解，但确定性机器学习模型难以捕捉这种多重性，通常会平均不同解而无法表示低对称性结果

Method: 基于流匹配的生成框架，引入对称匹配策略，在群作用下对齐预测和目标输出，通过等变建模保持系统对称性

Result: 在从玩具模型到复杂物理问题（如屈曲梁和Allen-Cahn方程）的一系列系统中验证，流匹配方法在捕捉多模态分布和对称破缺分岔方面显著优于非概率和变分方法

Conclusion: 该方法为高维系统中的多稳态建模提供了原则性和可扩展的解决方案，能够直接采样多个有效解同时保持系统对称性

Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple
coexisting stable solutions, particularly in the presence of symmetry breaking.
Deterministic machine learning models struggle to capture this multiplicity,
averaging over solutions and failing to represent lower-symmetry outcomes. In
this work, we propose a generative framework based on flow matching to model
the full probability distribution over bifurcation outcomes. Our method enables
direct sampling of multiple valid solutions while preserving system symmetries
through equivariant modeling. We introduce a symmetric matching strategy that
aligns predicted and target outputs under group actions, allowing accurate
learning in equivariant settings. We validate our approach on a range of
systems, from toy models to complex physical problems such as buckling beams
and the Allen-Cahn equation. Our results demonstrate that flow matching
significantly outperforms non-probabilistic and variational methods in
capturing multimodal distributions and symmetry-breaking bifurcations, offering
a principled and scalable solution for modeling multistability in
high-dimensional systems.

</details>


### [57] [On the MIA Vulnerability Gap Between Private GANs and Diffusion Models](https://arxiv.org/abs/2509.03341)
*Ilana Sebag,Jean-Yves Franceschi,Alain Rakotomamonjy,Alexandre Allauzen,Jamal Atif*

Main category: cs.LG

TL;DR: 本文首次对差分隐私生成模型（GANs和扩散模型）的隐私风险进行了统一的理论和实证分析，发现GANs在抵抗成员推理攻击方面具有结构优势。


<details>
  <summary>Details</summary>
Motivation: 虽然GANs和扩散模型都可以在差分隐私下训练以保护敏感数据，但它们在成员推理攻击(MIAs)方面的敏感性仍然未被充分理解，这是数据机密性的关键威胁。

Method: 通过基于稳定性的理论分析比较两种模型对数据扰动的敏感性，并使用标准化的MIA流程进行全面的实证研究，评估不同数据集和隐私预算下的隐私泄露情况。

Result: 研究结果显示GANs表现出明显较低的隐私泄露风险，即使在强差分隐私机制下，GANs相比扩散模型具有显著的隐私鲁棒性优势。

Conclusion: 模型类型本身对隐私泄露具有关键影响，GANs在抵抗成员推理攻击方面具有结构优势，这为隐私保护生成模型的选择提供了重要指导。

Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as
leading approaches for high-quality image synthesis. While both can be trained
under differential privacy (DP) to protect sensitive data, their sensitivity to
membership inference attacks (MIAs), a key threat to data confidentiality,
remains poorly understood. In this work, we present the first unified
theoretical and empirical analysis of the privacy risks faced by differentially
private generative models. We begin by showing, through a stability-based
analysis, that GANs exhibit fundamentally lower sensitivity to data
perturbations than diffusion models, suggesting a structural advantage in
resisting MIAs. We then validate this insight with a comprehensive empirical
study using a standardized MIA pipeline to evaluate privacy leakage across
datasets and privacy budgets. Our results consistently reveal a marked privacy
robustness gap in favor of GANs, even in strong DP regimes, highlighting that
model type alone can critically shape privacy leakage.

</details>


### [58] [epiGPTope: A machine learning-based epitope generator and classifier](https://arxiv.org/abs/2509.03351)
*Natalia Flechas Manrique,Alberto Martínez,Elena López-Martínez,Luc Andrea,Román Orus,Aitor Manteca,Aitziber L. Cortajarena,Llorenç Espinosa-Portalés*

Main category: cs.LG

TL;DR: epiGPTope是一个基于蛋白质数据预训练并在线性表位微调的大型语言模型，能够直接生成新型表位样序列，结合统计分类器预测细菌或病毒来源，用于表位发现和合成表位库构建。


<details>
  <summary>Details</summary>
Motivation: 合成表位库的理性设计面临巨大组合序列空间的挑战，传统筛选方法不可行，需要开发新的生成方法来创建生物可行的表位序列。

Method: 开发epiGPTope模型，在蛋白质数据上预训练后在线性表位数据上微调，直接生成表位样序列；训练统计分类器预测表位的细菌或病毒来源。

Result: 模型生成的序列具有与已知表位相似的统计特性，可用于构建表位候选库，分类器能有效缩小候选范围。

Conclusion: 生成模型与预测模型的结合有助于表位发现，仅需氨基酸序列而不依赖几何框架或手工特征，有望实现更快、更经济的合成表位生成和筛选。

Abstract: Epitopes are short antigenic peptide sequences which are recognized by
antibodies or immune cell receptors. These are central to the development of
immunotherapies, vaccines, and diagnostics. However, the rational design of
synthetic epitope libraries is challenging due to the large combinatorial
sequence space, $20^n$ combinations for linear epitopes of n amino acids,
making screening and testing unfeasible, even with high throughput experimental
techniques. In this study, we present a large language model, epiGPTope,
pre-trained on protein data and specifically fine-tuned on linear epitopes,
which for the first time can directly generate novel epitope-like sequences,
which are found to possess statistical properties analogous to the ones of
known epitopes. This generative approach can be used to prepare libraries of
epitope candidate sequences. We further train statistical classifiers to
predict whether an epitope sequence is of bacterial or viral origin, thus
narrowing the candidate library and increasing the likelihood of identifying
specific epitopes. We propose that such combination of generative and
predictive models can be of assistance in epitope discovery. The approach uses
only primary amino acid sequences of linear epitopes, bypassing the need for a
geometric framework or hand-crafted features of the sequences. By developing a
method to create biologically feasible sequences, we anticipate faster and more
cost-effective generation and screening of synthetic epitopes, with relevant
applications in the development of new biotechnologies.

</details>


### [59] [Fair Resource Allocation for Fleet Intelligence](https://arxiv.org/abs/2509.03353)
*Oguzhan Baser,Kaan Kale,Po-han Li,Sandeep Chinchali*

Main category: cs.LG

TL;DR: Fair-Synergy是一个开源的公平资源分配算法框架，通过利用智能体准确性与系统资源之间的凹关系，在多智能体智能系统中实现公平的资源分配，在推理和学习任务中分别比基准方法提升25%和11%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统资源分配方法往往忽视智能体不同的计算能力和复杂操作环境，导致资源分配效率低下且不公平，需要一种能够确保多智能体智能系统公平资源分配的新方法。

Method: 开发了Fair-Synergy算法框架，扩展传统分配方法以涵盖由模型参数、训练数据量和任务复杂性定义的多维机器学习效用景观，利用智能体准确性与系统资源之间的凹关系来确保公平分配。

Result: 在BERT、VGG16、MobileNet、ResNets等先进视觉和语言模型以及MNIST、CIFAR-10、CIFAR-100、BDD、GLUE等数据集上评估，Fair-Synergy在多智能体推理中比标准基准提升25%，在多智能体学习中提升11%。

Conclusion: Fair-Synergy框架有效解决了多智能体智能系统中的公平资源分配问题，提供了对公平性水平如何影响最弱势、最优势和平均智能体的深入见解，为公平的舰队智能提供了实用解决方案。

Abstract: Resource allocation is crucial for the performance optimization of
cloud-assisted multi-agent intelligence. Traditional methods often overlook
agents' diverse computational capabilities and complex operating environments,
leading to inefficient and unfair resource distribution. To address this, we
open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave
relationship between the agents' accuracy and the system resources to ensure
fair resource allocation across fleet intelligence. We extend traditional
allocation approaches to encompass a multidimensional machine learning utility
landscape defined by model parameters, training data volume, and task
complexity. We evaluate Fair-Synergy with advanced vision and language models
such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,
CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy
outperforms standard benchmarks by up to 25% in multi-agent inference and 11%
in multi-agent learning settings. Also, we explore how the level of fairness
affects the least advantaged, most advantaged, and average agents, providing
insights for equitable fleet intelligence.

</details>


### [60] [Some patterns of sleep quality and Daylight Saving Time across countries: a predictive and exploratory analysis](https://arxiv.org/abs/2509.03358)
*Bhanu Sharma,Eugene Pinsky*

Main category: cs.LG

TL;DR: 研究分析61个国家的平均睡眠时长，发现采用夏令时制的国家睡眠时长更长，但经纬度调节后显示低经纬度区睡眠时长短于非夏令时国家，高经纬度区则更长


<details>
  <summary>Details</summary>
Motivation: 探索夏令时制对不同地理位置国家睡眠时长的影响

Method: 对61个国家进行睡眠时长分析，统计相关分析，按夏令时制观察情况分组，可视化比较睡眠模式

Result: 夏令时国家平均睡眠时长更长，但经纬度调整后显示低经纬度区睡眠时长短于非夏令时国家，高经纬度区则更长

Conclusion: 夏令时对睡眠的影响受地理位置调节，经纬度是重要调节因素

Abstract: In this study we analyzed average sleep durations across 61 countries to
examine the impact of Daylight Saving Time (DST) practices. Key metrics
influencing sleep were identified, and statistical correlation analysis was
applied to explore relationships among these factors. Countries were grouped
based on DST observance, and visualizations compared sleep patterns between DST
and non-DST regions. Results show that, on average, countries observing DST
tend to report longer sleep durations than those that do not. A more detailed
pattern emerged when accounting for latitude: at lower latitudes, DST-observing
countries reported shorter sleep durations compared to non-DST countries, while
at higher latitudes, DST-observing countries reported longer average sleep
durations. These findings suggest that the influence of DST on sleep may be
moderated by geographical location.

</details>


### [61] [The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex](https://arxiv.org/abs/2509.03365)
*Paul-Gauthier Noé,Andreas Nautsch,Driss Matrouf,Pierre-Michel Bousquet,Jean-François Bonastre*

Main category: cs.LG

TL;DR: 本文扩展了似然函数校准理论，从传统的二元假设情况推广到多假设场景，使用Aitchison几何和等距对数比变换来定义多类校准、幂等性和分布约束。


<details>
  <summary>Details</summary>
Motivation: 虽然概率预测的校准已被广泛研究，但似然函数的校准研究相对较少，特别是在多假设情况下缺乏系统的理论框架。

Method: 使用Aitchison几何和等距对数比变换，将二元情况下的对数似然比校准理论扩展到多假设场景，恢复了向量形式的贝叶斯规则加性形式。

Result: 提出了多假设情况下似然函数校准的统一定义，扩展了幂等性性质和分布约束条件，并在机器学习中应用非线性判别分析验证了方法的有效性。

Conclusion: 该研究为多类似然函数校准提供了理论基础，提高了方法的可解释性和可靠性，主要贡献是概念性的理论扩展。

Abstract: While calibration of probabilistic predictions has been widely studied, this
paper rather addresses calibration of likelihood functions. This has been
discussed, especially in biometrics, in cases with only two exhaustive and
mutually exclusive hypotheses (classes) where likelihood functions can be
written as log-likelihood-ratios (LLRs). After defining calibration for LLRs
and its connection with the concept of weight-of-evidence, we present the
idempotence property and its associated constraint on the distribution of the
LLRs. Although these results have been known for decades, they have been
limited to the binary case. Here, we extend them to cases with more than two
hypotheses by using the Aitchison geometry of the simplex, which allows us to
recover, in a vector form, the additive form of the Bayes' rule; extending
therefore the LLR and the weight-of-evidence to any number of hypotheses.
Especially, we extend the definition of calibration, the idempotence, and the
constraint on the distribution of likelihood functions to this multiple
hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio
transformed likelihood function. This work is mainly conceptual, but we still
provide one application to machine learning by presenting a non-linear
discriminant analysis where the discriminant components form a calibrated
likelihood function over the classes, improving therefore the interpretability
and the reliability of the method.

</details>


### [62] [Cluster and then Embed: A Modular Approach for Visualization](https://arxiv.org/abs/2509.03373)
*Elizabeth Coda,Ery Arias-Castro,Gal Mishne*

Main category: cs.LG

TL;DR: 这篇论文提出了一种模块化的维度降低方法，先分类再嵌入最后对齐，解决了t-SNE和UMAP方法对全局几何结构的扭曲问题


<details>
  <summary>Details</summary>
Motivation: t-SNE和UMAP等维度降低方法虽然能够进行数据可视化并保持局部信息，但容易扭曲数据的全局几何结构，需要更透明的方法来解决这一问题

Method: 提出一种模块化方法：首先对数据进行聚类，然后对每个聚类分别进行嵌入，最后将所有聚类嵌入结果进行对齐以获得全局嵌入

Result: 在多个合成和实际数据集上进行了实验，证明该方法与现有方法竞争力相当，同时具有更高的透明性

Conclusion: 该模块化方法为维度降低提供了一种更透明的替代方案，能够在保持局部信息的同时更好地保持全局几何结构

Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods
for visualizing data with a potential (latent) clustered structure. They are
known to group data points at the same time as they embed them, resulting in
visualizations with well-separated clusters that preserve local information
well. However, t-SNE and UMAP also tend to distort the global geometry of the
underlying data. We propose a more transparent, modular approach consisting of
first clustering the data, then embedding each cluster, and finally aligning
the clusters to obtain a global embedding. We demonstrate this approach on
several synthetic and real-world datasets and show that it is competitive with
existing methods, while being much more transparent.

</details>


### [63] [Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2509.03393)
*Taisiya Khakharova,Lucas Sakizloglou,Leen Lambers*

Main category: cs.LG

TL;DR: 本研究将MIMIC-III患者数据建模为时变异质图，使用GraphSAGE和GATv2图神经网络学习患者状态表示，并与dBCQ算法结合进行脓毒症治疗的强化学习策略优化。


<details>
  <summary>Details</summary>
Motivation: 脓毒症治疗中确定静脉输液和升压药的合适剂量具有挑战性。传统基于关系数据的强化学习方法存在局限，而图结构能更自然地表示复杂的医疗数据关系。

Method: 将患者数据构建为时变异质图，使用GraphSAGE和GATv2进行表示学习（与状态预测解码器联合训练），然后用dBCQ算法进行策略学习。

Result: 实验结果证实了基于图的方法的潜力，同时凸显了在该领域表示学习的复杂性。

Conclusion: 图神经网络为医疗强化学习提供了有前景的方向，但表示学习仍面临挑战，需要进一步研究优化患者状态表示方法。

Abstract: Sepsis is a serious, life-threatening condition. When treating sepsis, it is
challenging to determine the correct amount of intravenous fluids and
vasopressors for a given patient. While automated reinforcement learning
(RL)-based methods have been used to support these decisions with promising
results, previous studies have relied on relational data. Given the complexity
of modern healthcare data, representing data as a graph may provide a more
natural and effective approach. This study models patient data from the
well-known MIMIC-III dataset as a heterogeneous graph that evolves over time.
Subsequently, we explore two Graph Neural Network architectures - GraphSAGE and
GATv2 - for learning patient state representations, adopting the approach of
decoupling representation learning from policy learning. The encoders are
trained to produce latent state representations, jointly with decoders that
predict the next patient state. These representations are then used for policy
learning with the dBCQ algorithm. The results of our experimental evaluation
confirm the potential of a graph-based approach, while highlighting the
complexity of representation learning in this domain.

</details>


### [64] [Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training](https://arxiv.org/abs/2509.03403)
*Chenlu Ye,Zhou Yu,Ziji Zhang,Hao Chen,Narayanan Sadagopan,Jing Huang,Tong Zhang,Anurag Beniwal*

Main category: cs.LG

TL;DR: PROF是一种数据过程筛选方法，通过一致性驱动的样本选择来协调细粒度过程奖励和粗粒度结果奖励，提高数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法中，结果奖励模型(ORM)过于粗粒度，无法区分正确答案中的错误推理或错误答案中的有效推理；而过程奖励模型(PRM)虽然提供细粒度指导，但经常存在不准确性且容易受到奖励攻击。

Method: 提出PROF方法，通过一致性驱动的样本选择来协调嘈杂的细粒度过程奖励和准确的粗粒度结果奖励。保留具有较高平均过程值的正确响应和具有较低平均过程值的错误响应，同时保持正负训练样本平衡。

Result: 实验表明，该方法相比混合方法持续提高了4%以上的最终准确率，并加强了中间推理步骤的质量。

Conclusion: PROF方法有效解决了RLVR中奖励模型的粒度不匹配问题，通过一致性过滤机制显著提升了数学推理任务的性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a
predominant paradigm for mathematical reasoning tasks, offering stable
improvements in reasoning ability. However, Outcome Reward Models (ORMs) in
RLVR are too coarse-grained to distinguish flawed reasoning within correct
answers or valid reasoning within incorrect answers. This lack of granularity
introduces noisy and misleading gradients significantly and hinders further
progress in reasoning process quality. While Process Reward Models (PRMs) offer
fine-grained guidance for intermediate steps, they frequently suffer from
inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an
effective data process curation method that harmonizes noisy, fine-grained
process rewards with accurate, coarse-grained outcome rewards. Rather than
naively blending PRM and ORM in the objective function
(arXiv:archive/2506.18896), PROF leverages their complementary strengths
through consistency-driven sample selection. Our approach retains correct
responses with higher averaged process values and incorrect responses with
lower averaged process values, while maintaining positive/negative training
sample balance. Extensive experiments demonstrate that our method not only
consistently improves the final accuracy over $4\%$ compared to the blending
approaches, but also strengthens the quality of intermediate reasoning steps.
Codes and training recipes are available at https://github.com/Chenluye99/PROF.

</details>


### [65] [Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study](https://arxiv.org/abs/2509.03417)
*Spyros Rigas,Dhruv Verma,Georgios Alexandridis,Yixuan Wang*

Main category: cs.LG

TL;DR: 本文研究了Kolmogorov-Arnold Networks (KANs)的初始化策略，提出了理论驱动的LeCun和Glorot初始化方法以及经验性的幂律初始化方法，在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: KANs作为一种新兴的神经网络架构，虽然已在科学和机器学习任务中成功应用，但其初始化策略尚未得到充分研究，需要系统性的初始化方案来提升性能。

Method: 提出了两种理论驱动的初始化方法（LeCun和Glorot启发式）和一个可调指数的经验幂律初始化族，通过大规模网格搜索在函数拟合、前向PDE基准和Feynman数据集子集上进行评估，并结合神经正切核分析训练动态。

Result: Glorot启发的初始化在参数丰富的模型中显著优于基线，而幂律初始化在所有任务和不同规模架构中均实现了最强性能。

Conclusion: 本文为KANs提供了有效的初始化策略，Glorot初始化适合参数丰富的模型，幂律初始化在整体性能上表现最佳，所有代码和数据均已公开。

Abstract: Kolmogorov-Arnold Networks (KANs) are a recently introduced neural
architecture that replace fixed nonlinearities with trainable activation
functions, offering enhanced flexibility and interpretability. While KANs have
been applied successfully across scientific and machine learning tasks, their
initialization strategies remain largely unexplored. In this work, we study
initialization schemes for spline-based KANs, proposing two theory-driven
approaches inspired by LeCun and Glorot, as well as an empirical power-law
family with tunable exponents. Our evaluation combines large-scale grid
searches on function fitting and forward PDE benchmarks, an analysis of
training dynamics through the lens of the Neural Tangent Kernel, and
evaluations on a subset of the Feynman dataset. Our findings indicate that the
Glorot-inspired initialization significantly outperforms the baseline in
parameter-rich models, while power-law initialization achieves the strongest
performance overall, both across tasks and for architectures of varying size.
All code and data accompanying this manuscript are publicly available at
https://github.com/srigas/KAN_Initialization_Schemes.

</details>


### [66] [LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability](https://arxiv.org/abs/2509.03425)
*Phuc Pham,Viet Thanh Duy Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: LINKER是首个基于序列的模型，仅使用蛋白质序列和配体SMILES就能预测残基-功能基团相互作用类型，无需3D结构输入


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法的局限性：需要3D结构输入或使用基于距离的接触标签，限制了应用范围和生物学相关性

Method: 使用结构监督的注意力机制训练，通过功能基团基序提取从3D蛋白-配体复合物获得交互标签，将配体结构抽象为功能基团

Result: 在LP-PDBBind基准测试中，基于功能基团抽象的结构监督能够产生与真实生化注释高度一致的交互预测

Conclusion: LINKER仅需序列级输入即可进行大规模应用，特别适用于结构数据不可用的情况，为分子识别和理性药物设计提供了有力工具

Abstract: Accurate identification of interactions between protein residues and ligand
functional groups is essential to understand molecular recognition and guide
rational drug design. Existing deep learning approaches for protein-ligand
interpretability often rely on 3D structural input or use distance-based
contact labels, limiting both their applicability and biological relevance. We
introduce LINKER, the first sequence-based model to predict residue-functional
group interactions in terms of biologically defined interaction types, using
only protein sequences and the ligand SMILES as input. LINKER is trained with
structure-supervised attention, where interaction labels are derived from 3D
protein-ligand complexes via functional group-based motif extraction. By
abstracting ligand structures into functional groups, the model focuses on
chemically meaningful substructures while predicting interaction types rather
than mere spatial proximity. Crucially, LINKER requires only sequence-level
input at inference time, enabling large-scale application in settings where
structural data is unavailable. Experiments on the LP-PDBBind benchmark
demonstrate that structure-informed supervision over functional group
abstractions yields interaction predictions closely aligned with ground-truth
biochemical annotations.

</details>


### [67] [Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects](https://arxiv.org/abs/2509.03446)
*Niteesh Midlagajni,Constantin A. Rothkopf*

Main category: cs.LG

TL;DR: 基于图神经网络的液体与刚体交互动力学模拟框架，能够处理复杂表面形状并应用于控制任务


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在模拟液体与动态刚体复杂交互方面有限，需要能够处理复杂表面几何的方法

Method: 使用GNN表示粒子为图节点，采用包围体级层算法(BVH)处理粒子-物体碰撞，能够模拟液体与复杂表面几何的交互

Result: 模型在动态环境中准确捕捉液体行为，仅训练在单物体倒注任务上即可泛化到未见物体和新操作任务（搅拌、摇取），并能通过梯度优化法解决控制任务

Conclusion: 该GNN框架能够有效学习液体在刚体交互和主动操控下的动力学，具有良好的泛化能力和应用价值

Abstract: Simulating particle dynamics with high fidelity is crucial for solving
real-world interaction and control tasks involving liquids in design, graphics,
and robotics. Recently, data-driven approaches, particularly those based on
graph neural networks (GNNs), have shown progress in tackling such problems.
However, these approaches are often limited to learning fluid behavior in
static free-fall environments or simple manipulation settings involving
primitive objects, often overlooking complex interactions with dynamically
moving kinematic rigid bodies. Here, we propose a GNN-based framework designed
from the ground up to learn the dynamics of liquids under rigid body
interactions and active manipulations, where particles are represented as graph
nodes and particle-object collisions are handled using surface representations
with the bounding volume hierarchy (BVH) algorithm. This approach enables the
network to model complex interactions between liquid particles and intricate
surface geometries. Our model accurately captures fluid behavior in dynamic
settings and can also function as a simulator in static free-fall environments.
Despite being trained on a single-object manipulation task of pouring, our
model generalizes effectively to environments with unseen objects and novel
manipulation tasks such as stirring and scooping. Finally, we show that the
learned dynamics can be leveraged to solve control and manipulation tasks using
gradient-based optimization methods.

</details>


### [68] [Geometric Foundations of Tuning without Forgetting in Neural ODEs](https://arxiv.org/abs/2509.03474)
*Erkan Bayram,Mohamed-Ali Belabbas,Tamer Başar*

Main category: cs.LG

TL;DR: 本文证明了Tuning without Forgetting (TwF)方法中的参数子空间在非奇异控制下形成有限余维的Banach子流形，并刻画了其切空间，为TwF在顺序训练中保持映射不变性提供了精确的理论基础。


<details>
  <summary>Details</summary>
Motivation: 为之前提出的Tuning without Forgetting (TwF)方法提供严格的理论基础，证明其在顺序训练中能够精确保持先前学习样本的端点映射，而不仅仅是一阶近似意义上的保持。

Method: 通过数学分析证明在非奇异控制条件下，TwF的参数子空间构成有限余维的Banach子流形，并详细刻画该子流形的切空间结构。

Result: 证明了TwF对应的参数子空间确实形成Banach子流形，其切空间特性表明TwF方法实际上是沿着该子流形切空间进行控制函数的连续变形，从而精确保持映射不变性。

Conclusion: 该研究为Tuning without Forgetting方法提供了坚实的数学理论基础，表明该方法能够在顺序训练过程中精确保持先前学习到的映射关系，超越了之前的一阶近似理解。

Abstract: In our earlier work, we introduced the principle of Tuning without Forgetting
(TwF) for sequential training of neural ODEs, where training samples are added
iteratively and parameters are updated within the subspace of control functions
that preserves the end-point mapping at previously learned samples on the
manifold of output labels in the first-order approximation sense. In this
letter, we prove that this parameter subspace forms a Banach submanifold of
finite codimension under nonsingular controls, and we characterize its tangent
space. This reveals that TwF corresponds to a continuation/deformation of the
control function along the tangent space of this Banach submanifold, providing
a theoretical foundation for its mapping-preserving (not forgetting) during the
sequential training exactly, beyond first-order approximation.

</details>


### [69] [Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning](https://arxiv.org/abs/2509.03477)
*Duy A. Nguyen,Abhi Kamboj,Minh N. Do*

Main category: cs.LG

TL;DR: Robult是一个可扩展的多模态学习框架，通过信息论方法解决缺失模态和标注数据有限的问题，包含PU对比损失和潜在重建损失两个核心目标，在半监督学习和缺失模态场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中缺失模态和有限标注数据的关键挑战，提升模型的鲁棒性和实用性。

Method: 提出软正样本-未标注样本对比损失最大化任务相关特征对齐，以及潜在重建损失保留模态特定信息，采用模块化设计。

Result: 在多个数据集上的实验验证表明，Robult在半监督学习和缺失模态情境下均优于现有方法。

Conclusion: Robult框架具有轻量级设计，可扩展性强，能无缝集成到现有架构中，适用于实际多模态应用场景。

Abstract: Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.

</details>


### [70] [SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](https://arxiv.org/abs/2509.03487)
*Jigang Fan,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang,Zaixi Zhang*

Main category: cs.LG

TL;DR: SafeProtein是首个针对蛋白质基础模型的红队测试框架，通过多模态提示工程和启发式束搜索系统性地测试模型安全性，发现现有模型存在生物安全风险（攻击成功率高达70%）


<details>
  <summary>Details</summary>
Motivation: 蛋白质基础模型缺乏系统性红队测试，存在被滥用于生成具有生物安全风险蛋白质的潜在威胁

Method: 结合多模态提示工程和启发式束搜索，构建SafeProtein-Bench基准数据集和评估协议

Result: 成功对最先进的蛋白质基础模型进行持续越狱攻击（ESM3攻击成功率高达70%）

Conclusion: 揭示了当前蛋白质基础模型的潜在生物安全风险，为前沿模型的安全防护技术发展提供了重要见解

Abstract: Proteins play crucial roles in almost all biological processes. The
advancement of deep learning has greatly accelerated the development of protein
foundation models, leading to significant successes in protein understanding
and design. However, the lack of systematic red-teaming for these models has
raised serious concerns about their potential misuse, such as generating
proteins with biological safety risks. This paper introduces SafeProtein, the
first red-teaming framework designed for protein foundation models to the best
of our knowledge. SafeProtein combines multimodal prompt engineering and
heuristic beam search to systematically design red-teaming methods and conduct
tests on protein foundation models. We also curated SafeProtein-Bench, which
includes a manually constructed red-teaming benchmark dataset and a
comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks
on state-of-the-art protein foundation models (up to 70% attack success rate
for ESM3), revealing potential biological safety risks in current protein
foundation models and providing insights for the development of robust security
protection technologies for frontier models. The codes will be made publicly
available at https://github.com/jigang-fan/SafeProtein.

</details>


### [71] [On Entropy Control in LLM-RL Algorithms](https://arxiv.org/abs/2509.03493)
*Han Shen*

Main category: cs.LG

TL;DR: AEnt是一种针对LLM-RL训练的新型熵控制方法，通过钳位熵奖励和自动调整系数来解决传统熵正则化在大语言模型强化学习中的失效问题


<details>
  <summary>Details</summary>
Motivation: 传统熵正则化在机器人控制和游戏RL中有效，但在LLM-RL训练中效果不佳，主要原因是LLM响应空间极大且最优输出稀疏

Method: 提出AEnt方法：使用钳位熵奖励（基于重归一化策略在较小token空间计算），并自动调整熵系数来控制熵诱导偏差

Result: 在数学推理任务中，AEnt在不同基础模型和数据集上均优于基线方法，在多个基准测试中表现一致更好

Conclusion: AEnt通过创新的熵控制机制有效解决了LLM-RL训练中的熵正则化问题，为大规模语言模型的强化学习提供了有效的探索策略

Abstract: For RL algorithms, appropriate entropy control is crucial to their
effectiveness. To control the policy entropy, a commonly used method is entropy
regularization, which is adopted in various popular RL algorithms including
PPO, SAC and A3C. Although entropy regularization proves effective in robotic
and games RL conventionally, studies found that it gives weak to no gains in
LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL
setting. Specifically, we first argue that the conventional entropy
regularization suffers from the LLM's extremely large response space and the
sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy
control method that utilizes a new clamped entropy bonus with an automatically
adjusted coefficient. The clamped entropy is evaluated with the re-normalized
policy defined on certain smaller token space, which encourages exploration
within a more compact response set. In addition, the algorithm automatically
adjusts entropy coefficient according to the clamped entropy value, effectively
controlling the entropy-induced bias while leveraging the entropy's benefits.
AEnt is tested in math-reasoning tasks under different base models and
datasets, and it is observed that AEnt outperforms the baselines consistently
across multiple benchmarks.

</details>


### [72] [Invariant Features for Global Crop Type Classification](https://arxiv.org/abs/2509.03497)
*Xin-Yi Tong,Sherrie Wang*

Main category: cs.LG

TL;DR: 本研究构建了全球作物数据集CropGlobe，开发了轻量级CNN模型CropNet，通过时序数据增强技术提升跨区域作物分类的泛化能力，发现Sentinel-2的2D中值时序特征具有最强的地理不变性。


<details>
  <summary>Details</summary>
Motivation: 全球尺度的作物类型精确识别对粮食安全至关重要，但现有方法在地理迁移时性能下降严重，需要寻找对地理变化不变的特征表示来提升跨区域泛化能力。

Method: 构建包含30万个样本的全球作物数据集CropGlobe；比较时序多光谱特征和高光谱特征的迁移性；设计轻量CNN模型CropNet；采用时间偏移、时间尺度和幅度扭曲等时序数据增强技术。

Result: Sentinel-2的2D中值时序特征在所有迁移场景中表现出最强的地理不变性；数据增强技术显著提升了模型鲁棒性，特别是在训练数据多样性有限时效果更明显。

Conclusion: 研究发现了更具不变性的特征表示，增强了地理迁移能力，为在全球多样化区域实现可扩展、低成本的作物类型应用提供了有前景的路径。

Abstract: Accurately obtaining crop type and its spatial distribution at a global scale
is critical for food security, agricultural policy-making, and sustainable
development. Remote sensing offers an efficient solution for large-scale crop
classification, but the limited availability of reliable ground samples in many
regions constrains applicability across geographic areas. To address
performance declines under geospatial shifts, this study identifies remote
sensing features that are invariant to geographic variation and proposes
strategies to enhance cross-regional generalization. We construct CropGlobe, a
global crop type dataset with 300,000 pixel-level samples from eight countries
across five continents, covering six major food and industrial crops (corn,
soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,
CropGlobe enables a systematic evaluation under cross-country, cross-continent,
and cross-hemisphere transfer. We compare the transferability of temporal
multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic
coefficients) and hyperspectral features (from EMIT). To improve generalization
under spectral and phenological shifts, we design CropNet, a lightweight and
robust CNN tailored for pixel-level crop classification, coupled with temporal
data augmentation (time shift, time scale, and magnitude warping) that
simulates realistic cross-regional phenology. Experiments show that 2D median
temporal features from Sentinel-2 consistently exhibit the strongest invariance
across all transfer scenarios, and augmentation further improves robustness,
particularly when training data diversity is limited. Overall, the work
identifies more invariant feature representations that enhance geographic
transferability and suggests a promising path toward scalable, low-cost crop
type applications across globally diverse regions.

</details>


### [73] [Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients](https://arxiv.org/abs/2509.03503)
*Gwen Legate,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出ZOWarmUp联邦学习算法，通过零阶优化方法让内存和通信受限的边缘设备参与训练，减少系统偏见并提高数据多样性


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中边缘设备因内存和通信限制被排除在训练之外的问题，这些设备的缺失会导致数据不可访问和系统偏见增加

Method: 基于MeZO零阶优化方法，设计ZOWarmUp算法，利用不同客户端能力和方差减少技术，仅需传输随机种子而非完整梯度

Result: 实验表明ZOWarmUp在各种数据集和模型架构下表现稳健，能够有效让低资源客户端参与训练

Conclusion: ZOWarmUp算法为高比例受限边缘设备的系统提供了访问更多样化数据的途径，从而改善训练效果

Abstract: Federated learning enables collaborative model training across numerous edge
devices without requiring participants to share data; however, memory and
communication constraints on these edge devices may preclude their
participation in training. We consider a setting in which a subset of edge
devices are below a critical memory or communication threshold required to
conduct model updates. Under typical federated optimization algorithms, these
devices are excluded from training which renders their data inaccessible and
increases system induced bias. We are inspired by MeZO, a zeroth-order method
used for memory-efficient fine-tuning. The increased variance inherent to
zeroth-order gradient approximations has relegated previous zeroth-order
optimizers exclusively to the domain of fine tuning; a limitation we seek to
correct. We devise a federated, memory-efficient zeroth-order optimizer,
ZOWarmUp that permits zeroth-order training from a random initialization.
ZOWarmUp leverages differing client capabilities and careful variance reduction
techniques to facilitate participation of under-represented, low-resource
clients in model training. Like other federated zeroth-order methods, ZOWarmUp
eliminates the need for edge devices to transmit their full gradients to the
server and instead relies on only a small set of random seeds, rendering the
up-link communication cost negligible. We present experiments using various
datasets and model architectures to show that ZOWarmUp is a robust algorithm
that can can be applied under a wide variety of circumstances. For systems with
a high proportion of edge devices that would otherwise be excluded from
training, this algorithm provides access to a greater volume and diversity of
data, thus improving training outcomes.

</details>


### [74] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: LimiX是首个大型结构化数据基础模型，通过单一模型处理多种表格任务，在10个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 实现通用智能需要结合语言、物理世界和结构化数据的基础模型，当前缺乏专门处理结构化数据的统一模型

Method: 采用掩码联合分布建模和情景条件目标进行预训练，将结构化数据视为变量和缺失值的联合分布，支持基于查询的条件预测

Result: 在分类、回归、缺失值填补和数据生成等任务中均显著超越梯度提升树、深度表格网络和现有表格基础模型

Conclusion: LimiX证明了单一模型统一处理多样化表格任务的可行性，为结构化数据基础模型的发展奠定了基础

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


### [75] [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
*Haoran Huan,Mihir Prabhudesai,Mengning Wu,Shantanu Jaiswal,Deepak Pathak*

Main category: cs.LG

TL;DR: 本文系统研究了大语言模型的说谎行为，区别于幻觉现象，通过机制可解释性技术揭示了欺骗的神经机制，并开发了行为导向向量来精细操控说谎倾向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中自主性增强，其可信度引发担忧。虽然幻觉现象已被广泛研究，但模型为实现特定目标而故意说谎的行为仍未被充分探索。

Method: 采用机制可解释性技术，包括logit lens分析、因果干预和对比激活导向，识别和控制欺骗行为。研究真实世界说谎场景，引入行为导向向量来精细操控说谎倾向。

Result: 揭示了欺骗的神经机制，建立了说谎与最终任务性能之间的权衡关系，发现了不诚实行为可以增强目标优化的帕累托前沿。

Conclusion: 研究结果有助于AI伦理讨论，揭示了在高风险环境中部署LLM的风险和潜在保障措施，为理解和控制模型说谎行为提供了重要见解。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing](https://arxiv.org/abs/2509.02767)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 这篇论文提出了一种称为GreenCloud税的经济模型，通过对能源效率低的数据中心征税来促进云计算数据中心的能源效率提升。


<details>
  <summary>Details</summary>
Motivation: 云计算数据中心的能源消耗持续增长，需要有效的方法来提高能源效率。传统的算法和架构改进方法已经不足，需要创新的经济控制手段来引导供应商投资能源效率更高的设施。

Method: 设计了一种经济税收模型，对能源效率低的数据中心征收税以增加其运营成本，而能源效率高的数据中心则可以提供更便宜的服务价格。使用CloudSim模拟环境实现并采用SPEC净出的真实数据集进行模拟评估。

Result: 税收模型能够有效地引导工作负载从能源效率低的数据中心转移到能源效率高的数据中心，通过经济刺激控制数据中心的能源消耗行为。

Conclusion: 经济税收方式是一种有效的绿色云计算控制手段，可以通过市场机制促进数据中心投资能源效率更高的技术和设备，最终实现整体能源消耗的降低。

Abstract: The cloud computing technology uses datacenters, which require energy. Recent
trends show that the required energy for these datacenters will rise over time,
or at least remain constant. Hence, the scientific community developed
different algorithms, architectures, and approaches for improving the energy
efficiency of cloud datacenters, which are summarized under the umbrella term
Green Cloud computing. In this paper, we use an economic approach - taxes - for
reducing the energy consumption of datacenters. We developed a tax model called
GreenCloud tax, which penalizes energy-inefficient datacenters while fostering
datacenters that are energy-efficient. Hence, providers running
energy-efficient datacenters are able to offer cheaper prices to consumers,
which consequently leads to a shift of workloads from energy-inefficient
datacenters to energy-efficient datacenters. The GreenCloud tax approach was
implemented using the simulation environment CloudSim. We applied real data
sets published in the SPEC benchmark for the executed simulation scenarios,
which we used for evaluating the GreenCloud tax.

</details>


### [77] [Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training](https://arxiv.org/abs/2509.03018)
*Yangtao Deng,Lei Zhang,Qinlong Wang,Xiaoyun Zhi,Xinlei Zhang,Zhuo Jiang,Haohan Xu,Lei Wang,Zuquan Song,Gaohong Liu,Yang Bai,Shuguang Wang,Wencong Xiao,Jianxi Ye,Minlan Yu,Hong Xu*

Main category: cs.DC

TL;DR: Mycroft是一个轻量级分布式追踪和根因分析系统，用于解决LLM训练中集体通信的可靠性问题，通过追踪通信状态和利用内部依赖关系来快速检测和诊断问题。


<details>
  <summary>Details</summary>
Motivation: LLM训练中的可靠性问题导致资源浪费和性能下降，而现有的集体通信库作为黑盒运行，缺乏有效的根因分析所需的关键信息。

Method: 设计并实现Mycroft系统，通过追踪集体通信状态，利用内部控制和数据依赖关系来解决可靠性问题。系统已在实际环境中部署并进行故障注入实验验证。

Result: Mycroft在90%的情况下15秒内检测到异常，60%的情况下20秒内识别出根本原因。通过广泛的故障注入实验证明了其能力和效率。

Conclusion: Mycroft有效解决了LLM训练中集体通信的可靠性问题，提供了快速的问题检测和根因分析能力，在实际部署中表现出色。

Abstract: Reliability is essential for ensuring efficiency in LLM training. However,
many real-world reliability issues remain difficult to resolve, resulting in
wasted resources and degraded model performance. Unfortunately, today's
collective communication libraries operate as black boxes, hiding critical
information needed for effective root cause analysis. We propose Mycroft, a
lightweight distributed tracing and root cause analysis system designed to
address previously hidden reliability issues in collective communication.
Mycroft's key idea is to trace collective communication states and leverage
internal control and data dependencies to resolve reliability problems in LLM
training. Mycroft has been deployed at ByteDance for over six months to debug
collective communication related issues at runtime. It detected anomalies
within 15 seconds in 90% of cases and identified the root cause within 20
seconds in 60% of cases. We also conducted extensive fault injection
experiments to demonstrate Mycroft's capability and efficiency.

</details>


### [78] [FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs](https://arxiv.org/abs/2509.03047)
*Haijun Zhang,Jinxiang Wang,Zhenhua Yu,Yanyong Zhang,Xuejie Ji,Kaining Mao,Jun Zhang,Yaqing Zhang,Ting Wu,Fei Jie,Xiemin Huang,Zhifang Cai,Junhua Cheng,Shuwei Wang,Wei Li,Xiaoming Bao,Hua Xu,Shixiong Zhao,Jun Li,Hongwei Sun,Ziyang Zhang,Yi Xiong,Chunsheng Li*

Main category: cs.DC

TL;DR: FlashRecovery是一个针对大规模LLM训练故障恢复的快速低成本系统，通过实时故障检测、规模无关的任务重启和单步无检查点恢复三大模块，显著提升训练可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练需要庞大的AI加速器集群和复杂并行策略，硬件软件故障会导致大量训练时间损失，需要高效的故障恢复解决方案。

Method: 系统包含三个核心模块：1）主动实时故障检测，秒级识别故障；2）规模无关任务重启，针对正常和故障节点采用不同恢复策略；3）单步无检查点恢复机制，完全消除对传统检查点的依赖。

Result: 实验结果表明，FlashRecovery能在4800设备集群上150秒内完成训练恢复，且不同规模训练任务的故障恢复时间基本一致。

Conclusion: FlashRecovery通过创新技术实现了最优的恢复时间目标和恢复点目标，大幅提升了长时间LLM训练的可靠性和效率。

Abstract: Large language models (LLMs) have made a profound impact across various
fields due to their advanced capabilities. However, training these models at
unprecedented scales requires extensive AI accelerator clusters and
sophisticated parallelism strategies, which pose significant challenges in
maintaining system reliability over prolonged training periods. A major concern
is the substantial loss of training time caused by inevitable hardware and
software failures. To address these challenges, we present FlashRecovery, a
fast and low-cost failure recovery system comprising three core modules: (1)
Active and real-time failure detection. This module performs continuous
training state monitoring, enabling immediate identification of hardware and
software failures within seconds, thus ensuring rapid incident response; (2)
Scale-independent task restart. By employing different recovery strategies for
normal and faulty nodes, combined with an optimized communication group
reconstruction protocol, our approach ensures that the recovery time remains
nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery
within one step. Our novel recovery mechanism enables single-step restoration,
completely eliminating dependence on traditional checkpointing methods and
their associated overhead. Collectively, these innovations enable FlashRecovery
to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective
(RPO), substantially improving the reliability and efficiency of long-duration
LLM training. Experimental results demonstrate that FlashRecovery system can
achieve training restoration on training cluster with 4, 800 devices in 150
seconds. We also verify that the time required for failure recovery is nearly
consistent for different scales of training tasks.

</details>


### [79] [The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies](https://arxiv.org/abs/2509.03104)
*Leonid Kondrashov,Boxi Zhou,Hancheng Wang,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: 这篇论文通过建立开源服务器无代码系统模拟商业平台的自动扩缩行为，揭示了服务器无代码系统在性能成本交换方面的关键发现，包括计算和内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 服务器无代码计算正在改变云应用开发，但缺乏开放、跨平台的性能测试标准和详细系统分析，导致对控制平面设计的性能成本交换理解不深入。

Method: 设计了一个能够近似AWS Lambda和Google Cloud Run等商业平台扩缩行为的服务器无代码系统，通过重现实际工作负载和变化关键自动扩缩参数来系统比较同步和异步自动扩缩策略的性能和成本效果。采用了结合实际控制平面部署和大规模模拟的混合方法。

Result: 发现服务器无代码系统存在显著计算开销（实例替换导致10-40%的CPU周期消耗）和高内存分配（比实际使用量高2-10倍）。减少这些开销通常会导致性能显著降低。

Conclusion: 当前的自动扩缩策略在性能成本交换方面存在显著缺陷，突出了开发新型成本效率更高的自动扩缩策略的必要性。开源系统能够复现商业平台特性，为可复现实验提供了可能。

Abstract: Serverless computing is transforming cloud application development, but the
performance-cost trade-offs of control plane designs remain poorly understood
due to a lack of open, cross-platform benchmarks and detailed system analyses.
In this work, we address these gaps by designing a serverless system that
approximates the scaling behaviors of commercial providers, including AWS
Lambda and Google Cloud Run. We systematically compare the performance and
cost-efficiency of both synchronous and asynchronous autoscaling policies by
replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the
operational characteristics of commercial platforms, enabling reproducible and
transparent experimentation. By evaluating how autoscaling parameters affect
latency, memory usage, and CPU overhead, we reveal several key findings. First,
we find that serverless systems exhibit significant computational overhead due
to instance churn equivalent to 10-40% of the CPU cycles spent on request
handling, primarily originating from worker nodes. Second, we observe high
memory allocation due to scaling policy: 2-10 times more than actively used.
Finally, we demonstrate that reducing these overheads typically results in
significant performance degradation in the current systems, underscoring the
need for new, cost-efficient autoscaling strategies. Additionally, we employ a
hybrid methodology that combines real control plane deployments with
large-scale simulation to extend our evaluation closer to a production scale,
thereby bridging the gap between small research clusters and real-world
environments.

</details>


### [80] [Efficient and Secure Sleepy Model for BFT Consensus](https://arxiv.org/abs/2509.03145)
*Pengkun Ren,Hai Dong,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 通过结合PVSS预提交机制的BFT协议，在动态可用系统中实现了低延迟（4δ）和高容锐性（可承受1/2恶意节点）的平衡


<details>
  <summary>Details</summary>
Motivation: 解决动态可用系统中BFT共识协议在延迟性和安全性之间的争议，现有方案多需多轮投票导致高延迟或容锐性不足

Method: 将公开可验证秘密分享(PVSS)与预提交机制集成到消息传输中，通过PVSS绑定用户身份和消息，减少通信轮数

Result: 在常见场景中仅需4次网络延迟，可承受至1/2的恶意参与者，显著减少分叉发生并提高链稳定性，在中等参与波动场景中保持低延迟和稳定性

Conclusion: 该协议通过PVSS与预提交机制的集成，在不抑制整体性的前提下显著提升了BFT协议的效率和安全性，适用于动态可用系统的实际部署

Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available
systems face a critical challenge: balancing latency and security in
fluctuating node participation. Existing solutions often require multiple
rounds of voting per decision, leading to high latency or limited resilience to
adversarial behavior. This paper presents a BFT protocol integrating a
pre-commit mechanism with publicly verifiable secret sharing (PVSS) into
message transmission. By binding users' identities to their messages through
PVSS, our approach reduces communication rounds. Compared to other
state-of-the-art methods, our protocol typically requires only four network
delays (4$\Delta$) in common scenarios while being resilient to up to 1/2
adversarial participants. This integration enhances the efficiency and security
of the protocol without compromising integrity. Theoretical analysis
demonstrates the robustness of the protocol against Byzantine attacks.
Experimental evaluations show that, compared to traditional BFT protocols, our
protocol significantly prevents fork occurrences and improves chain stability.
Furthermore, compared to longest-chain protocol, our protocol maintains
stability and lower latency in scenarios with moderate participation
fluctuations.

</details>


### [81] [CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload](https://arxiv.org/abs/2509.03394)
*Amirhossein Shahbazinia,Darong Huang,Luis Costero,David Atienza*

Main category: cs.DC

TL;DR: CloudFormer是一个基于Transformer的双分支模型，用于在云环境中预测虚拟机性能退化，在黑色盒子环境下实现了7.8%的平均绝对误差，比现有方法提升至少28%。


<details>
  <summary>Details</summary>
Motivation: 多租户云环境中虚拟机共享物理资源导致性能干扰问题，现有管理技术需要准确的性能预测，但在公有云黑色盒子环境中面临挑战。

Method: 提出CloudFormer双分支Transformer模型，联合建模时间动态和系统级交互，利用206个系统指标在1秒分辨率下工作，适应静态和动态场景。

Result: 实验结果显示CloudFormer在多个评估指标上持续优于最先进基线方法，实现了7.8%的MAE，比现有方法提升至少28%，并在未见工作负载上表现出强大泛化能力。

Conclusion: CloudFormer能够有效捕获瞬时干扰效应并适应不同工作负载条件，无需场景特定调优，为云环境性能预测提供了有效解决方案。

Abstract: Cloud platforms are increasingly relied upon to host diverse,
resource-intensive workloads due to their scalability, flexibility, and
cost-efficiency. In multi-tenant cloud environments, virtual machines are
consolidated on shared physical servers to improve resource utilization. While
virtualization guarantees resource partitioning for CPU, memory, and storage,
it cannot ensure performance isolation. Competition for shared resources such
as last-level cache, memory bandwidth, and network interfaces often leads to
severe performance degradation. Existing management techniques, including VM
scheduling and resource provisioning, require accurate performance prediction
to mitigate interference. However, this remains challenging in public clouds
due to the black-box nature of VMs and the highly dynamic nature of workloads.
To address these limitations, we propose CloudFormer, a dual-branch
Transformer-based model designed to predict VM performance degradation in
black-box environments. CloudFormer jointly models temporal dynamics and
system-level interactions, leveraging 206 system metrics at one-second
resolution across both static and dynamic scenarios. This design enables the
model to capture transient interference effects and adapt to varying workload
conditions without scenario-specific tuning. Complementing the methodology, we
provide a fine-grained dataset that significantly expands the temporal
resolution and metric diversity compared to existing benchmarks. Experimental
results demonstrate that CloudFormer consistently outperforms state-of-the-art
baselines across multiple evaluation metrics, achieving robust generalization
across diverse and previously unseen workloads. Notably, CloudFormer attains a
mean absolute error (MAE) of just 7.8%, representing a substantial improvement
in predictive accuracy and outperforming existing methods at least by 28%.

</details>
