<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [PSketch: A Priority-Aware Sketch Architecture for Real-Time Flow Monitoring via eBPF](https://arxiv.org/abs/2509.07338)
*Yuanjun Dai,Qingzhe Guo,Xiangren Wang*

Main category: cs.ET

TL;DR: PSketch是基于eBPF的首个内核级优先级感知草图框架，通过哈希表无损追踪高优先级流，使用草图管道近似识别大象流，在10Gbps流量下实现96%检测精度和0.7%吞吐量损失


<details>
  <summary>Details</summary>
Motivation: SDN中的基于草图的监控存在管道和内存约束紧密耦合的问题，限制了算法灵活性并降低了准确性

Method: 提出PSketch框架：1）使用哈希表实现高优先级流的无损追踪 2）采用草图管道近似识别top-k大象流 3）支持TCP/UDP协议 4）实现内核级重传追踪 5）基于eBPF技术，可在普通Linux系统运行

Result: 在10Gbps CAIDA流量测试中：1）达到96.0%的top-k检测准确率 2）实现96.4%的重传召回率 3）仅产生0.7%的吞吐量性能下降

Conclusion: PSketch成功解决了SDN草图监控的约束问题，提供了硬件无关的高性能流量监控解决方案，在保持高精度的同时实现了极低的性能开销

Abstract: Sketch-based monitoring in SDN often suffers from tightly coupled pipeline
and memory constraints, limiting algorithmic flexibility and reducing accuracy.
We propose PSketch, the first in-kernel priority-aware sketching framework
implemented with eBPF. It ensures lossless tracking of high-priority flows via
a hash-based table and approximates top-k elephant flows using a sketch pipe.
PSketch supports both TCP and UDP and enables in-kernel retransmission tracking
with minimal overhead. Unlike SDN-based approaches, it runs on commodity Linux
systems, removing hardware dependencies. We perform evaluation on 10 Gbps CAIDA
traces. Results show that PSketch achieves 96.0% top-k detection accuracy,
96.4% retransmission recall, and only 0.7% throughput degradation.

</details>


### [2] [Gut-Brain Axis as a Closed-Loop Molecular Communication Network](https://arxiv.org/abs/2509.07911)
*Beyza E. Ortlek,Ozgur B. Akan*

Main category: cs.ET

TL;DR: 该论文提出了一个基于分子通信的肠脑轴数学模型，通过六个耦合的非线性延迟微分方程描述双向反馈机制，分析了在持续压力下系统从健康节律向病理性高皮质醇状态的转变及其对信息传输能力的影响。


<details>
  <summary>Details</summary>
Motivation: 建立定量分析框架来研究肠脑轴中的信息传递机制，特别是理解在长期压力下系统如何从稳定状态转变为病理状态，以及这种转变对信息处理能力的影响。

Method: 采用六个耦合的非线性延迟微分方程构建肠脑轴模型，包含肠道到大脑的炎症通道和大脑到肠道的神经内分泌通道。通过时域模拟、小信号频域表征和信息论容量分析来评估系统性能。

Result: 在稳态下系统保持稳定的昼夜节律动态并具有较高的信息吞吐量；持续压力导致系统转向失调的高皮质醇状态，此时频谱效率降低（有效带宽变窄、通带增益降低）。

Conclusion: 该模型量化了信号机制对系统稳定性和信息处理的影响，阐明了从健康昼夜节律向持续性高皮质醇病理状态的转变机制，为理解肠脑轴功能障碍提供了定量分析工具。

Abstract: Molecular communication (MC) provides a quantitative framework for analyzing
information transfer within biological systems. This paper introduces a novel
and comprehensive MC framework for the gut-brain axis (GBA) as a system of six
coupled, nonlinear delay differential equations (DDEs). The proposed model
defines a bidirectional feedback loop with a gut-to-brain inflammatory channel
and a brain-to-gut neuroendocrine channel. Under prolonged stress, this
feedback loop becomes self-perpetuating and drives the system into a
pathological state. We evaluate the end-to-end channel across varying
conditions using time-domain simulations, small-signal frequency-domain
characterization, and an information-theoretic capacity analysis. At
homeostasis, the system maintains stable circadian dynamics with higher
information throughput, whereas sustained stress drives a shift to dysregulated
hypercortisolism. In this pathological state, spectral efficiency decreases due
to a narrowed effective bandwidth and a lower passband gain driven by
neuroendocrine delays and saturating cytokine-hormone kinetics. These results
quantify the impact of these signaling mechanisms on stability and information
processing, elucidating the transition from healthy circadian rhythms to a
persistent pathological state of hypercortisolism.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [HYLU: Hybrid Parallel Sparse LU Factorization](https://arxiv.org/abs/2509.07690)
*Xiaoming Chen*

Main category: cs.AR

TL;DR: HYLU是一个混合并行LU分解求解器，在共享内存多核架构上高效求解稀疏线性系统，相比Intel MKL PARDISO在数值分解阶段性能提升1.74-2.26倍


<details>
  <summary>Details</summary>
Motivation: 针对多核共享内存架构设计高效的稀疏线性系统求解器，需要适应不同稀疏模式的系数矩阵

Method: 采用混合数值核的集成方法，结合并行LU分解技术，能够自适应处理各种稀疏模式

Result: 在SuiteSparse Matrix Collection的34个稀疏矩阵测试中，数值分解阶段性能比Intel MKL PARDISO提升1.74倍（单次求解）和2.26倍（重复求解）

Conclusion: HYLU是一个高效通用的稀疏线性系统求解器，在共享内存多核架构上表现出优异的性能，代码已开源

Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 提出RIGEO算法，结合改进金鹰优化和强化学习，在雾计算中优化IoT任务调度，降低能耗并提升服务质量


<details>
  <summary>Details</summary>
Motivation: IoT设备激增需要低延迟响应，雾计算面临资源分配和任务调度挑战，需要设计高效算法来降低能耗并满足任务截止时间要求

Method: 将雾节点按流量分为高低两类：低流量节点使用改进金鹰优化算法(IGEO)处理低截止时间任务，高流量节点使用强化学习(RL)处理高截止时间任务，组合成RIGEO算法

Result: 实验结果表明，相比现有先进算法，所提算法在系统响应时间、总截止时间违反时间、资源和系统能耗方面都有优化

Conclusion: RIGEO算法通过分类调度策略有效解决了雾计算环境中的IoT任务调度问题，在能耗和服务质量方面表现出色

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [5] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Crossword是一种灵活的共识协议，针对动态数据密集型工作负载设计，通过智能分片分配和擦除编码显著减少关键路径数据传输，在动态场景下性能比传统协议提升2.3倍


<details>
  <summary>Details</summary>
Motivation: 云环境中复制负载大小跨度大且带来间歇性带宽压力，现有协议静态分片分配无法适应动态工作负载和网络条件

Method: 采用每实例擦除编码，智能分发编码分片，支持分片分配和仲裁大小的自适应权衡，使用惰性跟随者八卦机制处理领导者故障转移

Result: 在静态场景下与传统协议性能相当，在动态工作负载和网络条件下性能提升达2.3倍，与CockroachDB集成使TPC-C聚合吞吐量提高1.32倍

Conclusion: Crossword协议在保持经典协议可用性保证的同时，有效应对动态数据密集型工作负载挑战，显著提升性能

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [6] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Bodega是第一个能够在任何节点本地提供线性化读取的共识协议，通过新型的roster leases算法实现，相比现有方法在WAN集群中读取速度提升5.6-13.1倍


<details>
  <summary>Details</summary>
Motivation: 解决现有共识协议无法在任何节点本地提供线性化读取的问题，特别是在存在写入干扰的情况下

Method: 采用roster leases算法，这是一种全对全租赁机制，通过维护roster（集群元数据）来跟踪任意副本子集作为本地读取的响应节点，同时使用乐观保持和早期接受通知来最小化写入干扰

Result: 在真实WAN集群中，Bodega相比现有方法（Leader Leases、EPaxos、PQR、Quorum Leases）将平均客户端读取请求速度提升5.6-13.1倍，写入性能相当，支持快速主动roster变更和容错

Conclusion: Bodega通过创新的roster leases机制成功实现了在任何节点本地提供线性化读取的目标，为共识协议设计开辟了新的设计空间，具有优异的性能和实用性

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [7] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 研究通过RabbitMQ消息框架模拟Deleria和LCLS科学工作流的数据流媒体需求，分析了消息参数配置对吞吐量和可靠性的影响，为用户提供最优流媒体配置建议


<details>
  <summary>Details</summary>
Motivation: 现代科学工作流需要内存到内存数据流媒体来支持近实时分析和实验控制，应用成熟消息框架是可行解决方案，但需要优化配置来满足低延迟、高吞吐量和可靠性要求

Method: 使用RabbitMQ消息框架在OLCF的HPC基础设施中进行流媒体模拟，采用来自Deleria和LCLS工作流的综合工作负载

Result: 模拟得出了关键观察结果和实用见解，帮助用户理解哪些配置能最好满足其流媒体工作负载需求

Conclusion: 通过细致的消息参数配置分析和优化，可以有效提升科学工作流中内存到内存数据流媒体的性能和可靠性

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [8] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: DREAMS是一个去中心化的微服务部署框架，通过Raft共识算法和成本效益投票实现跨计算域的协作优化，特别适用于现代制造业的动态计算需求。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要能够响应高度动态工作负载和定制化生产需求的自适应计算基础设施。传统集中式解决方案存在扩展性差、延迟瓶颈和单点故障等问题，无法有效应对计算连续体中的多利益相关方场景。

Method: 提出DREAMS去中心化框架，在每个计算域中部署自主运行的代理，通过Raft共识算法和成本效益投票机制进行全局协调，实现隐私保护和容错协调。

Result: DREAMS在现代制造环境中实现了全局优化的服务部署，同时保持高容错性。关键协调操作（如LDM注册和迁移投票）随域数量呈次线性扩展，证明了框架的高效性和可扩展性。

Conclusion: DREAMS框架为计算连续体中的微服务部署提供了有效的去中心化解决方案，能够满足现代制造业对响应性、隐私保护和容错性的要求，具有良好的扩展性能。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [9] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: DuoServe-MoE是一个针对MoE模型推理优化的服务系统，通过分离预填充和解码阶段并采用定制化专家调度策略，显著提升推理效率并降低内存使用。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活专家分支保持推理计算效率，但大量专家权重带来了巨大的GPU内存压力，特别是在单GPU服务器等资源受限环境中。同时，MoE推理包含预填充阶段（密集激活）和解码阶段（稀疏激活）两个不同阶段，统一的调度策略会导致延迟和内存使用不理想。

Method: 提出DuoServe-MoE系统，明确分离预填充和解码阶段：1）预填充阶段使用双流CUDA管道，重叠专家权重预取和非MoE层计算；2）解码阶段使用轻量级层级预测器离线预测最可能激活的专家进行预取，无需修改模型。

Result: 在4位Mixtral-8x7B和8x22B模型上的实验显示，DuoServe-MoE将端到端延迟提升了1.42到7.54倍，同时将峰值内存使用保持在完整模型大小的仅15%。

Conclusion: DuoServe-MoE通过阶段分离和定制化调度策略，有效解决了MoE模型推理中的内存压力和延迟问题，为资源受限环境下的高效MoE推理提供了实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [10] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 提出基于依赖感知的执行模型来优化Hyperledger Fabric性能，通过依赖标记、优化区块构建、DAG表示依赖关系和并行执行，在Fabric v2.5中实现吞吐量提升40%并显著降低拒绝率


<details>
  <summary>Details</summary>
Motivation: Hyperledger Fabric在企业级应用中面临高负载下交易吞吐量低和拒绝率高的问题，主要由于背书、排序和验证阶段的瓶颈，以及乐观并发控制和延迟验证导致的资源低效和竞争

Method: 1) 背书阶段依赖标记系统，使用哈希表标记交易独立性；2) 排序服务优化区块构建，优先处理独立交易；3) 区块内集成DAG表示依赖关系；4) 提交节点并行执行独立交易，按DAG顺序处理依赖交易

Result: 在Fabric v2.5中测试显示，不同依赖级别和系统负载下，吞吐量最高提升40%，高竞争场景下拒绝率显著降低

Conclusion: 依赖感知调度和基于DAG的执行能显著提升Fabric的可扩展性，同时保持与现有共识和智能合约层的兼容性

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [11] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Astra是首个基于LLM的多智能体系统，专门用于GPU内核优化，从现有CUDA实现而非PyTorch模块出发，通过多智能体协作实现代码生成、测试和分析，平均获得1.32倍加速。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对LLM训练和服务至关重要，但传统方法需要大量手动调优。现有编译器系统仍需大量人工设计，而之前的LLM方法主要关注从PyTorch到CUDA的代码转换。

Method: 构建多智能体LLM系统Astra，从SGLang框架提取现有CUDA实现，通过专门的LLM智能体进行迭代式代码生成、测试、性能分析和规划，实现自动优化。

Result: 在SGLang内核上，使用OpenAI o4-mini进行零样本提示，平均获得1.32倍加速。案例研究表明LLM能自主应用循环变换、优化内存访问模式、利用CUDA内置函数和快速数学运算。

Conclusion: 多智能体LLM系统为GPU内核优化提供了新的有前景的范式，能够自动实现显著的性能提升。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [12] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 这篇论文探索了通过变容量策略管理HPC系统能源成本的方法，建立了一个简单模型来估计这种策略对总所有成本的影响。


<details>
  <summary>Details</summary>
Motivation: 能源成本是HPC系统总所有成本的主要因素，间息性绿色能源的出现和电力市场的波动性给能源预算带来了挑战。

Method: 提出了一个简单的模型，帮助运营商使用关键系统参数估计变容量策略对总所有成本的影响，并将模型应用于大学HPC集群的实际数据。

Result: 评估了不同场景下变容量策略在未来的成本效益情况，为HPC系统能源成本管理提供了量化分析工具。

Conclusion: 变容量策略作为管理HPC能源成本的有效方法，虽然存在硬件利用率问题，但通过建模分析可以进行成本效益评估。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [13] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: AgentX是一种新型智能体工作流模式，通过设计器、规划器和执行器三个智能体组件，结合MCP工具和FaaS部署方式，在复杂多步骤任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI智能体系统在处理多工具、复杂多步骤任务和长上下文管理时存在困难，需要更有效的工作流模式来提升决策能力和避免幻觉。

Method: 提出AgentX工作流模式，包含阶段设计器、规划器和执行器三个智能体组件，利用MCP工具协议，并探索了两种FaaS云函数部署方式。

Result: 通过三个实际应用的实证评估，AgentX在成功率、延迟和成本方面都优于或与最先进的ReAct和Magentic One模式相当。

Conclusion: AgentX模式为设计和部署智能体工作流提供了新的机会，同时也揭示了相关挑战，在复杂任务处理方面展现出竞争优势。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [14] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: TRAM是一个专为共享内存系统设计的原子多播协议，采用覆盖树架构，性能显著优于现有协议


<details>
  <summary>Details</summary>
Motivation: 原子多播是可靠系统中的关键通信原语，但现有协议主要针对消息传递系统，共享内存系统上的协议较少且性能不足

Method: 设计基于覆盖树架构的TRAM协议，专门针对共享内存系统进行优化，采用简单实用的设计

Result: 相比最先进的共享内存协议，吞吐量提升3倍以上，延迟降低2.3倍以上；相比消息传递协议，吞吐量提升最高5.9倍，延迟降低最高106倍

Conclusion: TRAM协议在共享内存系统中实现了卓越的性能表现，证明了其设计的有效性和实用性

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可解释的两阶段适应性空间-时间模型，用于预测睡眠质量评分。模型结合多尺度卷积、递归层、注意机制和域适配策略，在各种预测窗口下都超越了基线方法，最佳RMSE为0.216。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量影响健康生活质量，需要可靠的预测工具进行预防性干预。现有方法在个人化适应和模型可解释性方面有限。

Method: 提出两阶段适应性空间-时间模型：1)多尺度卷积层模式化空间相互作用；2)递归层和注意机制捕捉长期时间依赖；3)两阶段域适配策略（训练时减少过拟合，测试时无标签适应新用户）

Result: 在5种输入窗口和5种预测窗口下都超越了LSTM、Informer、PatchTST、TimesNet等基线方法。最佳绩效（3天输入+1天预测）RMSE=0.216，甚至在长期预测中也保持良好性能（3天预测RMSE=0.257）。可解释性分析显示了不同特征对睡眠质量的影响。

Conclusion: 该框架为使用商业可穿戴设备的稀疏数据进行个人化睡眠预测提供了稳健、适应性强且可解释的解决方案。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [16] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出lmKANs作为线性层的替代方案，通过可训练低维多变量函数实现高维映射，显著降低推理计算成本同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 高维线性映射在现代深度学习模型中占据大量参数和计算成本，需要寻找更高效的替代方案

Method: 使用查找表实现的多变量样条函数来表达高维映射，每个函数包含数十到数百个可训练参数，通过少量乘法计算

Result: lmKANs减少推理FLOPs达6.0倍，在CIFAR-10和ImageNet-1k上分别降低1.6-2.1倍和1.7倍FLOPs，在同等精度下实现10倍以上H100吞吐量

Conclusion: lmKANs提供了容量与推理成本之间的更好权衡，是线性层的有效替代方案，代码已开源

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [17] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: GSTBench是首个系统评估图自监督学习方法跨数据集迁移能力的基准测试，发现大多数方法迁移效果不佳，而GraphMAE表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法主要在单一数据集上开发和评估，缺乏对跨数据集迁移性的系统研究，限制了知识迁移和大规模预训练的应用。

Method: 在ogbn-papers100M上进行大规模预训练，评估5种代表性自监督学习方法在多样化目标图上的表现，标准化实验设置以消除混杂因素。

Result: 大多数图自监督学习方法迁移效果差，甚至不如随机初始化；GraphMAE（掩码自编码器方法）在迁移性能上表现一致优异。

Conclusion: 研究揭示了图自监督学习迁移性的关键因素，为图学习的"预训练-迁移"范式奠定了基础，并指导未来可迁移图自监督学习方法的研究。

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [18] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 这篇论文提出了M3T联邦基础模型（FedFMs）的概念，通过结合多模态多任务基础模型与联邦学习，解决教育领域中的隐私保护、数据异构和领域数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 实际教育环境中多模态多任务基础模型部署遇到隐私法规、数据异构和领域特定数据稀缺的挑战，需要一种能够在保护隐私的前提下进行协作学习的方案。

Method: 提出M3T联邦基础模型（FedFMs）方法，将联邦学习（FL）与多模态多任务（M3T）基础模型相结合，支持在去中心化机构之间进行协作、隐私保护的训练，同时支持多模态和多任务。

Result: 论文提出了M3T FedFMs在教育领域的三大关键优势：隐私保护（保持敏感数据本地化）、个性化（通过模块化架构实现定制模型）、公平包容（促进资源受限实体参与）。

Conclusion: M3T FedFMs是一种有前景但尚未充分探索的方法，需要解决多个研究挑战包括：机构间异质性隐私规定、数据模态特征不均匀性、模型删除方法、持续学习框架和模型可解释性等问题。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [19] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 提出了一种结合交通时序数据和人类知识文本数据的知识引导跨模态特征表示学习模型(KGCM)，用于交通需求预测，通过图网络和跨模态融合机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时序数据，缺乏对人类知识和经验的利用。现实中人类交通知识和经验对精确预测有重要影响，能帮助模型发现数据中的潜在模式。

Method: 构建先验知识数据集，使用大语言模型结合人工编写修订；设计局部和全局自适应图网络学习多模态特征；采用跨模态特征融合机制；提出基于推理的动态更新策略优化图模型参数。

Result: 在多个交通数据集上的实验表明，该模型能准确预测未来交通需求，性能优于现有最先进模型。

Conclusion: 将结构化时序交通数据与表示人类知识的文本数据相结合，通过知识引导的跨模态特征学习，能够有效提升交通需求预测的准确性和鲁棒性。

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [20] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 该论文提出了一个配置先行的框架，用于评估深度学习系统在CPU、GPU和编译运行时之间的跨后端兼容性。通过672次测试发现运行通过率为72%，并提出了减少偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在不同运行后端（CPU、GPU、编译运行时）上存在的输出偏差问题，提供系统化的评估和减缓方法以确保多异构部署的可靠性。

Method: 使用YAML配置文件将实验与代码解耦，支持库和仓库模型，采用三层验证协议（张量级近似度、激活对齐、任务级指标）进行评估。

Result: 在672次检查中，72.0%的运行通过测试，大多数差异出现在更严格的宽容度设置下。检测模型和编译后端更容易出现偏差，通常因非确定性后处理导致。确定性适配器和选择性备送方案可显著提高一致性而无显著性能损失。

Conclusion: 该框架是首个系统化量化和减缓深度学习跨后端偏差的统一方法，为异构运行时环境中的可靠部署提供了可复现的方法论。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [21] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于Kriging-HDMR的主动学习代理模型方法，用于解决高维可靠性分析问题，通过多阶段建模和优化采样策略实现高效计算和准确预测。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型在高维随机变量下存在"维度灾难"问题，现有HDMR方法多用于优化问题，而可靠性分析需要重点关注临界区域的预测精度，缺乏专门研究。

Method: 采用三阶段建模框架：单变量子代理模型构建、耦合变量需求识别、耦合变量子代理模型构建；基于各阶段特点设计优化数学模型进行实验设计样本选择，采用无候选样本池方法选择信息丰富样本。

Result: 数值实验表明，该方法在解决高维可靠性问题时具有高计算效率，同时保持了强大的预测准确性。

Conclusion: 所提出的Kriging-HDMR主动学习方法有效解决了高维可靠性分析问题，在计算效率和预测精度方面都表现出色，为高维可靠性工程问题提供了有效的解决方案。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [22] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的公共交通到站时间预测方法NSATP，通过平衡预测性和非稳态性来提高多步预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 多步到站时间预测中，非稳态数据会降低模型性能，但传统的标准化方法可能会消除有用的非稳态特征，导致过度稳态化问题。

Method: 方法包含两个阶段：时间序列稳态化和非稳态性效应恢复。第一阶段提高预测性，第二阶段通过二维模型捕捉隔期性并学习缩放和偏移因子来补偿过度稳态化。

Result: 在德紫顿公共交通数据上验证，与基线方法相比，NSATP在有轨电车和公交车上分别减少了RMSE、MAE和MAPE指标，准确性显著提高。

Conclusion: NSATP方法能够有效平衡预测性和非稳态性的关系，通过保留有用的非稳态特征来提高多步到站时间预测的性能。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [23] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: RLFactory是一个即插即用的强化学习后训练框架，通过异步调用器和解耦架构解决多轮工具使用的稳定性和适应性，在Search-R1基准上超越更大模型表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基础推理方面表现出色，但在需要与外部工具交互的任务上表现不佳，需要解决多轮工具使用的挑战

Method: 采用异步调用器和解耦工具/训练架构，引入观察标记重建MDP，实现生成-解析-调用-更新的动态策略优化工作流

Result: 在Natural Questions数据集上获得0.486测试分数，超越Qwen2.5-7B-Instruct-GRPO等更大模型，训练吞吐量提升6.8倍

Conclusion: RLFactory提供了一个低门槛、高适应性的框架，可有效增强LLM在现实场景中的多轮工具使用能力

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [24] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: CARE框架通过实时安全监控、回滚机制和自省干预策略，在解码时实现安全对齐，在安全性和响应质量之间取得优越平衡


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的部署增加，确保解码时输出的安全性成为关键挑战，现有方法往往在安全性和响应质量之间做出严重权衡

Method: 提出CARE框架，包含三个核心组件：(1)守卫模型进行实时安全监控；(2)带令牌缓冲区的回滚机制早期纠正不安全输出；(3)基于自省的干预策略，模型生成对先前输出的自我反思批判并纳入上下文指导后续解码

Result: 实验结果表明该框架在安全性、质量和效率方面达到优越平衡，有害响应率低，对用户体验干扰最小，同时保持高响应质量

Conclusion: CARE框架通过精确的守卫模型干预、及时的回滚纠正和有效的自省自校正，实现了安全性和响应质量之间的优越权衡

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [25] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: FediLoRA是一个联邦多模态微调框架，解决了异构LoRA秩和缺失模态的问题，通过维度聚合和模型编辑提升性能


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法假设统一秩配置和单模态输入，忽略了客户端资源异构性和多模态数据中可能缺失模态的现实挑战

Method: 提出维度聚合策略重新加权LoRA更新，避免信息稀释；引入轻量级层级模型编辑方法，选择性整合全局参数修复本地组件

Result: 在三个多模态基准数据集上，FediLoRA在全局和个性化设置中均优于基线方法，特别是在模态不完整的情况下

Conclusion: FediLoRA有效解决了联邦多模态学习中的异构LoRA秩和缺失模态问题，为实际部署提供了实用解决方案

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [26] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 使用双向LSTM网络学习逆向映射，从流行病时间序列预测SIR模型参数，实现高效准确的流行病模型检定


<details>
  <summary>Details</summary>
Motivation: 传统的近似贝叶斯计算法计算成本高，需要快速准确的流行病模型检定方法

Method: 使用三层双向LSTM网络，输入60天发病率、人口规模和恢复率，输出传播概率、接触率和R0。训练中使用包含流行病学一致性罚项的复合损失函数

Result: 在1000个场景模拟中，方法在所有目标参数上错误更低（MAE：R0 0.0616 vs 0.275），生成更紧凑的预测区间，并将每次检定时间从77.4秒缩短到2.35秒

Conclusion: 该方法虽然存在部分参数不可识别性，但能更准确地重现流行病曲线，实现了快速应用的流行病模型检定

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [27] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 本文提出了一个基于离散事件模拟的简单时间顺序DRL环境用于柔性作业车间调度问题，并基于PPO算法构建了端到端DRL调度模型，通过新的状态表示和奖励函数设计，在公开基准实例上取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法主要关注DRL调度智能体的设计，而忽视了DRL环境的建模，因此需要开发一个简单有效的DRL环境来提升柔性作业车间调度问题的求解效果。

Method: 基于离散事件模拟构建时间顺序DRL环境，使用近端策略优化(PPO)算法建立端到端DRL调度模型，提出基于两个状态变量的短状态表示方法，并设计基于机器调度区域的可理解奖励函数。

Result: 在公开基准实例上的实验结果表明，简单优先级调度规则在该环境中的性能得到提升，所提出的DRL调度模型相比OR-Tools、元启发式、DRL和PDR调度方法具有竞争性性能。

Conclusion: 本文提出的简单DRL环境和端到端调度模型为柔性作业车间调度问题提供了有效的解决方案，证明了环境建模在DRL调度中的重要性，并在多个对比方法中展现出竞争优势。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [28] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT是一个针对异构和演化客户端数据的联邦学习框架，通过表示学习和进化聚类两阶段方法，在保持数据本地化的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在现实部署中因客户端数据分布随时间演化和跨客户端显著差异导致的性能下降问题，同时应对高资源成本和隐私担忧。

Method: 采用两阶段过程：第一阶段客户端学习本地模型提取特征表示；第二阶段服务器基于这些表示动态聚类客户端，并协调集群级别的任务特定模型训练。

Result: 理论分析了表示学习阶段，实证证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性。

Conclusion: Fed-REACT框架有效解决了联邦学习中的异构性和数据演化问题，为下游分类或回归任务提供了优越的性能表现。

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [29] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 使用单位二进制参数的二进归一化层，可以将神经网络的内存需求减少32倍，同时保持类似的性能表现。


<details>
  <summary>Details</summary>
Motivation: 大型神经网络模型的部署面临着内存需求大和计算效率低的挑战，需要开发更高效的模型缩减技术。

Method: 开发了一种新型的二进归一化层，所有参数（权重和偏置）都只有0和1两种值。这种层可以是全连接、卷积、注意力等任何类型。

Result: 在图像分类和语言模型任务上，二进归一化模型表现出与32位浮点数模型相似的性能，同时内存占用减少32倍。

Conclusion: 二进归一化层为大型神经网络模型提供了一种新的解决方案，可以在普通CPU和移动设备上部署，无需专门硬件支持。

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [30] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi是一种针对联邦边缘学习中动态数据演化的时间漂移和分歧感知调度算法，通过联合调度和带宽分配实现快速收敛


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多假设静态数据集，但现实场景中客户端数据具有时变和非独立同分布特性，需要及时高效地适应这种演化数据

Method: 提出FedTeddi算法，使用时序漂移和集体分歧量化数据动态特性，基于EMD距离建立优化目标，开发联合调度和带宽分配算法

Result: 在CIFAR-10和CIFAR-100数据集上比随机调度分别提升58.4%和49.2%的收敛速度，获得更高的测试精度

Conclusion: FedTeddi能有效处理联邦边缘学习中的动态数据演化问题，实现快速收敛并保持先前知识

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [31] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 提出了一种递归扩展的线性PASFA方法，用于从观测数据和模型中推断ARMA过程的状态（慢特征），解决了现有方法难以恢复原始状态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率自适应慢特征分析方法将慢特征建模为ARMA过程的状态，但需要开发高效的方法从观测数据和模型中推断这些状态。当前方法使用卡尔曼滤波但难以恢复原始状态。

Method: 提出了线性PASFA的递归扩展算法，对遵循ARMA过程的状态进行MMSE估计，给定观测数据和模型。

Result: 在合成数据集上评估了所提技术的正确性。

Conclusion: 该方法能够有效地从观测数据和模型中推断ARMA过程的状态，解决了现有方法难以恢复原始状态的问题。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [32] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 提出了一种简约贝叶斯框架，仅对感兴趣参数（如最优位置）设置先验，通过轮廓似然消除冗余参数，可处理复杂结构约束。基于此开发了MINTS算法，适用于结构化问题和经典凸优化算法分析。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法需要对所有参数建立概率模型，这限制了复杂结构约束的融入。需要一种更简约的框架来克服这一局限性。

Method: 采用简约贝叶斯框架，仅对目标参数设置先验，通过轮廓似然消除冗余参数。开发了MINTS（简约汤普森采样）算法作为具体实现。

Result: 框架能够处理结构化问题（如连续臂Lipschitz赌博机和动态定价），为经典凸优化算法提供概率视角，并在多臂赌博机问题上实现了接近最优的遗憾保证。

Conclusion: 该简约贝叶斯框架成功解决了传统方法在处理复杂约束时的局限性，提供了更灵活且理论保证良好的序列决策方法。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [33] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 提出使用误差有界压缩算法压缩MoE模型中的非激活专家，减少GPU内存与主存间的数据传输开销，并分析不同层次专家压缩误差对推理精度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型在LLM学习中的广泛应用，在有限GPU内存约束下高效服务MoE模型成为重要挑战。将非激活专家卸载到主内存是有效方法，但带来了专家数据传输的挑战，需要探索高效的专家压缩方法并分析压缩误差对推理性能的影响。

Method: 采用误差有界损失压缩算法（如SZ3和CuSZp）压缩非激活专家，通过大量实验分析不同专家层次压缩误差对整体推理精度的影响。

Result: 实验表明：浅层专家（主要负责注意力机制和输入标记向量化）在误差有界压缩下推理精度下降最小；中间层专家（模型推理核心）的误差会显著损害推理精度；深层专家（主要负责指令跟随和输出整合）的有界误差有时反而能提高推理精度。

Conclusion: 误差有界压缩是减少MoE模型数据传输开销的有效方法，但需要针对不同层次专家采用差异化的压缩策略，浅层和深层专家对压缩误差容忍度较高，而中间层专家需要更严格的误差控制。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [34] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 结合因果发现和不确定性感知预测的金融时间序列分析方法，应用于美国宏观经济指标，发现经济增长对GDP的单向因果联系，并利用大语言模型进行零样本失业率预测


<details>
  <summary>Details</summary>
Motivation: 研究宏观经济指标间的动态因果关系，并探索无需任务特定训练的概率预测方法，为经济政策提供信息并增强预测稳健性

Method: 使用LPCMCI框架和GPDC方法分析1970-2021年季度数据，发现因果结构；利用Chronos大语言模型进行零样本概率预测

Result: 发现经济增长到GDP的稳健单向因果联系，通胀连接性有限；失业率预测在1-2个季度前准确，提供90%置信区间，支持异常检测

Conclusion: 因果结构学习与概率语言模型结合具有重要价值，可为经济政策制定提供信息并提升预测稳健性

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [35] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 本研究系统比较了CNN和视觉Transformer在光伏热故障检测中的性能，通过XRAI显著性分析验证模型决策与热物理原理的一致性，Swin Transformer表现最佳，为能源监测AI决策提供了可解释性验证方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能在光伏监测部署中存在可解释性障碍，虽然深度学习在热故障检测中达到高精度，但缺乏模型决策与热物理原理一致性的验证，导致在需要理解模型推理的关键应用中部署犹豫。

Method: 使用卷积神经网络（ResNet-18、EfficientNet-B0）和视觉Transformer（ViT-Tiny、Swin-Tiny）进行热光伏故障检测，采用XRAI显著性分析评估与热物理原理的一致性。

Result: 在20,000张红外图像评估中，Swin Transformer达到最高性能（94%二元准确率；73%多类准确率）。XRAI分析显示模型学习了物理有意义的特征，但不同故障类型性能差异显著：电气故障检测强（F1分数>0.90），而污染等环境因素仍具挑战性（F1分数0.20-0.33）。

Conclusion: 热物理引导的可解释性方法为验证能源监测应用中AI决策提供了方法论，解决了可再生能源基础设施中的部署障碍。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [36] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: GyroBN是一个基于陀螺群结构的黎曼批量归一化框架，将欧几里得批量归一化扩展到非欧几里得流形数据上


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得归一化层不适用于流形数据，而许多机器学习中的黎曼流形具有陀螺结构，需要一种理论上有保证的黎曼批量归一化方法

Method: 提出了GyroBN框架，建立了伪约简和陀螺等距陀螺两个必要条件来保证理论控制，并在七个代表性几何结构上实例化该方法

Result: 实验证明GyroBN在这些几何结构上有效，且框架包含了多个现有黎曼归一化方法作为特例

Conclusion: GyroBN提供了一个理论上有保证的黎曼批量归一化通用框架，适用于具有陀螺结构的各种流形数据

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [37] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: TabGFM是一个将图学习问题重新表述为表格问题的图基础模型框架，通过特征和结构编码器将图转换为表格，应用多个表格基础模型进行零样本节点分类，并通过集成选择聚合结果。


<details>
  <summary>Details</summary>
Motivation: 现有的图基础模型(GFMs)在训练数据集上对真实世界图的代表性不足，限制了泛化性能。而表格基础模型(TFMs)不仅在表格预测任务中表现出色，还在其他领域显示出强大适用性。

Method: 将节点分类重新表述为表格问题，每个节点作为一行，特征、结构和标签信息作为列。使用特征和结构编码器将图转换为表格，应用多个TFMs对多样化子采样表格进行零样本学习，最后通过集成选择聚合输出。

Result: 在28个真实世界数据集上的实验表明，TabGFM在任务特定GNNs和最先进GFMs基础上实现了持续改进。

Conclusion: 表格重新表述为可扩展和可泛化的图学习提供了有前景的途径，展示了TFMs在图学习领域的潜力。

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [38] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的指标EICS，用于量化Transformer电路的一致性和可靠性，结合了套层不一致性和因果出现性的理论基础


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一种形式化的单次扫描方法来量化Transformer电路的一致性行为，以判断其可靠性

Method: 基于套层/同调论和因果出现理论，提出EICS指标，结合本地雅克比矩阵的标准化套层不一致性和基于向前状态的高斯EI代理

Result: 设计了白盒化、单次扫描的方法，提供了指标解释、计算开销分析和植物检验，实验验证尚未进行

Conclusion: EICS为机制解释性领域提供了一种新的形式化工具，能够单次扫描地评估Transformer电路的一致性和可靠性

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [39] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: PLaID++是一个基于大语言模型的晶体结构生成方法，通过Wyckoff文本表示和强化学习优化，显著提高了稳定性和新颖性晶体的生成效率


<details>
  <summary>Details</summary>
Motivation: 传统材料发现过程缓慢且昂贵，需要加速新材料开发流程，特别是针对太阳能电池、电池和碳捕获等关键技术领域

Method: 使用Qwen-2.5 7B模型进行微调，采用基于Wyckoff的文本表示法生成晶体结构，并通过基于Direct Preference Optimization的强化学习技术指导生成过程

Result: 相比先前方法，PLaID++生成热力学稳定、独特且新颖结构的比率提高了约50%，在无条件生成和空间群条件生成方面分别实现了约115%和50%的改进

Conclusion: 该工作展示了将自然语言处理的后训练技术应用于材料设计的潜力，为靶向高效发现新材料开辟了新途径

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [40] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 提出基于机器学习的模块化方法，利用药物-通路权重影响评分和患者数据预测未测试药物的疗效


<details>
  <summary>Details</summary>
Motivation: 药物研发中临床前疗效预测困难，需要新方法来利用真实世界数据和药物嵌入信息预测未测试药物的治疗效果

Method: 使用药物-通路权重影响评分和患者临床数据训练机器学习模型，分析未测试药物在生物分子-蛋白质通路上的加权影响评分来预测疗效

Result: 在真实世界患者治疗数据集上验证了方法的有效性，使用了两种不同的权重影响评分算法，并展示了在未见治疗上的泛化性能

Conclusion: 该方法提供了一个可迭代的初始框架，支持未来利用真实世界临床数据和药物嵌入来预测未测试药物效果的研究工作

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [41] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 该论文分析了训练后量化(PTQ)对少数群体的不成比例影响，提出了混合精度量化认知训练(QAT)结合数据集采样和权重损失函数的解决方案。


<details>
  <summary>Details</summary>
Motivation: 训练后量化虽然压缩效果好速度快，但会加剧对少数群体的不公平影响，需要找到减缓这种偏差的方法。

Method: 分析量化过程中权重和激活值变化对网络的链式影响，研究梯度范数和Hessian矩阵特征值，并提出混合精度QAT结合数据采样和权重损失函数的方案。

Result: 量化导致logits方差降低、损失增加、群体准确率受损，对少数群体影响更大，提出的方法可有效减缓这些偏差效应。

Conclusion: 量化会加剧模型偏见问题，通过混合精度QAT结合偏差罚减技术可实现更公平的量化模型部署。

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [42] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 这篇论文通过系统化参数优化研究，在数学推理任务中对五个状态起海模型进行微调，实现了计算成本降低29.4%和推理速度提升23.9%，保持了100%的优化成功率。


<details>
  <summary>Details</summary>
Motivation: 数学推理任务需要高效的模型配置，但现有研究缺乏对多种SOTA模型的系统化参数优化研究，需要一个标准化的生产导向优化框架。

Method: 通过系统化搜索参数空间（温度0.1-0.5、推理步6-12、规划周期1-4、核采样0.85-0.98），在数学推理基准测试上对Qwen2.5-72B、Llama-3.1-70B、DeepSeek-V3、Mixtral-8x22B和Yi-Lightning进行优化配置确定。

Result: 平均计算成本降低29.4%，推理速度提升23.9%，100%优化成功率。DeepSeek-V3达到最高准确率98%，Mixtral-8x22B以每个准确响应361.5个标记的成本效益最优。低温度区域（0.1-0.4）和减少推理步数（4-6）能够在不影响准确性的前提下提高效率。

Conclusion: 该研究提供了一个标准化的生产导向参数优化框架，发现了跨模型架构的普遍优化趋势，并为五个SOTA模型提供了生产准备就绪的优化配置，在保持解决方案正确性的同时实现了显著的性能提升。

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [43] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: IP-Basis PINNs是一种元学习框架，通过离线-在线分解方法解决物理信息神经网络在多查询逆问题中的计算效率问题，实现了快速参数识别和解决方案重建。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在多查询逆问题场景中计算成本高昂，每个新的观测数据集都需要重新进行昂贵的训练过程，需要一种更高效的推理方法。

Method: 采用离线-在线分解策略：离线训练深度网络生成参数化微分方程解空间的基函数集，在线冻结网络仅训练轻量级线性输出层，结合前向模式自动微分和新型在线损失函数。

Result: 在三个不同基准测试中表现出色，包括扩展到未知函数项的通用PINNs，在常数和函数参数估计中表现一致，相比标准PINNs显著加速，在稀缺和噪声数据下稳健运行。

Conclusion: IP-Basis PINNs框架有效解决了逆问题中的计算效率问题，为多查询场景提供了快速、高效的解决方案，具有重要的实际应用价值。

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [44] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: GCond是一种基于PCGrad原理的多任务学习梯度冲突解决方法，通过梯度累积和自适应仲裁机制实现计算加速和性能提升


<details>
  <summary>Details</summary>
Motivation: 多任务学习中梯度冲突是一个重要挑战，现有方法如PCGrad、CAGrad和GradNorm计算成本高，限制了其在大模型中的应用

Method: 提出Gradient Conductor (GCond)方法，结合PCGrad原理、梯度累积和自适应仲裁机制，在随机模式下实现两倍计算加速

Result: 在ImageNet 1K和头颈部CT数据集上，GCond在所有评估指标上表现优异，L1和SSIM损失更低，成功应用于MobileNetV3-Small和ConvNeXt等不同规模架构

Conclusion: GCond为多任务学习中的梯度冲突问题提供了可扩展且高效的解决方案，与现代优化器兼容性好

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [45] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出了一种基于变分贝叶斯推理的鲁棒框架，从噪声稀疏的相空间数据中无监督学习广义哈密顿动力学，能够同时处理保守、耗散和端口哈密顿系统。


<details>
  <summary>Details</summary>
Motivation: 传统哈密顿网络模型难以从观测的相空间轨迹中捕捉不同类型物理系统的独特动力学特性，需要一种能够统一处理保守、耗散和端口哈密顿系统的学习框架。

Method: 扩展稀疏辛随机傅里叶高斯过程学习，结合哈密顿景观的预测性数值估计，使用广义状态和共轭动量哈密顿动力学形式。通过核化ELBO损失保证数据保真度，并加入稳定性和守恒约束作为正则化项。

Result: 该方法能够从噪声稀疏数据中准确学习多种哈密顿动力学，具有有界不确定性，提高了预测精度和物理正确性。

Conclusion: 该框架为学习复杂哈密顿流形提供了有效的无监督方法，能够统一处理不同类型的物理系统动力学，在保持物理正确性的同时实现高精度预测。

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [46] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: 提出了ALICE架构，一个简单的编码器Transformer，在密码破解任务中实现了最先进的准确性和速度，仅需训练1500个独特密码就能泛化到未见过的密码。


<details>
  <summary>Details</summary>
Motivation: 将密码破解作为研究神经网络在组合复杂领域中泛化能力的理想测试平台，探索模型如何从极小样本中学习组合结构。

Method: 开发ALICE架构，使用编码器Transformer和基于Gumbel-Sinkhorn方法的双射解码头来显式建模排列，实现可解释的密码映射提取。

Result: ALICE在准确性和速度上都达到了新的最先进水平，仅用1500个密码（占可能密码空间的3.7×10^{-24}）就能泛化到未见过的密码。

Conclusion: 该架构和分析方法可扩展到任何具有双射映射和组合结构的领域，为神经网络泛化和可解释性提供了新见解。

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [47] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 使用LLM机器人自动生成非小细肺癌病人的NCCN指南一致治疗方案，通过混合人工注释咆模型一致性创建高准确度的预测框架咆验证系统。


<details>
  <summary>Details</summary>
Motivation: 将复杂的病人情况转换为指南符合的治疗建议需要专业知识且耗时，容易出错，需要LLM技术来提高效率咆准确性。

Method: 构建121例NSCLC病人纵向数据集，专家注解NCCN指南路径；利用LLM预测相关指南；采用混合人工注释咆模型一致性的方法创建预测框架咆验证系统。

Result: LLM在预测指南方面与专家注释强相关（Spearman r=0.88）；治疗建议验证系统准确度高（AUROC=0.800），能够提供检查信度分数。

Conclusion: 该研究建立了一个在准确性、可解释性咆监管要求之间取得平衡的临床可行LLM指南遵循系统，为自动化临床决策支持提供了可扩展的途径。

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [48] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 提出了一个通用人口统计预训练模型(GDP)，专注于学习和表示年龄与性别特征，通过顺序排序策略提升医疗风险预测性能


<details>
  <summary>Details</summary>
Motivation: 人口统计属性在电子健康记录中普遍存在且是重要的临床风险预测因子，但现有模型往往将其作为辅助特征，缺乏专门的表示学习方法

Method: 开发GDP预训练框架，探索不同的排序策略和编码方法，将表格化人口统计数据转换为潜在嵌入表示，并在不同疾病和人群数据集上进行预训练和评估

Result: 顺序排序策略显著提升了模型在判别、校准和决策树分裂信息增益方面的性能，特别是在年龄和性别对风险分层贡献显著的疾病中；即使在预测价值较低的数据集中，GDP也能增强表示重要性

Conclusion: 面向表格化人口统计属性的基础模型能够跨任务和人群泛化，为改善医疗健康应用的预测性能提供了有前景的方向

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [49] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: SBS方法通过单向排序平滑和自适应频率调制技术，有效抑制隐式神经表示中的频谱偏差，在更少参数下实现更好的神经网络权重重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络权重隐式表示方法存在明显的频谱偏差问题，难以有效重建高频细节，限制了参数压缩效果。

Method: 提出SBS方法：1）单向排序平滑技术改善输出空间核平滑性；2）基于单向排序平滑的自适应随机傅里叶特征，根据层间参数数量动态调节输入编码频率带宽。

Result: 在ResNet模型和CIFAR-10、CIFAR-100、ImageNet数据集上的广泛评估表明，SBS相比SOTA方法以更少参数获得显著更好的重建精度。

Conclusion: SBS通过有效抑制频谱偏差，为神经网络权重的隐式表示提供了参数效率更高的增强方案，在参数压缩和重建精度方面均有显著提升。

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [50] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 基于EfficientNet深度学习模型和数字双生系统的新案框，用于心脏停搏的早期检测和分析，通过心血管图像特征学习和个体化模型建立，提高预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 心脏停搏是全球重大健康问题，早期识别和管理对改善患者预后至关重要。需要主动个体化的方法来预测心脏疾病。

Method: 结合EfficientNet深度学习模型和数字双生系统，使用复合缩放技术学习心血管图像特征，通过IoT设备数据建立个体化心血管系统模型。

Result: 实验结果显示该系统具有高准确的预测能力，同时保持高效率。

Conclusion: 深度学习与数字双生技术的结合为实现主动个体化心脏疾病预测提供了可能性，在心脏停搏早期检测方面展现出良好效果。

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [51] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出混合GCN-GRU模型检测区块链非法交易，结合图结构和时序特征，在比特币数据上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络具有复杂的时变模式和节点间关系，传统方法难以有效检测非法活动

Method: 使用图卷积网络(GCN)捕捉结构特征，门控循环单元(GRU)捕捉时序特征，构建混合模型

Result: 在2020-2024年真实比特币交易数据上，模型达到0.9470准确率和0.9807 AUC-ROC，优于所有基线方法

Conclusion: GCN-GRU混合模型能有效结合结构和时序信息，在区块链非法交易检测中表现出色

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [52] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: 提出EMORF-II，一种基于学习的异常值鲁棒滤波器，能够处理相关测量噪声并学习异常特征


<details>
  <summary>Details</summary>
Motivation: 现有滤波器在处理相关测量噪声和异常值方面存在局限，需要一种能够同时进行异常检测和学习异常特征的增强方法

Method: 基于EM算法的异常值鲁棒滤波器增强版本，具备学习异常特征的能力，在推理过程中同时进行异常检测和学习

Result: 数值实验显示相比最先进方法在精度上有性能提升，但计算开销增加，计算复杂度与其他实用方法相当

Conclusion: EMORF-II是一个有用的选择，具有改进的异常缓解能力，适用于各种应用场景

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [53] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 提出了DPH-RL框架，通过使用mass-covering f-divergences（如前向KL和JS散度）作为排练机制，解决RLVR微调中多尝试性能下降和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR目标缺乏知识保留机制，反向KL散度会加速知识衰减，而无散度项则无法防止模型偏离其多样化知识库。

Method: 使用mass-covering f-divergences作为排练机制，持续参考初始策略，强制模型保持广泛的解决方案覆盖范围。

Result: 在数学和SQL生成任务上，DPH-RL不仅解决了Pass@k性能下降问题，还同时提升了Pass@1和Pass@k性能，且训练效率更高。

Conclusion: 散度度量的正确选择是构建更通用和多样化推理模型的有力工具，为改进RLVR提供了一个关键但被忽视的方向。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [54] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 一种新的卷积自动编码器结构，能够同时处理显式评分和隐式反馈，学习用户与物品间的多种关联关系，并提供了泛化界限证明。


<details>
  <summary>Details</summary>
Motivation: 解决现有推荐系统中无法同时处理显式评分和隐式反馈的问题，并提供更信息丰富的预测（如识别用户可能喜欢但不会自然消费的物品）。

Method: 使用卷积自动编码器结构，能够同时学习不同交互类型的关联关系，并分别预测消费概率和高评分概率。提供了泛化界限证明。

Result: 在多个实际数据集上达到了现有最优性能，同时在隐式和显式反馈预测任务上都表现优异，还获得了更好的可解释性。

Conclusion: 该模型通过单一结构就能处理多种反馈类型，提供了更丰富的预测信息和更好的可解释性，为推荐系统中的自动编码器提供了理论基础。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [55] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 通过无监督对比学习分类用户消费行为，结合小波变换卷积网络和交叉注意机制进行短期水需预测，在真实DMA区域进行了6个月的验证，MAPE指标最大提升4.9%，同时识别了受社会经济因素影响的用户行为。


<details>
  <summary>Details</summary>
Motivation: 智慧计量技术的进步提高了水资源监测能力，但水需预测仍面临气象等非确定性因素的挑战。在气候变化带来的不确定性增加背景下，保障水资源和供应成为了全球急需解决的问题。

Method: 1）采用无监督对比学习对DMA内的终端用户按消费行为进行分类 2）将不同消费行为作为特征用于需求预测任务 3）使用小波变换卷积网络结合交叉注意机制，结合历史数据和得到的表征进行需求预测

Result: 在真实世界的DMA区域进行了6个月的评估，证明方法在不同DMA中都实现了更好的预测性能（MAPE指标），最大提升达4.9%。同时能够识别出那些消费行为受社会经济因素影响的用户。

Conclusion: 该研究提出的新题方法能够有效提高短期水需预测的准确性，并且能够揭示用户消费行为中的确定性模式，为水资源管理提供了更深入的见解。

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [56] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: RoseCDL是一个可扩展且鲁棒的卷积字典学习算法，用于长信号中的无监督罕见事件检测，通过随机窗口化和在线异常检测解决计算成本和异常敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 卷积字典学习(CDL)在建模信号局部结构方面很强大，但在检测罕见或异常事件方面应用不足，面临高计算成本和对异常值敏感两大挑战。

Method: RoseCDL结合随机窗口化实现大数据集高效训练，并通过在线异常检测增强鲁棒性，隔离异常模式。

Result: 将CDL重新定位为事件发现和特征提取的实用工具，扩展了其在压缩或去噪等传统任务之外的作用。

Conclusion: RoseCDL为大规模信号中的罕见事件检测提供了一个可扩展且鲁棒的解决方案，使CDL成为现实世界信号分析中的实用工具。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [57] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: 提出了ΔL归一化方法，专门针对RLVR中动态生成长度特性设计的损失聚合方法，解决了梯度方差大和优化不稳定的问题


<details>
  <summary>Details</summary>
Motivation: RLVR在提升大语言模型推理能力方面显示强大潜力，但训练中响应长度的大幅变化导致高梯度方差和不稳定优化，现有方法存在偏差估计或高方差问题

Method: 通过理论分析和实证研究，将问题重新表述为寻找最小方差无偏估计量，提出ΔL归一化方法

Result: 大量实验表明，该方法在不同模型大小、最大长度和任务上都能获得优异结果

Conclusion: ΔL归一化不仅提供真实策略损失的无偏估计，还在理论上最小化梯度方差，是解决RLVR训练不稳定问题的有效方案

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [58] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: uGMM-NN是一种新型神经网络架构，将单变量高斯混合模型嵌入神经元中，替代传统的加权求和加非线性激活函数，能够捕捉多模态性和不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络神经元使用固定的非线性激活函数，缺乏对不确定性和多模态性的建模能力。uGMM-NN旨在将概率推理直接嵌入神经网络的计算单元中，使神经元能够表达更丰富的概率表示。

Method: 每个uGMM-NN节点将其激活参数化为单变量高斯混合模型，具有可学习的均值、方差和混合系数，同时保持标准前馈网络的可扩展性。

Result: uGMM-NN在判别性能上与传统多层感知机相当，同时提供了激活的概率解释，能够捕捉神经元层面的多模态性和不确定性。

Conclusion: 该框架为将不确定性感知组件集成到现代神经架构中奠定了基础，为判别性和生成性建模开辟了新方向。

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [59] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: 提出了PINNs的双重公式化方法，用于改进周期性热导复合材料均匀化的可靠性，通过推导保证的上下误差边界来增强PINNs在材料不连续性处理中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)在处理具有不连续系数（如分段常数属性）的材料时经常失败，需要更可靠的方法来解决多尺度建模中的偏微分方程问题。

Method: 引入双重公式化方法到PINN框架中，比较标准PINNs应用于平滑材料近似与变分PINNs(VPINNs)使用谱和神经网络基测试函数的效果。

Result: 强形式PINNs在受控设置中可能优于VPINNs，但对材料不连续性敏感且可能无明确诊断地失败；VPINNs能直接处理分段常数材料参数但需要仔细选择测试函数以避免不稳定性。

Conclusion: 双重公式化作为收敛质量的可靠指标，将其集成到PINN框架中增强了在微观力学均匀化问题中的适用性。

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [60] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 基于Transformer的深度学习策略优化半导体探针卡传感器布局，通过物理增强数据集训练混合CNN-Transformer模型，实现99.83%的健康状态分类准确率和99.73%的裂纹检测召回率。


<details>
  <summary>Details</summary>
Motivation: 探针卡的故障（如基板裂纹和螺丝松动）严重影响半导体制造良率和可靠性，需要有效的传感器布局来检测故障模式。

Method: 使用有限元模型模拟故障场景的频率响应函数，通过物理信息场景扩展和物理感知数据增强构建数据集，训练混合CNN-Transformer模型进行健康状态分类和传感器位置优化。

Result: 模型在探针卡健康状态分类（基线、螺丝松动、裂纹）上达到99.83%的准确率，裂纹检测召回率为99.73%，并通过3次10折分层交叉验证确认了模型鲁棒性。

Conclusion: 基于注意力的深度学习能够推进预测性维护，通过分析注意力权重优化传感器配置，提高半导体制造的运行可靠性和良率。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [61] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think是一个32B参数的推理系统，通过先进的训练后处理和测试时计算技术，在数学推理等任务上达到了与更大模型相当的最先进性能


<details>
  <summary>Details</summary>
Motivation: 证明较小参数模型通过集成先进的训练后配方和推理时增强技术，能够与最大型模型竞争，使开源推理系统更加可访问和经济实惠

Method: 基于六个关键技术支柱：长思维链监督微调、可验证奖励的强化学习、推理前的智能体规划、测试时缩放、推测解码和推理优化硬件，使用公开开源数据集

Result: 在开源模型的公共基准测试中达到最先进分数，数学推理表现优异，同时在代码和科学等领域也表现强劲，推理速度超过每秒2000个token

Conclusion: K2-Think 32B模型通过集成化的训练后配方和战略性的推理时增强，证明了参数效率更高的模型能够与最先进系统竞争，推动了开源推理系统的可访问性

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [62] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 这篇论文系统性评估了二元分类器在类不平衡情况下的表现，特别是在不使用重新平衡技术的情况下，采用实际和合成数据集进行测试。


<details>
  <summary>Details</summary>
Motivation: 类不平衡问题在超级监督分类中带来重大挑战，尤其是在医疗诊断和异常检测等关键领域。虽然有许多重新平衡技术研究，但少有研究关注在不使用这些技术时二元分类器的性能表现。

Method: 系统性评估多种二元分类器在少数类样本逐渐减少情况下的稳健性，使用一次学习和少量学习场景作为基准。通过合成决策边界生成模拟不同数据复杂度，包括了低采样、过采样策略和一类分类方法的实验。

Result: 结果显示随着数据复杂性增加和少数类样本量减少，分类变得更加困难。传统分类器在极端不平衡情况下性能恶化，而TabPFN和提升集成模型等先进模型保持了较高性能和更好的普适性。可视化解释性和评估指标进一步验证了这些发现。

Conclusion: 这项研究为不平衡学习中的模型选择提供了有价值的指导，揭示了在不依赖明确重新平衡技术的情况下分类器的稳健性特性。

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [63] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: 提出了图集成梯度(GB-IG)方法，将集成梯度扩展到图数据，解决了传统IG方法不适用于离散图结构的问题。


<details>
  <summary>Details</summary>
Motivation: 集成梯度(IG)方法假设数据是连续的，但图是离散结构，因此IG不适用于图数据。需要开发专门针对图结构的解释性方法。

Method: 开发了图集成梯度(GB-IG)方法，扩展了IG技术以适应图数据的离散特性。在四个合成数据集和三个真实世界图数据集上进行验证。

Result: 在合成数据集上，GB-IG能准确识别图中用于分类任务的关键结构组件。在真实数据集上，GB-IG在节点分类任务中比传统IG方法更能突出重要特征。

Conclusion: GB-IG方法成功将集成梯度技术扩展到图数据，为图神经网络提供了有效的解释性工具，在识别重要图结构特征方面表现优异。

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [64] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的邻层嵌入加速方法，在保持精细结构保存的同时实现高效计算，支持交互式数据探索和高维嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 解决现有邻层嵌入方法在速度和质量之间的争议：负样本采样粗略方法速度快但结构保存差，而精确方法质量好但速度慢且限制2-3维可视化。

Method: 提出一种新的迭代近似最近邻层搜索算法，每次迭代需要计算量小，支持超参数调整和高维嵌入，采用单阶段设计支持交互式数据探索。

Result: 通过GPU加速GUI集成进行实验，在速度、结构提取灵活性方面显示出了有前景的结果，并且可以通过最小算法修改应用于更广泛的机器学习场景。

Conclusion: 该方法成功在高效计算和精细结构保存之间取得了平衡，为交互式数据可视化和高维嵌入提供了一种有前景的解决方案。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [65] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出了IBN模型，通过不确定性感知插值和双向建模解决多元时间序列预测中的变量缺失问题，在多种缺失率场景下达到最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统时空图神经网络在处理变量缺失时难以建模变量间相关性，现有方法GinAR缺乏可解释性且无法捕捉潜在时间模式。

Method: 集成不确定性感知插值(UAI)和高斯核图卷积(GGCN)，使用MC Dropout估计重构值的不确定性，采用不确定性加权策略，双向递归单元增强时间依赖性建模。

Result: 在多种缺失率场景下实现了最先进的预测性能，提供了更可靠和可解释的框架。

Conclusion: IBN模型有效解决了多元时间序列预测中的变量缺失问题，提供了具有可解释性的高性能解决方案。

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [66] [Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models](https://arxiv.org/abs/2509.07813)
*Jonathan Teagan*

Main category: cs.LG

TL;DR: 这篇论文应用多种预测技术对乌克兰战争中俄罗斯设备损失进行模型预测，发现深度学习模型在高时间粒度条件下表现最佳，强调了集成预测和公开情报数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 通过对乌克兰战争中俄罗斯设备损失的预测模型研究，评估潜在趋势和未来损失模式，以提高冲突模型的准确性和可靠性。

Method: 采用ARIMA、Prophet、LSTM、TCN和XGBoost等多种预测技术，使用WarSpotting的每日和每月OSINT数据进行建模和预测，对比不同模型架构和输入结构的表现。

Result: 深度学习模型（特别是TCN和LSTM）在高时间粒度条件下产生了稳定且一致的预测结果，显示出超过其他方法的性能优势。

Conclusion: 研究强调了集成预测方法在冲突模型中的重要性，以及公开可用的OSINT数据在质量质量退化时间过程中的价值，为战争损失预测提供了可靠的技术支持。

Abstract: This study applies a range of forecasting techniques,including ARIMA,
Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional
Networks (TCN), and XGBoost, to model and predict Russian equipment losses
during the ongoing war in Ukraine. Drawing on daily and monthly open-source
intelligence (OSINT) data from WarSpotting, we aim to assess trends in
attrition, evaluate model performance, and estimate future loss patterns
through the end of 2025. Our findings show that deep learning models,
particularly TCN and LSTM, produce stable and consistent forecasts, especially
under conditions of high temporal granularity. By comparing different model
architectures and input structures, this study highlights the importance of
ensemble forecasting in conflict modeling, and the value of publicly available
OSINT data in quantifying material degradation over time.

</details>


### [67] [Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques](https://arxiv.org/abs/2509.07845)
*Mohammad Zana Majidi,Sajjad Karimi,Teng Wang,Robert Kluger,Reginald Souleyrette*

Main category: cs.LG

TL;DR: 本研究通过结合结构化事故数据和警察现场记录的非结构化事故叙述，使用TF-IDF和Word2Vec两种NLP技术提取语义信息，采用XGBoost等集成算法预测交通事故伤害严重程度，结果显示加入叙述数据的模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 预测交通事故中的伤害和死亡对提高道路安全、改善应急响应和指导公共卫生干预至关重要。研究探索非结构化事故叙述与结构化数据结合对伤害严重程度预测的附加价值。

Method: 使用TF-IDF和Word2Vec提取事故叙述语义特征，采用KNN过采样处理类别不平衡问题，基于三种道路分类方案（8个详细功能类别、4个广义配对类别、无分类统一数据集），结合XGBoost、随机森林和AdaBoost三种集成算法开发了102个机器学习模型。

Result: 包含叙述数据的模型始终优于仅使用结构化数据的模型。在所有组合中，TF-IDF与XGBoost在大多数子组中产生了最准确的预测结果。

Conclusion: 研究证明了整合文本和结构化事故信息在提升个人层面伤害预测能力方面的强大作用，为交通安全专业人员提供了实用的框架来改进事故严重程度建模、指导政策决策和设计更有效的对策。

Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role
in enhancing road safety, improving emergency response, and guiding public
health interventions. This study investigates the added value of unstructured
crash narratives (written by police officers at the scene) when combined with
structured crash data to predict injury severity. Two widely used Natural
Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency
(TF-IDF) and Word2Vec, were employed to extract semantic meaning from the
narratives, and their effectiveness was compared. To address the challenge of
class imbalance, a K-Nearest Neighbors-based oversampling method was applied to
the training data prior to modeling. The dataset consists of crash records from
Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three
road classification schemes were used: (1) eight detailed functional classes
(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four
broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and
(3) a unified dataset without classification. A total of 102 machine learning
models were developed by combining structured features and narrative-based
features using the two NLP techniques alongside three ensemble algorithms:
XGBoost, Random Forest, and AdaBoost. Results demonstrate that models
incorporating narrative data consistently outperform those relying solely on
structured data. Among all combinations, TF-IDF coupled with XGBoost yielded
the most accurate predictions in most subgroups. The findings highlight the
power of integrating textual and structured crash information to enhance
person-level injury prediction. This work offers a practical and adaptable
framework for transportation safety professionals to improve crash severity
modeling, guide policy decisions, and design more effective countermeasures.

</details>


### [68] [Addressing the Cold-Start Problem for Personalized Combination Drug Screening](https://arxiv.org/abs/2509.07850)
*Antoine de Mathelin,Christopher Tosh,Wesley Tansey*

Main category: cs.LG

TL;DR: 基于预训练深度学习模型的组合疗法个性化诊断策略，通过药物嵌入聚类和杂强度权重机制提高初始屏幕效率


<details>
  <summary>Details</summary>
Motivation: 解决个性化组合疗法中的冷启动问题，在无病人特异性信息时选择最信息丰富的药物组合进行初始实验

Method: 利用历史药物响应数据预训练深度学习模型，生成药物组合嵌入和杂强度重要性评分，结合聚类保证功能多样性，按权重优先选择权重高的杂强度

Result: 在大规模药物组合数据集上的回顾性模拟显示，该方法显著提高了初始屏幕效率，超过基线方法

Conclusion: 提供了一种可行的初期决策路径，为个性化组合药物屏幕的更有效早期决策创造条件

Abstract: Personalizing combination therapies in oncology requires navigating an
immense space of possible drug and dose combinations, a task that remains
largely infeasible through exhaustive experimentation. Recent developments in
patient-derived models have enabled high-throughput ex vivo screening, but the
number of feasible experiments is limited. Further, a tight therapeutic window
makes gathering molecular profiling information (e.g. RNA-seq) impractical as a
means of guiding drug response prediction. This leads to a challenging
cold-start problem: how do we select the most informative combinations to test
early, when no prior information about the patient is available? We propose a
strategy that leverages a pretrained deep learning model built on historical
drug response data. The model provides both embeddings for drug combinations
and dose-level importance scores, enabling a principled selection of initial
experiments. We combine clustering of drug embeddings to ensure functional
diversity with a dose-weighting mechanism that prioritizes doses based on their
historical informativeness. Retrospective simulations on large-scale drug
combination datasets show that our method substantially improves initial
screening efficiency compared to baselines, offering a viable path for more
effective early-phase decision-making in personalized combination drug screens.

</details>


### [69] [Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy](https://arxiv.org/abs/2509.07872)
*Yajun Yu,Steve Jiang,Robert Timmerman,Hao Peng*

Main category: cs.LG

TL;DR: 基于多组学数据的支持向量回归模型用于预测脱射疗法中脱射病变化，结合影像组学和涉量组学特征，取得了较好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测脱射疗法中病变的连续变化对于预后评估和个性化治疗具有重要价值，特别是在PULSAR这种新型脱射治疗方式中。

Method: 采用39名患者的69个脑转移症的影像组学和涉量组学特征，计算delta特征描述相对变化，使用Lasso算法进行特征选择，构建支持向量回归模型，采用5折交叉验证。

Result: 多组学模型表现超过单一组学模型，delta影像组学特征显著提升预测准确性，最佳模型达到R2 0.743和RRMSE 0.022。

Conclusion: 该多组学SVR模型能够准确预测GTV连续变化，为PULSAR疗法提供了量化和个性化的病人选择和治疗调整帮助。

Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)
is a novel treatment that delivers radiation in pulses of protracted intervals.
Accurate prediction of gross tumor volume (GTV) changes through regression
models has substantial prognostic value. This study aims to develop a
multi-omics based support vector regression (SVR) model for predicting GTV
change. A retrospective cohort of 39 patients with 69 brain metastases was
analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.
Delta features were computed to capture relative changes between two time
points. A feature selection pipeline using least absolute shrinkage and
selection operator (Lasso) algorithm with weight- or frequency-based ranking
criterion was implemented. SVR models with various kernels were evaluated using
the coefficient of determination (R2) and relative root mean square error
(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate
the limitation of small data size. Multi-omics models that integrate radiomics,
dosiomics, and their delta counterparts outperform individual-omics models.
Delta-radiomic features play a critical role in enhancing prediction accuracy
relative to features at single time points. The top-performing model achieves
an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows
promising performance in predicting continuous change of GTV. It provides a
more quantitative and personalized approach to assist patient selection and
treatment adjustment in PULSAR.

</details>


### [70] [A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges](https://arxiv.org/abs/2509.07887)
*Katherine Berry,Liang Cheng*

Main category: cs.LG

TL;DR: 这篇论文全面综述了图神经网络在药物发现领域的应用，涵盖了分子性质预测、药物-靶点结合亲和力预测、药物相互作用研究等多个研究类别，并为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 由于图神经网络能够处理药物分子等图结构数据，在药物发现领域得到了广泛应用，产生了大量方法和模型，需要对这些研究进行系统性的综述和总结。

Method: 通过文献综述的方法，全面收集和分析近期发表的图神经网络在药物发现各研究类别中的应用论文，包括分子性质预测、药物-靶点结合预测、药物相互作用研究等。

Result: 系统梳理了图神经网络在药物发现多个关键领域的应用现状，包括分子性质预测、药物-靶点结合亲和力预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等。

Conclusion: 该综述为图神经网络在药物发现领域的未来发展提供了重要指导，总结了当前研究进展并指出了未来研究方向。

Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of
drug discovery because of their ability to process graph-structured data such
as drug molecule models. This approach has resulted in a myriad of methods and
models in published literature across several categories of drug discovery
research. This paper covers the research categories comprehensively with recent
papers, namely molecular property prediction, including drug-target binding
affinity prediction, drug-drug interaction study, microbiome interaction
prediction, drug repositioning, retrosynthesis, and new drug design, and
provides guidance for future work on GNNs for drug discovery.

</details>


### [71] [Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings](https://arxiv.org/abs/2509.07896)
*Philipp Lepold,Jonas Leichtle,Tobias Röddiger,Michael Beigl*

Main category: cs.LG

TL;DR: 研究表明单通道耳内电生理信号可用于自动睡眠分期，在可穿戴设备中实现90.5%的二元睡眠检测准确率和65.1%的四分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统EEG睡眠监测设备笨重不便，限制了在家庭环境等真实场景中的应用。耳内EEG设备可以提供无创、舒适的长期睡眠监测方案

Method: 使用11名参与者进行睡眠研究，采用定制耳塞设备（干式耳尖电极）采集单通道耳内电生理信号，以Apple Watch Ultra的睡眠分期作为ground truth

Result: 系统在留一法交叉验证中达到90.5%的二元睡眠检测准确率（清醒vs睡眠）和65.1%的四分类准确率（清醒、REM、核心睡眠、深度睡眠）

Conclusion: 耳内电极作为一种低负担、舒适的睡眠监测方法具有巨大潜力，可用于如用户入睡时自动暂停媒体播放等消费级应用

Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which
are accurate but obtrusive and impractical for everyday use outside sleep
laboratories. This limits applicability in real-world settings, such as home
environments, where continuous, long-term monitoring is needed. Detecting sleep
onset is particularly relevant, enabling consumer applications (e.g.
automatically pausing media playback when the user falls asleep). Recent
research has shown correlations between in-ear EEG and full-scalp EEG for
various phenomena, suggesting wearable, in-ear devices could allow unobtrusive
sleep monitoring. We investigated the feasibility of using single-channel
in-ear electrophysiological (ExG) signals for automatic sleep staging in a
wearable device by conducting a sleep study with 11~participants (mean age:
24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)
as a measurement electrode in one ear and a reference in the other. Ground
truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep
staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake
vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)
using leave-one-subject-out validation. These findings demonstrate the
potential of in-ear electrodes as a low-effort, comfortable approach to sleep
monitoring, with applications such as stopping podcasts when users fall asleep.

</details>


### [72] [A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization](https://arxiv.org/abs/2509.07901)
*Qing-xin Meng,Xia Lei,Jian-wei Liu*

Main category: cs.LG

TL;DR: 提出了一种模块化算法解决在线凸凹优化问题，通过自适应模块、多预测器聚合器和集成模块实现最小化动态对偶间隙，达到了近乎最优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法在平稳或可预测环境中无法提供最优性能，特别是在处理动态对偶间隙(D-DGap)这一关键性能指标时表现不佳。

Method: 设计了三模块算法：自适应模块动态调整非平稳性水平，多预测器聚合器选择最佳预测器，集成模块有效结合各组件优势。

Result: 算法实现了最小化最优的D-DGap上界（达到对数因子），同时确保预测误差驱动的D-DGap边界，实证结果验证了方法的有效性。

Conclusion: 模块化设计允许灵活替换适应动态环境的组件，并能整合来自多个预测器的"侧边知识"，为在线凸凹优化提供了有效的解决方案。

Abstract: This paper investigates the problem of Online Convex-Concave Optimization,
which extends Online Convex Optimization to two-player time-varying
convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),
a critical performance measure that evaluates players' strategies against
arbitrary comparator sequences. Existing algorithms fail to deliver optimal
performance, particularly in stationary or predictable environments. To address
this, we propose a novel modular algorithm with three core components: an
Adaptive Module that dynamically adjusts to varying levels of non-stationarity,
a Multi-Predictor Aggregator that identifies the best predictor among multiple
candidates, and an Integration Module that effectively combines their
strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a
logarithmic factor, while also ensuring prediction error-driven D-DGap bounds.
The modular design allows for the seamless replacement of components that
regulate adaptability to dynamic environments, as well as the incorporation of
components that integrate ``side knowledge'' from multiple predictors.
Empirical results further demonstrate the effectiveness and adaptability of the
proposed method.

</details>


### [73] [Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings](https://arxiv.org/abs/2509.07905)
*Hamid Ahmad,Heiko Paulheim,Rita T. Sousa*

Main category: cs.LG

TL;DR: Bio-KGvec2go是一个扩展的Web API，用于生成和提供生物医学本体的知识图谱嵌入，支持定期更新以匹配本体版本发布，旨在促进高效及时的生物医学研究。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体在现代AI应用中日益重要，但将语义资源与机器学习模型集成需要知识图谱嵌入模型。预训练模型可以避免重复训练，有助于AI开发的民主化和可持续计算。

Method: 扩展KGvec2go Web API，设计Bio-KGvec2go来生成和服务广泛使用的生物医学本体的知识图谱嵌入，并支持与本体版本发布对齐的定期更新。

Result: Bio-KGvec2go提供了最新的嵌入，用户只需最小的计算努力即可获得，从而促进了高效和及时的生物医学研究。

Conclusion: Bio-KGvec2go通过提供易于访问和更新的生物医学本体嵌入，支持了AI开发的民主化和可持续计算，为生物医学研究提供了重要工具。

Abstract: Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.

</details>


### [74] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文主张通过逆问题方法来发现LLM的缩放定律，以更经济高效地指导大语言模型的构建


<details>
  <summary>Details</summary>
Motivation: 由于训练大语言模型成本高昂，传统的试错方法不可行，需要寻找更高效的方法来发现指导模型构建的缩放定律

Method: 借鉴逆问题在揭示基础科学定律方面的成功经验，提出使用逆问题方法来高效发现LLM的缩放定律

Result: 该方法有望以显著更好的成本效益实现期望的性能目标

Conclusion: 逆问题方法可以为LLM的构建提供更经济高效的指导，替代传统的试错方法

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


### [75] [One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](https://arxiv.org/abs/2509.07945)
*Yuan Pu,Yazhe Niu,Jia Tang,Junyu Xiong,Shuai Hu,Hongsheng Li*

Main category: cs.LG

TL;DR: ScaleZero通过混合专家架构和动态参数缩放策略，解决了异构多任务学习中的梯度冲突和模型可塑性损失问题，在多个基准测试中达到与专用单任务模型相当的性能，且只需80%的交互步骤。


<details>
  <summary>Details</summary>
Motivation: 传统多任务世界模型在处理大规模异构环境时存在梯度冲突和模型可塑性损失问题，限制了样本和计算效率。

Method: 采用混合专家(MoE)架构缓解梯度冲突，并提出基于LoRA的动态参数缩放(DPS)策略，根据任务进度动态调整计算负载。

Result: 在Atari、DMControl和Jericho基准测试中，ScaleZero仅使用在线强化学习就达到与专用单任务基线相当的性能，配合DPS策略后只需80%的交互步骤。

Conclusion: ScaleZero展示了在大规模多任务学习中有效处理异构任务的潜力，为多任务强化学习提供了新的解决方案。

Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse
observation and action spaces but also vary substantially in intrinsic
difficulty. While conventional multi-task world models like UniZero excel in
single-task settings, we find that when handling large-scale heterogeneous
environments, gradient conflicts and the loss of model plasticity often
constrain their sample and computational efficiency. In this work, we address
these challenges from two perspectives: the single learning iteration and the
overall learning process. First, we investigate the impact of key design spaces
on extending UniZero to multi-task planning. We find that a Mixture-of-Experts
(MoE) architecture provides the most substantial performance gains by
mitigating gradient conflicts, leading to our proposed model,
\textit{ScaleZero}. Second, to dynamically balance the computational load
across the learning process, we introduce an online, LoRA-based \textit{dynamic
parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA
adapters in response to task-specific progress, enabling adaptive knowledge
retention and parameter expansion. Empirical evaluations on standard benchmarks
such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying
exclusively on online reinforcement learning with one model, attains
performance on par with specialized single-task baselines. Furthermore, when
augmented with our dynamic parameter scaling strategy, our method achieves
competitive performance while requiring only 80\% of the single-task
environment interaction steps. These findings underscore the potential of
ScaleZero for effective large-scale multi-task learning. Our code is available
at \textcolor{magenta}{https://github.com/opendilab/LightZero}.

</details>


### [76] [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)
*Oliver Daniels,Stuart Armstrong,Alexandre Maranhão,Mahirah Fairuz Rahman,Benjamin M. Marlin,Rebecca Gorman*

Main category: cs.LG

TL;DR: ACE方法通过自训练学习一致性概念来解决完全伪相关性问题，在多个基准测试中优于现有方法，并在语言模型对齐中展现竞争力


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对伪相关性敏感，现有方法主要处理不完全伪相关，但完全伪相关时正确泛化本质上是未指定的，需要新方法来解决这种未指定问题

Method: 提出ACE方法，学习一组与训练数据一致但在新未标记输入上做出不同预测的概念，采用自训练方法鼓励自信和选择性分歧

Result: ACE在完全伪相关基准测试中匹配或优于现有方法，同时对不完全伪相关保持鲁棒性，在语言模型对齐的测量篡改检测基准中无需访问不可信测量即可获得有竞争力性能

Conclusion: ACE在克服未指定问题方面取得显著进展，虽然仍有重要限制，但提供了更可配置的解决方案，允许直接编码先验知识和原则性无监督模型选择

Abstract: Deep neural networks are notoriously sensitive to spurious correlations -
where a model learns a shortcut that fails out-of-distribution. Existing work
on spurious correlations has often focused on incomplete
correlations,leveraging access to labeled instances that break the correlation.
But in cases where the spurious correlations are complete, the correct
generalization is fundamentally \textit{underspecified}. To resolve this
underspecification, we propose learning a set of concepts that are consistent
with training data but make distinct predictions on a subset of novel unlabeled
inputs. Using a self-training approach that encourages \textit{confident} and
\textit{selective} disagreement, our method ACE matches or outperforms existing
methods on a suite of complete-spurious correlation benchmarks, while remaining
robust to incomplete spurious correlations. ACE is also more configurable than
prior approaches, allowing for straight-forward encoding of prior knowledge and
principled unsupervised model selection. In an early application to
language-model alignment, we find that ACE achieves competitive performance on
the measurement tampering detection benchmark \textit{without} access to
untrusted measurements. While still subject to important limitations, ACE
represents significant progress towards overcoming underspecification.

</details>


### [77] [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://arxiv.org/abs/2509.07963)
*Yilun Kuang,Noah Amsel,Sanae Lotfi,Shikai Qiu,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出了基于Block Tensor-Train和Multi-Level Low Rank矩阵的新评分函数，解决标准注意力机制在高维输入信息损失和缺乏距离依赖计算偏置的问题，在多种任务上表现优于标准注意力。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的低维投影会导致高维输入信息损失，且对所有输入对使用相同的评分函数，缺乏对序列中相邻token的距离依赖计算偏置。

Method: 使用计算高效的高秩结构化矩阵（包括Block Tensor-Train和Multi-Level Low Rank矩阵）构建新的评分函数。

Result: 在高维输入的上下文回归任务中，任何固定计算预算下都优于标准注意力；在语言建模中，MLR-based注意力相比标准注意力和滑动窗口注意力变体实现了更好的扩展规律；在长程时间序列预测中也显示出有希望的结果。

Conclusion: BTT和MLR属于一个更广泛的能够编码全秩或距离依赖计算偏置的高效结构化矩阵家族，成功解决了标准注意力的重要缺陷。

Abstract: The core component of attention is the scoring function, which transforms the
inputs into low-dimensional queries and keys and takes the dot product of each
pair. While the low-dimensional projection improves efficiency, it causes
information loss for certain tasks that have intrinsically high-dimensional
inputs. Additionally, attention uses the same scoring function for all input
pairs, without imposing a distance-dependent compute bias for neighboring
tokens in the sequence. In this work, we address these shortcomings by
proposing new scoring functions based on computationally efficient structured
matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level
Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional
inputs, our proposed scoring functions outperform standard attention for any
fixed compute budget. On language modeling, a task that exhibits locality
patterns, our MLR-based attention method achieves improved scaling laws
compared to both standard attention and variants of sliding window attention.
Additionally, we show that both BTT and MLR fall under a broader family of
efficient structured matrices capable of encoding either full-rank or
distance-dependent compute biases, thereby addressing significant shortcomings
of standard attention. Finally, we show that MLR attention has promising
results for long-range time-series forecasting.

</details>


### [78] [Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence](https://arxiv.org/abs/2509.07972)
*Yuxing Liu,Yuze Ge,Rui Pan,An Kang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出了广义平滑性假设，证明了学习率预热策略在理论和实践上的优势，显示预热能加速梯度下降收敛速度


<details>
  <summary>Details</summary>
Motivation: 解决学习率预热策略在实践中广泛应用但理论理解不足的问题，弥合理论与实践之间的差距

Method: 提出新的广义平滑性假设家族，在确定性和随机设置下研究梯度下降的收敛性质，验证假设的理论和实证适用性

Result: 学习率预热始终加速梯度下降，在某些特定情况下比非递增学习率调度快Θ(T)倍收敛

Conclusion: 学习率预热策略从优化理论角度具有明确的理论优势，为这一实用技术提供了理论依据

Abstract: Learning rate warmup is a popular and practical technique in training
large-scale deep neural networks. Despite the huge success in practice, the
theoretical advantages of this strategy of gradually increasing the learning
rate at the beginning of the training process have not been fully understood.
To resolve this gap between theory and practice, we first propose a novel
family of generalized smoothness assumptions, and validate its applicability
both theoretically and empirically. Under the novel smoothness assumption, we
study the convergence properties of gradient descent (GD) in both deterministic
and stochastic settings. It is shown that learning rate warmup consistently
accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times
faster than with a non-increasing learning rate schedule in some specific
cases, providing insights into the benefits of this strategy from an
optimization theory perspective.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [79] [Unikernels vs. Containers: A Runtime-Level Performance Comparison for Resource-Constrained Edge Workloads](https://arxiv.org/abs/2509.07891)
*Hai Dinh-Tuan*

Main category: cs.PF

TL;DR: 在边缘计算中，容器和unikernel的选择依赖于运行时行为和系统资源。Go应用中unikernel更优，而Node.js应用在内存突破点以下时容器性能更稳定。


<details>
  <summary>Details</summary>
Motivation: 工业边缘环境中的严重内存约束使得容器和unikernel之间的性能调整需要更深入研究，特别是在不同执行模型下的表现。

Method: 使用Go和Node.js应用进行实证比较，Go代表AOT编译，Node.js代表JIT编译，在资源约束环境下测试容器和Nanos unikernel的性能。

Result: unikernel在Go工作负载中起动更快且性能更好，但在Node.js中存在关键性能交叉点：在某个内存阈值以下，Docker容器性能稳定而unikernel性能快速下降。

Conclusion: 最优部署方案取决于运行时行为和系统资源，Linux的内存管理能力在资源稀缺时可能超过unikernel的简洁效率，边缘计算需要基于工作负载的部署策略。

Abstract: The choice between containers and unikernels is a critical trade-off for edge
applications, balancing the container's ecosystem maturity against unikernel's
specialized efficiency. However, until now, how this trade-off behaves under
the severe memory constraints of industrial edge environments remains
insufficiently investigated, especially across different execution models. This
work presents an empirical comparison using Go and Node.js applications,
representing ahead-of-time (AOT) and just-in-time (JIT) compilation,
respectively. While unikernels consistently deliver faster startup times and
outperform containers for Go-based workloads in resource-constrained
environments, the evaluation results identify a critical performance crossover
for Node.js. Below a certain memory threshold, Docker containers maintain
stable performance for both I/O-bound and CPU-bound applications, while the
Nanos unikernel's performance degrades sharply. This reveals that Linux's
memory management capabilities can outweigh the minimalist efficiency of
unikernels under resource scarcity, a critical trade-off that, until now, has
not been adequately quantified for JIT runtimes in this context. These findings
demonstrate that the optimal deployment paradigm depends on both runtime
behavior and available system resources, underscoring the need for
workload-aware deployment strategies in edge computing.

</details>
