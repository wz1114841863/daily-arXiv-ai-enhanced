<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 13]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [Quantum-Inspired Artificial Bee Colony for Latency-Aware Task Offloading in IoV](https://arxiv.org/abs/2508.13637)
*Mamta Kumari,Mayukh Sarkar,Rohit Kumar Nonia*

Main category: cs.ET

TL;DR: 量子受启的人工蜜蜂群算法(QABC)用于轻量化车联网任务离线，通过量子计算原理提升优化能力，降低延迟


<details>
  <summary>Details</summary>
Motivation: 在轻量化车联网(IoV)环境中，高效的任务离线对于降低延迟和确保及时决策至关重要，需要优化云服务器、路边单元和车辆节点之间的任务分配策略

Method: 提出量子受启的人工蜜蜂群算法(QABC)，结合量子计算的量子状态演化和概率编码原理，改善传统ABC算法的局部最优避免能力和高维解空间探索能力

Result: QABC算法能够更有效地优化实时离线策略，在延迟敏感型任务离线中表现优异，提高了车辆网络的性能效果

Conclusion: 量子受启的神经网络算法在未来车辆网络中具有广阔的应用潜力，能够有效地解决实时任务离线优化问题

Abstract: Efficient task offloading is crucial for reducing latency and ensuring timely
decision-making in intelligent transportation systems within the rapidly
evolving Internet of Vehicles (IoV) landscape. This paper introduces a novel
Quantum-Inspired Artificial Bee Colony (QABC) algorithm specifically designed
for latency-sensitive task offloading involving cloud servers, Roadside Units
(RSUs), and vehicular nodes. By incorporating principles from quantum
computing, such as quantum state evolution and probabilistic encoding, QABC
enhances the classical Artificial Bee Colony (ABC) algorithm's ability to avoid
local optima and explore high-dimensional solution spaces. This research
highlights the potential of quantum-inspired heuristics to optimize real-time
offloading strategies in future vehicular networks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures](https://arxiv.org/abs/2508.13159)
*Ruibai Tang,Wenlai Zhao*

Main category: cs.AR

TL;DR: 这篇论文提出了三种新的清洗方法，专门用于减少RC长链结构的仿真计算量，在仅有0.7%相对误差的情况下实现了平均8.8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 低级清洗仿真在验证集成电路的物理正确性方面致命感关键，但这种仿真计算成本贵。特别是RC长链结构占了标准电路中平均6.34%（最高达12%）的节点数量，需要有效的简化方法。

Method: 提出了三种新题的简化方法，专门为不同时间常数规模的RC长链结构而设计。这些方法针对这种特定结构的特性进行优化。

Result: 在包含ALU、加法器、乘法器、SEC/DED检查器和中断控制器等多种功能模块的标准电路上进行测试，结果显示方法实现了平均8.8%（最高达22%）的性能提升，仅产生0.7%的相对误差。

Conclusion: 该研究提供了一种高效的方法来减少RC长链结构的仿真计算复杂度，在保持高精度的同时显著提升了仿真性能，对低级清洗仿真领域有重要意义。

Abstract: Transistor-level simulation plays a vital role in validating the physical
correctness of integrated circuits. However, such simulations are
computationally expensive. This paper proposes three novel reduction methods
specifically tailored to RC long-chain structures with different scales of time
constant. Such structures account for an average of 6.34\% (up to 12\%) of the
total nodes in the benchmark circuits. Experimental results demonstrate that
our methods yields an average performance improvement of 8.8\% (up to 22\%) on
simulating benchmark circuits which include a variety of functional modules
such as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,
with only 0.7\% relative error.

</details>


### [3] [Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System](https://arxiv.org/abs/2508.13231)
*Yunhua Fang,Rui Xie,Asad Ul Haq,Linsen Ma,Kaoutar El Maghraoui,Naigang Wang,Meng Wang,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 这篇论文研究大语言模型推理中的KV缓存动态存储策略，通过异构内存系统提高带宽利用率以解决内存带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理越来越受到内存带宽的限制，关键值缓存的频繁访问占据了大部分数据传输。虽然注意力稀疏性减少了部分内存流量，但历史token的相关性随时间变化，需要保持完整KV缓存的可访问性，这续续给带宽和容量带来压力。

Method: 利用现代AI硬件集成的高带宽内存(HBM)和高速离包DRAM构成异构内存系统，研究动态KV缓存存放策略。论文将存放问题数学形式化，推导出理论上界，揭示了运行时优化的很大潜力空间。

Result: 这是首次对大语言模型推理中异构内存系统动态KV缓存调度的正式理论研究，为后续优化策略的开发提供了理论基础。

Conclusion: 通过理论分析证明，在容量约束下通过动态KV缓存存放策略可以最大化聚合带宽利用率，有效解决LLM推理中的内存带宽瓶颈问题。

Abstract: Large Language Model (LLM) inference is increasingly constrained by memory
bandwidth, with frequent access to the key-value (KV) cache dominating data
movement. While attention sparsity reduces some memory traffic, the relevance
of past tokens varies over time, requiring the full KV cache to remain
accessible and sustaining pressure on both bandwidth and capacity. With
advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now
integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making
heterogeneous memory systems a practical solution. This work investigates
dynamic KV cache placement across such systems to maximize aggregated bandwidth
utilization under capacity constraints. Rather than proposing a specific
scheduling policy, we formulate the placement problem mathematically and derive
a theoretical upper bound, revealing substantial headroom for runtime
optimization. To our knowledge, this is the first formal treatment of dynamic
KV cache scheduling in heterogeneous memory systems for LLM inference.

</details>


### [4] [EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code](https://arxiv.org/abs/2508.13156)
*Ping Guo,Yiting Wang,Wanghao Ye,Yexiao He,Ziyao Wang,Xiaopeng Dai,Ang Li,Qingfu Zhang*

Main category: cs.AR

TL;DR: EvoVerilog结合LLM推理能力和进化算法，自动生成和优化Verilog代码，无需人工干预，在基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工干预和精调数据集，限制了自动化设计工作流的扩展性；迭代搜索技术缺乏设计多样性探索。

Method: 采用多目标、基于种群的搜索策略，结合LLM的推理能力和进化算法来探索广泛的设计可能性。

Result: 在VerilogEval-Machine和VerilogEval-Human基准测试中分别达到89.1和80.2的pass@10分数，能够同时生成多种功能Verilog代码并优化资源利用。

Conclusion: EvoVerilog框架成功解决了现有方法的局限性，实现了无需人工干预的高性能Verilog代码自动生成，展示了探索多样化设计的能力。

Abstract: Large Language Models (LLMs) have demonstrated great potential in automating
the generation of Verilog hardware description language code for hardware
design. This automation is critical to reducing human effort in the complex and
error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and
fine-tuning using curated datasets, limiting their scalability in automated
design workflows.
  Although recent iterative search techniques have emerged, they often fail to
explore diverse design solutions and may underperform simpler approaches such
as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that
combines the reasoning capabilities of LLMs with evolutionary algorithms to
automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to
explore a wide range of design possibilities without requiring human
intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art
performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine
and VerilogEval-Human benchmarks, respectively. Furthermore, the framework
showcases its ability to explore diverse designs by simultaneously generating a
variety of functional Verilog code while optimizing resource utilization.

</details>


### [5] [Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists](https://arxiv.org/abs/2508.13157)
*Haohang Xu,Chengjie Liu,Qihang Wang,Wenhao Huang,Yongjian Xu,Weiyu Chen,Anlan Peng,Zhijun Li,Bo Li,Lei Qi,Jun Yang,Yuan Du,Li Du*

Main category: cs.AR

TL;DR: 本文提出Image2Net混合框架，将电路图转换为网表，以支持LLM在模拟集成电路设计中的应用，并构建了包含丰富样式的新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有模拟集成电路主要以图像形式的电路图表示，而LLM需要文本形式的网表来学习知识。现有的转换框架存在图像样式和电路元素支持有限的问题，难以有效转换复杂电路图。

Method: 构建包含丰富样式电路图的新数据集，提出Image2Net混合框架进行电路图到网表的转换，并引入网表编辑距离(NED)来精确评估转换结果。

Result: Image2Net实现了80.77%的成功率，比之前工作提高34.62%-45.19%；平均NED为0.116，比现有技术降低62.1%-69.6%。

Conclusion: Image2Net框架能有效解决电路图到网表的转换问题，为LLM在模拟IC设计中的应用提供了重要支持，显著优于现有方法。

Abstract: Large Language Model (LLM) exhibits great potential in designing of analog
integrated circuits (IC) because of its excellence in abstraction and
generalization for knowledge. However, further development of LLM-based analog
ICs heavily relies on textual description of analog ICs, while existing analog
ICs are mostly illustrated in image-based circuit diagrams rather than
text-based netlists. Converting circuit diagrams to netlists help LLMs to
enrich the knowledge of analog IC. Nevertheless, previously proposed conversion
frameworks face challenges in further application because of limited support of
image styles and circuit elements. Up to now, it still remains a challenging
task to effectively convert complex circuit diagrams into netlists. To this
end, this paper constructs and opensources a new dataset with rich styles of
circuit diagrams as well as balanced distribution of simple and complex analog
ICs. And a hybrid framework, named Image2Net, is proposed for practical
conversion from circuit diagrams to netlists. The netlist edit distance (NED)
is also introduced to precisely assess the difference between the converted
netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\%
successful rate, which is 34.62\%-45.19\% higher than previous works.
Specifically, the proposed work shows 0.116 averaged NED, which is
62.1\%-69.6\% lower than state-of-the-arts.

</details>


### [6] [Fine Grain 3D Integration for Microarchitecture Design Through Cube Packing Exploration](https://arxiv.org/abs/2508.13158)
*Yongxiang Liu,Yuchun Ma,Eren Kurshan,Glenn Reinman,Jason Cong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的3D IC设计方法，允许逻辑块跨越多个硅层，通过立方包装式式布局引擎同时优化物理和架构设计，实现了36%性能提升和30%功耗降低。


<details>
  <summary>Details</summary>
Motivation: 传统3D IC研究主要关注堆叠传统2D硅层，连接减少仅限于块间延迟。细粒度3D集成虽可实现进一步功耗和性能改善，但缺少必要的建模和工具支撑。

Method: 开发了立方包装式式布局引擎，能够同时优化物理和架构设计，采用热感知地布局和热通孔插入技术控制温度。

Result: 实验结果显示相比2D设计性能提升36%（BIPS），相比单层块3D设计提升14%。多层块还能降低30%的功耗消耗，并通过热感知技术保持峰值温度在限度内。

Conclusion: 该研究证明了细粒度3D集成在性能、面积和热管理方面的显著优势，为3D IC设计开启了新的探索方向。

Abstract: Most previous 3D IC research focused on stacking traditional 2D silicon
layers, so the interconnect reduction is limited to inter-block delays. In this
paper, we propose techniques that enable efficient exploration of the 3D design
space where each logical block can span more than one silicon layers. Although
further power and performance improvement is achievable through fine grain 3D
integration, the necessary modeling and tool infrastructure has been mostly
missing. We develop a cube packing engine which can simultaneously optimize
physical and architectural design for effective utilization of 3D in terms of
performance, area and temperature. Our experimental results using a design
driver show 36% performance improvement (in BIPS) over 2D and 14% over 3D with
single layer blocks. Additionally multi-layer blocks can provide up to 30%
reduction in power dissipation compared to the single-layer alternatives. Peak
temperature of the design is kept within limits as a result of thermal-aware
floorplanning and thermal via insertion techniques.

</details>


### [7] [Through Silicon Via Aware Design Planning for Thermally Efficient 3-D Integrated Circuits](https://arxiv.org/abs/2508.13160)
*Yibo Chen,Eren Kurshan,Dave Motschman,Charles Johnson,Yuan Xie*

Main category: cs.AR

TL;DR: 一种针对3D集成电路中TSV农场导致的温度阻塞问题的热敏感布局技术


<details>
  <summary>Details</summary>
Motivation: TSV在垂直方向提高了热导率，但密集的TSV农场反而会在偶合方向造成热阻塞效应，加剧局部热点问题

Method: 提出了一种热敏感的TSV农场布局方法，以最小化密集信号总线TSV结构导致的偶合热阻塞

Result: 该方法能够有效减少由密集TSV农场引起的热阻塞效应

Conclusion: 针对3D IC中密集TSV农场的热管理问题，热敏感布局技术是必要且有效的解决方案

Abstract: 3-D integrated circuits (3-D ICs) offer performance advantages due to their
increased bandwidth and reduced wire-length enabled by through-silicon-via
structures (TSVs). Traditionally TSVs have been considered to improve the
thermal conductivity in the vertical direction. However, the lateral thermal
blockage effect becomes increasingly important for TSV via farms (a cluster of
TSV vias used for signal bus connections between layers) because the TSV size
and pitch continue to scale in {\mu}m range and the metal to insulator ratio
becomes smaller. Consequently, dense TSV farms can create lateral thermal
blockages in thinned silicon substrate and exacerbate the local hotspots. In
this paper, we propose a thermal-aware via farm placement technique for 3-D ICs
to minimize lateral heat blockages caused by dense signal bus TSV structures.

</details>


### [8] [Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner](https://arxiv.org/abs/2508.13161)
*Zhexuan Xu,Kexin Zhou,Jie Wang,Zijie Geng,Siyuan Xu,Shixiong Kai,Mingxuan Yuan,Feng Wu*

Main category: cs.AR

TL;DR: Piano是一个同时优化模块布局和引脚分配的VLSI布图规划框架，通过基于图的引脚分配方法和局部优化器，在多种约束下显著提升布线质量和减少未放置引脚。


<details>
  <summary>Details</summary>
Motivation: 传统布图规划器在现代约束条件下往往忽略引脚分配，而引脚分配对后续详细布局和布线阶段的性能有重要影响，需要同时优化模块布局和引脚分配。

Method: 构建基于模块几何关系和网表连接的图，迭代搜索最短路径确定引脚分配；采用空白空间移除策略和三个局部优化器来提升多约束场景下的布局指标。

Result: 在基准电路上平均减少6.81%的HPWL、13.39%的馈通线长、16.36%的馈通模块数量和21.21%的未放置引脚，同时保持零空白空间。

Conclusion: Piano框架有效解决了现代VLSI布图规划中的多约束问题，通过同时优化模块布局和引脚分配显著提升了整体布局质量。

Abstract: Floorplanning is a critical step in VLSI physical design, increasingly
complicated by modern constraints such as fixed-outline requirements,
whitespace removal, and the presence of pre-placed modules. In addition, the
assignment of pins on module boundaries significantly impacts the performance
of subsequent stages, including detailed placement and routing. However,
traditional floorplanners often overlook pin assignment with modern constraints
during the floorplanning stage. In this work, we introduce Piano, a
floorplanning framework that simultaneously optimizes module placement and pin
assignment under multiple constraints. Specifically, we construct a graph based
on the geometric relationships among modules and their netlist connections,
then iteratively search for shortest paths to determine pin assignments. This
graph-based method also enables accurate evaluation of feedthrough and unplaced
pins, thereby guiding overall layout quality. To further improve the design, we
adopt a whitespace removal strategy and employ three local optimizers to
enhance layout metrics under multi-constraint scenarios. Experimental results
on widely used benchmark circuits demonstrate that Piano achieves an average
6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%
reduction in the number of feedthrough modules, and a 21.21% drop in unplaced
pins, while maintaining zero whitespace.

</details>


### [9] [FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design](https://arxiv.org/abs/2508.13162)
*Mahmoud Nazzal,Khoa Nguyen,Deepak Vungarala,Ramtin Zand,Shaahin Angizi,Hai Phan,Abdallah Khreishah*

Main category: cs.AR

TL;DR: FedChip是一个联邦学习框架，允许多个芯片设计方在保护专有数据隐私的前提下，协作微调共享大语言模型，用于自动化硬件设计生成。该方法显著提升设计质量77%以上。


<details>
  <summary>Details</summary>
Motivation: AI硬件设计自动化需求增长，但现有LLM面临数据隐私问题和领域专业知识不足的挑战，需要一种既能保护数据隐私又能提升模型性能的解决方案。

Method: 提出FedChip联邦微调方法，多个设计方在本地专有数据上训练，协作改进共享LLM；创建APTPU-Gen数据集（3万设计变体）；提出Chip@k评估指标来平衡多个质量指标。

Result: 实验结果显示FedChip相比高端LLM将设计质量提升超过77%，同时有效保护数据隐私。

Conclusion: FedChip成功解决了芯片设计领域LLM应用中的数据隐私和领域适应性问题，为自动化硬件设计提供了有效的隐私保护协作学习方案。

Abstract: AI hardware design is advancing rapidly, driven by the promise of design
automation to make chip development faster, more efficient, and more accessible
to a wide range of users. Amongst automation tools, Large Language Models
(LLMs) offer a promising solution by automating and streamlining parts of the
design process. However, their potential is hindered by data privacy concerns
and the lack of domain-specific training. To address this, we introduce
FedChip, a Federated fine-tuning approach that enables multiple Chip design
parties to collaboratively enhance a shared LLM dedicated for automated
hardware design generation while protecting proprietary data. FedChip enables
parties to train the model on proprietary local data and improve the shared
LLM's performance. To exemplify FedChip's deployment, we create and release
APTPU-Gen, a dataset of 30k design variations spanning various performance
metric values such as power, performance, and area (PPA). To encourage the LLM
to generate designs that achieve a balance across multiple quality metrics, we
propose a new design evaluation metric, Chip@k, which statistically evaluates
the quality of generated designs against predefined acceptance criteria.
Experimental results show that FedChip improves design quality by more than 77%
over high-end LLMs while maintaining data privacy

</details>


### [10] [Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures](https://arxiv.org/abs/2508.13163)
*Yashasvi Makin,Rahul Maliakkal*

Main category: cs.AR

TL;DR: 本文探讨针对NVIDIA、AMD等先进GPU架构的环境驱动性能优化方法，通过硬件软件协同设计提高内存级和内核级操作效率，实现显著的能效提升。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型训练消耗大量计算资源和能源，带来严重的可持续性问题。模型复杂度的快速增长导致能耗呈指数级增长，迫切需要提高计算效率并降低环境影响的技术。

Method: 研究硬件软件协同设计技术，包括专用张量和矩阵核心评估、先进内存优化方法、混合精度算术、能量感知调度算法和编译器驱动的内核增强等软件级优化。

Result: 通过硬件软件协同设计可获得训练效率的显著提升，在不影响性能的前提下降低人工智能的环境影响。实际案例研究证明了这些可持续AI训练方法的有效性。

Conclusion: 硬件软件协同设计是实现可持续人工智能系统的关键，本文指出了重要的研究空白并提出了未来发展方向，为创建真正可持续的人工智能系统提供了重要见解。

Abstract: In particular, large-scale deep learning and artificial intelligence model
training uses a lot of computational power and energy, so it poses serious
sustainability issues. The fast rise in model complexity has resulted in
exponential increases in energy consumption, increasing the demand for
techniques maximizing computational efficiency and lowering environmental
impact. This work explores environmentally driven performance optimization
methods especially intended for advanced GPU architectures from NVIDIA, AMD,
and other emerging GPU architectures. Our main focus is on investigating
hardware-software co-design techniques meant to significantly increase
memory-level and kernel-level operations, so improving performance-per-watt
measures. Our thorough research encompasses evaluations of specialized tensor
and matrix cores, advanced memory optimization methods, and creative
integration approaches that taken together result in notable energy efficiency
increases. We also discuss important software-level optimizations that augment
hardware capability including mixed-precision arithmetic, advanced energy-aware
scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we
methodically point out important research gaps and suggest future directions
necessary to create really sustainable artificial intelligence systems. This
paper emphasizes how major increases in training efficiency can be obtained by
co-design of hardware and software, so lowering the environmental impact of
artificial intelligence without compromising performance. To back up our
analysis, we use real-world case studies from top companies like Meta, Google,
Amazon, and others that show how these sustainable AI training methods are used
in the real world.

</details>


### [11] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 提出了一种结合LLM战略推理与gm/Id方法的协同推理框架，用于模拟IC设计自动化，在5次迭代内实现TT角规格满足，效率比资深工程师提升一个数量级


<details>
  <summary>Details</summary>
Motivation: 模拟IC设计依赖经验且仿真效率低，传统公式在先进节点失效，直接应用LLM存在盲目猜测风险，需要结合工程原理

Method: 通过为LLM配备gm/Id查找表，构建协同推理框架，将LLM的战略推理能力与gm/Id方法的物理精度相结合

Result: 在两阶段运放设计中，框架使Gemini模型在5次迭代内满足所有TT角规格，并扩展到所有PVT角优化；相比无gm/数据的情况，效率更高且偏差更小

Conclusion: 该工作验证了通过结合LLM推理与科学电路设计方法实现真正模拟设计自动化的路径，达到准专家质量的设计水平

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


### [12] [Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices](https://arxiv.org/abs/2508.13181)
*Dominik Loroch,Johannes Feldmann,Vladimir Rybalkin,Norbert Wehn*

Main category: cs.AR

TL;DR: 新型可穿戴式心戴监测设备，通过FPGA和深度学习实现了超低耗电（3.8mW）和高准确度（95%）的戱戴检测，可连续工作三周以上


<details>
  <summary>Details</summary>
Motivation: 尿动是常见心律失常，虽然现有技术能检测，但大规模普及仍面临挑战，需要更能源效的可穿戴解决方案

Method: 采用FPGA基础的补丁式可穿戴设备，嵌入深度学习AF检测算法，通过硬件-软件协同设计和硬件感知神经网络架构搜索优化功耗管理

Result: 设备系统耗电仅3.8mW，比当前最佳技术低1-3个数量级，可连续检测AF超过3周，准确度达95%，超越心血管专家水平

Conclusion: 这项技术为可扩展、可靠和可持续的心戴监测提供了重要进展，有力推动AF检测技术的大规模普及应用

Abstract: Atrial fibrillation (AF) is a common arrhythmia and major risk factor for
cardiovascular complications. While commercially available devices and
supporting Artificial Intelligence (AI) algorithms exist for reliable detection
of AF, the scaling of this technology to the amount of people who need this
diagnosis is still a major challenge. This paper presents a novel wearable
device, designed specifically for the early and reliable detection of AF. We
present an FPGA-based patch-style wearable monitor with embedded deep
learning-based AF detection. Operating with 3.8mW system power, which is 1-3
orders of magnitude lower than the state-of-the-art, the device enables
continuous AF detection for over three weeks while achieving 95% accuracy,
surpassing cardiologist-level performance. A key innovation is the combination
of energy-efficient hardware-software co-design and optimized power management
through the application of hardware-aware neural architecture search. This
advancement represents a significant step toward scalable, reliable, and
sustainable AF monitoring.

</details>


### [13] [Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller](https://arxiv.org/abs/2508.13244)
*Marco Giordano,Pietro Bonazzi,Luca Benini,Michele Magno*

Main category: cs.AR

TL;DR: 基于微控制器的低功耗事件驱动眼动追踪系统，通过专门优化的CNN模型实现了微秒级延迟和毫瓦级功耗


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式系统中实时眼动追踪的低延迟、低功耗挑战，适用于智能眼镜等可穿戴设备

Method: 采用DVXplorer Micro动态视觉传感器，在STM32N6微控制器上部署硬件感知优化的卷积神经网络，利用AI加速器实现实时推理

Result: 在Ini-30数据集上实现平均学生预测误差5.99像素，中位数误差5.73像素，结构延迟仅385微秒，每周期52次MAC操作，消耗155微焦耳能量

Conclusion: 该系统为可穿戴设备提供了全嵌入式、高能效的眼动追踪解决方案，在保持高精度的同时实现了极低的延迟和功耗

Abstract: This paper presents a novel event-based eye-tracking system deployed on a
resource-constrained microcontroller, addressing the challenges of real-time,
low-latency, and low-power performance in embedded systems. The system
leverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with
an average temporal resolution of 200 {\mu}s, to capture rapid eye movements
with extremely low latency. The system is implemented on a novel low-power and
high-performance microcontroller from STMicroelectronics, the STM32N6. The
microcontroller features an 800 MHz Arm Cortex-M55 core and AI hardware
accelerator, the Neural-ART Accelerator, enabling real-time inference with
milliwatt power consumption. The paper propose a hardware-aware and
sensor-aware compact Convolutional Neuron Network (CNN) optimized for
event-based data, deployed at the edge, achieving a mean pupil prediction error
of 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The
system achieves an end-to-end inference latency of just 385 {\mu}s and a neural
network throughput of 52 Multiply and Accumulate (MAC) operations per cycle
while consuming just 155 {\mu}J of energy. This approach allows for the
development of a fully embedded, energy-efficient eye-tracking solution
suitable for applications such as smart glasses and wearable devices.

</details>


### [14] [ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models](https://arxiv.org/abs/2508.13257)
*Wenhao Lv,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.AR

TL;DR: ViTAD是一个自动化RTL时序违规修复方法，通过构建信号时序依赖图和使用LLM分析违规根因，结合领域知识库生成定制修复方案，在真实数据集上达到73.68%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计对时序要求越来越高，传统时序优化依赖人工经验且效率低下，需要自动化解决方案来早期发现和修复时序违规问题。

Method: 首先解析Verilog代码和时序报告构建STDG图，然后进行违规路径分析并使用LLM推断违规根因，最后从领域知识库检索相关知识生成定制修复策略。

Result: 在包含54个违规案例的真实数据集上，ViTAD方法达到73.68%的修复成功率，相比仅使用LLM的基线方法(54.38%)提升了19.30%。

Conclusion: ViTAD方法通过结合图分析和LLM技术，有效实现了RTL时序违规的自动化分析和修复，显著提高了修复成功率和效率。

Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the
Register-Transfer Level (RTL) stage presents a critical opportunity for timing
optimization. Addressing timing violations at this early stage is essential, as
modern systems demand higher speeds, where even minor timing violations can
lead to functional failures or system crashes. However, traditional timing
optimization heavily relies on manual expertise, requiring engineers to
iteratively analyze timing reports and debug. To automate this process, this
paper proposes ViTAD, a method that efficiently analyzes the root causes of
timing violations and dynamically generates targeted repair strategies.
Specifically, we first parse Verilog code and timing reports to construct a
Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation
path analysis and use large language models (LLMs) to infer the root causes of
violations. Finally, by analyzing the causes of violations, we selectively
retrieve relevant debugging knowledge from a domain-specific knowledge base to
generate customized repair solutions. To evaluate the effectiveness of our
method, we construct a timing violation dataset based on real-world open-source
projects. This dataset contains 54 cases of violations. Experimental results
show that our method achieves a 73.68% success rate in repairing timing
violations, while the baseline using only LLM is 54.38%. Our method improves
the success rate by 19.30%.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: BERT-VQA模型在图表问答任务中表现不佳，证明VisualBERT的跨模态模块对图表组件与问题短语的对齐并非必需，为图表问答挑战提供了重要见解


<details>
  <summary>Details</summary>
Motivation: 解决图表视觉问答这一自然语言理解中的挑战性子任务，需要模型同时处理视觉和语言信息

Method: 开发基于VisualBERT的BERT-VQA模型，使用预训练的ResNet 101图像编码器，可能加入联合融合模块，并与基于LSTM、CNN和浅层分类器的基线模型进行比较

Result: 最终结果推翻了核心假设，证明VisualBERT中的跨模态模块对于图表组件与问题短语的对齐并非必需

Conclusion: 这项工作为图表问答任务的难度以及不同模型架构在解决此问题时的适用性提供了有价值的见解

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [16] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 基于CNN和LLM的多模态情感分析方法，通过上下文注意力机制融合文本和图像信息，在自然灾害情感分析中实现了准确率和F1指标的显著提升


<details>
  <summary>Details</summary>
Motivation: 自然灾害期间公众情感理解对危机管理至关重要，但传统多模态方法对文本和图像处理分离，需要更有效的融合方法来提升情感分析效果

Method: 结合CNN图像分析和LLM文本处理，使用GPT和prompt工程提取特征，通过上下文注意力机制模型模态间关系，深度神经网络学习融合特征

Result: 在CrisisMMD数据集上实现了准确率2.43%和F1指标5.18%的显著提升，能够更有效地将社交媒体数据分类为信息性和非信息性内容

Conclusion: 该方法为AI驱动的灾害管理提供了有前景的方向，能够在实时灾害管理中优化紧急干预的准确性，为多模态分析和灾害响应建立了桥梁

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [17] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: 物理信息神经网络通过将微分方程结构直接嵌入损失函数来近似解决方案，本文系统研究训练点分布对解决精度的影响


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络作为求解微分方程的有前景方法，但其性能受多种因素影响，需要系统研究核心组成部分之一的训练点分布策略

Method: 使用5种训练数据生成策略（包括新引入的正弦基分布）在2个常微分方程和2个偏微分方程上测试，使用浅层网络架构，并考虑随机和固定种子的参数组合

Result: 训练点分布对解决精度有显著影响，且这种影响与微分方程的特性相关联

Conclusion: 训练点分布策略是物理信息神经网络的关键因素，需要根据具体微分方程特性选择适当的分布方式，新提出的正弦基分布策略表现出良好澄清度

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [18] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: 提出DGNPP模型，结合图神经网络和点过程，同时捕捉时间交互网络的拓扑结构和动态演化，显著提升事件和时间预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间交互网络学习多被视为粗粒度的多序列预测问题，忽略了网络拓扑结构的影响，需要同时建模动态演化和静态结构。

Method: 设计包含节点聚合层（捕获拓扑结构生成静态表示）和自注意力层（动态更新嵌入）的双模块架构，将动态和静态嵌入整合到事件强度函数中，通过最大似然估计优化。

Result: 在三个公共数据集上的实验表明，DGNPP在事件预测和时间预测任务中均取得优越性能，显著超越基线模型，且具有高效性。

Conclusion: DGNPP有效解决了先前方法的局限性，通过结合图神经网络和点过程，成功建模了时间交互网络的拓扑结构和动态特性。

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [19] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: 使用递归神经网络对教育中广泛使用的S-P图进行聚类处理，通过网络动力学特性实现大规模数据的自动分类


<details>
  <summary>Details</summary>
Motivation: 随着学生数量增加，S-P图变得难以处理，需要一种有效的方法将大规模图表分解为更小的子图表

Method: 基于递归神经网络动力学的聚类方法，网络具有多个固定点，吸引域形成与小型S-P图对应的聚类

Result: 通过基础实验验证了方法的有效性，并提出了平均警戒指数作为评估聚类性能的重要特征量

Conclusion: 该方法能够有效地处理大规模S-P图数据，通过神经网络动力学特性实现自动聚类，为教育数据分析提供了新的解决方案

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [20] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: RISE是一个两阶段框架，通过强化学习生成视觉基础、逻辑一致的思维链，然后利用高质量CoT进行监督和强化微调，在复杂图像标注任务中超越SFT和Visual-RFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在复杂图像标注任务中存在推理能力不足的问题。标准监督微调只关注标注结果而忽略推理过程，而视觉强化微调由于缺乏高质量验证的思维链导致推理不一致。

Method: RISE包含两个阶段：1) Reason阶段(RISE-CoT)：使用强化学习的"标注-推理-标注"闭环生成视觉基础、逻辑一致的思维链；2) Inspire和Strengthen阶段(RISE-R1)：利用高质量CoT子集进行监督微调，然后进行强化微调。

Result: 在复杂和简单图像标注任务上，RISE训练的Qwen2-VL-2B模型超越了SFT和Visual-RFT方法，实现了鲁棒性能和增强的可解释性。

Conclusion: RISE提供了一种无需人工标注思维链的自监督解决方案，可有效提升视觉语言模型的推理能力。

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [21] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: 通过稀疏回归算法识别系统动态，采用Lie导数设计反馈控制器实现反馈线性化，结合堆叠回归和相对阶条件来发现真实系统方程


<details>
  <summary>Details</summary>
Motivation: 发现物理系统的控制方程并设计有效反馈控制器是一项具有挑战性的研究任务，需要深入理解包括非线性因素在内的系统动态行为

Method: 首先使用稀疏回归算法识别系统，然后对输出函数字典应用Lie导数来设计反馈控制器，得到一个扩充约束条件确保无内部动态观测

Result: 该方法能够结合堆叠回归算法和相对阶条件，发现并反馈线性化物理模型的真实经验方程

Conclusion: 本文提出的新方法在结合系统识别和反馈控制设计方面具有创新性，能够有效处理非线性系统的控制问题

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [22] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 通过物理模拟实现物理可行的数据增帽(PPDA)，在传感器基于人体活动识别中比传统信号变换方法提升了3.7pp的F1分数，并能在训练数据减少60%的情况下达到竞争性能效果


<details>
  <summary>Details</summary>
Motivation: 解决传感器基于人体活动识别中高质量标签数据稀缺问题，传统信号变换数据增帽方法存在物理不可行性，影响活动标签的原始含义保持

Method: 采用物理模拟实现物理可行数据增帽(PPDA)，利用动作捕获或视频姿势估计的人体运动数据，通过物理模拟结合现实变量（体验运动、传感器置位、硬件效应）来生成合成惯性测量单元数据

Result: 在3个公开数据集上，PPDA方法比传统STDA方法平均提升巨式F1分数3.7百分点(最高提升13百分点)，并能在训练主体数量减少60%的情况下达到竞争性能效果

Conclusion: 物理可行数据增帽在传感器基于人体活动识别中具有显著优势，物理模拟为生成合成惯性测量单元数据提供了成本效益高且可扩展的解决方案，有助于缓解标注数据稀缺挑战

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [23] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 人工智能协作匹配系统comatch通过选择性延迟决策来实现人机补充，在大规模人体实验中显著提升了匹配性能


<details>
  <summary>Details</summary>
Motivation: 现有的算法匹配系统无法实现人工智能的真正补充，人类使用算法系统做出的决策并不比单独使用人类或算法更好

Method: 提出conatch系统，只选择算法最有信心的匹配决策，将其余决策延迟给人类决策者，优化人机分配来最大化性能

Result: 在800名参与者的大规模人体实验中，comatch产生的匹配结果显著超过了单独使用人类或算法的表现

Conclusion: 协作式匹配系统能够实现人机补充，在高风险应用领域具有重要价值，系统实现和实验数据已开源可用

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [24] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: 本文提出了分层保形分类（HCC），将类别层次结构融入保形预测框架，在保持覆盖保证的同时生成包含不同层次节点的预测集。


<details>
  <summary>Details</summary>
Motivation: 标准保形预测将类别视为平坦无结构的，忽略了类别标签之间的语义关系或层次结构等先验知识。

Method: 将HCC建模为约束优化问题，通过证明一个更小、结构良好的候选解子集足以确保覆盖并保持最优性来解决组合复杂性。

Result: 在音频、图像和文本数据上的实证评估显示该方法的优势，用户研究表明标注者显著偏好分层预测集而非平坦预测集。

Conclusion: HCC成功地将类别层次结构整合到保形预测中，在保持统计保证的同时提供了更具语义意义的预测集。

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [25] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: 提出两种约束流匹配方法：基于可微距离函数的惩罚项方法和基于随机化的成员查询方法，能够在生成样本时满足约束条件


<details>
  <summary>Details</summary>
Motivation: 解决流匹配生成中需要满足特定约束条件的问题，特别是在只有成员查询权限（无法获取梯度信息）的复杂约束场景

Method: 对于可微距离约束，在FM目标中添加惩罚项；对于成员查询约束，使用随机化学习均值流，并采用两阶段高效训练策略

Result: 在多个合成约束生成任务中显著提高约束满足率，同时保持目标分布匹配性能，成功应用于黑盒分类器的对抗样本生成

Conclusion: 提出的方法能够有效处理不同类型的约束条件，为约束生成问题提供了灵活且高效的解决方案

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [26] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE是一个专门为新一代MoE架构设计的训练系统，通过跨平台内核、冗余绕过调度和混合并行等技术，在AMD GPU上实现了DeepSeek风格MoE模型的高效可扩展训练


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构存在激活内存开销大、通信成本高的问题，且当前MoE训练系统主要针对NVIDIA GPU优化，在非NVIDIA平台上性能不佳，未能充分利用计算潜力

Method: 采用高效无填充MoE训练与跨平台内核、冗余绕过调度技术、序列分片MoE块的混合并行方法

Result: 在Frontier超级计算机（AMD MI250X GPU）上，X-MoE成功将DeepSeek风格MoE扩展到5450亿参数，在1024个GPU上训练，比现有方法可训练的最大模型大10倍，同时保持高训练吞吐量

Conclusion: X-MoE系统有效解决了MoE训练的可扩展性问题，为非NVIDIA平台上的大规模MoE模型训练提供了高效解决方案

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [27] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: 该论文探讨了在部分可观测性条件下的机器语言习得问题，提出了一个基于学习的方法来推断私有信息以促进语言获取


<details>
  <summary>Details</summary>
Motivation: 传统语言习得模型假设学习者能够观察到所有相关信息，但现实中存在部分可观测性问题，学习者需要从环境、动作和发送的消息中推断缺失信息

Method: 提出了一个学习算法，通过分析环境知识、执行的动作和发送的消息来解码私有信息，并在玩具设置中验证方法有效性

Result: 在玩具设置中成功解决了部分可观测性下的语言习得问题，并正式探讨了在更一般设置中出现的挑战

Conclusion: 部分可观测性是语言习得中的重要问题，提出的学习算法能够有效推断私有信息，为更真实的语言学习场景提供了解决方案

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [28] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: Trans-XFed架构结合联邦学习和可解释AI技术，用于供应链信用评估，解决隐私、数据孤岛、类别不平衡、非IID数据和模型可解释性等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决供应链信用评估中的隐私保护、数据隔离、类别不平衡、非IID数据分布和模型黑盒问题，提供既准确又可解释的信用评估方案。

Method: 采用基于性能的客户端选择策略(PBCS)处理类别不平衡和非IID问题，使用增强同态加密的FedProx架构作为核心模型，集成transformer编码器提供特征洞察，并应用积分梯度可解释AI技术。

Result: 在真实供应链数据集上的实验表明，Trans-XFed相比多个基线方法能够提供更准确的信用评估，同时保持透明度和隐私保护。

Conclusion: Trans-XFed成功解决了供应链信用评估中的多重挑战，实现了准确性、隐私保护和模型可解释性的平衡，为实际应用提供了有效的解决方案。

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [29] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于动态图创建和时空注意机制的新框架，用于自闭谱系障碍(ASD)的fMRI诊断，通过动态抓取脑功能连接性和时空关系，在ABIDE数据集上超越了静态图方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI分类方法多基于静态功能连接性或无法全面抓取时空关系，而脑神活动动力学理解对神经科学领域发展至关重要。

Method: 使用transformer注意机制动态推断每个时间间隔的脑功能连接性，构建时变图并用图卷积网络(GCN)和transformer处理，抓取局部交互和全局时序依赖关系。

Result: 在ABIDE数据集子集上达到63.2%的准确率和60.0%的AUC，显著超过静态图方法(GCN:51.8%)。

Conclusion: 验证了联合建模动态连接性和时空上下文对fMRI分类的效果，核心新颖性在于注意驱动的动态图创建和通过GCN-transformer融合的层次时空特征融合。

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [30] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: 本文提出了一种基于线性代数的维度下界证明方法，用于分析解决L²函数逼近问题的线性方法，特别应用于核方法的样本量下界推导


<details>
  <summary>Details</summary>
Motivation: 为线性方法在L²函数逼近问题中提供维度下界的严格数学证明，扩展已有的Kolmogorov n-宽度下界理论

Method: 采用线性代数方法，基于已有的Kolmogorov n-宽度下界证明框架，将其应用于核方法的样本量下界分析

Result: 建立了核方法样本量下界的数学证明框架，为线性函数逼近方法提供了理论下界保证

Conclusion: 该线性代数方法为分析线性函数逼近方法的性能极限提供了有效的理论工具，特别是在核方法样本复杂度分析方面具有重要应用价值

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [31] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: ODE-Diff是一个结合机制模型和数据驱动方法的时序扩散框架，通过专家模型提供结构化先验，在数据稀缺情况下实现更可靠的反事实分布预测


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺时表现不佳，需要结合机制模型和数据驱动方法的优势来进行可靠的反事实预测

Method: 提出ODE-Diff框架，使用时序扩散模型，从非完美专家模型中提取高层信号作为结构化先验，桥接机制方法和数据驱动方法

Result: 在半合成COVID-19模拟、合成药理学动力学和真实案例研究中，ODE-Diff在点预测和分布准确性方面均优于强基线方法

Conclusion: ODE-Diff成功结合了机制模型和数据驱动方法的优势，提供了更可靠和可解释的因果推理框架

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [32] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 提出基于共形预测的统一框架，将采样的未来轨迹转换为具有理论覆盖保证的校准预测区间


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的轨迹集合通常未校准，无法提供可靠的预测不确定性估计

Method: 引入在线更新步骤和优化步骤来捕捉步间依赖关系，生成不连续的预测区间

Result: 能够自然捕捉时间依赖性，产生更锐利、更自适应的不确定性估计

Conclusion: 该框架为轨迹预测提供了理论保证的校准不确定性量化方法

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [33] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 这篇论文提出了J3O框架，通过聚合汇编优化和线性规划，在边缘设备上实现多任务模型的协同推理，在保证精度的同时大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有框架主要关注单任务、单模型场景，而实际应用如自动驾驶和增强现实需要并发执行多种任务，因此需要一个统一框架来在资源受限的边缘设备上优化多任务模型的部署和查询路由。

Method: 将问题形式化为混合整数规划问题，提出J3O交替算法：(i)通过拉格朗日松弛次模模优化谭式选择模型部署，(ii)通过约束线性规划确定最优负载分配。还扩展支持边缘批处理。

Result: 实验结果显示J3O在多任务测试集上达到了超过97%的最优精度，同时运行时间仅需最优求解器的15%以下。

Conclusion: J3O框架能够高效地解决边缘设备上多任务模型协同推理的挑战，在保持高准确性的同时显著提升了系统性能，为实际应用提供了可扩展的解决方案。

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [34] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 通过时间-频率分析鸟鸣事件识别统计异常通道，使用LOF算法和空间相关分析来评估临床疑惑突发区域的定位效果


<details>
  <summary>Details</summary>
Motivation: 需要一种量化框架来评估临床定义的突发区域与通过时间-频率分析识别的统计异常通道之间的空间一致性

Method: 两步方法：(1)无监督异常检测，使用局部异常因子(LOF)分析识别异常通道；(2)空间相关分析，计算准确匹配指标和加权指数相似性

Result: LOF方法(N邻居=20，污染率=0.2)有效检测异常通道，加权匹配效果更好。无突发病人(Index Precision均值:0.903)和手术成功病人(0.865)表现最佳，失败案例一致性较低(0.460)

Conclusion: 基于鸟鸣的异常检测结合加权空间指标，为疑惑突发区域定位提供了补充方法，尤其在手术成功病人中效果显著

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [35] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: NovoMolGen是一个基于Transformer的分子大语言模型，在15亿分子上预训练，系统研究了文本表示、分词策略、模型规模和数据集规模对分子生成性能的影响，在无约束和目标导向分子生成任务中达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前分子大语言模型在探索巨大化学空间（10^23到10^60个可合成候选分子）方面显示出潜力，但缺乏对标准语言建模实践（如文本表示、分词策略等）如何影响分子生成性能的系统理解。

Method: 引入NovoMolGen模型家族，基于Transformer架构，在15亿分子上进行预训练，通过大量实证分析研究不同因素对性能的影响。

Result: 发现预训练期间性能指标与下游实际性能之间相关性较弱，揭示了分子训练与通用NLP训练动态的重要区别。模型在无约束和目标导向分子生成任务中显著优于之前的Mol-LLMs和专用生成模型。

Conclusion: NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，建立了分子生成的新技术标准。

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [36] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: 网络环境下的上下文线性搏弈问题，提出了两种网络感知UCB算法，通过动态网络权重实现适应性信息共享，降低通信成本并改善悔弊界。


<details>
  <summary>Details</summary>
Motivation: 传统上下文搏弈假设数据要么完全集中要么完全隔离，在网络环境中部分信息共享的情况下研究不充分。需要解决多个位置同时学习且奖励分布既有结构相似性又有局部差异的问题。

Method: 提出两种网络感知UCB算法：NetLinUCB和Net-SGD-UCB。通过动态更新网络权重来导向适应性信息共享，将学习分解为全局和局部组件。算法只需共享计算的同质特征摘要，降低通信成本。

Result: 证明了悔弊界，将共享结构的学习复杂度从O(N)降低到次线性O(√N)（N为网结构大小）。NetLinUCB在低噪声和细粒度异质性情况下表现优异，Net-SGD-UCB在高维度高方差上下文中更稳健。在模拟定价环境中超越了标准基准。

Conclusion: 该研究为网络环境下的上下文搏弈问题提供了有效解决方案，通过动态网络权重实现了效率与性能的平衡。两种算法各有优势，适用于不同的应用场景。

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [37] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: MAVIS是一个轻量级的推理时对齐框架，通过价值引导搜索实现多目标LLM对齐，无需修改基础模型权重


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每个目标或偏好配置微调模型，计算成本高且不灵活，需要更轻量的多目标对齐方案

Method: 训练一组小型价值模型对应不同目标，推理时根据用户指定权重组合这些模型生成倾斜函数来调整基础模型输出分布

Result: MAVIS优于为每个目标微调模型并事后组合的基线方法，接近为用户精确偏好微调模型的理想性能

Conclusion: MAVIS提供了一种计算高效且灵活的多目标LLM对齐方法，能够在推理时动态控制模型行为

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [38] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: EventTSF是一个创新的多模态时间序列预测框架，通过自回归扩散和流匹配技术整合文本事件信息，解决了文本事件与时间序列的细粒度同步、时间不确定性和多分辨率对齐三大挑战，在8个数据集上显著优于12个基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测方法主要依赖单一模态，缺乏对文本事件等外部多模态信息的有效利用，导致上下文知识有限和模型性能不足。需要解决文本事件与时间序列之间的细粒度同步、时间不确定性和模态对齐三大核心问题。

Method: 提出EventTSF自回归生成框架，使用流匹配自回归扩散技术捕捉时间-事件交互；根据事件语义信号自适应控制流匹配时间步；采用多模态U形扩散transformer在不同分辨率下有效融合时间和文本模态。

Result: 在8个合成和真实数据集上的实验表明，EventTSF在多样化事件感知非平稳时间序列预测场景中优于12个基线方法，预测准确率提高10.7%，训练效率提升1.13倍。

Conclusion: EventTSF成功解决了多模态时间序列预测中的关键挑战，通过创新的扩散transformer架构和自适应流匹配机制，实现了文本事件与时间序列的有效融合，显著提升了非平稳时间序列的预测性能。

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [39] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: SVDformer是一个结合SVD和Transformer的新框架，用于有向图的表示学习，通过多头自注意力机制增强关键频谱分量并抑制高频噪声，在节点分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的有向图神经网络由于各向同性聚合机制和局部滤波机制，难以同时捕捉方向语义和全局结构模式。

Method: 首先通过多头自注意力精化奇异值嵌入，自适应增强关键频谱分量并抑制高频噪声，实现可学习的低通/高通图滤波；然后将奇异向量作为方向投影基，奇异值作为缩放因子，使用Transformer通过注意力权重建模入边/出边模式的多尺度交互。

Result: 在六个有向图基准测试上的广泛实验表明，SVDformer在节点分类任务上始终优于最先进的GNN和方向感知基线方法。

Conclusion: SVDformer为有向图上的表示学习建立了新范式，能够显式保持特征传播过程中的边方向性。

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [40] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于元学习的方法，通过动态设计AutoML的搜索空间来加速优化过程，减少计算成本和过拟合风险。


<details>
  <summary>Details</summary>
Motivation: 解决传统AutoML系统中高计算成本和大搜索空间导致的过拟合问题。

Method: 利用历史元知识选择有前景的搜索空间区域，动态设计搜索空间。

Result: 在Random Search中减少89%运行时间，大大缩小搜索空间，且保持竞争性能。

Conclusion: 该元学习方法能够有效加速AutoML优化过程，同时揭示了搜索空间缩减策略的承特关系。

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [41] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: ASAP是一种自适应学习率调整方法，通过计算当前和先前未标记输出的余弦距离来动态调整学习率，无需标签或模型集成即可实现快速轻量的在线标签偏移适应。


<details>
  <summary>Details</summary>
Motivation: 现实应用中机器学习模型面临在线标签偏移问题，标签分布随时间变化。传统方法需要仔细选择学习率，过低会减慢适应速度，过高会导致不稳定。

Method: 提出ASAP方法，动态调整学习率：计算当前和先前未标记输出的余弦距离，并将其映射到有界范围内。仅需使用先前的softmax输出，无需标签、模型集成或过去输入。

Result: 在多个数据集和偏移场景的实验表明，ASAP始终提高准确性和效率。

Conclusion: ASAP为无监督模型适应提供了一种实用解决方案，能够有效处理在线标签偏移问题。

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [42] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出了HCAL分类器，通过原型对比学习和自适应任务加权机制解决层次多标签分类中的结构一致性和损失平衡问题


<details>
  <summary>Details</summary>
Motivation: 解决层次多标签分类中保持结构一致性和多任务学习中损失权重平衡的关键挑战

Method: 基于多任务学习整合原型对比学习和自适应任务加权机制，包括语义一致性建模、自适应损失权重分配和原型扰动机制

Result: 在三个数据集上的广泛实验表明，相比基线模型，该分类器具有更高的分类准确率和更低的层次违规率

Conclusion: HCAL分类器通过创新的原型对比学习和自适应加权机制，有效解决了层次多标签分类中的结构一致性和优化偏差问题

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [43] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 该研究开发了一个基于t-SNE的管道，用于可视化chirp特征在不同临床结果中的分布，并通过机器学习分类器在嵌入空间中进行分类任务，最高达到88.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的可视化方法来分析chirp特征与临床结果之间的关系，为临床分层和决策支持提供见解。

Method: 使用t-SNE技术对chirp的时域、频域和频谱指标进行降维可视化，然后在2D嵌入空间上训练四种分类器（随机森林、SVM、逻辑回归、k-NN）进行三种分类任务，并使用SHAP生成特征重要性图。

Result: 随机森林和k-NN分类器表现最佳，在最优病例检测任务中达到88.8%的准确率。SHAP特征重要性图揭示了特征在嵌入空间中的局部影响模式。

Conclusion: 该集成框架展示了可解释嵌入和局部特征归因在临床分层和决策支持中的潜力，为理解数据潜在结构提供了新视角。

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [44] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: DyMixOp是一个新颖的神经算子框架，通过惯性流形理论将无限维非线性PDE动力学转换到有限维潜在空间，利用局部-全局混合变换有效捕捉细节和非线性交互，在多种PDE基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络近似非线性PDE动力学系统时的格式转换挑战，特别是处理不可线性化动力学或需要无限维空间进行线性化的情况。

Method: 基于惯性流形理论，将无限维非线性PDE动力学转换到有限维潜在空间；采用局部-全局混合(LGM)变换捕捉细节和非线性交互；构建动力学知情的架构连接多个LGM层来近似线性和非线性动力学。

Result: 在多种PDE基准测试中达到最先进性能，显著降低预测误差（在对流主导场景中高达86.7%），同时保持计算效率和可扩展性。

Conclusion: DyMixOp框架成功解决了PDE动力学近似中的关键挑战，通过理论驱动的变换和架构设计实现了优异的性能和物理可解释性。

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [45] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 一种新的不确定性可视化方法“不确定性管”，用于神经网络预测的粒子轨迹中的不确定性量化和可视化


<details>
  <summary>Details</summary>
Motivation: 神经网络预测粒子轨迹时，有效量化和可视化预测不确定性对于应用可靠性至关重要，但目前仍面临挑战

Method: 设计并实现了超椭圆形管道，能够准确捕捉和直观传达非对称不确定性，结合Deep Ensembles、MC Dropout和SWAG等不确定性量化技术

Result: 证明了不确定性管的实际用途，在合成和模拟数据集上展示了其效果

Conclusion: 不确定性管是一种计算高效的新题可视化方法，能够提升神经网络模型在关键应用中的可靠性和可信度

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [46] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: 这章论运算法不透明性的两种类型：技术复杂性导致的本质不透明和商业专利原因导致的意图不透明，并分析了解释性AI所面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 辨别不同类型的运算法不透明性及其伦理启示，以应对AI系统可解释性的挑战。

Method: 分析两种不透明性的本质和形成原因，评估现有解释性AI方法的效果和局限性。

Result: 识别了技术复杂性和商业专利两种不同的不透明性源头，并指出解释性AI仍面临重重困难。

Conclusion: 需要更深入地理解不同类型的不透明性，才能有效地应对AI系统可解释性的挑战，以支持运算法的谨慎发展。

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [47] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: 开发了MuFlex开源平台，用于多建筑灵活性协调的基准测试和控制策略验证，解决了现有测试平台在物理细节捕获和标准化方面的不足


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源在电网中渗透率增加，需要建筑群的协调需求灵活性。现有测试平台大多针对单建筑，多建筑平台有限且依赖简化模型，无法充分捕捉物理细节和中间变量，限制了作为基准测试工具的适用性

Method: 开发MuFlex平台，支持EnergyPlus建筑模型的同步信息交换，遵循最新OpenAI Gym接口，提供模块化、标准化的强化学习实现。使用Soft Actor-Critic算法进行案例研究

Result: 案例研究表明，协调四个办公建筑的灵活性需求，在保持室内环境质量的同时，将总峰值需求降低到指定阈值以下

Conclusion: MuFlex平台为多建筑灵活性协调提供了可扩展的基准测试解决方案，能够有效支持控制策略的开发和评估

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [48] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: CALYPSO是一个混合框架，结合神经网络和机制性元种群模型来预测MRSA在医疗和社区环境中的传播动态，相比机器学习基线性能提升4.5%以上


<details>
  <summary>Details</summary>
Motivation: MRSA是医院和长期护理机构的重要公共卫生威胁，现有预测模型缺乏流行病学可解释性且性能有限，机制性流行病模型难以校准且难以整合多样化数据集

Method: 整合神经网络与机制性元种群模型，利用患者级保险索赔数据、通勤数据和医疗转移模式，学习区域和时间特异性参数来捕捉MRSA传播动态

Result: CALYPSO在州级预测性能上比机器学习基线提升超过4.5%，能够识别高风险区域和制定感染预防资源分配的成本效益策略

Conclusion: CALYPSO框架能够提供准确、可解释的多空间分辨率预测，支持感染控制政策和暴发风险的反事实分析，为公共卫生决策提供有力工具

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [49] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: 提出了一种新的风险预测方法CROC，能够合并处理常见和稀有变异，提高了疾病预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基因风险预测模型主要基于常见变异，预测准确性不足以满足临床需求，需要考虑稀有变异的贡献。

Method: 提出CROC方法，是对之前FROC方法的扩展，特别添加了处理稀有变异的流程。使用533个SNPs和37个候选基因进行验证。

Result: 包含所有SNPs的预测模型准确性更高(AUC=0.605 vs 0.585)。当常见变异数量减少时，CROC方法优势更明显，在只有稀有变异的情况下AUC达到0.603，而FROC只有0.524。

Conclusion: CROC方法能够有效利用稀有变异提高风险预测的准确性，为更全面的基因风险预测提供了新的方法学基础。

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [50] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: 基于概率编程的GenHAI模型，用于预测医院患者MRSA感染风险，通过对检测结果序列建模来回答预测性、因果性和反事实问题


<details>
  <summary>Details</summary>
Motivation: MRSA被CDC认定为严重抗菌药风险，医院患者因兼并疾病、免疫抗制、抗生素使用等因素而面临高风险，需要有效的风险预测模型

Method: 提出GenHAI生成式概率模型，基于概率编程范式，对医院患者单次住院期间的MRSA检测结果序列进行建模

Result: 在两个真实世界数据集上与别的辨别式和生成式机器学习模型进行比较，证明了模型的有效性

Conclusion: GenHAI模型能够应用于医院管理者减少MRSA感染风险的决策支持，能够回答多种类型的预测性、因果性和反事实问题

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [51] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文提出了一个广义自监督对比学习框架(GLF)，将现有方法统一为对齐和约束两部分，并提出了自适应分布校准(ADC)方法来提升特征空间的类内紧凑性和类间分离性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督对比学习方法(BYOL、Barlow Twins、SwAV)虽然有效，但缺乏统一的理论框架。作者希望建立一个通用框架来分析这些方法，并解决如何在无标签情况下设计有效的约束机制来保持输入数据的类别信息。

Method: 1) 提出GLF框架，将自监督对比学习分解为对齐和约束两部分；2) 分析现有方法在GLF中的对应形式；3) 提出ADC方法，通过动态捕捉样本间关系来增强类内紧凑性和类间分离性。

Result: 理论分析和实验评估都证明了ADC方法的优越性，能够有效提升特征空间的质量，使相近样本在特征空间中更接近，相远样本更远离。

Conclusion: GLF框架为理解自监督对比学习提供了统一视角，ADC方法作为即插即用组件能够有效提升特征学习的性能，特别是在保持类别信息方面表现出色。

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [52] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: 通过量化或低精度算术进行概率推理，在离散参数空间中学习连续分布，使用概率电路实现可扩展的学习方法


<details>
  <summary>Details</summary>
Motivation: 机器学习社区需要量化或低精度算术来缩放大型模型，但现有方法在离散参数空间进行概率推理的能力有限

Method: 在量化离散参数空间进行概率推理，使用概率电路实现可处理的学习方法，考虑二维密度和量化神经网络

Result: 方法提供了管理复杂分布的可扩展解决方案，并提供了模型行为的清晰见解，多个模型验证显示了推理效率而不牺牲准确性

Conclusion: 这项工作通过利用离散近似进行概率计算，推进了可扩展、可解释的机器学习发展

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [53] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [54] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: FedOL是一个联邦学习框架，通过知识蒸馏在单轮通信中构建更大的服务器模型，使用未标记公共数据集交换预测输出而非模型参数，解决异构性和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习需要统一模型架构和多轮通信，忽视了资源异构性，给客户端带来沉重计算负担和通信开销。移动网络中客户端拥有宝贵私有数据但计算资源有限，需要更高效的解决方案。

Method: 采用知识蒸馏技术，客户端在未标记公共数据集上交换预测输出而非模型参数；引入专门的目标函数迭代优化伪标签和服务器模型；设计定制化的伪标签生成和知识蒸馏策略整合多样化知识。

Result: 仿真结果显示FedOL显著优于现有基线方法，为移动网络提供了成本效益高的解决方案。

Conclusion: FedOL通过单轮通信和知识蒸馏有效解决了联邦学习中的异构性、计算负担和通信开销问题，特别适合资源受限的移动网络环境。

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [55] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: T2W是一个基于扩散变换器的框架，通过自然语言描述生成任务特定的神经网络权重，在未见任务上表现出色并支持权重增强和文本引导模型融合等新应用。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络权重生成方法在泛化到未见任务和实际应用探索方面存在困难，需要一种能够根据文本描述生成任务特定权重的解决方案。

Method: 提出T2W扩散变换器框架，将网络参数分层处理为统一块，通过先验注意力机制集成CLIP文本嵌入，并采用对抗训练和权重空间增强来提高泛化能力。

Result: 在Cifar100、Caltech256和TinyImageNet上的实验表明，T2W能够为未见任务生成高质量权重，优于基于优化的初始化方法。

Conclusion: 该工作将文本语义与权重空间动态联系起来，通过开源的文本-权重对数据集推进了生成模型在神经网络参数合成中的实用性。

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [56] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: 通过利用随机梯度的本质变化自动调整学习率，无需手动调参，在SGD、SGDM和SIGNSGD等算法中展现高效和稳定性


<details>
  <summary>Details</summary>
Motivation: 解决现有学习率调整方法复杂、需手动调参、计算成本高的问题

Method: 基于随机二阶算法发展可解释的学习率调整机制，根据随机梯度的模长度自动增减学习率

Result: 新方法在多种经典随机算法和机器学习任务中展现了高效率、稳健性和扩展性

Conclusion: 通过利用随机梯度的本质特性可以实现无参数调整的自动学习率调整，为深度学习训练提供了更简洁高效的解决方案

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [57] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: 基于套庄协作机制的个性化子图联邦学习框架FedSheafHN，通过图级嵌入和套庄扩散丰富客户表征，使用超网络生成定制模型，在多个图数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 子图联邦学习中，客户端子图数据分布异质性导致模型性能差异较大，需要解决个性化模型性能变化问题。

Method: 构建服务器协作图嵌入客户子图，利用套庄扩散丰富客户表征，通过超网络生成定制化客户模型。

Result: 在多个图数据集上表现超过现有个性化子图联邦学习方法，模型收敛速快且能够有效泛化到新客户。

Conclusion: FedSheafHN框架通过套庄协作机制有效解决了子图联邦学习中的数据异质性问题，提供了高效的个性化模型生成方案。

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [58] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT是一种可扩展的训练中子集选择方法，通过低秩特征表示和Fast MaxVol采样器选择多样化子集，动态调整子集大小，在保持训练轨迹的同时减少计算时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在大数据集上训练的计算和环境成本很高，需要一种高效的训练方法来减少计算时间、能耗和碳排放。

Method: 提取每批数据的低秩特征表示，应用Fast MaxVol采样器选择覆盖主导子空间的小型多样化子集，使用梯度近似准则动态调整子集大小。

Result: 在多个基准测试中，GRAFT在准确性和效率方面匹配或超越现有选择基线，在准确性、效率和排放之间提供了有利的权衡。

Conclusion: GRAFT通过低秩子空间操作和精心选择的训练样本，有效减少了训练时间、能耗和碳排放，同时保持了训练性能。

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [59] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 输入时间缩放新范式，通过在训练和测试时组合元知识精炼输入，反转"垃圾进垃圾出"假设，在少量数据上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 补充现有的数据缩放和推理缩放方法，探索在输入时间资源投入的新缩放模式

Method: 在训练和测试阶段都使用元知识精炼输入的策略，发现训练-测试协同设计的重要性

Result: 在Qwen2.5-32B-Instruct上达到AIME24(76.7%)和AIME25(76.7%)的SOTA性能，多模型投票可达AIME25(80%)，从DeepSeek-R1-Distill-Qwen-32B起步可达AIME24(86.7%)

Conclusion: 输入时间缩放是有效的新范式，质量较低的数据反而能获得更好性能，数据集大小缩放需谨慎考虑，少数高质量示例即可激活高级推理能力

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [60] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 扩展CASH框架以适应现代ML流程，提出PS-PFN方法，通过后验采样和先验数据拟合网络高效选择最优ML管道


<details>
  <summary>Details</summary>
Motivation: 传统AutoML的CASH方法无法适应包含预训练模型微调、集成等现代ML流程的异质性需求

Method: 提出PS-PFN方法，将后验采样扩展到最大k臂老虎机问题，利用先验数据拟合网络通过上下文学习高效估计最大值后验分布

Result: 在一个新基准和两个现有基准任务上，PS-PFN相比其他老虎机和AutoML策略表现出优越性能

Conclusion: PS-PFN成功扩展了CASH框架，为现代异质性ML管道选择提供了有效的AutoML解决方案

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [61] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 提出了一种基于自注意力的全可微分通信模块，用于多智能体强化学习中的信息交换，可与任何动作价值函数分解方法无缝集成，在SMAC基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL通信协议通常复杂且不可微分，需要一种能够以奖励驱动方式学习生成消息的通信机制。

Method: 引入基于自注意力的通信模块，在智能体之间交换信息，模块完全可微分，参数数量固定且与智能体数量无关。

Result: 在SMAC基准测试中，该方法在多个地图上实现了最先进的性能表现。

Conclusion: 提出的自注意力通信模块为MARL提供了一种有效且可扩展的通信解决方案，能够显著提升多智能体协作任务的性能。

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [62] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: 提出了一个对抗性重尾bandit问题的通用框架，通过精心设计的奖励函数和HT-SPM学习率，在重尾MAB和线性bandit中实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注随机环境下的重尾bandit问题，对抗性环境的研究很少且仅限于MAB特例，需要开发适用于对抗性重尾bandit的通用算法框架。

Method: 采用FTRL方法，在损失估计上添加奖励函数，设计了HT-SPM数据依赖学习率，并使用方差缩减的线性损失估计器。

Result: 在重尾MAB中实现了Õ(T^(1/ε))对抗性遗憾和Õ(log T)随机性遗憾；在线性bandit中实现了Õ(d^(1/2)T^(1/ε))遗憾，与随机环境最佳结果匹配。

Conclusion: 该框架为对抗性重尾bandit问题提供了首个通用解决方案，在MAB和线性bandit中都实现了最佳性能，填补了该领域的研究空白。

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [63] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: 数据驱动的单机器调度算法，结合机器学习咄问题特性，在最小化迟到作业总重量问题上显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 传统的准确算法在某些问题区域性能会明显下降，而数据驱动方法可以根据特定数据集结构提供更强大的可扩展性能

Method: 提出了一种新题的数据驱动调度算法，将机器学习与问题特定特征相结合，确保解的可行性，并进行了系统的ML模型选择分析

Result: 实验结果显示该方法在最优距离、最优解数量咄适应不同数据场景方面显著超越了现有最先进方法

Conclusion: 该研究不仅提供了高效的调度算法，还填补了ML模型选择方面的研究空白，为实际应用提供了灵活的解决方案

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [64] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: DREAMS是一种新的降维方法，通过正则化项结合t-SNE的局部结构保持和PCA的全局结构保持，能够在多个尺度上同时保留数据的局部和全局结构。


<details>
  <summary>Details</summary>
Motivation: 现有的降维方法通常只能很好地保留数据的局部结构（如t-SNE、UMAP）或全局结构（如MDS、PCA），但没有方法能够同时很好地保留这两个方面。

Method: 通过简单的正则化项将t-SNE的局部结构保持与PCA的全局结构保持相结合，生成从局部结构良好的t-SNE嵌入到全局结构良好的PCA嵌入的频谱嵌入。

Result: 在七个真实世界数据集（包括五个单细胞转录组学数据集和一个人口遗传学数据集）上进行基准测试，定性和定量地展示了DREAMS在多个尺度上保持结构的优越能力。

Conclusion: DREAMS方法能够有效平衡局部和全局结构的保持，在多个尺度上优于先前的方法。

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [65] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [66] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: RLVR深度和幅度两个维度的优化：DARS通过难度适应性重重样本提升难题解决能力，大批量训练通过扩大数据幅度提升Pass@1性能。两者形成正交优益。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在深度（难题难度）和幅度（数据量）两个维度上存在偏偏，GRPO算法对中等准确度样本过度重视，而忽视了低准确度的难题样本对推动思维边界的重要性。

Method: 1. DARS：难度适应性滚动金样本方法，通过多阶段滚动重新权重难题，增加难题的正面滚动数量
2. 大批量训练：扩大批处理规模，用全批更新替代PPO的小批次迭
3. DARS-B：结合DARS和大幅度训练

Result: DARS在不增加推理成本的情况下持续提升Pass@K性能；大幅度训练显著提升Pass@1性能；DARS-B同时提升Pass@K和Pass@1，保持高令牌级熵但减少梯度噪声

Conclusion: 深度适应性探索和幅度扩展是RLVR中两个正交的优化维度，同时关注这两个方向能够充分发挥大语言模型的思维能力。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [67] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: PENGUIN是一种新的时间序列预测模型，通过周期嵌套组注意力机制显式建模周期模式，在多个标准数据集上超过了MLP和Transformer基础模型的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型在预测任务中取得了重要突破，但其在时间序列预测中的有效性仍有争议。需要重新考虑自注意力机制的重要性，并显式建模周期模式。

Method: 提出周期嵌套相对注意偏置，直接捐捕周期结构。使用组注意力机制处理多重共存周期性（如日周期和周周期），每个组使用多查询注意力机制针对特定周期性。

Result: 在多样化的标准数据集上进行了涉及广泛的实验，结果显示PENGUIN一贯地超过了基于MLP和Transformer的模型。

Conclusion: PENGUIN通过显式建模周期模式和组注意力机制，为长期时间序列预测提供了一种简单但有效的方案，证明了显式周期模建在时间序列建模中的重要性。

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [68] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 通信效率是联邦学习的关键瓶颈，本文提出ISP机制动态选择每轮客户端数量，在保持模型准确性的同时节省30%通信开销


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信效率问题特别是在异构和动态客户参与情况下，现有方法对每轮选择多少客户端的问题研究不深入

Method: 提出ISP（Intelligent Selection of Participants）机制，动态确定每轮最优客户端数量

Result: 在视觉Transformer、实际ECG分类和梯度压缩等多种场景下，均能节省达30%通信开销且不影响最终模型质量

Conclusion: 选择合适的客户端数量是联邦学习中一个独立且重要的任务，ISP机制能够有效解决这一问题

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [69] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: 基于随机学习自动机和带内网络测量的分布式网络内强化学习框架，实现了可程序网络中的自适应路径选择


<details>
  <summary>Details</summary>
Motivation: 解决网络拥塞问题，通过分布式、数据驱动的转发决策来动态适应网络条件变化

Method: 结合随机学习自动机(SLA)和带内网络测量(INT)实时远程数据，在P4可程序BMv2交换机上构建测试平台

Result: 系统能够收敛到有效的路径选择，并在线速度下适应网络条件变化

Conclusion: 证明了分布式网络内强化学习框架在自适应路径选择中的可行性和效果

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [70] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: 首个基于主观逻辑的正式框架，用于评估AI训练数据集的可信质量，特别是数据集级别的偏见等全局属性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越依赖训练数据，评估数据集的可信质量变得至关重要，特别是对于仅在数据集整体层面出现的公平性、偏见等属性。现有方法无法有效处理这类全局属性评估。

Method: 构建在主观逻辑之上的正式框架，支持信任命题和在证据不完整、分布式或冲突场景中量化不确定性。将框架应用于偏见属性评估，并在交通标志识别数据集上进行实验。

Result: 方法能够抓取类别不平衡，在集中和联邦学习环境下都保持解释性和稳健性。实验结果验证了框架的有效性。

Conclusion: 该框架为AI训练数据集的可信质量评估提供了第一个正式方法，特别是在处理全局属性如偏见时能够量化不确定性，具有良好的应用前景。

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [71] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: 使用解聆VAE和拟合负载量量空间的新题换方法来改善表格数据上的不平衡回归问题


<details>
  <summary>Details</summary>
Motivation: 不平衡分布学习是预测建模中的常见挑战，但现有方法多举中分类问题，对于不平衡回归问题的研究较少

Method: 提出结合解聆变分自动编码器(VAE)和在负载量量空间中应用拟合负载量量的创新数据生成方法

Result: 通过在IR标准数据集上的数值比较评估方法的效果

Conclusion: 该方法为解决表格数据上的不平衡回归问题提供了有效的新途径

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [72] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: 这篇论文系统性比较了一次性剪枝和迭代剪枝两种深度学习模型压缩方法，发现两者在不同剪枝比下各有优势，并提出了一种新的混合剪枝策略。


<details>
  <summary>Details</summary>
Motivation: 虽然迭代剪枝历史上更广泛采用，但这种偏好并没有经过严格测试。需要系统性地比较两种方法的实际效果。

Method: 进行了系统性的对比研究，包括严格定义、在结构化和非结构化设置下的测试、应用不同剪枝标准和模态，并提出了耐心基于剪枝和混合方法。

Result: 发现一次性剪枝在低剪枝比下效果更好，而迭代剪枝在高剪枝比下表现更优。混合方法在某些场景下能超过传统方法。

Conclusion: 研究为实践者选择剪枝策略提供了价值丰富的见解，应根据具体目标和约束来选择合适的剪枝方法。

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [73] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: FedUP是一种轻量级的联邦遗忘算法，通过剪枝特定连接来高效消除恶意客户端对全局模型的影响，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到模型投毒攻击，传统联邦遗忘假设客户端可信且合作，但在恶意客户端可能共谋的情况下难以应用，需要新的解决方案。

Method: 利用最后一轮训练中客户端的权重，识别并抑制良性客户端和恶意客户端之间差异最大的高幅度权重连接，通过精心选择和置零这些权重来隔离恶意影响。

Result: 在IID和非IID数据、标签翻转和后门攻击等多种场景下，FedUP能有效降低恶意数据准确率至重新训练模型水平，同时保持良性数据性能，比现有方法更快且节省存储。

Conclusion: FedUP提供了一种高效、鲁棒的联邦遗忘解决方案，能够在恶意客户端环境下有效消除其影响，同时保持模型性能和操作效率。

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [74] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: 重新评估生物识别模态通过24位专家调查，发现面部识别因技术进步而提升，指纹识别因漏洞而降级，专家评估与数据集结果高度一致


<details>
  <summary>Details</summary>
Motivation: 现有生物识别评估框架过时（1998年），无法反映最新技术进步和新兴安全漏洞，需要更可靠的评估方法

Method: 通过24位生物识别专家调查，评估各模态的属性分数，并与55个生物识别数据集的实验结果进行对比分析

Result: 专家评分显示显著变化：面部识别因技术进步而提升，指纹识别因攻击漏洞而可靠性降低，专家评估与数据集结果高度一致

Conclusion: 专家见解与实证数据结合能提供可靠评估，专家分歧之处揭示了关键挑战，为未来研究指明方向

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [75] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: 提出Fisher-正交投影(FOP)方法，解决大批量训练时二阶优化方法失效的问题，通过利用两个子批次的梯度信息构建方差感知的更新方向


<details>
  <summary>Details</summary>
Motivation: 现代GPU支持大批量训练(数万个样本)，但现有优化器在大批量下表现不佳。一阶方法梯度噪声减少难以逃离局部极小值，二阶方法(KFAC等)需要过高阻尼导致曲率信息丢失

Method: FOP技术利用两个子批次的梯度，在Fisher度量下构建与平均梯度正交的梯度差异分量，增强平均梯度形成方差感知的更新方向

Result: FOP恢复了二阶方法在大批量下的有效性，实现了可扩展训练，具有更好的泛化能力和更快的收敛速度

Conclusion: FOP是一种新颖的技术，能够在大批量训练中有效利用二阶信息，解决了现有优化器在大批量场景下的局限性

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [76] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: 通过将散布Q学习重构为流匹配框架，OFQL实现了高效的一步动作生成，在保持性能的同时大幅缩短训练和推理时间


<details>
  <summary>Details</summary>
Motivation: 当前散布Q学习(DQL)在离线强化学习中表现突出，但依赖多步去噪过程，导致训练和推理效率低下，直接采用一步去噪则性能会大幅下降

Method: 提出One-Step Flow Q-Learning (OFQL)，将DQL重构为样本效率高的流匹配(FM)框架，学习平均速度场以支持直接准确的动作生成，避免使用辅助模型或多阶段训练

Result: 在D4RL基准测试中，OFQL表现超过DQL和其他散布基线方法，同时大幅减少了训练和推理时间

Conclusion: OFQL通过流匹配框架成功解决了散布模型在决策中的效率问题，实现了高效的一步动作生成，为离线强化学习提供了更实用的解决方案

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [77] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于边缘设备的端到端预测框架，通过轻量化模型和硬件感知部署来实现基础设施溢流水位的能消预测，解决依赖云计算的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件加剧了老化合流制污水系统的压力，需要准确预测溢流水位来提前干预。传统AI预测方法依赖云计算，在通信故障时可靠性低。

Method: 集成轻量化Transformer和LSTM模型，通过整数量化压缩代码。使用自动化硬件感知部署流程，在AMD Spartan-7 FPGA上搜索最优模型配置。

Result: 8位Transformer模型准确度高(MSE 0.0376)，每次推理能耗0.370mJ。8位LSTM模型能耗更低(0.009mJ)但准确度差14.89%，训练时间更长。

Conclusion: 工作实现了本地化、能效预测，为合流制污水系统提供更高弹性。根据部署优先级选择LSTM(超低能耗)或Transformer(高准确度)。

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [78] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: 本文提出使用分类分布代替高斯分布来构建多模态策略，以解决连续控制中探索不足的问题，通过潜在分类分布选择行为模式，实现更好的探索和更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中的策略通常参数化为单峰高斯分布，限制了学习行为的多模态性。许多实际决策问题需要多模态策略来促进环境探索，解决稀疏奖励、复杂动态或需要适应不同上下文的学习挑战。

Method: 引入分类策略，使用中间分类分布建模多模态行为模式，然后基于采样模式生成输出动作。探索了两种采样方案，确保可微分离散潜在结构同时保持高效的基于梯度的优化。

Result: 在DeepMind Control Suite环境上的评估表明，通过更好的探索，学习到的策略收敛更快且优于标准高斯策略。

Conclusion: 分类分布是连续控制中结构化探索和多模态行为表示的有力工具。

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [79] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: 对53种自动特征工程方法的可用性调查显示，现有方法存在使用困难、缺乏文档和社区支持等问题，并缺乏时间和内存限制功能


<details>
  <summary>Details</summary>
Motivation: 自动化特征工程对机器学习性能至关重要，但现有方法在实践中的可用性未经过详细研究

Method: 调查了53种自动特征工程方法，分析其可用性、文档质量、社区活跃度和功能特性

Result: 现有方法普遍存在使用困难、文档不足、没有活跃社区支持，且无法设置时间和内存限制

Conclusion: 未来需要重点关注可用性高、工程化设计的自动特征工程方法

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [80] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了两种表格设置算法和一种函数近似设置算法来解决随机最短路径(SSP)问题，并在各自设置中展现优秀性能。


<details>
  <summary>Details</summary>
Motivation: 随机最短路径(SSP)问题是强化学习中的重要类别，因为其他类型的成本标准都可以在SSP设置中进行形式化。

Method: 提出了两种表格设置算法和一种函数近似设置算法，并证明了所有算法的近似几乎必然收敛性。

Result: 在表格设置中，算法表现出艰优于其他知名的收敛性强化学习算法；在函数近似设置中，算法也显示出比其他算法更可靠的性能。

Conclusion: 该研究为随机最短路径问题提供了高效的解决方案，在不同设置下都取得了显著的性能收益。

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [81] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: AutoScale是一个无需昂贵权重搜索的两阶段框架，通过多任务优化指标来指导线性标量化权重选择，在多个数据集上展现出高效且优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现固定任务权重的线性标量化可以达到与复杂多任务优化方法相当甚至更好的性能，但尚不清楚为什么某些权重表现优异以及如何在不依赖穷举超参数搜索的情况下确定这些权重。

Method: 建立了线性标量化与多任务优化方法的直接联系，发现性能良好的标量化权重在关键MTO指标上表现出特定趋势（如高梯度幅度相似性）。基于此提出了AutoScale两阶段框架，使用MTO指标来指导权重选择。

Result: AutoScale在包括新的大规模基准测试在内的多样化数据集上始终展现出优越的性能和高效率。

Conclusion: 通过揭示线性标量化权重与多任务优化指标之间的关系，提出的AutoScale框架能够有效指导权重选择，避免了昂贵的权重搜索过程，实现了高效且优越的多任务学习性能。

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [82] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: \u591a\u7528\u6237\u4e0a\u4e0b\u6587\u7ea7\u8054\u68af\u5e26\u5b50\u6a21\u578b\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\u5e76\u8bc1\u660e\u9057\u61be\u754c\u4f18\u52bf


<details>
  <summary>Details</summary>
Motivation: \u6a21\u62df\u5b9e\u9645\u5728\u7ebf\u5e7f\u544a\u573a\u666f\uff0c\u89e3\u51b3\u591a\u7528\u6237\u540c\u65f6\u4ea4\u4e92\u3001\u987a\u5e8f\u5c55\u793a\u9879\u76ee\u548c\u5f02\u8d28\u5956\u52b1\u7684\u6311\u6218

Method: \u63d0\u51faUCBBP\u548cAUCBBP\u4e24\u79cd\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e0a\u4fe1\u9650\u9884\u6d4b\u4e0e\u5411\u540e\u89c4\u5212\uff0c\u5904\u7406\u8054\u68af\u53cd\u9988\u548c\u5e76\u884c\u4e0a\u4e0b\u6587

Result: UCBBP\u9057\u61be\u754c\u4e3a$\widetilde{O}(\sqrt{THN})$\uff0cAUCBBP\u8fbe\u5230$\widetilde{O}(\sqrt{T+HN})$\uff0c\u5728\u7528\u6237\u7f29\u653e\u65b9\u9762\u66f4\u9ad8\u6548

Conclusion: \u65b0\u6846\u67b6\u6709\u6548\u6a21\u62df\u591a\u7528\u6237\u5e7f\u544a\u573a\u666f\uff0cAUCBBP\u5728\u7528\u6237\u89c4\u6a21\u6269\u5c55\u65f6\u663e\u793a\u660e\u663e\u4f18\u52bf

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [83] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: KMR框架是一个统一的深度学习模型效率技术表示和推理形式化体系，将剪枝、量化、知识蒸馏等方法抽象为可控旋钮、确定性规则和可测量仪表的三元组结构。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型效率优化方法众多但缺乏统一的形式化表示，难以系统组合和理论分析，需要建立统一的数学框架来整合各种效率技术。

Method: 提出KMR三元组框架（Knob-Meter-Rule），将效率技术抽象为可控参数（Knob）、测量指标（Meter）和优化规则（Rule），并开发Budgeted-KMR算法进行迭代预算优化。

Result: 成功将主流效率方法实例化为KMR三元组，揭示了方法间的内在联系，支持混合管道构建，为自动化策略学习和动态适应提供了基础。

Conclusion: KMR框架为模型效率研究提供了统一的概念和实践工具，促进了多种技术的系统组合和理论分析，为未来自动化效率优化奠定了基础。

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [84] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可差分的直通估计器方法，通过学习比特宽、噪声缩放和限定边界来优化量化神经网络，在极端低比特宽设置下仍能保持竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络会导致每个层的噪声增加和容量下降，需要找到一种方法来进行平滑的约束优化以解决量化瓶颈问题。

Method: 使用可学习比特宽、噪声缩放和限定边界的全可微分直通估计器(STE)，通过外点罚法来强制目标比特宽，并使用软指标（通过知识精炼）稳定训练。

Result: 该方法在极端的W1A1（每个参数1比特、激活函数1比特）设置下仍能获得竞争力的准确性，同时保持了STE的高效率。

Conclusion: 这种简单但有效的方法通过对量化过程进行平滑优化，成功解决了低比特宽量化时的性能瓶颈问题。

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [85] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: ASDFormer是一个基于Transformer的架构，结合混合专家池化分类器，用于从fMRI数据中识别自闭症谱系障碍的神经特征，在ABIDE数据集上实现了最先进的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍(ASD)与大脑连接性异常相关，功能MRI能够捕捉大规模神经动态。有效识别ASD相关的连接模式异常对于改善诊断和生物标志物发现至关重要。

Method: 提出ASDFormer架构，整合Transformer和混合专家池化分类器(MoE)，通过注意力机制自适应强调与自闭症相关的大脑区域和连接模式。

Result: 在ABIDE数据集上实现了最先进的诊断准确性，并揭示了与ASD相关的功能连接中断的稳健见解。

Conclusion: ASDFormer不仅提高了分类性能，还提供了更可解释的疾病相关生物标志物识别，展示了作为生物标志物发现工具的潜力。

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [86] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: 提出了基于类型拓扑空间的数据集分析方法，通过定义特殊类型集和类型拓扑，将数据集组织为有序轨迹和分支结构，并用伪树表示关系，为凸包计算、聚类等问题提供新算法平台


<details>
  <summary>Details</summary>
Motivation: 现有数据集研究主要基于统计方法和代数拓扑方法，需要从一般拓扑学角度开发新的分析方法来处理有限拓扑空间（如数据集）

Method: 引入类型拓扑空间概念，为开集分配类型；开发特殊类型集及其相关类型拓扑；将数据集组织为有序轨迹和组件；用整数序列表示组件顺序；通过伪树（typed-II pseudotree）表示分支关系

Result: 建立了数据集内部结构分析的新框架，能够将R^2空间中的数据集分解为有序的轨迹组件和分支结构

Conclusion: 类型拓扑方法为数据集分析提供了新的拓扑学视角，为凸包计算、孔洞检测、聚类和异常检测等问题的算法设计提供了新的平台

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [87] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 提出了一种高效的知识图谱遗忘算法，通过泰勒展开和Fisher矩阵近似来避免昂贵的二阶导数计算，显著提升了遗忘效率和效果。


<details>
  <summary>Details</summary>
Motivation: 由于"被遗忘权"等法规要求，需要从模型中移除训练数据及其影响。完全重新训练成本高昂，而现有方法在处理知识图谱特有结构和语义关系时面临挑战。

Method: 定义了知识图谱遗忘的影响函数，使用泰勒展开估计参数变化，通过Fisher矩阵和零阶优化近似逆Hessian向量积，避免构建计算图和计算昂贵的一阶、二阶导数。

Result: 实验结果表明，该方法在遗忘效率和遗忘质量方面显著优于其他最先进的图遗忘基线方法。

Conclusion: 提出的方法为大规模知识图谱的高效遗忘提供了有效解决方案，在计算效率和性能方面都有显著优势。

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [88] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: BLIPs是一个贝叶斯学习原子间势能框架，通过变分dropout为机器学习原子间势能提供校准的不确定性估计，在数据稀缺和分布外场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 传统机器学习原子间势能(MLIPs)在分布外数据和数据稀缺情况下预测不准，且缺乏不确定性估计，无法指导主动学习和确保模拟准确性

Method: 基于自适应变分dropout的可扩展架构无关变分贝叶斯框架，可训练或微调MLIPs，兼容等变消息传递架构

Result: 在计算化学任务中显示比标准MLIPs更好的预测准确性，提供可信的不确定性估计，特别是在数据稀缺和分布外场景下

Conclusion: BLIP框架能够为MLIPs提供校准的不确定性估计，在数据稀缺和分布外情况下表现优异，微调预训练模型能带来持续性能提升

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [89] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: LEOPARD算法通过结合偏好反馈和专家演示来学习奖励函数，在有限数据情况下显著优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 强化学习中为复杂任务设计合适的奖励函数很困难，现有方法结合偏好和演示数据时往往不够灵活或难以扩展

Method: 提出了reward-rational partial orderings框架，开发了LEOPARD算法，能够从多种反馈类型（包括负面演示）中学习奖励函数

Result: 在有限偏好和演示反馈数据情况下，LEOPARD相比现有基线方法有显著优势，结合多种反馈类型通常比单一反馈更有效

Conclusion: LEOPARD提供了一个灵活可扩展的框架，能够有效利用多种人类反馈类型来学习奖励函数，在复杂任务中表现出色

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [90] [Multi-Metric Algorithmic Complexity: Beyond Asymptotic Analysis](https://arxiv.org/abs/2508.13249)
*Sergii Kavun*

Main category: cs.PF

TL;DR: 提出加权操作复杂度模型，为不同指令类型分配多维度成本值（计算量、能耗、碳足迹、金钱成本），通过自动化代码分析实现架构感知的算法效率评估


<details>
  <summary>Details</summary>
Motivation: 传统算法分析将所有基本操作视为等成本，掩盖了现代处理器上不同类型计算在时间、能耗和成本方面的显著差异，需要更实用的多维度评估方法

Method: 开发加权操作复杂度模型，为不同指令类型分配多维度成本权重，支持用户自定义优先级，通过开源工具实现自动化代码分析和性能测量集成

Result: 验证显示与实测数据强相关（ρ>0.9），在多目标场景下优于Big-O、ICE和EVM gas等基线方法，能准确预测跨架构的时间/能耗

Conclusion: 该模型补充了现有理论模型，实现了考虑性能、可持续性和经济因素的实用化架构感知算法比较，为多维度算法效率评估提供了有效工具

Abstract: Traditional algorithm analysis treats all basic operations as equally costly,
which hides significant differences in time, energy consumption, and cost
between different types of computations on modern processors. We propose a
weighted-operation complexity model that assigns realistic cost values to
different instruction types across multiple dimensions: computational effort,
energy usage, carbon footprint, and monetary cost. The model computes overall
efficiency scores based on user-defined priorities and can be applied through
automated code analysis or integrated with performance measurement tools. This
approach complements existing theoretical models by enabling practical,
architecture-aware algorithm comparisons that account for performance,
sustainability, and economic factors. We demonstrate an open-source
implementation that analyzes code, estimates multi-dimensional costs, and
provides efficiency recommendations across various algorithms. We address two
research questions: (RQ1) Can a multi-metric model predict time/energy with
high accuracy across architectures? (RQ2) How does it compare to baselines like
Big-O, ICE, and EVM gas? Validation shows strong correlations (\r{ho}>0.9) with
measured data, outperforming baselines in multi-objective scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [91] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: MELISO+是一个全栈分布式内存计算框架，通过两层纠错机制和分布式RRAM计算架构，解决了阻变存储器非理想性和可扩展性问题，实现了超过65,000×65,000维度的矩阵计算，显著提升了能效和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统架构由于能耗密集型数据移动导致计算需求激增，而RRAM内存计算虽然能解决这一问题，但面临设备非理想性和大规模计算任务可扩展性差的挑战。

Method: 提出MELISO+框架，包含新颖的两层纠错机制来缓解设备非理想性，并开发分布式RRAM计算框架支持大规模矩阵运算。

Result: 将设备非理想性引起的一阶和二阶算术误差降低90%以上，能效提升3-5个数量级，延迟降低100倍，使低精度RRAM设备在精度、能耗和延迟指标上超越高精度替代方案。

Conclusion: MELISO+通过算法-硬件协同设计和可扩展架构的统一，显著推进了适用于大语言模型和生成式AI等应用的可持续高维计算。

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [92] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS分子动力学代码通过集成Kokkos性能可移植库，成功适应现代异构计算环境，在多种GPU架构和美国三大百亿亿次超级计算机上展示了优异的性能可移植性和强扩展性。


<details>
  <summary>Details</summary>
Motivation: LAMMPS作为世界级的分子动力学代码，需要适应现代异构计算环境，利用不同厂商和世代的GPU硬件来提升计算性能。

Method: 将Kokkos性能可移植库集成到现有的C++代码中，研究简单对势、多体反应势和机器学习力场三种原子间势的性能可移植性。

Result: 在不同厂商和世代的GPU上获得了优异的性能表现，分析了浮点运算吞吐量、内存带宽、缓存能力和线程原子操作性能，并在美国三大百亿亿次超级计算机上展示了强扩展性。

Conclusion: LAMMPS通过Kokkos库成功实现了跨平台性能可移植，为分子动力学模拟在现代异构计算架构上的高效运行提供了有效解决方案。

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [93] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: 该论文分析了迭代模板操作中MPI通信优化（非阻塞、持久性和分区通信）的性能表现，展示了持久性通信可带来37%加速，分区通信可达68%加速。


<details>
  <summary>Details</summary>
Motivation: 大规模并行应用中迭代模板操作的性能主要受通信成本主导，需要研究MPI优化技术来减少开销和提高通信效率。

Method: 使用Comb基准测试套件评估非阻塞、持久性和分区通信例程的性能，分析不同规模下的优化效果，并研究进程数、线程数和消息大小对分区通信的影响。

Result: 测量结果显示，持久性MPI通信相比基线可提供高达37%的加速，分区MPI通信可提供高达68%的加速。

Conclusion: MPI持久性和分区通信优化能显著提升迭代模板应用的通信性能，特别是在大规模并行环境中效果更为明显。

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [94] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: OrbitChain是一个用于地球观测星座的协作分析框架，通过将分析任务分解为微服务并在多卫星间分配计算资源，实现实时地球观测分析，减少通信开销并提高分析效率。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测卫星由于带宽限制和连接时间有限，数据下载和分析需要数小时甚至数天，无法满足实时应用需求（如灾害响应）。

Method: 将分析应用分解为微服务，在多卫星间分配计算资源；设计流量路由算法最小化星间通信开销；采用流水线工作流实现实时任务完成。

Result: 实验显示系统比现有框架多完成60%的分析工作量，通信开销减少高达72%。

Conclusion: OrbitChain能够实现实时地球观测分析，支持时间敏感应用和星座间协作，显著提升分析效率和减少通信成本。

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [95] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: \u901a\u8fc7\u5229\u7528\u591a\u4e2aCPU\u6838\u5fc3\u4e3a\u6bcf\u4e2aGPU\u52a0\u901f\u5927\u89c4\u6a21\u5168\u51cf\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u901f\u5ea6\u63d0\u5347


<details>
  <summary>Details</summary>
Motivation: \u73b0\u4ee3\u5f02\u6784\u8ba1\u7b97\u8282\u70b9\u5305\u542b\u591aGPU\u548c\u6570\u5341\u523\u767eCPU\u6838\u5fc3\uff0c\u4f46\u5e76\u884c\u5e94\u7528\u901a\u5e38\u53ea\u4f7f\u7528\u5355\u4e2aCPU\u6838\u5fc3\u6bcfGPU\uff0c\u5176\u4f59CPU\u6838\u5fc3\u7a7a\u95f2\uff0c\u901a\u4fe1\u6210\u4e3a\u5168\u51cf\u64cd\u4f5c\u7684\u6027\u80fd\u74f6\u9888

Method: \u6269\u5c55\u9053\u8def\u610f\u8bc6\u51cf\u7ea6\u6280\u672f\u5230GPU\uff0c\u5e76\u4f7f\u7528\u591a\u4e2aCPU\u6838\u5fc3\u6bcfGPU\u6765\u52a0\u901f\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u5b9e\u73b0\u591aCPU\u52a0\u901f\u7684GPU\u9053\u8def\u5168\u51cf\u64cd\u4f5c

Result: \u5728NCSA Delta\u8d85\u7b97\u673a\u7684NVIDIA A100 GPU\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad82.45\u500d\u7684MPI\u5168\u51cf\u901f\u5ea6\u63d0\u5347\uff0c\u5728NVIDIA\u548cAMD\u7684\u96c6\u7fa4\u901a\u4fe1\u5e93\u4e2d\u5206\u522b\u83b7\u5f971.77\u500d\u548c1.71\u500d\u7684\u901f\u5ea6\u63d0\u5347

Conclusion: \u901a\u8fc7\u5145\u5206\u5229\u7528\u5f02\u6784\u8282\u70b9\u4e2d\u7684\u591a\u4f59CPU\u6838\u5fc3\u8d44\u6e90\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21GPU\u901a\u4fe1\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7b49\u5e76\u884c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [96] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: 该研究给出了DDoS攻击的类型、检测和防范技术的全面概述，为组织和个人提供了安全防护指南


<details>
  <summary>Details</summary>
Motivation: DDoS攻击越来越复杂和频繁，对组织和个人构成严重威胁，需要综合性的检测和防范方案

Method: 分析了体积、协议层和应用层DDoS攻击的特征，评估了包过滤、入侵检测系统、机器学习等检测技术，以及防火墙、速率限制、CPP和ELD机制等防范方法

Result: 研究对各种DDoS攻击类型和防护技术进行了系统性评估，明确了不同方法的优势、局限性和适用场景

Conclusion: 该研究提供了一个完整的DDoS攻击防护框架，为安全专业人员和组织提供了实用的安全建议和最佳实践

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [97] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: 这篇论文提出了LUNDIsim数据集，为地球科学和油气工程预备了一个具有多分辨率和多环境的无缺陷地质网格模型，为数据压缩算法和网格处理工作流提供标准化测试基准。


<details>
  <summary>Details</summary>
Motivation: 数值模拟产生的科学数据量快速增长，导致计算性、可解释性和可持续性问题。特别是在地球科学领域，如气候研究，需要有效的数据管理方案来应对效率、差异、多样性、可解释性和可用性等评估问题。

Method: 提出了LUNDIsim数据集，这是一个受SPE10挑战演练的示例性无缺陷地质网格。该数据集包含四种不同的地下环境，增强了孔隙度/渗透率数据集，为多孔介质中的流动模拟而设计。提供了多种一致的分辨率（包括HexaShrink多尺度表示），并配备了油展工程上下文中重现典型两相流模拟的油展特征集。

Result: 开发了LUNDIsim网格数据集，通过Zenodo平台提供公开访问（DOI: 10.5281/zenodo.14641958）。该数据集遵循FAIR原则（可发现、可访问、可互操作、可重用），并包含最小可重现示例（MRE）辅助数据。

Conclusion: LUNDIsim数据集为数据大小缩减（上缩放）和真正的复合网格压缩算法的测试和评估提供了标准化的基准。同时也适用于地质学和油展工程中的其他高级网格处理工作流，从可视化到机器学习。

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [98] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: 开发了一个用于Batsim模拟器的碳足迹插件，通过计算模拟运行期间的CO₂排放来评估数据中心任务和资源管理策略的环境影响。


<details>
  <summary>Details</summary>
Motivation: 为了全面评估数据中心任务和资源管理策略相关的环境影响，需要能够计算碳排放的工具来帮助研究人员分析调度策略的碳效率。

Method: 在SimGrid框架内开发插件，基于模拟平台的能耗和模拟机器的碳强度因子来计算碳排放，然后集成到Batsim模拟器中。

Result: 成功开发并集成了碳足迹插件，确保与现有模拟工作流程兼容，使研究人员能够评估其调度策略的碳效率。

Conclusion: 该碳足迹插件为数据中心模拟研究提供了有效的碳排放评估工具，有助于推动更环保的调度策略开发。

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [99] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: CaPGNN是一个用于多GPU单服务器环境下高效并行全批量图神经网络训练的框架，通过自适应缓存和资源感知分区显著减少通信开销并平衡计算负载


<details>
  <summary>Details</summary>
Motivation: 全批量GNN训练在分布式环境中面临高通信开销和负载不平衡的问题，限制了其可扩展性

Method: 提出联合自适应缓存算法利用CPU和GPU内存减少顶点特征重复传输，以及资源感知图分区算法根据GPU异构能力动态调整子图大小

Result: 在大规模基准数据集上，CaPGNN将通信成本降低达96%，训练速度提升达12.7倍

Conclusion: 自适应缓存和资源感知分区技术能够促进全批量GNN训练在分布式计算环境中的可扩展、高效和实际部署

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [100] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: 对比SG2042，SG2044在高性能计算领域表现出色，64核性能提升4.91倍，缩小了与其他架构的性能差距


<details>
  <summary>Details</summary>
Motivation: RISC-V在高性能计算(HPC)领域尚未普及，需要评估SOPHGO新一代SG2044 CPU的HPC性能表现

Method: 通过对比SG2042和其他架构的性能测试，重点考察高核心数下的表现以及RVV v1.0和内存子系统升级的影响

Result: SG2044在64核配置下达到SG2042的4.91倍性能，对于计算密集型工作负载表现特别优异，显著缩小了与其他架构的性能差距

Conclusion: SG2044作为新一代RISC-V高性能CPU，通过RVV v1.0和内存子系统的重大改进，在HPC领域展现出竞争力，为RISC-V在高性能计算的普及提供了重要支撑

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>
