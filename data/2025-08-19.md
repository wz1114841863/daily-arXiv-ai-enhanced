<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 110]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 这篇论文比较了两种自定义评估函数FMAE和HEF在多元时间序列预测中的表现，HEF在全局指标上更优并提升模型稳健性，而FMAE在局部指标和运行效率方面更好，为不同场景提供了选择指南。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列建模面临数据复杂性、不确定性和频繁制度转移的挑战，传统评估指标容易导致偏差并限制模型的普适性。需要研究更有效的评估方法来优化预测模型在动态环境中的表现。

Method: 设计了两种自定义评估函数：FMAE（重点最小化绝对误差）和HEF（层次评估函数，权重全局指标并罚死大偏差）。采用三种数据分割比例（91:9、80:20、70:30）和三种优化器（网格搜索、PSO、Optuna）进行实验，评估模型拟合度、相对准确度、稳健性和计算效率。

Result: HEF在全局指标（R2、相对准确度、RMSE、RMSSE）上一贵表现更优，显著提升了模型的稳健性和解释力。FMAE在局部指标（MAE、MASE）和执行时间方面更有优势，适合短期场景。结果通过可视化和统计测试得到验证。

Conclusion: 研究呈现了方法论上的叙事交换：HEF适用于战略规划场景，而FMAE更适合运营效率。提出了一个可复现的框架，用于在动态环境中优化预测模型的选择。

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [2] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV是首个针对多上下文KV缓存进行注意力稀疏化的方法，通过考虑其他上下文的互补信息进行稀疏化并局部重计算，在RAG场景中将序列长度压缩至15%且不损失精度


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存重用方法在RAG多上下文场景中失效，因为检索文档的KV缓存是独立计算的，缺乏跨上下文注意力，现有方法无法减少内存开销

Method: SamKV方法在进行上下文稀疏化时考虑其他上下文的互补信息，然后对稀疏化信息进行局部重计算

Result: 实验表明该方法能将序列长度压缩至15%，相比完全重计算基线无精度损失，显著提升多上下文RAG场景的吞吐量

Conclusion: SamKV成功解决了多上下文KV缓存的高效压缩问题，为RAG场景提供了有效的推理加速方案

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [3] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出Representation Stability (RS)框架，通过掩码重要词汇检测嵌入表示变化来识别对抗文本攻击，无需重新训练模型，在多种数据集和攻击类型上达到88%以上检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗文本防御方法通常是攻击特定的或需要昂贵的模型重新训练，需要一个模型无关的检测框架来应对持续的对抗文本攻击威胁。

Method: RS使用重要性启发式对词汇排序，测量对前k个关键词汇掩码的嵌入敏感性，然后用BiLSTM检测器处理结果模式。使用NDCG评估扰动识别质量。

Result: 在三个数据集、三种攻击类型和两个受害者模型上，RS达到超过88%的检测准确率，计算成本较低。基于梯度的排序方法优于注意力和随机选择方法。

Conclusion: RS框架具有良好的泛化能力，无需重新训练即可适应未见过的数据集、攻击和模型，为对抗文本检测提供了实用解决方案。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [4] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级的KDCL_sInvResUNet模型，通过协同学习方案在嵌入式设备上实现了实时无创血压监测，在大规模围末期性数据集中达到了与大模型相当的性能，但在不同人群中普适性仍有限。


<details>
  <summary>Details</summary>
Motivation: 虽然现有深度学习模型能够从非侵入性生理信号重建血压波形，但少有研究考虑模型在嵌入式系统上部署的性能和计算负荷问题。

Method: 提出轻量级sInvResUNet模型和KDCL_sInvResUNet协同学习方案，模型仅有0.89万参数和0.02 GFLOPS计算负荷，在大规模异质性围手术期数据集上进行主体独立验证。

Result: 模型在嵌入设备上实现了10秒输出仅需8.49毫秒的推理时间，平均绝对误差为10.06 mmHg，相关系数为0.88，性能略好于大模型。

Conclusion: 该研究为实时无创血压监测奠定了基础，但所有深度学习模型在不同人群中都显示出显著的性能差异，表明在广泛异质人群中的普适性仍有限。

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [5] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MSLoRA-CR是一种多模态生物医学图像增量学习方法，通过模态特定的LoRA模块和对比正则化来解决跨模态增量学习中的知识保留和知识迁移问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要处理多种模态和任务，但为每个模态单独训练模型会增加推理成本。现有增量学习方法主要关注单模态内的任务扩展，而多模态生物医学图像增量学习需要跨模态的统一模型训练。

Method: 基于大型视觉语言模型，冻结预训练模型，为每个模态增量适配新的LoRA模块，同时引入对比正则化来增强模态内知识共享和促进模态间知识区分。

Result: 在生物医学图像增量学习实验中，MSLoRA-CR优于为每个模态单独训练模型的SOTA方法和通用增量学习方法，整体性能提升1.88%，同时保持计算效率。

Conclusion: MSLoRA-CR有效解决了多模态生物医学图像增量学习的两个核心挑战，实现了更好的性能表现和计算效率，为跨模态增量学习提供了有效解决方案。

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [6] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的终身学习框架，通过跨上下文自注意力机制和动态上下文调度器，增量式训练神经网络解决器来处理不同上下文中的车辆路由问题，提高了解决器的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经解决器通常在单一上下文中训练，如使用欧几里得距离和固定问题规模，导致其在不同场景下的应用受限。需要提升解决器的多样性和适配能力。

Method: 提出了一种终身学习框架，使用Transformer网络作为核心结构，通过跨上下文自注意力机制来转移前续问题的解决知识，并使用动态上下文调度器和跨上下文经验回放来促进知识的积累和转移。

Result: 在合成和标准实例（问题规模达18k）上的实验结果显示，该方法能够发现有效的策略来处理不同上下文中的通用VRP问题，性能超过其他神经解决器，在大部分VRP问题上达到最佳性能。

Conclusion: 该终身学习框架通过知识转移和经验回放机制，有效提升了神经解决器在不同上下文中的适配能力和解决效果，为复杂场景下的车辆路由问题提供了一种通用性更强的解决方案。

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [7] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 本研究评估了TimesFM时间序列基础模型在美国人口统计预测中的表现，相比传统方法在86.67%的测试案例中取得了最低MSE，特别在历史数据稀疏的少数族裔群体预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 人口统计变化受全球化、经济条件、地缘政治事件和环境因素影响，给政策制定者和研究者带来重大挑战。准确的人口预测对于城市规划、医疗保健和经济政策等领域的决策至关重要。

Method: 使用美国人口普查局和FRED的数据集，将TimesFM时间序列基础模型与LSTM、ARIMA和线性回归等传统基线方法进行比较，在六个不同人口特征的州进行实验评估。

Result: TimesFM在86.67%的测试案例中实现了最低的均方误差，特别是在历史数据稀疏的少数族裔人口预测方面表现尤为突出。

Conclusion: 研究结果表明预训练的基础模型有潜力增强人口统计分析，无需大量任务特定微调即可为主动政策干预提供信息支持。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [8] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: 基于多源数据的基地规划布局指标系统(SPLI)，通过五大维度结合深度学习技术，实现了城市空间布局的系统化量化评估


<details>
  <summary>Details</summary>
Motivation: 传统城市基地规划依靠经验判断和单一数据源，无法系统量化多功能布局效果，需要数据驱动的结构化解决方案

Method: 构建SPLI指标系统，整合OpenStreetMap、POI、建筑形态、土地利用、卫星影像等多源数据，包含五大维度：层级化建筑功能分类、空间组织模式、功能多样性、基础服务可达性、土地利用强度，使用RGNN和GNN等深度学习技术补充数据缺口

Result: 实验结果显示SPLI系统提高了功能分类的准确性，为自动化的数据驱动城市空间分析提供了标准化基础

Conclusion: SPLI系统成功将经验性知识与多源异构数据相结合，构建了结构化的城市空间信息生成框架，推动了城市规划从传统经验判断向数据驱动科学决策的转变

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [9] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 通过分离前向噪声模拟和向后梯度计算，扩展STE框架实现了更准确的模拟CIM硬件噪声训练，在减少计算开销的同时提升了模型性能


<details>
  <summary>Details</summary>
Motivation: 模拟CIM硬件的复杂噪声对神经网络部署构成重大挑战，现有噪声诊断训练方法依赖理想化的可微分噪声模型，无法抓取模拟CIM硬件的完整复杂性

Method: 受刻度化中直通估计器(STE)框架的启发，将前向噪声模拟与向后梯度计算解耦合，允许使用更准确但计算上难以处理的噪声模型进行噪声诊断训练

Result: 在图像分类任务上达到了5.3%的准确率提升，文本生成任务上降住了0.72的困惑度，训练时间加速2.2倍，并节省37.9%的峰值内存使用量

Conclusion: 扩展STE框架为模拟CIM硬件的噪声诊断训练提供了一种高效解决方案，在保持计算可行性和优化稳定性的同时，能够处理更复杂的硬件噪声模型，显著提升了模型在模拟CIM硬件上的部署性能

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [10] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了一个用于识别多元霍克斯过程中潜在子过程和因果影响的方法，通过离散时间模型表示连续时间事件序列，并建立了可识别性的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂系统往往只有部分被观测到，存在潜在子过程，这对现有的主要关注观测子过程因果结构的方法提出了重大挑战。

Method: 提出了一个两阶段迭代算法，交替推断已发现子过程间的因果关系和发现新的潜在子过程，基于路径条件保证可识别性。

Result: 在合成和真实数据集上的实验表明，该方法能有效恢复因果结构，尽管存在潜在子过程。

Conclusion: 该方法为存在潜在子过程的多元霍克斯过程提供了有效的因果结构识别解决方案。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [11] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD是首个基于超维计算的无监督联邦学习框架，通过kNN聚类超向量移除和加权HDC聚合技术，解决了非IID数据分布、高计算通信成本和通信噪声问题，相比基于神经网络的方案实现了显著的速度提升、能效改善和准确性提高。


<details>
  <summary>Details</summary>
Motivation: 无监督联邦学习面临非IID数据分布、高计算通信成本和通信噪声脆弱性三大挑战，现有基于深度神经网络的方法存在显著的计算和通信开销。

Method: 提出基于超维计算的FedUHD框架：客户端使用kNN聚类超向量移除方法处理非IID数据异常值；服务器端采用加权HDC聚合技术平衡非IID数据分布。

Result: FedUHD在训练中实现173.6倍速度提升和612.7倍能效改善，通信成本降低271倍，平均准确率提高15.50%，对各种噪声具有优异鲁棒性。

Conclusion: 基于超维计算的FedUHD框架为无监督联邦学习提供了轻量级、高效且鲁棒的解决方案，显著优于现有神经网络方法。

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [12] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: 提出一种受脑组织启发的特征融合框架BRIEF，通过改进神经网络连接搜索策略和Transformer融合模块，自动优化网络结构并提升fMRI基于精神障碍分类性能


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在fMRI分类中存在两个主要问题：网络架构确定依赖经验，特征空间融合多为简单拼接，缺乏相互学习机制

Method: 1）提取4种fMRI时间表征（时间序列、静态/动态功能连接、多尺度分散熵）构建四个编码器
2）在每个编码器中使用改进Q学习动态优化神经网络连接搜索（NCS）
3）通过Transformer融合所有特征向量，利用稳定/时变连接和多尺度依赖关系
4）嵌入注意力模块提升可解释性

Result: 在精神分裂症（SZ，n=1100）和自闭谱系障碍（ASD，n=1550）识别中，BRIEF模型比较了21个最新模型显示出2.2%到12.1%的显著改善，达到AUC值分别为91.5%±0.6%（SZ）和78.4%±0.5%（ASD）

Conclusion: 这是首次尝试将受脑组织启发的强化学习策略应用于fMRI基于精神障碍分类，显示了在识别精确神经影像生物标记物方面的重要潜力

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [13] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC是一个新颖的联邦学习框架，同时确保差分隐私、拜占庭鲁棒性和通信效率，通过RobAJoL方法结合JL变换压缩和鲁棒聚合


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法难以同时保证差分隐私、拜占庭鲁棒性和通信效率，需要一种统一的解决方案

Method: 提出鲁棒兼容压缩概念，结合Johnson-Lindenstrauss变换进行压缩和鲁棒平均进行聚合，开发RobAJoL方法

Result: 理论证明JL变换与鲁棒平均兼容，实验在CIFAR-10和Fashion MNIST上验证了RobAJoL在拜占庭攻击下的优越鲁棒性和效用

Conclusion: Fed-DPRoC框架成功实现了差分隐私、拜占庭鲁棒性和通信效率的三重目标，RobAJoL方法在理论和实验上都表现出色

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [14] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 利用AlphaEarth Foundations全球地球科表征扩展地理标签数据集的地理覆盖范围，通过基础模型在美加植被分类任务中取得了过胜于人的精度。


<details>
  <summary>Details</summary>
Motivation: 高质量标签地理数据集通常受限于特定地理区域，无法涵盖全球范围，影响地球科研究的深度和广度。

Method: 利用Google DeepMind的AlphaEarth Foundations全球地理表征作为输入，采用随机森林和逻辑回归等基础模型，将LANDFIRE植被类型数据集从美国扩展到加拿大。

Result: 在植被类型分类任务中，模型在美国和加拿大验证集上分别达到了81%和73%的分类准确率，预测结果与真实数据呈现出良好的一致性。

Conclusion: 通过AlphaEarth Foundations全球地理表征，可以有效扩展地理标签数据集的地理覆盖范围，为全球地理研究提供更完整的数据支撑。

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [15] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: Fed-Meta-Align是一个四阶段联邦学习框架，通过序列化元初始化和平行联邦学习，在非IID物联网设备数据上实现91.27%的平均故障分类准确率，比现有方法提升3%以上。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限物联网设备在非IID数据环境下实时故障分类的挑战，传统联邦学习在异构环境中容易导致模型发散。

Method: 四阶段框架：1）公共数据集训练基础模型；2）序列化元初始化学习异构感知初始化；3）并行联邦学习阶段使用双标准聚合机制；4）设备端个性化适配。

Result: 在异构物联网设备上达到91.27%的平均测试准确率，比个性化FedAvg和FedProx分别提升3.87%和3.37%。

Conclusion: 通过序列化初始化和自适应聚合的多阶段方法，为多样化TinyML网络部署高性能智能提供了稳健路径。

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [16] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 这篇论文研究了在随机结果领域中强化学习方法对语言模型的优化效果，发现GRPO方法会导致过份自信的概率预测，而PPO和RLOO能产生良好的检验模型。


<details>
  <summary>Details</summary>
Motivation: 检验当前强化学习方法在可验证但具有随机结果的领域（如科学实验）中的效果，以扩展RL在非确定性领域的应用。

Method: 通过在合成数据和真实生物实验中应用各种RL方法（GRPO、PPO、RLOO），分析它们对二元随机结果概率预测的影响。

Result: GRPO导致过份自信的概率预测，而PPO和RLOO产生良好检验的模型。移除GRPO中的组标准化正规化可修复其检验问题。

Conclusion: 提供了反对在GRPO中使用标准正规化的新证据，为RL在超越确定性领域的理由语言模型中的应用打下基础。

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [17] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen是一个基于大语言模型的公平性感知表格数据生成框架，通过上下文学习、提示优化和公平性数据管理，在保持数据效用的同时显著提升反事实和因果公平性。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感和数据稀缺的环境中，生成合成表格数据时面临的关键挑战是如何在保持高数据效用的同时改善反事实和因果公平性。

Method: 使用基于大语言模型的框架，整合多种公平性定义到生成和评估流程中，采用上下文学习、提示优化和公平性感知数据管理来平衡公平性和效用。

Result: 在多个数据集上优于最先进的GAN和LLM方法，公平性指标（如人口统计均等和路径特定因果效应）提升高达10%，同时保持统计效用，仅使用不到20%的原始数据。

Conclusion: 该方法提供了一种原则性和实用的方法来生成公平且有用的合成表格数据，在低数据场景下表现出高效性。

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [18] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: 基于Kolmogorov-Arnold表示定理的神经网络改进，使用ReLU咄三角函数组合替代多项式函数，提升计算效率和训练速度


<details>
  <summary>Details</summary>
Motivation: 现有基于KART的KAN网络使用的B样条咄RBF等多项式函数在GPU设备上支持度不高且计算效率低，需要更高效的计算函数来提升网络性能

Method: 提出使用ReLU、sin、cos、arctan等高速计算函数组合作为基础组件构建KAN网络，将这些函数集成到网络结构中

Result: 实验结果显示这种函数组合在保持竞争性能的同时，能够提供训练时间咄泛化能力方面的潜在改善

Conclusion: 使用ReLU咄三角函数组合的KAN网络在计算效率咄性能之间取得了良好的平衡，为基于KART的神经网络提供了更实用的实现方案

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [19] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: 本文提出PCA-Grad-CAM和SVM-Grad-CAM方法，解决了在CNN中集成PCA和SVM层时无法直接应用传统Grad-CAM的问题，通过求解闭式雅可比矩阵实现了对这些层的注意力可视化。


<details>
  <summary>Details</summary>
Motivation: 当训练样本有限时，在CNN中集成PCA层和/或SVM分类器可以提高分类性能，但传统Grad-CAM无法直接应用于这些层，需要新方法来可视化其注意区域以支持白盒方法的发展。

Method: 提出PCA-Grad-CAM和SVM-Grad-CAM方法，通过求解从最后卷积层到PCA和/或SVM层的闭式雅可比矩阵（Jacobian），实现对这些层的注意力可视化。

Result: 在多个主要数据集上展示了方法的可视化结果，验证了方法的有效性。

Conclusion: 该研究成功开发了能够可视化CNN中PCA和SVM层注意区域的新方法，为小样本情况下的白盒分类方法提供了重要技术支持。

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [20] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 提出了Efficient N-dimensional Attention (ENA)架构，结合线性循环和高维滑动窗口注意力，为超长高维数据建模提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列高维数据时效率不足，需要更高效的架构来扩展线性循环模型到高维数据

Method: 研究扫描策略和注意力混合架构，发现注意力混合模型效果更好。重点评估了不同类型的注意力，发现平铺高维滑动窗口注意力(SWA)在理论和实践中都高效。将线性循环与高维SWA结合形成ENA架构

Result: 实证结果表明扫描策略收益有限，而注意力混合模型显示出有希望的结果。ENA通过线性循环压缩全局信息，SWA补充严格的局部建模

Conclusion: 线性循环和SWA的结合形成了一个简单框架，为超长高维数据建模提供了有前景且实用的解决方案

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [21] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 通过解耦多尺度特征来提高长期交通排放预测的准确性，避免传统方法在长期推理中的错误扩大问题


<details>
  <summary>Details</summary>
Motivation: 传统时空图模型在长期交通排放预测中存在多尺度特征缀缠和错误扩大问题，需要一种能够解耦不同尺度特征并保持其独立性的方法

Method: 提出尺度解耦时空建模(SDSTM)框架，采用Koopman提升算子将系统提升到无穷维线性空间，通过门控波射分解边界来识别预测性差异，并构建双流独立约束融合机制

Result: 在西安二环路级交通排放数据集上进行广泛实验，证明该模型达到了最先进的性能

Conclusion: SDSTM框架通过有效解耦多尺度特征并保持其独立性，显著提高了长期交通排放预测的准确性，为城市空气污染综合管理提供了有效技术支撑

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [22] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 提出一种高效算法，解决了带有对手损失和随机动作集的线性上下文机器问题，达到了多项式时间内的poly(d)√T悔悔界


<details>
  <summary>Details</summary>
Motivation: 解决Liu等人(2023)提出的开改问题：能否在不依赖动作数量的多项式时间内实现poly(d)√T悔悔界，对于组合机器问题尤其重要

Method: 将问题约束为带错误精度检测的固定动作集对手线性机器问题，无需上下文分布知识或仿真器

Result: 达到了Õ(min{d²√T, √(d³T log K)})悔悔界，运行时间为poly(d,C,T)，在有仿真器时可改进为Õ(d√L∗)，其中L∗是最佳策略的累计损失

Conclusion: 算法成功解决了开改问题，首次在多项式时间内实现了组合机器问题的poly(d)√T悔悔界，为该领域提供了重要的算法基础

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [23] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD是一个基于元学习的框架，用于在多模态设置中自动选择最佳的OOD检测器，通过历史模型行为学习和多模态特征表示来适应新的数据分布偏移。


<details>
  <summary>Details</summary>
Motivation: 解决多模态环境下OOD检测器选择难题，由于OOD检测的无监督特性和新数据测试成本高昂，需要一种自动化的模型选择方法。

Method: 结合多模态嵌入和手工制作的元特征来表示数据集，利用元学习从历史性能数据中学习，为新数据分布推荐合适的检测器。

Result: 在12个测试场景中持续优于10个竞争基线方法，且计算开销最小。

Conclusion: M3OOD框架有效解决了多模态OOD检测器选择问题，展现了优异的性能和实用性。

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [24] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 该研究提出了CFF方法，结合反事实和事实解释来为MTPP模型提供最小且合理的解释子集，在解释质量和处理效率上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络标记时间点过程模型在高风险应用中广泛使用，但其输出的可信度存在担忧，需要提供可解释的解释来增强信任。

Method: 提出CFF方法，将MTPP解释定义为反事实解释和事实解释的组合，使用一系列精心设计的技术来解决解释问题。

Result: 实验证明CFF在解释质量和处理效率方面优于基线方法，能够提供正确且优越的解释。

Conclusion: CFF方法成功解决了MTPP模型的解释问题，提供了最小且合理的解释子集，增强了模型输出的可信度。

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [25] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 提出Set-Valued Transformer Network (SVTN)来解决高排放车辆识别中的数据长尾分布问题，通过transformer测量微行程条件变化的时间相似性，结合集合值识别算法提高检测精度


<details>
  <summary>Details</summary>
Motivation: 实际监测数据中高排放状态数据比例远低于正常排放状态，这种长尾分布特征严重阻碍了排放状态识别中判别性特征的提取，同时车辆排放状态的高度非线性和缺乏先验知识也给模型构建带来挑战

Method: 使用transformer测量微行程条件变化的时间相似性，构建从高维排放数据到低维特征空间的映射规则；采用集合值识别算法对生成的特征向量与其标签之间的关系进行概率建模

Result: 在合肥市2020年柴油车监测数据上的实验表明，相比基于transformer的基线方法，该方法将高排放车辆的漏检率降低了9.5%

Conclusion: SVTN方法能够有效学习高排放样本的判别性特征，显著提高高排放移动污染源的准确识别能力

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [26] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: 通过超位原理测试发现，在不相交领域训练的LoRA模块通过简单加法组合可实现效果的组合，性能可与合并数据训练相当，且模块之间的余弦相似性与性能变化呈正相关。


<details>
  <summary>Details</summary>
Motivation: 利用超位原理，验证在不相交领域训练的LoRA模块是否近似正交，以及通过简单加法组合无需重新训练即可实现多领域知识的组合学习。

Method: 使用GPT-2 Small (117M)模型配置LoRA调整参数（rank 4, alpha=64），在数学、医学、金融三个问答领域训练独立的适配器。通过测试对比简单加法组合适配器与合并数据训练的效果，并分析LoRA参数差异矩阵之间的余弦相似性与性能变化的关系。

Result: 数学+医学组合的混淆磅降低9.10%，而数学+金融和金融+医学的混淆磅分别上升4.54%和27.56%。LoRA参数差异的RMS余弦相似性与性能变化呈正相关关系。简单加法无需额外训练且速度快，性能可与合并数据训练相当。

Conclusion: 证明了超位原理在LoRA模块组合中的适用性，简单加法是一种高效的多领域知识组合方法，同时通过模块间的相似性可以预测组合时可能出现的干扰效应。

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [27] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: 提出了一种基于谱滤波的算法，用于学习具有有限边际稳定模式的非线性动力系统，通过在线凸优化技术实现预测误差的渐近消失。


<details>
  <summary>Details</summary>
Motivation: 解决学习未知非线性动力系统的基本问题，特别是那些具有边际稳定模式的系统，这类系统在控制理论和机器学习中具有重要意义。

Method: 使用谱滤波技术，通过系统的谱表示构建从过去观测到未来状态的映射，并开发了新的谱滤波算法处理线性动力系统，支持非对称动态和噪声校正。

Result: 算法能够实现预测误差的渐近消失，学习率由新的可学习性定量控制理论概念决定，显著推广了原始谱滤波算法的应用范围。

Conclusion: 该方法为学习边际稳定非线性动力系统提供了有效的解决方案，新的谱滤波算法对线性系统处理具有独立的理论和应用价值。

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [28] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习中的性能公平性问题，提出了FairGrad和FairGrad*两种梯度方差正则化方法，在异构数据环境下同时提升了公平性和整体模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据异构性可能导致某些客户端对全局模型产生不成比例的影响，造成性能差异。现有公平性方法在异构数据环境下的有效性不明确，不同方法之间的关系也缺乏理解。

Method: 研究专注于性能公平性，评估了显式正则化客户端损失的公平性方法，包括新提出的FairGrad（近似）和FairGrad*（精确）两种梯度方差正则化变体。

Result: 从理论上解释了不同公平方法之间的联系，实证表明FairGrad和FairGrad*在异构数据环境下能同时改善公平性和整体模型性能。

Conclusion: 提出的梯度方差正则化方法有效解决了联邦学习中的性能公平性问题，在保持隐私保护优势的同时提升了模型的公平性和整体表现。

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [29] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN是一个动态层聚合框架，通过输入相关的权重分配和专门化的探测头，为每个输入自适应地选择最合适的特征层组合，解决了传统静态聚合方法的信息瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统的自监督语音模型微调中，使用最终层或加权求和的层聚合方法存在信息瓶颈问题，且对所有数据样本采用静态的特征权重分配，无法根据输入内容动态调整。

Method: 提出VARAN框架，使用层专门化的探测头和数据依赖的权重分配机制，根据输入内容动态地为不同层分配权重，自适应地优先选择最相关的特征层。

Result: 在自动语音识别和语音情感识别任务上的评估显示，VARAN表现出优越性能，特别是在使用LoRA微调技术时效果更佳。

Conclusion: VARAN框架解决了保留层特定信息与实现灵活特征利用之间的权衡问题，推进了自监督语音表示的高效适应。

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [30] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的资源分配算法LPDRL-F，用于优化集成感知与通信的AIGC网络中的感知-生成-通信资源分配，显著提高了用户体验质量。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC服务假设输入数据准确，仅关注生成质量，但在ISAC-AIGC网络中感知数据不准确且AIGC模型自身也引入错误，需要考虑内容准确性和质量的综合评估。

Method: 提出CAQA评估指标，并设计LP导向的深度强化学习算法LPDRL-F，通过LP导向和动作筛选器将三维解空间降为二维，降低复杂度同时提高学习性能。

Result: 模拟显示LPDLR-F比现有DRL咄生成滴涼模型算法收敛速度提高60%以上，找到更优的资源分配方案，AvgCAQA提高14%以上，与仅关注CGQ的方案相比AvgCAQA提高50%以上。

Conclusion: LPDRL-F算法能够高效解决ISAC-AIGC网络中的复杂资源分配问题，显著提升了用户体验质量，为集成感知与通信的AIGC网络提供了有效的资源优化方案。

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [31] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是一个基于160亿次医疗事件训练的医疗基础模型，通过自回归生成模拟患者健康时间线，在78个医疗任务中无需微调即可达到或超越专用模型性能。


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，基础模型预训练为扩展真实世界证据生成和泛化到多样化下游任务提供了有前景的方向。

Method: 使用Epic Cosmos数据集（163亿次就诊、3亿患者记录），训练解码器Transformer模型CoMET，进行最大规模的医疗事件数据缩放定律研究，预训练计算最优模型（达10亿参数）。

Result: CoMET在诊断预测、疾病预后和医疗运营等78个任务中，无需任务特定微调或few-shot示例，普遍优于或匹配专用监督模型，预测能力随模型规模和预训练规模持续提升。

Conclusion: CoMET作为生成式医疗事件基础模型，能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供可扩展和可泛化的框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [32] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT是一种动态自动化的指令调优数据集混合优化方法，将问题建模为多臂老虎机，通过先验缩放玻尔兹曼探索和1步前瞻奖励来动态调整数据集采样概率，在Tulu-v2-mixture上实现了2.2%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着后训练阶段大量指令调优数据集的出现，如何动态平衡和优化这些数据集的混合比例成为了关键挑战。传统固定比例的方法无法适应模型训练过程中的动态需求。

Method: 将数据集混合优化问题建模为多臂老虎机，提出先验缩放玻尔兹曼探索方法，软性地将更新后的采样分布锚定到原始数据集比例，保持集合的固有多样性和覆盖范围。使用轻量级的1步前瞻奖励来更新采样概率，反映数据集对当前模型性能改进的贡献程度。

Result: 在包含16个指令调优数据集的Tulu-v2-mixture集合上应用DynamixSFT，在10个基准测试中实现了高达2.2%的性能改进。

Conclusion: DynamixSFT提供了一种有效的动态数据集混合优化方法，通过自适应调整采样分布来提升模型性能，同时保持了数据集的多样性。该方法为指令调优数据集的动态管理提供了新的解决方案。

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [33] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 门控机制在RNN中通过耦合状态空间时间尺度与参数空间动态，隐式地产生自适应学习率行为，即使使用固定全局学习率训练时也是如此。


<details>
  <summary>Details</summary>
Motivation: 研究门控RNN中门控机制如何影响梯度传播和参数更新，揭示门控不仅控制隐藏状态记忆保留，还作为数据驱动的预处理器调节优化轨迹。

Method: 通过推导泄漏积分器和门控RNN的精确雅可比矩阵，获得一阶展开式，分析标量和多维门控如何重塑梯度传播、调节有效步长并引入参数更新各向异性。

Result: 门控机制自然产生学习率调度、动量和自适应方法（如Adam）等优化行为，数值实验验证了微扰分析的有效性。

Conclusion: 这项工作提供了统一的动力学系统视角，解释了门控如何耦合状态演化与参数更新，说明了门控架构在实践中实现鲁棒可训练性和稳定性的原因。

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [34] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE是一种基于微分熵的不确定性感知变分自编码器，用于改进参数化和可逆投影，在处理分布外样本时表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有自编码器方法在处理数据空间或嵌入空间的分布外样本时表现不佳，需要一种能够分析嵌入不确定性的改进方法

Method: 使用微分熵的变分自编码器，学习从原始空间到2D空间的映射及其逆映射，基于固定投影进行训练

Result: 在四个知名数据集上的定量和定性评估显示，DE-VAE在保持与其他AE方法相当精度的同时，能够分析嵌入不确定性

Conclusion: DE-VAE能够创建准确且可分析不确定性的参数化和可逆投影，为多维数据投影提供了改进解决方案

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [35] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的深度学习模型AICRN，通过空间和通道注意力机制来提高心电图参数回归的精确度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统心电图分析中的人为错误、关注点失空间位置等挑战，提高心脏疾病诊断的准确性和预测能力。

Method: 设计了注意力集成卷积殊残网络(AICRN)，结合空间和通道注意力机制，使用卷积殊残网络解决渐消和爆炸梯度问题。

Result: AICRN模型在心电图参数回归任务中表现超过现有模型，具有更高的精度。

Conclusion: 深度学习在心电图分析的可解释性和精确性方面发挥关键作用，为心脏监测和管理开启了新的临床应用。

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [36] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC是一个轻量级的两阶段压缩框架，通过联合嵌入压缩和自压缩模块，将蛋白质输入长度减少一半，演示长度从751个token压缩到16个以下，在16-shot设置下实现93.68%的压缩比，显著提升了蛋白质功能预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决ProtTeX等蛋白质大语言模型的两个主要限制：1）序列和结构token拼接导致蛋白质长度翻倍并破坏模态间对齐；2）受限于训练语料和上下文窗口，无法进行上下文学习，限制了泛化能力。

Method: 提出两阶段压缩框架：1）联合嵌入压缩机制在残基级别融合序列和结构表示；2）自压缩模块将完整演示聚合到最后几个语言token的潜在空间中。仅通过PEFT调优和单个可训练投影层增加少量参数。

Result: 在16-shot设置下实现93.68%的总提示长度压缩比。蛋白质功能预测实验中，域内基准性能提升2%，域外数据集性能提升11%。

Conclusion: ProtTeX-CC在不修改主干模型的情况下，通过高效的压缩机制显著提升了蛋白质语言模型的上下文学习能力和泛化性能，为多模态蛋白质建模提供了有效的解决方案。

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [37] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: 这篇论文研究大语言模型的"被忘却权"，通过记录训练过程的微观数据和确定性重放来实现准确的模型删除功能。


<details>
  <summary>Details</summary>
Motivation: 满足GDPR条例第17条规定的"被忘却权"，解决大语言模型中数据删除的技术挑战，确保模型能够准确移除特定训练数据的影响。

Method: 将训练视为确定性程序，记录每个微批次的关键信息（ID哈希、随机种子、学习率、优化器步数等）。通过重放训练尾部过程并过滤需要删除的数据，实现与仅在保留集上训练得到相同的模型参数。还提出了多种补充方案来满足延迟和可用性要求。

Result: 在满足前提条件的控制实验中，实现了模型和优化器状态的字节级别完全一致，证明了方法的准确性。同时报告了存储和延迟预算指标。

Conclusion: 该方法为大语言模型的数据删除提供了一种可重现、系统化的解决方案，能够在保证模型性能的前提下实现准确的"被忘却权"功能，符合GDPR法规要求。

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [38] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: 基于一致性模型的新型分布匹配方法，充分结合了连续正则化流的直接范数最小化优点和GAN的灵活性


<details>
  <summary>Details</summary>
Motivation: 生成对抗网络(GAN)在分布匹配任务中存在训练困难、双层最小最大优化目标和模式冲突等问题，需要提出更稳定有效的方法

Method: 受连续正则化流(CNF)中一致性模型的启发，提出了一种新的分布匹配方法，继承了CNF的直接范数最小化目标优点，同时保持了GAN的灵活性

Result: 通过理论验证和在合成数据集以及真实世界数据集上的实验表明了方法的性能优势

Conclusion: 该方法为分布匹配任务提供了一种更稳定有效的解决方案，能够充分利用不同生成模型的优势，在潜变量建模、领域翻译、领域适配等应用中具有广阔前景

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [39] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 提出在异步ADMM中引入粗粒度量化来减少通信开销，适用于大规模联邦学习和分布式优化


<details>
  <summary>Details</summary>
Motivation: 分布式优化和联邦学习中，异步ADMM虽然具有优势，但通信成本可能成为主要瓶颈，特别是在节点通信预算有限或数据量巨大的情况下

Method: 在异步交替方向乘子法(ADMM)中，对需要交换的数据进行粗粒度量化，以降低通信开销

Result: 通过实验验证了该方法在多个分布式学习任务（包括神经网络）中的收敛性

Conclusion: 粗粒度量化是减少异步ADMM通信开销的有效方法，适用于大规模联邦学习和分布式优化应用

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [40] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CC-Time是一个结合预训练语言模型和时间序列模型的跨模态跨模型学习方法，通过文本描述和时间序列数据的联合学习，在时间序列预测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训练语言模型的时间序列预测方法未能充分发挥语言模型的强大序列建模能力，预测精度不够理想。研究者希望探索语言模型在时间序列预测中的潜力，解决两个关键问题：语言模型能够建模哪些时间序列特征，以及仅依赖语言模型是否足够。

Method: 提出了CC-Time方法，包含两个核心组件：1）跨模态学习，通过时间序列序列和对应文本描述在语言模型中建模时间依赖性和通道相关性；2）跨模型融合块，自适应地整合语言模型和时间序列模型的知识，形成更全面的时间序列模式建模。

Result: 在9个真实世界数据集上的大量实验表明，CC-Time在全数据训练和少样本学习情况下都达到了最先进的预测精度。

Conclusion: CC-Time成功证明了通过跨模态和跨模型学习，预训练语言模型在时间序列预测中具有巨大潜力，能够显著提升预测性能，特别是在数据稀缺的情况下表现优异。

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [41] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 提出了DHG-Bench，这是首个深度超图学习的综合基准测试，包含20个数据集、16种最先进的超图神经网络算法，并在统一的数据处理和实验协议下评估算法在效果、效率、鲁棒性和公平性四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络方法缺乏全面的基准测试，导致在数据集覆盖、算法性能评估和实验设置一致性方面存在不足，阻碍了对深度超图学习进展的理解。

Method: 构建了DHG-Bench基准测试，整合了20个多样化数据集（涵盖节点、边和图级别任务）和16种最先进的HNN算法，采用一致的数据处理和实验协议进行系统性评估。

Result: 广泛的实验揭示了现有算法的优势和固有局限性，为未来研究提供了有价值的见解和方向。

Conclusion: DHG-Bench填补了深度超图学习领域基准测试的空白，通过系统性的评估框架促进了该领域的可重复研究和进一步发展。

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [42] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: 提出了STM2和STM3模型，通过多尺度Mamba架构和自适应图因果卷积网络，有效解决长时空依赖学习中的多尺度信息提取和建模难题，在长时空时间序列预测中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以高效学习复杂的长期时空依赖关系，主要面临两个挑战：1）长期时间序列天然包含难以高效提取的多尺度信息；2）不同节点的多尺度时间信息高度相关且难以建模。

Method: STM2采用多尺度Mamba架构高效同时捕获多尺度信息，结合自适应图因果卷积网络学习复杂多尺度时空依赖；STM3进一步使用混合专家架构，包含更稳定的路由策略和因果对比学习策略来增强尺度区分性。

Result: 在真实世界基准测试上的大量实验表明，STM2/STM3在长时空时间序列预测中取得了最先进的性能表现。

Conclusion: 提出的STM2和STM3模型成功解决了长时空依赖学习的关键挑战，证明了多尺度Mamba架构和混合专家方法在时空预测任务中的有效性。

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [43] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: 时间序列预测解释性统一框架，结合LIME和SHAP方法来解释ARIMA和XGBoost模型的预测结果


<details>
  <summary>Details</summary>
Motivation: 解决ARIMA模型在非线性问题上的不足，以及XGBoost等树模型的黑盒特性，提供时间序列预测的可解释性

Method: 将单变量时间序列转换为无泄漏的监督学习问题，训练梯度提升树和ARIMA基准模型，应用LIME和SHAP进行后解释

Result: 通过航空乘客数据集案例研究，发现少数滞后特征（特别是12个月滞后）和季节性编码解释了大部分预测方差

Conclusion: 提出了一种不违背时序性的时间序列解释方法，为实践者提供了指南，并进行了理论和实践评估

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [44] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出了一种新颖的学习型二阶优化器，通过可训练预处理单元增强经典SR1算法，在单目人体网格恢复任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习依赖大数据集且泛化性差，经典优化方法计算轻量但收敛慢，学习型二阶优化方法研究不足

Method: 引入可训练预处理单元生成数据驱动向量构建正半定秩一矩阵，通过学习投影与割线约束对齐

Result: 在分析实验和单目人体网格恢复任务中优于现有学习型优化方法，模型轻量无需标注数据或微调

Conclusion: 该方法具有强泛化能力，适合集成到更广泛的优化框架中，为学习型二阶优化提供了新思路

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [45] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC是一个用于图异常检测的框架，通过上下文重构和对比学习，在标签有限的情况下有效利用未标记数据，提升GNN在异常检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络训练需要大量标注数据，但在图异常检测中异常样本稀少、标注成本高且可能伪装，这严重限制了GAD的发展。

Method: 提出CRoC框架：1）利用GAD中的类别不平衡重构节点上下文，保持交互模式的同时重组节点属性构建增强图；2）分别编码异质关系并整合到消息传递过程中；3）结合对比学习范式联合利用有限标注和大量未标注数据。

Result: 在7个真实世界GAD数据集上的实验表明，CRoC相比基线GNN提升高达14%的AUC，在有限标签设置下优于最先进的GAD方法。

Conclusion: CRoC通过上下文重构和对比学习的结合，有效解决了图异常检测中标签稀缺的问题，增强了GNN对复杂异常模式和对抗伪装的鲁棒性。

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [46] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文分析了Lion优化器的收敛性能，包括标准版本、方差缩减版本和分布式设置下的不同变体，并提出了各自的收敛率分析。


<details>
  <summary>Details</summary>
Motivation: 研究Lion优化器的收敛性能，以及如何通过方差缩减和分布式设计来提高其收敛速度和通信效率。

Method: 采用理论分析方法，设置了标准Lion优化器、方差缩减Lion优化器、分布式Lion优化器以及通信效率变体，并分析了各自的收敛率。

Result: 标准Lion收敛率为$Δ(d^{1/2}T^{-1/4})$，方差缩减版本提升到$Δ(d^{1/2}T^{-1/3})$，分布式版本为$Δ(d^{1/2}(nT)^{-1/4})$ 和 $Δ(d^{1/2}(nT)^{-1/3})$，通信效率变体分别为$Δ(\max\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}}\})$ 和 $Δ(\frac{d^{1/4}}{T^{1/4}})$。

Conclusion: Lion优化器具有良好的收敛性能，通过方差缩减和分布式设计可以显著提高收敛速度，通信效率变体在保持性能的同时减少了通信开销。

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [47] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 提出了针对扩散模型的推理时缩放方法，通过漏斗调度和自适应温度策略解决探索与利用的权衡问题，在保持计算效率的同时显著提升样本质量


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩散模型中面临早期噪声样本难以准确评估但改进潜力大，而后期样本可可靠评估但不可逆的基本困境，需要解决这种探索与利用的权衡

Method: 提出两种策略：1）漏斗调度 - 逐步减少维护的粒子数量；2）自适应温度 - 降低早期奖励的影响权重，这些方法专门针对扩散模型的生成动力学和相变行为设计

Result: 在多个基准测试和最先进的文本到图像扩散模型上的实验结果表明，该方法优于之前的基线方法，显著提升了样本质量且没有增加噪声函数评估的总次数

Conclusion: 通过从搜索算法角度出发设计的简单而有效的方法，成功解决了扩散模型中推理时缩放的探索与利用权衡问题，为扩散模型的推理优化提供了新思路

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [48] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: 提出了Bi-Axial Transformer (BAT)模型，通过同时关注临床变量和时间轴来处理电子健康记录数据，在脓毒症预测和死亡率分类任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)数据日益复杂，传统Transformer在处理EHR分类时受限于数据表示方式，无法有效处理数据稀疏性和信息缺失问题。

Method: 开发了双向轴Transformer(BAT)模型，同时关注临床变量轴和时间点轴，学习更丰富的数据关系，解决数据稀疏性难题。

Result: BAT在脓毒症预测任务上达到最先进性能，在死亡率分类任务上与顶级方法竞争。相比其他Transformer，BAT对数据缺失更具鲁棒性，并能学习可迁移的传感器嵌入。

Conclusion: BAT模型有效解决了EHR数据处理的挑战，为临床研究提供了更强大的工具，同时重新实现了基准模型以便复现和未来基准测试。

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [49] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: 基于机器学习的制造成本估算框架，通过弹程决策树模型直接从2D工程图中提取几何特征，实现了近10%的平均绝对百分比误差，提供了可解释的成本驱动因素分析。


<details>
  <summary>Details</summary>
Motivation: 传统制造成本估算需要人工密集的过程规划，效率低且不一致。需要一种可扩展、自动化的方法来缩短报价周期，提供一致透明的成本评估。

Method: 从13,684张汽车悬挂和转向部件DWG图纸中提取200个几何和统计描述符。使用弹程决策树模型(XGBoost、CatBoost、LightGBM)进行训练，结合SHAP等可解释性工具识别设计驱动因素。

Result: 模型在24个产品组中实现了近10%的平均绝对百分比误差，显示了超越部件特定经验法则的健壮扩展性。识别出了旋转尺寸极大值、弧线统计和分散度量等关键设计因素。

Conclusion: 该端到端CAD到成本流水线显著缩短了报价周期，确保了部件家族间一致透明的成本评估，为工业4.0环境中实时、ERP集成的决策支持提供了可部署的途径。

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [50] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: 通过利用局部距离分布估计聚类数量，实现了能够适应不同局部规模和聚类大小的自适应均倾算法


<details>
  <summary>Details</summary>
Motivation: 传统的KDE基于核密度估计的均倾算法只能提供局部区域的见解，无法处理具有变化局部规模和聚类数量的数据集

Method: 使用点到所有其他点的局部距离分布来估计局部聚类数量，通过识别距离分布中的局部最小值来定义聚类大小，然后基于这些估计计算局部聚类参数

Result: 该算法在原始数据集上超过了最近提出的自适应均倾算法，并在更广泛的聚类性能测试中表现竞争力

Conclusion: 通过局部聚类数量估计来动态调整带宽和核半径阈值的方法有效地处理了变化局部规模的聚类问题，为均倾算法提供了更好的适应性

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [51] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL是一个基于强化学习的NGINX缓存驱逐策略，使用DQN网络在500微秒预算内选择驱逐对象，相比传统LRU策略在25MB缓存下命中率提升146%。


<details>
  <summary>Details</summary>
Motivation: 传统LRU缓存驱逐策略对对象大小不敏感，在周期性突发流量和混合对象大小场景下容易出现抖动问题，需要更智能的驱逐策略。

Method: 使用双决斗深度Q网络(Dueling DQN)作为ONNX侧车服务，每次驱逐时从K个最近最少使用对象中提取6个轻量级特征(年龄、大小、命中次数、到达间隔时间、剩余TTL、最后源RTT)，通过强化学习训练策略。

Result: 在25MB缓存下，命中率从0.1436提升到0.3538(146%提升)；100MB缓存下从0.7530提升到0.8675(15%提升)；400MB时与传统方法相当(约0.918)。推理增加不到2%的CPU开销，95%分位驱逐延迟在预算内。

Conclusion: Cold-RL是首个集成到NGINX中具有严格SLO的强化学习驱逐策略，在中小缓存规模下显著提升性能，同时保持低延迟和低开销。

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [52] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: CSCR是一个轻量级框架，通过将提示和模型映射到共享嵌入空间，实现快速、成本敏感的LLM路由选择，在多个基准测试中比基线方法提升25%的准确率-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略提示特定上下文、依赖昂贵的模型分析、假设固定专家集合或使用低效的试错策略，需要一种更高效的成本感知路由方案。

Method: 使用紧凑的logit足迹（开源模型）和困惑度指纹（黑盒API），通过对比编码器训练在自适应成本带内选择最便宜准确的专家，推理时通过FAISS索引进行k-NN查找。

Result: 在多个基准测试中 consistently 优于基线方法，准确率-成本权衡提升达25%，对未见过的LLM和分布外提示具有强泛化能力。

Conclusion: CSCR提供了一个高效、轻量级的成本感知路由框架，支持微秒级延迟，无需重新训练即可适应专家池变化，显著改善了LLM路由的性能-成本平衡。

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [53] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出了一种基于信任区域的几何退火方法，用于解决控制成本较高的随机最优控制问题，通过逐步逼近目标路径空间测度来改善优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理目标测度与先验分布差异较大的随机最优控制问题时面临优化挑战，需要一种更系统化的渐进逼近方法。

Method: 采用迭代求解约束问题的方法，结合信任区域策略，实现从先验测度到目标测度的几何退火过程，并通过信任区域原理性地选择退火路径的时间步长。

Result: 在多个最优控制应用中表现出显著性能提升，包括基于扩散的采样、转移路径采样和扩散模型微调等任务。

Conclusion: 基于信任区域的几何退火方法为解决具有挑战性的随机最优控制问题提供了一种有效且原理性的解决方案，能够显著改善优化性能。

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [54] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO竞赛吸引了200多名参与者，参赛者训练的目标条件策略能够泛化到训练中未见过的任务、地图和对手。最佳解决方案在单块4090 GPU上训练8小时后，得分比基线高4倍。所有相关资源都已开源。


<details>
  <summary>Details</summary>
Motivation: 举办大规模多智能体竞赛，推动目标条件策略在复杂环境中的泛化能力研究，促进多智能体系统的发展和应用。

Method: 组织NeurIPS 2023竞赛，要求参赛者训练目标条件策略，这些策略需要泛化到未见过的任务、地图和对手。使用基线模型作为对比基准。

Result: 竞赛吸引了200多名参与者，最佳解决方案在单块4090 GPU上训练8小时后，得分达到基线模型的4倍，表现出优异的泛化性能。

Conclusion: 竞赛成功展示了目标条件策略在复杂多智能体环境中的强大泛化能力，开源所有资源将为后续研究提供宝贵的基础和参考。

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [55] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 提出Latent Reconstruction损失函数来解决VAE后验坍塌问题，无需特定网络架构约束，在多个数据集上验证有效


<details>
  <summary>Details</summary>
Motivation: 变分自编码器(VAE)存在后验坍塌问题，现有方法需要在重建和正则化之间权衡，且需要网络架构约束，需要更通用的解决方案

Method: 定义局部后验坍塌概念，基于单射和复合函数的数学特性提出Latent Reconstruction损失函数，无需特定架构限制

Result: 在MNIST、fashionMNIST、Omniglot、CelebA和FFHQ等多个数据集上实验验证了该方法能有效控制后验坍塌

Conclusion: LR损失函数提供了一种无需架构约束的后验坍塌控制方法，提高了生成样本的多样性

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [56] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: 通过系统的调优选择（如学习率、批处理大小）和EMA动量技术，可以在细调语言模型时保持安全性，将有害响应从16%降至5%，而无需额外安全数据或专门干预


<details>
  <summary>Details</summary>
Motivation: 对抗传统观念，证明细调语言模型并不必然会损害其安全性，常见的安全问题应归因于优化选择不佳而非内在抗衡

Method: 系统测试不同训练超参数（学习率、批处理大小、梯度步数）影响，提出指数移动平均（EMA）动量技术来稳定优化路径保持预训练模型的安全性

Result: 在Llama模型家族上，通过优化超参数将有害响应从16%降至约5%，保持了模型效能同时优于需要额外安全数据的现有方法

Conclusion: 细调过程中的安全问题可以通过合理的优化选择避免，无需专门干预措施，为保持模型性能和安全性提供了实用指南

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [57] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: 这篇论文从数据中心AI角度系统研究了功能怠共振成像构建脑图的设计空间，通过对三个关键阶段的系统化评测，证明数据流水线的细心选择能够显著提升下游分类性能


<details>
  <summary>Details</summary>
Motivation: 当前脑图构建实践多重视模型中心方法，忽视了数据流水线中的关键选择对下游性能的重要影响，需要从数据中心角度系统研究脑图构建的设计空间

Method: 将脑图构建设计空间组织为三个阶段：时间信号处理、拓扑提取和图特征化，系统评估高振幅BOLD信号筛波、连接性稀疏化与统一策略、替代相关指标以及多视角节点和边特征

Result: 在HCP1200和ABIDE数据集上的实验显示，细心的数据中心配置能够一贯地提高分类准确性，超过标准流水线

Conclusion: 上游数据决策在基于图的神经成像学中发挥着关键作用，系统探索数据中心设计空间对该领域至关重要

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [58] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1是一个基于规则强化学习的Linux内核调优框架，通过将内核配置空间抽象为RL环境，利用LLM进行高效探索和准确配置修改，在实验中比启发式调优性能提升5.6%，具有高数据效率和跨场景适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的Linux内核调优方法在效率、可扩展性和泛化性方面面临挑战，需要一种更高效、可扩展且能泛化到不同场景的自动化调优解决方案。

Method: 提出OS-R1框架：1）将内核配置空间抽象为RL环境；2）利用LLM进行高效探索和准确配置修改；3）设计定制奖励函数增强LLM的推理标准化、配置准确性和系统性能感知；4）采用两阶段训练过程加速收敛并减少跨场景重新训练。

Result: 实验结果显示OS-R1显著优于现有基线方法，相比启发式调优实现最高5.6%的性能提升，保持高数据效率，并在各种实际应用中展现出良好的适应性。

Conclusion: OS-R1框架通过结合规则强化学习和LLM，为Linux内核调优提供了一种高效、可扩展且泛化性强的解决方案，具有实际部署的潜力，代码和数据集已开源。

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [59] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: 基于视觉分析系统的代码生成代理行为分析方案，支持代码、过程和LLM三个级别的对比分析


<details>
  <summary>Details</summary>
Motivation: 当前人工审查代码生成代理输出的方式效率低下，难以跟踪代码迭代过程和识别改进机会

Method: 设计了一个视觉分析系统，专注于AIDE框架，支持代码级、过程级和LLM级三个级别的对比分析

Result: 通过Kaggle竞赛案例研究验证，系统能够提供有价值的迭代代码生成过程洞察

Conclusion: 该视视化分析系统能够帮助ML科学家更有效地调试和提示工程，提升代码生成代理的行为理解

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [60] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: 基于VMD分解和滑动窗口的深度学习模型，提升金融时间序列预测的准确性和稳定性


<details>
  <summary>Details</summary>
Motivation: 金融时间序列具有复杂性和非稳定性特征，传统预测模型面临挑战，需要提高模型的适应性和预测性能

Method: 采用滑动窗口技术构建数据集，通过变分模态分解(VMD)将非稳定金融时间序列分解为平滑子组件，然后使用LSTM深度学习模型进行预测

Result: 与直接使用原始时间序列的LSTM模型相比，VMD处理后的模型显示出更好的预测效果和更高的稳定性

Conclusion: 结合VMD分解技术和深度学习模型能够有效提升金融时间序列预测的准确性和可靠性，为复杂金融数据预测提供了有效解决方案

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [61] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于度量-辛括号形式的多尺度系统粗粒化机器学习框架，能够保持热力学定律、动量守恒和涨落-耗散平衡，并通过自监督学习识别涌现结构变量。


<details>
  <summary>Details</summary>
Motivation: 多尺度系统模拟具有挑战性，粗粒化过程中信息损失导致涌现物理具有耗散性、历史依赖性和随机性，需要开发能够保持这些特性的机器学习方法。

Method: 使用度量-辛括号形式构建框架，保证热力学第一、第二定律、动量守恒和涨落-耗散平衡；引入自监督学习策略识别涌现结构变量；在粒子离散化中应用该方法。

Result: 在基准系统上验证了方法的有效性，成功应用于星形聚合物的挑战性粗粒化（保持非平衡统计）和胶体悬浮液高速视频的动力学学习（捕捉局部重排事件与涌现随机动力学的耦合）。

Conclusion: 该框架能够有效学习粗粒化动力学，保持关键物理特性，提供了PyTorch和LAMMPS的开源实现，适用于各种粒子系统的大规模推理和扩展。

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [62] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: 使用预训练的蛋白质大语言模型提取序列上下文特征，通过双向LSTM和GRU预测胆固胆蛋白激素区域，准确度达84.5%


<details>
  <summary>Details</summary>
Motivation: 现有胆固胆白激素预测方法主要基于进化模体和氨基酸特性，序列信息特征显示出高预测性能，需要探索更先进的计算方法

Method: 利用预训练的蛋白大语言模型提取序列上下文特征，结合双向LSTM和GRU网络构造预测模型

Result: 在10折交叉验证中获得84.5%的准确度，测试数据集上达到83%准确度，表现竞争力

Conclusion: 证明了大语言模型在提高胆固胆白激素预测准确性方面的潜力

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [63] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: 这篇论文分析了过参数化联邦学习中FedAvg算法的收敛性，证明网络宽度增加时数据异质性影响逐渐消失，在无限宽度时FedAvg可达到与中心化学习相同的汇总性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的非同分布特性对全局模型的汇总性构成挑战，需要理论分析过参数化模型如何减少异质性影响。

Method: 使用理论分析方法，证明过参数化FedAvg使用梯度下降时的收敛性，并在无限宽度构造中分析全局和本地模型的线性行为。

Result: 异质性影响随网络宽度增加而减少，在无限宽度时完全消失；FedAvg在无限宽度时与中心化学习有相同的汇总性能；实验验证了理论结论。

Conclusion: 过参数化可以有效减少联邦学习中数据异质性的负面影响，在极限情况下可实现与中心化学习相等的性能。

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [64] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: 通过令牌级判断机制，给合成语言模型实现了能源效率和通信效率的显著提升，在保持准确性的同时节省40.7%能消耗


<details>
  <summary>Details</summary>
Motivation: 解决质源受限环境下设备上LLM推理的需求，当前混合语言模型研究对通信和能源效率关注不够

Method: 采用令牌级过滤机制，结合认知不确定性和关注重要性，优先上传信息量高的令牌来减少LLM使用和通信成本

Result: 在TinyLlama-1.1B和LLaMA-2-7B上达到87.5% BERT Score，令牌吞吐率0.37 tokens/sec，相比标准HLM节省40.7%能消耗，相比U-HLM基线在准确性、能源节省和吞吐量方面都有显著提升

Conclusion: 该方法能够在带宽受限的边缘环境中实现能源效率高且准确的LLM部署

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [65] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络（PI-DeepONet）的交通状态估计方法，通过将TSE重新定义为算子学习问题，将交通流守恒定律直接整合到算子学习过程中，相比传统PINNs方法具有更好的物理一致性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理信息的神经网络（PINNs）在交通状态估计中逐点强制执行PDE约束存在局限性，需要一种能够更好地整合物理约束并处理高维时空PDE的方法。

Method: 采用物理信息深度算子网络框架，训练参数化神经算子将稀疏输入数据映射到完整的时空交通状态场，直接将交通流守恒模型和基本图整合到算子学习过程中。

Result: 在NGSIM数据集上的实验表明，该方法优于最先进的基线方法，能够有效捕捉拥堵传播、空间相关性和时间演化，同时确保了物理一致性。

Conclusion: PI-DeepONet框架为交通状态估计提供了一种有效的算子学习方法，具有鲁棒性和高效性，为最优函数生成策略和分支网络复杂度的分析提供了新的见解。

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [66] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE是一种线性复杂度的自注意力机制，通过固定长度的潜在序列路由注意力，解决了传统自注意力二次复杂度的问题，在大型非结构化网格上实现了更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制的二次复杂度限制了其在大型非结构化网格上的应用和可扩展性，需要一种更高效的注意力机制来处理大规模问题。

Method: FLARE通过可学习的查询令牌将输入序列投影到固定长度的潜在序列（M << N），在瓶颈序列上路由注意力，学习低秩形式的注意力，实现O(NM)的计算复杂度。

Result: FLARE不仅能够扩展到前所未有的问题规模，而且在各种基准测试中相比最先进的神经PDE代理模型提供了更优越的准确性。

Conclusion: FLARE通过低秩注意力路由机制成功解决了自注意力的可扩展性问题，为处理大规模非结构化网格问题提供了有效的解决方案，并发布了新的增材制造数据集以促进进一步研究。

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [67] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: 这篇论文提出了一种系统化方法来构建不变和均变操作，以支持包含对称性的神经网络设计，并应用于几何图神经网络和材料本构关系学习。


<details>
  <summary>Details</summary>
Motivation: 设计包含对称性的神经网络对几何深度学习至关重要，核心是发展不变和均变操作。需要一种系统化方法来构建这些操作，以处理不同类型的张量输入和输出。

Method: 提出了一种系统化方法来构建合法的不变和均变操作，能够处理不同秩的卡尔张量和不同类型的球形张量。方法使用对称张量网络的图形表示，简化了不变和均变函数的证明和构造。

Result: 方法成功应用于设计几何图神经网络的均变互动消息，以及构建均变机器学习模型来学习材料的本构关系。

Conclusion: 该系统化方法为构建不变和均变操作提供了一种有效的工具，通过张量网络的图形表示简化了相关证明和构造过程，并成功应用于实际的几何深度学习任务。

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [68] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: 一种混合代模模型，通过统一的神经网络和物理模块从速度和加速度估计电动汽车参数和电耗，达到了1% 的精度。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种解释性强、能够从简单的车辆运动数据估算多种物理参数的方法，以支持路径优化、健康管理等应用。

Method: 统一的神经网络架构（Spectral Parameter Operator + Fourier Neural Operator）结合可微分物理模块，从速度和加速度输入估计多种时变参数，并通过物理嵌入方式计算电池功率。

Result: 在Tesla车型上平均绝对误差0.2kW（约高速拖引功率的1%），Kia EV9上0.8kW，具有良好的演绎性和样本率适应性。

Conclusion: 该模型不仅准确还解释性强，能够抓取车辆实际状态，在多种应用场景中实用性强。

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [69] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: SSPO是一种无需辅助模型或人工标注的强化学习过程监督框架，通过模型自生成的步进偏好信号来优化推理步骤，实现准确简洁的推理过程


<details>
  <summary>Details</summary>
Motivation: 主流后训练方法（如带思维链的强化学习）存在计算开销大和过度思考问题，错误答案部分源于冗长推理过程缺乏自我修正能力

Method: 提出Self-traced Step-wise Preference Optimization (SSPO)，利用模型自生成的步进偏好信号指导推理压缩优化过程

Result: 实验表明SSPO生成的推理序列准确简洁，有效缓解过度思考行为，在不同领域和语言中保持模型性能

Conclusion: SSPO提供了一种高效的可插拔RL过程监督框架，能够实现细粒度的推理步骤优化，无需额外资源

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [70] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: 该论文提出了可解释人工智能(XAI)的可信度评估框架，强调解释稳健性(ER)和方法稳健性(EMR)两个关键标准，认为仅靠单个方法的稳健性不足以保证可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法虽然预测准确但内部机制不透明，现有XAI方法在性能评估方面存在不足，需要建立更可靠的可信度标准来确保解释反映算法的真实决策过程。

Method: 论文开发并形式化了ER和EMR的评估标准，ER要求不同XAI方法在可比情境下产生相同解释，EMR要求单个方法在不同条件下保持解释一致性。

Result: 提出了一个系统性的可信度评估框架，为建立对深度学习算法的信任提供了理论基础和方法指导。

Conclusion: 可信的XAI解释需要同时满足解释稳健性和方法稳健性，该框架为未来XAI方法的发展和评估指明了方向。

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [71] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3是一个开源的多模态流匹配模型，通过三种架构无关的技术（自条件、假原子和训练时几何扭曲）显著提升了分子生成性能，实现了近100%的药物分子有效性，且参数量比同类方法少一个数量级。


<details>
  <summary>Details</summary>
Motivation: 开发能够生成具有所需属性的真实分子的生成模型，可以加速化学发现。当前需要同时采样分子拓扑和3D结构的模型，FlowMol3旨在推进全原子小分子生成的最新技术。

Method: 基于流匹配框架，采用三种低计算成本的技术：自条件（self-conditioning）、假原子（fake atoms）和训练时几何扭曲（train-time geometry distortion），无需改变图神经网络架构或流匹配公式。

Result: 实现了近100%的药物分子有效性，更准确地复现了训练数据的功能基团组成和几何结构，参数量比同类方法少一个数量级，性能显著优于之前的FlowMol版本。

Conclusion: 这些技术缓解了基于传输的生成模型的普遍病理问题，能够在推理过程中检测和纠正分布漂移，为改进基于扩散和流的分子生成模型的稳定性和质量提供了简单可转移的策略。

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [72] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: SciNO是一种基于分数匹配的因果发现方法，通过神经算子稳定估计Hessian对角项，相比DiffAN在合成图和真实数据集上分别减少42.7%和31.5%的排序差异，同时保持内存效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Stein梯度估计器的因果排序方法计算成本高且内存密集，而DiffAN方法虽然解决了这些问题，但由于分数模型的二阶导数导致数值不稳定。

Method: 提出Score-informed Neural Operator (SciNO)，一种在平滑函数空间中的概率生成模型，用于稳定近似Hessian对角项并在分数建模过程中保持结构信息。

Result: 实验结果显示，SciNO相比DiffAN在合成图上减少42.7%的排序差异，在真实数据集上减少31.5%的排序差异，同时保持内存效率和可扩展性。

Conclusion: SciNO方法有效解决了现有因果排序方法的数值不稳定问题，提出的概率控制算法能够将SciNO的概率估计与自回归模型先验结合，无需额外微调或提示工程即可增强LLMs的因果推理能力。

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [73] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可以在密链学习中抵御导致的Byzantine攻击的方法，仅需要服务器和一个客户端是可信的。该方法在各种攻击场景下都显著超越了现有的基准方法。


<details>
  <summary>Details</summary>
Motivation: 密链学习中客户端可能受到Byzantine攻击，而现有的防御方法要么需要预先知道恶意客户端数量，要么需要多个可信客户端。需要一种更加简单但有效的方法来处理这些攻击。

Method: 使用服务器拥有的可信边缘数据集，仅需要服务器和一个客户端是可信的。通过理论分析证明方法在强Byzantine攻击下仍能保证有界的最优性间隔，无需预先知道恶意客户端的数量。

Result: 在MNIST、FMNIST和CIFAR-10数据集上进行实验，测试了标签翻转、符号翻转和高斯噪声等多种攻击策略。实验结果显示该算法显著超过了Mean、Trimmed Mean、Median、Krum和Multi-Krum等标准和稳健的FL基准方法。

Conclusion: 该方法为密链学习提供了一种简单有效的Byzantine防御方案，仅需要服务器和一个客户端可信，且无需预先知道恶意客户端数量，在各种攻击场景下都显示出优异的性能。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [74] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero是一个新颖的联邦学习方法，通过超网络动态生成针对非参与客户端的专用模型，解决了数据异构性和资源约束问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理数据异构性方面取得进展，但无法泛化到具有域内分布偏移和资源约束的非参与客户端

Method: 使用基于分布感知嵌入的超网络动态生成专用模型，采用NoisyEmbed增强提取器和平衡惩罚机制防止特征坍塌，分块生成模型以适应不同客户端

Result: 在多个数据集和模型上的实验显示HyperFedZero性能显著优于竞争方法，计算、存储和通信开销最小

Conclusion: 该方法有效解决了联邦学习中非参与客户端的泛化问题，各组件均被证明必要且有效

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [75] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: 这篇论文提出了BuilDa框架，用于生成量足质优的维护结构热动力学数据，以支持转移学习研究，免去了传统建筑模拟所需的深度专业知识。


<details>
  <summary>Details</summary>
Motivation: 转移学习可以改善建筑热动力学建模，但目前缺乏足够质量的数据来支持相关研究。现有公开数据集和数据生成器都无法满足转移学习研究对数据质量和数量的要求。

Method: 研究者开发了BuilDa框架，使用单区域Modelica模型，将其导出为功能模拟单元(FMU)并在Python中进行模拟。该框架无需深度的建筑模拟知识就能生成大量数据。

Result: 研究展示了BuilDa生成的数据，并利用这些数据进行了预训练和微调转移学习模型的应用。

Conclusion: BuilDa框架能够生成足够质量的维护结构热动力学数据，有效解决了转移学习研究中的数据短缺问题，且无需深度专业知识即可使用。

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [76] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用于汽车网络中交通标志检测的联邦学习框架，通过分布式模型训练避免原始数据共享，解决隐私和通信挟手问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车产生的大量传感器数据带来隐私泄露和通信压力挟手，需要一种不需要共享原始数据的协作学习方案来实现感知任务。

Method: 使用联邦学习框架，将交通标志类别在车辆间分配进行专门化本地训练，采用轻量级对象检测器，通过FedProx、FedAdam和FedAVG等算法聚合模型参数，在Flower框架中模拟评估多种配置。

Result: 实验结果显示：服务器轮数从2增到20时准确率从0.1以下提升到0.8以上；适中的本地迭代次数（8-10）准确率约0.67；更高的客户参与比例提升到约0.83；FedProx在处理异质性时表现更优；非IID数据分布性能下降；训练时长主要受轮数影响。

Conclusion: 这种联邦学习方法可以为真实世界汽车部署提供可扩展、保护隐私的解决方案，为未来智能交通系统的健壁聚合和通信优化集成提供指导。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [77] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA是一个资源高效的联邦微调框架，通过层剪枝和蒸馏对齐技术，在保持模型性能的同时显著降低通信、存储和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调中客户端资源受限的问题，传统全模型微调需要大量计算和内存资源，限制了在资源受限设备上的应用。

Method: 提出相似性组剪枝(SGP)模块剪枝冗余层，保留关键层；引入协同蒸馏对齐(ODA)模块减少子模型与全模型间的梯度差异；结合QLoRA技术进行量化子模型和轻量适配器微调。

Result: 平均减少70.6%通信开销，降低75.6%存储使用，提升3.1%任务准确率，在三个开源LLM和多种下游任务上验证有效。

Conclusion: FedSODA框架在资源受限环境下实现了高效的联邦微调，为实际应用提供了可行的解决方案。

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [78] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet是一个轻量级、架构无关的联邦学习框架，通过U-Net风格的附加模块实现异构客户端之间的高效知识迁移，仅需共享紧凑的瓶颈层，在低通信开销下达到93.11%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多假设客户端模型架构相同，限制了在异构现实环境中的应用。需要解决不同架构客户端间的协同学习问题。

Method: 为每个客户端的主干网络附加U-Net风格的加法模块，仅共享紧凑的瓶颈层。利用编码器-解码器设计和跳跃连接捕获多层次特征，提取客户端不变表示。

Result: 在VGG变体上的实验显示，FedUNet达到93.11%准确率，紧凑版本达到92.68%准确率，仅需0.89MB的低通信开销。

Conclusion: FedUNet成功实现了架构无关的联邦学习，通过轻量级的U-Net模块设计，在保持高性能的同时显著降低了通信成本，为异构环境下的联邦学习提供了有效解决方案。

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [79] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: 这篇论文提出了一个系统性评估神经网络空间推理能力的基准框架，采用VoxLogicA生成迷宫连通性和空间距离计算任务的合成数据集，并发现神经网络在基本几何和拓扑理解任务中存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 为了系统性地评估神经网络的空间推理能力，特别是形态学性质如连通性和距离关系，以发现并解决其在临床应用中的限制。

Method: 使用VoxLogicA空间模型检查器生成两类合成数据集：迷宫连通性问题用于拓扑分析，空间距离计算任务用于几何理解。构建了包含数据生成、标准化训练、推理执行和综合评估的自动化流水线。

Result: 预实验结果显示神经网络在空间推理能力方面存在显著挑战，在基本几何和拓扑理解任务中出现系统性失败。

Conclusion: 该框架提供了可复现的实验协议，能够识别具体限制，为通过组合神经网络与符号推理的混合方法解决这些限制奠定了基础，以改善临床应用中的空间理解能力。

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [80] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: 提出约束质心聚类(CCC)方法，通过限制聚类中心到最远点的最大距离来扩展经典质心聚类，使用拉格朗日公式推导出闭式解，在合成圆形数据上验证了其优于K-means和GMM的性能。


<details>
  <summary>Details</summary>
Motivation: 传统质心聚类方法缺乏对聚类扩散的控制，需要一种能够约束聚类最大半径的方法来获得更紧凑且结构化的聚类结果。

Method: 使用拉格朗日公式推导约束质心聚类的闭式解，通过限制聚类中心到最远点的最大距离来控制聚类扩散。

Result: 在合成圆形数据上，CCC通过减少径向扩散同时保持角度结构，实现了更紧凑的聚类，在环向熵、扇区熵和联合熵指标上优于K-means和GMM。

Conclusion: CCC方法适用于需要结构化聚类和扩散控制的应用场景，如传感器网络、协作机器人和可解释模式分析。

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [81] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: 提出了一种基于极限学习机(ELM)的短期能源预测新方法，使用多输入多输出架构预测多种能源的产量和总产量，在1小时预测范围内表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决能源预测中的非平稳性和季节性变化问题，需要开发一种能够动态适应波动的高效预测方法，特别适用于实时应用场景。

Method: 采用极限学习机(ELM)结合滑动窗口技术和循环时间编码的多输入多输出(MIMO)架构，处理六年的多源能源小时数据。

Result: 模型显著优于基于持续性的预测方法，太阳能和热能预测的nRMSE分别为17.9%和5.1%，R²>0.98（1小时预测），在5小时内保持高精度。

Conclusion: ELM-MIMO方法提供了闭式解，计算需求低，适合实时应用和在线学习，能够适应不同环境和数据集，具有很好的实用性。

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [82] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: 提出E3Former在线集成模型，用于服务器less系统中的工作负载预测，相比单模型方法平均减少10%预测误差，已在字节跳动IHPA平台部署，支持60万+CPU核心的预测自动扩缩容，资源利用率降低40%以上。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型难以快速适应在线工作负载流的动态变化，且无法有效捕捉细粒度高频预测任务带来的复杂周期性，需要更准确和鲁棒的预测方法来实现服务器less系统的最优资源分配。

Method: 提出E3Former在线集成模型，通过协同多个子网络的预测能力来克服单模型方法的局限性，在计算开销最小的情况下确保预测准确性和鲁棒性。

Result: 在真实工作负载数据集上的实验表明，该方法在在线预测任务中平均减少10%的预测误差，已在字节跳动IHPA平台部署，支持30+应用的稳定运行，预测自动扩缩容能力达60万+CPU核心。

Conclusion: E3Former模型显著提升了服务器less系统中工作负载预测的准确性和鲁棒性，实现了高效的资源利用和运营效率，在实际生产环境中验证了其有效性。

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [83] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: 基于随机化PCA森的新题无监督异常检测方法，在多个数据集上表现超过现有方法，具有良好的统一性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 受随机化PCA森在近似KNN搜索中表现的启发，开发一种新的无监督异常检测方法。

Method: 使用随机化PCA森（RPCA Forest）进行异常检测，利用其在近似KNN搜索中的优势。

Result: 在多个数据集上表现超过传统和最新方法，其他数据集上也具有竞争力，高度统一化和计算效率。

Conclusion: 该方法是无监督异常检测的良好选择，具有强大的模型演绎能力和高效计算性能。

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [84] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer通过引入二阶波动动力学解决Transformer中的过平滑问题，在NLP和CV任务中提升性能且无需额外超参数调优


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型存在过平滑问题，即token表示在连续transformer块中收敛到相似值。研究发现注意力层隐藏状态动态与完全图上的图神经扩散等价，过平滑是底层扩散动态耗散特性的结果

Method: 提出基于二阶波动动力学的新型注意力层，设计保持物理状态-速度关系的FFN和归一化层，扩展transformer架构

Result: 在多种NLP和CV任务的transformer模型上验证，Wavy Transformer以最小额外参数显著提升性能

Conclusion: 物理启发的Wavy Transformer有效解决过平滑问题，为transformer架构改进提供了新方向

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [85] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: 一个统计框架Bridge，通过模型化LLM评估与人类偏好的系统性偏差，来提升LLM作为评判者的准确性


<details>
  <summary>Details</summary>
Motivation: LLM作为评判者在大规模评估中应用越来越普遍，但其评估结果与人类判断存在系统性偏差，需要一个统一的统计框架来学习和正这些偏差

Method: 提出Bridge统计框架，假设每个提示-响应对都有一个潜在的人类偏好分数，并将LLM的偏差模型化为变量的线性变换，提供了高效的拟合算法和统计推断保证

Result: 在6个LLM评判者和两个案例（BigGen Bench和Chatbot Arena）上，Bridge实现了与人类评分更高的一致性（准确性、检验和KL散度），同时曝露了人类与LLM之间的系统性差距

Conclusion: Bridge框架为精炼LLM评分和定量分析人类-LLM评估差异提供了一个简洁而有理论基础的方法，有助于提升LLM作为评判者的可靠性

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [86] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文重新审视因果建模在AI泛化中的作用，挑战了现有领域泛化基准的结论，提出了更细致的因果理论框架


<details>
  <summary>Details</summary>
Motivation: 针对近期领域泛化基准测试对因果建模促进AI稳健泛化能力的质疑，需要重新审视因果理论与泛化性能之间的关系

Method: 通过理论分析和文献综述，调和因果理论与领域泛化文献中的表面矛盾，提出更细致的因果作用理论框架

Result: 建立了更完善的因果建模与AI泛化关系的理论框架，澄清了现有基准测试中的误解

Conclusion: 因果建模确实对AI稳健泛化具有重要价值，但需要更细致的理论框架来理解其作用机制，而非简单地否定其有效性

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [87] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore是一种新的MoE路由范式，通过建模为最小成本最大流问题并集成SoftTopk算子，解决了传统MoE网络中的token丢弃和硬件效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE网络存在专家容量约束导致的token丢弃问题和硬件效率低下问题，而去除容量约束又会损害负载平衡和计算效率。

Method: 将路由建模为最小成本最大流问题，集成SoftTopk算子，避免了迭代重路由和最优传输公式的根本限制。

Result: 在相同FLOPs下，相比有约束和无约束基线，实现了更低的训练损失和更高的评估分数。

Conclusion: MaxScore解决了MoE路由中的关键问题，提供了更好的性能和效率平衡。

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [88] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 本文提出L2S方法，通过训练小型辅助模块预测输入特定的转向向量，实现多模态大语言模型的细粒度控制，有效减少幻觉并增强安全性。


<details>
  <summary>Details</summary>
Motivation: 现有转向技术（如均值转向）依赖单一转向向量且独立于输入查询，在面对依赖具体示例的期望行为时存在局限性。例如安全回答可能需要根据问题类型采取不同策略。

Method: 提出基于对比输入特定提示的输入特定线性偏移方法，训练小型辅助模块来预测测试时未知的输入特定转向向量。

Result: L2S方法在减少多模态大语言模型幻觉和增强安全性方面表现优异，优于其他静态基线方法。

Conclusion: 输入特定的细粒度转向方法能够更有效地指导多模态大语言模型的行为，L2S为实现这种动态控制提供了实用解决方案。

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [89] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: 这篇论文通过实验研究探讨了设备端机器学习中的存储问题，发现根据数据样本的压缩敏感性进行适配性压缩比简单的均匀数据删除或压缩更有效。


<details>
  <summary>Details</summary>
Motivation: 设备端机器学习在持续数据收集场景中遇到存储空间有限的挑战，需要找到数据数量与质量之间的最佳平衡点。

Method: 进行了一个关于存储感知学习的实证研究，重点分析了通过压缩来实现数据数量与质量间的权衡。识别了根据样本压缩敏感性进行适配性压缩的可行性。

Result: 证明了简单的均匀数据删除或一刀切的压缩策略是次优的。发现不同数据样本对压缩的敏感性存在差异，支持根据样本进行适配性压缩的策略。

Conclusion: 这些发现为开发新一代存储感知学习系统奠定了基础，通过系统性的特征化探索了这个被忽视的挑战，提供了提升存储感知学习理解的价值见解。

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [90] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文研究了transformer模型在上下文token预测任务中的损失景观，发现子n-gram模型是总体交叉熵损失的近稳态点，这解释了阶段式学习动态和涌现相变现象。


<details>
  <summary>Details</summary>
Motivation: 受训练过程中观察到的长时间平台期和阶段式进展的启发，研究transformer模型在上下文next-token预测任务中的损失景观特性。

Method: 研究在交叉熵损失下学习上下文n-gram语言模型，建立参数配置为稳态点的充分条件，构建简化transformer模型的k-gram估计器参数配置集。

Result: 在无限序列长度和参数范数极限下，这些解的总体损失梯度消失，表明子n-gram是总体交叉熵损失的近稳态点。数值实验支持了n-grams学习动态的特征。

Conclusion: 子n-gram作为近稳态点的发现为阶段式学习动态和涌现相变现象提供了理论解释，深化了对transformer训练动态的理解。

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [91] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: 一种结合数值和图像表示的混合框架HRS，通过调度感知损失函数提高流量预测准确性，在网络负载预测中显著降低SLA违约率和增加利润


<details>
  <summary>Details</summary>
Motivation: 解决流媒体服务带来的时变和爆发性网络负载问题，充分平衡预测准确性与资源配置成本，避免低供应导致SLA违约或过供应造成资源浪费

Method: 提出HRS混合表示框架，整合数值和图像表示来抓取极端负载动态；设计调度感知损失函数SAL，考虑预测误差的不对称影响，优化预测结果以支持更好的调度决策

Result: 在4个真实数据集上，HRS较10个基线方法都更优，达到最高水平，SLA违约率降低63.1%，总利润损失减少32.3%

Conclusion: HRS框架通过混合表示和调度感知损失，有效解决了网络负载预测中的极端倾向问题，在保障QoS的同时提升了平台利润能力

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [92] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 基于动态图模型和深度异常检测的TGN-SVDD入侵检测方法，在实际入侵检测数据上表现优于多个基准方法


<details>
  <summary>Details</summary>
Motivation: 网络安全重要性日益增长，机器学习入侵检测面临检测新频网络事件、时间序列事件和网络通信图结构等挑战

Method: 提出TGN-SVDD方法，结合现代动态图模型和深度异常检测技术

Result: 在实际入侵检测数据上表现超过多个基准方法，并建议了更具挑战性的数据变体

Conclusion: TGN-SVDD方法为网络入侵检测提供了有效的解决方案，能够处理动态图结构和异常检测的复杂问题

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [93] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ是一种用于TinyML流式处理的单次无标签不确定性监控方法，通过时间一致性和流式保形校准提供资源高效的设备端监控方案


<details>
  <summary>Details</summary>
Motivation: 在资源受限的TinyML设备上实现高效的不确定性监控，避免传统方法（如早期退出和深度集成）的高内存和计算开销

Method: 使用短时域时间一致性，通过轻量级信号捕捉后验和特征的一致性，转换为校准风险评分，采用O(W)环形缓冲区和O(1)每步更新，结合流式保形层实现预算化的接受/弃权规则

Result: 在微控制器上占用空间比早期退出和深度集成小50-60%，速度快30-45%；在损坏分布流中，准确率下降检测提升3-7 AUPRC点，最高达0.86 AUPRC；故障检测最高达0.92 AUROC

Conclusion: 时间一致性结合流式保形校准为TinyML设备端监控提供了实用且资源高效的基础方案

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [94] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: SparseMap是一个基于进化策略的稀疏张量加速器优化框架，能够联合优化映射策略和稀疏策略，在巨大的设计空间中高效寻找最优解。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏张量加速器设计多为手动设计，局限于特定场景且难以调整。现有工作要么只关注映射策略，要么只关注稀疏策略，缺乏综合考虑，导致设计不优。

Method: 提出SparseMap框架，构建包含映射和稀疏策略的综合设计空间，通过改进遗传编码和进化算子来高效探索巨大的设计空间。

Result: SparseMap相比现有工作和经典优化方法，能够持续找到更优的解决方案。

Conclusion: SparseMap为稀疏张量加速器的自动化设计提供了有效的解决方案，能够处理组合爆炸的设计空间问题。

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [95] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ是一种面向TinyML的单次前向传播、无标签不确定性量化方法，通过深度激活预测来估计风险，具有资源高效的特点


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法在TinyML设备上存在内存占用大、延迟高的问题，需要开发一种轻量级的单次前向传播解决方案

Method: 使用int8量化头部预测下一层激活的统计信息，通过轻量级单调映射器将预测偏差转换为可操作的分数，无需时间缓冲、辅助出口或重复前向传播

Result: 相比早期退出和深度集成方法，SNAP-UQ在视觉和音频任务上减少40-60%存储和25-35%延迟，在损坏数据流中AUPRC提升数个点，AUROC达到约0.9

Conclusion: 基于层间动态的不确定性量化为TinyML设备上的实时监控提供了实用且资源高效的解决方案

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [96] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: SL-ACC是一个通信高效的拆分学习框架，通过自适应通道重要性识别和通道分组压缩来减少传输数据量，同时保持训练精度


<details>
  <summary>Details</summary>
Motivation: 随着参与设备数量增加，拆分学习中过多的传输数据成为主要瓶颈，影响模型训练速度

Method: 使用香农熵识别通道重要性，然后基于熵值进行通道分组和自适应压缩

Result: 在多个数据集上的实验表明，SL-ACC比最先进基准方法用更少时间达到目标精度

Conclusion: 提出的SL-ACC框架有效解决了拆分学习中的通信瓶颈问题，显著提升了训练效率

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [97] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 图卷积网络(GCN)性能与图的代数连通性(Fiedler值)密切相关，Fiedler值相似的图具有类似的结构特性，可以使用相似的滤波器和超参数，且迁移学习在代数连通性相似的图之间更有效。


<details>
  <summary>Details</summary>
Motivation: GCN文献中常见观察是堆叠GCN层可能不会带来更好的节点分类和边预测性能，需要找到预测GCN性能的有效指标。

Method: 通过理论和实证研究，在合成图和真实图数据(Cora、CiteSeer、Polblogs)上进行实验，探索多种聚合图中连通分量Fiedler值的方法来获得整个图的值。

Result: 发现图的Fiedler值是GCN性能的良好预测指标，Fiedler值相似的图具有类似的结构特性，可以使用相似的滤波器和超参数。

Conclusion: 代数连通性(Fiedler值)可以作为GCN性能预测的有效指标，为图神经网络的设计和迁移学习提供了重要指导。

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [98] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta是一种改进的Adam优化器，通过动态调整beta2参数来处理梯度尖峰问题，在物理驱动的PDE代理模型和PINNs中显著提升训练稳定性和性能


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在物理问题中训练时，由于变化的边界条件和初始条件导致的梯度尖峰和不稳定损失问题，特别是在PINNs中复合损失函数会放大这种效应

Method: 将Adam优化器的固定beta2参数替换为层级的动态值，基于当前梯度范数与历史EMA范数的比值（sunspike ratio）来调整beta2，梯度尖峰时降低beta2，平稳时保持较高值

Result: 在四个测试场景中（Heat2D Transformer、Heat3D PINN、MLX合成任务、small-enwik8字符Transformer）均表现出更好的稳定性和更低的最终损失，在small-enwik8上比Adam-0.95降低38%的bits-per-character，比Adam-0.999降低58%

Conclusion: Kourkoutas-Beta保持了Adam的收敛保证，同时显著提升了在梯度尖峰情况下的鲁棒性，计算开销与Adam相当，可作为即插即用的优化器替代方案

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [99] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出FAML方法解决多视图证据学习中的证据偏见问题，通过自适应先验、公平性约束和意见对齐机制实现更平衡的证据分配，提升预测性能和不确定性估计可靠性


<details>
  <summary>Details</summary>
Motivation: 现有多视图证据学习方法假设视图特定证据学习是可靠的，但实践中证据学习过程存在偏见，样本倾向于为数据丰富的类别分配更多证据，导致不确定性估计不可靠

Method: FAML方法包含三个核心组件：1）基于训练轨迹的自适应先验作为正则化策略；2）基于类间证据方差的公平性约束；3）多视图融合阶段的意见对齐机制

Result: 在五个真实世界多视图数据集上的实验表明，FAML实现了更平衡的证据分配，在预测性能和不确定性估计可靠性方面均优于现有最先进方法

Conclusion: FAML有效解决了多视图证据学习中的偏见问题，为可靠的不确定性估计提供了新思路，在多视图学习中具有重要应用价值

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [100] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: 通过蒙特卡洛采样和矩阵矩阵方法近似模型预测分布，结合Wasserstein和KL距离构建正则化函数，提出高效的持续学习方法MCFRCL


<details>
  <summary>Details</summary>
Motivation: 解决基于功能正则化的持续学习方法存在的高计算成本和大线性近似误差问题

Method: 利用蒙特卡洛采样近似模型预测分布，通过三种连续分布和矩阵方法捕捉MC样本的统计特征，使用Wasserstein和KL距离构建正则化函数

Result: 在MNIST和CIFAR数据集上评估显示，MCFRCL在预测准确性和训练效率方面都显示出显著效果

Conclusion: MCFRCL框架通过蒙特卡洛采样和矩阵方法有效地解决了功能正则化方法的计算效率问题，为持续学习领域提供了一种高效的解决方案

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [101] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: 提出一种针对脉冲噪声的鲁棒自适应滤波算法FXHEKM，在α稳定噪声环境下表现出优越的主动噪声控制性能


<details>
  <summary>Details</summary>
Motivation: 传统主动噪声控制算法在脉冲噪声环境下性能下降，需要开发更鲁棒的自适应滤波方法来处理α稳定噪声等加性干扰信号

Method: 开发了滤波-x双曲正切指数广义核M估计函数(FXHEKM)鲁棒自适应算法，进行了统计分析和计算成本研究

Result: 数值结果表明FXHEKM算法在均方误差和平均噪声减少性能指标上优于竞争算法，能有效消除α稳定噪声等加性干扰

Conclusion: FXHEKM算法为脉冲噪声环境下的主动噪声控制提供了一种有效的鲁棒解决方案

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [102] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 本文研究利用NLP和深度学习技术，特别是BERT和层次注意网络，来分析网络攻击的影响后果。实验结果显示BERT在多标签分类任务中达到了0.972的准确率，显著超过传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 网络攻击越来越复杂，行业每年花费数十亿美元进行防御。威胁建模可以帮助安全专业人员理解攻击后果，及时采取行动和分配资源。目前对自动化方法评估攻击描述和预测未来影响有着紧迫需求。

Method: 研究使用自然语言处理(NLP)和深度学习技术，利用MITRE CWE数据库的文本描述来分析网络攻击的潜在影响。将攻击后果分为五个主要类别：可用性、访问控制、保密性、完整性和其他。研究了BERT结合层次注意网络(HAN)在多标签分类中的应用，并与传统的CNN和LSTM模型进行性能比较。

Result: 实验结果显示，BERT在多标签分类任务中达到了0.972的总体准确率，显著超过传统深度学习模型。HAN在特定的安全标签上表现超过了CNN和LSTM基础模型。但BERT在精度和召回率方面一贯表现更好，更适合用于预测网络攻击的后果。

Conclusion: 研究证明了NLP和深度学习技术在网络安全威胁建模中的有效性。BERT模型在多标签分类任务中表现出艰洞的性能，为自动化分析网络攻击影响提供了强大的工具。这项研究为安全专业人员提供了更加准确和高效的威胁建模方法，有助于提高网络安全防御能力。

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [103] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 通过利用分开的内部数据和公开人口数据来估计AI模型的公平性指标，解决完整数据获取困难的问题


<details>
  <summary>Details</summary>
Motivation: 实际业界中由于法律和隐私考虑，难以收集完整的人口统计属性数据来评估AI系统公平性，需要找到可行的备选方案

Method: 利用分开的数据源（内部预测属性数据和外部保护属性数据）估计可行的联合分布，计算公平性指标的可行范围

Result: 通过模拟和实验验证，能够得到有意义的公平性指标上下界，并获得真实指标的可靠估计

Conclusion: 该方法为实际应用中无法获得完整数据的情况下进行公平性测试提供了实用有效的解决方案

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [104] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: 这篇论文提出了一种通过神经代理模型来可视化反向问题中可能输入参数分布的方法，解决了代理模型近似误差和交互式参数分布构建两大挑战。


<details>
  <summary>Details</summary>
Motivation: 目前的代理模型解决方案主要关注找到少量匹配参数，而忽视了可能参数分布的广泛图景，特别是圩高维参数空间中找到这些参数的成本很高。

Method: 通过密度估计来建模代理模型的近似误差，只在参数配置接近训练参数时报告高密度。将密度估计作为参数的先验信念，结合特征的可能性，得到了一种高效的方法来采样生成目标输出特征的可能参数配置。

Result: 在三个模拟数据集上进行了特征驱动的参数分析，展示了该方法的可用性，并通过可视化界面进行交互。

Conclusion: 该方法能够有效地建模和可视化反向问题中可能输入参数的分布，解决了高维参数空间中找到可能参数的挑战，为科学模拟提供了更全面的参数分析能力。

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [105] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 基于海底声响传感器网络和对数高斯科克斯过程，提出了一种分类和检测海上空间偶发异常的框架，通过第二阶近似提高分类准确性，并集成了动态传感器部署策略。


<details>
  <summary>Details</summary>
Motivation: 海上环境中的空间偶发异常检测对于航行安全和监控至关重要，但传统方法往往只考虑强度函数的均值，导致分类准确性不足。

Method: 使用对数高斯科克斯过程模型目标到达，将其模型为正常过程和异常过程的混合。提出第二阶概率近似方法，同时考虑正常强度函数的均值和方差，并集成动态传感器部署策略。

Result: 在美国纽约克岛际边的真实船舶交通数据上验证，数值结果表明该方法在分类性能和异常检测方面都取得了显著改善。

Conclusion: 该框架通过第二阶概率近似和动态传感器部署，有效提高了海上空间偶发异常的分类和检测准确性，为海上监控提供了有力的技术支持。

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [106] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: 这篇论文设计了一种完全真实的批处理设置下的检查清单测量方法（ATB），解决了现有检查清单测量在有限样本上不真实的问题，并为检查清单测试提供了更高效简洁的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的检查清单测量在有限样本上存在不真实性，预测器会为了在样本上显得更检查清单而请偷，虽然有约真实的检查清单测量，但在批处理设置下仍缺少完全真实的测量方法。

Method: 设计了平均化双箱检查清单误差（ATB）作为一种完全真实的检查清单测量，并提出了构造真实测量的通用方法，包括分位箱l_2-ECE等。

Result: ATB测量不仅真实，还具有健壮性、完备性、连续性，且与现有的平滑检查清单误差和距离检查清单存在二次关系，实现更高效简洁的检查清单测试算法。

Conclusion: 该研究成功构造了第一个完全真实的检查清单测量ATB，解决了检查清单预测中的基本问题，为检查清单预测领域提供了重要的理论和实践基础。

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [107] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 提出了CGPT架构，通过因果图引导的成对建模范式解决多维时间序列建模中通道依赖与通道独立的权衡问题，在保持维度无关性的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 解决工业系统中多维时间序列建模的核心权衡：通道依赖模型能捕捉特定跨变量动态但缺乏鲁棒性，通道独立模型具有通用性但无法建模显式交互

Method: CGPT架构整合已知因果图作为归纳偏置，采用成对建模范式将多维数据分解为对，使用通道无关可学习层，在成对级别强制CD信息流，在跨对级别实现CI式泛化

Result: 在合成和真实工业数据集的长周期和单步预测任务上，CGPT显著优于CI和CD基线模型，在预测精度上表现优异，同时保持与端到端训练CD模型竞争的性能

Conclusion: CGPT通过因果引导的成对建模有效解决了多维时间序列建模的权衡问题，实现了可扩展性和任意变量适应性，为工业系统预测提供了灵活而准确的解决方案

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [108] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: CRTR方法通过负采样方案消除虚假特征，在复杂时序结构领域（如Sokoban和魔方）实现强时序推理能力，仅用学习表征就能高效解决任意魔方状态


<details>
  <summary>Details</summary>
Motivation: 研究感知和时序结构能否从表征中共同涌现，解决传统时序对比学习依赖虚假特征而无法捕获时序结构的问题

Method: 提出组合时序推理表征（CRTR），使用负采样方案可证明地移除虚假特征，促进时序推理

Result: 在Sokoban和魔方等复杂时序结构领域取得强劲结果，魔方问题上学习到的表征能泛化到所有初始状态，用比BestFS更少的搜索步骤解决谜题（但解决方案更长）

Conclusion: 这是首个仅使用学习表征就能高效解决任意魔方状态的方法，无需依赖外部搜索算法

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [109] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: 本文重点研究个体级别人类移动预测的最佳训练方法，通过综合实验分析多种模型和训练策略，提出包含语义信息和采用分层抽样的方法来提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在微观层面的短期轨迹预测，对宏观层面的生活常规移动模式关注不够，需要探索如何使用历史数据训练机器学习模型来预测未来整体轨迹的最佳实践方法。

Method: 进行了全面的实验分析，包括LSTM和Transformer等多种模型架构、参数配置和训练策略。深入研究了如何通过包含星期数和用户特定历史信息等语义信息来提升预测效果。采用用户语义聚类和分层抽样来缓解数据偏斜问题。

Result: 显示明确包含语义信息能够帮助模型更好理解个体生活模式，显著提高预测准确性。用户抽样可能引发数据偏斜并导致预测准确性大幅下降。小批量随机梯度优化在训练数据有限时能够改善模型性能。

Conclusion: 通过系统性的实验分析，证明了在个体级别人类移动预测中，明确包含语义信息、采用分层抽样来保持数据多样性、以及适当的训练策略对提升预测效果的重要性。

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [110] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 本文提出MDPO方法来解决扩散语言模型训练与推理阶段的结构不一致问题，通过强化学习优化去噪轨迹，显著提升性能并减少训练成本。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在推理时逐步揭示序列结构，而训练时随机掩码忽略这种结构，这种不一致导致性能下降，需要解决这一长期被忽视的问题。

Method: 将去噪轨迹学习建模为序列决策问题，提出Masked Diffusion Policy Optimization (MDPO)方法，利用扩散的马尔可夫性质，在推理相同的渐进细化调度下显式训练模型。

Result: MDPO以60倍更少的梯度更新达到之前SOTA性能，在相同权重更新次数下，MATH500提升9.6%，Countdown提升54.2%。提出的RCR重掩码策略作为即插即用推理替代进一步提升性能。

Conclusion: 研究揭示了MDLMs预训练与推理间差异的巨大潜力，MDPO和RCR为扩散语言模型提供了有效的训练和推理优化方案。

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [111] [HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware](https://arxiv.org/abs/2508.11935)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Hanjie Liu,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.AR

TL;DR: 该论文分析了状态空间模型(SSMs)在计算存储架构中的噪声突变问题，识别出最后块和输出投影层对噪声敏感，并提出HPD混合投影分解策略来提升模型稳健性。


<details>
  <summary>Details</summary>
Motivation: 计算存储(CIM)架构能提高能消效率，但设备非理想性引入的权重突变会降低SSMs的推理准确性，需要研究模型的稳健性问题。

Method: 提出HPD混合投影分解策略，将最后输出投影层的权重矩阵替换为其SVD分解中的U和Σ矩阵乘积，同时将V转置矩阵移植到数字硬件进行精确管理。

Result: 在各种噪声条件下，HPD方法将Mamba模型的困惑度降低了最高99.57%，在PIQA公共理解测试中准确率提升了最高96.67%。

Conclusion: 该研究系统性地分析了SSMs在噪声环境下的脆弱点，HPD方法能够有效提升模型在计算存储架构中的稳健性和性能。

Abstract: State Space Models (SSMs) are efficient alternatives to traditional sequence
models, excelling at processing long sequences with lower computational
complexity. Their reliance on matrix multiplications makes them ideal for
compute-in-memory (CIM) architectures, which improve energy efficiency by
computing within memory arrays. However, device non-idealities in CIM introduce
weight perturbations that can degrade inference accuracy. In this paper, we
systematically analyze the robustness of SSMs under noisy conditions,
identifying that the final block and output projection layers are more
susceptible to perturbations compared to other components. Building on these
insights, we propose HPD, a Hybrid Projection Decomposition strategy for the
last output projection layer. We replace the original weight matrix with the
multiplication of U and {\Sigma} in its SVD to ensure compatibility with
existing hardware architectures, while offloading V> to digital hardware for
precise and robust correction. Comprehensive tests on Mamba models show that
our method reduces perplexity by up to 99.57% under various noise conditions
compared to baseline models, with accuracy gains of up to 96.67% on the PIQA
benchmark for commonsense reasoning.

</details>


### [112] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI是一个超低延迟的端到端边缘AI平台，结合事件相机和FPGA芯片，通过硬件优化的预处理管道实现了94%的DVS手势识别准确率和1000fps的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 事件相机在边缘机器人应用中具有异步操作和稀疏事件驱动输出的优势，但现有解决方案缺乏完整的端到端实现，延迟高且未能充分利用事件数据的稀疏性。

Method: 开发了HOMI平台，包含Prophesee IMX636事件传感器和Xilinx Zynq UltraScale+ MPSoC FPGA芯片，部署了内部开发的AI加速器。开发了支持恒定时间和恒定事件模式的硬件优化预处理管道，包括直方图累积、线性和指数时间表面。

Result: 在高精度配置下，DVS手势数据集上达到94%的准确率；在低延迟配置下提供1000fps的吞吐量。硬件优化管道仅使用33%的FPGA LUT资源，内存占用紧凑。

Conclusion: HOMI平台为准确性和低延迟应用提供了通用解决方案，为进一步降低延迟、模型并行化、多任务部署和集成更复杂架构留有充足余量。

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [113] [Special Session: Sustainable Deployment of Deep Neural Networks on Non-Volatile Compute-in-Memory Accelerators](https://arxiv.org/abs/2508.12195)
*Yifan Qin,Zheyu Yan,Wujie Wen,Xiaobo Sharon Hu,Yiyu Shi*

Main category: cs.AR

TL;DR: 提出了一种基于负反馈理论的负优化训练机制OVF，用于解决NVM计算内存加速器中由于器件随机性和变化导致的性能下降问题，在减少写验证操作依赖的同时显著提升推理精度。


<details>
  <summary>Details</summary>
Motivation: NVM计算内存加速器虽然能提升DNN推理的能效和延迟，但由于NVM器件的随机性和内在变化导致性能下降。传统的写验证操作虽然能提高精度但能耗和时间成本高昂。

Method: 基于负反馈理论提出负优化训练机制，开发了定向变分前向(OVF)训练方法，通过在训练阶段模拟器件变化来增强模型的鲁棒性。

Result: 实验显示OVF相比现有技术推理精度提升高达46.71%，同时降低了认知不确定性，减少了对写验证操作的依赖。

Conclusion: 该机制实现了NVCIM加速器的鲁棒部署，在保持可持续计算优势的同时解决了性能退化问题，为NVCIM加速器的实际部署提供了可行方案。

Abstract: Non-volatile memory (NVM) based compute-in-memory (CIM) accelerators have
emerged as a sustainable solution to significantly boost energy efficiency and
minimize latency for Deep Neural Networks (DNNs) inference due to their in-situ
data processing capabilities. However, the performance of NVCIM accelerators
degrades because of the stochastic nature and intrinsic variations of NVM
devices. Conventional write-verify operations, which enhance inference accuracy
through iterative writing and verification during deployment, are costly in
terms of energy and time. Inspired by negative feedback theory, we present a
novel negative optimization training mechanism to achieve robust DNN deployment
for NVCIM. We develop an Oriented Variational Forward (OVF) training method to
implement this mechanism. Experiments show that OVF outperforms existing
state-of-the-art techniques with up to a 46.71% improvement in inference
accuracy while reducing epistemic uncertainty. This mechanism reduces the
reliance on write-verify operations and thus contributes to the sustainable and
practical deployment of NVCIM accelerators, addressing performance degradation
while maintaining the benefits of sustainable computing with NVCIM
accelerators.

</details>


### [114] [A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips](https://arxiv.org/abs/2508.12251)
*Wenyong Zhou,Yuan Ren,Jiajun Zhou,Tianshu Hou,Ngai Wong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种RRAM友好的轻量级卷积神经网络设计，通过改进DenseNet结构来提高存储器交叉柱利用率，降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统轻量级CNN设计（如深度卷积）在RRAM存储器中导致的交叉柱利用率低下问题，以满足边缘AI设备的效率需求。

Method: 采用一种新的特征图拼接方案，将每个阶段前端层的特征图拼接作为最后一层的输入，以提高RRAM交叉柱利用率。

Result: 实验结果显示，该方案在CIFAR和ImageNet数据集上实现了竞争性的准确率，同时比传统ResNet和DenseNet更节省时间和能量。

Conclusion: 该研究为设计RRAM友好的高效轻量级CNN提供了有效方案，在保持模型性能的同时明显提升了硬件效率。

Abstract: Designing lightweight convolutional neural network (CNN) models is an active
research area in edge AI. Compute-in-memory (CIM) provides a new computing
paradigm to alleviate time and energy consumption caused by data transfer in
von Neumann architecture. Among competing alternatives, resistive random-access
memory (RRAM) is a promising CIM device owing to its reliability and multi-bit
programmability. However, classical lightweight designs such as depthwise
convolution incurs under-utilization of RRAM crossbars restricted by their
inherently dense weight-to-RRAM cell mapping. To build an RRAM-friendly yet
efficient CNN, we evaluate the hardware cost of DenseNet which maintains a high
accuracy vs other CNNs at a small parameter count. Observing the linearly
increasing channels in DenseNet leads to a low crossbar utilization and causes
large latency and energy consumption, we propose a scheme that concatenates
feature maps of front layers to form the input of the last layer in each stage.
Experiments show that our proposed model consumes less time and energy than
conventional ResNet and DenseNet, while producing competitive accuracy on CIFAR
and ImageNet datasets.

</details>


### [115] [AutoPower: Automated Few-Shot Architecture-Level Power Modeling by Power Group Decoupling](https://arxiv.org/abs/2508.12294)
*Qijun Zhang,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: AutoPower是一个自动化架构级功耗建模工具，通过功耗组分拆解方法，仅需少量已知配置即可实现高精度功耗预测，相比传统方法显著提升了准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现代CPU设计中功耗效率至关重要，但传统分析模型不准确，而基于机器学习的模型需要大量训练数据，这在早期设计阶段不现实

Method: 提出功耗组分拆解方法：1）跨功耗组拆解，为每个组建立独立模型；2）在组内进一步拆分为多个子模型。基于时钟和SRAM功耗占主导且与架构级结构信息相关的观察

Result: 仅需2个已知配置训练即可达到4.36%的MAPE和0.96的R²，比代表性ML模型McPAT-Calib的MAPE低5%，R²高0.09

Conclusion: AutoPower实现了有限已知配置下的全自动架构级功耗建模，显著提高了功耗评估的准确性和实用性

Abstract: Power efficiency is a critical design objective in modern CPU design.
Architects need a fast yet accurate architecture-level power evaluation tool to
perform early-stage power estimation. However, traditional analytical
architecture-level power models are inaccurate. The recently proposed machine
learning (ML)-based architecture-level power model requires sufficient data
from known configurations for training, making it unrealistic.
  In this work, we propose AutoPower targeting fully automated
architecture-level power modeling with limited known design configurations. We
have two key observations: (1) The clock and SRAM dominate the power
consumption of the processor, and (2) The clock and SRAM power correlate with
structural information available at the architecture level. Based on these two
observations, we propose the power group decoupling in AutoPower. First,
AutoPower decouples across power groups to build individual power models for
each group. Second, AutoPower designs power models by further decoupling the
model into multiple sub-models within each power group. In our experiments,
AutoPower can achieve a low mean absolute percentage error (MAPE) of 4.36\% and
a high $R^2$ of 0.96 even with only two known configurations for training. This
is 5\% lower in MAPE and 0.09 higher in $R^2$ compared with McPAT-Calib, the
representative ML-based power model.

</details>


### [116] [Soft Error Probability Estimation of Nano-scale Combinational Circuits](https://arxiv.org/abs/2508.12345)
*Ali Jockar,Mohsen Raji*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的软错误概率分析框架，统一考虑制造变异和老化效应，在保持高精度的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着技术缩小，纳米级数字电路更易受单事件故障影响，而现有方法多单独考虑制造变异或老化，或者计算成本过高，限制了大规模电路优化的实用性。

Method: 提出了一种新的SEP分析框架，整体集成制造变异和老化效应，包括改进的电气拦截模型和统计方法来量化软错误概率。

Result: 实验结果显示，该方法在保持高精度的同时，计算开销比蒙特卡洛法约降低了2.5%。

Conclusion: 这项工作通过提供高效、准确的SEP估计方法，推动了可靠纳米级电路设计的发展。

Abstract: As technology scales, nano-scale digital circuits face heightened
susceptibility to single event upsets (SEUs) and transients (SETs) due to
shrinking feature sizes and reduced operating voltages. While logical,
electrical, and timing masking effects influence soft error probability (SEP),
the combined impact of process variation (PV) and aging-induced degradation
further complicates SEP estimation. Existing approaches often address PV or
aging in isolation, or rely on computationally intensive methods like Monte
Carlo simulations, limiting their practicality for large-scale circuit
optimization. This paper introduces a novel framework for SEP analysis that
holistically integrates PV and aging effects. We propose an enhanced electrical
masking model and a statistical methodology to quantify soft error probability
under process and aging variations. Experimental results demonstrate that the
proposed approach achieves high accuracy while reducing computational overhead
by approximately 2.5% compared to Monte Carlo-based methods. This work advances
the design of reliable nano-scale circuits by enabling efficient, accurate SEP
estimation in the presence of manufacturing variability and long-term
transistor degradation.

</details>


### [117] [An ECC-based Fault Tolerance Approach for DNNs](https://arxiv.org/abs/2508.12347)
*Mohsen Raji,Mohammad Zaree,Kimia Soroush*

Main category: cs.AR

TL;DR: 提出了一种基于纠错码(ECC)的深度神经网络容错方法SPW，通过检测和纠正单比特错误或屏蔽多比特错误来提高DNN在内存位翻转故障下的可靠性


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已部署在数据中心和安全关键系统中，需要确保在内存位翻转错误情况下的正确功能，这对安全关键应用至关重要

Method: 使用纠错码(ECC)检测错误，单比特错误时进行纠正，多比特错误时将权重置零(屏蔽)。采用统计故障注入活动评估方法效果

Result: 在误码率为10^(-1)的情况下，DNN准确率相比应用ECC技术时提高了300%以上，仅带来47.5%的面积开销

Conclusion: SPW方法能有效提高深度神经网络在内存位翻转故障下的容错能力，以相对较小的硬件开销显著提升系统可靠性

Abstract: Deep Neural Network (DNN) has achieve great success in solving a wide range
of machine learning problems. Recently, they have been deployed in datacenters
(potentially for business-critical or industrial applications) and
safety-critical systems such as self-driving cars. So, their correct
functionality in the presence of potential bit-flip errors on DNN parameters
stored in memories plays the key role in their applicability in safety-critical
applications. In this paper, a fault tolerance approach based on Error
Correcting Codes (ECC), called SPW, is proposed to ensure the correct
functionality of DNNs in the presence of bit-flip faults. In the proposed
approach, error occurrence is detected by the stored ECC and then, it is
correct in case of a single-bit error or the weight is completely set to zero
(i.e. masked) otherwise. A statistical fault injection campaign is proposed and
utilized to investigate the efficacy of the proposed approach. The experimental
results show that the accuracy of the DNN increases by more than 300% in the
presence with Bit Error Rate of 10^(-1) in comparison to the case where ECC
technique is applied, in expense of just 47.5% area overhead.

</details>


### [118] [ATLAS: A Self-Supervised and Cross-Stage Netlist Power Model for Fine-Grained Time-Based Layout Power Analysis](https://arxiv.org/abs/2508.12433)
*Wenkai Li,Yao Lu,Wenji Fang,Jing Wang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AR

TL;DR: ATLAS是一个无需布局信息的门级网表时序功耗预测模型，在时钟树、寄存器和组合逻辑功耗预测上分别达到0.58%、0.45%和5.12%的MAPE误差，总功耗误差<1%，推理速度显著快于商业工具。


<details>
  <summary>Details</summary>
Motivation: 传统精确功耗仿真需要耗时的后端处理和仿真步骤，严重阻碍设计优化流程，需要一种能够直接从门级网表预测最终布局功耗的方法。

Method: 提出了专门为电路功耗定制的新预训练和微调范式，支持时序功耗仿真和跨设计功耗建模，无需任何布局信息即可预测最终时序布局功耗。

Result: 在时钟树、寄存器和组合逻辑功耗组上分别达到0.58%、0.45%和5.12%的MAPE误差，总功耗误差小于1%，推理速度显著快于商业工具标准流程。

Conclusion: ATLAS是首个支持时序功耗仿真和通用跨设计功耗建模的工作，能够高效准确地从门级网表预测最终布局功耗，极大加速设计优化流程。

Abstract: Accurate power prediction in VLSI design is crucial for effective power
optimization, especially as designs get transformed from gate-level netlist to
layout stages. However, traditional accurate power simulation requires
time-consuming back-end processing and simulation steps, which significantly
impede design optimization. To address this, we propose ATLAS, which can
predict the ultimate time-based layout power for any new design in the
gate-level netlist. To the best of our knowledge, ATLAS is the first work that
supports both time-based power simulation and general cross-design power
modeling. It achieves such general time-based power modeling by proposing a new
pre-training and fine-tuning paradigm customized for circuit power. Targeting
golden per-cycle layout power from commercial tools, our ATLAS achieves the
mean absolute percentage error (MAPE) of only 0.58%, 0.45%, and 5.12% for the
clock tree, register, and combinational power groups, respectively, without any
layout information. Overall, the MAPE for the total power of the entire design
is <1%, and the inference speed of a workload is significantly faster than the
standard flow of commercial tools.

</details>


### [119] [MemorySim: An RTL-level, timing accurate simulator model for the Chisel ecosystem](https://arxiv.org/abs/2508.12636)
*Ansh Chaurasia*

Main category: cs.AR

TL;DR: MemorySim是一个RTL级内存模拟器，提供精确的时序和功能正确性，与Chisel/Verilog仿真无缝集成，支持性能功耗评估


<details>
  <summary>Details</summary>
Motivation: AI硬件需求增长，内存子系统成为性能瓶颈，现有模拟器在时序仿真和RTL集成方面存在正确性问题

Method: 开发RTL级内存模拟器MemorySim，与Chisel/Verilog仿真和Chisel/Chipyard生态系统完全兼容

Result: 能够提供精确的性能和功耗估算，支持通过FireSim等仿真平台进行下游评估

Conclusion: MemorySim解决了现有内存模拟器在时序准确性和RTL集成方面的不足，为AI硬件开发提供了更好的仿真工具

Abstract: The rapid growth of AI applications has driven increased demand for
specialized AI hardware, highlighting critical opportunities within the memory
subsystem, which often serves as a performance bottleneck in high-demand
workloads such as large language models (LLMs). Existing high-level memory
simulators, such as DRAMSim2 and DRAMSim3, offer timing simulations but
frequently compromise on correctness or integration at the register-transfer
level (RTL). We present MemorySim, an RTL-level memory simulator designed to
deliver both accurate timing and functional correctness. MemorySim integrates
seamlessly with existing Chisel and Verilog simulations and is fully compatible
with the Chisel/Chipyard ecosystem. This enables users to obtain precise
performance and power estimates, supporting downstream evaluation through
simulation platforms such as FireSim.

</details>


### [120] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE是一个面向XR感知工作负载的高吞吐量混合精度SIMD神经处理引擎，支持多种低精度格式，显著降低内存带宽需求，在28nm CMOS工艺下实现1.72GHz频率和0.016mm²面积，相比现有技术减少42%面积和38%功耗。


<details>
  <summary>Details</summary>
Motivation: 针对扩展现实(XR)设备中视觉惯性里程计、物体分类和眼球追踪等感知工作负载的高计算需求，需要设计一个高效能、低功耗的神经处理引擎来满足资源受限设备的性能要求。

Method: 采用混合精度算法支持FP4、Posit(4,1)、Posit(8,0)、Posit(16,1)等格式，使用可重构尾数乘法和指数处理电路(RMMEC)减少暗硅，配合选择性电源门控技术降低能耗，并通过量化感知训练最小化精度损失。

Result: 在28nm CMOS工艺下实现1.72GHz最高工作频率，面积0.016mm²，算术强度14pJ，相比最先进的MAC方法减少42%面积和38%功耗。基于AXI的矩阵乘法协处理器在VCU129上相比SoTA加速器减少1.4x LUTs和1.77x FFs，能效提升1.2x。

Conclusion: XR-NPE被证明是一个可扩展、精度自适应的计算引擎，适用于未来资源受限的XR设备，为设计者和研究人员提供了完整的开源实现。

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [121] [Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs](https://arxiv.org/abs/2508.12743)
*Jacob Wahlgren,Gabin Schieffer,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 首次对AMD MI300A APU的统一物理内存(UPM)架构进行全面分析，证明在统一内存模型下应用性能可以超过或平逗显式管理模型，同时降低内存成本达44%


<details>
  <summary>Details</summary>
Motivation: 解决离散GPU在HPC和数据中心系统中的分离内存空间管理问题，而统一虚拟内存(UVM)存在性能成本高的缺点

Method: 首先分析MI300A UPM系统特性(内存延迟、带宽、一致性开销)，然后评估系统软件在内存分配、页缺陷处理、TLB管理等方面的效率，最后提出应用程序迁移策略并在6个应用上进行评测

Result: 统一物理内存架构能够使应用在统一内存模型下达到或超过显式管理模型的性能，同时降低内存成本达44%

Conclusion: AMD MI300A APU的UPM架构为HPC系统提供了更高效的内存管理方案，在保持或提升性能的同时显著降低内存成本

Abstract: Discrete GPUs are a cornerstone of HPC and data center systems, requiring
management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)
has been proposed to ease the burden of memory management; however, at a high
cost in performance. The recent introduction of AMD's MI300A Accelerated
Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables
HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)
for the first time. This work presents the first comprehensive characterization
of the UPM architecture on MI300A. We first analyze the UPM system properties,
including memory latency, bandwidth, and coherence overhead. We then assess the
efficiency of the system software in memory allocation, page fault handling,
TLB management, and Infinity Cache utilization. We propose a set of porting
strategies for transforming applications for the UPM architecture and evaluate
six applications on the MI300A APU. Our results show that applications on UPM
using the unified memory model can match or outperform those in the explicitly
managed model--while reducing memory costs by up to 44%.

</details>


### [122] [Proceedings 18th Interaction and Concurrency Experience](https://arxiv.org/abs/2508.12308)
*Clément Aubert,Cinzia Di Giusto,Simon Fowler,Violet Ka I Pun*

Main category: cs.DC

TL;DR: ICE'25会议论文集，包含4篇论文和1个口头报告，采用匿名互动评审机制，共收到7篇投稿并进行了75条评论交流


<details>
  <summary>Details</summary>
Motivation: 促进并发与交互领域的研究交流，通过独特的匿名互动评审机制提高论文质量

Method: 采用PC成员与作者匿名互动的评审流程，每篇投稿由3名PC成员评审，通过评论交流进行论文筛选

Result: 从7篇投稿中接受了4篇论文发表和1个口头报告，进行了约75条评论交流

Conclusion: ICE会议通过创新的互动评审机制成功促进了学术交流，为并发与交互领域的研究提供了高质量的平台

Abstract: This volume contains the proceedings of ICE'25, the 18th Interaction and
Concurrency Experience, which was held on Friday 20th June 2025 at the \'Ecole
National Sup\'erieure des Arts et M\'etiers in Lille, France, as a satellite
workshop of DisCoTec 2025. The ICE workshop series features a distinguishing
review and selection procedure: PC members are encouraged to interact,
anonymously, with authors. The 2025 edition of ICE received 7 submissions, each
reviewed by three PC members, and about 75 comments were exchanged during the
review process, witnessing very lively discussions. Four papers were accepted
for publication plus 1 oral communication, which was accepted for presentation
at the workshop. We were proud to host one invited talk, by Kirstin Peters. The
abstract of her talk is included in this volume, together with the final
versions of the research papers, which take into account the discussion at the
workshop and during the review process.

</details>


### [123] [Breaking the Aggregation Bottleneck in Federated Recommendation: A Personalized Model Merging Approach](https://arxiv.org/abs/2508.12386)
*Jundong Chen,Honglei Zhang,Chunxu Zhang,Fangyuan Luo,Yidong Li*

Main category: cs.DC

TL;DR: FedEM通过弹性融合全局和局部模型来解决联邦推荐中的聚合瓶颈问题，在保持隐私的同时提升个性化推荐性能


<details>
  <summary>Details</summary>
Motivation: 发现联邦推荐中服务器端聚合会损害客户端个性化，导致次优性能，这是由于客户端异构性导致全局模型偏离局部最优解

Method: 提出FedEM方法，弹性合并全局和局部模型来补偿受损的个性化，利用现成的局部模型而非设计额外机制

Result: 在真实数据集上的大量实验表明，该方法在协作训练过程中保持了客户端个性化，优于最先进的基线方法

Conclusion: FedEM通过理论驱动的弹性融合方法有效解决了联邦推荐中的聚合瓶颈问题，为个性化联邦推荐提供了新的解决方案

Abstract: Federated recommendation (FR) facilitates collaborative training by
aggregating local models from massive devices, enabling client-specific
personalization while ensuring privacy. However, we empirically and
theoretically demonstrate that server-side aggregation can undermine
client-side personalization, leading to suboptimal performance, which we term
the aggregation bottleneck. This issue stems from the inherent heterogeneity
across numerous clients in FR, which drives the globally aggregated model to
deviate from local optima. To this end, we propose FedEM, which elastically
merges the global and local models to compensate for impaired personalization.
Unlike existing personalized federated recommendation (pFR) methods, FedEM (1)
investigates the aggregation bottleneck in FR through theoretical insights,
rather than relying on heuristic analysis; (2) leverages off-the-shelf local
models rather than designing additional mechanisms to boost personalization.
Extensive experiments on real-world datasets demonstrate that our method
preserves client personalization during collaborative training, outperforming
state-of-the-art baselines.

</details>


### [124] [DIT: Dimension Reduction View on Optimal NFT Rarity Meters](https://arxiv.org/abs/2508.12671)
*Dmitry Belousov,Yury Yanovich*

Main category: cs.DC

TL;DR: 本文提出了基于降维方法的NFT稀有度计量新框架，引入DIT性能指标和最优稀有度计量器，在ROAR基准测试中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: NFT稀有度评估缺乏标准化比较框架，现有稀有度计量器难以直接对比，需要统一的评估基准和方法论改进

Method: 采用非度量加权多维尺度分析进行最优稀有度计量器设计，提出基于降维技术的DIT性能指标，开发非可解释性稀有度计量器DIT

Result: DIT稀有度计量器在ROAR基准测试中表现出优于现有方法的性能

Conclusion: 降维方法为NFT稀有度计量提供了有效框架，DIT指标和计量器设计解决了稀有度评估标准化问题，推动了该领域的方法论发展

Abstract: Non-fungible tokens (NFTs) have become a significant digital asset class,
each uniquely representing virtual entities such as artworks. These tokens are
stored in collections within smart contracts and are actively traded across
platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is
closely tied to their distinctive characteristics that define rarity, leading
to a growing interest in quantifying rarity within both industry and academia.
While there are existing rarity meters for assessing NFT rarity, comparing them
can be challenging without direct access to the underlying collection data. The
Rating over all Rarities (ROAR) benchmark addresses this challenge by providing
a standardized framework for evaluating NFT rarity. This paper explores a
dimension reduction approach to rarity design, introducing new performance
measures and meters, and evaluates them using the ROAR benchmark. Our
contributions to the rarity meter design issue include developing an optimal
rarity meter design using non-metric weighted multidimensional scaling,
introducing Dissimilarity in Trades (DIT) as a performance measure inspired by
dimension reduction techniques, and unveiling the non-interpretable rarity
meter DIT, which demonstrates superior performance compared to existing
methods.

</details>


### [125] [Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement](https://arxiv.org/abs/2508.12851)
*Tian Wu,Liming Wang,Zijian Wen,Xiaoxi Zhang,Jingpu Duan,Xianwei Zhang,Jinhang Zuo*

Main category: cs.DC

TL;DR: DanceMoE是一个高效的MoE推理框架，通过激活感知的专家放置算法和轻量级迁移机制，在异构边缘服务器上实现协作式MoE推理，显著降低延迟和通信开销。


<details>
  <summary>Details</summary>
Motivation: MoE模型在边缘环境中的推理面临内存占用大、通信复杂等挑战，现有方法主要关注单设备或同构设置，无法满足异构边缘服务器的需求。

Method: 提出激活感知的专家放置算法，利用MoE模型的稀疏性和工作负载局部性，平衡服务器间的本地覆盖和内存使用，并设计轻量级迁移机制适应动态工作负载。

Result: 在现代MoE模型和广泛使用的数据集上评估，相比最先进基线方法，推理延迟降低30.6%，通信开销大幅减少。

Conclusion: DanceMoE证明了协作式边缘MoE推理的有效性，为资源受限的边缘环境提供了高效的MoE服务解决方案。

Abstract: Mixture-of-Experts (MoE) have become a cornerstone for training and scaling
large language models (LLMs), offering substantial gains in model capacity and
efficiency through sparse expert activation. However, serving these models
remains challenging in practice, particularly in resource-constrained edge
environments, due to their large memory footprint and complex communication
demands. While centralized cloud inference is common, it incurs high
infrastructure costs, along with latency and privacy concerns. A few recent
edge MoE works propose memory-efficient strategies but typically focus on
single-device or homogeneous setups. This paper presents DanceMoE, an efficient
MoE inference framework that enables activation-aware expert placement across
collaborative, heterogeneous, GPU-equipped edge servers. DanceMoE leverages the
inherent sparsity of MoE models and workload locality to minimize cross-server
communication and enable efficient expert placement under heterogeneous
resource constraints. It introduces a data-driven, activation-aware placement
algorithm that balances local coverage and memory usage across servers,
alongside a lightweight migration mechanism that adapts expert assignments
under evolving workloads. We evaluate DanceMoE on modern MoE models and widely
used datasets, demonstrating up to 30.6\% lower inference latency, and
substantial communication reduction compared to state-of-the-art baselines,
showcasing the effectiveness of collaborative edge-based MoE inference.

</details>


### [126] [WANify: Gauging and Balancing Runtime WAN Bandwidth for Geo-distributed Data Analytics](https://arxiv.org/abs/2508.12961)
*Anshuman Das Mohapatra,Kwangsung Oh*

Main category: cs.DC

TL;DR: WANify是一个通过机器学习预测动态WAN带宽的框架，帮助地理分布式数据分析系统优化数据传输决策，减少延迟和成本


<details>
  <summary>Details</summary>
Motivation: 现有的GDA系统静态独立测量数据中心间WAN带宽，无法准确反映运行时动态并发传输的实际带宽容量，导致次优决策和性能下降

Method: 使用基于决策树的随机森林机器学习算法动态预测运行时WAN带宽，确定异构并行连接的最优数量，充分利用可用WAN容量

Result: 在AWS 8个地理分布式数据中心上测试，WANify通过平衡最强和最弱WAN链路，将延迟降低26%，成本降低16%

Conclusion: WANify能够有效处理网络和工作负载动态性以及数据偏斜等异构性，以最小代价显著提升GDA系统性能

Abstract: Accurate wide area network (WAN) bandwidth (BW) is essential for
geo-distributed data analytics (GDA) systems to make optimal decisions such as
data and task placement to improve performance. Existing GDA systems, however,
measure WAN BW statically and independently between data centers (DCs), while
data transfer occurs dynamically and simultaneously among DCs during workload
execution. Also, they use a single connection WAN BW that cannot capture actual
WAN capacities between distant DCs. Such inaccurate WAN BWs yield sub-optimal
decisions, inflating overall query latency and cost. In this paper, we present
WANify, a new framework that precisely and dynamically gauges achievable
runtime WAN BW using a machine learning prediction scheme, decision tree-based
Random Forest. This helps GDA systems make better decisions yielding reduced
latency and costs including WAN BW monitoring costs. Based on predicted runtime
WAN BW, WANify determines the optimal number of heterogeneous parallel
connections for data transfer among DCs. This approach improves performance
without additional, or even at reduced cost, by fully exploiting available WAN
capacities. In addition, WANify considers dynamics like network and workloads,
and heterogeneity like skewed data, heterogeneous compute resources, and a
varying number of DCs while making decisions. The WANify prototype running on
state-of-the-art GDA systems is evaluated on AWS with 8 geo-distributed DCs.
Results show that WANify enhances WAN throughput by balancing between the
strongest and weakest WAN links, enabling GDA systems to reduce latency and
cost by up to 26% and 16% respectively with minimal effort, all while handling
dynamics and heterogeneity efficiently.

</details>


### [127] [Congested Clique Counting for Local Gibbs Distributions](https://arxiv.org/abs/2508.13083)
*Joshua Z. Sobel*

Main category: cs.DC

TL;DR: 首个在CongestedClique模型中的近似计数算法，能够在O~(n^{1/3}/ε^2)轮次内近似计算图的q-着色数量和Gibbs分布的分配函数，当q>2Δ或满足快速混合条件时


<details>
  <summary>Details</summary>
Motivation: 建立组合采样与计数问题之间的减约关系，并在分布式计算环境中实现高效的近似计数算法

Method: 通过并行抽取n个随机样本的算法，继承了三角形计数和半环矩阵乘法的思想，利用马尔可夫链的快速混合性质

Result: 在q>2Δ时，能够在O~(n^{1/3}/ε^2)轮次内达到ε-乘性误差的近似计数，在硬核模型中甚至可以达到O~(1/ε^2)轮次

Conclusion: 该算法不仅解决了近似计数问题，还为需要大量样本的其他应用提供了新的并行采样技术

Abstract: There are well established reductions between combinatorial sampling and
counting problems (Jerrum, Valiant, Vazirani TCS 1986). Building off of a very
recent parallel algorithm utilizing this connection (Liu, Yin, Zhang arxiv
2024), we demonstrate the first approximate counting algorithm in the
CongestedClique for a wide range of problems. Most interestingly, we present an
algorithm for approximating the number of $q$-colorings of a graph within
$\epsilon$-multiplicative error, when $q>\alpha\Delta$ for any constant
$\alpha>2$, in $\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds. More
generally, we achieve a runtime of
$\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds for approximating the
partition function of Gibbs distributions defined over graphs when simple
locality and fast mixing conditions hold. Gibbs distributions are widely used
in fields such as machine learning and statistical physics. We obtain our
result by providing an algorithm to draw $n$ random samples from a distributed
Markov chain in parallel, using similar ideas to triangle counting (Dolev,
Lenzen, Peled DISC 2012) and semiring matrix multiplication (Censor-Hillel,
Kaski, Korhonen, Lenzen, Paz, Suomela PODC 2015). Aside from counting problems,
this result may be interesting for other applications requiring a large number
of samples. In the special case of estimating the partition function of the
hardcore model, also known as counting weighted independent sets, we can do
even better and achieve an $\Tilde{O}\big(\frac{1}{\epsilon^2}\big)$ round
algorithm, when the fugacity $\lambda \leq \frac{\alpha}{\Delta-1}$, where
$\alpha$ is an arbitrary constant less than $1$.

</details>


### [128] [Team Formation and Applications](https://arxiv.org/abs/2508.13084)
*Yuval Emek,Shay Kutten,Ido Rafael,Gadi Taubenfeld*

Main category: cs.DC

TL;DR: 新的长存期分布式问题Team Formation（TF）及其随机算法，在异步模型中用于构建团队，并通过减化解决多个分布式问题。算法突破了异步隐式领导者选举的线性消息复杂度上限，并达到了消息复杂性的紧罐下界。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式系统中的多种问题（包括领导者选举、阈值检测等），需要一个基础性的问题模型和高效算法。Team Formation问题可以作为这些问题的通用减化基础，提高算法效率。

Method: 在完全通信图的异步模型中，使用有界消息大小，通过随机化算法将环境注入的token组织成固定大小σ的团队。算法能够应对部分节点的初始故障。

Result: 算法首次突破了异步隐式领导者选举的线性消息复杂度上限，改善了异步显式领导者选举的时间复杂性，并且达到了Team Formation问题消息复杂性的紧罐下界。

Conclusion: Team Formation问题是一个基础性的长存期分布式问题，可以作为多种分布式问题的通用减化基础。所提出的随机化算法在消息和时间复杂度上都很高效，并且达到了理论上的最优界。

Abstract: A novel long-lived distributed problem, called Team Formation (TF), is
introduced together with a message- and time-efficient randomized algorithm.
The problem is defined over the asynchronous model with a complete
communication graph, using bounded size messages, where a certain fraction of
the nodes may experience a generalized, strictly stronger, version of initial
failures. The goal of a TF algorithm is to assemble tokens injected by the
environment, in a distributed manner, into teams of size $\sigma$, where
$\sigma$ is a parameter of the problem.
  The usefulness of TF is demonstrated by using it to derive efficient
algorithms for many distributed problems. Specifically, we show that various
(one-shot as well as long-lived) distributed problems reduce to TF. This
includes well-known (and extensively studied) distributed problems such as
several versions of leader election and threshold detection. For example, we
are the first to break the linear message complexity bound for asynchronous
implicit leader election. We also improve the time complexity of
message-optimal algorithms for asynchronous explicit leader election. Other
distributed problems that reduce to TF are new ones, including matching players
in online gaming platforms, a generalization of gathering, constructing a
perfect matching in an induced subgraph of the complete graph, quorum sensing
in message-passing networks, and more. To complement our positive contribution,
we establish a tight lower bound on the message complexity of TF algorithms.

</details>
