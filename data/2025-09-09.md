<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]
- [cs.PF](#cs.PF) [Total: 4]
- [cs.LG](#cs.LG) [Total: 107]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device](https://arxiv.org/abs/2509.05451)
*Niansong Zhang,Wenbo Zhu,Courtney Golden,Dan Ilan,Hongzheng Chen,Christopher Batten,Zhiru Zhang*

Main category: cs.AR

TL;DR: 计算在SRAM中的商业设备GSI APU在现实工作负荷下表现优异，通过优化策略在RAG应用中实现了比CPU更快4.8-6.6倍的检索速度，且比GPU更节能54.4-117.9倍


<details>
  <summary>Details</summary>
Motivation: 以往对计算在SRAM架构的评估主要依赖仿真器或小规模原型，限制了对其真实潜力的理解。本研究通过对商业设备的实际测试，评估计算在SRAM在现实应用中的性能和能消缓潜力

Method: 对商业计算在SRAM设备GSI APU进行综合性能和能消测试，与CPU、GPU进行对比。提出分析框架建模性能交换关系，并提出三项优化策略：通信感知的缩减映射、聚合DMA和广播友好的数据布局

Result: 在10GB-200GB大规模数据集上，优化后的计算在SRAM系统实现了比优化CPU基准4.8-6.6倍的检索加速，结合RAG延迟提升1.1-1.8倍。性能与NVIDIA A6000 GPU相当，但能效高出54.4-117.9倍

Conclusion: 计算在SRAM技术在复杂实际应用中具有可行性，通过适当的优化策略可以充分发挥其性能和能效优势，为该技术的进一步发展提供了指导

Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.

</details>


### [2] [High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator](https://arxiv.org/abs/2509.05688)
*Kuan-Ting Lin,Ching-Te Chiu,Jheng-Yi Chang,Shi-Zong Huang,Yu-Ting Li*

Main category: cs.AR

TL;DR: 这篇论文提出了一种高利用率的能消消识深度卷积神经网络加速器，通过数据重用策略和芯片内计算减少数据交换，实现了显著的性能提升和能消消改善。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络在边缘设处上的推理计算复杂度和数据访问量过大，导致推理延迟过长，无法满足实际应用的实时性要求。

Method: 1) 使用1x1卷积核作为计算单元基础 2) 设计适合的计算单元 3) 使用Reuse Feature SRAM存储层输出作为下一层输入 4) 采用输出重用策略和环形流数据流减少芯片与DRAM的数据交换 5) 实现芯片内直接池化计算模块

Result: 加速芯片具有极高硬件利用率，在ECNN模块上减少533倍数据传输量，能够实时执行VGG16和MobileNet模型，相比VWA设计速度提升7.52倍，能效提升1.92倍。

Conclusion: 该加速器设计通过数据重用策略和芯片内计算有效减少了数据交换开销，显著提升了边缘设处上深度卷积神经网络的推理性能和能效，为实际应用提供了可行的解决方案。

Abstract: Deep convolution Neural Network (DCNN) has been widely used in computer
vision tasks. However, for edge devices even inference has too large
computational complexity and data access amount. The inference latency of
state-of-the-art models are impractical for real-world applications. In this
paper, we propose a high utilization energy-aware real-time inference deep
convolutional neural network accelerator, which improves the performance of the
current accelerators. First, we use the 1x1 size convolution kernel as the
smallest unit of the computing unit. Then we design suitable computing unit
based on the requirements of each model. Secondly, we use Reuse Feature SRAM to
store the output of the current layer in the chip and use the value as the
input of the next layer. Moreover, we import Output Reuse Strategy and Ring
Stream Dataflow to reduce the amount of data exchange between chips and DRAM.
Finally, we present On-fly Pooling Module to let the calculation of the Pooling
layer directly complete in the chip. With the aid of the proposed method, the
implemented acceleration chip has an extremely high hardware utilization rate.
We reduce a generous amount of data transfer on the specific module, ECNN.
Compared to the methods without reuse strategy, we can reduce 533 times of data
access amount. At the same time, we have enough computing power to perform
real-time execution of the existing image classification model, VGG16 and
MobileNet. Compared with the design in VWA, we can speed up 7.52 times and have
1.92x energy efficiency

</details>


### [3] [Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems](https://arxiv.org/abs/2509.05937)
*Wei-Hsing Huang,Jianwei Jia,Yuyao Kong,Faaiq Waqar,Tai-Hao Wen,Meng-Fan Chang,Shimeng Yu*

Main category: cs.AR

TL;DR: 本文提出了一种KAN网络的算法-硬件协同设计方法，通过硬件感知量化和稀疏映射策略，结合模拟存内计算电路，在22nm工艺下实现了大规模KAN网络的高效能效比加速。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KAN) 虽然参数效率高，但其B样条函数组件在硬件加速方面存在复杂性，传统查找表方法需要大量电路资源，因此需要专门的硬件加速方案。

Method: 采用算法-硬件协同设计：算法层面使用Alignment-Symmetry和PowerGap硬件感知量化、KAN稀疏映射策略；电路层面采用N:1时间调制动态电压输入生成器和模拟存内计算(ACIM)电路。

Result: 在22nm工艺下，相比之前的小规模任务，大规模任务的参数数量增加500Kx-807Kx，但面积开销仅增加28Kx-41Kx，功耗增加51x-94x，精度损失仅为0.11%-0.23%。

Conclusion: 提出的协同设计方法有效解决了KAN硬件加速的挑战，展示了优异的可扩展性，为大规模KAN网络的高效能效比实现提供了可行方案。

Abstract: Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an
innovative architectural paradigm capable of replicating conventional deep
neural network (DNN) capabilities while utilizing significantly reduced
parameter counts through the employment of parameterized B-spline functions
with trainable coefficients. Nevertheless, the B-spline functional components
inherent to KAN architectures introduce distinct hardware acceleration
complexities. While B-spline function evaluation can be accomplished through
look-up table (LUT) implementations that directly encode functional mappings,
thus minimizing computational overhead, such approaches continue to demand
considerable circuit infrastructure, including LUTs, multiplexers, decoders,
and related components. This work presents an algorithm-hardware co-design
approach for KAN acceleration. At the algorithmic level, techniques include
Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity
aware mapping strategy, and circuit-level techniques include N:1 Time
Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)
circuits. This work conducts evaluations on large-scale KAN networks to
validate the proposed methodologies. Non-ideality factors, including partial
sum deviations from process variations, have been evaluated with statistics
measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally
determined KAN hyperparameters in conjunction with circuit optimizations
fabricated at the 22nm technology node, despite the parameter count for
large-scale tasks in this work increasing by 500Kx to 807Kx compared to
tiny-scale tasks in previous work, the area overhead increases by only 28Kx to
41Kx, with power consumption rising by merely 51x to 94x, while accuracy
degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling
potential of our proposed architecture.

</details>


### [4] [SCREME: A Scalable Framework for Resilient Memory Design](https://arxiv.org/abs/2509.06101)
*Fan Li,Mimi Xie,Yanan Guo,Huize Li,Xin Xin*

Main category: cs.AR

TL;DR: SCREME框架利用低成本低性能芯片提供额外内存空间，通过重新配置ECC芯片带宽和利用未充分利用的I/O资源来满足内存可靠性需求


<details>
  <summary>Details</summary>
Motivation: 内存技术进步带来了性能提升，但也加剧了可靠性挑战。传统ECC方案假设奇偶校验数据存储空间昂贵且不可扩展，需要新的解决方案

Method: 提出SCREME框架：1）识别ECC芯片不需要与数据芯片相同的性能水平；2）利用低性能芯片降低成本；3）重新配置服务器内存芯片未充分利用的I/O资源；4）建立灵活的DIMM内部连接

Result: 能够以成本效益方式提供额外的内存空间用于弹性内存设计，降低ECC芯片成本，同时满足不断增长的可靠性需求

Conclusion: SCREME打破了传统ECC方案的刻板印象，提供了一种可扩展的、成本效益高的内存可靠性解决方案，利用技术演进中自然产生的低成本芯片

Abstract: The continuing advancement of memory technology has not only fueled a surge
in performance, but also substantially exacerbate reliability challenges.
Traditional solutions have primarily focused on improving the efficiency of
protection schemes, i.e., Error Correction Codes (ECC), under the assumption
that allocating additional memory space for parity data is always expensive and
therefore not a scalable solution.
  We break the stereotype by proposing an orthogonal approach that provides
additional, cost-effective memory space for resilient memory design. In
particular, we recognize that ECC chips (used for parity storage) do not
necessarily require the same performance level as regular data chips. This
offers two-fold benefits: First, the bandwidth originally provisioned for a
regular-performance ECC chip can instead be used to accommodate multiple
low-performance chips. Second, the cost of ECC chips can be effectively
reduced, as lower performance often correlates with lower expense. In addition,
we observe that server-class memory chips are often provisioned with ample, yet
underutilized I/O resources. This further offers the opportunity to repurpose
these resources to enable flexible on-DIMM interconnections. Based on the above
two insights, we finally propose SCREME, a scalable memory framework leverages
cost-effective, albeit slower, chips -- naturally produced during rapid
technology evolution -- to meet the growing reliability demands driven by this
evolution.

</details>


### [5] [Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects](https://arxiv.org/abs/2509.06365)
*Omar Al Habsi,Safa Mohammed Sali,Anis Meribout,Mahmoud Meribout,Saif Almazrouei,Mohamed Seghier*

Main category: cs.AR

TL;DR: 本文综述了可移动MRI系统硬件加速技术的最新进展，提出通过GPU、FPGA、ASIC等硬件加速技术提升图像重建速度和降低功耗，并建议成立低场MRI联盟和标准化测试框架以促进AI重建技术发展。


<details>
  <summary>Details</summary>
Motivation: 可移动MRI系统在远程和资源受限环境中具有重要价值，但图像重建和机器学习算法的计算复杂性构成了重要挑战，而现有研究对硬件加速方面关注不够。

Method: 通过综述性评论方法，评估GPU、FPGA、ASIC等硬件加速技术在pMRI图像金收集和重建中的性能表现、功耗效果和应用潜力。

Result: 硬件加速技术能够显著提升pMRI图像重建速度，降低功耗，并为AI驱动的重建算法提供高效计算支持。同时提出了低场MRI数据集和标准化测试框架的需求。

Conclusion: 硬件加速技术是提升下一代可移动MRI技术性能的关键，能够改善图像质量、减少功耗并增强系统可移动性。建立标准化的数据集和测试框架将有助于促进AI重建技术的可复现发展和相关规范制定。

Abstract: There is a growing interest in portable MRI (pMRI) systems for point-of-care
imaging, particularly in remote or resource-constrained environments. However,
the computational complexity of pMRI, especially in image reconstruction and
machine learning (ML) algorithms for enhanced imaging, presents significant
challenges. Such challenges can be potentially addressed by harnessing hardware
application solutions, though there is little focus in the current pMRI
literature on hardware acceleration. This paper bridges that gap by reviewing
recent developments in pMRI, emphasizing the role and impact of hardware
acceleration to speed up image acquisition and reconstruction. Key technologies
such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays
(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent
performance in terms of reconstruction speed and power consumption. This review
also highlights the promise of AI-powered reconstruction, open low-field pMRI
datasets, and innovative edge-based hardware solutions for the future of pMRI
technology. Overall, hardware acceleration can enhance image quality, reduce
power consumption, and increase portability for next-generation pMRI
technology. To accelerate reproducible AI for portable MRI, we propose forming
a Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,
retrospective multi-center testing, prospective reader and non-inferiority
trials) to provide standardized datasets, benchmarks, and regulator-ready
testbeds.

</details>


### [6] [VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing](https://arxiv.org/abs/2509.06698)
*Leidy Mabel Alvero-Gonzalez,Matias Miguez,Eric Gutierrez,Juan Sapriza,Susana Patón,David Atienza,José Miranda*

Main category: cs.AR

TL;DR: VCO-CARE是一种基于压控振荡器的模拟读出系统，专为连续EDA传感设计，具有高灵敏度(40pS)、低功耗(2.3μW)和低噪声(0.8μVrms)的特点。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备中电活动(EDA)连续监测对模拟前端系统的高灵敏度、低功耗和最小校准要求的挑战。

Method: 开发了基于压控振荡器(VCO)的模拟读出系统VCO-CARE，通过后布局验证进行性能评估。

Result: 系统在0-20μS范围内达到40pS的平均灵敏度，固定电阻相对误差小于0.0025%，功耗仅2.3μW，在0-1.5Hz EDA信号频带内噪声仅为0.8μVrms。

Conclusion: 该研究推动了具有无缝适应性、最小功耗和出色噪声抗扰度的可穿戴传感器的发展。

Abstract: Continuous monitoring of electrodermal activity (EDA) through wearable
devices has attracted much attention in recent times. However, the persistent
challenge demands analog front-end (AFE) systems with high sensitivity, low
power consumption, and minimal calibration requirements to ensure practical
usability in wearable technologies. In response to this challenge, this
research introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog
Readout tailored for continuous EDA sensing. The results show that our system
achieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS
range and a negligible relative error of less than 0.0025% for
fixed-resistance. Furthermore, the proposed system consumes only an average of
2.3 uW based on post-layout validations and introduces a low noise
contribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.
This research aims to drive the evolution of wearable sensors characterized by
seamless adaptability to diverse users, minimal power consumption, and
outstanding noise resilience.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [7] [Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs](https://arxiv.org/abs/2509.05365)
*Dingcui Yu,Yunpeng Song,Yiyang Huang,Yumiao Zhao,Yina Lv,Chundong Wang,Youtao Zhang,Liang Shi*

Main category: cs.PF

TL;DR: Waltz是一种温度感知的协同压缩方法，通过在主机和设备端调度压缩任务来平衡性能和温度控制，避免SSD过热和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有SSD压缩方案存在温度过高（设备端压缩）或性能下降（主机端压缩）的问题，需要一种能同时保证高性能和温度控制的解决方案。

Method: 提出Waltz温度感知协同压缩方法，通过监控设备温度来调度主机和设备端的压缩任务，并设计了Waltzs（空间优化）和Waltzp（性能优化）两个变体。

Result: Waltz在F2FS中实现，实现了高性能的同时延长了SSD寿命并防止过热导致的意外关机。

Conclusion: Waltz通过协同压缩调度有效解决了SSD压缩中的温度控制和性能平衡问题，为现代存储系统提供了实用的解决方案。

Abstract: Data compression is widely adopted for modern solid-state drives (SSDs) to
mitigate both storage capacity and SSD lifetime issues. Researchers have
proposed compression schemes at different system layers, including device-side
solutions like CCSDs ( c ompression-based c omputational SSDs) and compression
supported by host-side, like F2FS (flash-friendly file system). We conduct
quantitative studies to understand how host-side and device-side compression
schemes affect the temperature and performance of SSD-based storage systems.
From our experiments, device-side compression, facilitated by a hardware
compression engine, can raise the temperature of CCSDs to intolerable levels,
resulting in throttling and service shutdown. In contrast, host-side
compression causes software-stack overhead, which often results in large
performance degradation and resource consumption. To ensure efficient data
compression with high performance and better temperature control, we propose
Waltz, a temperature-aware cooperative compression method that schedules
(de)compression tasks at the host and device sides by monitoring device
temperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for
space and performance optimization, respectively. Waltz is implemented within
F2FS, achieving high performance while extending SSD lifetime and preventing
overheating-induced in-flight shutdowns.

</details>


### [8] [Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology](https://arxiv.org/abs/2509.05511)
*Dhanya R Mathews,Mudit Verma,Pooja Aggarwal,J. Lakshmi*

Main category: cs.PF

TL;DR: 这篇论文提出了一种基于服务拓扑结构的方法，通过选择最具信息值的监控指标来提高异常根因检测的准确性和效率，从而增强云应用服务的弹性能力。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的普及导致服务组件数量大幅增加，产生的可观测数据量巨大，给实时异常响应和服务质量恢复带来了重大挑战。需要一种自主的方法来识别关键监控指标并准确定位异常根因。

Method: 提出了一种新方法，考虑应用服务拓扑结构来选择最具信息值的质量指标，支持高效、可解释和准确的根因识别。进而提出Topology-Aware-RCD (TA-RCD)算法，将端到端服务拓扑结构结合到RCD中。

Result: 故障注入研究的评估显示，提出的方法在Top-3和Top-5回归率方面平均至少比当前最先进的RCD算法好2倍以上。

Conclusion: 通过考虑服务拓扑结构来选择关键监控指标，可以显著提高异常根因检测的准确性和效率，从而有效增强云应用服务的弹性能力。

Abstract: Cloud application services are distributed in nature and have components
across the stack working together to deliver the experience to end users. The
wide adoption of microservice architecture exacerbates failure management due
to increased service components. To be effective, the strategies to enhance the
application service resilience need to be autonomous and developed at the
service's granularity, considering its end-to-end components. However, the
massive amount of observability data generated by all these components across
the service stack poses a significant challenge in reacting to anomalies and
restoring the service quality in real time. Identifying the most informative
observability data from across the cloud service stack and timely localization
of root causes of anomalies thus becomes crucial to ensure service resilience.
This article presents a novel approach that considers the application service
topology to select the most informative metrics across the cloud stack to
support efficient, explainable, and accurate root cause identifications in case
of performance anomalies. The usefulness of the selected metrics is then
evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for
localizing the root cause of performance anomalies. As a step towards improving
the accuracy and efficiency of RCD, this article then proposes the
Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application
service topology in RCD. The evaluation of the failure injection studies shows
that the proposed approach performs at least 2X times better on average than
the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.

</details>


### [9] [Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach](https://arxiv.org/abs/2509.05790)
*Hai Dinh-Tuan,Franz Florian Six*

Main category: cs.PF

TL;DR: 这篇论文提出了一种基于服务亲和性图的方法（SAGA），通过形成最小权重k切问题并使用近似算法进行服务分组，在Kubernetes集群上实现了平均延迟提升23.40%的效果。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务架构虽然高效，但在动态分布式服务中维护端到端服务质量面临复杂挑战，需要一种新方法来解决服务优化问题。

Method: 提出服务亲和性图基础方法（SAGA），使用图论模型建模微服务间交互，将服务部署形式化为最小权重k切问题，并采用近似算法进行服务聚类。

Result: 在Kubernetes集群上部署原型进行实验评估，结果显示平均延迟提升23.40%，验证了方法的有效性。

Conclusion: 该方法通过服务亲和性图模型有效解决了微服务架构中的服务优化问题，在应用性能、数据隐私和运营成本等多个目标上都取得了显著效果，为云原生系统提供了一种综合性解决方案。

Abstract: Modern software architectures are characterized by their cloud-native,
modular, and microservice-based designs. While these systems are known for
their efficiency, they also face complex challenges in service optimization,
especially in maintaining end-to-end quality of service across dynamically
distributed services. This paper introduces a novel approach using the concept
of Service Affinity to address this challenge. The proposed method, termed
Service Affinity Graph-based Approach, employs a graph-based model to model the
interactions among microservices. It formulates the service placement as a
minimum-weight k-cut problem and utilizes an approximation algorithm for
service clustering. This approach is realized through a conceptual framework
that takes into account a wide range of optimization objectives, ranging from
enhancing application performance and enforcing data privacy to optimizing
operational costs. In addition to presenting the SAGA framework in details,
this paper conducts an in-depth empirical evaluation using a prototype deployed
on a Kubernetes cluster. The results demonstrate a mean latency improvement of
23.40%, validating the effectiveness of our approach. Finally, the paper
comprehensively discusses various aspects of the proposed methods, including
their implications, challenges, and benefits, providing a thorough analysis of
the approach's impact.

</details>


### [10] [Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing](https://arxiv.org/abs/2509.05794)
*Hai Dinh-Tuan,Jialun Jiang*

Main category: cs.PF

TL;DR: 这篇论文提出了一种优化的状态微服务迁移方案，通过结合MS2M框架和Kubernetes FCC特性，实现了过渡时间减少96.986%的效果。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的普及导致对分布式服务管理的需求增加，但现有容器策列器（如Kubernetes）缺乏对状态服务活迁移的原生支持。

Method: 整合Message-based Stateful Microservice Migration (MS2M)框架与Kubernetes Forensic Container Checkpointing (FCC)特性，增加对StatefulSet管理Pod的迁移支持，并引入阈值基于切断机制处理高消息速率。

Result: MS2M对单个Pod的迁移过渡时间比冷迁移方法减少96.986%，StatefulSet方法为状态服务管理提供更大灵活性。

Conclusion: 该研究为云原生环境中状态微服务迁移优化提供了实用策略和技术见解。

Abstract: The widespread adoption of microservices architecture in modern software
systems has emphasized the need for efficient management of distributed
services. While stateless microservices enable straightforward migration,
stateful microservices introduce added complexity due to the need to preserve
in-memory state during migration. However, most container orchestrators,
including Kubernetes, lack native support for live stateful service migration.
This paper proposes an optimized migration scheme for stateful services in
Kubernetes by integrating the Message-based Stateful Microservice Migration
(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)
feature. Key enhancements include support for migrating StatefulSet-managed
Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high
incoming message rates. Evaluation results demonstrate that MS2M for individual
Pods reduces downtime by 96.986% compared to cold migration methods, while the
StatefulSet approach provides greater flexibility in managing stateful
services. These insights provide practical strategies for optimizing stateful
microservice migration in cloud-native environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文系统评估了LLM遗忘学习中的常见实践，发现单一邻居集和标准采样方法存在问题，提出了包含多样邻居集和模块化实体级遗忘策略的最佳实践方案。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM遗忘学习基准通常只使用单一邻居集和通用知识集，无法反映真实世界数据的复杂性，且标准采样方法的有效性和稳定性未经严格检验。

Method: 系统评估现有常见实践，提出并验证最佳实践：1）引入多样邻居集平衡遗忘效果和模型效用；2）证明标准1:1采样方法效率低下；3）提出模块化实体级遗忘策略替代循环采样。

Result: 研究发现单一邻居集效果不佳，标准采样方法会掩盖性能权衡。模块化方法结合鲁棒算法为有效遗忘提供了清晰稳定的路径。

Conclusion: 需要采用包含多样邻居集的更全面基准，并使用模块化实体级遗忘策略来改进LLM遗忘学习的有效性和稳定性。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [12] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 基于空间-时间图卷积网络(ST-GCN)的统一框架，用于快速准确预测大型序列电路中长周期故障影响概率，减少模拟时间10倍以上仍保持高精度。


<details>
  <summary>Details</summary>
Motivation: 沉默数据错误(SDEs)由于时间零缺陷和老化导致，会降低安全关键系统的可靠性。功能测试可以检测SDE相关故障，但模拟成本较高。

Method: 将门级网表建模为空间-时间图，以抓取拓扑结构和信号时序。使用专门的空间和时间编码器来高效预测多周期故障影响概率。

Result: 在ISCAS-89测试集上，方法减少模拟时间10倍以上，同时保持高精度(5周期预测的均方误差为0.024)。通过预测FIPs选择观测点，提高了长周期难检测故障的检测率。

Conclusion: 该方法能够支持量化风险评估，适用于SoC级别的测试策略优化，并能够雅入下游的电子设计自动化流程。

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [13] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的正则化方法，通过在函数空间中约束微调模型与预训练模型的距离，并使用模拟OOD样本来保持预训练模型的OOD鲁棒性，同时引入一致性正则化来增强微调模型的OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒微调方法通过保留预训练权重、特征或logits来保持OOD鲁棒性，但这些方法不能始终提高不同模型架构的OOD鲁棒性，因为它们无法很好地代理函数空间中的优化。

Method: 提出在函数空间中约束微调模型与预训练模型距离的正则化方法，使用模拟OOD样本；并引入一致性正则化来促进扰动样本的稳定预测。

Result: 大量实验表明，该方法在各种CLIP骨干网络上都能一致地提高下游任务的ID微调性能和OOD鲁棒性，优于现有的基于正则化的鲁棒微调方法。

Conclusion: 通过在函数空间中直接约束模型距离并使用一致性正则化，可以有效地保持预训练模型的OOD鲁棒性，同时提高下游任务的性能。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [14] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent是一个基于LLM的多智能体系统，通过分析静态指标和动态性能信号，为不同神经网络架构自动设计剪枝和量化策略，在保持精度的同时显著减少内存使用和提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 基础模型面临计算和内存瓶颈，传统压缩技术使用统一启发式方法，忽略了架构和运行时异质性。现有的性能分析工具很少集成到自动化流程中，需要更智能的压缩方法。

Method: 使用大语言模型构建多智能体系统，分析MACs、参数量等静态指标和延迟、内存等动态信号，为特定架构设计结构化剪枝和训练后动态量化策略。

Result: 在多个数据集和模型上验证：剪枝保持或提升精度（ImageNet-1K精度下降约1%，ViT-B/16在小数据集上提升2%）；量化实现74%内存节省且精度损失<0.5%，推理速度提升1.74倍。

Conclusion: 智能体系统是可扩展的性能分析引导模型优化解决方案，LLM推理质量对迭代剪枝至关重要，该方法能有效解决资源受限平台的部署挑战。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [15] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 本文研究了图神经网络(GNNs)中的拓扑隐私风险，提出了拓扑推断攻击(TIAs)并发现现有边缘级差分隐私机制不足，进而设计了Private Graph Reconstruction (PGR)防御框架来保护拓扑隐私同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: GNNs在图结构数据学习中表现出强大能力，但其广泛应用引发了严重的隐私问题。现有研究主要关注边缘级隐私，而拓扑隐私（图整体结构的机密性）这一关键威胁尚未得到充分探索。

Method: 提出了拓扑推断攻击(TIAs)套件，能够仅通过黑盒访问GNN模型来重建目标训练图的结构。为防御此类攻击，设计了Private Graph Reconstruction (PGR)框架，采用双层优化问题形式，通过元梯度迭代生成合成训练图，并基于演化图同时更新GNN模型。

Result: 研究发现GNNs对这些攻击高度脆弱，现有边缘级差分隐私机制要么无法缓解风险，要么严重损害模型精度。PGR框架显著减少了拓扑泄露，同时对模型精度影响最小。

Conclusion: 拓扑隐私是GNNs中一个被忽视但重要的安全威胁，需要专门的防御机制。PGR框架为保护图结构隐私提供了一种有效解决方案，在保持模型性能的同时增强了隐私保护。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [16] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: memTrace框架通过分析LLM内部表示而非仅输出，发现传统损失函数方法可能遗漏的记忆指纹，在MIA基准测试中达到0.85平均AUC分数，表明即使输出信号看似安全，内部行为仍可能泄露训练数据信息。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明成员推理攻击对大型语言模型效果仅略优于随机猜测，认为现代预训练方法可能不存在隐私泄露风险。本研究旨在探索通过分析模型内部表示而非仅输出来发现潜在的成员推理信号。

Method: 提出memTrace框架，通过分析transformer隐藏状态和注意力模式在处理候选序列时的层间表示动态、注意力分布特征和跨层转换模式，追踪"神经痕迹"来检测传统损失函数方法可能无法捕捉的记忆指纹。

Result: 在多个模型家族上实现了强大的成员检测能力，在流行的MIA基准测试中达到平均0.85的AUC分数，显著优于传统方法。

Conclusion: 即使基于输出的信号看似受到保护，模型内部行为仍能揭示训练数据暴露的方面，强调需要进一步研究成员隐私问题，并开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [17] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify提出基于上下文多臂老虎机的校准方法，动态学习用户在不同情境下的最优内容类型分布，解决历史数据偏向音乐内容的问题，提升用户参与度和非音乐内容的曝光。


<details>
  <summary>Details</summary>
Motivation: Spotify首页包含多种内容类型（音乐、播客、有声书），但历史数据严重偏向音乐内容，难以提供平衡且个性化的内容组合。用户对不同内容类型的偏好会随时间、星期和设备等情境因素变化。

Method: 提出基于上下文多臂老虎机的校准方法，动态学习每个用户在不同情境下的最优内容类型分布，而不是依赖历史平均值。

Result: 离线和在线实验结果都显示，该方法提高了Spotify首页的推荐精度和用户参与度，特别是对播客等代表性不足的内容类型有显著改善。

Conclusion: 上下文感知的校准方法能够有效适应用户兴趣在不同情境下的变化，提升内容推荐的平衡性和个性化效果，特别有利于非音乐内容的推广。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [18] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS是一个周期性感知的自监督学习框架，通过多粒度分块机制和对比损失来建模多元时间序列的潜在状态和转换，在多个下游任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列具有高维度、标注数据有限和非平稳性等挑战，现有自监督学习方法忽略了内在周期性结构，无法捕捉潜在状态的动态演化。

Method: 提出周期性感知的多粒度分块机制和广义对比损失，设计下一个转换预测的预训练任务来编码未来状态演化的预测信息。

Result: PLanTS在多分类、多标签分类、预测、轨迹跟踪和异常检测等任务中持续提升表示质量，相比基于DTW的方法具有更好的运行时效率。

Conclusion: PLanTS通过显式建模周期性结构和状态转换，有效解决了多元时间序列表示学习的挑战，为相关领域提供了有效的自监督学习解决方案。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [19] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 利用信号时序逻辑(STL)规范来训练生物分子神经网络(BNNs)，解决生物系统中缺乏目标数据的训练挑战，实现回归和控制任务


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络具有通用函数逼近能力，但由于缺乏目标数据，训练面临挑战。需要一种方法来定义训练目标，使BNNs能够在生物系统中执行复杂任务

Method: 基于STL的定量语义，开发梯度优化算法来训练BNN权重。使用STL规范定义训练目标，支持回归和控制任务，包括疾病状态报告和慢性病模型的反馈控制

Result: 数值实验表明，基于STL的学习方法能够高效解决所研究的回归和控制任务，BNNs成功作为失调状态报告器，并在慢性疾病模型中减少炎症同时避免对外部感染的不良反应

Conclusion: STL规范为BNNs提供了一种有效的训练框架，解决了生物系统中缺乏目标数据的问题，使BNNs能够在复杂生物环境中执行回归和控制功能

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [20] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 提出一种基于参数化模板的近似计算方法，通过优化算术运算电路来降低神经网络在边缘设备上的能耗和面积，同时保持可接受的精度损失


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署神经网络需要在推理能耗和分类精度之间取得平衡，近似计算技术通过略微降低算术运算精度来减少能耗消耗

Method: 改进布尔重写技术XPAT，提出基于参数化乘积共享的新型模板，该模板可以作为合成面积的近似代理，通过优化模板参数来寻找低面积解决方案

Result: 实验证明该方法比原始XPAT和其他两种最先进方法能更好地收敛到低面积解决方案，并找到更好的近似电路

Conclusion: 所提出的参数化模板方法在减少神经网络算术运算电路面积方面表现出色，为边缘设备上的高效神经网络部署提供了有效的近似计算解决方案

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [21] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 这篇论文探索了训练数据分布对图像分类模型性能的影响，提出了一种无需重新训练的框架，通过分析嵌入空间中的距离来预测模型对未见数据的预测信心度，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决模型在遇到距离训练数据分布的未见数据时预测不可靠的问题，提高分类模型在实际应用中的可靠性。

Method: 通过分析训练数据在嵌入空间中的分布，根据测试数据与训练分布的距离来评估预测信心度，过滤低信心度的预测。使用多个嵌入模型来获得更稳健的信心度估计。

Result: 在多个分类模型上都实现了一致的性能提升，显著提高了分类准确性。多嵌入模型的结合进一步改善了距离分布样本的检测能力，带来了更大的准确性提升。

Conclusion: 该方法具有模型无关性和普遍性，不仅适用于计算机视觉领域，还有潜力应用于自然语言处理等需要预测可靠性的领域。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [22] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 首个在资源受限微控制器上部署Mamba模型的方案MambaLite-Micro，通过C语言实现、算子融合和内存优化，减少83%峰值内存，保持与PyTorch一致的精度。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在微控制器部署面临内存限制、缺乏原生算子支持和嵌入式友好工具链等挑战，需要开发轻量级推理引擎。

Method: 将PyTorch Mamba模型导出为轻量格式，用C语言手工实现Mamba层和支持算子，采用算子融合和内存布局优化技术。

Result: 减少83.0%峰值内存，数值误差仅1.7x10-5，在关键词检测和人体活动识别任务上保持100%精度一致性，在ESP32S3和STM32H7上验证可移植性。

Conclusion: MambaLite-Micro成功将先进序列模型部署到资源受限嵌入式平台，为实际应用铺平道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [23] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: SAR是一种自对齐奖励信号，通过相对困惑度差异来鼓励推理准确性和效率，在保持准确性的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励信号过于粗糙（只有二元正确性反馈），导致推理过程冗长、计算成本高，且现有解决方案往往牺牲准确性

Method: 引入自对齐奖励(SAR)，定义为基于查询的答案与独立答案之间的相对困惑度差异，偏好简洁且针对查询的响应

Result: 在7个基准测试的4个模型上，SAR与PPO和GRPO等RL算法结合，准确率提高4%，推理成本降低30%，实现了正确性和效率的帕累托最优权衡

Conclusion: SAR作为可验证奖励的细粒度补充，能够在不丢失关键推理的情况下抑制不必要的阐述，为更高效有效的LLM训练铺平道路

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [24] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 这篇论文提出了一种集成数据并行和全解耦并行反向传播算法的分布式训练方法，用于加速深度神经网络的训练过程。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练需要大量时间，特别是当网络深度增加和数据集扩大时，需要找到更高效的训练方法来缩短训练时间。

Method: 采用多个计算单元并行运行，结合数据并行和全解耦并行反向传播算法，增加每迭代处理的训练数据量，并减少反向传播算法中的锁定问题。

Result: 方法在某些条件下被严格证明能够收敛到关键点，并通过在CIFAR-10数据集上进行分类任务的实验评估验证了其有效性。

Conclusion: 该分布式训练方法通过并行计算和解耦算法，显著提高了深度神经网络训练的效率，为解决大规模DNN训练的时间挑战提供了有效方案。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [25] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一个通过双层优化自适应调整训练样本权重的多模态过程奖励模型框架，在MMMU基准测试上达到84.6%准确率，超越GPT-5


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型训练面临分布偏移和噪声数据的挑战，需要开发能够自适应处理这些问题的方法

Method: 提出基于双层优化的实例重加权框架，包含Instance Table（适用于小数据集）和Instance Net（适用于大数据集）两种互补策略，并集成到测试时扩展中

Result: 在MMMU基准测试上达到84.6%的准确率，性能超过GPT-5

Conclusion: DreamPRM-1.5通过自适应实例重加权有效解决了多模态过程奖励模型训练中的分布偏移和噪声问题，取得了state-of-the-art的性能

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [26] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: Persona是一个无需反向传播的原型参数编辑框架，通过云端神经适配器生成参数编辑矩阵，动态调整设备端模型以适应实时数据分布变化，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决设备端实时数据分布变化对轻量级模型泛化能力的挑战，避免传统方法需要大量数据和计算资源进行微调的问题。

Method: 使用原型基础的参数编辑框架，通过云端神经适配器生成参数编辑矩阵，动态聚类为原型模型，并利用跨层知识转移实现多层参数一致性调整。

Result: 在视觉任务和推荐任务的多个数据集上验证了Persona的有效性和通用性。

Conclusion: Persona提供了一种高效的后部署模型适应方法，能够显著提升设备端模型在实时数据分布变化下的泛化性能。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [27] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: RLA框架通过低层目标策略和高层预期模型解决长时域目标导向任务，使用价值几何一致性原则训练，具有理论收敛保证


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中层次结构自动发现和多级策略联合训练的不稳定性问题，缺乏理论保证的局限性

Method: 学习两个协同模型：低层目标条件策略（到达子目标）和高层预期模型（作为规划器提出中间子目标），通过价值几何一致性原则训练预期模型

Result: RLA在各种条件下接近全局最优策略，为长时域目标导向任务提供了原则性和收敛的分层规划与执行方法

Conclusion: RLA是一个有原则且可扩展的框架，通过理论证明的收敛性解决了分层强化学习的稳定性和理论保证问题

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [28] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 提出一个因果推理框架来解决医学多模态学习中的缺失模态偏差问题，通过结构因果分析和双分支网络来消除缺失性偏差和分布偏差


<details>
  <summary>Details</summary>
Motivation: 现实医学数据集中常存在模态缺失问题，现有方法主要从原始数据或特征空间学习，但忽略了数据采集过程本身引入的潜在偏差，阻碍了模型的泛化能力

Method: 通过结构因果分析数据生成过程，包含两个关键组件：(1)基于后门调整的缺失性去混淆模块近似因果干预，(2)双分支神经网络显式解耦因果特征和伪相关

Result: 在真实世界的公共和院内数据集上验证了方法的有效性和因果洞察力

Conclusion: 提出的统一框架能够有效处理医学多模态表示学习中的缺失模态偏差问题，提供因果解释并提升模型泛化性能

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [29] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: OptiProxy-NAS是一个端到端的神经架构搜索优化框架，通过代理表示将离散的NAS空间转化为连续、可微分且平滑的空间，从而可以使用梯度优化方法进行架构搜索。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)是一个计算昂贵的离散优化问题，搜索空间巨大且不平滑。现有方法如基于预测器的方法和可微分架构搜索存在局限性，需要更高效的优化框架。

Method: 提出OptiProxy-NAS，使用代理表示将NAS空间重新表述为连续、可微分且平滑的空间，使任何可微分优化方法都能应用于梯度搜索。

Result: 在12个NAS任务、4个搜索空间和三个不同领域（计算机视觉、自然语言处理、资源受限NAS）的全面实验中表现出优越的搜索结果和效率，在低保真度场景中也验证了灵活性。

Conclusion: OptiProxy-NAS提供了一个有效的端到端优化框架，能够显著提高神经架构搜索的效率和性能，并在多个领域和场景中展现出优越性。

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [30] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 本文提出了一种基于主动学习的无监督时间序列异常检测方法，通过DQS查询策略选择多样化样本进行标注，优化阈值选择，在小样本场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有无监督时间序列异常检测方法存在阈值设置不佳的问题，而一些声称无监督的方法仍需使用标注数据进行校准，这在现实世界中往往不可行。

Method: 将主动学习集成到现有无监督异常检测方法中，通过提出的DQS（基于差异的查询策略）选择多元时间序列标签进行查询，使用动态时间规整评估异常得分相似性以最大化查询样本多样性。

Result: DQS在小预算场景下表现最佳，其他策略在面对错误标注时更稳健。所有查询策略在存在错误标注的情况下都优于无监督阈值。

Conclusion: 在实际应用中，查询策略的选择取决于标注专家的专业知识和愿意标注的样本数量。只要有可能查询专家，建议采用基于主动学习的阈值方法。

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [31] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 提出了GraMFedDHAR框架，使用图卷积网络处理多模态传感器数据，在联邦学习中通过注意力融合和差分隐私保护，显著提升了人类活动识别的准确性和隐私保护性能


<details>
  <summary>Details</summary>
Motivation: 解决多模态传感器数据中的噪声、标签稀缺、隐私问题，以及传统联邦学习在处理异构多模态数据和差分隐私要求时的挑战

Method: 将不同传感器数据建模为模态特定图，使用残差图卷积网络处理，通过注意力加权融合而非简单拼接，在联邦学习框架中加入差分隐私保护

Result: 在非差分隐私设置下准确率提升2%，在差分隐私约束下性能优势更显著（7-13%提升），图神经网络对隐私噪声更具鲁棒性

Conclusion: 基于图的建模在多模态学习中表现出色，GNN对差分隐私噪声的性能衰减更具抵抗力，为隐私保护的联邦多模态学习提供了有效解决方案

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [32] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 本文提出使用凸凹过程(CCP)来训练具有竞争层的形态感知器(MPCL)网络，解决了形态算子不可微分导致的梯度优化方法不适用的问题。


<details>
  <summary>Details</summary>
Motivation: 形态感知器中的形态算子具有不可微分性，使得基于梯度的优化方法无法用于训练这类网络，需要寻找不依赖梯度信息的替代策略。

Method: 将训练问题表述为凸函数之差(DC)形式，使用凸凹过程(CCP)迭代求解，产生一系列线性规划子问题。

Result: 计算实验证明了所提出的训练方法在处理MPCL网络分类任务方面的有效性。

Conclusion: CCP方法为训练不可微分的形态感知器网络提供了一种有效的替代方案，能够成功解决多类分类任务。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [33] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL是一种将第一性原理模型与贝叶斯深度学习相结合的方法，使用低保真模拟器作为先验，在数据稀缺时利用模拟器知识，在数据充足时发挥深度学习的灵活性，同时量化认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在现实世界中高效学习的问题。第一性原理模型因简化假设难以捕捉现实复杂性，而深度学习方法需要大量代表性数据。

Method: 使用低保真模拟器作为贝叶斯深度学习的先验，结合第一性原理模型和数据驱动学习，在低数据和高数据场景下都能有效工作。

Result: 在生物、农业和机器人等多个领域展示出优越性能，特别是在高速RC汽车漂移停车任务中，比现有基线方法使用更少数据学习高度动态操作。

Conclusion: SimPEL能够有效弥合模拟到现实的差距，在复杂现实环境中实现数据高效的学习和控制。

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [34] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 模型基于强化学习中，在线训练比离线训练效果更好，主要因为离线数据集导致分布外状态问题。通过添加在线交互或探索数据可以缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 研究在线与离线数据收集对模型基于强化学习中世界模型学习效果的影响，展现离线训练的性能下降问题及其根源。

Method: 在31个不同环境中进行实验对比，分析在线与离线训练的性能差异，识别分布外状态问题，并探索通过添加在线交互或探索数据来缓解这个问题的方法。

Result: 在线训练效果明显优于离线训练。离线训练导致测试时遇到分布外状态，影响政策训练。通过添加在线交互或探索数据可以恢复性能。

Conclusion: 建议在收集大规模数据集时添加探索数据，而不仅仅依赖专家数据，以缓解离线训练的性能下降问题。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [35] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合PRC随机森林和自编码器的混合框架，用于解决异常检测中的类别不平衡和高维问题，在多个基准数据集上表现出优越性能


<details>
  <summary>Details</summary>
Motivation: 异常检测面临极端类别不平衡和维度灾难两大挑战，需要同时解决这两个问题来提高检测性能

Method: 将之前提出的PRC随机森林(PRC-RF)与自编码器结合，自编码器学习紧凑的潜在表示来降维，PRC-RF处理类别不平衡

Result: 在多个基准数据集上的广泛实验表明，Autoencoder-PRC-RF模型在准确性、可扩展性和可解释性方面优于现有方法

Conclusion: 该混合框架在高风险异常检测任务中具有巨大潜力，能够同时有效应对类别不平衡和高维挑战

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [36] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 提出了Real-E数据集，包含30多个欧洲国家74个发电站10年数据，通过全面基准测试发现现有方法在复杂非平稳相关性动态中表现不佳，并提出了新的相关性结构偏移度量指标


<details>
  <summary>Details</summary>
Motivation: 现有能源预测基准在时空范围和多能源特征方面有限，影响其在真实世界部署的可靠性和适用性

Method: 构建Real-E数据集，进行广泛数据分析，基准测试20多种不同模型类型的方法，引入新的相关性结构偏移度量指标

Result: 现有方法在Real-E数据集上表现不佳，该数据集展现出更复杂和非平稳的相关性动态

Conclusion: 研究结果揭示了当前方法的关键局限性，为构建更稳健的预测模型提供了强有力的实证基础

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [37] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 本文提出了一种双重交叉验证框架DCV-ROOD，用于更稳健地评估分布外检测模型的性能，通过分别处理分布内和分布外数据的分割策略，并在类别层次结构数据上进行验证。


<details>
  <summary>Details</summary>
Motivation: 分布外(OOD)检测对于提高AI系统的稳健性至关重要，但现有的评估方法在处理ID和OOD数据的特异性时遇到挑战，需要一种更可靠的评估框架来确保OOD检测方法的有效性。

Method: 提出双重交叉验证框架，对ID数据采用标准分割方式，对OOD数据则按类别分组进行分割。在类别层次数据上，考虑整个类别层次来获得公平的ID-OOD分割。

Result: 对多个独创性的OOD检测方法进行测试，结果显示该方法能够快速收敛到真实性能值，证明了评估框架的有效性。

Conclusion: DCV-ROOD框架为OOD检测方法提供了一种可靠的评估方法，通过适应性地处理ID和OOD数据的特异性，能够更准确地估计模型的真实性能。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [38] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出ExoST框架，通过"选择再平衡"范式解决外生变量在时空预测中的不一致性和不平衡性挑战


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方法仅使用有限的目标变量，而现实场景中外生变量可以作为额外输入特征提升预测精度，但面临不同外生变量对目标系统影响不一致以及历史变量与未来变量影响不平衡的挑战

Method: 构建潜在空间门控专家模块将融合的外生信息投影到潜在空间，通过专门子专家动态选择和重组显著信号；设计孪生网络架构，将过去和未来外生变量的重组表示输入双分支时空骨干网络捕获动态模式；通过上下文感知加权机制实现建模过程中的动态平衡

Result: 在真实世界数据集上的大量实验证明了所提出框架的有效性、通用性、鲁棒性和效率

Conclusion: ExoST框架成功解决了外生变量建模中的关键挑战，为时空预测提供了有效的解决方案

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [39] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 这篇论文通过激活移植技术证明了Transformer基础模型在时间序列预测中内部存在可操控的语义表征空间，能够通过水平控制模型预测行为，为市场冲击测试提供新方法。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer基础模型是否真正内部化了语义概念（如市场治理），还是仅仅拟合曲线，以及是否能利用其内部表征来模拟稀罕但高风险的市场崩溃事件。

Method: 提出激活移植技术，通过将某个事件（如历史崩溃）的统计矩征强制注入到另一个事件（如平静期）的隐藏状态中，从而因果性地操控模型预测。

Result: 注入崩溃语义会导致下跌预测，注入平静语义则可压制崩溃并恢复稳定性。模型编码了事件严重程度的等级概念，潜在向量的模长与系统性冲击的强度直接相关。

Conclusion: 可操控、语义基础的表征是大型时间序列Transformer模型的稳健特性，证明了存在一个潜在概念空间来管理模型预测，为战略性压力测试提供了新的解释性方法。

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [40] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文放宽了AMOO框架的强凸性假设，在更符合深度学习实践的凸性条件下研究梯度下降算法，提出了新的分析工具和可扩展算法，并证明了朴素等权重方法的次优性。


<details>
  <summary>Details</summary>
Motivation: 现有AMOO分析依赖强凸性假设，这与深度学习实践不符。需要研究在标准凸性、光滑性或Lipschitz连续性条件下的AMOO算法。

Method: 开发新的分析工具和度量标准，提出可扩展的凸AMOO梯度下降算法，建立收敛保证。

Result: 提出了在凸AMOO设置下的收敛性分析框架和算法，证明了所提方法的有效性。

Conclusion: 在更一般的凸性条件下，AMOO问题仍然可以通过梯度下降方法有效求解，且提出的方法优于朴素等权重策略。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [41] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文研究发现，虽然保形预测能够以高概率覆盖真实类别，但其预测集大小与人类标注者提供的标注数量之间相关性很弱，表明保形预测在捕捉数据固有模糊性（偶然不确定性）方面的能力有限。


<details>
  <summary>Details</summary>
Motivation: 保形预测被期望能够通过预测集大小来捕捉偶然不确定性，但缺乏实证证据支持这一特性。本研究旨在验证保形预测是否能有效量化由类别重叠引起的数据集固有模糊性。

Method: 使用三种保形预测方法为八个深度学习模型生成预测集，这些模型在四个包含多人标注（5-50人）的数据集上训练。通过测量预测集大小与人类标注者分配的不同标签数量之间的相关性来评估效果。

Result: 绝大多数保形预测输出与人类标注之间呈现非常弱到弱的相关性，只有少数显示中等相关性。预测集虽然能提供更高的真实类别覆盖率，但在捕捉偶然不确定性方面能力有限。

Conclusion: 需要重新审慎评估保形预测生成的预测集。虽然它们能提供更高的类别覆盖率，但在捕捉数据固有模糊性方面的能力仍然有限，不能可靠地量化偶然不确定性。

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [42] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 通过在大规模社会科学实验数据集SocSci210上微调LLMs，显著提高了对人类行为模拟的准确性，在未见实验中的预测比基础模型提升26%，比GPT-4o提升13%。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs模拟社会科学实验结果，但需要提高模拟的准确性和泛化能力，特别是在处理多样化人类响应分布时的表现。

Method: 构建包含290万响应、40万参与者、210个实验的SocSci210数据集，使用该数据集对Qwen2.5-14B等基础模型进行微调，开发Socrates-Qwen-14B模型。

Result: 微调后模型在完全未见研究中预测准确率提升26%，在未见条件下泛化能力提升71%，人口统计偏差减少10.6%。

Conclusion: 在社会科学特定数据上微调LLMs能够显著提升实验假设筛选的模拟准确性，为社会科学研究提供更可靠的计算工具。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [43] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 这是一个全面的分布式梯度市场评测框架，重点考虑了经济效率、公平性和市场稳定性等聚合方法在梯度市场中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习测试忽视了分布式梯度市场中的关键经济和系统因素，如成本效益、卖家公平性和市场稳定性，特别是当买家依赖私有基准数据集进行评估时。

Method: 提出了一个综合评测框架：(1)模拟环境模拟变化买家基准和多样化卖家分布的市场动态；(2)在标准FL指标基础上增加了经济效率、公平性和选择动态等市场相关维度；(3)对MartFL框架进行深入实证分析，集成了FLTrust和SkyMask等聚合策略。

Result: 该评测框架涵盖了多样化数据集、本地攻击和软仲攻击等场景，提供了模型性能、稳健性、成本、公平性和稳定性之间的具体交换拉换见解。

Conclusion: 这个评测框架为社区提供了重要工具和实证证据，以评估和设计更稳健、公平和经济可行的分布式梯度市场。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [44] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 基于自回归序列模型的队列网络建模框架，通过学习事件流数据自动构建高保真模拟器，提高建模效率和可访性


<details>
  <summary>Details</summary>
Motivation: 传统队列网络模型需要大量人工劳动和领域专业知识，为了让这种建模方法更具可扩展性和可访性

Method: 使用Transformer类架构的自回归序列模型，通过学习事件类型和事件时间的条件分布，将建模任务重构为序列分布学习问题

Result: 在多样化队列网络生成的事件表上验证了框架的有效性，展示了其在模拟、不确定性量化和反事实评估中的应用价值

Conclusion: 利用AI进步和数据可用性，该框架向更自动化的数据驱动建模流程进行了插译，支持队列网络模型在服务领域的更广泛采用

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [45] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 该论文研究了机器学习遗忘中的对抗性伪造问题，分析了伪造集合的测度随容忍度ε的衰减规律，证明了对抗性伪造在理论上具有根本性限制，虚假遗忘声明可以被检测。


<details>
  <summary>Details</summary>
Motivation: 受隐私法规和消除有害数据影响的需求驱动，机器学习遗忘旨在修改训练模型以有效"遗忘"指定数据。验证遗忘的关键挑战是伪造问题——对抗性制作模仿目标点梯度的数据，造成已遗忘的假象而实际上未移除信息。

Method: 提出了ε-伪造集合的分析框架，对于线性回归和单层神经网络，证明了该集合的Lebesgue测度很小，按ε的阶数缩放。在一般正则性假设下，证明了伪造集合测度以ε^{(d-r)/2}衰减，其中d是数据维度，r是模型梯度变化矩阵的零度。

Result: 研究显示伪造集合的测度随ε减小而快速衰减，在线性回归和单层神经网络中为ε阶，足够小时为ε^d阶。在非退化数据分布下，随机采样伪造点的概率极小。

Conclusion: 对抗性伪造具有根本性限制，虚假的遗忘声明在原则上可以被检测到，这为机器学习遗忘的可验证性提供了理论依据。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [46] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 基于深度强化学习的稀疏参考文献选择框架，模拟人类知识构建过程，在有限时间和成本下优先选择要阅读的论文


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速扩张使得获取新知识变得困难，特别是在专业领域中推理复杂、全文访问受限且目标参考文献稀疏的情况下

Method: 使用深度强化学习框架进行稀疏参考文献选择，模拟人类知识构建过程，优先选择在有限时间和成本下应该阅读的论文

Result: 在药物-基因关系发现任务中评估，仅使用标题和摘要信息，证明人类和机器都能从部分信息中有效构建知识

Conclusion: 该深度强化学习框架能够有效处理稀疏参考文献选择问题，在信息受限的情况下仍能实现有效的知识构建

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [47] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 开发了预测溶液钠在微型热汇中对流热传道系数的代理模型，结合物理知识的自监督神经网络和迁移学习方法实现了约+8%的高精度预测


<details>
  <summary>Details</summary>
Motivation: 高保真度的CFD模拟液体金属强强对流热传递时间和计算成本较高，需要一种替代方案来优化微型热汇设计

Method: 首先使用核机器学习和浅层神经网络，然后采用自监督神经网络和迁移学习。自监督神经网络通过额外层动态调整物理损失函数权重，迁移学习则将水数据训练的模型调整用于钠数据

Result: 自监督神经网络成功预测钠的热传递率，错误范围约+8%。仅使用物理回归时错误为5%-10%，其他机器学习方法大多在+8%以内

Conclusion: 机器学习基于模型为液体金属冷却微型热汇提供了高效的设计优化工具，可以替代计算成本高的CFD模拟

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [48] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: X-SQL是一个基于开源模型的Text-to-SQL框架，通过创新的数据库模式专家组件（X-Linking和X-Admin）显著提升SQL生成质量，在Spider数据集上达到84.9%和82.5%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL框架忽视了数据库模式信息的重要性，而研究发现模式信息在SQL生成任务中起着关键甚至主导作用。

Method: 提出包含两个组件的数据库模式专家：1）X-Linking：基于LLM监督微调的Schema Linking方法；2）X-Admin：通过桥接抽象模式信息和自然语言问题来实现Schema Understanding。还采用多LLM策略为不同组件分配专用模型。

Result: 在Spider-Dev数据集上达到84.9%的执行准确率，在Spider-Test数据集上达到82.5%的执行准确率，成为基于开源模型的领先Text-to-SQL框架。

Conclusion: 数据库模式信息对Text-to-SQL任务至关重要，X-SQL框架通过创新的模式专家组件和多LLM策略，显著提升了SQL查询生成的质量和准确性。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [49] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 提出了SOOTT框架，整合目标跟踪、对抗扰动和切换成本三个目标，用于在线决策。开发了BEST鲁棒算法和CoRT学习增强算法，在负载调度案例中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中在线决策的不确定性挑战，如AI集群中的弹性工作负载调度，需要平衡长期服务协议和突发需求。

Method: 提出SOOTT问题框架，开发BEST算法提供竞争性保证，并设计CoRT学习增强算法整合黑盒预测。

Result: 理论分析显示CoRT在预测准确时优于BEST，同时保持对任意预测错误的鲁棒性。案例研究验证了算法在平衡轨迹跟踪、决策平滑性和抗干扰方面的有效性。

Conclusion: SOOTT框架为在线决策提供了综合解决方案，BEST和CoRT算法在实际应用中表现出良好的性能和鲁棒性。

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [50] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 提出了统一交互基础模型(UIFM)，采用复合标记化方法将多属性事件作为语义连贯单元处理，解决了传统模型在序列事件理解中的语义碎片化问题


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的基础模型在处理电信、电商、金融等领域的结构化交互时，通过文本序列化会破坏事件的整体语义，丢失关键上下文信息

Method: 采用复合标记化原则，将每个多属性事件视为单个语义连贯单元，学习用户行为的底层"语法"，感知完整的交互而非离散数据流

Result: 该架构不仅提高了准确性，而且代表了创建更适应性和智能预测系统的根本性进步

Conclusion: UIFM模型为真正的行为理解提供了新的基础模型范式，能够更好地理解和预测复杂演化的事件序列

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [51] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: PolicyEvolve是一个基于LLM的多智能体强化学习框架，通过程序化策略生成减少对人工代码的依赖，提高策略可解释性和执行效率


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法需要大量样本和计算资源，且策略缺乏可解释性，限制了实际部署。LLM在单智能体任务中成功生成程序化策略的经验启发了多玩家游戏中的类似应用

Method: 包含四个模块：全局池（保存精英策略）、局部池（存储临时策略）、策略规划器（核心生成模块，采样精英策略并基于环境信息生成初始策略）、轨迹评判器（分析交互数据，识别漏洞并提供改进方向）。通过迭代优化过程生成高性能策略

Result: 显著减少对环境交互的依赖，能够以最小交互成本实现高性能策略生成

Conclusion: PolicyEvolve框架为多玩家游戏提供了一种高效、可解释的程序化策略生成方法，降低了人工编码需求，提高了策略部署的实用性

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [52] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出基于机器学习和计算流体力学的生物质流化床气化耦合模型，提高复杂热化学反应过程的预测精度和计算效率


<details>
  <summary>Details</summary>
Motivation: 提高生物质流化床气化过程中复杂热化学反应预测的准确性和计算效率

Method: 基于实验数据和高保真模拟结果构建高质量数据集，训练描述反应动力学特性的代理模型，并将其嵌入CFD框架实现反应速率和组分演化的实时更新

Result: 建立了机器学习与CFD耦合的预测模型

Conclusion: 该方法能够有效提升生物质气化过程模拟的精度和计算效率

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [53] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: ARIES框架通过构建合成数据集和评估系统，建立了时间序列属性与建模策略之间的关联，并实现了首个深度预测模型推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集缺乏多样化的时间模式，无法系统评估模型性能与数据属性之间的关系，且缺乏有效的模型推荐方法，导致测试不同架构时成本高昂。

Method: 1) 构建具有多种不同模式的合成数据集；2) 设计计算时间序列属性的综合系统；3) 对50多个预测模型进行广泛基准测试；4) 建立时间序列属性与建模策略之间的关系。

Result: 实验结果显示时间序列属性与建模策略之间存在明显相关性，基于此开发了首个深度预测模型推荐器，能够为现实世界时间序列提供可解释的建议。

Conclusion: ARIES是首个建立时间序列数据属性与建模策略关系的研究，同时实现了模型推荐系统，为时间序列预测提供了系统化的评估和推荐框架。

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [54] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 基于殊余神经网络的代模型提高高温超导磁铁计算效率，减少有限元法计算成本


<details>
  <summary>Details</summary>
Motivation: 有限元法在大规模高温超导磁铁中计算成本高，限制了磁铁系统的快速设计

Method: 开发基于全连接殊余神经网络的代模型，预测REBCO螺线管中的空间-时间电流密度分布

Result: 12个殊余块和每层76个神经元的网络结构效果最佳，能够向外推断50%且误差低于10%，计算速度提高数个数量级

Conclusion: 此代模型既准确又高效，为大规模HTS磁铁快速分析提供了有前景的工具

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [55] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文针对具有准双曲线(QH)折扣偏好的预承诺智能体，提出了首个理论分析和实用算法，证明了最优策略简化为简单的一步非平稳形式，并设计了具有可证明收敛保证的模型无关算法。


<details>
  <summary>Details</summary>
Motivation: 准双曲线折扣是人类和动物决策中时间不一致偏好的关键模型，但其在强化学习框架中的整合有限，存在理论和算法空白需要解决。

Method: 通过形式化分析最优策略结构，证明其可简化为一步非平稳形式；设计模型无关的策略评估和Q学习算法，并提供收敛性证明。

Result: 首次证明了QH偏好下最优策略的简化结构，开发了首个实用的模型无关算法，为QH偏好融入RL提供了理论基础。

Conclusion: 该研究为准双曲线折扣偏好强化学习奠定了重要基础，提供了理论洞察和实用算法工具，推动了时间不一致偏好建模的发展。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [56] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文探讨生成式AI的理论基础，将其作为与预测、压缩和决策相关的独立机器学习任务，综述了五大生成模型家族，提出了概率和博弈论框架，并讨论了部署准备和社会责任问题。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI作为独立机器学习任务的理论基础，建立系统的生成问题框架，超越单纯的技术实现层面。

Method: 采用概率框架区分密度估计和生成，引入博弈论的双玩家对抗学习设置，综述五大生成模型家族（自回归模型、变分自编码器、标准化流、生成对抗网络、扩散模型）。

Result: 建立了生成任务的系统理论框架，提供了从概率和博弈论角度理解生成问题的新视角，为生成模型的部署和社会责任应用提供了指导。

Conclusion: 生成式AI需要从任务角度进行系统性理解，而不仅仅是技术实现；概率和博弈论框架为生成问题提供了理论基础；社会责任问题（隐私、AI内容检测、版权）是生成模型部署的重要考量。

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [57] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 通过结合图神经网与显式数值时间步进方案，Graph Neural Simulators (GNS) 在稀缩训练数据情况下显著提升了时间依赖波动方程的模拟准确性和数据效率


<details>
  <summary>Details</summary>
Motivation: 传统神经算子需要大量数据且在稀缩数据情况下表现差，同时没有显式编码物理过程的因果关系和时间本地性结构

Method: 采用Graph Neural Simulators (GNS)框架，结合消息传递图神经网经网络与显式数值时间步进方案，通过学习瞬时时间导数来建立准确的向前模型，并介绍了PCA+KMeans轨迹选择策略

Result: 在三个典型PDE系统上，GNS仅需30个训练样本（3%数据）就能达到小于1%的相对L2误差，比DeepONet和FNO基线更准确。在长时间范围内，GNS相寰自回归误差减少82.48%（vs FNO AR）和99.86%（vs DON AR）

Conclusion: 结合图基本地归纳偏置与传统时间积分器，可以构建准确、物理一致且可扩展的时间依赖PDE代理模型

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [58] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: 本研究探讨使用超宽带(UWB)技术和深度学习模型在家庭环境中追踪居住者路径，提出基于指纹识别的方法，比较CNN、LSTM和混合CNN+LSTM模型性能，获得接近50厘米的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: UWB技术通过飞行时间和到达时间差方法估计用户位置，但在真实环境中受墙壁和障碍物影响精度降低，需要解决这些挑战以提高家庭环境中的活动识别精度。

Method: 采用基于指纹识别的方法，利用从两个公寓(60m²和100m²)收集的RSSI数据，比较CNN、LSTM和混合CNN+LSTM模型，评估不同类型和时间窗口的影响，并与蓝牙技术进行对比。

Result: 研究结果显示平均绝对误差接近50厘米，混合模型在提供准确位置估计方面表现最优，优于单独的CNN或LSTM模型。

Conclusion: 混合CNN+LSTM模型在家庭环境中能够提供精确的位置估计，有助于日常人类活动识别在住宅环境中的应用。

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [59] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 这篇论文研究了融合异质多模态城市数据的潜在表征在发现模式方面是否比单独数据更有效，通过可视化分析框架进行验证。


<details>
  <summary>Details</summary>
Motivation: 城市分析中的多模态数据存在粒度细、异质性和多模态性挑战，现有可视化工具少有探索融合数据是否能涵盖更深层次的见解。

Method: 开发了一个可视化辅助框架，用于分析融合潜在数据表征与单独数据表征在发现动态和静态城市数据模式方面的效果对比。

Result: 分析显示结合的潜在表征能产生更结构化的模式，而单独表征在特定情况下也有其用处。

Conclusion: 融合异质多模态数据的潜在表征在城市分析中具有更好的模式发现能力，但单源数据在某些场景下仍有价值。

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [60] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: 提出了一种推理语言模型（RLM），通过整合放射学发现和纵向医疗记录进行个性化肺癌风险评估，显著提升了风险预测性能。


<details>
  <summary>Details</summary>
Motivation: Lung-RADS系统在敏感性和特异性之间存在权衡，仅基于肺结节特征进行风险分层，未整合多种风险因素，需要更准确的个性化风险评估方法。

Method: 使用推理语言模型整合放射学发现和纵向医疗记录，通过数据集构建和蒸馏、监督微调、强化学习等方法，将风险评估任务分解为子组件并合成最终风险评分。

Result: 在国家肺癌筛查试验数据集上风险预测性能显著提升，改善了预测准确性和可监测性。

Conclusion: 该方法通过思维链推理过程促进了肺癌筛查的临床转化，为个性化肺癌风险评估提供了有效解决方案。

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [61] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: 研究在GOHR环境中使用Transformer-A2C算法，比较了特征中心和对象中心两种状态表示策略在隐藏规则推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索在部分观测环境下，通过经验同时推理隐藏规则并学习最优策略的强化学习方法。

Method: 使用Transformer基础的Advantage Actor-Critic (A2C)算法，比较了特征中心(FC)和对象中心(OC)两种状态表示策略。

Result: 在多个规则基础和试验列表实验中评估模型，分析了迁移效果和表示方式对学习效率的影响。

Conclusion: 该研究为隐藏规则推理任务提供了有效的强化学习方案，并证明了状态表示策略对学习效果的重要影响。

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [62] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出了一种基于度量嵌入初始化的差分隐私可解释图聚类方法，通过SDP优化、关键集提取和HST初始化，结合k-median聚类策略，在保证隐私的同时提升聚类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私图聚类研究面临高噪声、低效率和差可解释性等挑战，严重制约了该领域的发展，需要构建既能保护隐私又具有良好性能的方法。

Method: 构建SDP优化问题，提取关键集，使用基于HST的初始化方法提供良好初始聚类配置，然后应用k-median聚类策略获得聚类结果，并通过与聚类中心的差异为查询集提供比较解释。

Result: 在公开数据集上的大量实验表明，该框架在各种聚类指标上优于现有方法，同时严格保证了隐私保护。

Conclusion: 该方法成功解决了差分隐私图聚类中的关键挑战，在保持隐私保护的同时显著提升了聚类性能和可解释性，为后续研究提供了有价值的参考。

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [63] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: MCIGLE是一个无需样本的类增量学习框架，专门处理多模态图结构数据，通过特征提取对齐和递归最小二乘法解决灾难性遗忘等问题


<details>
  <summary>Details</summary>
Motivation: 随着多模态图结构数据的普及，现有方法在灾难性遗忘、分布偏差、内存限制和泛化能力弱等方面面临挑战，需要新的解决方案

Method: 提出MCIGLE框架，通过提取和对齐多模态图特征，应用串联递归最小二乘法进行知识保留，采用多通道处理平衡准确性和内存保护

Result: 在公共数据集上的实验验证了该方法的有效性和泛化能力

Conclusion: MCIGLE成功解决了多模态图数据类增量学习中的关键问题，为相关领域提供了有效的解决方案

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [64] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: 这篇论文提供了UrbanMIMOMap数据集，为6G系统的环境感知通信提供大规模、高精度的MIMO通道状态信息数据，充实了无线电地图构建的数据基础。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集主要集中在SISO系统和路径损耗数据，无法满足6G系统对高级MIMO通道状态信息的需求，需要大规模、高质量的训练数据来支持机器学习方法构建高保真度的无线电地图。

Method: 使用高精度光线追踪技术生成大规模城市MIMO CSI数据集，提供了在密集空间格点上的全面复杂CSI矩阵，超越了传统的路径损耗数据。

Result: 构建了UrbanMIMOMap数据集，通过代表性机器学习方法进行基准性能评估，证明了该数据集在无线电地图构建中的实用性。

Conclusion: 该工作为高精度无线电地图生成、MIMO空间性能和6G环境感知的机器学习研究提供了关键数据集和参考，代码和数据已开源。

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [65] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: IPR是一个智能提示路由框架，通过预测响应质量和用户指定的容忍度来动态选择最优LLM模型，在保持质量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模商业系统中路由查询到最具成本效益的LLM同时保持响应质量的基本挑战，优化性能-成本权衡。

Method: 采用模块化架构，包含轻量级质量估计器（基于150万标注提示训练）、用户控制的容忍度参数路由机制，以及使用冻结编码器和模型特定适配器的可扩展设计。

Result: 在主要云平台上部署后，IPR实现了43.9%的成本降低，同时保持与Claude家族最强模型的质量相当，请求处理延迟低于150毫秒。

Conclusion: IPR框架通过智能路由和质量约束机制，成功解决了LLM部署中的成本-质量权衡问题，具有实际工业应用价值。

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [66] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: RecMind是一个LLM增强的图推荐系统，将语言模型作为偏好先验而非单一排序器，通过对比学习对齐文本和协同视图，使用门控机制融合，在冷启动和长尾场景表现优异


<details>
  <summary>Details</summary>
Motivation: 个性化推荐面临稀疏交互、快速内容更新和异构文本信号的挑战，需要有效融合语言模型和协同过滤的优势

Method: 使用冻结LLM加轻量适配器生成文本条件嵌入，LightGCN学习协同嵌入，通过对称对比目标对齐两个视图，采用层内门控融合机制

Result: 在Yelp和Amazon-Electronics数据集上所有8个指标均达到最佳结果，相对改进最高达+4.53%(Recall@40)和+4.01%(NDCG@40)

Conclusion: 跨视图对齐是必要的，门控融合优于后期融合和纯LLM变体，语言模型在冷启动/长尾场景主导，图结构在其他场景稳定排序

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [67] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: LoaQ是一种新的层级后训练量化方法，通过直接优化输出一致性而非局部近似，在LLaMA和Qwen模型上实现了更好的量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的层级后训练量化方法只关注局部近似，无法实现与完整模型输出的一致性，导致量化效果不理想。

Method: 基于对主流LLM结构特性的深入理解，提出输出近似方法LoaQ，具有简单的闭式解，可与现有量化技术正交集成。

Result: 在LLaMA和Qwen模型家族上，LoaQ在仅权重量化和权重-激活联合量化中都表现有效，显著提升了量化质量。

Conclusion: LoaQ方法更好地符合量化直觉，能够无缝集成到现有量化流程中，有潜力推动后训练量化的前沿发展。

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [68] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: WindFM是一个轻量级生成式基础模型，专门用于概率性风电功率预测，通过离散化-生成框架和Transformer架构，在零样本情况下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法要么训练特定站点模型无法泛化到其他位置，要么依赖通用时间序列基础模型的微调，难以融入能源领域特定数据。

Method: 采用离散化-生成框架：专用时间序列分词器将连续多变量观测转换为离散分层标记，解码器Transformer通过自回归预训练学习通用风能动态表示。使用WIND Toolkit数据集（约1500亿时间步，12.6万+站点）进行预训练。

Result: 仅810万参数的紧凑模型在确定性和概率性任务上实现零样本最先进性能，超越专用模型和更大基础模型，无需微调。在跨大陆分布外数据下表现出强适应性。

Conclusion: WindFM展示了学习表示的鲁棒性和可迁移性，为风电预测提供了有效的轻量级基础模型解决方案。

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [69] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: 提出了一个冗余指数rho(C)来直接量化潜在表示中的维度间依赖性，通过分析耦合矩阵并与正态分布比较来评估表示质量。


<details>
  <summary>Details</summary>
Motivation: 深度网络常产生冗余的潜在空间，多个坐标编码重叠信息，降低了有效容量并阻碍泛化。标准指标如准确率或重构损失只能提供间接证据，无法直接识别冗余问题。

Method: 引入冗余指数rho(C)，通过分析从潜在表示导出的耦合矩阵，并通过能量距离将其非对角线统计量与正态分布进行比较。

Result: 在多个数据集和架构上验证，低rho(C)可靠预测高分类准确率或低重构误差，而高冗余与性能崩溃相关。TPE优先探索低rho区域，表明rho(C)可指导神经架构搜索。

Conclusion: rho(C)通过暴露模型和任务中的冗余瓶颈，为评估和改进学习表示的效率提供了理论视角和实用工具。

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [70] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: LLMs能够通过上下文学习准确预测偏微分方程的时空动态，无需微调或自然语言提示，预测精度随上下文长度增加而提高，但随空间离散化变细而下降


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在零样本时间序列预测中的涌现能力，特别是对偏微分方程解的时空动态外推能力

Method: 使用文本训练的基础模型处理离散化的偏微分方程解，分析多步预测中的误差积累，并研究token级输出分布以理解模型内部处理机制

Result: 模型预测精度随时间上下文长度增加而提高，但在更细的空间离散化下性能下降；多步预测误差随时间范围代数增长；发现一致的ICL进展模式：从语法模式模仿到探索性高熵阶段，最终形成数值基础预测

Conclusion: LLMs能够通过上下文学习有效处理偏微分方程解的时空外推任务，表现出可预测的神经缩放规律，为理解模型内部处理机制提供了新见解

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [71] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: 机器学习结合营养领域知识能有效分析患者生成的自由文本饮食记录，准确分类膳食是否符合特定营养目标，性能优于自我评估


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何利用机器学习和领域知识分析患者生成的自由文本饮食数据，为精准医疗提供营养指导支持

Method: 使用TFIDF和BERT文本嵌入，结合营养本体、成分解析器和宏量营养素信息作为输入，评估逻辑回归和多层感知机分类器的性能

Result: 即使没有领域知识增强，机器学习也优于自我评估；最佳组合（机器学习分类器+领域知识增强）达到更高准确率，在多个营养目标上表现良好

Conclusion: 机器学习能可靠地利用非结构化自由文本饮食记录分类膳食是否符合特定营养目标，特别在融入营养领域知识时效果更佳，展现了在精准医疗中支持以患者为中心营养指导的潜力

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [72] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: 大语言模型在算法执行任务上表现优异，但在需要创造性数学思维的组合排列问题中显示出显著的缺陷


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型数学数值计算能力的突出表现是否具有稳健性，识别基础性的弱点

Method: 逐渐增加复杂度的数学问题集，测试多个先进LLM基于的代理，包括基础算术、高级运算、素数检查和24点游戏数学谛题

Result: 代理在前三类需要算法执行的任务上准确率高，但在需要在大规模组合空间进行启发式搜索的数学谛题中一致失败

Conclusion: LLM的数学推理能力更像是精细的模式匹配，而非灵活的分析思维，在需要创造性数学见解的任务中有限

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [73] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: Ban&Pick是一种无需重新训练的后训练策略，通过强化关键专家和动态剪枝冗余专家，提升MoE模型的性能和推理速度


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在预训练中路由器过早收敛并强制平衡使用，导致关键专家利用不足和冗余专家问题，限制了模型性能和效率

Method: 提出Ban&Pick后训练策略：Pick发现并强化对性能影响大的关键专家；Ban根据层和token敏感性动态剪枝冗余专家

Result: 在DeepSeek、Qwen3等MoE-LLMs上，数学、代码和推理基准测试显示准确率显著提升（如Qwen3-30B-A3B在AIME2024从80.67提升到84.66），推理速度提升1.25倍

Conclusion: Ban&Pick无需重新训练或架构修改，即可为细粒度MoE模型带来免费的性能增益和推理加速

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [74] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: 论文分析Android系统服务SafetyCore中的本地AI模型安全漏洞，展示攻击者如何提取和操纵模型来绕过敏感内容检测


<details>
  <summary>Details</summary>
Motivation: 随着硬件和软件改进，越来越多的AI模型部署在设备端，这虽然增强了隐私和降低了延迟，但也带来了不同于传统软件的安全风险

Method: 通过真实案例研究SafetyCore（Android系统服务），分析本地AI模型的安全漏洞，展示模型提取和操纵的具体方法

Result: 研究表明本地AI模型可以被提取和操纵，从而绕过敏感图像内容检测，使保护机制失效

Conclusion: 分析揭示了本地AI模型的脆弱性，并提供了攻击者如何利用这些漏洞的实际演示，强调了本地AI部署的安全风险

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [75] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: 本文重新审视并改进了基于统计物理的变分门控(VG)方法，用于高维数据中的稀疏变量选择，通过自动微分技术实现高效优化，在高度稀疏场景下表现优于Ridge和LASSO回归。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从高维数据中选择关键变量变得越来越重要。稀疏回归通过促进模型简洁性和可解释性为此提供了强大工具，但现有方法在高度稀疏场景下的表现有待改进。

Method: 改进基于统计物理的变分门控(VG)方法，引入显式特征选择自旋变量，利用变分推理推导可处理的损失函数，并整合现代自动微分技术实现可扩展的高效优化。

Result: VG在高度稀疏场景下表现优异，比Ridge和LASSO回归提供更一致和稳健的变量选择。发现了一个尖锐的过渡现象：当引入多余变量时，泛化能力突然下降，选择变量的不确定性增加。

Conclusion: VG在稀疏建模方面具有强大潜力，适用于压缩感知和机器学习中的模型剪枝等广泛应用。过渡点为估计相关变量数量提供了实用信号，成功应用于识别真实数据中的关键预测因子。

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [76] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: 本文提出多粒度知识蓄纳(MGKD)框架，通过知识蓄纳把服务中用户行为数据转移到服务前风险预测中，提高风险评估性能。


<details>
  <summary>Details</summary>
Motivation: 传统金融风险管理中服务前风险评估和服务中默认检测分离建模，本文想通过整合服务中用户行为数据来改善服务前风险预测。

Method: 采用知识蓄纳思路，训练于历史服务数据的教师模型指导训练于服务前数据的学生模型。使用多粒度蓄纳策略（粗粒度、细粒度和自蓄纳）对齐表征和预测，并采用重新加权策略减少少数类偏差。

Result: 在腾讯移动支付大规模实际数据集上的实验结果显示，该方法在离线和在线场景中都有效提高了服务前风险评估性能。

Conclusion: MGKD框架通过知识蓄纳有效整合了服务中行为数据到服务前风险预测中，能够转移关键行为模式并改善风险评估效果。

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [77] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 提出了一种基于GNN和拉格朗日对偶优化的理论保证框架JCPGNN-M，用于无线网络中的多信道资源分配，在满足QoS约束的同时显著提升计算效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通过惩罚项处理数据率约束缺乏理论收敛保证，在实际场景中经常无法满足QoS要求，需要一种理论严谨的可扩展解决方案。

Method: 首先扩展WMMSE算法到多信道QoS约束场景得到eWMMSE算法，然后开发GNN-based的JCPGNN-M算法支持用户多信道同时分配，并在拉格朗日框架下训练GNN确保约束满足和收敛性。

Result: 大量仿真表明JCPGNN-M在性能上匹配eWMMSE，同时在推理速度、对大网络的泛化能力以及不完美信道状态信息下的鲁棒性方面都有显著提升。

Conclusion: 这项工作为未来无线网络中的约束资源分配提供了一个可扩展且理论严谨的解决方案。

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [78] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: NeuroDeX是一个利用LLM语义理解和动态分析的DNN可执行文件反编译器，能够处理编译优化和量化模型，实现操作符类型识别、属性恢复和模型重建。


<details>
  <summary>Details</summary>
Motivation: 设备端深度学习模型需求广泛，但编译后的可执行文件面临逆向工程威胁。现有方法难以处理编译优化和量化编译模型的反编译问题。

Method: 结合LLM的语义理解能力和动态分析技术，准确高效地进行操作符类型识别、操作符属性恢复和模型重建。

Result: 在96个DNN可执行文件上的实验表明，NeuroDeX能将非量化可执行文件反编译为几乎相同的高级模型，对量化可执行文件能恢复功能相似模型，平均top-1准确率达72%。

Conclusion: NeuroDeX相比之前的DNN可执行文件反编译器提供了更全面有效的解决方案，支持编译优化、不同架构和量化编译模型的反编译。

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [79] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: CAPMix是一个可控异常增强框架，通过CutAddPaste机制注入多样化异常，使用标签修正策略减少异常偏移，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中标注异常稀缺的问题，现有异常假设方法存在patchy generation（分散异常知识导致注入过于简单）和Anomaly Shift（合成异常与真实异常差异过大或过小）两个根本局限性。

Method: 设计CutAddPaste机制进行目标性异常注入，引入标签修正策略自适应优化异常标签，在时序卷积网络中使用双空间mixup实现更平滑的决策边界。

Result: 在AIOps、UCR、SWaT、WADI和ESA五个基准数据集上的广泛实验表明，CAPMix相比最先进基线方法取得显著改进，对污染训练数据具有增强的鲁棒性。

Conclusion: CAPMix有效解决了异常注入中的两个关键问题，为时间序列异常检测提供了更有效的解决方案，代码已开源。

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [80] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: CAME-AB是一个用于抗体结合位点预测的多模态注意力框架，结合了五种生物特征模态和混合专家架构，在多个指标上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单视图特征，无法有效识别抗原上的抗体特异性结合位点，存在表示和预测的双重局限性

Method: 整合五种生物模态特征（氨基酸编码、BLOSUM谱、预训练语言模型嵌入、结构特征、GCN精炼生化图），采用自适应模态融合模块、Transformer编码器+MoE模块、监督对比学习和随机权重平均

Result: 在基准抗体-抗原数据集上，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上一致优于强基线

Conclusion: 多模态特征整合和提出的架构组件有效提升了抗体结合位点预测性能，消融研究验证了各组件的重要性

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [81] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: DyC-STG是一个用于物联网数据可信度分析的动态因果时空图网络框架，通过事件驱动的动态图模块和因果推理模块解决传统STG模型在动态人本环境中的局限性


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生大量时空数据流，但数据可信度是关键挑战。传统时空图模型在动态人本环境中存在两个根本局限：静态图拓扑无法捕捉物理动态，以及容易混淆虚假相关性与真实因果关系

Method: 提出DyC-STG框架，包含两个协同模块：事件驱动的动态图模块（实时调整图拓扑反映物理状态变化）和因果推理模块（通过严格时间优先约束提取因果感知表示）

Result: DyC-STG建立了新的最先进水平，比最强基线高出1.4个百分点，F1分数最高达到0.930。同时发布了两个新的真实世界数据集

Conclusion: 该框架有效解决了传统时空图模型在动态人本环境中的局限性，为物联网数据可信度分析提供了创新的解决方案

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [82] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: HANNA模型通过将物理定律作为硬约束集成到神经网络中，准确预测多组分混合物的过量吉布斯自由能，在精度和范围上明显优于现有方法


<details>
  <summary>Details</summary>
Motivation: 从分子结构预测多组分混合物的过量吉布斯自由能是化学工程和化学领域的长期挑战，这对于液体混合物的热力学性质建模至关重要

Method: 将物理定律作为硬约束集成到灵活的神经网络中，在多特蒙德数据库的二元混合物实验数据上进行端到端训练，开发新型替代求解器包含液-液平衡数据，应用几何投影方法实现向多组分混合物的稳健外推

Result: HANNA模型提供了出色的预测性能，在准确性和范围上明显优于最先进的基准方法

Conclusion: 该方法成功解决了从分子结构预测多组分混合物过量吉布斯自由能的挑战，模型和代码已开源，并在MLPROP网站上提供交互界面

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [83] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: 本文针对Wasserstein GAN（WGAN）参数选择问题，在超越线性-二次-高斯（LQG）设定的情况下，推导出了一维WGAN的闭式最优参数解，并扩展到高维切片Wasserstein框架。


<details>
  <summary>Details</summary>
Motivation: GAN参数选择通常需要穷举搜索，且很少有方法能被证明是理论最优的。现有WGAN最优参数研究仅限于LQG设定（线性网络和高斯数据），需要扩展到更一般的非线性网络和非高斯数据场景。

Method: 1）推导一维WGAN在非线性激活函数和非高斯数据下的闭式最优参数；2）采用切片Wasserstein框架扩展到高维情况，用原始数据联合分布约束替代随机投影数据的边缘分布约束；3）证明线性生成器在切片WGAN中对于非高斯数据具有渐近最优性。

Result: 经验研究表明，推导的闭式WGAN参数在高斯和拉普拉斯分布数据下均表现出良好的收敛行为。与r-PCA解决方案相比，所提出的切片WGAN解决方案在达到相同性能的同时需要更少的计算资源。

Conclusion: 本文成功突破了LQG设定的限制，为WGAN提供了在更一般情况下的理论最优参数解，特别是在非线性网络和非高斯数据场景下，为WGAN的实际应用提供了理论指导和计算效率提升。

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [84] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: QualityFM是一个用于PPG和ECG生理信号质量评估的多模态基础模型，通过大规模预训练和双轨架构设计，解决了信号质量差导致的误报警问题。


<details>
  <summary>Details</summary>
Motivation: ICU和手术室中PPG和ECG信号质量差、不完整和不一致的问题导致误报警和诊断不准确，现有方法泛化性有限且需要大量标注数据。

Method: 采用双轨架构处理不同质量的配对生理信号，使用自蒸馏策略，集成窗口稀疏注意力机制的Transformer模型，结合蒸馏损失和重建损失的复合损失函数。

Result: 在超过2100万个30秒波形和17.9万小时数据上预训练，开发了三个参数规模不同的模型（9.6M到319M），在室性心动过速误报警、房颤识别和动脉血压估计三个临床任务上验证有效。

Conclusion: QualityFM通过大规模预训练和创新的架构设计，为生理信号质量评估提供了通用的基础模型，具有良好的跨任务迁移能力和实用价值。

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [85] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: 这篇论文研究了变换器模型在不同数据集上的车道更换意图预测性能，发现单一数据集训练的模型在异构数据集上性能大幅下降，而多数据集训练可显著提升模型通用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多在单一数据集上训练车道更换意图预测算法，缺乏对模型在不同数据集和不同群体上通用性的测试。

Method: 使用LevelX在德国和香港收集的两个数据集，测试变换器模型在单一数据集训练和多数据集同时训练情况下的车道更换意图预测性能。

Result: 单一数据集训练的模型在异构数据集上准确率仅为39.43%，而同时在两个数据集上训练的模型准确率可达86.71%。

Conclusion: 车道更换意图预测模型的通用性需要在多样化的数据集上进行训练和测试，多数据源训练可有效提升模型在不同场景下的表现。

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [86] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文提出了基于POMDP框架的CAGE-2网络安全防御模型，开发了BF-PPO算法，在训练时间和防御策略效果上都优于当前最优方法CARDIFF。


<details>
  <summary>Details</summary>
Motivation: CAGE-2是网络安全防御策略的标准测试平台，现有方法仍有改进空间，需要更高效的防御策略学习方法。

Method: 使用部分可观测马尔可夫决策过程(POMDP)建立CAGE-2的正式模型，基于PPO算法开发BF-PPO方法，利用粒子滤波处理大状态空间的计算复杂度问题。

Result: 在CAGE-2 CybORG环境中评估，BF-PPO在防御策略效果和训练时间方面都优于当前排行榜第一的CARDIFF方法。

Conclusion: 基于POMDP的BF-PPO方法为网络安全防御提供了更优的解决方案，在性能和效率方面都有显著提升。

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [87] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 这篇论文开发了一种可解释的监督变分自编码器模型，用于根据妊婴结果分类胎心电监拟信号，并通过潜在空间分析揭示了临床意义特征的编码情况。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在胎心电监拟信号分析中的可解释性限制，开发能够同时进行信号重建和结果预测的可解释模型。

Method: 使用OxMat CTG数据集训练监督变分自编码器，结合Kullback-Leibler散度和总相关约束来结构潜在空间，并通过决定系数、潜在遍历和无监督分解评估可解释性。

Result: 模型在段落级别获得0.752 AUROC，在CTG级别获得0.779 AUROC。基线相关特征（妊心率基线等）在潜在空间中得到了良好表征，而短期、长期变异性等指标编码较弱。

Conclusion: 监督变分自编码器能够实现竞争性的胎儿结果预测，部分编码了临床意义特征，但由于FHR信号的复杂多时间尺度特性，完全解结仍靠挑战，为未来可解释生成模型奠定了基础。

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [88] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 提出CLAN方法，通过将增强样本作为负样本（代表恶意流量）来改进网络入侵检测，在二分类和多分类任务中均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，异常检测方法误报率高，自监督学习方法通过数据增强生成正负样本对但效果有限

Method: CLAN方法将增强样本作为负样本（代表恶意分布），其他良性样本作为正样本，通过对比学习训练模型

Result: 在Lycos2017数据集上，CLAN在二分类任务中超越现有自监督和异常检测方法，在有限标注数据微调后多分类性能也优于现有自监督模型

Conclusion: CLAN通过创新的负样本生成策略有效提升了网络入侵检测的准确性和效率，为实际应用提供了实用解决方案

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [89] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: 论文认为AI科学发展的主要障碍是社会和制度性的，而非技术性的，需要重新构建AI科学为集体社会项目


<details>
  <summary>Details</summary>
Motivation: 人工智能有望加速科学发现，但其效益分布不均，技术障碍虽然重要，但主要障碍是社会和制度性的

Method: 分析社区功能障碍、研究优先级错配、数据碎片化和基础设施不平等四个相互关联的挑战，提出需要社区建设、跨学科教育、共享基准和可访问基础设施

Result: 识别出文化组织实践是这些挑战的根源，技术创新的同时需要社会层面的变革

Conclusion: 呼吁将AI科学重新定义为集体社会项目，将可持续合作和公平参与视为技术进步的前提条件

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [90] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: 提出了一个理论框架来分析动态非线性系统中的静态和动态失真耦合问题，通过结构化分解、方差分析和任务感知复杂度界限来提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 动态非线性系统中的静态和动态效应相互耦合，给数据驱动建模带来重大挑战，需要建立更基础的理论框架来理解这些系统的行为特性。

Method: 采用结构化分解方法，扩展内积空间中的正交性概念到结构不对称设置，引入行为指标和记忆有限性指数，建立基于功率的条件来连接有限记忆和热力学第一定律。

Result: 提出了行为不确定性原理，证明静态和动态失真不能同时最小化；建立了函数方差与均方Lipschitz连续性及学习复杂度的联系；开发了模型无关的任务感知复杂度度量。

Conclusion: 该框架为复杂动态非线性系统建模提供了可扩展的理论基础方法，解释了结构化残差学习的经验优势，包括改进的泛化能力、减少参数数量和降低训练成本。

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [91] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了GCN在图神经网络中的PAC-Bayesian理论分析，针对动态图环境中的归纳节点分类问题，建立了考虑数据依赖性和非平稳性的泛化边界。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图具有动态特性，新节点不断加入，现有连接随时间变化。以往基于转导学习的理论研究无法充分建模这种时间演化和结构动态。

Method: 使用PAC-Bayesian理论框架，将节点视为依赖且非同分布的数据点，推导单层和双层GCN的泛化边界，明确纳入数据依赖性和非平稳性的影响。

Result: 建立了单层GCN的泛化边界，确定了在节点数量增加时泛化差距收敛到零的充分条件。对双层GCN的分析表明需要更强的图拓扑假设来保证收敛。

Conclusion: 这项工作为理解和改进动态图环境中GNN的泛化能力建立了理论基础，为处理动态图数据提供了理论指导。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [92] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: 使用LLM驱动的HAO系统自动生成医疗虽会病人摘要，通过TBFact框架评估摘要的完整性和简洁性，结果显示能捐描94%重要信息。


<details>
  <summary>Details</summary>
Motivation: 传统手工编写医疗虽会病人摘要工作量大、主观性强且容易遗漏关键信息，需要自动化解决方案提高效率和质量。

Method: 开发HAO（医疗助手系统），使用大语言模型协调多个临床流程生成病人摘要；提出TBFact评估框架，通过"模型作为判官"方式评估摘要的完整性和简洁性。

Result: 在脱标识化虽会讨论数据集上，系统捐描了94%高重要性信息（包括部分含义），在严格含义标准下TBFact回归率达到0.84。

Conclusion: HAO和TBFact为医疗虽会提供了可靠、可扩展的支持基础，能够在不分享敏感数据的情况下实现本地化部署和评估。

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [93] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 论文研究了推理训练如何通过轻量级引导向量重塑语言模型计算，发现最后一层引导向量像token替换偏置，而倒数第二层引导向量通过MLP和反嵌入机制作用。


<details>
  <summary>Details</summary>
Motivation: 理解推理训练如何改变语言模型的计算机制，目前这方面的机制仍不明确。

Method: 使用强化学习目标训练插入基础模型残差流的轻量级引导向量，通过logit-lens读取、路径修补和电路分析等方法分析两个模型。

Result: 发现最后一层引导向量表现为集中在第一个生成token上的token替换偏置，倒数第二层引导向量主要通过MLP和反嵌入机制优先加权过程词和结构符号。

Conclusion: 这些结果为解释推理训练引起的行为变化建立了一个原则性框架。

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [94] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: 这是一篇关于图异常检测领域中模型泛化能力的系统性综述论文，进行了问题定义、分类系统和现有方法的全面分析，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大多数图异常检测方法假设训练和测试分布相同，且为特定任务设计，导致在实际场景中的逆应性有限，需要系统性研究模型泛化能力。

Method: 通过追溯图异常检测泛化能力的进化过程，形式化问题设定，建立系统的分类系统，对现有的泛化图异常检测方法进行全面的综述分析。

Result: 完成了对泛化图异常检测领域的系统性分析和综述，包括问题定义、方法分类和现有技术的全面评估。

Conclusion: 识别了当前的开放挑战和未来研究方向，为这个新兴领域的发展提供了重要的导向和启示。

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [95] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: 提出了BEAM深度学习框架，利用多视角EEG信号预测4-6岁儿童共情水平，通过时空特征提取、特征融合和对比学习模块，在CBCP数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统共情评估方法依赖主观报告，存在偏差且无法客观捕捉共情形成过程；现有EEG方法主要提取静态模式，忽略了时间动态性

Method: BEAM框架包含三个核心组件：1)LaBraM编码器进行时空特征提取，2)特征融合模块整合多视角信号信息，3)对比学习模块增强类别分离

Result: 在CBCP数据集上，BEAM在多个指标上优于最先进方法，证明了其客观评估共情的潜力

Conclusion: BEAM为儿童共情能力提供了客观评估工具，并为早期干预儿童亲社会发展提供了初步见解

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [96] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 通过深度学习从单个图样本学习可推广的本地路由策略，解决全对路近最短路径问题，发现了能够超越贴贝尔路由的新策略GreedyTensile


<details>
  <summary>Details</summary>
Motivation: 解决在欧几里得度量空间中几何随机图上的所有对路近最短路径问题，通过深度学习实现高效、可扩展的本地路由策略

Method: 使用深度神经网络训练本地路由策略，利用网络领域知识选择输入特征（目标距离和节点伸缩度），从单个种子图的少量样本学习可推广策略

Result: 学习到的策略准确匹配Greedy Forwarding，新策略GreedyTensile几乎总是超过贴贝尔路由，具有可解释性和超低延迟运行特性

Conclusion: 通过域知识指导的深度学习方法能够从单个图学习出具有良好推广性的路由策略，并发现了性能更优的新策略

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [97] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出了gcGAIL模型，通过利用乘客群体的共享行为模式来提高个体出行行为建模效率，在准确性、泛化能力和模式演示效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 理解和建模个体出行行为响应对于城市交通监管和政策评估至关重要，但现有的马尔可夫决策过程方法数据需求量大，面临数据数量、时空覆盖和情境多样性等挑战。

Method: 提出了群体效应增强的生成对抗模仿学习（gcGAIL）模型，利用乘客群体间的共享行为模式来提高个体行为建模效率。

Result: 实验结果表明，gcGAIL在准确性、泛化能力和模式演示效率方面优于AIRL、基线GAIL和条件GAIL等方法，对空间变化、数据稀疏性和行为多样性具有鲁棒性。

Conclusion: gcGAIL模型能够预测任何时间的个体行为响应，为个性化激励措施提供基础，以诱导可持续的行为改变（更好的激励时机选择）。

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [98] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: TrajAware是一个基于强化学习的VANET路由框架，通过动作空间剪枝、图交叉注意力和轨迹感知预测三个组件，解决了动态拓扑和边缘设备资源限制的问题，在部分观测和完整观测场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车载自组织网络(VANET)路由面临动态拓扑、不完全观测和边缘设备资源有限的挑战，现有强化学习方法假设固定图结构且需要重新训练，不适合在受限硬件上部署。

Method: 提出TrajAware框架，包含：(1)动作空间剪枝减少冗余邻居选项但保持两跳可达性；(2)图交叉注意力将剪枝邻居映射到全局图上下文；(3)轨迹感知预测利用历史路由和路口信息估计实时位置。

Result: 在SUMO模拟器中使用真实城市地图进行留一城市外评估，TrajAware实现了接近最短路径和高投递率，同时在受限边缘设备上保持高效，在完整和部分观测场景下均优于最先进基线方法。

Conclusion: TrajAware为VANET中的边缘AI部署提供了一个有效的强化学习框架，能够处理动态网络条件和部分观测，适用于资源受限的边缘设备。

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [99] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新型的浅层神经网络——重心神经网络(BNN)，利用重心坐标和固定基点集来实现连续分段线性函数的精确表示，并结合新的持久性熵杂度提升了近似性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度或过参数化神经网络计算成本高的问题，为资源受限环境提供灵活、可解释的函数近似工具。

Method: 提出BNN模型，利用固定基点集和其重心坐标定义网络结构。引入长度权重持久性熵杂度(LWPE)作为稳定的拓扑特征，并直接优化BNN的基点而非内部权重。

Result: 实验结果显示，该方法在近似性能方面超过了MSE、RMSE、MAE和log-cosh等传统损失函数，并且达到更好的效果更快。

Conclusion: BNN结合LWPE损失函数的框架为资源受限情况下的非线性连续函数近似提供了灵活、可解释且高效的解决方案。

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [100] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: 提出了基于概率建模的智能代理理论，通过加权对数池化实现代理组合，严格改善每个成员的福利。证明了在线性池化或二元结果空间中不可能实现严格一致性，但在三个或更多结果中可能。框架具有递归结构，并通过倾斜分析排除平凡重复。最后用该理论形式化了LLM中的代理对齐现象。


<details>
  <summary>Details</summary>
Motivation: 为神经模型开发一个基于概率建模的智能代理理论框架，以理解子代理如何聚合成连贯的高层实体，并为智能AI系统的对齐问题提供新的理论见解。

Method: 将代理表示为结果分布，使用对数评分作为认知效用，通过加权对数池化定义组合。采用克隆不变性、连续性和开放性实现递归结构，使用倾斜分析排除平凡重复。

Result: 证明了在线性池化或二元结果空间中严格一致性不可能，但在三个或更多结果中可能。形式化了LLM中的Waluigi效应：激发仁慈角色会诱导对抗对应角色，而先显现后抑制策略比单纯强化仁慈角色能更大程度减少一阶错位。

Conclusion: 发展一个关于子代理如何聚合成连贯高层实体的原则性数学框架，为智能AI系统的对齐问题提供了新的理论含义和实际应用价值。

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [101] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: 提出使用嵌套最优传输距离作为金融时间序列生成模型的评估指标，并开发了高效的并行计算算法


<details>
  <summary>Details</summary>
Motivation: 金融时间序列模拟对于压力测试和决策制定至关重要，但缺乏统一的生成模型评估标准

Method: 采用时间因果的最优传输距离变体（嵌套最优传输距离），并提出统计一致且可并行化的计算算法

Result: 相比现有方法实现了显著的速度提升，该指标对套期保值、最优停止和强化学习等任务具有鲁棒性

Conclusion: 嵌套最优传输距离是评估金融时间序列生成模型的有效指标，新算法大幅提高了计算效率

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [102] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: 通过提出RT-HCP算法和一般框架来解决模型基于强化学习的样本效率和推理时间挑战，在FURUTA摇摆平台上验证了其优势


<details>
  <summary>Details</summary>
Motivation: 直接在机器人上学习控制器需要极高的样本效率，模型基于强化学习方法样本效率最高，但推理时间过长无法满足机器人高频控制要求

Method: 提出一个处理推理延迟的一般框架，让慢速推理的控制器提供动作序列以满足控制需求，并在此框架下比较多种强化学习算法，提出RT-HCP算法

Result: RT-HCP算法在性能、样本效率和推理时间之间完成了优秀的平衡，在FURUTA摇摆平台上验证了其优越性

Conclusion: 通过提出的框架和RT-HCP算法，成功解决了模型基于强化学习在机器人控制中的样本效率和推理时间挑战

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [103] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: LR-GWN通过将图小波滤波器分解为局部和全局组件，统一处理短程和长程信息流，在长程图基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习中长程交互建模的挑战，现有基于小波的图神经网络受限于有限阶多项式近似，限制了感受野并阻碍长程传播

Method: 将小波滤波器分解为互补的局部和全局组件：局部聚合使用高效低阶多项式处理，长程交互通过灵活的谱域参数化捕获

Result: 在长程基准测试中达到基于小波方法的最先进性能，同时在短程数据集上保持竞争力

Conclusion: LR-GWN的混合设计在原则性小波框架内统一了短程和长程信息流，有效解决了图长程交互建模问题

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [104] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: 该论文综述了使用深度强化学习(DRL)和直接偏好优化(DPO)技术来微调大型视觉语言模型(LVLMs)，以实现与人类价值观对齐、提升任务性能和实现自适应多模态交互的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模预训练推动了LVLMs的显著进展，但如何将这些模型与人类价值观对齐或适应特定任务和行为仍然是一个关键挑战。需要探索有效的微调范式来提升模型的人类对齐性和任务性能。

Method: 论文探讨了两种主要方法：深度强化学习(DRL)使用奖励信号优化模型行为，而不依赖监督偏好数据；直接偏好优化(DPO)直接使策略与偏好对齐，无需显式奖励模型。

Result: 研究对关键方法进行了分类，分析了偏好数据来源和奖励信号，并提供了DRL和DPO在LVLMs对齐中的应用框架。

Conclusion: DRL和DPO技术为构建强大且与人类对齐的大型视觉语言模型提供了重要途径，但仍面临可扩展性、样本效率、持续学习、泛化性和安全性等开放挑战。

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [105] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 提出异步消息传递框架解决GNN中的过度压缩问题，通过基于节点中心性的批量更新来提升长距离交互性能


<details>
  <summary>Details</summary>
Motivation: GNN存在过度压缩问题，特别是在需要长距离交互的任务中。传统图重连方法会破坏归纳偏置并导致信息损失，而增加通道容量又会增加模型复杂度

Method: 提出模型无关的异步更新框架，基于节点中心性值在每层创建节点批次，只更新批次内节点特征，实现顺序信息处理避免同步压缩

Result: 在6个标准图数据集和2个长距离数据集上测试，在REDDIT-BINARY和Peptides-struct上分别获得5%和4%的性能提升

Conclusion: 异步消息传递框架能有效缓解GNN的过度压缩问题，在保持较低参数复杂度的同时显著提升长距离任务的性能

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [106] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出基于物理信息的正则化损失函数，通过Eikonal偏微分方程在值函数学习中引入几何归纳偏置，提升离线目标条件强化学习的性能和泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在自主导航和运动控制等领域的应用受到状态-动作空间覆盖有限和长时程任务泛化困难的挑战

Method: 基于Eikonal偏微分方程推导物理信息正则化损失，结合分层隐式Q学习算法，形成Pi-HIQL方法

Result: 在性能和泛化方面取得显著提升，特别是在stitching机制和大规模导航任务中表现突出

Conclusion: 物理信息正则化方法为离线GCRL提供了有效的几何归纳偏置，能够显著改善值函数学习效果

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [107] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: 这篇位置论文提出了安全-by-协同进化的新范式，通过R²AI框架结合对知威胁的抵抗力和对未预见风险的恢复力，实现AI安全与能力的协同进化。


<details>
  <summary>Details</summary>
Motivation: 解决AI能力快速增长与安全进展迟滞之间的持续差距，现有的"Make AI Safe"和"Make Safe AI"范式都有其局限性。

Method: 提出R²AI实践框架，整合快速和慢速安全模型、通过安全风洞进行对抗模拟和验证、以及持续反馈循环来导安全与能力协同进化。

Result: 该框架提供了一条可扩展的、主动的路径，在动态环境中维持持续安全，应对近期的脏强性和长期的存在风险。

Conclusion: 安全-by-协同进化作为一种新的"Make Safe AI"形式，受生物免疫力的启发，将安全作为一个动态的、对抗性的、持续的学习过程，为AI向AGI和ASI发展提供了更可靠的安全保障。

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [108] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: FLOQ方法将强化学习中的Q函数参数化为流匹配的向量场，通过数值积分步骤实现迭代计算，相比传统的单块架构能更好地扩展容量，在离线RL基准测试中性能提升近1.8倍


<details>
  <summary>Details</summary>
Motivation: 受现代大规模机器学习技术使用密集监督训练中间计算的启发，研究迭代计算在时序差分方法中的优势，传统TD方法以单块方式表示价值函数而不使用迭代计算

Method: 引入floq方法，使用流匹配技术参数化Q函数为向量场，通过TD学习目标训练该向量场，利用目标向量场进行多步数值积分来引导训练

Result: 在具有挑战性的离线RL基准测试和在线微调任务中，floq将性能提升了近1.8倍，比标准TD学习架构具有更好的容量扩展能力

Conclusion: 迭代计算在价值学习中具有巨大潜力，floq方法通过流匹配和数值积分实现了对Q函数容量的精细控制和扩展

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [109] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair是一个用于评估和验证深度神经网络个体公平性的形式化框架，通过生成公平性特定的路径约束来系统探索DNN行为，提供完整性保证


<details>
  <summary>Details</summary>
Motivation: 为了在关键领域中推进算法公平性，需要一种严谨、系统的方法来测试和验证预训练DNN的公平性

Method: 采用双网络架构，通过适配concolic测试工具PyCT生成公平性特定的路径约束，系统性地探索DNN行为

Result: 在25个基准模型上评估显示，PyFair能有效检测歧视性实例和验证公平性，但复杂模型存在可扩展性挑战

Conclusion: PyFair为预训练DNN的公平性测试和验证提供了系统方法，推动了关键领域的算法公平性发展

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [110] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelSMOTE是一种基于智能体交互的新型过采样方法，通过文化传播模型解决传统过采样技术的局限性，在多个不平衡数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术存在特征独立处理、缺乏相似性控制、样本多样性有限和合成多样性管理不足等问题，需要新的方法来有效解决类别不平衡问题。

Method: 基于Axelrod文化传播模型，提出四项创新：1)基于特征的分组保持相关性；2)相似性概率交换机制；3)Beta分布混合实现真实插值；4)受控多样性注入避免过拟合。

Result: 在8个不平衡数据集上的实验表明，AxelSMOTE在保持计算效率的同时，性能优于最先进的采样方法。

Conclusion: AxelSMOTE通过智能体交互框架有效解决了传统过采样技术的局限性，为类别不平衡问题提供了创新解决方案。

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [111] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: 本文研究了目标数据投毒攻击中不同测试样本的易受攻击性差异，提出了三个预测标准来评估攻击难度，为实践者提供漏洞评估见解。


<details>
  <summary>Details</summary>
Motivation: 目标数据投毒攻击因其部署简单且成功率高而构成严重威胁，但现有研究对攻击难度在不同测试样本间的差异缺乏理解，需要识别影响漏洞的关键特征。

Method: 提出了三个预测标准：遍历预测准确率（通过干净训练动态分析）、毒物距离和毒物预算，用于预测目标投毒攻击的难度变化。

Result: 实验结果表明，这些指标能够有效预测不同场景下真实世界目标投毒攻击的难度变化，在不同情境下都表现出良好的预测性能。

Conclusion: 研究揭示了目标数据投毒攻击难度的可预测性，为实践者提供了评估漏洞和理解数据投毒攻击的重要工具，有助于提高模型安全性。

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [112] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的精强外部分布检测框架，通过结合标签噪声学习和信号处理技术，有效解决了在噪声标签情况下外部分布检测性能治失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的外部分布(OOD)检测方法在噪声训练标签下效果显著降低，而直接结合标签噪声嬉耐方法与OOD检测策略无法有效解决这一挑战，需要原理性的解决方案。

Method: 提出了一种精强的OOD检测框架，将标签噪声学习中的损失美正技术与信号处理中的低秩和稀疏分解方法相结合。

Result: 在合成和真实世界数据集上进行了涉及广泛的实验，结果显示该方法显著超过了最先进的OOD检测技术，尤其在严重噪声标签设置下表现优异。

Conclusion: 该研究成功地解决了标签噪声对OOD检测性能的负面影响，提供了一种有效的精强检测框架，对于安全关键应用中的AI系统具有重要意义。

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [113] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: SEELE是一个新颖的监督辅助强化学习框架，通过动态调整问题难度来提升大型语言模型的推理能力探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在探索效率低下的问题，当问题过于困难时模型无法找到可行的推理路径，而问题过于简单时学习效果有限。

Method: SEELE通过为每个训练样本附加可调节长度的提示（部分解决方案），使用多轮采样策略和项目反应理论模型来动态确定最优提示长度，使问题难度保持在高效学习区间。

Result: 实验结果显示，SEELE在六个数学推理基准测试中平均比GRPO和SFT分别高出11.8和10.5个百分点，比之前最好的监督辅助方法高出3.6个百分点。

Conclusion: SEELE通过实例级实时难度调整，有效解决了RLVR中的探索效率问题，显著提升了语言模型的推理性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [114] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: 提出基于自动微分技术的梯度下降方法，优化中子反射率数据分析，替代传统优化程序和机器学习代理模型，保持物理直觉的同时实现高效分析。


<details>
  <summary>Details</summary>
Motivation: 传统中子反射率数据分析需要解决逆建模问题，对于大量数据或复杂多层结构效率低下。现有机器学习方法虽然快速但失去了物理直觉。

Method: 使用自动微分技术对前向反射模型本身进行梯度下降，精确计算误差函数相对于感兴趣参数的梯度，利用现代优化和推理技术。

Result: 在厚氧化石英薄膜上展示了最先进的性能，在有机LED多层器件的高复杂度场景中表现出稳健的协同拟合性能。

Conclusion: 该方法为中子反射率数据分析提供了高效且物理直观的解决方案，并提供了开源可微分反射率核库，便于梯度方法应用于其他数据集。

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [115] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: 两层神经网络可以学习任意有限群中的词操作，并在学习过程中出现grokking现象。通过将问题重构为学习低秩3-张量，网络能够找到低秩实现方式，使用有限宽度来近似词张量。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何学习有限群中的词操作，特别是理解grokking现象背后的机制，以及网络如何通过低秩分解来实现高效计算。

Method: 将词操作学习问题重构为学习特定的3-张量，证明该张量通常是低秩的。通过分解到群的基本自共轭表示三元组，并利用融合结构来排除许多分量，获得低秩实现。使用替代模型进行分析。

Result: 神经网络能够找到低秩实现或近似，使用有限宽度来泛化地近似词张量。对于简单乘法词，网络有效实现了Strassen意义上的高效矩阵乘法。

Conclusion: 该工作揭示了神经网络在梯度下降下达到此类解决方案的机制，提供了对grokking现象和网络学习群操作能力的理论理解。

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [116] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文通过稀疏自编码器分析transformer模型的概念表示，揭示了输入不确定性增加时模型会激活与输入无关的语义特征导致幻觉，并发现纯噪声输入也能触发有意义的内部概念。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在科学、商业和政府领域的普及，理解其失败模式（如幻觉问题）对于高风险应用中的信任和采用至关重要。

Method: 使用稀疏自编码器捕获预训练transformer模型的概念表示，在输入空间不确定性受控的实验场景下系统分析幻觉产生机制。

Result: 研究发现输入信息越非结构化，模型使用的语义概念数量越多；面对输入不确定性时，模型倾向于激活连贯但输入不敏感的语义特征；纯噪声输入也能触发有意义的内部概念。

Conclusion: 这些发现对AI模型与人类价值观对齐、AI安全、对抗攻击防御以及自动量化模型幻觉风险提供了重要基础。

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


### [117] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 强化学习虽然能提高大语言模型的推理准确性，但会导致生成多样性下降。本文提出基于结果的探索方法，通过历史探索和批次探索两种算法，在保持准确性的同时缓解多样性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 基于结果的强化学习虽然能显著提升大语言模型的推理准确性，但会引发系统性多样性损失，这在实际应用中至关重要，因为多样性对于测试时的扩展性很关键。

Method: 提出基于结果的探索方法：1）历史探索 - 使用UCB风格奖励鼓励罕见答案；2）批次探索 - 惩罚批次内重复以促进测试时多样性。在Llama和Qwen模型上进行数学竞赛标准测试。

Result: 实验表明两种方法都能在提高准确性的同时缓解多样性崩溃问题。理论方面通过新的基于结果的多臂老虎机模型形式化了基于结果探索的益处。

Conclusion: 这些贡献为开发既能增强推理能力又不牺牲可扩展部署所需多样性的强化学习方法指明了实用路径。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [118] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: 这篇论文提出了一个名为Multi-IaC-Bench的标准化测试集，用于评估大语言模型在多种基础设施代码（IaC）格式中的生成和修改能力。研究发现虽然现代LLM能够生成语法正确的IaC，但在语义对齐和处理复杂模式方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 不同云服务提供商使用不同的IaC格式，缺乏标准化格式导致云架构师需要掌握多种语言。虽然大语言模型在自动化IaC创建和维护方面有潜力，但缺乏跨多格式的综合性测试标准限制了进展。

Method: 研究提出了Multi-IaC-Bench测试集，包含AWS CloudFormation、Terraform和CDK格式的三元组数据（初始模板、自然语言修改请求和更新后模板）。通过合成数据生成流程进行严格验证。评估多个最新的LLM模型，并进行消融研究分析提示式工程和重试机制的重要性。

Result: 现代LLM在多种IaC格式中能够达到较高的语法正确率（>95%），但在语义对齐和处理复杂基础设施模式方面仍面临显著挑战。消融研究显示了提示工程和重试机制对成功生成IaC的重要性。

Conclusion: 论文提供了一个标准化的评估框架Multi-IaC-Bench，以促进AI辅助基础设施管理领域的进一步研究，并为这个关键领域建立了标准化的评估指标。

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [119] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: 重新研究分布式计数问题，证明原有半随机协议在适应性攻击下不稳健，并提出了一种更简单且通信复杂度最优的新协议


<details>
  <summary>Details</summary>
Motivation: 解决分布式计数问题中的适应性攻击问题，尝试确定原有简单协议是否已经稳健，并开发更简单且最优的适应性协议

Method: 首先构造明确的适应性攻击来证明Huang等人的协议不稳健，然后设计了一种新的简单协议，达到O(√k/ε log N)的最优通信复杂度

Result: 证明了Huang等人的协议在适应性攻击下失效，并成功开发出一种更简单且通信复杂度最优的新协议，在适应性环境下达到了与偏见环境相同的最优复杂度

Conclusion: 该研究解决了分布式计数问题中的重要问题，证明了简单协议的弱点，并提供了一种新的最优解决方案，对分布式系统的设计有重要意义

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [120] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: DISTRIBUTEDANN是一个分布式向量搜索服务，能够在1000多台机器上搜索500亿向量的图索引，提供26ms中位查询延迟和超过10万QPS，比现有方案效率高6倍


<details>
  <summary>Details</summary>
Motivation: 解决大规模向量搜索系统的扩展性问题，传统分区和路由策略效率低下，需要更高效的分布式架构

Method: 结合分布式键值存储和内存ANN索引两个成熟组件构建分布式向量搜索系统

Result: 成功替换Bing搜索引擎的传统扩展架构，实现26ms中位查询延迟和10万+ QPS处理能力，效率提升6倍

Conclusion: DISTRIBUTEDANN证明了结合成熟组件构建分布式向量搜索系统的可行性，为大规模向量搜索提供了高效解决方案

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [121] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在非顶点传递图中，机器人无法检测多重性的敌对环境下，机器人聚集问题的分布式算法设计与分析。


<details>
  <summary>Details</summary>
Motivation: 传统机器人聚集问题通常假设机器人能够检测多重性且初始配置无重复，但在实际敌对环境中这些假设可能不成立。本文旨在解决在非顶点传递图中，机器人无法检测多重性且初始配置可能包含多重性的聚集问题。

Method: 提出了一种分布式算法，采用轮询调度器（每次激活一个机器人），让机器人在图边上移动，最终聚集到某个顶点。算法针对非顶点传递图设计，其中顶点被划分为至少两个不同的等价类。

Result: 提供了适用于任何机器人配置的解析算法，并证明了其正确性。同时分析了算法的时间复杂度，实现了对非顶点传递图中机器人聚集问题的完整表征。

Conclusion: 该研究成功解决了在敌对环境下非顶点传递图中的机器人聚集问题，为分布式机器人系统在现实复杂环境中的应用提供了理论基础和算法支持。

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [122] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: KaOS是一个基于现成IoT硬件的分布式控制平台，通过容器化和资源管理实现灵活、安全、容错的智能建筑自动化系统


<details>
  <summary>Details</summary>
Motivation: 解决智能建筑自动化系统面临的硬件故障、供应商淘汰、安全威胁等挑战，这些挑战限制了大规模智能自动化部署的可行性

Method: 使用分布式控制平台架构，通过容器化技术和管理资源访问来支持控制应用和分布式系统操作

Result: 初步评估证实了该方法的实际可行性，展示了在长期时间内可持续维护和逐步演进建筑控制功能的潜力

Conclusion: KaOS平台能够在保持成本效益的同时，实现智能建筑自动化系统的灵活性、安全性和容错性，为大规模智能自动化部署提供了可行的解决方案

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [123] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: FineServe是一个用于混合精度大语言模型推理服务的框架，通过KV Slab内存管理和两级调度系统，解决了量化模型的内存碎片和调度效率问题，显著提升了服务吞吐量和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型虽然能提高吞吐量和减少内存使用，但存在KV缓存块大小较小导致的内存碎片问题，以及与非量化模型不同的资源使用模式需要高效调度。

Method: 提出了FineServe框架，包含：(1) KV Slab - 基于模型量化特性的自适应内存管理技术，动态分配KV缓存；(2) 两级调度框架 - 全局调度器根据请求率、延迟SLO和内存约束进行模型放置，本地调度器根据实时请求波动自适应调整批处理大小。

Result: 实验结果表明，FineServe相比最先进的GPU共享系统，实现了高达2.2倍的SLO达成率和1.8倍的token生成吞吐量提升。

Conclusion: FineServe通过创新的内存管理和调度机制，有效解决了混合精度LLM推理服务中的关键挑战，显著提升了服务效率和资源利用率。

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [124] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: MaaSO是首个MaaS编排器，通过分析不同并行策略和批处理大小的实例性能，优化异构实例配置，实现SLO感知请求分发，显著提升SLO满足率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统通常使用相同配置的实例，无法利用不同配置带来的性能差异来满足多样化的SLO需求，存在研究空白。

Method: 包含三个模块：性能分析器（分析不同并行策略和批处理大小的实例性能）、配置优化器（优化异构实例配置）、分发器（SLO感知请求分发和防止级联超时）。

Result: 实验显示MaaSO将SLO满足率提高15-30%，响应延迟降低40-60%，并显著降低整体编排开销。

Conclusion: MaaSO通过利用LLM实例的异构配置能力，有效解决了MaaS平台中多样SLO需求的编排问题，性能显著优于现有方法。

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [125] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: 首个基于内存计算(PIM)的多服务器隐私信息检索方案，通过利用PIM的并行处理和高带宽优势，实现了比CPU基础方案超过3.7倍的查询吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 当前隐私信息检索(PIR)方案存在计算成本高、内存带宽限制的问题，需要扫描大量数据库，在传统CPU架构下性能受限

Method: 设计并实现IM-PIR，一种基于UPMEM PIM商业化内存计算架构的多服务器PIR方案，利用PIM的核心优势：带宽高和并行性强

Result: 评测结果显示，PIM基础的多服务器PIR实现比标准CPU基础方案提高了超过3.7倍的查询吞吐量

Conclusion: 内存计算(PIM)是解决PIR性能瓶颈的有效途径，通过结合PIM的并行处理能力和高带宽特性，可以显著提升隐私信息检索的性能

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [126] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove是一种新颖的区块链扩展方法，通过为每个智能合约使用独立的共识实例，实现并行智能合约支持，无需全局排序。


<details>
  <summary>Details</summary>
Motivation: 解决单体区块链中单一共识机制导致交易总序限制的问题，通过并行化提高区块链性能。

Method: 采用Parallel Optimistic Agreement机制确保并行实例运行时无冲突交易提交，对简单交易使用轻量级Byzantine Reliable Broadcast原语降低延迟。

Result: 在乐观条件下（无恶意行为、网络同步），协议可在创建和执行交易之间实现2个通信步骤的延迟。

Conclusion: Mangrove为构建支持并行智能合约的高性能区块链提供了一种有效的扩展方案，特别在理想网络条件下表现优异。

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [127] [SuperSNN: A Hardware-Aware Framework for Physically Realizable, High-Performance Superconducting Spiking Neural Network Chips](https://arxiv.org/abs/2509.05532)
*Changxu Song,Arda Caliskan,Beyza Zeynep Ucpinar,Yasemin Kopur,Mustafa Altay Karamuftuoglu,Sasan Razmkhah,Shahin Nazarian,Massoud Pedram*

Main category: cs.ET

TL;DR: 超导神经网络实现框架SuperSNN，解决制造约束下的网络设计问题，在3.02GHz频率下实现高准确度和极低能耗


<details>
  <summary>Details</summary>
Motivation: 现有超导神经网络设计忽视了实际制造约束（芯片面积、路由、I/O插针数量），导致网络规模和复杂度受限

Method: 1）硬件感知训练方法，采用离芯剪枝和权重量化 2）设计高扇入神经元和自定义超导单元 3）优化LAGS时钟分配方案管理数据传输延迟

Result: 1）全量化后在MNIST数据集上达到96.47%准确率 2）芯片对数字分类达到80.07%-86.2%准确率 3）运3.02GHz频率运行 4）占面3.4x3.9mm²，包吥5822个约瑟夫结，静态功耗2.15mW，每次推理仅耗6.55fJ

Conclusion: SuperSNN框架成功解决了超导神经网络的实际制造挑战，在严格硬件约束下实现了高性能、高效率的完整网络集成，为超导AI芯片的实用化提供了重要技术基础

Abstract: Despite numerous proposed designs for superconducting neural networks (SNNs),
most have overlooked practical fabrication constraints, leading to
implementations limited to only a few neurons or synapses. Current
superconducting technologies, such as MIT LL SFQ5ee, impose severe limitations
on chip area, routing, and input/output pin counts (e.g., 5x5 mm^2 chip with 40
pins), drastically restricting network size and complexity. These hardware
constraints necessitate a comprehensive framework to tailor network designs for
physical realizability while minimizing accuracy loss. This paper introduces
SuperSNN, a comprehensive framework for the implementation of full
superconducting SNNs on a chip within these constraints. The key technical
contributions include: (1) A hardware-aware training methodology for SNNs,
utilizing off-chip pruning and weight quantization for energy-efficient
superconducting implementations. (2) Design and layout of an inference SNN chip
that incorporates novel high fan-in neurons and custom superconducting cells.
(3) An optimized locally synchronous, globally synchronous (LAGS) clock
distribution scheme for robust circuit implementation and management of data
transfer delays in SFQ SNNs. The main results and findings demonstrate the
effectiveness of the framework: (1) The complete network achieved 96.47%
accuracy on the full MNIST dataset after quantization and pruning. (2) The
fabricated SuperSNN chip successfully classified a reduced set of digits (2, 3,
and 4) with 80.07% accuracy, reaching a maximum of 86.2% accuracy for digits 0,
1, and 2. (3) The chip operates at an ultra-high 3.02 GHz clock frequency. (4)
It occupies a compact area of 3.4 x 3.9 mm^2, incorporates 5,822 Josephson
Junctions, consumes 2.15 mW static power, and has an exceptionally low energy
cost of 6.55 fJ (or 1.31e-6 nJ) per inference.

</details>
