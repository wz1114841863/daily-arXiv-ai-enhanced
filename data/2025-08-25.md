<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.LG](#cs.LG) [Total: 59]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator](https://arxiv.org/abs/2508.16011)
*Chukwufumnanya Ogbogu,Gaurav Narang,Biresh Kumar Joardar,Janardhan Rao Doppa,Krishnendu Chakrabarty,Partha Pratim Pande*

Main category: cs.ET

TL;DR: 提出HePGA：基于3D异构内存计算架构的GNN训练加速器，通过优化不同PIM设备和平面层的映射，实现能效和计算效率的显著提升


<details>
  <summary>Details</summary>
Motivation: 现有各种PIM设备（ReRAM、FeFET、PCM、MRAM、SRAM）各有不同的功耗、延迟、面积和非理想特性权衡，需要异构架构来充分发挥各自优势

Method: 利用3D集成技术构建异构多核架构，结合多种PIM设备；基于GNN层和计算核的独特特性，优化其在不同PIM设备和平面层上的映射

Result: HePGA在能效（TOPS/W）和计算效率（TOPS/mm2）上分别比现有PIM架构提升3.8倍和6.8倍，且不牺牲GNN预测精度

Conclusion: 该架构不仅适用于GNN训练，还可加速新兴Transformer模型的推理，展示了异构PIM架构在AI加速方面的广泛应用前景

Abstract: Processing-In-Memory (PIM) architectures offer a promising approach to
accelerate Graph Neural Network (GNN) training and inference. However, various
PIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each device
offering unique trade-offs in terms of power, latency, area, and
non-idealities. A heterogeneous manycore architecture enabled by 3D integration
can combine multiple PIM devices on a single platform, to enable
energy-efficient and high-performance GNN training. In this work, we propose a
3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA.
We leverage the unique characteristics of GNN layers and associated computing
kernels to optimize their mapping on to different PIM devices as well as planar
tiers. Our experimental analysis shows that HePGA outperforms existing
PIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W)
and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNN
prediction accuracy. Finally, we demonstrate the applicability of HePGA to
accelerate inferencing of emerging transformer models.

</details>


### [2] [Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization](https://arxiv.org/abs/2508.16200)
*Mika Leo Hube,Filip Lemic,Ethungshan Shitiri,Gerard Calvo Bartra,Sergi Abadal,Xavier Costa Pérez*

Main category: cs.ET

TL;DR: 这篇论文探讨了使用Set Transformer架构来改进流动导向定位(FGL)技术，通过处理无序的纳米设备循环时间集合，提高对解剖变异性的适应性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有FGL方案依赖固定拓扑的图模型或手工特征，限制了对解剖变异性的适应能力和可扩展性。

Method: 采用Set Transformer处理无序的纳米设备循环时间集合，并集成CGAN、WGAN、WGAN-GP和CVAE等深度生成模型进行合成数据增强，以提高类别不平衡情况下的验健性。

Result: Set Transformer达到了与图神经网络(GNN)基线相当的分类准确率，同时在解剖变异性方面主动提供了更好的泛化能力。

Conclusion: 该研究高度识别了秩序不变模型和合成数据增强在建立健壮、可扩展的纳米级定位方面的潜力。

Abstract: Flow-guided Localization (FGL) enables the identification of spatial regions
within the human body that contain an event of diagnostic interest. FGL does
that by leveraging the passive movement of energy-constrained nanodevices
circulating through the bloodstream. Existing FGL solutions rely on graph
models with fixed topologies or handcrafted features, which limit their
adaptability to anatomical variability and hinder scalability. In this work, we
explore the use of Set Transformer architectures to address these limitations.
Our formulation treats nanodevices' circulation time reports as unordered sets,
enabling permutation-invariant, variable-length input processing without
relying on spatial priors. To improve robustness under data scarcity and class
imbalance, we integrate synthetic data generation via deep generative models,
including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate
realistic circulation time distributions conditioned on vascular region labels,
and are used to augment the training data. Our results show that the Set
Transformer achieves comparable classification accuracy compared to Graph
Neural Networks (GNN) baselines, while simultaneously providing by-design
improved generalization to anatomical variability. The findings highlight the
potential of permutation-invariant models and synthetic augmentation for robust
and scalable nanoscale localization.

</details>


### [3] [Energy-Information Trade-Off in Self-Directed Channel Memristors](https://arxiv.org/abs/2508.16236)
*Waleed El-Geresy,Dániel Hajtó,György Cserey,Deniz Gündüz*

Main category: cs.ET

TL;DR: 本文研究了自导通道(SDC)忆阻器的能量-信息权衡关系，通过实验建模和生成对抗网络分析，展示了能量消耗与设备有效容量之间的优雅权衡


<details>
  <summary>Details</summary>
Motivation: 理解忆阻器信息存储的本质对于其在新型数据存储和神经形态应用中的使用至关重要，特别是需要研究存储的能量成本以及可用能量对设备信息容量的影响

Method: 通过实验建模设备设置到不同状态所需的能量并评估状态稳定性，采用条件生成对抗网络(cGAN)来表征存储条件分布，从而估计不同存储延迟下的能量-信息曲线

Result: 研究展示了SDC忆阻器在能量消耗与有效容量之间存在优雅的权衡关系，为忆阻器在能量受限应用中的优化使用提供了重要见解

Conclusion: 该方法成功量化了忆阻器的能量-信息权衡特性，为设计高效能存储和神经形态计算系统提供了理论基础和实用工具

Abstract: Understanding the nature of information storage on memristors is vital to
enable their use in novel data storage and neuromorphic applications. One key
consideration in information storage is the energy cost of storage and what
impact the available energy has on the information capacity of the devices. In
this paper, we propose and study an energy-information trade-off for a
particular kind of memristive device - Self-Directed Channel (SDC) memristors.
We perform experiments to model the energy required to set the devices into
various states, as well as assessing the stability of these states over time.
Based on these results, we employ a generative modelling approach, using a
conditional Generative Adversarial Network (cGAN) to characterise the storage
conditional distribution, allowing us to estimate energy-information curves for
a range of storage delays, showing the graceful trade-off between energy
consumed and the effective capacity of the devices.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling](https://arxiv.org/abs/2508.15919)
*Zahra Yousefijamarani,Xinglu Wang,Qian Wang,Morgan Lindsay Heisler,Taha Shabani,Niloofar Gholipour,Parham Yassini,Hong Chang,Kan Chen,Qiantao Zhang,Xiaolong Bai,Jiannan Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: HyperFlexis是一个统一的LLM服务系统，通过算法和系统级创新联合优化调度和扩展，支持多SLO调度、成本效益扩展决策和快速角色转换，显著提升SLO达成率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型服务系统面临请求长度、优先级和阶段特定服务级别目标(SLOs)的高度可变性挑战，需要实时调度、快速经济扩展以及支持并置和分离的Prefill/Decode架构。

Method: 采用多SLO感知调度器进行预算估计和请求优先级排序，支持P/D分离架构的prefill和decode阶段多SLO调度，提出设备到设备(D2D)权重传输机制降低权重加载开销，实现快速扩展和冷启动延迟降低。

Result: 系统实现了高达4.44倍的SLO达成率提升，65.82%的请求延迟降低，权重加载开销降低达19.39倍，与最先进基线相比成本相当。

Conclusion: HyperFlexis通过统一的系统设计和多项优化技术，有效解决了LLM服务系统中的调度和扩展挑战，在保证SLO的同时实现了优异的性能表现和成本效益。

Abstract: Modern large language model (LLM) serving systems face challenges from highly
variable requests with diverse lengths, priorities, and stage-specific
service-level objectives (SLOs). Meeting these requires real-time scheduling,
rapid and cost-effective scaling, and support for both collocated and
disaggregated Prefill/Decode (P/D) architectures.
  We present \textbf{HyperFlexis}, a unified LLM serving system that integrates
algorithmic and system-level innovations to jointly optimize scheduling and
scaling under multiple SLOs. It features a multi-SLO-aware scheduler that
leverages budget estimation and request prioritization to ensure proactive SLO
compliance for both new and ongoing requests. The system supports prefill- and
decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV
cache transfers. It also enables cost-effective scaling decisions,
prefill-decode instance linking during scaling, and rapid P/D role transitions.
To accelerate scaling and reduce cold-start latency, a device-to-device (D2D)
weight transfer mechanism is proposed that lowers weight loading overhead by up
to \textbf{19.39$\times$}. These optimizations allow the system to achieve up
to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request
latency, and cost parity with state-of-the-art baselines. The code will be
released soon.

</details>


### [5] [Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally](https://arxiv.org/abs/2508.16308)
*Jan Bok,Avinandan Das,Anna Gujgiczer,Nikola Jedličková*

Main category: cs.DC

TL;DR: 本文研究了k-部分k-着色问题的计算复杂性，发现当颜色数从k+1减少到k时，问题难度显著增加。在经典计算模型中证明该问题是NP完全的，在分布式LOCAL模型中建立了Ω(n)轮的下界。


<details>
  <summary>Details</summary>
Motivation: Das等人之前研究了k-部分(k+1)-着色问题并提出了高效算法，但对于k-部分k-着色问题的分布式复杂性提出了开放性问题。本文旨在填补这一研究空白。

Method: 通过构造新颖的结构特征来描述"困难实例"，其中部分着色问题退化为正常着色问题。使用复杂的图构造和不可区分性论证来证明下界。

Result: 证明对于任意常数k≥3，判断图是否允许k-部分k-着色是NP完全的。在分布式LOCAL模型中建立了Ω(n)轮的下界，与(k+1)-着色问题的O(log²k·logn)轮算法形成指数级分离。

Conclusion: 颜色数从k+1减少到k会导致问题复杂性发生质变，从多项式时间可解变为NP完全，分布式复杂性也从对数轮变为线性轮，揭示了这一问题的内在难度。

Abstract: We investigate the classical and distributed complexity of \emph{$k$-partial
$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where
each vertex should be colored from the palette $\{1,\ldots,c\} =
\{1,\ldots,k\}$ such that it must have at least $\min\{k, \deg(v)\}$ neighbors
colored differently. Das, Fraigniaud, and Ros{\'{e}}n~[OPODIS 2023] showed that
the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and
distributed algorithms and posed an open problem about the status of the
distributed complexity of $k$-partial $k$-coloring. We show that the problem
becomes significantly harder when the number of colors is reduced from $k+1$ to
$k$ for every constant $k\geq 3$.
  In the classical setting, we prove that deciding whether a graph admits a
$k$-partial $k$-coloring is NP-complete for every constant $k \geq 3$,
revealing a sharp contrast with the linear-time solvable $(k+1)$-color case.
For the distributed LOCAL model, we establish an $\Omega(n)$-round lower bound
for computing $k$-partial $k$-colorings, even when the graph is guaranteed to
be $k$-partial $k$-colorable. This demonstrates an exponential separation from
the $O(\log^2 k \cdot \log n)$-round algorithms known for $(k+1)$-colorings.
  Our results leverage novel structural characterizations of ``hard instances''
where partial coloring reduces to proper coloring, and we construct intricate
graph gadgets to prove lower bounds via indistinguishability arguments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation](https://arxiv.org/abs/2508.15940)
*Ahmed Allam,Youssef Mansour,Mohamed Shalan*

Main category: cs.AR

TL;DR: ASIC-Agent是一个专为数字ASIC设计任务设计的自主系统，通过多智能体架构增强基础LLM，解决了LLM在硬件设计工作流中的执行、调试和记忆限制问题。


<details>
  <summary>Details</summary>
Motivation: LLM在RTL设计中表现出色，但在实际硬件设计工作流中存在无法执行代码、缺乏调试能力和长期记忆等限制，需要专门系统来解决这些问题。

Method: 采用多智能体架构，包含RTL生成、验证、OpenLane硬化和Caravel芯片集成等专门子智能体，在沙盒环境中运行，并利用包含文档、API参考和错误知识的向量数据库。

Result: 基于Claude 4 Sonnet的ASIC-Agent成功自动化了各种复杂度的ASIC设计任务，显著加速了ASIC设计工作流程。

Conclusion: ASIC-Agent展示了在硬件设计任务中应用智能体系统的潜力，为ASIC设计工作流提供了有效的自动化解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
Register Transfer Level (RTL) design, enabling high-quality code generation
from natural language descriptions. However, LLMs alone face significant
limitations in real-world hardware design workflows, including the inability to
execute code, lack of debugging capabilities, and absence of long-term memory.
To address these challenges, we present ASIC-Agent, an autonomous system
designed specifically for digital ASIC design tasks. ASIC-Agent enhances base
LLMs with a multi-agent architecture incorporating specialized sub-agents for
RTL generation, verification, OpenLane hardening, and Caravel chip integration,
all operating within a comprehensive sandbox environment with access to
essential hardware design tools. The system leverages a vector database
containing documentation, API references, error knowledge, and curated insights
from the open-source silicon community. To evaluate ASIC-Agent's performance,
we introduce ASIC-Agent-Bench, the first benchmark specifically designed to
assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with
various base LLMs, providing quantitative comparisons and qualitative insights
into agent behavior across different design scenarios. Our results demonstrate
that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a
broad range of ASIC design tasks spanning varying levels of complexity, showing
the potential of significantly accelerating the ASIC design workflow.

</details>


### [7] [Bare-Metal RISC-V + NVDLA SoC for Efficient Deep Learning Inference](https://arxiv.org/abs/2508.16095)
*Vineet Kumar,Ajay Kumar M,Yike Li,Shreejith Shanker,Deepu John*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新题的SoC架构，通过硬件软件优化结合加速深度学习模型，适用于边缘计算应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决边缘计算中深度学习模型执行速度慢和存储效率低的问题，需要一种高效的硬件软件协同优化方案。

Method: 采用NVDLA加速器与Codasip uRISC-V核心紧密耦合的硬件架构，并通过生成空运行应用代码来避免操作系统开销。

Result: 在AMD ZCU102 FPGA板上进行测试，LeNet-5、ResNet-18和ResNet-50模型的推理时间分别为4.8ms、16.2ms和1.1s（系统时钟100MHz）。

Conclusion: 该方案通过硬件软件紧密结合实现了高效的深度学习模型加速，适合边缘计算应用。

Abstract: This paper presents a novel System-on-Chip (SoC) architecture for
accelerating complex deep learning models for edge computing applications
through a combination of hardware and software optimisations. The hardware
architecture tightly couples the open-source NVIDIA Deep Learning Accelerator
(NVDLA) to a 32-bit, 4-stage pipelined RISC-V core from Codasip called uRISC_V.
To offload the model acceleration in software, our toolflow generates
bare-metal application code (in assembly), overcoming complex OS overheads of
previous works that have explored similar architectures. This tightly coupled
architecture and bare-metal flow leads to improvements in execution speed and
storage efficiency, making it suitable for edge computing solutions. We
evaluate the architecture on AMD's ZCU102 FPGA board using NVDLA-small
configuration and test the flow using LeNet-5, ResNet-18 and ResNet-50 models.
Our results show that these models can perform inference in 4.8 ms, 16.2 ms and
1.1 s respectively, at a system clock frequency of 100 MHz.

</details>


### [8] [Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates](https://arxiv.org/abs/2508.16151)
*Yang Liu,Yi Chen,Yongwei Zhao,Yifan Hao,Zifu Zheng,Weihao Kong,Zhangmai Li,Dongchen Jiang,Ruiyang Xia,Zhihong Ma,Zisheng Liu,Zhaoyong Wan,Yunqi Lu,Ximing Liu,Hongrui Guo,Zhihao Yang,Zhe Wang,Tianrui Ma,Mo Zou,Rui Zhang,Ling Li,Xing Hu,Zidong Du,Zhiwei Xu,Qi Guo,Tianshi Chen,Yunji Chen*

Main category: cs.AR

TL;DR: 本文提出了一种金属嵌入方法的专用语言处理单元(HNLPU)，通过在金属线3D拓扑中硬编码LLM权重参数，实现了112倍光罩成本降低和显著的计算效率提升


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM推理系统日益增长的能耗问题，需要开发专门的语言处理单元(LPU)来提高计算效率，但传统硬编码方法面临光罩成本过高的经济挑战

Method: 采用Metal-Embedding方法，将权重参数嵌入到金属线的3D拓扑结构中，而不是传统的2D硅器件网格，实现了15倍密度提升和光罩同质化

Result: HNLPU实现了249,960 tokens/s的处理速度(比GPU快5,555倍)，36 tokens/J的能效(比GPU高1,047倍)，光罩成本降低112倍，NRE成本降至1.84亿美元

Conclusion: Metal-Embedding方法使HNLPU在经济上可行，相比H100集群实现了8.57倍成本效益和230倍碳足迹减少，为专用LLM推理硬件提供了可行解决方案

Abstract: The rapid advancement of Large Language Models (LLMs) has established
language as a core general-purpose cognitive substrate, driving the demand for
specialized Language Processing Units (LPUs) tailored for LLM inference. To
overcome the growing energy consumption of LLM inference systems, this paper
proposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically
hardwires LLM weight parameters into the computational fabric, achieving
several orders of magnitude computational efficiency improvement by extreme
specialization. However, a significant challenge still lies in the scale of
modern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires
fabricating at least 6 billion dollars of photomask sets, rendering the
straightforward solution economically impractical. Addressing this challenge,
we propose the novel Metal-Embedding methodology. Instead of embedding weights
in a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters
into the 3D topology of metal wires. This brings two benefits: (1) a 15x
increase in density, and (2) 60 out of 70 layers of photomasks are made
homogeneous across chips, including all EUV photomasks. In total,
Metal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring
Engineering (NRE) cost of HNLPU into an economically viable range. Experimental
results show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36
tokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed
rectangular area in a 300 mm wafer), \$184M estimated NRE at 5 nm technology.
Analysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon
footprint reduction compared to H100 clusters, under an annual weight updating
assumption.

</details>


### [9] [RIROS: A Parallel RTL Fault SImulation FRamework with TwO-Dimensional Parallelism and Unified Schedule](https://arxiv.org/abs/2508.16376)
*Jiaping Tang,Jianan Mu,Zizhen Liu,Ge Yu,Tenghui Hua,Bin Sun,Silin Liu,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: 该论文提出了一种二维并行方法RIROS来加速RTL故障模拟，通过结构级和故障级并行组合，实现了7.0到11.0倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着自主驾驶等安全关键应用的发展，芯片功能安全性变得更加重要。RTL故障模拟耗时过长，传统单维并行方法故障传播路径动态性导致任务负载不均表现差。

Method: 提出二维并行方法：结构级并行配合工作盺取机制处理众多低负载任务，故障级并行处理少量高负载任务。同时提出统一计算/全局同步调度方法来消除气泡。实现了RIROS并行RTL故障模拟框架。

Result: 实验结果显示，与最先进的RTL故障模拟和商业工具相比，性能提升了7.0倍和11.0倍。

Conclusion: 该方法有效解决了RTL故障模拟中的任务负载不均表问题，显著提高了模拟效率，为芯片功能安全验证提供了高效的并行加速方案。

Abstract: With the rapid development of safety-critical applications such as autonomous
driving and embodied intelligence, the functional safety of the corresponding
electronic chips becomes more critical. Ensuring chip functional safety
requires performing a large number of time-consuming RTL fault simulations
during the design phase, significantly increasing the verification cycle. To
meet time-to-market demands while ensuring thorough chip verification, parallel
acceleration of RTL fault simulation is necessary. Due to the dynamic nature of
fault propagation paths and varying fault propagation capabilities, task loads
in RTL fault simulation are highly imbalanced, making traditional
singledimension parallel methods, such as structural-level parallelism,
ineffective. Through an analysis of fault propagation paths and task loads, we
identify two types of tasks in RTL fault simulation: tasks that are few in
number but high in load, and tasks that are numerous but low in load. Based on
this insight, we propose a two-dimensional parallel approach that combines
structurallevel and fault-level parallelism to minimize bubbles in RTL fault
simulation. Structural-level parallelism combining with workstealing mechanism
is used to handle the numerous low-load tasks, while fault-level parallelism is
applied to split the high-load tasks. Besides, we deviate from the traditional
serial execution model of computation and global synchronization in RTL
simulation by proposing a unified computation/global synchronization scheduling
approach, which further eliminates bubbles. Finally, we implemented a parallel
RTL fault simulation framework, RIROS. Experimental results show a performance
improvement of 7.0 times and 11.0 times compared to the state-of-the-art RTL
fault simulation and a commercial tool.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [10] [Two-Timescale Dynamic Service Deployment and Task Scheduling with Spatiotemporal Collaboration in Mobile Edge Networks](https://arxiv.org/abs/2508.16293)
*Yang Li,Xing Zhang,Yunji Zhao,Wenbo Wang*

Main category: cs.PF

TL;DR: 提出一个两时间尺度的在线优化框架，通过凸优化和深度强化学习技术协同优化边缘计算中的服务部署和任务调度，显著降低任务处理延迟


<details>
  <summary>Details</summary>
Motivation: 现有边缘计算优化方法未能充分解决跨时空维度的协作问题，难以适应用户需求和系统状态的时空变化特性

Method: 采用两时间尺度交替优化方法：大时间尺度使用多智能体深度强化学习进行服务部署决策，小时间尺度使用凸优化技术进行任务调度决策

Result: 相比基线算法，所提方案实现了更好的延迟性能，同时具有低运行时间和良好的收敛行为

Conclusion: 该两时间尺度优化框架有效解决了边缘计算中的时空协同优化问题，为资源受限的边缘节点提供了高效的协作计算方案

Abstract: Collaborative edge computing addresses the resource constraints of individual
edge nodes by enabling resource sharing and task co-processing across multiple
nodes. To fully leverage the advantages of collaborative edge computing, joint
optimization of service deployment and task scheduling is necessary. Existing
optimization methods insufficiently address the collaboration across spatial
and temporal dimensions, which hinders their adaptability to the
spatiotemporally varying nature of user demands and system states. This paper
focuses on optimizing the expected task processing delay in edge networks. We
propose a two-timescale online optimization framework to jointly determine: i)
service deployment decisions at each large timescale; and ii) task scheduling
decisions at each small timescale. Specifically, the convex optimization
technique is used to solve the task scheduling problem, while a multi-agent
deep reinforcement learning technique is employed for the service deployment
problem. These two methods are combined for spatiotemporal co-optimization
through a two-timescale alternating optimization approach. Compared to the
baseline algorithms, the proposed scheme achieves better delay performance,
while also exhibiting low running time and favorable convergence behavior.

</details>


### [11] [GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM Serving](https://arxiv.org/abs/2508.16449)
*Qunyou Liu,Darong Huang,Marina Zapater,David Atienza*

Main category: cs.PF

TL;DR: GreenLLM是一个SLO感知的服务框架，通过分离prefill和decode阶段的GPU频率控制，显著降低LLM推理的GPU能耗，最高可节省34%能源，同时保持吞吐量且SLO违规率仅增加3.5%


<details>
  <summary>Details</summary>
Motivation: LLM推理存在prefill和decode两个特性不同的阶段，但当前GPU功率管理机制将其统一处理，导致电压频率设置不匹配、队头阻塞和能源浪费问题

Method: 采用基于长度的队列路由请求避免队头阻塞；对prefill阶段建立延迟-功率模型进行队列感知优化；对decode阶段使用轻量级双环控制器精细调整频率

Result: 在阿里云和Azure跟踪重放测试中，相比默认DVFS基线，总能耗降低最高34%，吞吐量无损失，SLO违规率仅增加不到3.5%

Conclusion: GreenLLM通过区分处理LLM推理的两个阶段，实现了显著的能源节省，同时保持了服务质量和性能表现

Abstract: Large Language Models (LLMs) are becoming the backbone of modern cloud
services, yet their inference costs are dominated by GPU energy. Unlike
traditional GPU workloads, LLM inference has two stages with different
characteristics: the prefill phase, which is latency sensitive and scales
quadratically with prompt length, and the decode phase, which progresses token
by token with unpredictable length. Current GPU power governors (for example,
NVIDIA's default) overlook this asymmetry and treat both stages uniformly. The
result is mismatched voltage and frequency settings, head-of-line blocking, and
excessive energy use.
  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU
energy by explicitly separating prefill and decode control. At ingress,
requests are routed into length-based queues so short prompts avoid
head-of-line blocking and TTFT improves. For prefill, GreenLLM collects short
traces on a GPU node, fits compact latency-power models over SM frequency, and
solves a queueing-aware optimization to select energy-minimal clocks per class.
During decode, a lightweight dual-loop controller tracks throughput (tokens per
second) and adjusts frequency with hysteretic, fine-grained steps to hold tail
TBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM
reduces total energy by up to 34 percent versus the default DVFS baseline, with
no loss of throughput and with less than 3.5 percent additional SLO violations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner是一种新颖的后训练剪枝方法，无需重新训练即可在预训练大语言模型中诱导稀疏性，通过结合权重更新幅度和激活模式来更有效地识别和消除冗余参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模不断增大带来了部署、可扩展性和能效方面的挑战，现有剪枝方法要么导致性能显著下降，要么需要计算成本高昂的微调。

Method: Z-Pruner利用权重更新幅度和激活模式来识别冗余参数，是模型无关、高效且易于实现的后训练剪枝方法，无需任何重新训练。

Result: 在LLaMA-2、LLaMA-3和OPT等多种LLM架构上评估显示，Z-Pruner超越了需要密集权重更新的最先进剪枝方法，获得了最低的困惑度分数和最高的零样本准确率平均分数。

Conclusion: Z-Pruner提供了一种有效的大语言模型压缩解决方案，在保持性能的同时显著减少模型大小和推理延迟，代码已开源。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [13] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net是一个用于多模态情感分析的新型深度学习框架，通过渐进式层内融合、自适应门控仲裁和参数高效微调策略，实现了高效且可解释的多模态融合，在MOSI数据集上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态情感分析中深度、动态和可解释的融合问题，同时保持参数效率，特别是在资源有限场景下的应用需求。

Method: 提出渐进式层内融合范式（Cross-Attention机制）、自适应门控仲裁机制和混合参数高效微调策略（LoRA+Post-Fusion Adapters），集成到分层编码器架构中。

Result: 在MOSI数据集上达到MAE 0.691和F1-Score 86.9%的最先进性能，仅需3.09M可训练参数，展现了优异的性能与计算效率平衡。

Conclusion: PGF-Net通过创新的融合机制和参数效率策略，成功实现了深度、动态且可解释的多模态情感分析，为资源受限场景提供了有效的解决方案。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [14] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 提出了一种结合频谱分析和概率预测的简化ECG分割架构，替代复杂的BiLSTM模型，在提高计算效率的同时实现了高精度分割，并引入可解释AI增强模型透明度


<details>
  <summary>Details</summary>
Motivation: 现有ECG分割模型依赖复杂的多层架构如BiLSTM，计算量大且效率低，需要开发更高效且可解释的解决方案

Method: 结合频谱分析和概率预测的简化架构，用简单层替换复杂层来捕捉P、QRS和T波的时空特征，并应用可解释AI(XAI)方法增强模型可解释性

Result: 实现了高分割准确率：QRS波97.00%、T波93.33%、P波96.07%，在提高计算效率的同时保持精确分割

Conclusion: 简化架构不仅提高了计算效率，还提供了精确的分割效果，结合物理AI原理确保了ECG分析的可靠性和透明度，是心脏信号监测的实用有效解决方案

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [15] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出张量并行潜在注意力(TPLA)方案，解决多头潜在注意力(MLA)在张量并行中的效率问题，在保持KV缓存压缩优势的同时实现高效并行计算。


<details>
  <summary>Details</summary>
Motivation: 多头潜在注意力(MLA)将KV状态压缩为低秩潜在向量，但在张量并行中每个设备仍需加载全部缓存，导致优势失效。

Method: 提出TPLA方案：将潜在表示和每个头的输入维度分割到多个设备，每个分片独立执行注意力计算，然后通过all-reduce操作结合结果。使用正交变换如Hadamard变换或PCA减少分片干扰。

Result: 在DeepSeek-V3和Kimi-K2模型上实现了1.79x和1.93x的速度提升，在32K标记上下文长度保持性能，并与FlashAttention-3兼容。

Conclusion: TPLA方案在保持MLA压缩优势的同时，有效解决了张量并行环境下的效率问题，并保持了正确性能。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [16] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 提出基于Transformer的时间序列因果发现框架，通过梯度分析提取因果结构和时滞，结合注意力掩码整合先验知识，显著提升因果发现性能


<details>
  <summary>Details</summary>
Motivation: 解决时间序列因果发现中的两个关键挑战：复杂非线性依赖关系和伪相关性问题

Method: 使用多层Transformer时间序列预测器捕捉长期非线性时序关系，通过梯度分析提取因果图和时滞，引入基于注意力掩码的先验知识整合机制

Result: 在因果发现F1分数上提升12.8%，因果时滞估计准确率达到98.9%，显著优于现有最优方法

Conclusion: 该框架有效解决了复杂非线性时序因果发现问题，通过整合先验知识成功缓解了伪因果关系的影响

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [17] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 本文是一篇关于高维数据降维嵌入方法的综述论文，系统回顾了该领域近年来的发展，提出了最佳实践指南，评估了流行方法，并讨论了该领域的挑战和开放性问题。


<details>
  <summary>Details</summary>
Motivation: 随着高维数据在各领域的广泛应用，降维嵌入算法的需求急剧增长，但该研究领域碎片化严重，缺乏对实践者的明确指导，需要系统性的综述来提升领域的一致性和促进未来发展。

Method: 采用文献综述方法，详细批判性地回顾了近年来的发展，制定了创建和使用低维嵌入的最佳实践清单，并在多个数据集上评估了流行方法。

Result: 提供了该领域的系统性概述，建立了最佳实践指南，并通过实证评估展示了不同方法的性能表现。

Conclusion: 该综述为高维数据降维嵌入领域提供了重要的参考框架，指出了当前的技术挑战和开放性问题，为未来研究和发展方向提供了指导。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [18] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 本文提出GL-GRASP方法，将图表示学习(GRL)与GRASP元启发式算法结合，用于解决约束增量图绘制问题(C-IGDP)，在解质量和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习与元启发式算法结合的方法往往耗时且不如手工设计的启发式算法有竞争力，因此需要探索更高效的学习策略来提取图的潜在结构。

Method: 提出Graph Learning GRASP (GL-GRASP)，在图表示学习(GRL)的基础上改进GRASP算法的构建阶段，使用深度学习为基础的节点嵌入技术。

Result: GL-GRASP在原始积分度量上优于现有文献中的GRASP启发式算法，在固定时间限制下的可扩展性测试中表现出更强的鲁棒性。

Conclusion: 将图表示学习与元启发式算法结合是一种有效的方法，特别是在图可视化问题中，深度学习为基础的嵌入技术能够显著提升算法性能。

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [19] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [20] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 这篇论文研究在套利矢量有序的情况下，分布偏移上下文的机器学习问题，提出了一种自适应的策略，并引入了偏好基于悔弊来评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决在套利矢量有序的情境下，分布偏移对上下文机器学习的挑战，提出能够自动调整适应分布变化的策略。

Method: 提出了一种基于自适应分格化和乐观消除的策略，该策略能够自动调整以适应基础分布的偏移。

Result: 在各种分布偏移假设下，建立了该策略悔弊的上界，这些界限将现有无分布偏移和矢量套利设置的结果进行了推广，并在分布偏移存在时以问题参数的方式渐进地缩放。

Conclusion: 该研究提供了一种有效的方法来处理分布偏移下的上下文机器学习问题，通过偏好基于悔弊来评估性能，并在各种偏移情况下实现了良好的性能。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [21] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 提出了一个改进的平衡传播框架，通过引入中间误差信号解决深度网络中的梯度消失问题，使EP能够训练更深的网络架构，在CIFAR数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统的平衡传播方法在深层网络中面临梯度消失问题，导致能量最小化和梯度计算难以收敛，限制了其在深层架构中的应用。

Method: 提出新颖的EP框架，整合中间误差信号和知识蒸馏技术，增强信息流动和神经元动力学收敛性。

Result: 在CIFAR-10和CIFAR-100数据集上实现了最先进的性能，成功在深度VGG架构上展示了可扩展性。

Conclusion: 这项工作显著提升了平衡传播的可扩展性，为其在现实世界系统中的应用铺平了道路。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [22] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 量子联邦学习(QFL)是分布式量子计算与联邦机器学习的结合，通过量子增强能力实现隐私保护的分散式学习，本文对此进行了全面综述。


<details>
  <summary>Details</summary>
Motivation: 整合量子计算和联邦学习的优势，解决分布式量子系统中高效安全模型训练的挑战，实现隐私保护的量子增强学习能力。

Method: 采用系统性文献综述方法，从概念基础、架构分类、网络拓扑、通信方案、优化技术和安全机制等多个维度分析QFL框架，并探讨各领域应用案例。

Result: 全面梳理了QFL的理论基础、技术架构和应用场景，识别了当前发展现状和关键技术挑战，提供了详细的案例研究和实现平台分析。

Conclusion: QFL作为一个快速发展的新兴领域，在隐私保护分布式量子学习方面具有巨大潜力，但仍面临技术实现、安全性和标准化等多方面挑战，需要进一步研究探索。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [23] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 本文继续发展Cartan神经网络计划，重点研究非缩简对称空间上的数学基础问题，包括分隔器结构、制图群、拉普拉斯绿函数和温核等新表示方法。


<details>
  <summary>Details</summary>
Motivation: 为了在神经网络中引入非缩简对称空间层，通过可解群同态映射连接，并建立Tits Satake向量数据包框架来支撑这一概念。

Method: 使用群论方法构造所有非缩简对称空间的分隔器，研究制图群和正规Fuchsian子群，探索双曲空间上拉普拉斯绿函数和温核的新表示，并通过Abel-Jacobi映射和Siegel谚函数来构造Bolza曲面上的拉普拉斯本征函数。

Result: 完成了所有非缩简对称空间分隔器的群论构造，获得了科属g=3的Fermat四次曲面和g=2的Bolza曲面的包装群，发现了双曲空间上拉普拉斯绿函数和温核的新表示方法。

Conclusion: 本文为Cartan神经网络提供了强大的数学基础，特别是在非缩简对称空间和自守函数理论方面的进展，为下一步实现神经网络层提供了重要的理论支撑。

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [24] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: 提出了PAC-MCoFL框架，使用多智能体强化学习和博弈论解决多服务提供商联邦学习中的非合作问题，通过帕累托优化和风险建模实现资源分配优化


<details>
  <summary>Details</summary>
Motivation: 多服务提供商联邦学习生态系统存在非合作动态问题，隐私约束和竞争利益阻碍了通信和计算资源的集中优化

Method: 整合帕累托行动者-评论家原则和期望回归，设计三元笛卡尔分解机制处理高维动作空间，开发可扩展变体PAC-MCoFL-p降低计算复杂度

Result: 在总奖励和超体积指标上分别实现约5.8%和4.2%的提升，能更有效平衡个体服务提供商和系统性能

Conclusion: 该框架为多服务提供商联邦学习提供了有效的解决方案，具有理论收敛保证并在各种数据异构性下表现优越

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [25] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 这篇论文提出了一种处理时间分布漏淆问题的非稳态判别分析框架，通过将判别分析与状态空间模型结合，在线性-高斯动态下使用Kalman平滑，在非线性/非高斯情况下使用粒子平滑，显著提升了在噪声、缺失数据和类不平衡情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统判别分析假设训练数据同分布，但实际应用中观测数据通常随时间收集且类条件分布会漏淆，这使得稳态分类器变得不可靠。需要一种能够处理时间分布漏淆的方法。

Method: 提出了一种基于模型的框架，将判别分析嵌入状态空间模型中，得到非稳态线性判别分析(NSLDA)和非稳态二次判别分析(NSQDA)。对于线性-高斯动态，使用Kalman平滑处理每时间步多个样本，并提出两种扩展：(i)EM方法聚合估计未知系统参数，(ii)GMM-Kalman方法同时恢复未观测时间标签和参数。对于非线性或非高斯漏淆，使用粒子平滑估计时变类垂心。

Result: 涉及广泛模拟实验，结果显示该方法比稳态LDA、QDA和SVM基线方法有一致性的性能提升，并且对噪声、缺失数据和类不平衡具有稳健性。

Conclusion: 这篇论文为时间分布漏淆下的判别分析建立了一个统一且数据高效的基础框架，通过结合状态空间模型来处理分布漏淆问题，显著提升了在实际应用中的性能和稳健性。

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [26] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 任务算术是一种有效的模型合并技术，本文为其提供了理论解释，证明任务向量与损失梯度等价，单轮微调即可获得良好合并效果


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在模型合并中表现出色，但缺乏对其工作原理的理论解释，需要建立严谨的理论基础

Method: 通过理论分析建立任务向量与损失梯度的等价关系，在标准梯度下降下证明单轮微调的任务向量等于负梯度乘以学习率，并对多轮情况给出近似误差界限

Result: 在7个视觉基准测试中验证理论，证明首轮梯度在范数和方向上主导微调轨迹，单轮微调模型合并效果与完全收敛模型相当

Conclusion: 任务算术可视为近似多任务学习，其有效性源于早期训练动态，单轮微调足以获得良好的模型合并效果

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [27] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于遗传编程的对称相位紧急度函数方法，用于交通信号控制策略，通过共享子树表示转向运动的紧急度，显著提升了传统GP方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗传编程的交通信号控制方法无法一致处理不同交通信号相位的共同交通特征，需要解决这一局限性。

Method: 使用对称相位紧急度函数，基于当前道路条件计算特定相位的紧急度，表示为两个共享子树的聚合，每个子树代表相位中转向运动的紧急度。

Result: 在CityFlow交通模拟器上基于多个真实世界数据集进行实验，结果显示对称紧急度函数表示法相比传统GP表示在多种场景下显著提升了学习到的交通信号控制策略的性能。

Conclusion: 该方法能够演化出有效、人类可理解且易于部署的交通信号控制策略，具有实际应用价值。

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [28] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 该论文主张医疗AI模型必须具备可解释性、可共享性、可重现性和可问责性，提出了替代黑盒模型的透明建模方法，并探讨了生成式AI和协作学习在医疗数据隐私保护下的应用。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的高风险特性要求机器学习模型必须透明可信，黑盒模型虽然准确但缺乏透明性，难以获得医疗信任和监管批准。

Method: 采用内在可解释建模方法（如稀疏核方法、原型学习和深度核模型），结合严格评估、公平性和不确定性量化，以及生成式AI和联邦学习等协作学习范式。

Result: 提出了构建透明、可信、可转化到真实临床环境的医疗AI系统的完整框架和方法论。

Conclusion: 通过重新思考机器学习的基础原则，可以开发出不仅准确而且透明、可信、可转化到现实临床环境的医疗人工智能系统。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [29] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV是一种无需训练的跨层KV缓存压缩方法，通过相邻参数共享和SVD分解实现高效压缩，配合自适应预算分配策略，在多种压缩比下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的KV缓存随序列长度增长而急剧增加，现有跨层共享方法要么需要修改架构重新预训练，要么在高压缩率下性能显著下降。

Method: 利用跨层隐藏状态高相似性，通过奇异值分解(SVD)实现相邻参数权重共享，创建更易合并的潜在KV缓存，并引入基于余弦相似度的自适应预算分配策略。

Result: 在多个骨干模型和基准测试中，该方法在各种压缩比下始终优于现有的低秩和跨层方法，可与其他量化和驱逐方法正交结合，最终实现98%压缩率而无显著性能损失。

Conclusion: CommonKV提供了一种高效且无需训练的KV缓存压缩解决方案，通过参数共享和自适应策略有效解决了内存挑战，具有良好的实用性和扩展性。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [30] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 本文对机器学习在微出行系统中的应用进行了全面综述，包括数据集分析、ML技术及其在需求预测、能源管理和安全等领域的应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 微出行系统已成为城市交通的重要组成部分，但缺乏针对机器学习在微出行中应用问题的专门文献，需要填补这一研究空白。

Method: 收集和分析各种微出行相关数据集，从空间、时间和特征维度进行讨论；详细概述应用于微出行的ML模型，介绍其优势、挑战和具体用例；探索多个ML应用领域。

Result: 提供了微出行领域数据集的特征分析，系统整理了ML技术在该领域的应用现状，识别了现有方法的优势和局限性。

Conclusion: 该综述为未来研究者更好地理解该领域提供了基础，并提出了需要解决的关键问题和未来研究方向，以促进微出行系统的优化和发展。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [31] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 一种无需微调LLM的自适应代理学习方法，通过内存基于线强化学习实现低成本持续适应，在GAIA和DeepResearcher数据集上达到独占龚头的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理方法不能动态适应或需要计算成本高的微调问题，提供一种可扩展、高效的方案来开发能够持续实时学习的通用LLM代理

Method: 提出内存增强马尔可夫决策过程(M-MDP)，配备神经案例选择策略指导行动决策。通过异构记忆存储过去经验，使用内存重写机制根据环境反馈更新策略，通过高效内存读取实现策略改进

Result: 在GAIA验证集上达到87.88% Pass@3，测试集79.40%；在DeepResearcher数据集上达到66.6% F1和80.4% PM，超过最新基于训练的方法，并在分布外任务上提升4.7%-9.6%的绝对收益

Conclusion: 该方法为开发能够持续、实时学习的通用LLM代理提供了可扩展且高效的途径，无需梯度更新，推动机器学习向开放式技能获取和深度研究场景进展

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [32] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 本文发现ODE-based扩散采样中存在collapse errors现象，即采样数据过度集中在局部数据空间，并揭示了其根本原因在于低噪声区域和高噪声区域的score learning存在see-saw效应。


<details>
  <summary>Details</summary>
Motivation: 尽管确定性采样器在扩散模型中广泛应用，但其潜在局限性尚未得到充分探索。本文旨在识别和分析ODE-based扩散采样中未被认识的collapse errors现象。

Method: 引入新的度量指标来量化collapse errors，通过多种设置验证该现象，并分析其根本原因。应用现有的采样、训练和架构技术来实证支持对collapse errors的解释。

Result: 发现collapse errors在各种设置中普遍存在，观察到低噪声区域的score learning对高噪声区域产生负面影响的see-saw效应，这种在高噪声区域的不匹配与确定性采样器的动态特性共同导致了collapse errors。

Conclusion: 这项工作为ODE-based扩散采样中的collapse errors提供了深入的实证证据，强调了需要进一步研究score learning与确定性采样之间相互作用的必要性，这是扩散模型中一个被忽视但基本的重要方面。

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [33] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN是一个基于GNN的时空克里金框架，通过解耦相位模块、动态数据驱动的元数据图建模和对抗迁移学习策略，有效处理时空数据缺失问题，提升推断模式的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前时空克里金模型在确保推断时空模式的有效性和泛化性方面存在困难，特别是在捕捉动态空间依赖性和时间偏移，以及优化未知传感器的泛化能力方面。

Method: 提出STA-GANN框架，包含三个核心组件：(1)解耦相位模块检测和调整时间戳偏移；(2)动态数据驱动的元数据图建模利用时序数据和元数据更新空间关系；(3)对抗迁移学习策略确保模型泛化性。

Result: 在来自四个领域的九个数据集上进行广泛验证，理论和实验结果均表明STA-GANN具有优越性能。

Conclusion: STA-GANN通过创新的图神经网络架构和对抗学习策略，有效解决了时空数据缺失推断中的模式有效性和泛化性问题，为时空克里金任务提供了新的解决方案。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [34] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: SPL-LNS是一种采样增强的神经大邻域搜索求解器，通过局部信息提案和事后重标记方法，有效解决整数线性规划问题，超越了现有神经LNS求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的LNS求解器采用贪婪预测方法，存在局部最优解和样本效率低的问题，需要改进以提升求解效果。

Method: 将LNS建模为随机过程，提出SPL-LNS方法，利用局部信息提案逃离局部最优，并开发事后重标记方法进行高效训练。

Result: 实验结果表明，SPL-LNS在不同规模的各类ILP问题上显著超越了先前的神经LNS求解器。

Conclusion: SPL-LNS通过采样增强和高效训练方法，成功解决了神经LNS求解器的局部最优和样本效率问题，为组合优化提供了更有效的解决方案。

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [35] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 本文提出了一种基于MiniRocket特征提取和线性分类器的MI-EEG分类新方法，在PhysioNet数据集上达到98.63%的准确率，优于深度学习模型且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 脑机接口中MI-EEG信号分类面临非平稳性、时变性和个体差异等挑战，传统方法难以获得高分类精度，需要开发更有效的特征提取和分类方法。

Method: 提出使用Minimally Random Convolutional Kernel Transform (MiniRocket)进行高效特征提取，然后使用线性分类器进行分类。同时提出基于CNN-LSTM的深度学习模型作为基线进行比较。

Result: 在PhysioNet数据集上，MiniRocket方法达到98.63%的平均准确率，CNN-LSTM模型达到98.06%的准确率，表明MiniRocket方法性能更优且计算成本更低。

Conclusion: 所提出的方法能显著提高运动想象EEG信号的分类准确率，为MI-EEG特征提取和分类提供了新的见解，证明了MiniRocket特征提取在脑机接口应用中的有效性。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [36] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM是一种参数规模感知的稀疏微调框架，通过梯度-权重比和熵引导掩码来最大化相对参数规模的更新，在仅更新0.1%参数的情况下，性能超越全参数微调1.6%


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法只关注更新的绝对大小而忽略参数原始规模，导致模型行为变化有限。需要一种能最大化相对参数规模更新的方法

Method: 提出GEM框架：1) 梯度-权重比优先选择更新显著的参数；2) 基于参数值熵自适应确定每层需要微调的参数量

Result: 在GLUE、SuperGLUE、GSM8k和MBPP等任务上，仅更新0.1%参数就实现了比全参数微调高1.6%的准确率

Conclusion: GEM通过参数规模感知和分布敏感的稀疏微调，有效利用计算预算，在极低参数更新下实现优异性能

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [37] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO是一种改进的降维技术，通过两阶段优化同时捕捉数据的局部和全局结构，解决了传统方法在保持数据结构完整性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统降维技术要么专注于保持局部邻域结构，要么专注于保持全局距离结构，但都无法完整保留高维数据的全部结构特征，可能导致分析得出错误结论。

Method: UMATO将UMAP的优化过程分为两个阶段：第一阶段使用代表性点构建骨架布局，第二阶段在保持区域特征的同时投影剩余点。

Result: 定量实验表明UMATO在全局结构保持方面优于包括UMAP在内的广泛使用的降维技术，虽然在局部结构保持方面略有损失，但在可扩展性和对初始化和子采样的稳定性方面表现更佳。

Conclusion: UMATO通过有效平衡局部和全局结构保持，能够生成更忠实的投影，提高了使用降维技术进行可视化分析的可靠性。

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [38] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: PIANO是一个改进的物理信息神经网络框架，通过自回归建模解决时间相关PDE的稳定性问题，显著提升预测精度和稳定性


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在求解时间相关PDE时存在点预测问题，忽略了动力系统的自回归特性，导致不稳定和不准确预测

Method: 重新设计PINNs为自回归框架，通过自监督rollout机制训练，显式地将未来预测条件化于过去状态，同时强制物理约束

Result: 理论分析证明PIANO通过自回归建模实现稳定性，在挑战性时间相关PDE上达到最先进性能，在天气预报中优于现有方法

Conclusion: PIANO框架通过自回归建模有效解决了PINNs的时间不稳定性问题，为时间相关PDE求解提供了更准确稳定的解决方案

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [39] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 基于可解释人工智能的咳嗽声频谱分析框架，通过CNN和遮挡图识别诊断相关频段，能够区分COPD与其他呼吸疾病，提供可解释的频谱标记


<details>
  <summary>Details</summary>
Motivation: 开发一种可解释的人工智能方法来分析慢性呼吸疾病（特别是COPD）相关的咳嗽声频谱特征，为呼吸疾病诊断提供更深入的病理生理学见解

Method: 使用卷积神经网络在咳嗽信号的时频表示上进行训练，利用遮挡图识别频谱图中的诊断相关区域，并将这些区域分解为五个频率子带进行针对性频谱特征提取和分析

Result: 发现不同子带和疾病组之间的频谱模式存在差异，揭示了跨频率谱的互补和补偿趋势。该方法能够基于可解释的频谱标记区分COPD与其他呼吸疾病，以及慢性与非慢性患者组

Conclusion: 该方法为咳嗽声学的潜在病理生理特征提供了深入见解，证明了频率分辨、XAI增强的分析方法在生物医学信号解释和转化性呼吸疾病诊断中的价值

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [40] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 本研究比较了LSTM和Prophet模型在尼日利亚北部空气质量预测中的表现，发现Prophet在季节性数据中表现优异，而LSTM在结构突变数据中更好，挑战了深度学习模型必然优于简单模型的假设。


<details>
  <summary>Details</summary>
Motivation: 解决低资源地区空气质量数据不规律和稀缺的问题，系统比较先进机器学习模型在资源受限环境下的性能表现。

Method: 使用2018-2023年19个州的月度观测数据，评估LSTM网络和Facebook Prophet模型对多种污染物（CO、SO2、SO4）的预测能力。

Result: Prophet模型在季节性和长期趋势主导的时间序列中往往匹配或超越LSTM的准确性，而LSTM在具有突然结构变化的数据集中表现更好。

Conclusion: 研究强调了模型与数据匹配的重要性，支持在资源受限环境中采用上下文敏感、计算效率高的预测方法，而非盲目追求模型复杂性。

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [41] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST是一个用于评估合成表格数据的系统性框架，集成了隐私指标、相似性指标和机器学习效用指标，提供全面的隐私-效用权衡分析


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对合成数据生成的综合评估框架，特别是在平衡隐私保护和数据效用方面存在空白

Method: 开发FEST开源Python库，整合多种隐私指标（攻击型和距离型）以及相似性和机器学习效用指标

Result: 在多个数据集上验证了FEST的有效性，能够有效分析不同合成数据生成模型的隐私-效用权衡

Conclusion: FEST提供了一个系统化的评估框架，有助于更好地理解和优化合成数据生成中的隐私-效用平衡

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [42] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: 提出Chunked Data Shapley (C-DaSh)方法，通过分块处理和优化子集选择来高效计算数据Shapley值，大幅提升计算效率同时保持高质量结果


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模和多样性增加，评估数据质量对机器学习分析至关重要。现有Data Shapley计算方法在大型数据集上面临计算复杂度高的问题，限制了实际应用

Method: 将数据集分块处理，使用优化子集选择和单次随机梯度下降来估计每个数据块的贡献值，显著降低计算时间

Result: 在多样化真实分类和回归任务上，C-DaSh比现有Shapley近似方法在计算效率上提升80-2300倍，同时在检测低质量数据区域方面更准确

Conclusion: C-DaSh方法实现了对大型表格数据集质量的实用化测量，支持分类和回归流程，解决了Data Shapley计算的可扩展性问题

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [43] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 这篇论文对聚合学习中的大语言模型微调进行了综述性调查，提出了基于模型访问和参数效率的分类法，将方法分为白盒、灰盒和黑盒技术，并讨论了黑盒推理API方向的研究趋势。


<details>
  <summary>Details</summary>
Motivation: 解决聚合学习中调整大语言模型时遇到的计算和通信挑战，尤其是在实际场景中模型内部信息访问受限的问题。

Method: 提出了一个双轴分类法：基于模型访问的分类（白盒、灰盒、黑盒）和基于参数效率的优化。评估了各类别中的代表性方法。

Result: 完整地系统化了聚合学习中大语言模型微调的现有研究，为该领域提供了清晰的分类框架。

Conclusion: 黑盒推理API方向是有前景的研究领域，论文指出了未来研究的潜在方向和开放性挑战。

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [44] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 提出了一种学习稀疏二元表示（辅助知识概念）的深度学习模型，用于改进知识追踪和学生建模，无需依赖人工标注的知识概念


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型依赖人工标注的知识概念，但这些标注可能不完整、有错误或过于笼统，限制了模型的准确性和推荐效果

Method: 使用深度学习模型学习练习的稀疏二元表示（辅助知识概念），每个比特表示潜在概念的存否，这些表示可兼容经典模型和现代深度学习架构

Result: 辅助知识概念提高了学生建模的预测性能，增强了基于强化学习的推荐策略和规划方法，在模拟学生环境中显著改善了学习效果

Conclusion: 辅助知识概念能够捕捉超越人工标注的概念结构，有效提升个性化推荐系统的性能，为智能辅导系统提供了更准确的学生知识建模方法

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [45] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE是一个教师-学生框架，通过结构化错误分析和针对性反馈来提升多模态大语言模型的推理能力，相比现有方法在效率和效果上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏结构化框架来分析错误，特别是在多模态大语言模型中，视觉和文本输入的整合增加了复杂性，需要更系统的错误处理机制。

Method: 提出REFINE框架，使用三种结构化查询（Feed-Target、Feed-Check、Feed-Path）来构建针对性反馈，优先处理相关视觉信息、诊断关键失败点并制定纠正措施。

Result: 实验结果显示REFINE实现了显著的速度提升、计算成本降低和成功的泛化能力。

Conclusion: REFINE通过优化结构化反馈检索，在多模态推理方面展现出巨大潜力，提高了推理效率、token使用效率和可扩展性。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [46] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 提出基于意图的威胁评估框架，整合能力和意图分析，通过多任务学习提升空间网络安全性和可靠性评估


<details>
  <summary>Details</summary>
Motivation: 传统方法将可靠性和安全性分开分析会导致对系统特定标准的过拟合，需要更全面的网络物理感知框架

Method: 三步骤框架：1)信号特征提取算法 2)多任务学习架构（可靠性能力评估+意图解析）3)可适配的威胁评估方法

Result: 框架增强了威胁检测和评估的鲁棒性，优于传统顺序方法，能有效处理复杂威胁场景

Conclusion: 该意图驱动的威胁模型为具有新兴链间链路的空间网络提供了有效的威胁评估解决方案

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [47] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: 通过强化学习训练专门化生物医学LLM，在8个生物医学基准上超越大型商业模型，发现单任务训练模型在未见任务上也能泛化


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生物医学推理任务上表现不佳，限制了其在转化医学和生物医学发现中的应用

Method: 创建8个包含30万问答对的生物医学基准数据集，通过基于可验证奖励的强化学习策略对开源LLM进行后训练

Result: 8-32B参数的OwkinZero模型在生物医学基准上显著优于更大的商业LLM，发现单任务训练模型在未见任务上表现出泛化能力

Conclusion: 针对性的强化学习和精心策划的数据可以解锁专门化模型的泛化性能，加速AI驱动的生物医学发现

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [48] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM-VAE的无监督在线学习框架，用于检测供水管网中的管道堵塞和背景泄漏故障，在非平稳条件下表现优异


<details>
  <summary>Details</summary>
Motivation: 供水管网对公共福祉和经济稳定至关重要，但面临管道堵塞和背景泄漏等挑战，且存在数据非平稳性和标记数据有限等操作约束

Method: 结合长短期记忆变分自编码器(LSTM-VAE)与双重漂移检测机制的无监督在线学习框架，具有轻量级、内存高效的设计

Result: 在两个实际供水管网上的实验表明，该方法在检测异常和适应循环漂移方面持续优于强基线方法

Conclusion: 该方法在动态供水管网环境中进行无监督事件检测方面表现出有效性，特别适用于边缘级实时监测

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [49] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: NIAQUE是一个用于概率回归迁移学习的神经网络模型，通过排列不变性实现任意分位数估计，在预训练和微调后能提升回归任务性能


<details>
  <summary>Details</summary>
Motivation: 概率回归领域的迁移学习研究不足，需要开发能够利用预训练提升回归性能的新方法

Method: 提出NIAQUE模型，基于排列不变性设计，先在多样化回归数据集上预训练，然后在特定目标数据集上微调

Result: 在Kaggle竞赛中优于树模型和最新的神经基础模型TabPFN、TabDPT，证明了概率迁移学习的有效性

Conclusion: NIAQUE是一个强大且可扩展的概率回归框架，通过迁移学习显著提升了预测性能

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [50] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille是一个用于轮廓数据学习的深度学习框架，通过复值循环卷积实现旋转和循环位移等变性，并在形状分类、重建和轮廓回归任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 轮廓数据在多个领域普遍存在，如计算机视觉中的物体边界、气象学中的等值线等。这些数据具有旋转和循环位移的特性，因此需要深度学习模型具备相应的等变性。

Method: 使用复值循环卷积实现旋转和循环位移等变性，引入等变非线性、粗化层和全局池化层来获得不变表示用于下游任务。

Result: 在形状分类、重建和轮廓回归等实验中证明了RotaTouille框架的有效性。

Conclusion: RotaTouille框架成功实现了对轮廓数据的旋转和循环位移等变性处理，为相关领域的深度学习应用提供了有效的解决方案。

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [51] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 这篇论文通过分析204个GitHub仓库，研究了13个开源偏见检测API的实际使用情况、用途和开发者面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统在敏感环境中做出生活改变决策，确保这些系统不会对特定群体产生歧视是至关重要的。需要了解开源偏见API的实际使用情况和开发挑战。

Method: 进行定性研究，分析了204个GitHub仓库（来自1885个候选仓库），这些仓库使用了13个专门处理ML偏见的API。

Result: 发现这些API主要用于两种目的：学习和解决实际问题，涵盖17个独特用例。开发者对偏见检测知识掌握不深，遇到许多故障排除问题，经常寻求建议和资源。

Conclusion: 研究结果对未来偏见相关软件工程研究具有重要意义，同时为教育工作者开发更先进的课程提供了指导。

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [52] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 研究大学招生中的公平队列选择问题，比较一次性设置和顺序设置下的招生政策优化方法，分析公平性属性


<details>
  <summary>Details</summary>
Motivation: 解决大学招生中的公平性问题，特别是在未知申请人群的情况下制定公平透明的招生政策

Method: 使用从先前招生周期数据训练得到的人口模型来优化招生政策，比较一次性设置（政策预先固定）和顺序设置（政策可随新数据更新）

Result: 开发了在两种设置下优化招生政策的方法，并分析了相关政策

Conclusion: 该研究为大学招生提供了公平透明的政策制定框架，特别是在面对未知申请人群时具有重要意义

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [53] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级、数据效率高的图神经网络模型，用于预测多种拓扑结构的主动无线电频电路的关键性能指标，在减少训练数据需求的同时显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习代模模型需要大量数据才能在不同拓扑结构上实现良好的泛化能力，而传统的仿真工具计算成本高。因此需要开发一种轻量级且数据效率高的方法来准确预测主动RF电路的性能。

Method: 采用图神经网络(GNN)模型，将电路在器件端子层面建模，以捐据组成结构的对称性和细粒度连接信息。模型包含掩码自回归流(MAF)输出头，以提高对复杂目标分布的建模稳健性。

Result: 实验结果显示高预测准确性，对称均安百分比误差(sMAPE)和均相对误差(MRE)分别为2.40%和2.91%。与之前的方法相比，MRE提高了3.14倍，同时训练样本数量减少2.24倍。

Conclusion: 该方法通过端子级电路转图和能够建模复杂密度分布的ML架构，显著提高了RF电路设计自动化的速度和准确性，为快速准确的RF电路设计提供了有效解决方案。

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [54] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: 提出了Doctor方法，通过双重检查机制改进离线强化学习中的目标回报对齐问题，实现更精确的策略性能控制


<details>
  <summary>Details</summary>
Motivation: 现有RvS方法如Decision Transformer在目标回报对齐方面存在不足，特别是在数据集未充分覆盖的回报区间内插值或外推时，无法可靠地将实际回报与指定目标对齐

Method: Doctor方法采用双重检查机制，通过目标对齐来增强Transformer模型，在数据集内外都能实现优越的目标对齐性能

Result: 在动态治疗机制基准测试EpiCare上，该方法能有效调节治疗策略的激进程度，平衡治疗效果回报与不良事件风险

Conclusion: Doctor方法解决了离线强化学习中目标回报对齐的关键问题，为需要精确控制策略性能的实际应用提供了有效解决方案

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [55] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 研究评估了大型语言模型（Claude、DeepSeek、ChatGPT）根据自然语言规则实现棋盘游戏代码的能力，最佳模型Claude 3.7 Sonnet实现了55.6%的无错误游戏生成。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够从自然语言描述中生成棋盘游戏的数字版本代码，为快速棋盘游戏代码生成建立LLM辅助框架。

Method: 使用三个先进LLM（Claude、DeepSeek、ChatGPT）对12款流行和冷门游戏进行编码测试，在自由形式和Boardwalk API两种环境下实现，并通过匿名化避免预训练知识影响。

Result: 方法可行，最佳模型Claude 3.7 Sonnet产生55.6%无错误游戏。API合规性增加错误频率，但错误严重性主要取决于LLM本身。

Conclusion: LLM在棋盘游戏代码生成方面具有潜力，为创建集成框架使棋盘游戏开发更加便捷奠定了基础。

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [56] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA是一种针对噪声、稀疏和稀缺数据的多目标贝叶斯优化算法，通过整合实验不确定性先验知识和信任区域策略，在有限实验预算下高效收敛到帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化在处理具有实验不确定性、稀疏且观测数据稀缺的问题时表现不佳，导致实验资源分配效率低下和次优设计。

Method: 提出NOSTRA算法框架，整合实验不确定性的先验知识构建更精确的代理模型，并采用信任区域策略将采样集中在设计空间的有希望区域。

Result: 通过两个具有不同实验不确定性水平的测试函数验证，NOSTRA在处理噪声、稀疏和稀缺数据方面优于现有方法，能有效优先采样提升帕累托前沿精度的区域。

Conclusion: NOSTRA提供了一种资源高效的算法，在有限实验预算的实际场景中确保高效性能，加速收敛到帕累托前沿并提高解决方案质量。

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [57] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 提出了BAD-ACTS基准测试，用于评估LLM智能体系统在遭受攻击时的安全性，发现单个恶意智能体就能显著影响系统安全，并提出基于消息监控的防御策略。


<details>
  <summary>Details</summary>
Motivation: 确保智能体系统的安全使用需要全面了解这些系统在遭受攻击时可能表现出的恶意行为范围，因此需要系统性地评估LLM智能体系统在攻击下的鲁棒性。

Method: 提出了针对智能体系统的新型危害分类法和BAD-ACTS基准测试，包含4个不同应用环境的智能体系统实现和188个高质量有害行为示例，通过控制系统中一个智能体来攻击其他智能体。

Result: 攻击成功率很高，表明即使系统中只有一个对抗性智能体也能对安全性产生显著影响，即使使用简单的基于提示的防御策略，攻击仍然有效。

Conclusion: BAD-ACTS基准为智能体系统的安全研究提供了多样化的测试平台，提出的基于消息监控的防御策略更为有效，有助于提升智能体系统的安全性。

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [58] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出了FraPPE算法，通过高效求解下界中的最小化和最大化问题，解决了基于偏好的纯探索问题，显著加速了计算并达到最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的PrePEx算法在任意偏好锥下无法高效跟踪理论下界，需要开发计算效率高的最优算法。

Method: 推导下界的三个结构特性来简化最小化问题，使用Frank-Wolfe优化器加速最大化问题，实现O(KL²)时间复杂度的求解。

Result: FraPPE算法在合成和真实数据集上都实现了最低的样本复杂度来精确识别帕累托最优集。

Conclusion: FraPPE算法成功填补了计算效率空白，为基于偏好的多目标赌博机问题提供了最优且高效的解决方案。

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [59] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine是一种模型无关的即插即用后处理方法，通过结合基础回归器输出和基于排名的估计来提升回归精度，特别适用于数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习回归器在标签充足时表现优异，但在数据稀缺时精度下降。需要一种方法能够利用专家知识（如成对排序）来提升回归性能，而无需重新训练模型。

Method: RankRefine通过逆方差加权将基础回归器的输出与基于排名的估计相结合。给定查询项和具有已知属性的小参考集，该方法利用成对排序信息进行精炼，无需重新训练。

Result: 在分子属性预测任务中，仅使用20个通过通用大语言模型获得的成对比较，RankRefine实现了高达10%的平均绝对误差相对减少。

Conclusion: RankRefine提供了一种实用且广泛适用的方法，能够利用人类专家或通用LLM提供的排序信息来改进回归性能，特别是在低数据设置中。

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [60] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 该论文提出了在现实世界约束下进行零样本强化学习的方法，解决了数据质量、可观测性和数据可用性三大挑战


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多问题无法通过廉价模拟获得数据，导致训练环境与部署环境存在不可避免的错位，需要开发能够在零样本条件下泛化的强化学习方法

Method: 提出了一套在数据质量约束（数据集小且同质）、可观测性约束（状态、动态和奖励部分可观测）和数据可用性约束（无法假设先验数据访问）下执行零样本强化学习的方法

Result: 通过一系列实证研究揭示了现有方法的缺陷，并验证了所提出技术的有效性

Conclusion: 这些设计使强化学习方法向能够部署解决现实世界问题的目标迈进了一步

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [61] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: 提出了MuST2-Learn框架，通过多视角时空类型学习来预测市政服务请求的处理时间，解决了动态时空相关性、异质服务类型交互和服务时间高变异性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的市政311系统缺乏服务请求处理时间的透明度，导致居民满意度降低和重复查询增加。预测服务时间面临动态时空相关性、异质服务类型交互和同类型内服务时间高变异性的复杂挑战。

Method: 提出多视角时空类型学习框架(MuST2-Learn)，包含类型间编码器捕获异质服务类型关系、类型内变异编码器建模同类型服务时间变异，以及时空编码器捕获每种请求类型的时空相关性。

Result: 在两个真实数据集上的实验表明，MuST2-Learn将平均绝对误差降低了至少32.5%，优于现有最先进方法。

Conclusion: MuST2-Learn框架通过联合建模空间、时间和服务类型维度，有效解决了市政服务请求时间预测的复杂挑战，显著提升了预测精度。

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [62] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES框架系统评估数学推理数据合成策略，发现增加问题复杂度的数据代理效果最佳，问题覆盖率比解决方案可靠性更重要，并开发出在多个数学基准上表现优异的新数据集


<details>
  <summary>Details</summary>
Motivation: 现有改进LLM数学推理的合成数据研究采用不同实验设置，难以比较数据合成策略的效果，需要系统研究影响合成数学推理数据性能的各种因素

Method: 提出FLAMES框架，系统研究10种现有数据合成策略和多个影响因素，设计两种新的数据合成策略用于提升跨域泛化和鲁棒性，开发FLAMES数据集

Result: FLAMES数据集在OlympiadBench(+15.7)、CollegeMath(+4.5)、GSMPlus(+6.5)和MATH(+3.1)上超越公开数据集，微调Qwen2.5-Math-7B达到81.4%的MATH准确率，超过Llama3 405B、GPT-4o和Claude 3.5 Sonnet

Conclusion: 增加问题复杂度的数据代理最有效，问题覆盖率比解决方案可靠性更重要，GSM8K和MATH合成数据可实现从易到难的泛化，FLAMES框架为数学推理数据合成提供了系统评估方法

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [63] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF是一个结合强化学习和物理反馈的3D分子生成框架，通过力场评估奖励函数指导扩散模型生成更稳定的分子结构


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成3D分子结构时难以保证物理合理性，特别是力场一致性方面存在不足，需要引入物理反馈机制

Method: 将3D分子生成建模为马尔可夫决策过程，使用近端策略优化微调等变扩散模型，并引入基于力场评估的奖励函数提供物理反馈

Result: 在QM9和GEOM-drug数据集上，RLPF显著提高了分子稳定性，优于现有方法

Conclusion: 将物理反馈融入生成模型具有重要价值，RLPF框架为生成物理合理的3D分子结构提供了有效解决方案

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [64] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 提出了PSD算法用于逃离非凸优化中的严格鞍点，具有显式常数和梯度下降/鞍点逃离阶段的明确分离，在理论和实验上都验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 非凸优化中的鞍点问题是影响收敛到良好局部最优解的主要障碍，现有方法在理论分析和实际性能之间存在差距，需要更严格的算法设计和分析。

Method: 提出了扰动鞍点逃离下降（PSD）算法，包含梯度下降阶段和鞍点逃离阶段，使用有限差分变体PSD-Probe和随机扩展PSGD，具有明确的常数和阶段分离。

Result: PSD算法以高概率找到(ε,√(ρε))-近似二阶稳定点，梯度评估次数为O(ℓΔ_f/ε²)加上每次逃离事件的O((ℓ/√(ρε))log(d/δ))次评估，最多需要O(ℓΔ_f/ε²)次逃离事件。

Conclusion: 该算法在理论和实验上都表现出色，验证了对数维度依赖性和预测的每次事件函数下降，为非凸优化中的鞍点逃离问题提供了完整的算法规范和理论保证。

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [65] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: 本文提出了一种使基于深度学习的太阳风暴预测模型可解释的方法，通过LSTM网络和注意力机制来预测太阳活动区是否会产生与耀斑相关的CME，并利用模型无关的后处理技术来解释模型预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常被视为黑盒模型，其内部工作机制对用户不透明。由于缺乏透明度，很难理解模型预测背后的推理过程，特别是在太阳风暴预测这样的关键应用中。

Method: 使用基于长短期记忆（LSTM）网络和注意力机制的深度学习模型，将太阳活动区的数据样本建模为时间序列以捕捉时间动态。采用模型无关的后处理技术来解释模型预测，阐明输入序列中影响预测结果的因素。

Result: 成功实现了对LSTM太阳风暴预测模型的可解释性分析，能够解释单个输入序列的预测结果，并提供对活动区内多个序列模型行为的深入理解。

Conclusion: 这是首次为基于LSTM的太阳风暴预测模型添加可解释性，使模型的预测更加可靠和可问责，为太阳物理学研究提供了重要的分析工具。

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [66] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: RL微调可以恢复SFT导致的OOD性能损失，主要通过修正奇异向量方向偏移而非寻找新解决方案，低秩和浅层恢复策略可有效恢复70-80%性能


<details>
  <summary>Details</summary>
Motivation: 研究SFT和RL-FT两个后训练阶段如何重塑模型表示和OOD性能，特别是RL-FT能否恢复SFT导致的性能损失

Method: 使用24点卡游戏的OOD变体和基于频谱的诊断方法，分析奇异向量方向偏移和奇异值变化对性能的影响

Result: RL-FT可显著恢复SFT导致的OOD性能损失（如Llama-11B从8.97%到15.38%），但无法完全恢复严重过拟合的情况；方向偏移比奇异值幅度更重要；低秩和浅层恢复策略有效

Conclusion: RL主要通过对抗SFT诱导的方向漂移来提升OOD性能，而非发现新解决方案；频谱感知分析为实践者提供了低成本恢复策略

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [67] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于TinyML的工业过程监测方案，通过8位卷积神经网络在微控制器上实现了100%测试准确率的切削过程质量监测。


<details>
  <summary>Details</summary>
Motivation: 为了在工业4.0背景下对长期使用的工业机器进行过程监测功能改造，利用TinyML框架实现无线监测系统的智能化升级。

Method: 开发了完整的TinyML流程，包括数据集生成、机器学习模型开发，以及在微控制器上实现和评估预处理与分类流水线，使用8位量化卷积神经网络模型。

Result: 在ARM Cortex M4F微控制器上达到了100.0%的测试准确率，推理时间15.4ms，每次量化CNN推理消耗1.462mJ能量，模型参数存储仅需12.59kiB。

Conclusion: 证明了TinyML系统在结构集成式过程质量监测中的可行性，为未来的TinyML过程监测解决方案提供了参考。

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [68] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 研究表明稀疏自编码器(SAE)的L0超参数对特征学习至关重要，过高或过低都会导致特征混合，需要精确设置才能正确提取LLM的底层特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究将L0视为自由参数，通过稀疏性-重构权衡曲线比较SAE算法，但缺乏对L0设置如何影响特征学习质量的深入分析。

Method: 研究BatchTopK SAE中L0的影响，开发确定正确L0值的方法，在玩具模型和LLM中进行验证，并与稀疏探测性能峰值对比。

Result: 发现L0设置不当会导致特征混合：过低时SAE会混合相关特征以改善重构，过高时会产生退化解。大多数常用SAE的L0设置过低。

Conclusion: 为了训练具有正确特征的SAE，必须精确设置L0超参数，这是获得LLM真实底层特征表示的关键因素。

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [69] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 基础模型在私密敏感应用中的聚合学习适配问题，提出PSSFL框架和FedMox方案解决边缘设备计算资源和标签数据稀缺挑战


<details>
  <summary>Details</summary>
Motivation: 基础模型需要适配下游任务，但私密规定限制了云端访问私有数据，而现有聚合学习方法忽视了边缘设备的计算资源和标签数据稀缺问题

Method: 提出PSSFL框架，边缘设备持有未标注低分辨率数据，服务器有限标注高分辨率数据；设计FedMox框架，采用稀疏专家混合结构，通过空间路由器对齐不同分辨率特征，Soft-Mixture策略稳定半监督学习

Result: 在自动驾驶数据集上的实验表明，FedMox能够在限制边缘设备内存成本的情况下，显著提升基础模型在PSSFL中的适配性能

Conclusion: 该工作为聚合场景下的可扩展和私密保护基础模型适配探索了新路径

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [70] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 比较了儿童语音识别中从头训练与微调成人ASR模型的性能差异，发现SSL表示存在成人语音偏见，从头训练可缓解此问题，模型规模在1B参数内持续改进，开源模型更适合儿童语音研究


<details>
  <summary>Details</summary>
Motivation: 儿童语音识别面临声学多变性和标注数据有限的问题，现有研究主要关注在成人ASR模型上微调，而从头训练的比较研究不足，需要系统评估不同训练策略的效果

Method: 使用ESPnet框架，在多个数据集上比较从头训练方法，评估不同SSL表示（WavLM、XEUS）和解码器架构，分析模型规模缩放效应（最高达1B参数），并进行年龄相关的ASR和说话人验证分析

Result: SSL表示存在对成人语音的偏见，从头训练儿童语音可缓解这种偏见；模型性能在1B参数内持续提升，之后趋于平稳；专有模型（如Whisper）在儿童语音处理上存在局限性

Conclusion: 开源数据模型对于可靠的儿童语音研究至关重要，公开可用的基准测试为鲁棒儿童语音处理提供了训练策略的深入见解，从头训练是缓解SSL成人偏见的有效方法

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>
