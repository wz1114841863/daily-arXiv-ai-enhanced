<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.LG](#cs.LG) [Total: 56]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10](https://arxiv.org/abs/2601.16032)
*Yifan Zhu,Yekai Pan,Chen Ding*

Main category: cs.PF

TL;DR: 本文分析了CuTile-based Flash Attention的内存行为，提出Sawtooth Wavefront Reordering技术来减少L2缓存缺失，在GB10上实现L2缺失减少50%以上，吞吐量提升最高60%。


<details>
  <summary>Details</summary>
Motivation: 高性能注意力核对于大语言模型至关重要。现有CuTile-based Flash Attention在L2缓存方面存在性能瓶颈，特别是在NVIDIA GB10平台上，需要优化其缓存性能。

Method: 首先分析CuTile-based Flash Attention的内存行为，识别L2缓存缺失的主要原因。基于此洞察，提出Sawtooth Wavefront Reordering编程技术来重新组织计算顺序，减少缓存冲突。

Result: 在CUDA和CuTile两种实现中验证，观察到L2缓存缺失减少50%或更多，在GB10平台上吞吐量提升最高达到60%。

Conclusion: 通过深入分析内存行为并提出Sawtooth Wavefront Reordering技术，显著改善了Flash Attention的缓存性能，为大语言模型的高效计算提供了重要优化。

Abstract: High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to 60\% increase in throughput on GB10.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [2] [Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems](https://arxiv.org/abs/2601.15561)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.ET

TL;DR: 本文批判性研究了基于概率比特的模拟退火算法(pSA)在大规模组合优化问题中的局限性，提出了两种新算法TApSA和SpSA，通过部分停用p比特解决了能量停滞问题，在16个基准测试中相比传统pSA将归一化割值平均提高了0.8%到98.4%。


<details>
  <summary>Details</summary>
Motivation: 传统基于概率比特的模拟退火算法(pSA)在解决大规模组合优化问题时存在局限性，主要问题是p比特之间的意外振荡阻碍了伊辛模型的能量降低，导致算法在复杂任务中无法成功执行。

Method: 通过详细模拟分析pSA过程中的能量停滞根源，发现反馈机制是导致破坏性振荡的主要原因。提出两种新算法：时间平均pSA(TApSA)和停滞pSA(SpSA)，基于部分停用p比特的设计原理，在Python模拟中使用最大割基准测试进行验证。

Result: 在包含800到5,000个节点的16个基准测试中，提出的TApSA和SpSA方法相比传统pSA算法，将归一化割值平均提高了0.8%到98.4%。

Conclusion: pSA算法中的反馈机制会导致破坏性振荡，阻碍能量降低。通过部分停用p比特的TApSA和SpSA算法能有效解决这一问题，显著提升大规模组合优化问题的求解性能。

Abstract: This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.

</details>


### [3] [Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload](https://arxiv.org/abs/2601.16169)
*Robert Walkup,Juha Jäykkä,Igor Pasichnyk,Zachary Streeter,Kasia Świrydowicz,Mikko Tukiainen,Yasuko Eckert,Luke Bertels,Daniel Claudino,Peter Groszkowski,Travis S. Humble,Constantinos Evangelinos,Javier Robledo-Moreno,William Kirby,Antonio Mezzacapo,Antonio Córcoles,Seetharami Seelam*

Main category: cs.ET

TL;DR: 该论文提出了一种在混合量子-HPC系统中使用GPU加速Davidson对角化算法的方法，将经典处理时间从小时级缩短到分钟级，实现了100倍的节点级性能提升。


<details>
  <summary>Details</summary>
Motivation: 在混合量子-HPC算法中，对角化是经典计算机最耗时的任务。之前的研究使用CPU超级计算机，但GPU具有大规模线程级并行性，更适合对角化算法，需要开发高效、可扩展且可移植的GPU加速对角化方案。

Method: 采用基于样本的量子对角化（SQD）混合方法，使用Davidson算法计算基态能量和波函数。重点开发了异构系统的GPU加速对角化策略，包括卸载策略、代码转换和数据移动优化，并在Frontier超级计算机等六个GPU加速系统上进行测试。

Result: GPU实现了节点级约100倍的性能提升，将经典对角化处理时间从小时级缩短到分钟级，显著加速了基态和激发态能量的提取过程。

Conclusion: GPU作为主要计算引擎，为混合量子-HPC算法中的对角化任务提供了卓越的性能加速，使SQD方法更加实用，推动了量子计算与经典HPC系统的有效集成。

Abstract: Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.
  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: ELILLM是一个增强LLM在基于结构的药物设计能力的框架，通过编码、潜在空间探索和解码流程，结合贝叶斯优化和知识引导解码，生成化学有效且合成合理的分子。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具有强大的表示和推理能力，但在基于结构的药物设计中应用有限，主要因为对蛋白质结构理解不足和分子生成不可预测。

Method: 提出ELILLM框架，将LLM生成过程重新解释为编码、潜在空间探索和解码流程。使用贝叶斯优化指导潜在嵌入的系统探索，位置感知代理模型预测结合亲和力分布，知识引导解码减少随机性并施加化学有效性约束。

Result: 在CrossDocked2020基准测试中，ELILLM表现出强大的受控探索能力和高结合亲和力分数，优于七种基线方法。

Conclusion: ELILLM能够有效增强LLM在基于结构的药物设计中的能力，通过系统探索和知识引导解码生成化学有效且合成合理的分子。

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [5] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在不同语言间存在回答质量差异，低资源语言获得较低质量回答，且语言选择显著影响回答中的文化背景信息。


<details>
  <summary>Details</summary>
Motivation: 研究动机是确保用户不会因使用不同语言与LLMs交互而受到系统性不利影响，需要评估不同语言用户是否能获得相似质量的回答，以及语言选择如何影响回答中的文化背景信息。

Method: 方法包括：1)基于WildChat数据集创建真实世界开放式问题集；2)使用LLM-as-a-Judge评估回答质量和文化背景；3)在CulturalBench基准的翻译子集上进行多语言评估。

Result: 评估结果显示：1)LLMs对低资源语言的开放式问题提供较低质量回答；2)语言显著影响模型使用的文化背景；3)文化背景差异影响下游回答质量。

Conclusion: 结论是LLMs存在语言不平等问题，低资源语言用户获得较低质量回答，且语言选择影响文化背景信息，这可能导致系统性偏见和不公平。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [6] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 提出一种在因果token-choice MoE中实现数据稀疏性的方法，通过引入零计算(null)专家，让token可以路由到这些不消耗计算资源的专家，从而在保持因果性的同时获得数据稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有的专家选择路由虽然能实现数据稀疏性，但在自回归模型中违反因果性，导致训练-推理不匹配。需要一种既能保持因果性又能实现数据稀疏性的方法。

Method: 在路由池中引入零计算(null)专家，当token路由到null专家时，这些槽位不消耗计算资源。通过标准的负载均衡目标训练模型均匀使用所有专家（包括null专家），从而在期望上实现数据稀疏性而不违反因果性。

Result: 在视觉语言模型训练中，组合权重稀疏性和数据稀疏性比单独使用权重稀疏性在相同预期FLOPs下获得更高效的计算前沿，训练损失和下游性能都有提升。模型学习到隐式的模态感知分配，视觉token比文本token更积极地路由到null专家。

Conclusion: 通过引入null专家，可以在因果token-choice MoE中实现数据稀疏性，避免因果性违反，在视觉语言模型等数据异构场景中显著提升计算效率。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [7] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT是一种基于熵最优传输的广义注意力机制，通过可学习的连续先验替代标准注意力中的隐含均匀先验，解决了注意力下沉问题并改善了长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制存在隐含的均匀先验假设，这限制了其表达能力并导致注意力下沉问题。作者希望建立一个更通用的注意力框架，能够学习更合理的先验分布，同时保持与优化内核的兼容性。

Method: 从熵最优传输的角度重新审视注意力机制，提出GOAT（广义最优传输注意力），用可学习的连续先验替代标准注意力中的隐含均匀先验。该方法保持与FlashAttention等优化内核的完全兼容性，并将空间信息吸收到核心注意力计算中。

Result: GOAT提供了注意力下沉的EOT解释并实现了解决方案，避免了标准注意力的表示权衡。通过学习可外推的先验，GOAT结合了学习位置嵌入的灵活性和固定编码的长度泛化能力。

Conclusion: 通过熵最优传输框架，GOAT实现了更通用的注意力机制，解决了标准注意力的局限性，在保持计算效率的同时提升了模型的表达能力和泛化性能。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [8] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: FedUMM：一个用于非IID多模态数据的统一多模态模型联邦学习框架，通过LoRA适配器实现参数高效微调，大幅降低通信成本


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型通常在集中式设置中训练，限制了在隐私敏感和地理分布式场景中的部署。需要开发能够在非IID多模态数据下工作的联邦学习框架

Method: 基于NVIDIA FLARE构建FedUMM框架，使用BLIP3o骨干网络，通过参数高效微调（LoRA适配器）实现联邦学习：客户端训练轻量级LoRA适配器并冻结基础模型，服务器仅聚合适配器更新

Result: 在VQA v2和GenEval组合生成基准测试中，随着客户端数量和异构性增加，性能略有下降，但仍与集中式训练竞争。适配器联邦使每轮通信量比全微调减少一个数量级以上

Conclusion: FedUMM为隐私保护的联邦统一多模态模型研究提供了实证经验，展示了在非IID多模态数据下实现实用联邦UMM训练的可行性

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [9] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: 提出一种基于注意力嵌入的代理辅助多目标贝叶斯优化框架，用于HPC调度中自动选择最优节点数量，平衡运行时间和功耗。


<details>
  <summary>Details</summary>
Motivation: HPC调度器需要在用户性能和设施资源约束之间取得平衡，核心挑战是为给定作业选择最优节点数量。传统方法难以有效捕捉性能动态并平衡多个目标。

Method: 采用代理辅助多目标贝叶斯优化框架，使用基于注意力的作业遥测嵌入来构建代理模型，比标准回归技术更有效捕捉性能动态，并配合智能样本采集策略确保数据效率。

Result: 在两个生产HPC数据集上，嵌入方法相比基线始终识别出更高质量的运行时间-功耗权衡帕累托前沿，智能数据采样策略大幅降低训练成本并提高结果稳定性。

Conclusion: 这是首个成功将嵌入信息代理应用于HPC调度问题的MOBO框架，能够联合优化生产工作负载的性能和功耗，为自动化HPC调度决策提供了有效方法。

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [10] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops：一种通过数据集-模型协同进化的迭代框架，利用Ambient Diffusion技术逐步提升数据集质量，从而改善扩散模型的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含质量差异很大的样本，直接在这样异质的数据上训练通常会产生次优模型。需要一种方法来提升数据集质量，从而改善模型性能。

Method: 提出数据集-模型协同进化过程：每个迭代中，将合成改进的样本视为有噪声但噪声水平略低于前一次迭代的数据，使用Ambient Diffusion技术进行学习，逐步提升数据集质量。

Result: 在无条件图像生成、文本条件图像生成和从头蛋白质设计任务中取得了最先进的性能，并提供了理论分析证明数据循环过程的优势。

Conclusion: Ambient Dataloops框架通过迭代的数据集精炼过程，有效解决了异质数据集训练问题，显著提升了扩散模型的性能。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [11] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice是一个混合序列预测系统，通过二元置信度门控有条件地激活学习到的行为结构，在推荐系统、科学时间序列和金融市场等任务中显著提升性能，同时能在分布偏移时拒绝激活以防止错误预测。


<details>
  <summary>Details</summary>
Motivation: 为了解决在序列预测任务中如何智能地利用学习到的行为结构，同时避免在不确定或分布偏移时产生错误预测的问题。特别是在安全关键应用中，需要一种机制来管理认知不确定性，只在有足够置信度时激活特定行为模式。

Method: Lattice系统将行为窗口聚类为行为原型，使用二元置信度门控机制：当置信度超过阈值时激活基于原型的评分，否则回退到基线预测。系统在推荐系统（MovieLens）、科学时间序列（LIGO）和金融市场数据上验证，使用LSTM和Transformer作为骨干网络。

Result: 在MovieLens数据集上，Lattice比LSTM基线在HR@10指标上提升31.9%（p < 3.29×10^-25），比SASRec和BERT4Rec分别提升109.4%和218.6%。在LIGO和金融数据上，系统能在分布偏移时正确拒绝原型激活。在Transformer骨干上，Lattice保持0.0%改进（无退化），在结构已存在时优雅地推迟。

Conclusion: Lattice展示了置信度门控作为管理认知不确定性的有前景的架构原则：在模式适用时激活，在不适用时拒绝，在冗余时推迟。这种双向验证支持其在安全关键应用中的实用性。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [12] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL：首个实现扩散模型潜在表示与语义概念监督对齐的框架，通过稀疏自编码器和轻量级线性映射实现概念对齐，并提出CASL-Steer干预方法和EPR评估指标。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的内部激活编码了丰富的语义信息，但现有基于稀疏自编码器的方法都是无监督的，无法将稀疏特征与人类可理解的概念对齐，限制了语义控制生成图像的能力。

Method: 1. 在冻结的U-Net激活上训练稀疏自编码器获得解耦的潜在表示；2. 学习轻量级线性映射，将每个概念与少量相关潜在维度关联；3. 提出CASL-Steer作为因果探针，沿学习的概念轴移动激活以验证语义含义；4. 引入编辑精度比(EPR)联合评估概念特异性和无关属性保留。

Result: 实验表明，该方法在编辑精度和可解释性方面优于现有方法，首次实现了扩散模型中潜在表示与语义概念的监督对齐。

Conclusion: CASL是首个实现扩散模型潜在表示与语义概念监督对齐的框架，通过概念对齐的稀疏潜在和因果干预方法，显著提升了生成内容的语义控制能力和可解释性。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [13] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 该论文研究在合成数据污染环境下学习理论的基本问题，发现传统ERM方法存在局限，并提出能处理任意污染量的改进算法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成内容的普及和低成本，自然数据被合成数据污染的现象日益普遍。学习算法无法区分数据来源，这引发了在混合数据环境下学习理论基本问题的重新审视。

Method: 将场景建模为一系列学习任务，输入是自然和合成数据的混合，算法无法区分单个样本的来源。研究ERM在此设置下的表现，并与为不同代数据分配非均匀权重的算法进行比较。

Result: 对于估计任意d维分布均值问题，ERM虽能收敛到真实均值，但被非均匀加权算法超越。在PAC学习设置中，ERM不一定收敛到真实概念，但存在算法能够学习任意VC类和任意污染量的正确假设。

Conclusion: 在合成数据污染普遍存在的环境下，传统ERM方法存在根本性局限，需要开发能够处理混合数据的新算法，这些算法能够克服模型崩溃问题并实现可靠学习。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [14] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的RandNLA库，通过高效的随机化算法压缩深度学习模型，显著减少GPU内存使用（BERT上可达75%），同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习训练受GPU内存和计算限制，RandNLA技术虽然能有效压缩模型，但缺乏统一的生产级库阻碍了其广泛应用。

Method: 开发Panther库，整合成熟的RandNLA算法，提供高效的即插即用组件（线性层、卷积、注意力等），通过C++/CUDA后端(pawX)优化CPU/GPU性能。

Result: 用Panther替换PyTorch标准线性层（仅需几行代码），在BERT上实现高达75%的内存节省，同时保持可比较的损失性能。

Conclusion: Panther提供了一个统一、高效的RandNLA实现框架，使深度学习模型压缩技术更易于采用，有效解决GPU内存限制问题。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [15] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: 提出首个针对图分类任务的多目标后门攻击方法，通过子图注入而非替换的方式，在保持原图结构的同时植入多个触发器，实现将预测重定向到不同目标标签。


<details>
  <summary>Details</summary>
Motivation: 现有图分类后门攻击研究仅限于使用子图替换机制的单目标攻击，只能植入一个触发器。需要开发更强大的多目标后门攻击方法，以揭示图神经网络在更复杂攻击场景下的脆弱性。

Method: 提出子图注入方法（而非传统子图替换），在保持原始图结构的同时污染干净图。设计多触发器机制，使多个触发器能同时将预测重定向到不同目标标签。系统研究攻击设计参数的影响。

Result: 在五个数据集上实验表明，该方法对所有目标标签都实现了高攻击成功率，同时对干净准确率影响最小。在四种GNN模型上验证了攻击的泛化能力，且能抵抗最先进的防御方法（随机平滑和剪枝微调）。

Conclusion: 这项工作首次揭示了图神经网络在图分类任务中对多目标后门攻击的脆弱性。提出的子图注入方法比传统子图替换攻击性能更优，为图神经网络安全研究提供了重要洞见。

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [16] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 该研究比较了SARIMAX、XGBoost和LSTM三种模型在预测急诊科每日到达量方面的表现，使用澳大利亚医院2017-2021年数据，并针对COVID-19异常期使用Prophet模型生成合成数据。


<details>
  <summary>Details</summary>
Motivation: 急诊科过度拥挤是影响患者安全和运营效率的关键问题，需要准确的预测模型来优化资源分配。现有研究需要更精细的预测方法，考虑不同病房类别和患者临床复杂性。

Method: 使用澳大利亚三级转诊医院2017-2021年数据，将需求分解为8个病房类别并按临床复杂性分层。针对COVID-19数据异常，使用Prophet模型生成合成反事实值。比较SARIMAX、XGBoost和LSTM三种模型预测7天范围内的每日急诊到达量。

Result: 所有三种模型都优于季节性朴素基线。XGBoost在预测每日总入院量方面表现最佳（MAE=6.63），而SARIMAX在预测主要复杂性病例方面略优（MAE=3.77）。模型能成功复制日常模式，但普遍低估突发性、偶发性患者激增。

Conclusion: 虽然这些技术能成功复制日常模式，但都存在低估突发性患者激增的共同局限性。XGBoost在总体预测中表现最佳，而SARIMAX在特定复杂性病例预测中略有优势。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [17] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS将LLM解码重构为识别最优随机过程的问题，使用鞅理论设计算法，在推理基准上超越SOTA方法，同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码是短视的，难以找到全局最优推理路径。现有的前瞻采样方法依赖启发式机制进行路径评估和剪枝，缺乏理论依据。

Method: 提出鞅前瞻采样(MFS)框架，将LLM解码建模为识别最优随机过程的问题。利用鞅理论：1) 基于Doob分解定理进行步骤评估；2) 使用可选停止理论进行路径选择；3) 基于鞅收敛定理的自适应停止规则。

Result: 在六个推理基准测试中，MFS在准确率上超越了最先进的方法，同时显著提高了计算效率。

Conclusion: MFS提供了一个理论严谨的框架，用概率论原理替代启发式机制，有效解决了LLM解码中的短视问题，实现了更好的推理性能和计算效率。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [18] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin-Aware Speculative Verification方法，通过根据目标模型的局部决策稳定性调整验证策略，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在验证阶段采用严格的token级拒绝采样，但在现代LLM经常处于低边际区域（目标模型对候选token偏好较弱）时，拒绝合理的次优token带来的信息增益很小却导致大量回滚成本，造成验证效率低下。

Method: 提出无需训练、领域无关的Margin-Aware Speculative Verification策略，根据目标模型logits直接测量的决策稳定性来条件化验证过程，仅在严格验证提供最小收益时放宽拒绝标准。该方法只修改验证规则，与现有目标耦合推测解码框架完全兼容。

Result: 在8B到235B不同规模的模型上进行广泛实验，结果显示该方法相比最先进的基线方法在保持多样基准测试生成质量的同时，提供了持续且显著的推理加速。

Conclusion: 通过适应目标模型的局部决策能力，Margin-Aware Speculative Verification解决了推测解码中验证阶段的基本效率问题，在保持质量的同时显著提升推理速度，且与现有框架完全兼容。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [19] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 研究提出联合可行性函数，确定在保持与完整历史、全特征基线5%误差内的目标下，所需的最小训练历史和最少预测因子，为湖泊监测提供高效采样策略。


<details>
  <summary>Details</summary>
Motivation: 志愿者主导的湖泊监测产生不规则的季节性时间序列，存在大量数据缺失（冰盖、天气限制、人为错误），这给有害藻华预测和预警带来困难。

Method: 使用MICE处理缺失数据，评估六种候选模型，岭回归表现最佳。通过向后、近期历史协议量化最小样本量，识别最小特征集，并引入联合可行性函数统一历史长度和特征选择。

Result: 岭回归表现最佳；达到与完整历史5%误差内平均需要176个训练样本；四特征子集与十三特征基线性能相当；联合可行性分析显示达到5%目标仅需约64个近期样本和一个预测因子。

Conclusion: 联合可行性策略在固定精度目标下统一历史长度和特征选择，为湖泊研究人员提供了简单高效的采样努力和测量优先级设置规则，凸显了针对性监测的实用性。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [20] [SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model](https://arxiv.org/abs/2601.15504)
*Xianghao Zhan,Jingyu Xu,Yuanning Zheng,Zinaida Good,Olivier Gevaert*

Main category: cs.LG

TL;DR: SAGE-FM：基于图卷积网络的轻量级空间转录组学基础模型，通过掩码中心点预测任务训练，在416个人类Visium样本上学习空间一致的嵌入表示，在多个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学能够提供空间基因表达谱，需要计算模型来捕捉空间条件下的调控关系。现有方法在捕获空间关系和可解释性方面存在不足。

Method: 基于图卷积网络（GCNs）构建轻量级基础模型SAGE-FM，采用掩码中心点预测目标进行训练。在416个人类Visium样本（涵盖15个器官）上进行训练，学习空间一致的嵌入表示。

Result: 模型能够稳健地恢复掩码基因（91%的掩码基因显示显著相关性，p<0.05）。SAGE-FM的嵌入表示在无监督聚类和生物异质性保留方面优于MOFA和现有空间转录组学方法。在下游任务中，在口咽鳞状细胞癌中实现81%的病理学家定义点注释准确率，在胶质母细胞瘤亚型预测中优于MOFA。计算机扰动实验显示模型能够捕捉与真实情况一致的方向性配体-受体和上游-下游调控效应。

Conclusion: 简单、参数高效的图卷积网络可以作为生物学可解释且具有空间意识的大规模空间转录组学基础模型。

Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

</details>


### [21] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 提出机器学习方法，利用临床测试和MRI数据区分非典型阿尔茨海默病与非AD认知障碍，显著提高诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 非典型阿尔茨海默病（atAD）患者常被误诊，因为标准临床评估和海马体积测量主要针对典型AD。需要改进atAD的诊断方法。

Method: 使用机器学习分类器，结合临床测试特征、海马体积以及全脑MRI特征，通过Boruta统计方法识别重要脑区特征。

Result: 最佳性能通过加入重要MRI特征实现，优于仅使用海马体积。atAD诊断召回率从52%提升至69%（NACC）和34%提升至77%（ADNI），同时保持高精度。

Conclusion: 该方法仅使用临床测试和MRI数据就能显著提高非典型AD的诊断准确性，对临床环境有重要应用价值。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [22] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 量化会灾难性地恢复已遗忘的知识，本文提出量化感知的遗忘方法，通过logits空间铰链损失确保遗忘样本在量化后仍可区分


<details>
  <summary>Details</summary>
Motivation: 实际部署中模型常被量化（如4位），但研究发现量化会灾难性地恢复已遗忘的知识，现有遗忘方法在量化后失效

Method: 分析量化阈值与权重变化统计，提出logits空间铰链损失：强制遗忘样本在未学习模型和原始模型的输出logits差异至少达到量化步长的一半

Result: 在语言和分类任务（包括Twitter虚假信息数据集）上评估，该方法在4位量化下能保持遗忘效果，而现有方法几乎完全恢复遗忘知识

Conclusion: 量化会破坏机器遗忘效果，提出的量化感知遗忘方法能有效解决此问题，确保遗忘知识在量化部署后仍保持遗忘状态

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [23] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 提出Prism架构，基于MCR²原理，通过几何归纳偏置实现无监督功能解耦，在TinyStories上验证了注意力头自发分化为低频信号和高频噪声处理


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（特别是Transformer）常被批评为"黑箱"且缺乏可解释性，需要解决可解释性与性能之间的权衡问题

Method: 提出Prism白盒注意力架构，基于MCR²原理，将注意力机制建模为信号-噪声流形上的梯度上升过程，引入过完备字典扩展表示相空间和π-RoPE无理频率分离强制信号与噪声子空间不相干

Result: 在TinyStories上验证了Prism的注意力头自发分化为光谱不同的机制：低频头捕获长程因果依赖（信号），高频头处理局部句法约束（噪声）

Conclusion: 可解释性和性能不是权衡关系，可以通过原则性的几何构造统一起来，几何归纳偏置足以单独诱导无监督功能解耦

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [24] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，在持续测试时适应任务中显著提升了长期性能，特别是在CCC基准测试上比RDumb提高了约3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应方法（如Tent、EATA等）在短期分布变化下表现良好，但在测试分布快速变化或极长时间跨度下会失效，特别是在CCC基准测试中（包含750万个样本的连续变化）。

Method: 提出RDumb++，作为RDumb的扩展，引入两种漂移检测机制：基于熵的漂移评分和基于KL散度的漂移评分，结合自适应重置策略，使模型能够检测累积适应何时变得有害并在预测崩溃前恢复。

Result: 在CCC-medium基准测试中（三种速度、三种种子，共9次运行，每次包含100万个样本），RDumb++始终优于RDumb，获得约3%的绝对准确率提升，并在整个数据流中保持稳定的适应。

Conclusion: 漂移感知重置对于防止崩溃和实现可靠的长期持续测试时适应至关重要，RDumb++通过有效的漂移检测和自适应重置策略解决了长期CTTA的挑战。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [25] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 医疗ML优化应使用临床定制指标而非验证损失，以更好满足临床需求


<details>
  <summary>Details</summary>
Motivation: 传统ML使用验证损失进行优化，但医疗ML的目标是满足特定临床需求而非最小化损失函数。临床需求可以通过定制化指标更精确地捕捉。

Method: 通过两个受控实验，比较使用临床定制指标与验证损失进行模型优化的效果。这些优化任务不需要指标可微分，因此可以使用更广泛的临床相关指标。

Result: 实验显示使用临床定制指标进行优化相比验证损失能提供更好的模型优化效果，在临床任务上表现更优。

Conclusion: 虽然定义和编码临床相关指标需要额外努力，但能产生更符合医疗ML核心目标（在临床中表现优异）的模型，值得投入。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [26] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出首个从部分观测数据学习神经算子的系统框架，解决现实应用中传感器限制导致的数据不完整问题，通过掩码预测训练和物理感知潜在传播器实现18-69%相对误差降低。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中常遇到观测数据不完整的问题（传感器限制、地理约束、测量成本），而现有神经算子方法假设完全观测空间输入，严重限制了在实际应用中的适用性。

Method: 提出Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：1) 掩码预测训练策略，通过战略性地掩码观测区域创建人工监督；2) 物理感知潜在传播器，在潜在空间中通过边界优先的自回归生成重建解。同时开发了专门基准POBench-PDE。

Result: 在补丁式缺失率低于50%的所有基准测试中实现18-69%的相对L2误差降低，包括真实世界气候预测。方法能有效处理高达75%缺失率的实际场景，一定程度上弥合了理想研究设置与现实科学计算复杂性之间的差距。

Conclusion: 该工作首次系统性地解决了从部分观测学习神经算子的问题，通过创新的训练策略和潜在空间传播机制，显著提升了神经算子在现实不完整数据场景下的性能，推动了科学计算从理想化设置向实际应用的转变。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [27] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: BanditLP是一个可扩展的多利益相关者上下文老虎机框架，结合了神经汤普森采样和大规模线性规划，用于生产环境中的约束优化和探索。


<details>
  <summary>Details</summary>
Motivation: 为了解决在生产系统中同时进行探索和约束优化的挑战，特别是在多利益相关者环境下需要平衡不同目标和约束的场景。

Method: 使用神经汤普森采样学习目标特定结果，结合大规模线性规划在服务时进行约束动作选择，框架具有应用无关性，兼容任意神经架构，可处理数十亿变量。

Result: 在公共基准测试和合成数据上显示相对于强基线的持续增益，在LinkedIn的电子邮件营销系统中应用并展示了业务成功。

Conclusion: BanditLP证明了在生产环境中集成探索和约束优化的价值，为多利益相关者决策问题提供了可扩展的解决方案。

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [28] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 该论文提出了一种结合深度学习和库存理论的端到端学习方法，用于解决需求与交货时间未知的易腐品库存管理问题，通过嵌入启发式策略结构提高学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 易腐品库存管理面临需求随机、交货时间不确定且数据有限的挑战，传统方法难以有效处理未知分布和有限历史数据的情况，需要开发更智能的决策工具。

Method: 采用边际成本核算方案，为每个订单分配单一生命周期成本，构建统一的端到端损失函数。开发了三种方法：纯黑盒方法（E2E-BB）、嵌入预测库存水平策略的结构化方法（E2E-PIL），以及利用同质性进行增强的E2E-BPIL方法。

Result: 实验表明性能排序为：E2E-BB < E2E-PIL < E2E-BPIL。嵌入启发式策略结构能降低有效模型复杂度，提高学习效率，仅牺牲少量灵活性。深度学习决策工具在人类知识指导下更有效和稳健。

Conclusion: 将先进分析与库存理论相结合，通过嵌入启发式策略结构指导深度学习模型，能显著提升易腐品库存管理的学习效率和决策性能，体现了人机协同的价值。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [29] [Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance Portfolio Optimization](https://arxiv.org/abs/2601.15597)
*Liusha Yang,Siqi Zhao,Shuqi Chai*

Main category: cs.LG

TL;DR: 提出基于神经网络的非线性收缩协方差矩阵估计器，用于最小方差投资组合优化，结合统计模型与机器学习，通过Transformer网络学习特征值非线性收缩函数，直接以投资组合风险为损失函数训练。


<details>
  <summary>Details</summary>
Motivation: 传统协方差矩阵估计方法（如Ledoit-Wolf线性收缩）在最小方差投资组合优化中可能不是最优的，需要更灵活的非线性方法直接最小化投资组合风险，同时保持可扩展性。

Method: 从Ledoit-Wolf收缩估计器出发，将协方差矩阵分解为特征值和特征向量，使用轻量级Transformer神经网络学习特征值的非线性收缩函数，以投资组合风险作为损失函数训练，通过样本维度比条件化保持可扩展性。

Result: 在标普500指数股票日收益率数据上的实证结果表明，该方法相比基准方法能持续获得更低的样本外实现风险，验证了统计结构模型与数据驱动学习结合的有效性。

Conclusion: 该方法成功将统计估计与机器学习结合，通过神经网络学习非线性收缩函数直接优化投资组合风险，为协方差矩阵估计和投资组合优化提供了有前景的新方向。

Abstract: This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard & Poor's 500 Index (S&P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.

</details>


### [30] [When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards](https://arxiv.org/abs/2601.15609)
*Mingyuan Fan,Weiguang Han,Daixin Wang,Cen Chen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: 该论文研究了RLVR中的过度锐化现象，即策略坍缩到有限模式而抑制有效替代方案，并提出了两种校准方法来改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在逻辑密集型领域取得了经验性成功，但尚不清楚它是否真正激发了新能力，还是仅仅锐化了现有知识的分布。需要研究RLVR是否会导致过度锐化现象，即策略坍缩到有限模式而抑制有效替代方案。

Method: 1. 形式化过度锐化现象；2. 提出逆成功优势校准，优先处理困难查询；3. 提出分布级校准，通过记忆网络实现多样化采样。

Result: 实证评估验证了所提策略能有效改善泛化能力。

Conclusion: RLVR中的过度锐化现象确实存在，但通过适当的校准策略可以有效缓解，从而改善模型的泛化性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.

</details>


### [31] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文提出了一种新的1-identification多臂老虎机问题分析方法，通过优化公式推导下界，并设计了新算法，其期望总拉动次数上界与下界差距仅为对数多项式因子。


<details>
  <summary>Details</summary>
Motivation: 1-identification是纯探索多臂老虎机的基本问题，目标是判断是否存在平均奖励不低于已知阈值μ₀的合格臂，或者输出None。现有文献在存在多个合格臂时对期望总拉动次数𝔼τ的分析存在开放问题，本文旨在解决这一问题。

Method: 采用优化公式推导存在至少一个合格臂时的𝔼τ下界，并设计新算法，该算法在所有问题实例上都能达到紧上界，与下界的差距仅为对数多项式因子。

Result: 推导出存在合格臂时的𝔼τ新下界，设计了新算法，其𝔼τ上界与下界的差距在所有问题实例上仅为对数多项式因子，解决了存在多个合格臂时的分析开放问题。

Conclusion: 本文通过优化公式和算法设计，完善了1-identification问题的理论分析，特别是在存在多个合格臂的情况下，填补了历史文献的空白。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [32] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: Fission-GRPO框架通过将执行错误转化为纠正性监督，在RL训练循环中让模型从自身探索中产生的错误学习，显著提升了LLM在多轮工具调用中的错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多轮工具调用中存在脆弱性：遇到工具调用错误时，小模型往往陷入重复无效调用，无法解释错误反馈并进行自我纠正。这种脆弱性阻碍了在实际部署中的可靠性，因为工具交互过程中的执行错误是不可避免的。现有方法存在局限：标准RL将错误视为稀疏负奖励，不提供恢复指导；预收集的合成错误纠正数据集与模型在线错误模式存在分布不匹配问题。

Method: 提出Fission-GRPO框架，核心机制是将每个失败轨迹裂变为新的训练实例：通过使用微调的错误模拟器生成的诊断反馈来增强失败轨迹，然后在策略上重新采样恢复轨迹。这使得模型能够从探索过程中产生的具体错误中学习，而不是从静态的预收集错误案例中学习。

Result: 在BFCL v4 Multi-Turn基准测试中，Fission-GRPO将Qwen3-8B的错误恢复率提升了5.7%绝对值，关键的是，相比GRPO实现了4%的整体准确率提升（从42.75%到46.75%），并且超越了专门的工具使用智能体。

Conclusion: Fission-GRPO通过将执行错误转化为纠正性监督，在RL训练循环中实现了从错误中学习的能力，显著提升了LLM在多轮工具调用中的错误恢复性能和整体准确性，为解决工具交互中的脆弱性问题提供了有效方案。

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [33] [An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types](https://arxiv.org/abs/2601.15640)
*Natasha Trinkle,Huong Ha,Jeffrey Chan*

Main category: cs.LG

TL;DR: 本文对基于集成的迁移学习贝叶斯优化方法进行了实证分析，提出了正则化回归加权策略和正约束权重等新组件，并创建了三个新的实时迁移学习基准。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化是寻找昂贵黑盒目标函数全局最优的高效方法，但如何利用相关问题的历史数据集通过迁移学习提升贝叶斯优化性能需要进一步研究。

Method: 采用基于集成的迁移学习贝叶斯优化方法，提出了基于正则化回归的正约束权重加权策略，以及处理迁移学习无效情况的组件，并建立了三个新的实时迁移学习基准。

Result: 研究发现，热启动初始化和对集成代理模型权重施加正约束这两个组件能有效提升迁移学习贝叶斯优化的性能。

Conclusion: 迁移学习贝叶斯优化中，热启动初始化和正约束权重是提升性能的关键组件，为实际应用提供了有效指导。

Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.

</details>


### [34] [Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework](https://arxiv.org/abs/2601.15657)
*Yinxi Tian,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: SMSKD提出了一种顺序多阶段知识蒸馏框架，通过分阶段集成异构蒸馏方法，使用冻结参考模型防止遗忘，并引入基于教师真实类别概率的自适应权重机制，显著提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法（响应式、特征式、关系式）各有优势，但集成多种方法时面临实现复杂、组合不灵活、灾难性遗忘等问题，限制了实际应用效果。

Method: 提出SMSKD框架：1）顺序多阶段蒸馏，每个阶段使用特定蒸馏方法训练学生模型；2）冻结前一阶段的参考模型锚定已学知识防止遗忘；3）基于教师真实类别概率的自适应权重机制，动态调整参考损失权重。

Result: SMSKD在各种师生架构和方法组合中一致提升学生模型准确率，优于现有基线。消融研究表明，分阶段蒸馏和参考模型监督是性能提升的主要因素，TCP自适应权重提供补充优势。

Conclusion: SMSKD是一个实用且资源高效的异构知识蒸馏方法集成解决方案，支持任意方法组合和阶段数量，计算开销可忽略。

Abstract: Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.

</details>


### [35] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer是一个用于长期时间序列预测的双域Transformer框架，通过分层频率采样和周期性感知加权机制，解决传统Transformer在频率建模中的低通滤波效应问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在长期时间序列预测中存在固有的低通滤波效应，导致高频信息在层间传播中逐渐衰减，无法有效捕捉细粒度的时间变化模式。

Method: 提出了Dualformer框架，包含三个核心组件：1）时间域和频率域并行的双分支架构；2）分层频率采样模块，为不同层分配不同的频率带；3）基于谐波能量比的周期性感知加权机制。

Result: 在8个广泛使用的基准数据集上进行了大量实验，证明了Dualformer的鲁棒性和优越性能，特别是在异构或弱周期性数据上表现突出。

Conclusion: Dualformer通过结构化的频率建模和自适应的时间-频率特征集成，有效保留了高频信息并增强了泛化能力，为解决Transformer在时间序列预测中的频率建模问题提供了新思路。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [36] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit：基于递归最小二乘的LLM序列编辑方法，通过在线二次优化实现长期稳定编辑，支持万次编辑而不损失模型通用能力


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法面临可塑性-稳定性困境：硬写入方法会随时间累积干扰，硬保留方法可能覆盖过去编辑且未约束行为会偏离。在真实部署中，编辑以长序列形式到达，需要支持大规模连续编辑

Method: 将编辑建模为带软约束的在线二次优化问题，最小化累积键值拟合目标，包含两个正则项：控制与预训练权重的偏差和与指定锚映射的偏差。通过Woodbury恒等式实现高效在线递归，每次编辑成本与历史长度无关

Result: 在多个模型家族上实验，稳定扩展到10K次编辑，在编辑成功率和整体稳定性上优于强基线，能保留早期编辑，并在GLUE和保留推理/代码基准上保持通用能力

Conclusion: RLSEdit通过递归最小二乘方法有效解决了长期序列编辑的可塑性-稳定性困境，实现了大规模稳定编辑而不损害模型通用能力

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [37] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 论文提出零错误视界（ZEH）概念，用于评估LLM在无错误情况下能解决的最大问题范围，发现即使是先进模型如GPT-5.2在简单任务上也会出错，这对安全关键领域应用有重要警示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注准确率，但无法反映模型在无错误情况下的可靠解决范围。对于安全关键应用，需要了解模型在哪些范围内能完全可靠地工作，避免因简单错误导致严重后果。

Method: 提出零错误视界（ZEH）评估框架，通过逐步增加问题复杂度测试模型，直到出现第一个错误。使用树结构和在线softmax技术优化计算，实现高达一个数量级的加速。

Result: 发现GPT-5.2无法正确计算短字符串（如11000）的奇偶性，也无法判断括号平衡性。ZEH与准确率相关但行为模式不同，为算法能力涌现提供线索。优化方法显著降低计算成本。

Conclusion: ZEH是评估LLM可靠性的重要指标，揭示了即使先进模型在简单任务上也会出错，这对安全关键应用有重要警示。提出的优化方法使ZEH评估更实用。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [38] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: CeFGC是一种新颖的联邦图神经网络范式，通过仅需3轮通信来高效训练非IID图数据，利用生成扩散模型减少通信开销


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络面临两大挑战：多轮参数交换导致的高通信开销，以及客户端间非独立同分布数据特性。需要一种既能降低通信成本又能处理非IID数据的方法

Method: CeFGC采用三阶段方法：1) 客户端训练生成扩散模型捕获本地图分布并上传服务器；2) 服务器分发生成模型给所有客户端；3) 客户端使用生成模型合成图数据与本地图结合训练本地GNN模型，最后上传权重进行聚合

Result: 理论分析显示CeFGC将通信轮次减少到仅3轮，实验证明在多个真实图数据集上优于现有方法，特别是在非IID图数据上表现优异，通过对齐本地与全局目标并丰富训练数据多样性

Conclusion: CeFGC通过创新性地结合生成扩散模型，有效解决了联邦图神经网络中的通信开销和非IID数据挑战，实现了高效且性能优越的分布式图学习

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [39] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 这篇论文是关于LLM驱动内核生成与优化的系统性综述，旨在整合该领域碎片化的研究现状，提供结构化概览、数据集基准和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统性能受限于底层内核质量，而内核工程需要硬件架构和编程模型的专家知识，过程耗时且难以扩展。LLM和基于LLM的智能体为自动化内核生成与优化提供了新可能，但该领域研究分散，缺乏系统性视角。

Method: 通过系统性综述方法，对现有LLM驱动内核生成方法进行结构化梳理，涵盖基于LLM的方法和智能体优化工作流程，并系统整理支撑该领域学习和评估的数据集与基准。

Result: 建立了LLM驱动内核生成领域的结构化概览，编译了相关数据集和基准，识别了关键开放挑战，并维护了开源GitHub仓库以跟踪该领域发展。

Conclusion: 该综述填补了LLM驱动内核生成领域的系统性空白，为下一代自动化内核优化提供了全面参考框架，并指出了未来研究方向。

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [40] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI提出了一种基于关系学习的药物相互作用预测框架，通过将DDI预测重新定义为关系中心的学习问题，学习独立于药物身份的相互作用表示，从而显著提升对未见药物和新药对的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于分子中心的DDI预测方法虽然在标准基准上表现良好，但在实际部署场景中泛化能力不足，特别是当大多数候选药物对涉及未见药物且已验证的相互作用稀缺时。现有方法在嵌入空间中的邻近性并不能可靠地对应相互作用标签，单纯扩大模型容量无法改善泛化性能。

Method: 提出GenRel-DDI框架，将DDI预测重新定义为关系中心的学习问题。该方法学习独立于药物身份的相互作用表示，通过关系级抽象捕捉可迁移的相互作用模式。这种设计使模型能够泛化到未见药物和新的药物对组合。

Result: 在多个基准测试上的广泛实验表明，GenRel-DDI一致且显著优于现有最先进方法，特别是在严格的实体不相交评估中取得了特别大的性能提升，突显了关系学习对于稳健DDI预测的有效性和实际效用。

Conclusion: 关系学习框架为DDI预测提供了更稳健和可泛化的解决方案，能够有效处理实际部署场景中的未见药物问题，具有重要的实际应用价值。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [41] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出一种基于混合LLMs的主动学习框架，用多LLM标注模型替代人工标注，通过聚合多个LLM优势提升标注鲁棒性，并引入标注差异和负学习处理噪声标签。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，人们开始将其用于主动学习以减少标注成本，但单个LLM生成的标注质量往往达不到实际应用要求，需要提升LLM标注的鲁棒性和可靠性。

Method: 提出Mixture of LLMs in the Loop Active Learning框架：1) 使用混合LLMs标注模型替代人工标注；2) 引入标注差异分析识别不可靠标注；3) 采用负学习增强学习效果；4) 基于轻量级LLMs实现本地化部署。

Result: 实验表明该框架达到与人工标注相当的性能，持续优于单LLM基线和其他LLM集成方法，且基于轻量级LLMs可在本地机器上运行。

Conclusion: 该混合LLMs主动学习框架有效提升了LLM标注的鲁棒性和实用性，为降低标注成本提供了可行方案，同时保持了本地部署的可行性。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [42] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV框架通过全局优化识别LLM中的安全关键注意力头，发现恶意注入向量和安全抑制向量两种空间分离的安全向量，并基于此开发了新的白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖局部、贪婪的属性归因，假设组件贡献独立，忽略了LLM中不同组件（如注意力头）之间的协同交互作用，导致对安全机制的理解有限。

Method: 提出GOSV框架，通过全局优化同时考虑所有注意力头来识别安全关键头；采用有害补丁和零消融两种互补的激活重补丁策略，提取恶意注入向量和安全抑制向量。

Result: 发现对齐的LLM为安全目的维护着分离的功能通路；当约30%的总头被重补丁时，所有模型都会出现完全的安全崩溃；基于此开发的白盒越狱攻击在所有测试模型上显著优于现有方法。

Conclusion: GOSV框架有效提升了LLM安全可解释性，识别出的安全向量为理解LLM安全机制提供了新视角，同时揭示了现有安全防护的脆弱性。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [43] [Uncertainty-guided Generation of Dark-field Radiographs](https://arxiv.org/abs/2601.15859)
*Lina Felsner,Henriette Bast,Tina Dorosti,Florian Schaff,Franz Pfeiffer,Daniela Pfeiffer,Julia Schnabel*

Main category: cs.LG

TL;DR: 提出首个从标准衰减胸片生成暗场图像的框架，使用不确定性引导的渐进生成对抗网络，提高可解释性和可靠性


<details>
  <summary>Details</summary>
Motivation: X射线暗场成像通过小角度散射可视化微观组织结构变化，提供与传统衰减成像互补的诊断信息，但此类数据有限，给开发稳健的深度学习模型带来挑战

Method: 使用不确定性引导的渐进生成对抗网络，结合偶然不确定性和认知不确定性，直接从标准衰减胸片生成暗场图像

Result: 实验显示生成的图像具有高结构保真度，各阶段定量指标持续改善，分布外评估确认模型泛化能力强

Conclusion: 不确定性引导的生成建模能够实现逼真的暗场图像合成，为未来临床应用提供可靠基础

Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.

</details>


### [44] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出了一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖关系，揭示稳定独立的子结构，实现无需修改模型功能或接口的并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的推理通常基于密集参数矩阵，导致推理成本和系统复杂度随模型规模不可持续地增长。这种限制并非源于模型容量不足，而是因为将后训练推理系统视为整体操作符，忽视了学习过程中形成的内部结构。

Method: 研究发现大模型中的梯度更新事件高度局部化和选择性，许多参数依赖关系在训练后与初始化分布统计上无法区分。基于此，提出了后训练统计准则和结构退火程序，移除未支持的依赖关系，揭示稳定独立的子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口。

Conclusion: 通过利用大模型训练后参数依赖关系的统计特性，可以分解推理系统，实现高效并行化，解决大规模模型推理成本过高的问题。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [45] [SoK: Challenges in Tabular Membership Inference Attacks](https://arxiv.org/abs/2601.15874)
*Cristina Pêra,Tânia Carvalho,Maxime Cordy,Luís Antunes*

Main category: cs.LG

TL;DR: 论文对成员推理攻击在表格数据中的效果进行了全面分析，发现攻击效果普遍较差，但能有效暴露单例记录，且使用不同代理模型可提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击是评估机器学习隐私的主流方法，但在表格数据领域存在未充分探索的问题，特别是在集中式和联邦学习两种范式下的攻击效果、单例记录的脆弱性以及攻击在不同模型架构间的迁移性。

Method: 1) 对集中式和联邦学习中的成员推理攻击进行扩展性分类和系统分析；2) 在表格数据上测试多种攻击策略及防御方法；3) 在联邦学习场景中考虑常被忽视的外部攻击者威胁；4) 评估单例记录对攻击的脆弱性；5) 探索攻击在不同模型架构间的迁移性。

Result: 1) 表格数据上的成员推理攻击效果普遍较差，与先前研究结果形成对比；2) 即使攻击性能有限，仍能成功暴露大量单例记录；3) 使用不同的代理模型能显著提升攻击效果；4) 联邦学习中的外部攻击者威胁确实存在。

Conclusion: 成员推理攻击在表格数据中的实际效果被高估，但单例记录仍面临严重隐私风险。攻击效果受模型架构影响，使用多样化代理模型可增强攻击能力。这为隐私评估和防御策略提供了重要启示。

Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.

</details>


### [46] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 提出IA-HVAE模型，结合摊销推理与迭代优化，在变换域实现线性可分离解码器，获得35倍加速并提升重建质量


<details>
  <summary>Details</summary>
Motivation: 传统分层变分自编码器（HVAE）在推理时面临效率与精度权衡：完全摊销推理速度快但精度有限，完全迭代推理精度高但速度慢。需要一种混合方法平衡两者优势。

Method: 提出迭代摊销分层变分自编码器（IA-HVAE）：1）使用变换域（如傅里叶空间）的线性可分离解码器架构；2）采用混合推理方案：初始摊销猜测+解码器梯度迭代优化；3）实现实时应用的高模型深度。

Result: 1）相比传统HVAE，迭代推理速度提升35倍；2）在精度和速度上分别优于完全摊销和完全迭代方法；3）在去模糊和去噪等逆问题中，重建质量优于原始HVAE。

Conclusion: IA-HVAE通过混合摊销-迭代推理方案和变换域线性可分离解码器设计，在保持高精度的同时显著提升推理速度，为实时逆问题应用提供有效解决方案。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [47] [Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data](https://arxiv.org/abs/2601.15977)
*Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou*

Main category: cs.LG

TL;DR: 该研究整合医院属性、人口社会经济因素和空间流动性，使用多种机器学习模型预测医疗访问模式，发现Deep Gravity模型表现最佳，不同因素对访问模式的影响随距离变化。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往孤立地分析医疗访问模式的影响因素，缺乏对医院属性、人口社会经济因素和空间流动性的综合考量。本研究旨在填补这一空白，通过整合多维度数据来全面理解医疗访问模式的形成机制。

Method: 使用四年SafeGraph移动数据和Google Maps Reviews用户评价数据，训练五种流量预测模型（朴素回归、梯度提升、多层感知机、Deep Gravity、异构图神经网络），应用SHAP分析和PDP方法评估各因素对访问模式的影响。

Result: Deep Gravity模型表现最优。医院容量、ICU占用率、评分和受欢迎程度显著影响访问模式，且影响随距离变化：短距离访问主要受便利性驱动，长距离访问受医院评分影响更大。不同种族和教育水平人群对医院评分的敏感度不同，社会经济因素进一步影响访问频率。

Conclusion: 医疗访问模式受多因素复杂交互影响，需要综合考虑医院属性、空间距离和社会经济因素。研究结果为优化医疗资源配置和改善医疗可及性提供了实证依据，强调了针对不同人群和距离制定差异化医疗政策的重要性。

Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.

</details>


### [48] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD算法在平滑在线凸优化中连接了贪婪OGD和懒惰GD，通过懒惰度参数k在反应性和稳定性之间建立连续谱，证明了在不牺牲命中性能的情况下实现懒惰更新的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习算法在反应性（贪婪OGD）和稳定性（懒惰GD/双重平均）之间存在明显差距，缺乏一个能够根据比较器路径长度自适应调整懒惰度的统一框架。需要探索在平滑在线凸优化中，如何在不牺牲跟踪能力的情况下实现懒惰更新。

Method: 提出k-lazyGD算法，基于FTRL框架，通过懒惰度参数k在贪婪OGD（k=1）和懒惰GD（k=T）之间建立连续谱。算法允许根据比较器路径长度P_T自适应调整懒惰度，使用多种懒惰度参数的集成学习器。

Result: 证明了k-lazyGD在懒惰度k达到Θ(√(T/P_T))时仍能实现最优动态遗憾O(√((P_T+1)T))，建立了懒惰度与比较器移动之间的形式化联系。提供了匹配的下界证明，并展示了集成方法在可能时保持稳定、必要时保持敏捷的特性。

Conclusion: 该研究形式化地连接了懒惰度与比较器移动，证明了在平滑在线凸优化中，可以在不牺牲命中性能的情况下实现懒惰更新。k-lazyGD算法提供了一个统一框架，能够根据问题特性自适应地在反应性和稳定性之间取得平衡。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [49] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数（CFI），通过从历史数据学习参数化可接受不确定性集，并利用上下文信息使其条件化，从而扩展传统灵活性指数。


<details>
  <summary>Details</summary>
Motivation: 随着过程柔性化增加，确定鲁棒调度决策变得重要。传统灵活性指数使用简单可接受不确定性集（如超立方体）近似可接受不确定性区域，但未考虑可用上下文信息（如预测）来定义可接受不确定性集。

Method: 使用归一化流学习从高斯基分布到数据分布的双射映射，在潜在空间中构建超球面作为可接受潜在不确定性集，然后映射到数据空间。通过纳入上下文信息，使可接受不确定性集条件化。

Result: 通过示例表明，数据驱动的可接受不确定性集不一定优于简单集，条件集也不一定优于无条件集。但两者都确保只考虑包含实际实现的参数空间区域。在安全约束机组组合示例中，CFI通过纳入时间信息提高了调度质量。

Conclusion: 提出的条件灵活性指数（CFI）通过从数据学习参数化可接受不确定性集并使其条件化，提供了更灵活性的信息估计，能够改进调度决策质量。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [50] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP算法在约束在线凸优化中同时最小化累积损失和平方约束违反，首次为强凸问题提供了对数级遗憾和惩罚保证。


<details>
  <summary>Details</summary>
Motivation: 研究约束在线凸优化问题，其中学习者在每次迭代中面临未预期的凸损失和凸约束，需要在累积损失的同时处理约束违反惩罚。现有方法未能充分利用凸投影算子的非扩张性，且缺乏对强凸问题的对数级性能保证。

Method: 提出CLASP算法，通过充分利用凸投影算子的严格非扩张性这一新的证明策略，同时最小化累积损失和平方约束违反。算法适用于任意β∈(0,1)参数选择，平衡遗憾和惩罚性能。

Result: 对于凸损失：遗憾为O(T^{max{β,1-β}})，累积平方惩罚为O(T^{1-β})；对于强凸问题：首次获得对数级保证，遗憾和累积平方惩罚均为O(log T)。

Conclusion: CLASP算法通过创新的证明策略实现了约束在线凸优化的显著性能提升，特别是为强凸问题提供了首个对数级遗憾和惩罚保证，填补了该领域的重要空白。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [51] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 该论文探讨如何利用可解释AI（XAI）提升工业CPS中机器学习模型的预测性能，通过SHAP值分析时间序列分解组件对预测的影响，发现训练数据上下文信息不足的问题，并通过增加数据窗口大小来改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 工业CPS对安全和经济效益至关重要，但集成的深度学习模型因其复杂性而缺乏透明度。需要严格评估以防止模型在未来未见数据上出现意外行为，XAI可用于揭示模型推理过程，从而进行更全面的行为分析。

Method: 应用XAI技术，具体使用SHAP值分析时间序列数据分解组件对模型预测的影响。通过这种方法识别模型训练中上下文信息不足的问题，并根据XAI发现增加数据实例的窗口大小。

Result: 通过XAI分析发现了模型训练中缺乏足够上下文信息的证据。基于XAI的发现，通过增加数据窗口大小，成功提升了模型的预测性能。

Conclusion: XAI不仅可用于解释模型行为，还能指导改进模型设计。在工业CPS中，通过XAI分析识别训练数据局限性并相应调整数据预处理策略，可以有效提升机器学习模型的预测性能。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [52] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 提出了用于MAP推理的PAC算法，在可变和固定计算预算下提供可证明的最优解，使用概率电路实现，并通过随机化策略为启发式方法提供严格保证。


<details>
  <summary>Details</summary>
Motivation: MAP估计是概率推断中的基本任务，但通常难以处理，即使在许多常见的结构约束和近似方案下仍然困难。需要开发具有理论保证的MAP推理方法。

Method: 引入PAC-MAP算法，使用信息论度量来表征可处理性条件，通过具有适当架构的概率电路高效实现，并开发随机化策略作为独立的MAP推理技术或改进现有启发式方法。

Result: 在多个基准测试中验证了方法的优势，提供了在可变和固定计算预算下的可证明最优解，能够从有限样本中估计信息论度量。

Conclusion: PAC-MAP算法为MAP推理提供了具有严格理论保证的实用解决方案，能够处理传统上难以处理的MAP估计问题，并为现有启发式方法提供理论支撑。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [53] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 该研究首次系统性地对多个专门为拉曼光谱设计的深度学习分类器在多个开源数据集上进行基准测试，比较了五种代表性架构在统一训练协议下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前拉曼光谱深度学习分类器的评估往往孤立进行，或仅与传统机器学习方法比较，缺乏专门为拉曼光谱设计的深度学习模型之间的直接比较。现有研究缺少在共享开源数据集上对多个拉曼专用深度学习模型的系统性基准测试。

Method: 在三个开源拉曼数据集上评估五种代表性深度学习架构，采用统一的训练和超参数调优协议。数据集选择支持标准评估、微调和显式分布偏移测试。使用分类准确率和宏平均F1分数作为评估指标。

Result: 研究提供了拉曼光谱分类深度学习模型的公平、可重复比较结果，报告了五种架构在三个数据集上的分类准确率和F1分数，填补了该领域系统性基准测试的空白。

Conclusion: 该研究建立了拉曼光谱深度学习分类器的首个系统性基准测试框架，为未来模型开发提供了可靠的比较基准，促进了该领域研究的可重复性和公平性。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [54] [Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation](https://arxiv.org/abs/2601.16112)
*Yuta Nakahara,Shota Saito,Kohei Horinouchi,Koshi Shimada,Naoki Ichijo,Manabu Kobayashi,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 提出基于贝叶斯上下文树的变量分裂二叉树模型，用于时间序列分割，通过逻辑回归系数调整分割位置，实现更紧凑的树表示。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯上下文树模型在时间序列分割应用中，树结构通常表示序列的上下文关系，而非时间域上的区间划分。需要一种能够灵活表示任意分割位置且结构紧凑的时间序列分割方法。

Method: 提出变量分裂二叉树模型，将树结构表示为时间域的区间划分，通过递归逻辑回归模型表示区间划分，调整逻辑回归系数可在区间内任意位置设置分割点。结合局部变分近似和上下文树加权算法进行推理。

Result: 在合成数据上的数值实验证明了模型和算法的有效性，能够同时估计分割位置和树深度，实现更紧凑的树表示。

Conclusion: VSBT模型为时间序列分割提供了一种灵活且紧凑的表示方法，通过结合逻辑回归和上下文树加权算法，能够有效处理分割位置和树深度的联合估计问题。

Abstract: We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.

</details>


### [55] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文研究了核岭回归中两种内在维度概念的关系，证明了有效维度d_K能比Minkowski维度d_ρ更小，并给出了基于有限样本估计n-宽度的方法，最终推导出更优的泛化误差界。


<details>
  <summary>Details</summary>
Motivation: 流形假设认为当输入分布的内在维度较低时，机器学习方法的泛化性能会显著提升。在核岭回归背景下，需要研究两种不同的内在维度概念（Minkowski维度d_ρ和有效维度d_K）之间的关系，以更好地理解泛化行为。

Method: 1. 分析Kolmogorov n-宽度与积分算子特征值的关系；2. 证明Kolmogorov n-宽度刻画了所有概率测度下的最坏情况特征值衰减；3. 提出基于有限样本估计n-宽度上界的算法；4. 计算各种分形集上的有效维度d_K。

Result: 1. 推导出约束核岭回归的过剩误差界为O(n^{-(2+d_K)/(2+2d_K)+ε})；2. 证明对于接近均匀的分布，可以用O(ε^{-d_ρ}log(1/ε))样本高概率计算ε-精确的n-宽度上界；3. 发现对于拉普拉斯核等核函数，有效维度d_K可以显著小于Minkowski维度d_ρ。

Conclusion: 有效维度d_K比Minkowski维度d_ρ能更准确地刻画核岭回归的泛化性能，特别是在不规则域上。提出的样本估计算法能够有效估计内在维度，为实际应用提供了理论基础和实用工具。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [56] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL是一个针对ECG信号的对比学习框架，通过双上下文学习（节律级和心跳级对比）和软目标来解决ECG数据标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 获取标注的ECG数据用于监督学习模型具有挑战性。现有对比学习框架要么只关注全局上下文，要么未能充分利用ECG特定特征，且依赖硬对比目标，无法充分捕捉ECG信号特征相似性的连续性。

Method: 提出Beat-SSL对比学习框架，通过节律级和心跳级的双上下文对比学习，并使用软目标来更好地捕捉ECG特征的连续相似性。

Result: 在多标签分类任务中达到ECG基础模型93%的性能，在分割任务中超越所有其他方法4%。

Conclusion: Beat-SSL通过双上下文学习和软目标，在有限标注数据下实现了有效的ECG表示学习，在多个下游任务中表现出色。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [57] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover通过测试时强化学习让LLM针对特定问题持续训练，在多个科学领域取得了新的最先进成果


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题进行持续学习。需要一种方法让LLM在测试时能够基于具体问题的经验进行训练，专注于生成单个优秀解决方案而非平均表现

Method: TTT-Discover（测试时训练发现）方法：在测试时进行强化学习，设计学习目标和搜索子程序以优先考虑最有希望的解决方案。专注于连续奖励问题，使用开源模型OpenAI gpt-oss-120b，通过Tinker API进行测试时训练

Result: 在多个领域取得新的最先进成果：1) 数学：Erdős最小重叠问题和自相关不等式；2) GPU内核工程：GPUMode内核竞赛（比先前技术快2倍）；3) 算法设计：过去AtCoder算法竞赛；4) 生物学：单细胞分析中的去噪问题。所有结果均使用开源模型实现，成本仅几百美元

Conclusion: TTT-Discover通过测试时强化学习，使LLM能够针对特定科学问题持续训练，在多个领域实现了新的最先进成果，且使用开源模型和公开代码，具有可重复性和成本效益

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [58] [Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing](https://arxiv.org/abs/2601.16200)
*Song Xia,Meiwen Ding,Chenqi Kong,Wenhan Yang,Xudong Jiang*

Main category: cs.LG

TL;DR: 提出特征空间平滑(FS)方法，为多模态大语言模型(MLLMs)提供理论上的认证鲁棒性保证，并通过纯化和平滑映射器(PSM)模块进一步提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在各种应用中表现出强大能力，但对对抗性扰动仍然脆弱，这些扰动会扭曲特征表示并导致错误预测。

Method: 提出特征空间平滑(FS)方法，将任何特征编码器转换为平滑变体，保证在ℓ₂有界攻击下保持清洁和对抗特征之间的特征余弦相似度下界。进一步提出纯化和平滑映射器(PSM)模块，提高MLLMs的高斯鲁棒性分数，从而增强FS下的认证鲁棒性。

Result: FS-PSM不仅提供强大的理论鲁棒性保证，而且在实证性能上优于对抗训练。在各种MLLMs和下游任务上的实验表明，FS-PSM将各种白盒攻击的成功率从近90%降低到约1%。

Conclusion: 特征空间平滑与纯化和平滑映射器模块相结合，为多模态大语言模型提供了有效的认证鲁棒性防御方法，显著提高了模型对抗对抗性攻击的能力。

Abstract: Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\% to about 1\%.

</details>


### [59] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出一种名为"反事实训练"的新训练方法，利用反事实解释来增强模型的可解释性，使模型在训练阶段就学习生成合理且可操作的反事实解释，同时提高对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前反事实解释主要作为后处理方法应用于不透明的机器学习模型，但为了在真实决策系统中真正有用，反事实需要既合理（符合数据分布）又可操作（符合特征可变性约束）。现有研究多集中于开发后处理方法，而本文希望让模型直接对最终目标负责。

Method: 提出反事实训练方法，在训练阶段使用反事实解释，最小化学到的表示与合理、可操作的解释之间的差异。通过将反事实解释融入训练过程，使模型能够生成内在符合要求的反事实解释。

Result: 通过实证和理论分析证明，该方法能够训练出既能提供理想反事实解释，又具有更好对抗鲁棒性的模型。

Conclusion: 反事实训练是一种有效的训练范式，能够直接优化模型的可解释性和鲁棒性，相比后处理方法具有优势，为构建更可信赖的机器学习系统提供了新思路。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [60] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一个可组合的HLS库，用于快速开发领域特定的LLM加速器，支持阶段定制化推理和高效量化，在FPGA上实现了优于GPU的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理加速器开发需要大量手动工作，难以快速实现算法创新与高性能硬件之间的结合。需要一种能够简化开发流程、支持灵活架构设计的工具。

Method: 开发FlexLLM HLS库，提供架构自由度暴露、阶段定制化推理支持、混合设计能力（为prefill和decode阶段分别定制时间重用和空间数据流），以及全面的量化套件。

Result: 在2个月内用1K行代码构建了Llama-3.2 1B模型完整推理系统：1）阶段定制化加速器（12.68 WikiText-2 PPL）超越SpinQuant基线；2）HMT插件支持长上下文处理。在AMD U280 FPGA上相比NVIDIA A100 GPU：端到端加速1.29倍，解码吞吐量提高1.64倍，能效提升3.14倍。

Conclusion: FlexLLM以最小手动工作量桥接了LLM推理算法创新与高性能加速器开发，显著提升了开发效率和系统性能。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [61] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: 该论文提出将脉冲神经网络从图抽象提升到超图，以更准确地建模神经元间的尖峰复制，并基于超图设计映射算法，在神经形态硬件上实现更优的神经元到核心的映射。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在神经形态硬件上执行时，神经元到核心的映射是一个关键问题。随着SNN和硬件规模扩展到数十亿神经元，现有的基于图模型的映射方法难以有效处理尖峰在核心内的复制问题，导致通信流量和硬件资源使用效率低下。

Method: 提出将SNN从图抽象提升到超图模型，通过超边共成员关系准确捕捉神经元间尖峰复制。基于超图的重叠性和局部性特性设计映射算法，包括新设计的算法和从文献中改编的算法，通过共享超边对神经元进行分组。

Result: 实验结果表明，基于超图的映射技术在不同执行时间范围内都能实现比现有技术更好的映射质量。超图方法能够比仅压缩单个连接的方法更有效地减少通信流量和硬件资源使用。

Conclusion: 超图抽象能够更准确地建模SNN的通信模式，基于超图特性的映射算法能够实现更有效的硬件映射。研究识别出了一组有前景的算法，可在任何规模下实现有效的映射。

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: 本文介绍了一个开源多租户平台，帮助小企业通过无代码工作流部署定制化的LLM支持聊天机器人，解决基础设施成本、工程复杂性和安全风险等挑战。


<details>
  <summary>Details</summary>
Motivation: 小企业在部署基于LLM的问答系统时面临基础设施成本高、工程复杂度大和安全风险（特别是在RAG设置中）等实际挑战，需要一种经济实惠且安全的解决方案。

Method: 构建基于分布式轻量级k3s集群的开源多租户平台，使用加密覆盖网络连接异构低成本机器，实现资源池化、容器隔离和租户数据访问控制，并集成针对RAG聊天机器人提示注入攻击的平台级防御机制。

Result: 通过真实电商部署评估，证明该平台能在小企业面临的现实成本、运营和安全约束下，实现安全高效的LLM聊天机器人服务。

Conclusion: 该平台展示了小企业可以在不依赖模型重训练或企业级基础设施的情况下，通过分布式轻量级架构和平台级安全机制，成功部署安全高效的LLM支持聊天机器人。

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [63] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 本文提出三种改进RT Core粒子FRNN物理模拟的方法：BVH更新/重建比例优化器、无需邻居列表的RT Core新用法、支持周期性边界条件的RT Core技术，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有RT Core在粒子FRNN物理模拟中存在性能瓶颈，如BVH管理效率低、需要内存密集的邻居列表、不支持周期性边界条件等，需要改进以充分发挥RT Core潜力。

Method: 1) 实时BVH更新/重建比例优化器，根据模拟动态自适应调整；2) 两种无需邻居列表的RT Core新变体；3) 支持周期性边界条件的RT Core技术。

Result: BVH优化器使RT Core流水线快约3.4倍；新变体将速度提升从约1.3倍（小半径）提高到约2.0倍（对数正态半径分布）；支持周期性边界条件无显著性能损失；方法在不同GPU代际上均能扩展。

Conclusion: 提出的三种方法显著提升了RT Core在FRNN物理模拟中的性能和能效，同时明确了RT Core的优势和局限性，为实际应用提供了指导。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>
