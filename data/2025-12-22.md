<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [BEOL Ferroelectric Compute-in-Memory Ising Machine for Simulated Bifurcation](https://arxiv.org/abs/2512.17165)
*Yu Qian,Alptekin Vardar,Konrad Seidel,David Lehninger,Maximilian Lederer,Zhiguo Shi,Cheng Zhuo,Kai Ni,Thomas Kämpfe,Xunzhao Yin*

Main category: cs.ET

TL;DR: 提出基于铁电场效应晶体管(FeFET)的存内计算(CiM)伊辛机框架，通过算法-硬件协同设计高效解决大规模组合优化问题，相比GPU实现获得175.9倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构在解决NP-hard组合优化问题时效率低下，现有伊辛机存在初始化差、算法性能与硬件效率难以兼顾的问题，需要新的硬件友好型解决方案。

Method: 采用两步算法流程：1) 注意力启发的初始化利用全局自旋拓扑，减少80%迭代次数；2) 专为CiM实现的轻量级模拟分岔算法。使用32x256 FeFET CiM芯片原生加速核心向量-矩阵运算。

Result: 在多达100,000个节点的Max-Cut问题上，相比GPU实现的模拟分岔算法获得175.9倍加速，同时保持更优的解质量。

Conclusion: 通过算法与硬件的紧密协同设计，FeFET基CiM伊辛框架为大规模组合优化问题提供了高效解决方案，克服了现有方法的局限性。

Abstract: Computationally hard combinatorial optimization problems are pervasive in science and engineering, yet their NP-hard nature renders them increasingly inefficient to solve on conventional von Neumann architectures as problem size grows. Ising machines implemented using dynamical, digital and compute-in-memory (CiM) approaches offer a promising alternative, but often suffer from poor initialization and a fundamental trade-off between algorithmic performance and hardware efficiency. Hardware-friendly schemes such as simulated annealing converge slowly, whereas faster algorithms, including simulated bifurcation, are difficult to implement efficiently in CiM hardware, limiting both convergence speed and solution quality. To address these limitations, here we present a ferroelectric field-effect transistor (FeFET)-based CiM Ising framework that tightly co-designs algorithms and hardware to efficiently solve large-scale combinatorial optimization problems. The proposed approach employs a two-step algorithmic flow: an attention-inspired initialization that exploits global spin topology and reduces the required iterations by up to 80%, followed by a lightweight simulated bifurcation algorithm specifically tailored for CiM implementation. To natively accelerate the core vector-matrix and vector-matrix-vector operations in both steps, we fabricate a 32x256 FeFET CiM chip using ferroelectric capacitors integrated at the back end of line of a 180-nm CMOS platform. Across Max-Cut instances with up to 100,000 nodes, the proposed hardware-software co-designed solver achieves up to a 175.9x speedup over a GPU-based simulated bifurcation implementation while consistently delivering superior solution quality.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 提出使用events executor实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型，在单处理器系统上调度任意ROS2图


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要关注简单的链式任务，缺乏对任意有向无环图（DAG）的分析能力，需要填补实时系统理论与ROS2调度分析之间的差距

Method: 使用events executor实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型，需要特殊的事件队列实现和支持LIFO顺序消息传递的通信中间件

Result: 实现能够生成与传统固定优先级DAG任务调度器相同调度结果的方法，尽管缺乏通常所需的优先级信息，进一步缩小了实时系统理论与ROS2调度分析之间的差距

Conclusion: 提出的方法成功将ROS2应用映射到传统实时DAG任务模型，通过events executor实现了有效的调度，为ROS2系统提供了更强大的调度分析能力

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [3] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 系统评估主流大语言模型在生成高性能计算代码方面的能力，特别是针对曼德博集合的C++并行实现，发现ChatGPT-4和5在语法准确性和可扩展性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 并行编程仍然是高性能计算中最具挑战性的方面之一，需要深入了解同步、通信和内存模型。尽管现代C++标准和OpenMP、MPI等框架简化了并行化，但掌握这些范式仍然很复杂。虽然大语言模型在自动化代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的有效性尚未得到充分理解。

Method: 系统评估了包括ChatGPT 4和5、Claude和LLaMA在内的主流大语言模型，任务是生成使用共享内存、基于指令和分布式内存范式的曼德博集合C++实现。每个生成的程序都用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示，ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现出色。

Conclusion: 大语言模型在生成高性能计算代码方面具有潜力，特别是ChatGPT系列模型在并行编程任务中表现突出，能够生成语法正确且可扩展的代码实现。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [4] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve是一个针对扩散大语言模型的高效服务系统，通过内存优化、计算调度和生成质量协同优化，解决了扩散模型特有的内存危机和资源振荡问题，在多种硬件上实现了显著的吞吐量提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。作者识别出dLLM特有的"内存足迹危机"，由单体logit张量和"刷新"与"重用"阶段之间的严重资源振荡驱动。

Method: dLLM-Serve包含三个核心技术：1) Logit-Aware Activation Budgeting分解瞬态张量峰值；2) Phase-Multiplexed Scheduler交错处理异构请求阶段；3) Head-Centric Sparse Attention将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载和GPU上评估显示，相比最先进基线，dLLM-Serve在消费级RTX 4090上提升吞吐量1.61-1.81倍，在服务器级NVIDIA L40S上提升1.60-1.74倍，在重度竞争下将尾部延迟降低近4倍。

Conclusion: dLLM-Serve建立了首个可扩展dLLM推理蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，为扩散大语言模型的高效服务提供了系统级解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [5] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE提出了一种可扩展的近似最近邻搜索索引，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现高吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有分布式索引设计在扩展到数十亿向量时难以平衡准确性、延迟和吞吐量之间的权衡，需要新的解决方案

Method: 1) 识别避免读取成本爆炸的平衡分区粒度；2) 引入保持准确性的递归构建方法，构建具有可预测搜索成本和稳定准确性的多级索引

Result: 在46个节点上对80亿向量进行实验，SPIRE实现了高可扩展性，吞吐量比最先进系统高出9.64倍

Conclusion: SPIRE通过平衡分区粒度和递归构建多级索引的设计决策，成功解决了大规模近似最近邻搜索中的可扩展性挑战

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [6] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦系统，作为NIH HEAL计划数据的统一搜索、发现和分析平台


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生的数据分散在多个NIH和第三方数据存储库中，需要一个统一的发现平台来促进数据的二次利用

Method: 基于开源Gen3平台构建，采用网状架构，包含认证授权、数据对象标识符创建、元数据管理等框架服务，与19个数据存储库互操作

Result: 平台成功集成了1000多项HEAL研究，每月数百用户使用，提供丰富元数据，通过STRIDES集成的安全云环境支持二次分析

Conclusion: HEAL数据平台实现了对连接数据存储库中数据的搜索、发现和分析，通过确保数据符合FAIR原则，最大化HEAL计划数据的价值

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [7] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型推理系统，通过GPU协同视频解码和资源共享，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM推理系统存在两大瓶颈：1) 多模态预处理（特别是视频解码）在CPU上执行，主导了首token时间；2) 视觉编码器作为独立计算密集型阶段，无法与LLM预填充或解码批量处理，导致阶段间阻塞和资源利用率低下。

Method: 提出两个互补设计：FlashCodec通过多GPU协同视频解码加速预处理阶段；UnifiedServe通过逻辑解耦但物理共享GPU资源的方式优化视觉到文本和推理阶段，消除阶段间阻塞。

Result: 该框架能够服务3.0倍更多请求，强制执行1.5倍更严格的SLOs，同时相比最先进系统实现4.4倍更高的吞吐量。

Conclusion: FlashCodec和UnifiedServe共同构成了端到端优化的MLLM推理堆栈，通过协同多GPU解码和资源共享，显著提升了系统性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: 提出Dion2方法简化Muon优化器的正交化步骤，通过采样部分行或列来降低计算和通信成本，提升可扩展性


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然性能优秀，但其正交化步骤的超线性计算成本随规模增长而增加，现有方法尝试减少正交化矩阵尺寸但不够简单

Method: Dion2在每次迭代中采样部分行或列，仅对这些采样部分进行正交化，使更新变得稀疏，从而降低计算和通信成本

Result: Dion2相比先前方法更简单，能有效减少Muon的计算和通信开销，提升其可扩展性

Conclusion: Dion2提供了一种简单有效的方法来缓解Muon优化器的扩展瓶颈，通过稀疏化正交化步骤来改善大规模应用中的性能

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [9] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 开发低成本双模式神经肌肉控制系统，结合EEG和EMG信号实现假肢手臂的多自由度实时控制，总成本约240美元。


<details>
  <summary>Details</summary>
Motivation: 传统低成本上肢假肢缺乏直观控制系统，限制了截肢者在资源匮乏环境下的功能性和可及性。

Method: 使用NeuroSky MindWave Mobile 2采集EEG信号，通过ThinkGear蓝牙传输到ESP32运行轻量级分类模型；使用MyoWare 2.0传感器采集EMG信号，通过SparkFun无线模块传输到第二个ESP32进行阈值检测。EEG控制手指伺服，EMG控制肘部伺服。

Result: 构建了功能性原型，总成本约240美元，EEG检测眨眼事件切换手部开合状态，EMG通过三个激活带实现直观肘部控制，需要连续8帧确认以提高稳定性。

Conclusion: 该系统展示了低成本、生物直观的假肢控制可行路径，适合资源匮乏和全球健康应用，未来将改进3D打印外壳、降低EMG延迟并提升伺服扭矩。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [10] [QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification](https://arxiv.org/abs/2512.16960)
*Bikash K. Behera,Giuseppe Sergioli,Robert Giuntini*

Main category: cs.LG

TL;DR: 量子启发机器学习方法（PGM和KPGM）在合成过采样场景中显著优于经典随机森林，其中PGM在特定编码下表现最佳，KPGM在不同采样策略中更稳定。


<details>
  <summary>Details</summary>
Motivation: 量子启发机器学习利用量子理论数学框架增强经典算法，但缺乏对核心方法（核技巧与量子态判别中的PGM）的统一理论实证比较，需要评估它们在合成过采样场景中的性能差异。

Method: 提出统一理论实证比较框架，分析PGM和KPGM分类器在量子SMOTE变体合成过采样场景中的性能，使用多种编码方式（stereo、amplitude）和量子副本数（n_copies）进行实验评估。

Result: PGM和KPGM分类器均显著优于经典随机森林基线，特别是使用多个量子副本时。PGM在stereo编码和n_copies=2时获得最高准确率（0.8512）和F1分数（0.8234），KPGM在不同QSMOTE变体中表现更稳定，最高得分0.8511（stereo）和0.8483（amplitude）。

Conclusion: 量子启发分类器在召回率和平衡性能方面提供实质性提升，PGM受益于编码特定增强，KPGM确保跨采样策略的鲁棒性，为不同数据特征和计算约束下的应用提供实用指导。

Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.

</details>


### [11] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: GreedySnake是一个新的SSD卸载训练系统，采用垂直调度策略，相比水平调度系统在较小批量下实现更高训练吞吐量，显著提升LLM训练效率


<details>
  <summary>Details</summary>
Motivation: SSD卸载训练为LLM训练提供了一种经济实用的方法，但现有系统存在I/O瓶颈和训练吞吐量不足的问题，需要更高效的调度策略来接近理论性能极限

Method: 基于梯度累积和微批次，引入垂直调度策略（逐层执行所有微批次），并重叠优化步骤与下一迭代的前向传播以缓解I/O瓶颈

Result: 在A100 GPU上，相比ZeRO-Infinity，GreedySnake对GPT-65B在1GPU和4GPU上分别实现1.96倍和1.93倍吞吐量提升，对GPT-175B在1GPU上实现2.53倍提升

Conclusion: GreedySnake通过垂直调度和计算重叠技术，显著提升了SSD卸载训练的效率，使系统更接近屋顶线模型预测的理想性能，代码已开源

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [12] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 提出"压缩即路由"新架构哲学，通过Transformer自编码器实现64倍序列压缩，利用重建误差作为内在分布指纹来自动调度专家模块，无需显式门控网络。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型的三个主要挑战：上下文长度限制、高推理成本、持续学习中的灾难性遗忘。现有MoE架构依赖显式训练的路由分类器，增加系统复杂性且缺乏可解释性。

Method: 训练87M参数的Transformer自编码器，实现512个token压缩为8个潜在向量的64倍序列长度压缩。利用重建误差作为内在分布指纹，自动调度专家模块。

Result: 压缩器展现出极端的领域判别能力：在域内（代码）验证集上重建准确率达99.47%；在半分布外领域（Wiki文本）降至47.76%；在完全分布外领域（随机序列）仅0.57%。

Conclusion: 重建误差可作为有效的内在分布指纹，用于自动调度专家模块而无需显式门控网络。该架构为可扩展模块化神经网络提供了新视角，并探索了处理超长上下文的VRAM压缩方法。

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [13] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: M2RU是一种混合信号架构，实现了minion循环单元，用于边缘设备上的高效时序处理和片上持续学习，能效比数字CMOS设计提高29倍。


<details>
  <summary>Details</summary>
Motivation: 边缘平台上的持续学习面临挑战，因为循环网络需要能耗高的训练过程和频繁的数据移动，这在嵌入式部署中不切实际。

Method: 提出M2RU混合信号架构，集成加权位流技术（使多比特数字输入无需高分辨率转换即可在交叉阵列中处理）和经验回放机制（稳定域偏移下的学习）。

Result: M2RU达到15 GOPS/48.62 mW（312 GOPS/瓦），在顺序MNIST和CIFAR-10任务上保持与软件基线5%以内的准确率，能效比数字CMOS设计提高29倍，预期操作寿命12.2年。

Conclusion: M2RU为边缘级时序智能中的实时适应提供了一个可扩展且高能效的平台。

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [14] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 基于XGBoost的轻量级梯度提升框架，利用地面观测数据和物理引导特征工程，实现低能见度和降水事件的短期预测，在多个国际机场验证中显著优于传统TAF预报。


<details>
  <summary>Details</summary>
Motivation: 当前航空天气预报依赖计算密集的数值天气预报和人工发布的TAF产品，存在保守偏差和时间分辨率有限的问题，需要更高效、准确的短期预测方法保障航空安全。

Method: 采用XGBoost梯度提升框架，仅使用地面观测数据（METAR），通过热力学原理进行物理引导的特征工程，在11个不同气候区的国际机场（2000-2024年数据）进行训练和评估。

Result: 模型在3小时战术预测中显著优于传统TAF预报，召回率提高2.5-4.0倍，同时减少误报。SHAP分析显示模型能够隐式重建局部物理驱动因素（平流、辐射、下沉）。

Conclusion: 该轻量级物理引导机器学习框架能够有效捕捉局部物理过程，提供可解释的预测结果，为航空运营提供实用的短期天气预报解决方案。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [15] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 论文提出turn-PPO算法，针对多轮交互任务改进PPO，在WebShop和Sokoban数据集上表现优于GRPO


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练交互式LLM智能体方面具有优势，但现有GRPO算法在多轮任务中，特别是在需要长时推理的场景下存在局限性。需要更稳定有效的优势估计策略来应对多轮交互的挑战。

Method: 首先探索PPO作为GRPO的替代方案，发现PPO更鲁棒。然后提出turn-PPO变体，基于轮次级MDP（而非常用的令牌级MDP）进行优化，专门针对多轮场景设计。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO算法有效，无论是否包含长推理组件都能取得良好效果。

Conclusion: turn-PPO算法通过轮次级MDP公式改进了PPO在多轮任务中的性能，为解决需要长时推理的交互式LLM智能体训练提供了更有效的强化学习方法。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [16] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: GB-DQN使用梯度提升集成方法解决深度强化学习中的非平稳环境问题，通过增量残差学习来适应动态变化，相比传统方法具有更快的恢复速度和更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境（动态或奖励变化）对深度强化学习构成根本性挑战，会导致学习到的价值函数失效和灾难性遗忘问题。

Method: 提出梯度提升深度Q网络（GB-DQN），这是一种自适应集成方法。它构建一个加法集成，其中每个新的学习器被训练来近似当前集成在动态变化后的Bellman残差，通过增量残差学习来适应模型漂移。

Result: 理论分析表明每个提升步骤都能减少经验Bellman残差，集成在标准假设下收敛到动态变化后的最优价值函数。在多种控制任务中的实验显示，相比DQN和常见非平稳基线方法，GB-DQN具有更快的恢复速度、更好的稳定性和更强的鲁棒性。

Conclusion: GB-DQN通过梯度提升集成方法有效解决了深度强化学习在非平稳环境中的适应性问题，提供了一种理论保证且实验验证有效的解决方案。

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [17] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 提出SFBD-OMNI框架，利用大量噪声样本和少量干净样本来恢复真实分布，通过单边熵最优传输和EM算法解决任意测量模型下的分布恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取完全观测样本成本高昂甚至不可行，而部分噪声观测相对容易收集。需要解决在已知黑盒生成器作为腐败过程的情况下，利用大量噪声样本恢复真实分布的问题。

Method: 将任务构建为单边熵最优传输问题，采用EM-like算法求解。提出测试准则判断在每样本信息损失下真实分布是否可恢复，并在不可恢复时利用少量干净样本。基于此开发SFBD-OMNI框架，将Stochastic Forward-Backward Deconvolution推广到处理任意测量模型。

Result: 在基准数据集和多样化测量设置下的实验表明，该方法在定性和定量性能上均有显著提升，能够有效恢复真实分布。

Conclusion: SFBD-OMNI框架能够利用大量噪声样本和少量干净样本有效恢复真实分布，特别是在处理任意测量模型方面表现出优越性能，为现实世界中的分布恢复问题提供了实用解决方案。

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [18] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: DTDR是一种轻量级动态工具依赖检索方法，通过结合初始查询和动态执行上下文来改进LLM函数调用代理的工具选择，相比静态检索方法显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的LLM函数调用代理使用静态和有限的输入进行工具选择，无法捕捉多步骤工具依赖关系和动态任务上下文，导致引入无关工具误导代理，降低效率和准确性。

Method: 提出动态工具依赖检索(DTDR)，这是一种轻量级检索方法，同时考虑初始查询和动态执行上下文。DTDR从函数调用演示中建模工具依赖关系，支持随着计划展开的自适应检索。

Result: 在多个数据集和LLM骨干网络上进行基准测试，评估检索精度、下游任务准确性和计算效率。动态工具检索相比最先进的静态检索器，将函数调用成功率提高了23%到104%。

Conclusion: DTDR通过动态建模工具依赖关系和任务上下文，显著提升了LLM函数调用代理的工具选择性能，解决了静态检索方法的局限性。

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [19] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 证明了k近邻分类器在可分度量空间中弱普适一致性的充要条件：该空间必须具有Nagata意义下的σ有限维性。


<details>
  <summary>Details</summary>
Motivation: 完成k近邻分类器理论中最后一个未证明的蕴含关系，建立三个重要条件的等价性，并修正先前文章中的错误主张。

Method: 通过数学证明建立蕴含关系(1)⇒(3)，即从k近邻分类器的弱普适一致性推导出空间的σ有限维性（Nagata意义）。

Result: 成功证明了(1)⇒(3)，从而完成了三个条件的等价性证明：(1)k近邻分类器弱普适一致 ⇔ (2)强Lebesgue-Besicovitch可微性 ⇔ (3)σ有限维空间。

Conclusion: k近邻分类器在可分度量空间中具有弱普适一致性的充要条件是空间具有Nagata意义下的σ有限维性，这为机器学习理论提供了重要的几何特征刻画。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [20] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法降低ST-GNN在边缘计算中的通信开销，同时引入SEPA新指标评估交通事件预测能力


<details>
  <summary>Details</summary>
Motivation: ST-GNN在处理智能交通系统分布式传感器数据时，相邻边缘节点间重复传输重叠特征导致通信开销巨大

Method: 1) 自适应剪枝算法动态过滤冗余邻居特征，基于近期模型性能调整剪枝率；2) 提出SEPA新指标专门评估交通减速和恢复事件的预测能力

Result: 在PeMS-BAY和PeMSD7-M数据集上，自适应剪枝算法在保持预测精度的同时显著降低通信成本，SEPA指标揭示了空间连接性在预测动态不规则交通中的真正价值

Conclusion: 自适应剪枝算法可在不牺牲关键交通事件响应能力的前提下降低通信开销，SEPA指标比标准误差指标更能反映模型对动态交通事件的预测能力

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [21] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出BEAM-LRC方法，通过低秩补偿实现带宽高效的MoE推理，在保持精度的同时减少内存传输


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏激活扩展容量，但给内存和带宽带来压力。现有的卸载技术虽然缓解了GPU内存问题，但token级路由导致不规则传输，使推理受限于I/O。静态均匀量化会降低精度，且忽略了专家异质性

Method: 提出带宽高效的自适应混合专家低秩补偿方法：1）使用路由器引导的精度恢复，通过预计算低秩补偿器；2）推理时传输紧凑的低秩因子给Top-n专家，应用补偿恢复精度，其他专家保持低比特；3）与GPU和GPU-NDP系统的卸载技术集成

Result: 该方法在带宽-精度权衡方面表现优异，提高了推理吞吐量

Conclusion: BEAM-LRC方法通过自适应低秩补偿有效解决了MoE模型推理中的带宽瓶颈问题，在保持模型精度的同时显著提升了系统性能

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [22] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 训练模型在包含故意错误的推理轨迹上，可以提高模型检测和恢复错误的能力，而不降低标准解题能力。


<details>
  <summary>Details</summary>
Motivation: 链式思维提示已成为大语言模型数学推理的核心方法，但模型对早期错误非常脆弱：单个算术错误或不合理推断通常会导致最终答案错误。研究是否可以通过训练模型处理有缺陷的推理轨迹来教模型检测和恢复错误。

Method: 使用MATH-lighteval竞赛级问题，生成包含单一控制错误（计算错误或推理错误）的CoT前缀，使用GRPO和二元最终答案奖励对Qwen3-4B模型进行微调。

Result: Mixed-CoT-RL模型在干净问题上与标准RL表现相当（41% vs 41%），但在包含缺陷推理的问题上显著优于标准RL（24% vs 19%）。仅使用干净数据训练的RL会降低鲁棒性（19% vs 20%）。混合训练效果最好，推理错误训练比计算错误训练带来更大的鲁棒性提升。

Conclusion: 在训练中暴露有缺陷的推理轨迹可以改善错误恢复行为而不牺牲准确性，为LLMs中更鲁棒的数学推理提供了路径。

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [23] [How to Square Tensor Networks and Circuits Without Squaring Them](https://arxiv.org/abs/2512.17090)
*Lorenzo Loconte,Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 平方张量网络及其扩展平方电路作为分布估计器具有表达力且支持闭式边缘化，但平方操作增加了计算复杂度。本文提出通过参数化平方电路来克服边缘化开销，保持表达能力的同时实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 平方张量网络和平方电路作为分布估计器虽然表达力强且支持闭式边缘化，但平方操作在计算配分函数或边缘化变量时引入了额外复杂度，限制了其在机器学习中的应用。现有的张量网络规范形式不适用于电路，因为电路可以表示不直接映射到已知张量网络的分解。

Method: 受规范形式中的正交性和电路中确定性可实现可处理最大化的启发，本文展示了如何参数化平方电路以克服其边缘化开销。提出的参数化方法即使在不同于张量网络的分解中也能实现高效边缘化，这些分解以电路形式编码，其结构原本会使边缘化计算困难。

Result: 实验表明，提出的平方电路条件不会损失表达力，同时实现了更高效的学习。参数化方法解锁了在电路编码的不同分解中的高效边缘化，即使这些结构原本会使边缘化计算困难。

Conclusion: 本文提出的平方电路参数化方法解决了平方操作带来的边缘化计算复杂度问题，在保持表达力的同时实现了高效学习，扩展了平方电路在机器学习中的应用范围。

Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.

</details>


### [24] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出结合强化学习与MPC规划的分层规划新方法，通过自适应采样提升数据效率和性能


<details>
  <summary>Details</summary>
Motivation: 解决具有分层结构的规划问题，传统方法在复杂规划任务中效率有限，需要融合不同规划范式的优势

Method: 紧密耦合强化学习与MPPI规划，用RL动作指导MPPI采样，自适应聚合MPPI样本来改进价值估计，在价值估计不确定时利用更多MPPI探索

Result: 在赛车驾驶、改进Acrobot、带障碍物的Lunar Lander等多个领域验证，相比现有方法成功率提升最高72%，收敛速度加快2.1倍，数据效率和整体性能更优

Conclusion: 提出的自适应分层规划方法能处理复杂规划问题，适应不同应用，提升训练鲁棒性和策略质量

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [25] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: UniCoMTE是一个模型无关的框架，用于为多元时间序列分类器生成反事实解释，提高深度学习模型的可解释性，特别是在医疗等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在复杂时间序列分类中表现出色，但其黑盒特性限制了在高风险领域（如医疗）的信任和采用。需要提高模型的可解释性以增强用户信任。

Method: UniCoMTE是一个模型无关的框架，通过修改输入样本并评估其对模型预测的影响，识别对模型预测影响最大的时间特征。该框架兼容多种模型架构，可直接处理原始时间序列输入。

Result: 在ECG时间序列分类器上的评估显示，UniCoMTE生成的解释比现有方法（LIME和SHAP）更简洁、稳定且符合人类认知。医学专家问卷评估证实了其临床实用性。

Conclusion: UniCoMTE通过将模型预测与有意义的信号模式联系起来，提升了深度学习模型在现实世界时间序列应用中的可解释性，特别是在医疗等高风险领域。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [26] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 提出基于可微分快速故障仿真模型(DFFSM)和梯度优化方法的光伏阵列故障量化新方法，实现高精度故障诊断


<details>
  <summary>Details</summary>
Motivation: 现有光伏故障量化方法存在效率低和可解释性差的问题，需要开发更准确高效的故障诊断技术

Method: 构建可微分快速故障仿真模型(DFFSM)准确建模多故障下的I-V特性，利用其解析梯度特性开发基于Adahessian优化器的梯度故障参数识别(GFPI)方法

Result: 在仿真和实测I-V曲线上验证，GFPI方法对不同故障类型（部分阴影、短路、串联电阻退化）均实现高精度量化，I-V重构误差低于3%

Conclusion: 可微分物理仿真器在光伏系统故障诊断中具有可行性和有效性，为光伏阵列智能维护提供了新方法

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [27] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: Atom是一个在设备端系统，通过分解和重用视频语言模型模块来优化多阶段流水线执行，在移动设备上实现27-33%的加速，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型在移动设备上执行多阶段流水线（如检索、字幕生成、组装）时面临效率挑战，包括冗余模型加载和碎片化执行，需要更高效的执行方案。

Method: 将十亿参数模型分解为可重用模块（如视觉编码器和语言解码器），在字幕生成、推理和索引等子任务中重用这些模块，消除重复模型加载并实现并行执行。

Result: 在商用智能手机上，相比非重用基线，Atom实现27-33%的端到端延迟降低，性能损失极小（检索Recall@1下降≤2.3，字幕生成CIDEr下降≤1.5）。

Conclusion: Atom为边缘设备上的高效视频语言理解提供了一个实用、可扩展的方法，通过重用设计显著提升执行效率，同时保持高性能。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [28] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，在训练时维护因子化动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合，避免重复计算并实现更优的模型合并。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程在训练时计算曲率信息后丢弃，然后在模型合并时重新计算类似信息，造成计算浪费并丢失有价值的轨迹数据。需要统一框架来重用优化轨迹信息。

Method: 在训练过程中维护因子化动量和曲率统计信息，积累任务显著性分数，实现无需事后Fisher计算的曲率感知合并。建立非凸目标的收敛保证。

Result: 在自然语言理解基准测试中，曲率感知参数选择在所有稀疏度水平上都优于仅基于幅度的基线，多任务合并优于强基线。框架表现出秩不变收敛和超参数鲁棒性。

Conclusion: 通过将优化轨迹视为可重用资产而非丢弃，该方法消除了冗余计算，同时实现了更原则性的模型组合，为大规模神经网络训练和模型合并提供了统一解决方案。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [29] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 首个针对低资源历史语言古尼泊尔语的端到端手写文本识别系统，采用编码器-解码器架构，达到4.9%字符错误率


<details>
  <summary>Details</summary>
Motivation: 古尼泊尔语作为具有历史意义但资源匮乏的语言，缺乏有效的手写文本识别系统，需要开发专门解决方案

Method: 采用行级转录方法，系统探索编码器-解码器架构和数据中心化技术，实现解码策略并分析令牌级混淆

Result: 最佳模型字符错误率（CER）达到4.9%，同时发布了训练代码、模型配置和评估脚本以支持后续研究

Conclusion: 成功构建了古尼泊尔语首个端到端手写文本识别系统，为低资源历史文字识别提供了可行方案和开源工具

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [30] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 研究评估了CLIP模型在医学影像中处理否定语句的能力，通过微调方法提升了模型对否定提示的检索准确性，同时分析了模型内部行为变化。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在医学影像任务中应用广泛，但存在处理否定语句能力不足的问题，这在医学诊断中尤为关键，需要改进模型对否定临床语言的理解能力。

Method: 使用Stanford AIMI CheXagent模型评估胸部X光图像检索能力，对比有无否定提示的表现，采用微调方法改进模型，并通过token attribution、t-SNE投影和attention-head ablation分析模型内部行为。

Result: 微调后CLIP模型处理否定语句的能力得到提升，但正提示评估准确率略有下降；通过内部行为分析揭示了微调方法如何重塑文本编码器对否定临床语言的表示。

Conclusion: 研究深入理解了CLIP模型的内部行为，通过微调方法改进了其对否定临床语言的处理能力，有助于提升医学AI设备的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [31] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: DiffeoMorph是一个端到端可微分框架，通过学习形态发生协议，指导相同智能体群体形成目标3D形状，使用注意力SE(3)等变图神经网络和基于3D Zernike多项式的形状匹配损失。


<details>
  <summary>Details</summary>
Motivation: 生物系统通过相同智能体的集体行为形成复杂三维结构，这种分布式控制如何产生精确全局模式不仅是发育生物学的核心问题，也是分布式机器人、可编程物质和多智能体学习的关键挑战。

Method: 引入DiffeoMorph框架，使用基于注意力的SE(3)等变图神经网络更新智能体位置和内部状态；提出基于3D Zernike多项式的形状匹配损失，通过双层优化（内层优化单位四元数对齐，外层更新智能体模型）实现SO(3)不变性。

Result: 通过系统基准测试验证了形状匹配损失相对于其他标准距离度量的优势；展示了DiffeoMorph能够仅使用最小空间线索形成从简单椭球体到复杂形态的各种形状。

Conclusion: DiffeoMorph为学习分布式形态发生协议提供了有效的可微分框架，能够指导智能体群体自组织形成目标3D形状，在发育生物学、分布式机器人和多智能体学习领域具有应用潜力。

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [32] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: GPA是一种改进的优化算法，通过解耦Nesterov方法中的插值常数，实现平滑的迭代平均，克服了DiLoCo和Schedule-Free等现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有平均化优化器如单工作者DiLoCo和Schedule-Free存在局限性：DiLoCo的周期性平均引入了双循环结构，增加了内存需求和超参数数量；Schedule-Free虽然维护均匀平均，但仍有改进空间。需要一种更高效、内存友好的平均化优化方法。

Method: GPA扩展了Nesterov方法的原始平均化公式，通过解耦插值常数实现平滑的迭代平均。它在每个步骤都进行平均，避免了双循环结构，只需单个额外缓冲区，简化了超参数调优。

Result: 在Llama-160M模型上，GPA相比基线(AdamW)达到相同验证损失的速度提升了24.22%。在ImageNet ViT任务中，小批量和大批量设置下分别实现了12%和27%的速度提升。理论证明GPA能匹配或超越原始优化器的收敛保证。

Conclusion: GPA是一种有效的优化算法改进，通过解耦插值常数实现平滑平均，在性能和效率上都优于现有的平均化优化器，同时保持了理论收敛保证。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [33] [Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing](https://arxiv.org/abs/2512.17161)
*Liad Lea Didi,Kobi Cohen*

Main category: cs.LG

TL;DR: SMILE算法：在干扰图建模的无线网络中，通过分布式学习实现频谱共享的全局稳定分配，结合多对一匹配与未知马尔可夫信道学习。


<details>
  <summary>Details</summary>
Motivation: 解决认知通信实体（如小区、子网络）在通信受限无线网络中的频谱共享问题，需要在干扰约束和未知时变信道环境下实现全局稳定的信道分配。

Method: 提出SMILE算法，将不安定多臂老虎机学习与图约束协调相结合，实现分布式探索-利用平衡，确保收敛到最优稳定分配。

Result: SMILE收敛到最优稳定分配，相对于完全信息基准实现对数遗憾，仿真验证了算法的鲁棒性、可扩展性和效率。

Conclusion: SMILE首次在随机时变不安定环境中实现了全局Gale-Shapley稳定的信道分配，为频谱共享提供了通信高效的分布式学习解决方案。

Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.

</details>


### [34] [BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions](https://arxiv.org/abs/2512.17198)
*Shao-Ting Chiu,Ioannis G. Kevrekidis,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: BumpNet是一种基于无网格基函数展开的稀疏神经网络框架，用于PDE数值解和算子学习，通过可训练基函数和动态剪枝实现模型简洁性和自适应。


<details>
  <summary>Details</summary>
Motivation: 传统径向基函数网络在PDE求解中存在局限性，需要一种能够充分利用现代训练技术、参数完全可训练且能动态适应求解区域的神经网络框架。

Method: 基于普通sigmoid激活函数构建基函数，所有参数（形状、位置、振幅）完全可训练，通过动态剪枝实现模型简洁性和h-自适应。结合PINNs、EDNNs和DeepONet分别提出Bump-PINNs、Bump-EDNN和Bump-DeepONet。

Result: 数值实验表明BumpNet框架在PDE求解和算子学习方面具有高效性和准确性，能够有效处理一般PDE、时间演化PDE和PDE算子学习问题。

Conclusion: BumpNet是一个通用的稀疏神经网络框架，通过可训练的基函数和动态剪枝机制，能够与现有神经网络架构有效结合，为PDE数值解和算子学习提供了高效准确的解决方案。

Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.

</details>


### [35] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: DM-KRR方法通过结合扩散映射核与动态感知验证策略，为复杂动力系统的长期预测提供了简单而强大的基准，在精度和数据效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出复杂的非线性动力学，难以进行长期准确预测。现有数据驱动模型在长期行为几何结构未知或表示不佳时性能会下降。

Method: 提出扩散映射核岭回归（DM-KRR）方法，结合扩散映射导出的数据驱动核与动态感知验证策略，无需显式流形重构或吸引子建模，即可适应系统不变集的内在几何结构。

Result: 在包括光滑流形、混沌吸引子和高维时空流在内的广泛系统中，DM-KRR在精度和数据效率上一致优于最先进的随机特征、神经网络和算子学习方法。

Conclusion: 长期预测能力不仅取决于模型表达能力，更关键的是通过动态一致的模型选择尊重数据中编码的几何约束。简单性、几何意识和强大的实证性能为复杂动力系统的可靠高效学习指明了有前景的路径。

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [36] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 该研究系统评估了五种时间序列预测模型在不同时间尺度（分钟、小时、天）和空间聚合水平（单个充电站到城市级别）上对电动汽车充电需求的预测效果，使用了四个真实世界数据集。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及以应对气候变化，其对电网管理的影响日益受到关注。预测EV充电需求成为重要研究问题，但现有研究缺乏对不同时间尺度和空间聚合水平下多种预测方法的系统性比较。

Method: 研究采用五种时间序列预测模型（包括传统统计方法、机器学习和深度学习方法），在四个公开真实世界数据集上，评估短期（分钟级）、中期（小时级）和长期（天级）预测性能，以及从单个充电站到区域和城市级别的空间聚合水平。

Result: 研究首次系统评估了EV充电需求预测在广泛时间尺度和空间聚合水平上的表现，为不同应用场景提供了模型选择依据，但具体结果需参考各数据集独立报告。

Conclusion: 该研究填补了EV充电需求预测领域系统性比较研究的空白，为电网管理者和研究人员提供了在不同时空尺度下选择合适预测方法的参考框架。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [37] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: SHARP-QoS是一个统一的联合QoS预测框架，通过双曲卷积提取分层特征，自适应特征共享机制，以及EMA损失平衡策略，有效解决了稀疏性、噪声和负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的QoS数据极其稀疏、噪声大且具有分层依赖关系，现有方法要么单独预测每个QoS参数导致计算成本高、泛化能力差，要么联合预测时因数值范围不一致导致负迁移和表示学习不足。

Method: 1. 使用双曲卷积在庞加莱球中提取QoS和上下文结构的分层特征；2. 提出自适应特征共享机制，允许信息丰富的QoS和上下文信号之间进行特征交换；3. 设计基于EMA的损失平衡策略，实现稳定的联合优化。

Result: 在包含2、3、4个QoS参数的三个数据集上评估，SHARP-QoS优于单任务和多任务基线模型，有效解决了稀疏性、异常值鲁棒性和冷启动等主要挑战，同时保持适度的计算开销。

Conclusion: SHARP-QoS通过统一策略解决了联合QoS预测中的关键挑战，包括分层依赖建模、负迁移缓解和表示学习不足，为可靠的联合QoS预测提供了有效解决方案。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [38] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于测量任意马尔可夫决策过程（MDP）对之间的状态相似性，解决了传统双模拟度量在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量（BSM）在分析单个MDP内状态相似性方面很有效，但在多个MDP之间的应用存在挑战。先前工作尝试将BSM扩展到MDP对，但由于缺乏完善的数学性质，限制了MDP间进一步的理论分析。

Method: 建立广义双模拟度量（GBSM），严格证明其三个基本度量性质：GBSM对称性、MDP间三角不等式和相同空间上的距离边界。利用这些性质，理论分析MDP间的策略迁移、状态聚合和基于采样的估计。

Result: GBSM提供了比标准BSM更严格的显式边界，并为估计提供了闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了理论发现，并展示了GBSM在多MDP场景中的有效性。

Conclusion: GBSM为测量任意MDP对之间的状态相似性提供了理论基础，具有严格的数学性质，能够支持跨MDP的策略迁移、状态聚合和采样估计等应用，性能优于传统BSM方法。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [39] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文提出了一种信息论度量R-EMID来量化角色扮演模型在分布偏移下的性能退化，并开发了强化学习框架来增强模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 角色扮演模型在实际应用中广泛使用，但在真实部署时性能下降。现有方法（如LLM-as-a-judge）无法提供细粒度的诊断来分析用户、角色和对话组合偏移如何影响模型泛化，缺乏形式化框架来表征RPM的泛化行为。

Method: 1. 提出信息论度量R-EMID（推理基础的有效互信息差异）来可解释地测量RPM性能退化；2. 推导R-EMID的上界来预测最坏情况泛化性能；3. 提出协同演化强化学习框架，自适应建模用户、角色和对话上下文之间的连接，增强对话响应生成概率估计。

Result: 评估显示：1. 用户偏移在所有偏移中风险最高；2. 强化学习是增强RPM泛化的最有效方法；3. R-EMID能够有效量化RPM在不同分布偏移下的性能退化。

Conclusion: 本文提出的R-EMID为角色扮演模型的泛化性能提供了可解释的量化框架，揭示了不同分布偏移对性能的影响机制，并通过强化学习框架有效提升了模型在实际部署中的泛化能力。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [40] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: MINPO是一个统一的神经网络框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学，通过学习非局部算子及其逆来直接重构未知解场。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统表现出由积分-微分方程描述的非局部时空行为。经典方法需要重复计算卷积积分，成本随核复杂度和维度快速增加。现有神经求解器可以加速特定计算，但不能泛化到不同的非局部结构。

Method: MINPO使用KANs或MLPs作为编码器，直接学习非局部算子及其逆的神经表示，然后显式重构未知解场。学习过程通过轻量级非局部一致性损失项来保证学习算子与重构解之间的一致性。

Result: 与经典技术和基于MLPs的先进神经策略（A-PINN、fPINN）及其KAN变体（A-PIKAN、fPIKAN）相比，MINPO在准确性上表现出色，并展示了在处理不同核类型、不同核维度和重复核积分计算需求方面的鲁棒性。

Conclusion: MINPO超越了特定问题的表述，为受非局部算子支配的系统提供了一个统一的框架，能够自然捕获并高效解决由广泛IDE谱及其子集（包括分数阶PDE）支配的非局部时空依赖关系。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [41] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD是一个半监督学习框架，通过结合深度表示学习、图标签传播和最优传输理论，在仅有三分之一标注数据的情况下实现近乎完美的阿尔茨海默病诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断面临临床评估昂贵且侵入性强的问题，导致神经影像数据集中只有部分样本有真实标签，需要开发能够利用大量未标注数据的半监督学习方法。

Method: 提出MATCH-AD框架，整合深度表示学习、基于图的标签传播和最优传输理论（Wasserstein距离），利用神经影像数据的流形结构将诊断信息从有限标注样本传播到大量未标注样本。

Result: 在NACC近5000名受试者数据集上，尽管只有不到三分之一的样本有真实标签，MATCH-AD实现了近乎完美的诊断准确率，Kappa系数显示几乎完全一致，显著优于所有基线方法。

Conclusion: 该框架证明原则性半监督学习能够释放全球积累的部分标注神经影像数据的诊断潜力，大幅减少标注负担，同时保持适合临床部署的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [42] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: 提出一个基于图论的固有可解释性标准，使用结构局部解释和全局重组，区分"可解释"与"已解释"模型，并以临床心血管风险模型PREDICT为例验证


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能缺乏一致的固有可解释性定义和测试标准，现有工作要么依赖指标，要么诉诸直觉。需要建立严谨、可验证的固有可解释性标准

Method: 使用图论表示和分解模型，形成结构局部解释作为可验证的假设-证据结构注释，然后重组为全局解释。提出区分"可解释"（允许解释）和"已解释"（具有验证解释）的概念

Result: 提出的标准与现有固有可解释性直觉一致，能解释为什么大型回归模型可能不可解释而稀疏神经网络可能可解释。成功为新西兰临床使用的心血管风险模型PREDICT提供了完整解释，证明其固有可解释性

Conclusion: 该工作为可解释性研究提供了结构化框架，为监管机构提供了灵活而严谨的合规测试标准，有助于推动可解释人工智能的规范化发展

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [43] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 论文通过因果机制验证，证明上下文学习可分解为任务图式（抽象任务类型识别）和绑定（具体输入输出关联）两个可分离机制，揭示了ICL的双过程理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究将上下文学习视为单一机制（检索式、梯度下降式或纯贝叶斯式），缺乏对其内部机制的因果验证和分解理解。需要探究ICL是否由可分离的神经机制组成。

Method: 在9个模型（7个Transformer家族和Mamba，370M-13B参数）上进行激活修补实验，通过晚期MLP修补实现任务图式100%转移，通过残差流修补实现绑定62%转移，证明机制可分离。

Result: 1. 双重分离：任务图式与绑定机制可分离；2. 先验-图式权衡：图式依赖与先验知识负相关；3. 架构普适性：机制在所有测试架构中都存在，包括非Transformer的Mamba模型。

Conclusion: 上下文学习由任务图式和绑定两个可分离的神经机制组成，而非单一机制。模型在先验知识缺失时依赖任务图式，先验知识通过注意力误路由干扰而非直接输出竞争。这解释了为什么任意映射能成功而事实覆盖会失败，瓶颈在于注意力层面而非输出层面。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [44] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 提出LLM-SGA框架和ARHOCD检测器，通过集成多个基础检测器、动态权重分配和对抗训练，提升有害内容检测的对抗鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体有害内容检测模型容易受到对抗攻击，现有方法难以同时实现高泛化性和准确性，需要开发更鲁棒的检测系统。

Method: 1) 提出LLM-SGA框架，利用文本对抗攻击的关键不变性；2) 实例化ARHOCD检测器，包含三个组件：多基础检测器集成、基于可预测性和能力动态调整权重的分配方法、迭代优化基础检测器和权重分配器的对抗训练策略。

Result: 在仇恨言论、谣言和极端主义内容三个数据集上评估，ARHOCD在对抗条件下表现出强泛化性和更高的检测准确性。

Conclusion: ARHOCD通过创新的框架设计和组件，有效提升了有害内容检测的对抗鲁棒性，解决了现有研究的局限性。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [45] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 论文发现奖励模型和LLM-as-a-Judge系统存在漏洞：低困惑度的控制令牌序列可以翻转二元评估结果，导致高误判率，并提出对抗训练方法来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和LLM-as-a-Judge系统在现代后训练流程（如RLHF、DPO、RLAIF）中至关重要，但它们存在安全漏洞，可能被恶意利用进行奖励攻击，影响模型选择和微调。

Method: 提出AdvJudge-Zero方法，利用模型的下一令牌分布和束搜索探索从头发现多样化的控制令牌序列，分析隐藏状态扰动集中在低秩"软模式"中，并与拒绝方向反对齐。

Result: 实验表明这些控制令牌在数学和推理基准测试中导致大型开放权重和专业评判模型对错误答案产生极高的误判率（假阳性）。

Conclusion: 基于LoRA的对抗训练可以在少量控制令牌增强的示例上显著减少误判率，同时保持评估质量，为评判系统的安全性提供了实用解决方案。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [46] [DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference](https://arxiv.org/abs/2512.17398)
*Yonathan Bornfeld,Shai Avidan*

Main category: cs.LG

TL;DR: 提出一种新的私有推理激活模块，通过原型通道机制大幅减少DReLU计算，在保持隐私的同时提升效率


<details>
  <summary>Details</summary>
Motivation: 私有推理中的主要计算瓶颈是ReLU门计算，现有方法致力于减少ReLU数量，但仍有优化空间

Method: 设计新的激活模块：仅对原型通道执行DReLU操作，复制通道从对应原型通道神经元复制DReLU结果，并扩展到跨层应用

Result: 大幅减少ResNet类型网络中的DReLU操作数量；理论上能解决扩展XOR问题（仅需1个非线性和2个神经元）；在多个分类任务和图像分割上达到SOTA结果

Conclusion: 提出的原型通道机制能有效减少私有推理中的DReLU计算开销，在理论和实验上都取得了显著效果

Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.

</details>


### [47] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 本文提出了一个用于医学影像模型性能亚组分析的统计工具箱，解决样本量差异、多重比较校正等统计挑战，并在皮肤病变和胸部X光数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 在医学影像机器学习模型中，按患者和记录属性进行分层性能分析已成为标准做法，能揭示重要模型失败模式。然而，以统计严谨的方式进行此类分析具有挑战性：需要选择合适的性能指标以适应不同样本量和基础率，确定指标不确定性，校正多重比较，并在交叉分析中从组合众多的亚组中找到"最有趣"的亚组。

Method: 开发了一个统计工具箱，专门解决医学影像应用中的亚组性能差异分析挑战。该工具箱包含：1）适用于不同样本量和基础率的性能指标选择；2）指标不确定性的确定方法；3）多重比较校正机制；4）交叉分析中从组合众多亚组中识别"最有趣"亚组的机制。

Result: 在ISIC2020皮肤病变恶性分类数据集和MIMIC-CXR胸部X光疾病分类数据集的两个案例研究中展示了工具箱的分析能力。该工具箱使实践者能够轻松而严谨地评估模型潜在的亚组性能差异。

Conclusion: 提出的统计工具箱解决了医学影像机器学习模型亚组性能分析的统计挑战，使研究人员能够以统计严谨的方式进行分层性能分析，识别模型在不同患者亚组中的性能差异，这对于确保模型公平性和临床适用性至关重要。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [48] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 提出一个基于多智能体强化学习的模型，用于模拟和分析长期电力市场机制，支持政策制定者设计和评估电力系统脱碳路径。


<details>
  <summary>Details</summary>
Motivation: 电力系统是向零碳经济转型的关键，长期电力市场机制（包括拍卖、支持计划和其他政策工具）对塑造发电结构至关重要。目前缺乏先进工具来支持政策制定者和其他利益相关者设计、测试和评估长期市场。

Method: 采用多智能体强化学习模型，发电公司作为利润最大化主体在批发电力市场中做出投资决策。使用独立近端策略优化算法，适合去中心化和竞争性环境。通过广泛的超参数搜索确保去中心化训练产生符合竞争行为的结果。模型应用于意大利电力系统的简化版本。

Result: 结果强调了市场设计对电力部门脱碳和避免价格波动的关键作用。该框架能够评估多种政策和市场机制同时交互的长期电力市场，市场参与者能够响应和适应脱碳路径。

Conclusion: 提出的多智能体强化学习框架为政策制定者提供了评估长期电力市场设计的有效工具，特别是在多种政策和市场机制交互的复杂环境中，有助于实现电力系统脱碳目标。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [49] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: WG-KV通过写门控机制学习预测token效用，在进入KV缓存前过滤低效用状态，显著减少内存使用并提升推理速度


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理存在二次注意力复杂度和线性KV缓存增长瓶颈，现有方法通过后处理选择或驱逐来缓解，但忽视了根本低效问题：无差别写入持久内存

Method: 将KV缓存管理形式化为因果系统，包含KV准入、选择和驱逐三个原语。通过Write-Gated KV实现KV准入，这是一个轻量级机制，学习在token进入缓存前预测其效用，过滤低效用状态，维护紧凑全局缓存和滑动局部缓存

Result: 在Llama模型上，内存使用减少46-57%，预填充速度提升3.03-3.45倍，解码速度提升1.89-2.56倍，精度损失可忽略，且兼容FlashAttention和分页KV系统

Conclusion: 学习"写什么"是实现高效长上下文推理的原则性和实用方法，Write-Gated KV通过早期过滤低效用状态有效解决了KV缓存管理问题

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [50] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Lite-STGNN是一个轻量级时空图神经网络，通过分解式时间建模和可学习稀疏图结构，在长期多元预测中实现高效准确预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长期多元时间序列预测中面临计算复杂度高、参数过多的问题，需要更轻量、高效且可解释的框架。

Method: 结合趋势-季节性分解的时间模块，以及使用低秩Top-K邻接学习和保守水平门控的空间模块，通过空间校正增强线性基线。

Result: 在四个基准数据集上达到最先进精度（最长720步预测），参数效率高，训练速度显著快于基于Transformer的方法。消融研究显示空间模块带来4.6%提升，Top-K增强局部性3.3%。

Conclusion: Lite-STGNN提供了一个紧凑、可解释且高效的长期多元时间序列预测框架，在准确性和效率之间取得良好平衡。

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [51] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: 使用深度学习替代模型加速Inconel 625高温合金的蠕变模拟，BiLSTM-VAE提供概率预测，BiLSTM-Transformer实现高精度确定性预测，比传统ANSYS模拟快数百倍。


<details>
  <summary>Details</summary>
Motivation: Inconel 625等高温合金的时间依赖性变形（特别是蠕变）是航空航天和能源系统组件长期可靠性的关键因素。虽然Inconel 625具有优异的抗蠕变性，但ANSYS等工具中的有限元蠕变模拟计算成本高昂，单个10,000小时模拟需要数十分钟，限制了设计优化和结构健康监测的效率。

Method: 使用ANSYS基于Norton定律在单轴应力50-150 MPa和温度700-1000°C条件下生成蠕变应变数据，训练两种深度学习架构：1) BiLSTM变分自编码器（BiLSTM-VAE）用于不确定性感知和生成预测；2) BiLSTM-Transformer混合模型，利用自注意力机制捕捉长期时间行为。两种模型都作为替代预测器，BiLSTM-VAE提供概率输出，BiLSTM-Transformer提供高确定性精度。

Result: BiLSTM-VAE提供稳定可靠的蠕变应变预测，BiLSTM-Transformer在整个时间范围内实现强精度。性能评估使用RMSE、MAE和R²指标。延迟测试显示显著加速：每个ANSYS模拟需要30-40分钟，而替代模型在几秒内产生预测，速度提升数百倍。

Conclusion: 提出的深度学习框架为高温合金应用提供了快速蠕变评估的解决方案，支持设计优化和结构健康监测，是可扩展的高效替代方案，显著降低了计算成本和时间。

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [52] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: SafeBench-Seq：一个基于公开数据的蛋白质序列危害筛查基准测试，通过同源性聚类确保训练/测试集无重叠，提供可解释特征和校准概率评估。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计基础模型存在生物安全风险，但社区缺乏简单、可复现的序列级危害筛查基准，特别是在同源性控制下评估且能在普通CPU上运行。

Method: 使用公开数据（SafeProtein危害数据和UniProt良性数据），提取可解释特征（全局理化描述符和氨基酸组成），在≤40%同源性下聚类数据集，进行聚类级留出验证（训练/测试集无聚类重叠），提供校准概率并评估概率质量。

Result: 随机分割会显著高估模型鲁棒性；校准线性模型表现出较好的校准性，而树集成模型保留略高的Brier/ECE分数；基准测试仅需CPU、可复现，且只发布元数据（不发布危害序列）。

Conclusion: SafeBench-Seq为蛋白质序列危害筛查提供了一个简单、可复现的基准测试，通过同源性控制评估确保模型泛化能力，同时保护生物安全不发布危害序列。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [53] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: CFF算法通过层间协作机制改进Forward-Forward算法，解决了原算法层间隔离问题，在保持前向计算优势的同时提升收敛效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统Forward-Forward算法存在层间隔离问题，各层独立优化goodness函数，缺乏集体学习动态，限制了表征协调和深层架构的收敛效率。

Method: 提出协作式Forward-Forward学习框架，包含两种协作范式：固定协作（F-CFF）和自适应协作（A-CFF）。协作goodness函数整合所有层的加权贡献，实现协调特征学习。

Result: 在MNIST和Fashion-MNIST数据集上评估显示，相比基线Forward-Forward实现有显著性能提升。

Conclusion: 层间协作是Forward-Forward学习的重要增强，适用于神经形态计算架构和能源受限AI系统。

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [54] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 提出新的贝叶斯优化变体，用于具有解耦黑盒约束的问题，通过知识梯度获取函数，仅评估相关约束以提高效率


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在昂贵的全局黑盒优化问题中表现出色，但对于具有解耦约束的问题，现有方法可能效率不高。通常只有少数约束在最优解处是有效的，因此需要仅评估相关约束来优化函数

Method: 提出了基于知识梯度获取函数的贝叶斯优化新变体，专门处理解耦黑盒约束问题。该方法能够识别并仅评估与优化相关的约束，而不是评估所有约束函数

Result: 通过实证基准测试，这些新方法在性能上超越了现有最先进的方法，证明了其在处理解耦约束优化问题上的优越性

Conclusion: 提出的贝叶斯优化变体能够有效处理解耦黑盒约束问题，通过智能选择评估相关约束，显著提高了优化效率，为这类问题提供了更优的解决方案

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [55] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: 开发用于静态和单事件动态网络的图表示学习新算法，基于潜在空间模型，创建结构感知的网络表示，实现统一学习过程


<details>
  <summary>Details</summary>
Motivation: 开发能够自然捕捉网络重要特性（如同质性、传递性、平衡理论）的图表示学习方法，创建结构感知的网络表示，避免启发式和多阶段处理过程

Method: 基于潜在空间模型，特别是潜在距离模型，开发统一的网络嵌入学习过程，消除后处理步骤的需求

Result: 实现了能够表征网络结构、识别极端轮廓、量化时间网络影响动态的结构感知网络表示，支持层次结构表达和社区特征化

Conclusion: 成功开发了统一且强大的网络嵌入方法，能够全面表征网络结构并处理多样化的图分析任务

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [56] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL框架通过预测未来约束违规来改进自动驾驶中的安全强化学习，在真实数据集上验证了其在奖励-安全权衡、成功率和成本降低方面的有效性，同时提升了对观测噪声的鲁棒性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的安全强化学习需要在性能优化和安全约束之间取得平衡，传统方法要么过于保守影响效率，要么过于冒险导致安全违规。SRPL框架通过预测未来安全违规来解决这一矛盾，但此前主要在受控环境中验证，需要考察其在真实自动驾驶场景中的有效性。

Method: 使用Safety Representations for Safer Policy Learning (SRPL)框架，该框架为智能体配备预测未来约束违规的模型。在Waymo Open Motion Dataset (WOMD)和NuPlan两个真实自动驾驶数据集上进行系统实验，评估SRPL在奖励-安全权衡、成功率和成本降低方面的表现，同时测试其对观测噪声的鲁棒性和跨数据集泛化能力。

Result: SRPL能显著改善奖励-安全权衡，在成功率（效应大小r=0.65-0.86）和成本降低（效应大小r=0.70-0.83）方面取得统计显著改进（p<0.05）。预测安全表示对提升观测噪声鲁棒性起关键作用，在零样本跨数据集评估中，SRPL增强的智能体比非SRPL方法表现出更好的泛化能力。但有效性取决于底层策略优化器和数据集分布。

Conclusion: 预测安全表示在真实自动驾驶场景中能有效增强安全强化学习，改善安全-性能权衡，提升鲁棒性和泛化能力，展示了SRPL框架在实际应用中的潜力。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [57] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 该论文研究了在数据分散且无法共享的医疗领域，如何通过异步协作（仅共享已训练模型）结合stitching技术来提升模型性能，实现竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据分散且无法共享的领域，传统联邦学习需要同步训练和权重交换，存在局限性。研究探索异步协作（仅共享已训练模型）对性能的影响。

Method: 提出使用stitching（缝合）技术来组合独立训练的模型，通过在适当位置添加stitching层来结合中间表示，实现模型融合。

Result: 研究发现：1）单方数据训练模型在自身数据上性能相似，但在其他方数据上表现较差；2）集成模型泛化更好但各方自身数据性能下降；3）stitching技术能恢复竞争性性能并保持改进的泛化能力。

Conclusion: 异步协作通过stitching技术组合独立训练模型，能在保持各方自身数据性能的同时提升泛化能力，为数据分散场景提供可行的解决方案。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [58] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: 论文提出了一种分布式参数神经网络（DiPaNet）的统一表示框架，将有限和无限维神经网络架构通过同质化/离散化联系起来，并推导了神经元数量和隐藏层数趋于无穷时的近似误差。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络架构在隐藏层神经元数量和层数趋于无穷时的极限情况，建立有限维和无限维神经网络之间的统一理论框架，为不同神经网络架构提供统一的数学表示。

Method: 1. 首先推导单隐藏层神经网络的积分无限宽度表示；2. 扩展到具有有限积分隐藏层和残差连接的深度残差CNN；3. 重新审视神经ODE与深度残差NN的关系，通过离散化技术形式化近似误差；4. 将两种方法合并为统一的DiPaNet表示。

Result: 提出了DiPaNet作为神经网络的统一同质表示，证明了大多数现有的有限和无限维神经网络架构都可以通过DiPaNet表示的同质化/离散化联系起来，并推导了神经元数量和隐藏层数相关的近似误差。

Conclusion: DiPaNet框架为神经网络提供了统一的数学表示，建立了有限维和无限维架构之间的理论联系，为神经网络的进一步推广和应用提供了理论基础，并讨论了与神经场的异同。

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [59] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 论文提出了一种系统分析算法在扰动、噪声和系统互连影响下收敛性的理论框架，通过逆Lyapunov定理量化扰动影响，并应用于分布式学习、机器学习泛化、隐私保护等多个场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的算法通常在复杂的物理、社会和工程系统中运行，会受到扰动、噪声以及与其他动态系统互连的影响。现有分析多关注孤立环境下的算法收敛性，缺乏对扰动影响下算法性能的系统性理论分析。

Method: 利用逆Lyapunov定理，推导出量化扰动影响的关键不等式，将已知孤立环境下算法的收敛保证扩展到存在扰动的情况，系统性地推导出稳定性边界和收敛速率。

Result: 建立了扰动下算法分析的理论框架，能够量化扰动对算法性能的影响，并成功应用于分布式学习的通信约束、机器学习泛化的敏感性分析、以及隐私保护中的有意噪声注入等多个实际场景。

Conclusion: 该研究为分析噪声、扰动和系统互连影响下的算法性能提供了一个统一的工具，填补了算法鲁棒性分析的理论空白，具有广泛的应用价值。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [60] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: 提出了一种结合硬优先级和易优先级的混合训练策略，通过交替训练算法提升PINNs在求解偏微分方程时的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs训练策略存在局限性：硬优先级方法（受有限元方法启发）和易优先级方法各有优劣，在不同类型PDE上表现不一致且存在明显权衡。需要开发更稳健的训练策略来提升PINNs在各种PDE问题上的性能。

Method: 提出混合训练策略，结合硬优先级和易优先级的优势，采用交替训练算法。该方法能够处理具有陡峭梯度、非线性和高维度的PDE问题。

Result: 在具有陡峭梯度、非线性和高维度的PDE上，该方法实现了持续高精度，相对L2误差主要在O(10^-5)到O(10^-6)范围内，显著超越基线方法。同时在不同问题上表现出更好的可靠性，而对比方法往往因PDE类型不同而精度变化较大。

Conclusion: 该工作为设计混合训练策略提供了新见解，能够增强PINNs的性能和鲁棒性，为解决不同类型PDE问题提供了更可靠的训练方法。

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [61] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: SCOPE是一种新的规范性过程监控方法，使用后向归纳和因果学习来优化连续干预序列，直接利用观测数据，无需过程模拟或强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有的规范性过程监控方法在处理连续干预时存在局限：要么只关注单一干预决策，要么将多个干预视为独立事件，忽略了干预随时间推移的相互作用。依赖模拟或数据增强的方法会引入现实差距和偏差。

Method: SCOPE采用后向归纳法估计每个候选干预行动的效果，将其影响从最终决策点向前传播到第一个决策点。利用因果学习器直接使用观测数据，无需构建过程近似模型进行强化学习训练。

Result: 在现有合成数据集和新半合成数据集上的实验表明，SCOPE在优化关键绩效指标方面始终优于最先进的规范性过程监控技术。新的半合成设置基于真实事件日志，为未来的连续规范性过程监控研究提供了可重复的基准。

Conclusion: SCOPE通过后向归纳和因果学习解决了连续干预对齐问题，能够直接利用观测数据优化关键绩效指标，为规范性过程监控中的连续决策提供了更有效的方法。

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [62] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO是一个混合训练框架，通过在同一训练实例中交替使用SFT和RL，解决了传统两阶段训练中SFT抑制探索的问题，在数学推理基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的两阶段训练流程（先SFT后RL）存在不一致性：SFT强制模仿会抑制探索并导致遗忘，限制了RL的改进潜力。需要一种更高效的训练框架来统一外部监督和自我探索。

Method: TRAPO框架在同一训练实例中交替优化SFT损失（在专家前缀上）和RL损失（在模型自己的补全上）。引入Trust-Region SFT（TrSFT）来稳定训练，最小化信任区域内的前向KL散度，在区域外衰减优化。还包含自适应前缀选择机制，根据测量效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO始终优于标准SFT、RL、SFT-then-RL流程以及最近的最先进方法，为推理增强的LLMs建立了强大的新范式。

Conclusion: TRAPO通过统一SFT和RL，解决了传统两阶段训练的不一致性，提供了一种更有效的LLM推理能力提升方法，在数学推理任务上取得了显著改进。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [63] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: 使用神经网络估计医疗辐射场中散射辐射空间分布的方法研究，通过Geant4蒙特卡洛模拟生成三个复杂度递增的数据集，评估卷积和全连接网络架构，并开源数据集和训练流程。


<details>
  <summary>Details</summary>
Motivation: 为医疗辐射防护剂量学（如介入放射学和心脏病学）开发准确估计散射辐射空间分布的方法，解决传统测量方法耗时且复杂的问题。

Method: 使用基于Geant4的蒙特卡洛模拟生成三个复杂度递增的合成数据集，评估卷积神经网络和全连接神经网络架构，比较不同设计决策对重建辐射通量和能谱空间分布的效果。

Result: 论文展示了神经网络在重建医疗辐射场空间分布方面的可行性，识别了有效的网络架构设计决策，所有数据集和训练流程已开源发布。

Conclusion: 神经网络可以有效地估计医疗辐射场中的散射辐射空间分布，为辐射防护剂量学提供了一种高效的计算方法，开源资源将促进该领域的进一步研究。

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [64] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出"多谐级联"深度学习架构，基于随机函数理论和无差别原理，通过解全局线性系统而非梯度下降进行训练，在MNIST上实现快速学习且无过拟合


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构缺乏严格的数学理论基础，梯度下降训练方法存在收敛慢、易过拟合等问题。需要一种既能逼近任意复杂非线性函数，又能保持全局光滑性和概率解释的架构，并提供更高效、理论更严谨的训练方法。

Method: 提出多谐级联架构，由多谐样条包序列组成，每层基于随机函数理论和无差别原理严格推导。训练方法采用替代梯度下降的全局线性系统求解：在每个批次上，针对固定"星座"节点处的函数值求解单一全局线性系统，实现所有层的同步更新。

Result: 该方法保持各层概率解释和与原始模型的理论一致性，计算可简化为高效的2D矩阵运算，适合GPU执行。在MNIST数据集上展示了快速学习且无过拟合的性能。

Conclusion: 多谐级联架构提供了一种理论严谨、计算高效的深度学习新范式，通过全局线性系统求解替代梯度下降，在保持概率解释和全局光滑性的同时，实现了快速收敛和良好泛化性能。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [65] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO是一个端到端的单细胞转录组特征选择框架，通过联合学习离散基因子集和预测任务，实现紧凑且信息丰富的基因选择。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法多为多阶段流程或依赖事后特征归因，导致选择与预测任务弱耦合，需要训练额外的下游分类器。

Method: 提出YOTO框架，在单一可微分架构中联合识别离散基因子集并进行预测，通过稀疏性约束确保只有被选基因参与推理，采用多任务学习设计共享表征。

Result: 在两个代表性单细胞RNA-seq数据集上评估，YOTO始终优于最先进的基线方法。

Conclusion: 稀疏、端到端、多任务的基因子集选择提高了预测性能，产生了紧凑且有意义的基因子集，推动了生物标志物发现和单细胞分析。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [66] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 本文首次为具有线性函数逼近和本地训练的联邦SARSA算法提供了收敛性保证，量化了异构性影响，证明了多智能体线性加速效果。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习中的异构性（本地转移和奖励的差异）对算法收敛的影响尚未得到充分理论分析，需要为FedSARSA提供收敛保证和复杂度界限。

Method: 提出了新的多步误差展开分析框架，用于单智能体SARSA算法；在此基础上分析FedSARSA在异构环境下的收敛性，考虑多个本地更新步骤。

Result: 首次建立了FedSARSA在异构环境下的样本和通信复杂度界限，精确量化了异构性影响，证明了算法在智能体数量上的线性加速效果（除马尔可夫采样高阶项外）。

Conclusion: FedSARSA在异构联邦强化学习环境中具有理论收敛保证，能够实现多智能体线性加速，数值实验验证了理论发现。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [67] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: 提出了一种空间感知的Transformer架构，通过可学习的协方差核将地统计归纳偏置注入自注意力机制，在保持深度学习灵活性的同时融入空间拓扑约束，实现了高维时空过程的高精度概率预测。


<details>
  <summary>Details</summary>
Motivation: 传统地统计学（如高斯过程）具有理论严谨性和精确的不确定性量化，但计算复杂度高，难以处理大规模传感器网络；而现代Transformer架构虽然擅长序列建模，但缺乏空间几何归纳偏置，将空间传感器视为排列不变的token，无法理解距离关系。需要一种既能保持深度学习灵活性又能融入空间拓扑约束的方法。

Method: 提出空间感知Transformer，通过可学习的协方差核将地统计归纳偏置直接注入自注意力机制。将注意力结构形式化分解为平稳的物理先验和非平稳的数据驱动残差，施加软拓扑约束，优先考虑空间邻近交互，同时保留建模复杂动态的能力。

Result: 在合成高斯随机场和真实世界交通基准测试中，该方法优于最先进的图神经网络。网络能够通过反向传播端到端地恢复底层过程的真实空间衰减参数（称为"深度变异函数"现象）。严格的统计验证表明，该方法不仅提供更优的预测精度，还能产生良好校准的概率预测。

Conclusion: 该方法有效桥接了物理感知建模和数据驱动学习之间的差距，将地统计学的理论严谨性与深度学习的灵活高容量表示相结合，为高维时空过程建模提供了新的解决方案。

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [68] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA：一种基于拉普拉斯近化的权重空间正则化方法，应用于LoRA微调，通过估计参数置信度来约束高曲率方向的更新，减少灾难性遗忘，保持预训练知识。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法（如LoRA）虽然能快速适应下游任务，但会导致模型遗忘预训练阶段获得的领域知识（灾难性遗忘）。需要一种方法在保持微调效率的同时，减少知识遗忘。

Method: LaLoRA将拉普拉斯近似应用于LoRA权重，估计模型对每个参数的置信度，约束高曲率方向的参数更新。该方法仅对LoRA权重应用拉普拉斯近似，保持轻量级特性。

Result: 在Llama模型数学推理微调实验中，LaLoRA改善了学习-遗忘权衡，且可通过正则化强度直接控制这一权衡。研究了不同损失曲面曲率近似方法、拉普拉斯近似所用数据的影响，并验证了超参数鲁棒性。

Conclusion: LaLoRA通过权重空间正则化有效缓解了LoRA微调中的灾难性遗忘问题，在保持参数高效的同时，更好地平衡了新任务学习和预训练知识保留。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [69] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了ECHO基准测试，用于系统评估图神经网络在长距离信息传播方面的能力，包含三个合成图任务和两个真实化学数据集，揭示了现有GNN在长程交互处理上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 有效捕获长程交互是图神经网络研究中的基本但未解决的挑战，对科学应用至关重要。现有GNN在长距离信息传播方面存在局限，需要系统评估工具来推动该领域发展。

Method: 提出ECHO基准测试，包含三个合成图任务（单源最短路径、节点离心率、图直径）和两个真实化学数据集（ECHO-Charge和ECHO-Energy）。合成任务构建在具有信息瓶颈的挑战性拓扑上，化学数据集基于密度泛函理论计算，用于预测原子部分电荷和分子总能量。

Result: 对流行GNN架构的广泛基准测试揭示了明显的性能差距，强调了真正长程传播的困难，并指出了能够克服固有局限性的设计选择。

Conclusion: ECHO为评估长程信息传播设立了新标准，展示了其在科学AI中的必要性，为GNN在长程交互处理方面的改进提供了重要基准。

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [70] [Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments](https://arxiv.org/abs/2512.17771)
*Dong Chen,Zhengqing Hu,Shixing Zhao,Yibo Guo*

Main category: cs.LG

TL;DR: 提出Easy Adaptation方法，通过设计特定小模型来补充大模型未充分拟合的数据分布，无需访问大模型参数即可达到PEFT性能，且资源需求极低。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法面临两个主要挑战：1) 资源成本高，仍需要大量时间和内存；2) 参数依赖性强，需要更新大模型参数，但许多领先模型只提供API访问且费用昂贵。小模型在特定分布上表现优异且资源需求极低。

Method: 提出Easy Adaptation方法，设计特定小模型来补充大模型未充分拟合的数据分布。该方法不访问大模型参数，而是通过小模型来增强大模型在特定任务上的表现。

Result: 大量实验表明，EA方法在多样化任务上能够匹配PEFT的性能，同时不需要访问大模型参数，且仅需极少的资源。

Conclusion: EA方法为解决PEFT的资源成本和参数依赖问题提供了有效方案，通过特定小模型补充大模型能力，在资源受限环境中具有实用价值。

Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.

</details>


### [71] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出一种可校准的消歧损失（CDL），用于提升多示例部分标签学习（MIPL）的分类准确性和校准性能，可无缝集成到现有框架中。


<details>
  <summary>Details</summary>
Motivation: 现有MIPL方法存在校准性能差的问题，影响了分类器的可靠性，需要同时提升分类准确性和校准性能。

Method: 提出可插拔的可校准消歧损失（CDL），包含两种实现：第一种基于候选标签集的概率进行校准，第二种整合候选和非候选标签集的概率。

Result: 理论分析证明了CDL的下界和正则化特性，实验结果表明CDL在基准和真实数据集上显著提升了分类和校准性能。

Conclusion: CDL是一种有效的可插拔损失函数，能够同时改善MIPL和PLL框架的分类准确性和校准可靠性。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [72] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 本文研究了序列推荐中ID特征和模态特征的互补性，提出了一种简单的集成方法，证明两者结合能提升性能但不需要复杂的融合架构。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型对ID特征和模态特征（如文本）的互补性缺乏理解。一些工作完全用模态特征替代ID特征，另一些则采用复杂的多阶段训练或对齐架构来联合使用两者。本文旨在填补这一理解空白。

Method: 提出一种新的序列推荐方法：首先通过独立训练保持ID模型和文本模型的互补性，然后采用简单的集成策略来利用这种互补性。

Result: 该方法虽然简单，但优于多个竞争性序列推荐基线，表明ID和文本特征对于达到最先进的序列推荐性能都是必要的，但不需要复杂的融合架构。

Conclusion: ID特征和模态特征确实学习到了互补信号，简单的集成策略就能有效利用这种互补性，实现更好的推荐性能，而无需复杂的融合方法。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


### [73] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Wasserstein-Fisher-Rao几何的扩散采样方法，通过引入显式修正项和加权随机微分方程来改善非凸或多模态分布下的采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的扩散模型在非凸或多模态分布（如双势阱）下混合速率会指数级恶化，而许多实际生成建模任务涉及高度非对数凹的目标分布，因此需要开发能改善探索性的采样方案。

Method: 利用信息几何工具，通过Wasserstein-Fisher-Rao几何将样本空间中的传输与概率测度空间上的垂直（反应）动力学耦合，引入显式修正项，并使用Feynman-Kac表示通过加权随机微分方程实现重加权机制。

Result: 该研究对基于WFR的采样动力学进行了初步但严谨的探索，阐明了其几何和算子理论结构，为未来的理论和算法发展奠定了基础。

Conclusion: Wasserstein-Fisher-Rao几何为改善扩散采样在非凸分布下的探索能力提供了有前景的框架，通过结合传输和重加权机制可以克服传统扩散模型的局限性。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [74] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出一种基于正则化随机傅里叶特征(RRFF)和有限元重构映射(RRFF-FEM)的算子学习方法，用于从噪声数据中学习偏微分方程的解算子。该方法使用多元t分布采样随机特征，结合频率加权Tikhonov正则化抑制高频噪声，在保证精度的同时提高计算效率和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 核基算子学习方法虽然能提供理论保证的准确近似且训练需求较少，但在大规模训练集下计算代价高昂，且对噪声敏感。需要开发一种既能保持核方法理论优势，又能高效处理噪声数据的方法。

Method: 1. 使用多元Student's t分布采样随机傅里叶特征；2. 引入频率加权的Tikhonov正则化抑制高频噪声；3. 结合有限元重构映射(RRFF-FEM)增强算子学习；4. 理论分析随机特征矩阵的极端奇异值，证明当特征数N与训练样本数m满足N∝mlogm时系统良态。

Result: 1. 建立了随机特征矩阵极端奇异值的高概率界；2. 理论证明系统良态条件；3. 在多个基准PDE问题(对流、Burgers、达西流、Helmholtz、Navier-Stokes、结构力学)上验证了方法对噪声的鲁棒性；4. 相比未正则化的随机特征模型，训练时间减少且性能提升；5. 与核方法和神经算子方法相比保持竞争性精度。

Conclusion: RRFF和RRFF-FEM方法为从噪声数据中学习算子提供了一种高效、鲁棒的框架，在保持核方法理论优势的同时，解决了计算效率和噪声敏感性问题，在多种PDE应用中表现出优越性能。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [75] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: Torrent是一种分布式DMA架构，通过链式写入机制在不修改NoC硬件和互连协议的情况下实现高效的点对多点数据传输，相比单播基线最高可提升7.88倍性能。


<details>
  <summary>Details</summary>
Motivation: 现代SoC中计算能力与片上通信带宽之间的差距日益扩大，特别是对于AI等数据并行工作负载。高效的点对多点数据传输（如组播）对高性能至关重要，但标准互连协议缺乏原生组播支持。现有的P2MP解决方案需要修改网络硬件和协议，影响了可扩展性和兼容性。

Method: Torrent采用分布式DMA架构，通过Chainwrite机制在NoC上形成逻辑链，数据像链表一样遍历目标节点。开发了两种调度算法来确定基于NoC拓扑的最优链顺序，以优化性能和能耗。

Result: RTL和FPGA原型评估显示，相比网络层组播在性能、灵活性和可扩展性方面有显著优势。相比单播基线，Torrent最高可实现7.88倍加速。16nm ASIC合成显示面积开销仅1.2%，功耗开销2.3%。每个目标的周期开销为82CC，面积开销为207um²。

Conclusion: Torrent通过创新的Chainwrite机制实现了高效、可扩展的点对多点数据传输，无需修改NoC硬件和互连协议，在保持兼容性的同时显著提升了系统性能。

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>


### [76] [A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX](https://arxiv.org/abs/2512.17834)
*Darja Nonaca,Jérémy Guichemerre,Reinhard Wiesmayr,Nihat Engin Tunali,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种新型短块长多速率二进制LDPC码，在128位块长下优于5G-LDPC码，适用于URLLC应用，通过全并行消息传递实现14ns最低延迟解码。


<details>
  <summary>Details</summary>
Motivation: URLLC需要短块长编码，传统SCL解码的极化码虽然性能好但延迟高、面积效率差，而基于消息传递的LDPC码在短块长下性能不足，需要设计更优的短块长LDPC码。

Method: 设计新型短块长多速率二进制LDPC码，采用全并行消息传递解码架构，在GlobalFoundries 22FDX工艺上实现ASIC解码器，支持三种码率。

Result: 实现0.44mm²面积的解码器ASIC，达到同类最低14ns解码延迟，在码率1/2、128位块长下实现9Gb/s信息吞吐量和62pJ/b能效。

Conclusion: 提出的新型LDPC码在短块长下优于5G-LDPC码，其全并行解码器ASIC实现了极低延迟和高能效，适合URLLC应用需求。

Abstract: Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.

</details>
