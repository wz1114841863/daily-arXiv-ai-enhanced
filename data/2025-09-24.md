<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.LG](#cs.LG) [Total: 125]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Lightweight Congruence Profiling for Early Design Exploration of Heterogeneous FPGAs](https://arxiv.org/abs/2509.18295)
*Allen Boston,Biruk Seyoum,Luca Carloni,Pierre-Emmanuel Gaillardon*

Main category: cs.AR

TL;DR: 提出了一种基于Roofline模型的轻量级分析方法，通过三个一致性评分快速识别异构FPGA中的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: 随着FPGA从均匀逻辑阵列发展为集成DSP、存储器和专用加速器的异构架构，复杂的资源交互使得设计空间探索和应用优化变得困难

Method: 采用受Roofline模型启发的轻量级分析方法，引入三个一致性评分来识别异构资源、架构和应用逻辑相关的瓶颈

Result: 在Stratix 10类似FPGA上使用Koios和VPR基准套件进行评估，证明了该方法的有效性

Conclusion: 该方法支持高效的FPGA架构协同设计，能够提升异构FPGA的性能

Abstract: Field-Programmable Gate Arrays (FPGAs) have evolved from uniform logic arrays
into heterogeneous fabrics integrating digital signal processors (DSPs),
memories, and specialized accelerators to support emerging workloads such as
machine learning. While these enhancements improve power, performance, and area
(PPA), they complicate design space exploration and application optimization
due to complex resource interactions.
  To address these challenges, we propose a lightweight profiling methodology
inspired by the Roofline model. It introduces three congruence scores that
quickly identify bottlenecks related to heterogeneous resources, fabric, and
application logic. Evaluated on the Koios and VPR benchmark suites using a
Stratix 10 like FPGA, this approach enables efficient FPGA architecture
co-design to improve heterogeneous FPGA performance.

</details>


### [2] [Chiplet-Based RISC-V SoC with Modular AI Acceleration](https://arxiv.org/abs/2509.18355)
*P. Ramkumar,S. S. Bharadwaj*

Main category: cs.AR

TL;DR: 本文提出了一种基于小芯片的RISC-V SoC架构，通过模块化AI加速和智能系统级优化，解决了边缘AI设备在性能、能效和成本效益方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统单片SoC设计在先进工艺节点下制造良率低（低于16%），难以平衡高性能、能效和成本效益。需要一种新的架构来应对边缘AI设备的复杂需求。

Method: 采用小芯片架构，在30mm x 30mm硅中介层上集成4项关键技术：自适应跨芯片动态电压频率调节、AI感知的UCIe协议扩展、分布式加密安全以及智能传感器驱动的负载迁移。架构包含7nm RISC-V CPU小芯片、双5nm AI加速器、16GB HBM3内存和专用电源管理控制器。

Result: 在行业标准基准测试中，AI优化配置相比基础小芯片实现实现了14.7%延迟降低、17.3%吞吐量提升和16.2%功耗降低，综合效率提升40.1%，达到每MobileNetV2推理3.5mJ的能效。

Conclusion: 模块化小芯片设计能够实现接近单片计算密度的性能，同时具备成本效益、可扩展性和可升级性，适合下一代边缘AI设备应用。

Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while
maintaining architectural flexibility is a critical challenge in the
development and deployment of edge AI devices. Monolithic SoC designs struggle
with this complex balance mainly due to low manufacturing yields (below 16%) at
advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based
RISC-V SoC architecture that addresses these limitations through modular AI
acceleration and intelligent system level optimization. Our proposed design
integrates 4 different key innovations in a 30mm x 30mm silicon interposer:
adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware
Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring
streaming flow control units and compression-aware transfers; distributed
cryptographic security across heterogeneous chiplets; and intelligent
sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V
CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory
stacks, and dedicated power management controllers. Experimental results across
industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video
processing demonstrate significant performance improvements. The AI-optimized
configuration achieves ~14.7% latency reduction, 17.3% throughput improvement,
and 16.2% power reduction compared to previous basic chiplet implementations.
These improvements collectively translate to a 40.1% efficiency gain
corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while
maintaining sub-5ms real-time capability across all experimented workloads.
These performance upgrades demonstrate that modular chiplet designs can achieve
near-monolithic computational density while enabling cost efficiency,
scalability and upgradeability, crucial for next-generation edge AI device
applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [3] [Static Estimation of Reuse Profiles for Arrays in Nested Loops](https://arxiv.org/abs/2509.18684)
*Abdur Razzak,Atanu Barai,Nandakishore Santhi,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: 本文提出了一种新颖的静态分析框架，用于预测嵌套循环结构中数组引用的重用配置文件，无需运行时信息即可估计重用距离和缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 传统计算重用距离直方图需要程序执行和内存跟踪收集，这通常耗时且资源密集。静态预测虽然速度快但缺乏准确性，特别是在处理嵌套循环中的数组时。

Method: 通过分析循环边界、小问题规模下的访问模式以及预测方程，在编译时预测数组访问模式并估计重用距离和缓存命中率。

Result: 与PARDA工具相比，静态预测器实现了相当的准确性，同时在分析速度上提供了数量级的改进。

Conclusion: 这项工作为动态重用分析提供了实用的替代方案，并为集成到编译器和静态性能建模工具铺平了道路。

Abstract: Efficient memory access patterns play a crucial role in determining the
overall performance of applications by exploiting temporal and spatial
locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is
a widely used metric to quantify temporal locality, measuring the distance
between consecutive accesses to the same memory location. Traditionally,
calculating RDH requires program execution and memory trace collection to
obtain dynamic memory access behavior. This trace collection is often
time-consuming, resource-intensive, and unsuitable for early-stage optimization
or large-scale applications. Static prediction, on the other hand, offers a
significant speedup in estimating RDH and cache hit rates. However, these
approaches lack accuracy, since the predictions come without running the
program and knowing the complete memory access pattern, more specifically when
arrays are used inside nested loops. This paper presents a novel static
analysis framework for predicting the reuse profiles of array references in
programs with nested loop structures, without requiring any runtime
information. By analyzing loop bounds, access patterns in smaller problem
sizes, and predictive equations, our method predicts access patterns of arrays
and estimates reuse distances and cache hit rate at compile time. This paper
extends our previous study by incorporating more analysis and improving
prediction by addressing previously unhandled reuse patterns. We evaluate our
technique against a widely accepted traditional trace-driven profiling tool,
Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our
static predictor achieves comparable accuracy while offering
orders-of-magnitude improvement in the analysis speed. This work offers a
practical alternative to dynamic reuse profiling and paves the way for
integration into compilers and static performance modeling tools.

</details>


### [4] [Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs](https://arxiv.org/abs/2509.18886)
*Marcin Chrapek,Marcin Copik,Etienne Mettaz,Torsten Hoefler*

Main category: cs.PF

TL;DR: 本文研究了在CPU和GPU可信执行环境(TEEs)中运行大型语言模型(LLM)推理的可行性和性能，证明了TEEs可以以较小的性能开销（CPU TEEs低于10%吞吐量开销，GPU TEEs为4-8%）实现端到端的机密LLM推理。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在融合云和高性能计算基础设施上的部署增加，处理机密输入和专有数据集的LLMs面临更高的安全需求，这阻碍了在医疗和金融等隐私敏感行业的采用。

Method: 使用Intel TDX和SGX CPU TEEs（加速于AMX）运行完整的Llama2推理管道（7B、13B、70B），并在NVIDIA H100机密计算GPU上运行LLM推理，比较性能、成本和安全性权衡。

Result: CPU TEEs在各种数据类型、批大小和输入长度下施加低于10%的吞吐量和20%的延迟开销，AMX进一步减少开销；GPU TEEs的吞吐量惩罚为4-8%，随着批和输入大小增长而减少。

Conclusion: CPU TEEs在成本效益或安全性方面可能优于GPU TEEs，本文首次全面展示了现代TEEs在CPU和GPU上实现机密LLMs的性能和实用性。

Abstract: Large Language Models (LLMs) are increasingly deployed on converged Cloud and
High-Performance Computing (HPC) infrastructure. However, as LLMs handle
confidential inputs and are fine-tuned on costly, proprietary datasets, their
heightened security requirements slow adoption in privacy-sensitive sectors
such as healthcare and finance. We investigate methods to address this gap and
propose Trusted Execution Environments (TEEs) as a solution for securing
end-to-end LLM inference. We validate their practicality by evaluating these
compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,
we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,
70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions
(AMX). We derive 12 insights, including that across various data types, batch
sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency
overheads, further reduced by AMX. We run LLM inference on NVIDIA H100
Confidential Compute GPUs, contextualizing our CPU findings and observing
throughput penalties of 4-8% that diminish as batch and input sizes grow. By
comparing performance, cost, and security trade-offs, we show how CPU TEEs can
be more cost-effective or secure than their GPU counterparts. To our knowledge,
our work is the first to comprehensively demonstrate the performance and
practicality of modern TEEs across both CPUs and GPUs for enabling confidential
LLMs (cLLMs).

</details>


### [5] [Glass-Box Analysis for Computer Systems: Transparency Index, Shapley Attribution, and Markov Models of Branch Prediction](https://arxiv.org/abs/2509.19027)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.PF

TL;DR: 本文提出了三种玻璃盒分析工具：GTI指数量化性能方差可解释性，ETD方法分解吞吐量贡献，以及分支预测器的马尔可夫分析框架，并建立了硬件计数器事件率的可识别性理论。


<details>
  <summary>Details</summary>
Motivation: 计算机系统性能分析通常缺乏对内部特征的透明解释，需要量化工具来理解性能方差来源和系统行为。

Method: 开发了GTI透明度指数、基于Shapley值的ETD吞吐量分解方法、分支预测器的精确马尔可夫分析模型，以及硬件计数器事件率恢复的识别理论。

Result: GTI提供了性能方差解释的量化指标，ETD实现了效率保持的吞吐量归因，分支预测分析得到了闭式解，事件率恢复理论具有稳定性保证。

Conclusion: 提出的玻璃盒分析工具为计算机系统性能提供了可量化的透明解释框架，有助于深入理解系统内部行为和改进性能优化。

Abstract: We formalize glass-box analysis for computer systems and introduce three
principled tools. First, the Glass-Box Transparency Index (GTI) quantifies the
fraction of performance variance explainable by internal features and comes
equipped with bounds, invariances, cross-validated estimation, and bootstrap
confidence intervals. Second, Explainable Throughput Decomposition (ETD) uses
Shapley values to provide an efficiency-preserving attribution of throughput,
together with non-asymptotic Monte Carlo error guarantees and convexity
(Jensen) gap bounds. Third, we develop an exact Markov analytic framework for
branch predictors, including a closed-form misprediction rate for a two-bit
saturating counter under a two-state Markov branch process and its i.i.d.
corollary. Additionally, we establish an identifiability theorem for recovering
event rates from aggregated hardware counters and provide stability bounds
under noise.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: 本文评估了FireSim框架在模拟RISC-V处理器时的性能预测准确性，通过对比模拟结果与真实硬件性能，发现虽然FireSim能提供有价值的架构趋势分析，但模拟与实测运行时间仍存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V处理器在高性能计算领域的应用潜力日益显现，需要系统评估FireSim这一开源FPGA加速仿真框架的性能预测准确性，但目前缺乏对仿真结果与真实硬件性能的系统性对比研究。

Method: 研究通过FireSim框架模拟商业单板计算机和桌面级RISC-V CPU，首先通过基准测试比较单核和四核配置下的运行时行为，然后使用代表性小型应用和LAMMPS分子动力学代码评估性能。

Result: 研究发现FireSim虽然能提供架构性能趋势的有价值见解，但模拟运行时间与实测结果存在偏差，这些偏差源于仿真环境的固有限制和CPU制造商提供的详细性能规格信息有限。

Conclusion: FireSim是进行RISC-V架构探索的有用工具，但需要更精确的配置匹配和考虑仿真环境限制，以提高性能预测的准确性。

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [7] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 6G Twin是首个端到端AI原生无线接入网络设计，集成了神经高斯无线电场、持续信道预测和能量最优非线性预编码器，实现了100倍导频开销降低、毫秒级闭环操作和4-10倍能量效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决5G网络在信道状态信息获取、移动性管理和能量效率方面的瓶颈，设计一个完整的AI原生RAN架构来支持6G网络的实时性、动态性和能效要求。

Method: 采用三部分集成方法：(1)神经高斯无线电场(GRF)压缩CSI获取，(2)重放驱动的持续学习器进行信道预测和切换持久性，(3)最小功率多址信道(minPMAC)非线性预编码器进行能量最优传输。

Result: 实现了100倍导频开销降低，1.1毫秒推理时间，小于2分钟现场训练；信道NMSE改善超过10dB；能量效率提升4-10倍，在相同功率下数据速率提升高达5倍。

Conclusion: 6G Twin提供了一个实用的GPU就绪框架，在3GPP标准环境下实现了实时CSI获取、动态网络中的鲁棒跟踪和先进的吞吐量-能量权衡，为6G网络部署奠定了基础。

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [8] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: 本文介绍了ReproRAG框架，用于系统测量和量化基于向量的检索系统的可重现性，旨在解决RAG系统中检索组件非确定性问题。


<details>
  <summary>Details</summary>
Motivation: RAG在生成式AI驱动的科学工作流中应用日益广泛，但其可靠性常因检索组件的非确定性而受到损害，需要系统评估可重现性。

Method: 开发ReproRAG基准测试框架，研究整个流程中的不确定性来源，包括嵌入模型、精度、检索算法、硬件配置和分布式执行环境，并使用精确匹配率、Jaccard相似度和Kendall's Tau等指标。

Result: 大规模实证研究显示，不同嵌入模型对RAG可重现性有显著影响，框架能有效表征可重现性与性能之间的权衡。

Conclusion: 开源的ReproRAG框架为研究人员和工程师提供了验证部署、基准测试可重现性和做出明智设计决策的工具，从而促进更可信的科学AI。

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [9] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: TD3-Sched是一种基于TD3算法的分布式强化学习调度器，用于云边系统中的资源调度，相比现有方法能显著降低延迟并提高SLO合规性。


<details>
  <summary>Details</summary>
Motivation: 云边系统中的资源调度面临挑战，边缘节点运行延迟敏感的工作负载且资源受限，现有集中式调度器存在性能瓶颈和用户体验下降问题。

Method: 提出TD3-Sched分布式强化学习调度器，基于Twin Delayed Deep Deterministic Policy Gradient算法，对CPU和内存分配进行连续控制，在动态工作负载下实现优化的资源供应决策。

Result: 在真实云边测试平台上，TD3-Sched相比其他强化学习和基于规则的基线方法，在相同负载下延迟降低17.9%-38.6%，高负载下降低16%-31.6%，SLO违规率仅为0.47%。

Conclusion: TD3-Sched在容器化云边环境中表现出更快的收敛速度、更低的延迟和更稳定的性能，同时保持服务质量优于基线方法。

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [10] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: SJA是一种新的调度范式，通过在调度器和作业之间建立双向交互，将作业原子化为可适应执行间隙的子作业，以提高GPU集群的利用率。


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群（特别是基于NVIDIA MIG架构的）存在效率低下的问题，因为作业被视为刚性不可分割的块，依赖静态峰值内存估计导致碎片化、利用率不足和作业拒绝。

Method: 调度器公布可用执行间隙，作业响应信号表明是否能在提供的时间容量窗口内生成合适的子作业。调度器基于分配策略选择作业，被选中的作业生成针对该机会的安全自包含子作业。

Result: SJA主动在作业执行前塑造工作负载，避免昂贵的状态转移和不可预测的中断，旨在提高GPU利用率、减少等待时间和最小化迁移开销。

Conclusion: 本文作为概念论文介绍了SJA范式，定义了其构建模块并概述了未来研究方向，而非提供完整的实验评估。

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [11] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: SimAI-Bench是一个用于原型设计和评估耦合AI-仿真工作流的工具，在Aurora超级计算机上测试了两种常见模式的数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 随着耦合AI-仿真工作流在HPC设施中成为主要工作负载，其复杂性不断增加，需要新的性能分析和原型设计工具。

Method: 使用SimAI-Bench工具在Aurora超级计算机上对两种模式进行基准测试：一对一工作流（仿真与AI训练实例共置）和多对一工作流（多个仿真训练单个AI模型）。

Result: 对于一对一模式，节点本地和DragonHPC数据暂存策略比Redis和Lustre文件系统表现更好；对于多对一模式，随着集成规模增大，数据传输成为主要瓶颈，文件系统是最佳解决方案。

Conclusion: SimAI-Bench是评估耦合AI-仿真工作流性能的有效工具，不同工作流模式需要不同的数据管理策略来优化性能。

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [12] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文研究了在面向环网络中的领导者选举问题，在消息内容可能被任意篡改的内容无关异步消息传递系统中，分析了消息复杂度的限制和优化算法。


<details>
  <summary>Details</summary>
Motivation: 现有的领导者选举算法在面向环网络中具有较高的消息复杂度（O(n·ID_max)），本文旨在探索在内容无关模型中是否存在更高效的消息复杂度算法，特别是在进程发送消息数量受限的情况下。

Method: 通过理论分析证明在均匀设置下，如果每个进程在一个方向上只能发送常数数量的消息，则无法解决领导者选举问题。在非均匀设置下，提出了两种算法：一种基于ID_min的算法（O(n·U·ID_min)复杂度），另一种基于对数ID_min的算法（O(U·logID_min)复杂度）。在匿名设置下，提出了随机化算法。

Result: 证明了均匀算法在消息数量受限时的不可行性，提出了非均匀算法显著降低了消息复杂度（从依赖ID_max改进为依赖ID_min），并展示了匿名设置下的高效随机化算法。

Conclusion: 领导者选举的消息复杂度在内容无关模型中存在根本性限制，但通过非均匀假设和随机化方法可以显著优化性能，其中对数依赖ID_min的算法达到了最优复杂度。

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [13] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: RISC-V加速器在科学计算中的应用：天体物理N体代码在Wormhole n300卡上实现2倍加速和2倍节能


<details>
  <summary>Details</summary>
Motivation: 虽然RISC-V最初为AI工作负载开发，但研究发现其在高性能科学计算领域也具有潜力，特别是在天体物理模拟方面

Method: 将天体物理N体代码移植到Tenstorrent开发的RISC-V-based Wormhole n300加速卡上，并与高度优化的CPU实现进行对比

Result: RISC-V平台在该类算法上表现出高度竞争力，相比CPU实现实现了2倍以上的速度提升和约2倍的能耗节省

Conclusion: RISC-V-based加速器不仅适用于AI工作负载，在科学计算特别是天体物理模拟领域也具有显著优势

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [14] [Energy-convergence trade off for the training of neural networks on bio-inspired hardware](https://arxiv.org/abs/2509.18121)
*Nikhil Garg,Paul Uriarte Vicandi,Yanming Zhang,Alexandre Baigol,Donato Francesco Falcone,Saketh Ram Mamidala,Bert Jan Offrein,Laura Bégon-Lours*

Main category: cs.ET

TL;DR: 该论文研究了基于HfO2/ZrO2超晶格的铁电突触器件，通过硬件感知神经网络模拟分析短脉冲编程对能效的影响，并提出对称点偏移技术来恢复精度损失。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴传感器和植入式设备的部署增加，AI处理需求向极端边缘转移，需要超低功耗的连续操作。受大脑启发，新兴的忆阻器件有望通过消除计算和内存之间的昂贵数据传输来加速神经网络训练，但平衡性能和能效仍然是一个挑战。

Method: 研究基于HfO2/ZrO2超晶格的铁电突触器件，将实验测量的权重更新输入硬件感知神经网络模拟。分析20ns到0.2ms脉冲宽度下的能效和训练效果，比较普通SGD和混合精度SGD的性能差异，并提出对称点偏移技术解决非对称更新问题。

Result: 短脉冲降低了每次更新的能量但需要更多训练轮次，总体上仍能减少总能量而不牺牲精度。普通SGD的分类精度相比混合精度SGD有所降低。对称点偏移技术成功解决了非对称更新问题，恢复了精度。

Conclusion: 研究揭示了精度、收敛速度和能量使用之间的权衡关系，表明通过短脉冲编程和定制化训练可以显著提高片上学习效率。

Abstract: The increasing deployment of wearable sensors and implantable devices is
shifting AI processing demands to the extreme edge, necessitating ultra-low
power for continuous operation. Inspired by the brain, emerging memristive
devices promise to accelerate neural network training by eliminating costly
data transfers between compute and memory. Though, balancing performance and
energy efficiency remains a challenge. We investigate ferroelectric synaptic
devices based on HfO2/ZrO2 superlattices and feed their experimentally measured
weight updates into hardware-aware neural network simulations. Across pulse
widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require
more training epochs while still reducing total energy without sacrificing
accuracy. Classification accuracy using plain stochastic gradient descent (SGD)
is diminished compared to mixed-precision SGD. We analyze the causes and
propose a ``symmetry point shifting'' technique, addressing asymmetric updates
and restoring accuracy. These results highlight a trade-off among accuracy,
convergence speed, and energy use, showing that short-pulse programming with
tailored training significantly enhances on-chip learning efficiency.

</details>


### [15] [Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron](https://arxiv.org/abs/2509.18143)
*Mike Smart,Sachin Maheshwari,Himadri Singh Raghav,Alexander Serb*

Main category: cs.ET

TL;DR: 本文研究了将软件训练的ANN抽象权重映射到物理ACN电容值的方法，提出了优化映射方法以减小芯片尺寸并提高分类精度。


<details>
  <summary>Details</summary>
Motivation: DTSC ACN电路在模拟IC设计中具有高能效潜力，但软件训练的ANN权重到物理ACN电容值的有效映射方法尚未充分研究。

Method: 使用TensorFlow和Larq软件框架训练三种不同的ANN网络，将其权重映射到能量高效的DTSC ACN电容值域，并研究权重量化对ACN性能的影响。

Result: 实现了100%功能等效性，提出的AN到ACN映射方法能够促进更小的芯片尺寸和改善的分类精度。

Conclusion: 提出的优化映射方法对于实际IC部署至关重要，能够平衡精度、设计和实现要求。

Abstract: Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits
offer the potential for highly energy-efficient Artificial Neural Network (ANN)
computation in full custom analog IC designs. The efficient mapping of
Artificial Neuron (AN) abstract weights, extracted from the software-trained
ANNs, onto physical ACN capacitance values has, however, yet to be fully
researched. In this paper, we explore the unexpected hidden complexities,
challenges and properties of the mapping, as well as, the ramifications for IC
designers in terms accuracy, design and implementation. We propose an optimal,
AN to ACN methodology, that promotes smaller chip sizes and improved overall
classification accuracy, necessary for successful practical deployment. Using
TensorFlow and Larq software frameworks, we train three different ANN networks
and map their weights into the energy-efficient DTSC ACN capacitance value
domain to demonstrate 100% functional equivalency. Finally, we delve into the
impact of weight quantization on ACN performance using novel metrics related to
practical IC considerations, such as IC floor space and comparator
decision-making efficacy.

</details>


### [16] [Lightweight Targeted Estimation of Layout Noise in a Quantum Computer using Quality Indicator Circuits](https://arxiv.org/abs/2509.18679)
*Shikhar Srivastava,Ritajit Majumdar,Padmanabha Venkatagiri Seshadri,Anupama Ray,Yogesh Simmhan*

Main category: cs.ET

TL;DR: 提出了一种轻量级的实时布局质量评估方法——质量指示电路（QICs），用于在量子硬件上选择最优的电路映射布局，相比现有方法在布局选择质量上表现更好，同时将硬件开销平均降低79%。


<details>
  <summary>Details</summary>
Motivation: 在量子计算中，将抽象量子电路映射到物理硬件布局对电路性能有重要影响。现有解决方案如Mapomatic和JIT Transpilation存在校准数据过时或硬件使用率高的局限性，需要一种轻量级的实时布局质量评估方法。

Method: 提出质量指示电路（QICs）方法：设计小型探测电路保留用户电路基本结构，其理想无噪声结果是已知的。通过执行QIC评估量子硬件不同区域，选择最适合执行目标电路的布局。包括基本方法、无重叠的Union QIC方法和允许部分重叠的Distortion Threshold方法。

Result: 所提方法在布局选择质量上优于Mapomatic，同时将JIT的硬件开销平均降低79%。证明该方法轻量、可靠，适用于近期量子设备的布局选择。

Conclusion: QICs方法为量子电路布局选择提供了一种轻量级且可靠的解决方案，在保持高质量布局选择的同时显著降低了硬件开销，是近期量子设备中可行的布局选择技术。

Abstract: In the current era of quantum computing, minimizing noise is essential for
reliably executing quantum circuits on hardware. A key factor affecting circuit
performance is the mapping of the abstract quantum circuit to the physical
layout of the quantum hardware. This mapping can significantly influence output
quality, especially since hardware noise profiles are non-uniform and dynamic.
Existing solutions such as Mapomatic and Just-In-Time (JIT) Transpilation
attempt to address this issue but are limited either by relying on stale
calibration data or high hardware usage, respectively. In this article, we
propose Quality Indicator Circuits (QICs) as a lightweight, real-time method
for assessing layout quality. A QIC is a small probe circuit that is designed
to retain the basic structure of the user's circuit and whose ideal noiseless
outcome is known. It is used to evaluate which region of the quantum hardware
is best suited for executing the circuit of interest. We first propose a basic
method where a QIC is executed for each isomorphic layout to detect the best
among them. Although this requires several targeted circuit executions, we show
that it still, in most cases, reduces the execution overheads as compared with
JIT. To reduce the overheads further, we propose the union of multiple layouts
with a Union QIC approach that has no overlaps, and a Distortion Threshold
based approach allowing some overlap. Our results show that these outperform
Mapomatic in the quality of layout selection while reducing the hardware
overhead of JIT by 79 percent on average. This makes our proposed method
lightweight and reliable, and a viable technique for layout selection in
near-term quantum devices.

</details>


### [17] [Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks](https://arxiv.org/abs/2509.18906)
*Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: cs.ET

TL;DR: 提出了一种新颖的边缘推理框架，利用堆叠智能超表面控制无线传播，使信道本身能够执行空中计算，从而消除接收端符号估计需求，显著降低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统边缘推理将无线信道视为噪声，存在计算和通信开销大的问题。本文旨在通过智能超表面技术让信道直接参与计算，提高能效。

Method: 将发射机-信道-接收机系统建模为端到端深度神经网络，其中超表面响应作为可训练参数。引入专用DNN模块根据用户位置动态调整发射功率以应对信道变化。

Result: 性能评估表明，所提出的集成超表面的DNN框架能够在多样化场景下平衡分类精度和功耗，提供显著的能效改进。

Conclusion: 该框架通过智能超表面技术实现了无线信道的计算功能，为边缘推理提供了高效的解决方案，在能效方面表现优异。

Abstract: This paper introduces a novel framework for Edge Inference (EI) that bypasses
the conventional practice of treating the wireless channel as noise. We utilize
Stacked Intelligent Metasurfaces (SIMs) to control wireless propagation,
enabling the channel itself to perform over-the-air computation. This
eliminates the need for symbol estimation at the receiver, significantly
reducing computational and communication overhead. Our approach models the
transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)
where the response of the SIM elements are trainable parameters. To address
channel variability, we incorporate a dedicated DNN module responsible for
dynamically adjusting transmission power leveraging user location information.
Our performance evaluations showcase that the proposed metasurfaces-integrated
DNN framework with deep SIM architectures are capable of balancing
classification accuracy and power consumption under diverse scenarios, offering
significant energy efficiency improvements.

</details>


### [18] [A Stateless Transparent Voting Machine](https://arxiv.org/abs/2509.19257)
*Juan E. Gilbert,Jean D. Louis*

Main category: cs.ET

TL;DR: 本文描述了一种无状态透明投票机(STVM)的实现，该设备使用透明交互式打印界面，让选民在填写选票时能够验证纸质选票，并通过只读媒体启动以确保安全性。


<details>
  <summary>Details</summary>
Motivation: 投票系统的透明性和安全性至关重要，需要解决现有投票方法中存在的安全漏洞和可访问性问题。

Method: 采用无状态架构，从只读BD-R光盘启动投票软件，使用透明交互式打印界面将纸质选票转变为交互界面，并采用开源投票系统。

Result: 该设计结合了高可用性、可访问性和安全性，能够有效防止投票翻转攻击，并使黑客攻击难以持续存在或不被缓解。

Conclusion: STVM设计是目前最安全的投票标记设备，能够为所有选民提供安全、透明的投票体验。

Abstract: Transparency and security are essential in our voting system, and voting
machines. This paper describes an implementation of a stateless, transparent
voting machine (STVM). The STVM is a ballot marking device (BMD) that uses a
transparent, interactive printing interface where voters can verify their paper
ballots as they fill out the ballot. The transparent interface turns the paper
ballot into an interactive interface. In this architecture, stateless describes
the machine's boot sequence, where no information is stored or passed forward
between reboots. The machine does not have a hard drive. Instead, it boots and
runs from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R)
to boot the voting software. This system's statelessness and the transparent
interactive printing interface make this design the most secure BMD for voting.
Unlike other voting methods, this system incorporates high usability,
accessibility, and security for all voters. The STVM uses an open-source voting
system that has a universally designed interface, making the system accessible
for all voters independent of their ability or disability. This system can make
voting safer by simultaneously addressing the issue of voters noticing a vote
flip and making it difficult for a hack to persist or go unmitigated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 本文使用图像机器学习模型分析Ulam螺旋中不同区域的质数分布规律性，发现在较大整数区域（约5亿附近）的质数分布比小整数区域（低于2500万）更容易被模型学习，表明更高数量级区域存在更强的规律性。


<details>
  <summary>Details</summary>
Motivation: 质数分布具有确定性定义但表现出类似随机过程的统计特性，研究旨在探索机器学习能否作为数论研究的新实验工具，特别是分析质数在不同数量级区域的分布模式差异。

Method: 使用图像聚焦的机器学习模型，在Ulam螺旋的不同区域提取数据块进行训练和比较，具体对比了5亿附近区域和低于2500万区域的模型性能。

Result: 在5亿附近区域训练的模型在准确率上优于在低数值区域训练的模型，且模型在不同区域采用不同的分类策略：低数值区域更关注识别质数模式，高数值区域更关注排除合数。

Conclusion: 机器学习可作为数论研究的新实验工具，该方法在密码学中强质数和弱质数的模式研究方面具有应用潜力，支持了数论中关于质数分布在高数量级趋于规律化的猜想。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [20] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一个基于Wasserstein距离的联邦学习框架，用于解决数据市场中的模型交易问题，通过预测模型性能和数据兼容性来优化数据选择，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，数据市场需要可信的模型交易解决方案。联邦学习虽然能保护隐私，但在异构数据源的有效估值和选择方面仍面临挑战。

Method: 提出基于Wasserstein距离的估计器，预测模型在未见数据组合上的性能，并揭示数据异质性与FL聚合算法的兼容性。采用分布式方法近似Wasserstein距离以保护隐私，利用神经缩放定律外推模型性能。

Result: 在标签偏斜、错误标签和未标记数据等多种场景下的实验表明，该方法能一致识别高性能数据组合。

Conclusion: 该框架为构建更可靠的基于联邦学习的模型市场铺平了道路，实现了有效的数据选择和隐私保护。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [21] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该研究比较了完全学习的神经ODE与物理信息通用微分方程在连续时间库存动态预测中的表现，发现在结构化的需求模式下UDE表现更好，而在重尾分布需求下NODE更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究结构偏差在不同需求模式下对牛鞭效应预测的影响，为科学和工程系统中的混合建模提供指导。

Method: 使用单级测试平台，比较完全学习的神经ODE(NODE)与保持守恒和补货结构的物理信息通用微分方程(UDE)在三种需求模式(AR(1)、i.i.d.高斯、重尾对数正态)下的表现。

Result: 在结构化需求模式下，UDE泛化能力更好：训练数据占90%时，AR(1)需求下库存RMSE从4.92(NODE)降至0.26(UDE)；高斯需求下从5.96降至0.95。在重尾对数正态冲击下，NODE的灵活性更优。

Conclusion: 当噪声为轻尾或时间相关时强制执行结构约束，当极端事件占主导时放松结构约束。这一结果为科学和工程系统中的混合建模提供了具体指导。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [22] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 该论文将轻量级随机梯度下降（L-SGD）算法扩展到RISC-V架构的微控制器，通过8位量化版本显著降低了内存使用和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现代物联网设备缺乏GPU或专用加速器，导致本地训练困难，而联邦学习需要高效的优化算法。RISC-V作为新兴开源架构，目前缺乏对设备端训练的稳健支持。

Method: 将L-SGD算法适配到RISC-V MCU平台，评估32位浮点运算性能，并引入8位量化版本的L-SGD来缓解RISC-V MCU缺乏浮点运算单元的限制。

Result: 在RISC-V平台上，8位量化L-SGD实现了内存使用减少近4倍，训练速度提升2.2倍，且精度损失可忽略不计。

Conclusion: 该工作证明了在资源受限的RISC-V MCU上实现高效神经网络训练的可行性，为物联网设备的本地机器学习应用提供了重要支持。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [23] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 提出基于神经网络代理模型的迁移学习方法，使在一个桥梁上训练的模型能够适应具有相似特征的其他桥梁，实现跨结构知识迁移和可扩展的监测框架。


<details>
  <summary>Details</summary>
Motivation: 永久监测系统的广泛使用增加了数据可用性，但大型桥梁网络的可扩展性面临挑战。管理多个结构需要高效跟踪和比较长期行为，因此相似结构之间的知识迁移变得至关重要。

Method: 使用神经网络代理模型构建模型驱动的迁移学习方法，将在一个桥梁上训练的模型适应到另一个相似桥梁。将迁移后的模型集成到贝叶斯推理框架中，基于监测数据的模态特征进行连续损伤评估。

Result: 使用两个桥梁的真实数据进行验证，结果显示该方法对损伤位置、严重程度和范围具有高敏感性。

Conclusion: 该方法增强了实时监测能力，实现了跨结构知识迁移，促进了智能监测策略的发展，提高了网络层面的韧性。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [24] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT架构，通过自适应多尺度专家变换器解决多元时间序列预测中多尺度特征融合不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖预定义的单尺度补丁或缺乏有效的多尺度特征融合机制，无法充分捕捉时间序列中的复杂模式，导致性能受限和泛化能力不足

Method: AdaMixT引入多种补丁，利用通用预训练模型和领域特定模型进行多尺度特征提取，通过门控网络动态分配不同专家的权重，实现自适应多尺度融合

Result: 在8个广泛使用的基准数据集上的综合实验一致证明了AdaMixT在真实场景中的有效性

Conclusion: AdaMixT通过自适应多尺度专家融合机制，显著提升了多元时间序列预测的性能和泛化能力

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [25] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen是一个基于生成式AI和势博弈理论的协作学习框架，用于解决跨机构联邦学习中的统计异质性和经济竞争问题。


<details>
  <summary>Details</summary>
Motivation: 现有的跨机构联邦学习主要关注统计异质性，但忽视了机构间的经济竞争关系。当机构是市场竞争对手时，它们可能因担心效用损失而不愿参与联合训练。统计异质性和机构间竞争对组织行为和社会福利的综合影响尚未得到充分探索。

Method: CoCoGen框架通过生成式AI和势博弈理论来建模、分析和优化异构竞争环境下的协作学习。具体包括：1）通过学习性能和效用公式表征竞争和统计异质性；2）将每个训练轮次建模为加权势博弈；3）推导基于生成式AI的数据生成策略以最大化社会福利。

Result: 在Fashion-MNIST数据集上的实验结果表明，CoCoGen能够有效分析不同异质性和竞争水平对组织行为的影响，并且在各种设置下都优于基线方法。

Conclusion: CoCoGen为解决跨机构联邦学习中的统计异质性和经济竞争问题提供了一个有效的框架，通过生成式AI和博弈论方法实现了社会福利的最大化。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [26] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化的框架，利用大语言模型进行迭代式算法解决方案生成，集成了生成、测试、分析和评估功能。


<details>
  <summary>Details</summary>
Motivation: 简化算法设计过程，为用户提供对错误处理、分析和质量评估的完全控制，同时抽象化提示设计和模型管理的复杂性。

Method: 通过可复现的反馈循环整合多个LLM在互补角色中的协作，支持生成器、分析师和评估器等不同角色的编排。

Result: 提供了一个透明且可扩展的平台，使研究人员和从业者能够在不同领域共同设计算法和其他生成式解决方案。

Conclusion: EASE框架有效降低了算法设计的复杂性，为生成式解决方案的开发提供了系统化的支持。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [27] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一个结合信任和公平感知的联邦学习框架，通过基于适应度的客户端选举和分槽聚合来提升FedFaSt方法，解决非独立同分布数据、客户端不可靠性和对抗性攻击等挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗等敏感领域部署面临非独立同分布数据、客户端不可靠性和对抗性操纵等持续挑战，需要平衡收敛效率与鲁棒性。

Method: 采用三阶段参与策略（自由训练、自然选择、分槽团队参与），结合动态客户端评分、自适应阈值和基于队列的调度，实现信任感知聚合和公平导向的客户端选择。

Result: 在医学影像、视觉基准和表格农业数据上的实验表明，FedFiTS在准确性、达到目标时间和对投毒攻击的韧性方面持续优于FedAvg、FedRand和FedPow。

Conclusion: FedFiTS通过整合信任感知聚合和公平导向的客户端选择，推进了可扩展和安全的联邦学习，特别适合现实世界的医疗和跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [28] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 本文提出了一种基于AIS数据的机器学习管道，用于在狭窄水域实时分类船舶类型，使用随机森林模型在测试集上达到92.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 准确识别AIS轨迹中的船舶类型对于安全监管和打击非法、未报告和无管制(IUU)活动至关重要。

Method: 使用丹麦海事局的AIS历史数据，通过轨迹特征提取（包括运动学、时空和船形属性），采用基于树的机器学习模型（随机森林）进行五类船舶分类。

Result: 随机森林模型在测试集上达到92.15%准确率，宏精度94.11%，宏召回率92.51%，宏F1分数93.27%，ROC-AUC最高达0.9897。

Conclusion: 基于AIS轨迹的轻量级特征能够在狭窄水域实现实时船舶类型分类，最具区分性的特征是桥位比和最大航速。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [29] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion是一个联邦迁移学习框架，通过统一域适应和节俭标注技术，结合多样性/聚类感知编码器，解决联邦学习中的异构特征空间、非IID数据和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决实际联邦学习中的三大挑战：异构特征空间、严重的非IID数据分布以及跨客户端的标签稀缺问题。

Method: 使用标记的教师客户端通过置信度过滤的伪标签和域自适应迁移指导学习者客户端，同时客户端维护针对本地数据定制的个性化编码器。采用相似性加权的分类器耦合（可选聚类平均）来保持全局一致性，减少数据丰富站点的支配，提高少数客户端性能。节俭标注管道结合自监督/半监督预训练和选择性微调。

Result: 在表格和图像基准测试中，在IID、非IID和标签稀缺情况下，FedFusion在准确性、鲁棒性和公平性方面始终优于最先进的基线方法，同时保持相当的通信和计算预算。

Conclusion: 协调个性化、域适应和标签效率是构建在现实世界约束下鲁棒联邦学习的有效方法。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [30] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于patch的PCA-Net框架，通过将解场分解为小块并在每个patch内应用PCA，在降维后的PCA空间中训练神经算子，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统PCA应用于高维解场时计算开销大，需要更高效的方法来降低计算复杂度。

Method: 提出两种patch-based方法：局部到全局patch PCA和局部到局部patch PCA，并探索重叠patch平滑滤波和CNN精炼两种优化策略。

Result: 基于patch的PCA将端到端流水线处理时间减少3.7-4倍，同时保持高精度。

Conclusion: 该方法为PDE系统中高效算子学习提供了有前景的技术路径。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [31] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于上下文优化的新框架，将子空间表示学习与提示调优相结合，用于提高大规模视觉语言模型在开放世界中的OOD检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽略了视觉语言模型在数百万样本上学习到的特征嵌入的丰富判别潜力。

Method: 通过将ID特征投影到提示向量张成的子空间中，同时将ID无关特征投影到正交零空间中，提高ID-OOD可分性。设计了易于处理的端到端学习准则。

Result: 在真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 该方法通过整合子空间表示学习和提示调优，显著提升了OOD检测性能，同时保持了高ID分类准确率。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [32] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本文首次全面比较了用于自动产前CTG分析的最先进AI方法，发现微调的LLM在性能上优于基础模型和领域特定方法，为临床CTG解释提供了有前景的替代途径。


<details>
  <summary>Details</summary>
Motivation: 电子胎儿监护(CTG)分析对评估胎儿健康至关重要，但目前主要依赖主观临床解释，导致诊断准确性存在差异。基础模型和LLM在医疗领域表现出色，但在CTG分析方面的潜力尚未充分探索。

Method: 系统比较时间序列基础模型、LLM与已建立的CTG特定架构，评估超过500个不同时长的CTG记录，提供跨不同建模范式的稳健性能基准。

Result: 微调的LLM在性能上优于基础模型和领域特定方法，在胎儿监护应用中表现出色。

Conclusion: 研究结果为不同AI方法在胎儿监护应用中的相对优势提供了关键见解，并为产前护理中未来临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [33] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 本文提出了一种基于DPU的框架来检测和缓解大型语言模型在多GPU推理过程中的负载不均衡问题，通过卸载监控任务到DPU来分析GPU遥测数据和节点间通信模式。


<details>
  <summary>Details</summary>
Motivation: 大型transformer语言模型的自回归推理在解码阶段面临严重的运行时效率挑战，特别是GPU分片间的负载不均衡会导致吞吐量下降和延迟峰值。

Method: 利用BlueField-3数据处理器构建DPU辅助框架，通过DPU进行实时监控，分析GPU遥测和节点间通信模式，为推理控制器和调度器提供可操作的反馈。

Result: 研究旨在识别多GPU执行LLM张量计算时出现的负载不均衡和病态条件，评估其对计算性能的影响，并判断这些指标是否可以通过DPU网络进行跟踪和缓解。

Conclusion: DPU辅助的监控框架有望有效解决大规模语言模型推理中的负载不均衡问题，提升多节点张量并行推理的效率和稳定性。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [34] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间平衡注意力块用于时空预测，通过将空间图划分为子图并分别应用子图内注意力和子图间注意力，在保持空间邻近性的同时捕获全局相关性。


<details>
  <summary>Details</summary>
Motivation: 为了在遵循空间邻近性和捕获全局相关性之间取得平衡，解决现有方法在空间相关性建模方面的局限性。

Method: 将空间图划分为子图，使用子图内注意力学习局部空间相关性，通过子图间注意力进行子图间的消息传递，并构建多尺度预测模型。

Result: 在真实世界的中大型时空数据集上，相比基线方法性能提升最高达7.7%，且运行成本较低。

Conclusion: 该方法既能产生结构化的空间相关性，又具有良好的可扩展性和易实现性，在时空预测任务中表现出色。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [35] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 本文提出了一种摊销潜在引导（ALS）方法，通过在推理时以恒定成本应用离线计算的向量来替代昂贵的逐查询优化循环，从而显著提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时优化方法（如迭代优化和多步验证）计算成本过高，难以大规模应用。潜在空间优化方法虽然更直接，但仍需要昂贵的逐查询优化循环。

Method: ALS计算成功与不成功生成之间隐藏状态的平均差异，然后使用这个方向来校准模型的隐藏表示：当解码偏离成功流形时，ALS将激活状态推回正确方向。

Result: 在GSM8K和MATH-500基准测试中，ALS相比迭代方法实现了2-5倍的加速，同时达到或超过贪婪思维链和自一致性基线，效率-准确率权衡提升高达101%。

Conclusion: 研究结果表明，潜在优化的很大一部分收益可以通过离线方式获得，使得复杂的推理技术能够在生产环境中实际部署。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [36] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 该论文提出了一种基于机器学习的数字界面设计方法，能够动态适应不同用户和使用策略。算法使用贝叶斯统计建模用户浏览行为，专注于个人习惯而非群体偏好，具有在线增量学习能力。


<details>
  <summary>Details</summary>
Motivation: 设计能够根据用户个人使用习惯动态调整的数字界面，提升用户体验，帮助用户更有效地导航和操作界面。

Method: 使用贝叶斯统计方法建模用户浏览行为，采用在线增量学习算法，生成任务模型并提供当前用户使用统计的图形化导航表示。

Result: 模拟实验表明该方法在静态和非静态环境中都有效，能够学习新任务同时保留先验知识，即使在数据量少和环境变化的情况下也能做出可靠预测。

Conclusion: 这项研究为开发自适应系统铺平了道路，通过帮助用户更好地导航和操作界面来改善用户体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [37] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线强化学习框架，通过难度自适应GRPO算法（ADAGRPO）提升移动GUI代理的性能，在AndroidWorld和AndroidLab上取得了最先进的成功率。


<details>
  <summary>Details</summary>
Motivation: 开发有效的移动GUI代理面临任务难度分布不均和大规模环境采样效率低下的挑战，需要新的强化学习框架来解决这些问题。

Method: 提出MOBILERL框架，核心是ADAGRPO算法，包含难度自适应正回放、失败课程筛选和最短路径奖励调整策略，应用于Qwen2.5-VL-7B和GLM-4.1V-9B模型。

Result: MOBILERL-9B模型在AndroidWorld上达到75.8%的成功率，在AndroidLab上达到46.8%的成功率，均达到最先进水平。

Conclusion: MOBILERL框架通过难度自适应策略稳定了强化学习训练，提高了样本效率，在多样化移动应用和任务中表现出色，已被集成到AutoGLM产品中并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [38] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 本研究应用监督机器学习算法，基于用户评论中的文本和数值属性预测咖啡评分，发现集成方法和多层感知器在性能上优于简单分类器。


<details>
  <summary>Details</summary>
Motivation: 探索数据驱动方法来补充传统咖啡品鉴专家评估，通过机器学习预测咖啡质量评分。

Method: 采用六种机器学习模型（决策树、K近邻、多层感知器、随机森林、极端随机树、XGBoost），结合TF-IDF特征提取和SelectKBest特征选择进行数据预处理和超参数优化。

Result: 集成方法（极端随机树、随机森林、XGBoost）和多层感知器在F1分数、G-mean和AUC等评估指标上表现优于简单分类器。

Conclusion: 严格的特征选择和超参数调优对于构建稳健的感官产品评估预测系统至关重要，为传统咖啡品鉴提供了数据驱动的补充方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [39] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: 提出了NurseSchedRL强化学习框架，用于护士-患者分配，解决传统方法难以处理动态多约束环境的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临有限护理资源的高效分配压力，需要考虑技能异质性、患者病情、员工疲劳和护理连续性等多重因素。

Method: 使用PPO算法结合结构化状态编码、约束动作掩码和基于注意力的技能、疲劳及地理上下文表示，确保分配符合现实约束。

Result: 在模拟实验中，相比基线启发式和未约束RL方法，NurseSchedRL实现了更高的调度效率、更好的技能与患者需求匹配以及更低的疲劳度。

Conclusion: 强化学习在复杂高风险医疗人力管理决策支持中具有巨大潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [40] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 本文评估了联邦学习在电动汽车充电站异常检测中的性能，特别是在系统异构性和非独立同分布数据下的表现，发现FedAvgM在异构环境中优于FedAvg。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的快速发展，保护基于物联网的充电站免受网络威胁变得至关重要。集中式入侵检测系统存在隐私问题，而联邦学习提供了隐私保护的替代方案，但现有评估忽略了系统异构性和非独立同分布数据等实际挑战。

Method: 通过实验评估联邦学习在电动汽车充电站异常检测中的性能，使用FedAvg和FedAvgM两种优化方法，分析它们在系统异构性和数据异构性条件下的有效性。

Result: 在独立同分布设置下，FedAvg使用相同神经网络时表现优于集中式模型。但在非独立同分布数据和系统异构性条件下性能下降。FedAvgM在异构环境中始终优于FedAvg，表现出更好的收敛性和更高的异常检测准确率。

Conclusion: 联邦学习能够处理物联网电动汽车充电站的异构性而不会造成显著性能损失，FedAvgM是构建鲁棒、隐私保护的电动汽车充电站安全系统的有前景解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [41] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL是一个用于解释大型语言模型中稀疏自编码器（SAE）特征的框架，旨在提升安全领域的机制理解。该框架系统性地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实应用中的部署增加，安全性问题日益突出。现有安全研究主要关注评估模型输出或特定安全任务，难以应对更广泛、未定义的风险。稀疏自编码器虽然有助于解释模型行为，但之前的研究未能将特征与细粒度的安全概念联系起来，无法充分解决安全关键行为。

Method: 提出Safe-SAIL框架，系统性地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。框架包括识别最有潜力生成安全概念特定神经元的SAE，以及降低详细特征解释成本的方法。

Result: 将发布一个全面的工具包，包括SAE检查点和人类可读的神经元解释，支持对安全风险进行实证分析，促进LLM安全研究。

Conclusion: Safe-SAIL框架通过解释SAE特征来推进安全领域的机制理解，为解决LLM安全风险提供了新的方法论和工具支持。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [42] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种Gauss-Hermite求积方法，用于解耦机器学习代理模型中的认知不确定性和偶然不确定性，提高可靠性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在物理可靠性分析中应用广泛，但其模型近似误差引入的认知不确定性会与模型输入的偶然不确定性耦合，影响可靠性预测的准确性。

Method: 使用Gauss-Hermite求积方法解耦嵌套不确定性，通过一阶和二阶可靠性方法评估偶然不确定性下的条件失效概率，然后在整个认知不确定性实现范围内积分这些概率。

Result: 三个示例表明，该方法在保持计算效率的同时，比忽略模型不确定性的传统方法产生更可信的预测结果。

Conclusion: 所提出的方法能够有效处理机器学习代理模型中的不确定性耦合问题，为可靠性分析提供更准确的预测工具。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [43] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合STL时间序列分解和GRU神经网络的轨道交通换乘客流预测模型，通过分解客流时间序列并处理异常值，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 在轨道交通智能系统中，准确的换乘客流预测是优化运营计划和提升运输效率的关键环节，需要改进现有预测理论为智能运营决策提供更可靠支持。

Method: 首先使用Keras构建GRU模型，预处理地铁刷卡数据并通过深度优先搜索算法识别乘客出行路径，构建换乘客流时间序列；然后采用STL算法将时间序列分解为趋势、周期和残差分量，使用3σ原则处理异常值；最后完成客流预测。

Result: 实验结果表明，与LSTM、GRU和STL-LSTM模型相比，STL-GRU组合模型在工作日（除周五）、周五和休息日的换乘客流预测精度显著提升，MAPE分别降低了至少2.3、1.36和6.42个百分点。

Conclusion: STL-GRU组合预测模型能够有效提高轨道交通换乘客流的预测准确性，为智能运营决策提供了可靠的技术支持。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [44] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 研究表明，基于Transformer的机器学习应用在解决物理问题时，其权重矩阵呈现随机性特征，与物理问题的数学结构没有直接可识别联系，这表明机器学习与科学方法可能是两种不同但互补的知识获取路径。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习方法（特别是Transformer架构）在解决物理问题时的内在工作机制，以及其与传统的科学方法在知识获取路径上的差异。

Method: 分析两个代表性物理应用中Transformer模型的权重矩阵特性，观察其随机性特征，并与物理问题的数学结构进行对比。

Result: 发现权重矩阵具有随机性特征，与物理问题的数学结构没有直接对应关系；提出Transformer操作可能类似于广义路径积分技术。

Conclusion: 机器学习与科学方法代表两种不同的知识获取路径，虽然存在解释性挑战，但两者可能具有互补性；强调了缺乏洞察力直接获取知识的潜在风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [45] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一个参数高效的对抗性混合专家框架，用于工业规模的大语言模型持续指令调优，通过双专家设计和GAN鉴别器来平衡知识保留和跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在工业环境中持续学习时的灾难性遗忘问题，现有方法在新任务训练时会过度拟合新分布并削弱泛化能力。

Method: 采用双LoRA专家设计：专用专家保留任务特定知识，共享专家实现跨任务迁移；集成任务感知鉴别器通过对抗学习确保共享专家只传递任务相关信息。

Result: 在MTL5基准和腾讯工业基准上的实验验证了有效性，在腾讯视频平台的内容合规审查A/B测试中减少了15.3%的人工审核成本。

Conclusion: MoE-CL适用于需要持续适应和稳定迁移的大规模工业部署场景，具有实际应用价值。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [46] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文提出了一种加权梯度跟踪的分布式隐私保护算法，解决了梯度跟踪技术中的隐私泄露风险，并在时变异构步长下证明了算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 梯度跟踪技术虽然能提高分布式优化的收敛速度，但存在固有的隐私泄露风险，攻击者可能通过梯度信息获取代理的私有数据。

Method: 提出加权梯度跟踪分布式隐私保护算法，通过衰减权重因子消除梯度跟踪中的隐私泄露风险，并在时变异构步长条件下分析算法收敛性。

Result: 理论证明算法在温和假设下精确收敛到最优解，数值模拟通过分布式估计问题和卷积神经网络训练验证了算法的有效性。

Conclusion: 该方法成功解决了梯度跟踪中的隐私保护问题，为隐私敏感的分布式优化应用提供了可行的解决方案。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [47] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 该论文提出了一种静态-动态图融合网络（SDGF），通过双路径图结构学习方法捕捉多尺度序列间相关性，以解决现有方法在建模多尺度依赖关系方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，序列间的相关性对于准确性至关重要，但这些关系在不同时间尺度上表现出复杂的动态特性。现有方法在建模这些多尺度依赖关系方面存在局限，难以捕捉其复杂且演化的本质。

Method: SDGF模型采用双路径图结构学习方法：使用基于先验知识的静态图锚定长期稳定依赖关系，同时利用多级小波分解提取多尺度特征构建自适应学习的动态图。设计了注意力门控模块智能融合这两种互补信息源，并使用多核扩张卷积网络加深对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上进行的综合实验证明了所提出模型的有效性。

Conclusion: SDGF模型通过静态-动态图融合机制成功捕捉了多尺度序列间相关性，在多元时间序列预测任务中表现出优越性能。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [48] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 该研究构建了一个大规模数据集，系统分析了大型语言模型结构配置与性能之间的关系，旨在为未来模型优化提供数据驱动的指导。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型取得了显著成功，但关于结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。

Method: 创建包含多样化开源LLM结构及其在多基准测试中性能的大规模数据集，采用数据挖掘驱动的方法进行系统分析，并结合机制可解释性技术验证发现。

Result: 研究量化了结构配置与性能之间的关系，分析了不同结构选择对基准测试性能的影响。

Conclusion: 通过提供数据驱动的LLM优化见解，该工作旨在指导未来模型的针对性开发和应用，并将公开数据集。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [49] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了LoRALib统一基准，标准化了40个下游任务的数据集和超参数，建立了包含680个LoRA模块的库，对3种代表性LoRA-MoE方法进行大规模实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA-MoE方法在模型、数据集、超参数和评估方法上缺乏统一标准，难以进行公平比较。

Method: 构建LoRALib基准，将40个下游任务数据集标准化为统一格式，使用相同超参数微调得到680个LoRA模块，基于OpenCompass测试工具对3种LoRA-MoE方法和不同LoRA选择机制进行大规模实验。

Result: 实验表明LoRAMoE方法表现最佳，优先选择与目标任务相关的LoRA可以进一步提升MoE性能。

Conclusion: 这些发现将为未来工作提供启发，数据集和LoRA库已开源。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [50] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM算法通过利用排名基准与分布基准的结构等价性，直接在排名诱导的Plackett-Luce参数化中更新，确保算法在每个回合中保持排名诱导分布的特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于专家身份进行操作，而RIPLM旨在直接在排名诱导的Plackett-Luce参数化中更新，以保持与排名基准的等价性。

Method: RIPLM算法利用排名基准与分布基准的结构等价性，通过镜像下降方法在排名诱导的Plackett-Luce参数空间中进行更新。

Result: RIPLM是第一个在睡眠专家设置中同时满足排名忠实性和方差自适应性的算法。

Conclusion: RIPLM算法通过直接在排名诱导的Plackett-Luce参数化中操作，成功实现了排名忠实性和方差自适应性的结合，为睡眠专家问题提供了新的解决方案。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [51] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本文比较了规则分类器FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。研究发现FOLD-SE在保持可解释性的同时，性能损失很小，是平衡准确性和可解释性的可行方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型需要在准确性、效率和可解释性之间取得平衡。传统模型如神经网络虽然准确性高但缺乏透明度，因此需要开发既能提供可解释规则集又能保持良好性能的新算法。

Method: 使用分类数据集，以准确率、F1分数和处理时间作为主要性能指标，比较FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。

Result: FOLD-SE在二分类中比FOLD-R++提供更少的规则，仅损失少量准确性和处理效率；在多分类中比XGBoost更精确高效，同时生成可理解的规则集。

Conclusion: 规则型方法如FOLD-SE能够弥合可解释性与性能之间的差距，是多样化分类任务中黑盒模型的可行替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [52] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究开发了一种结合逻辑回归和主成分分析的机器学习框架，用于预测2型糖尿病风险，并通过基因无关的途径映射方法识别潜在治疗靶点，在Pima印第安人数据集上达到78.43%的准确率。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病等代谢性疾病对遗传易感人群（如Pima印第安人）造成严重健康负担，需要开发可解释且可扩展的早期检测和干预方案。

Method: 使用逻辑回归和t检验识别T2DM关键预测因子，结合主成分分析构建预测模型，开发基因无关的途径映射策略将预测因子与胰岛素信号、AMPK、PPAR等关键信号网络连接。

Result: 模型整体准确率达到78.43%，成功识别出与T2DM相关的关键预测因子，并通过途径映射揭示了潜在的治疗靶点，包括GLP-1/GIP受体激动剂、AMPK激活剂等。

Conclusion: 该框架为代谢性疾病的精准医疗提供了可解释且可扩展的解决方案，能够实现早期检测和靶向干预，特别适用于高风险人群。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [53] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个完全自动化的AI驱动管道，用于从Kaplan-Meier图中高精度重建个体患者数据，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工数字化，容易出错且缺乏可扩展性，需要自动化解决方案来提高临床研究中的证据合成效率

Method: 集成高级图像预处理、GPT-5驱动的多模态推理和迭代重建算法，采用混合推理架构将非结构化信息转换为结构化数据流

Result: 在合成和真实数据集上严格评估，始终表现出卓越的准确性，成功应用于胃癌免疫治疗试验的荟萃分析

Conclusion: KM-GPT通过自动化传统手动流程，为临床研究提供可扩展的基于网络的解决方案，支持基于证据的决策制定

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [54] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI是一种基于条件扩散模型的新型时空数据填补方法，通过双向S4模型进行预填补，并设计噪声感知时空网络来捕捉不同扩散步骤中的依赖关系变化。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因存在缺失值，现有扩散模型方法在提取时空依赖关系时存在误差累积问题，且忽略了不同噪声水平下依赖关系的变化。

Method: 提出AdaSTI框架，包含基于双向S4模型的BiS4PI网络进行预填补，时空条件化器(STC)提取条件信息，以及具有门控注意力机制的噪声感知时空(NAST)网络。

Result: 在三个真实世界数据集上的实验表明，AdaSTI在所有设置下都优于现有方法，填补误差最多减少46.4%。

Conclusion: AdaSTI通过自适应捕捉不同扩散步骤中的时空依赖关系变化，显著提升了时空数据填补的性能。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [55] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本研究提出了一种多标签分类框架，用于预测ICU患者的护理升级触发因素（CETs），包括呼吸衰竭、血流动力学不稳定、肾功能损害和神经功能恶化，使用ICU入院前24小时的数据。


<details>
  <summary>Details</summary>
Motivation: 传统早期预警系统（如SOFA或MEWS）局限于单一结果预测，无法捕捉临床恶化的多维度特性。ICU患者常表现出复杂、重叠的生理恶化迹象，需要及时升级护理。

Method: 使用MIMIC-IV数据库，基于规则定义CETs（如血氧饱和度低于90%、平均动脉压低于65 mmHg等）。从ICU入院前24小时提取特征，包括生命体征汇总、实验室值和静态人口统计学数据。在85,242例ICU住院数据上训练和评估多个分类模型。

Result: XGBoost模型表现最佳，F1分数分别为：呼吸0.66、血流动力学0.72、肾功能0.76、神经系统0.62。特征分析显示呼吸频率、血压和肌酐等临床相关参数是最有影响力的预测因子。

Conclusion: 该框架展示了在不需复杂时间序列建模或自然语言处理的情况下，实现早期、可解释临床警报的实际潜力。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [56] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一个基于概念的CNN可解释性框架，通过追踪概念在层间的传播来模拟模型的内部"思考路径"，包含概念注意力和概念通路两个核心组件。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了单个过滤器的语义角色和概念在层间的动态传播，无法全面解释CNN的内部推理过程。

Method: 提出ConceptFlow框架：1）概念注意力将每个过滤器与相关高级概念关联；2）概念通路通过概念转移矩阵量化概念在过滤器间的传播和转换。

Result: 实验结果表明ConceptFlow能够产生语义上有意义的模型推理洞察，验证了概念注意力和概念通路在解释决策行为方面的有效性。

Conclusion: 通过建模层次化的概念通路，ConceptFlow为CNN内部逻辑提供了更深入的洞察，支持生成更忠实且与人类对齐的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [57] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出基于稀疏表示的稀疏训练方案（STS），通过视觉标记压缩器和层动态跳过器来提高多模态大语言模型的训练效率


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型训练效率低下，主要由于多模态数据引入的长输入序列和层间计算利用率低

Method: STS方案包含两个核心组件：视觉标记压缩器（减少视觉标记信息负载）和层动态跳过器（在前向和后向传播中动态跳过语言模型的不必要层）

Result: 该方法在多个基准测试中广泛评估，证明了其有效性和效率

Conclusion: 该框架适用于多种MLLM架构，能够显著提高训练效率

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [58] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS提出了一种新的神经预测器范式，通过全局编码方案和共享超网络增强架构表示学习，在少样本场景下显著提升性能预测准确率。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索中的性能评估耗时严重，现有神经预测器由于难以捕捉架构间复杂关系而泛化能力差。

Method: HyperNAS包含全局编码方案捕捉宏观结构信息，共享超网络作为辅助任务研究架构间模式，并使用动态自适应多任务损失确保训练稳定性。

Result: 在5个代表性搜索空间上的实验显示，HyperNAS在少样本场景下表现优异，在CIFAR-10上达到97.60% top-1准确率，ImageNet上达到82.4% top-1准确率，样本使用量减少至少5倍。

Conclusion: HyperNAS在神经架构搜索性能预测方面实现了新的最先进结果，特别是在数据稀缺情况下表现出色。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [59] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一个用于测井解释的基础模型，通过多阶段预训练在1200口井的多曲线数据上，在孔隙度估算和岩性分类任务上优于现有方法，并展现出层感知能力和可重用地质词汇表。


<details>
  <summary>Details</summary>
Motivation: 测井解释面临工具响应异质性、噪声信号和标签有限的挑战，需要开发能够处理这些问题的可扩展AI方法。

Method: 三阶段方法：将测井片段标记化为地质标记；使用掩码标记建模和地层感知对比学习进行自监督预训练；通过少样本微调进行多任务适应。

Result: WLFM在孔隙度估算上达到0.0041 MSE，岩性分类准确率74.13%；微调后进一步提升至0.0038 MSE和78.10%准确率。模型展现出层感知能力并能合理重建掩码曲线。

Conclusion: WLFM为地质AI建立了可扩展、可解释和可迁移的骨干网络，为测井、地震和文本数据的多模态集成提供了基础。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [60] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion是一个深度学习框架，通过结合64亿参数蛋白质语言模型和强化学习，用于从头设计抗生素肽，实现了100%的成功率和纳摩尔级别的抗菌活性。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）预计到205年每年导致1000万人死亡，迫切需要新的抗生素。现有方法在生成有效且多样化的抗生素肽方面存在挑战。

Method: 使用64亿参数蛋白质语言模型，通过强化学习进行优化。模型先在精选肽数据上微调以捕捉抗菌序列规律，然后用近端策略优化结合最小抑菌浓度分类器预测和可微分物理化学目标的复合奖励进行优化。

Result: 体外评估100个设计肽显示所有候选物都具有低MIC值（有些达到纳摩尔范围），100%命中率。99/100化合物对至少两种临床相关细菌表现出广谱抗菌活性。主要通过靶向细胞质膜杀死细菌。

Conclusion: 该方法将生成、评分和多目标优化与深度强化学习统一在单一流程中，能够快速产生多样化、强效的候选物，为肽抗生素提供了可扩展的途径，并在数小时内实现效力和可开发性的迭代指导。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [61] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的多模态大语言模型，通过架构、数据和训练方法的创新实现了高效能和强性能，在多项基准测试中超越了更大的专有和开源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的训练和推理效率已成为AI发展的核心瓶颈，需要解决可访问性和可扩展性问题。

Method: 采用统一3D-Resampler模型架构实现图像和视频的紧凑编码，统一文档知识和文本识别的学习范式，以及混合强化学习策略支持长短推理模式。

Result: 在OpenCompass评估中超越GPT-4o-latest和Qwen2.5-VL 72B等模型，在VideoMME基准上达到30B以下模型的最佳性能，仅需Qwen2.5-VL 7B的46.7%GPU内存和8.7%推理时间。

Conclusion: MiniCPM-V 4.5证明了通过精心设计的架构和训练策略，可以在保持小模型规模的同时实现卓越的性能和效率。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [62] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本文提出并比较了9种训练方法，用于探索具有参数化线性B样条激活函数的神经网络中的双重优化动态，相比传统ReLU模型实现了显著更低的错误率。


<details>
  <summary>Details</summary>
Motivation: 传统的激活函数（如ReLU、tanh、sigmoid）是静态选择的，通过优化激活函数的形状，可以训练出更参数高效和准确的模型。

Method: 使用参数化线性B样条激活函数，并比较了9种不同的训练方法来实现激活函数和网络权重的双重优化。

Result: 实验结果显示，在FNN中实现了高达94%的更低最终模型错误率，在CNN中实现了51%的更低错误率。

Conclusion: 虽然这种方法带来了显著的性能提升，但也增加了开发和训练复杂性以及最终模型的延迟。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [63] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 本文提出了一种混合强化学习方法，用于优化卡车和无人机的最后一公里配送问题，考虑了电池管理约束，在50个节点的实例上比传统ALNS方法平均提升2.73%的性能。


<details>
  <summary>Details</summary>
Motivation: 研究最后一公里配送中卡车和无人机的协同配送问题，特别关注电池管理约束（无人机飞行速度是卡车的两倍，每次飞行有续航预算限制，每次配送后需要在卡车上充电）。

Method: 开发了混合强化学习求解器，结合ALNS卡车路线规划（使用2/3-opt和Or-opt优化）和基于指针/注意力机制的无人机调度策略。策略通过硬可行性掩码处理发射-服务-会合三元组，使用精确时间线模拟器确保发射/回收操作。

Result: 在N=50、E=0.7、R=0.1的欧几里得实例上，平均完成时间为5.203±0.093，比ALNS的5.349±0.038提升2.73%，与NN的5.208±0.124相当（仅差0.10%）。RL调度器在所有实例上都不差于ALNS，在三分之二的种子实例上优于或等于NN。

Conclusion: 学习的调度器能够平衡卡车等待时间和无人机飞行时间，最小化总完成时间。方法提供了配置优先的实现，包含绘图和显著性测试工具以支持复现。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [64] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 提出DSFT策略，通过调整掩码策略和损失函数，提升扩散大语言模型在数学和逻辑任务上的性能


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在数学和逻辑任务上存在学习困难，现有训练方法缺乏对数学和逻辑模式的全面理解

Method: DSFT策略，通过调整掩码策略和损失函数来引导模型理解数学和逻辑模式，可与预训练、强化学习等方法灵活结合

Result: 在LLaDA和Dream系列模型上验证，数学问题提升5-10%，逻辑问题提升约2%

Conclusion: DSFT策略为学习特定模式提供了新思路，可轻松高效地与其他训练方法结合并应用于各种扩散大语言模型

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [65] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个用于移动数据预测的基础模型，能够统一预测基站流量、用户应用使用和信道质量三种数据类型，通过软提示学习和时间掩码机制实现多任务预测，在真实数据集上表现出优异的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法需要为不同数据类型定制专门模型，增加了大规模异构网络中的复杂性和部署成本。需要设计一个统一的基础模型来简化预测流程并提高效率。

Method: 提出MobiGPT基础模型，采用软提示学习方法帮助模型理解不同数据类型的特征，引入时间掩码机制指导模型完成短期预测、长期预测和分布生成三种预测任务。

Result: 在包含10万+样本的真实数据集上评估，MobiGPT相比现有模型预测准确率分别提升27.37%、20.08%和7.27%，在未见场景下的零/少样本学习性能提升超过21.51%。

Conclusion: MobiGPT作为移动数据预测的基础模型，具有强大的泛化能力和迁移能力，能够有效支持大规模异构网络中的多类型数据预测需求。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [66] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种新的训练和推理架构，通过物理隔离的专家混合模型将计算能力内生地集成到神经网络中，解决了大语言模型无法进行高精度数值计算的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法将高精度数值计算作为内在可解释能力集成到现有架构中，而主流的多智能体方法存在通信开销大、多模态能力效率低和可扩展性有限的问题。

Method: 提出PiMoE架构，通过分别训练专家、文本到计算模块和路由器，在推理时路由器在token级别指导计算和推理，实现单链思维内的迭代交替。

Result: 在两个推理-计算任务上的评估显示，PiMoE不仅比直接微调LLM获得更高准确率，相比主流多智能体方法在响应延迟、token使用和GPU能耗方面都有显著改善。

Conclusion: PiMoE为下一代科学或工业智能系统提供了一个高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [67] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA是一个联邦图学习框架，通过投影优先策略解决领域偏移问题，使用重要性感知的两阶段管道来去噪客户端更新，实现更稳定收敛和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习在领域偏移下（如Twitch Gamers和多语言Wikipedia网络）会导致客户端模型表示不兼容，使简单聚合变得不稳定且无效。研究发现问题根源是噪声梯度信号而非权重方案。

Method: FedIA采用投影优先策略，包含两阶段管道：1）服务器端top-ρ掩码保留约5%最具信息量的坐标；2）轻量级影响力正则化动量权重抑制异常客户端。该方法无需额外上行流量且服务器内存开销可忽略。

Result: 在同质（Twitch Gamers）和异质（Wikipedia）图上，FedIA相比9个强基线方法实现了更平滑、更稳定的收敛和更高的最终准确率。收敛分析显示动态投影保持了最优的O(σ²/√T)收敛速率。

Conclusion: FedIA通过重要性感知的投影优先策略有效解决了联邦图学习中的领域偏移问题，提供了一种可部署的高效解决方案。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [68] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR是一种新型的大语言模型量化方法，通过高斯分布感知的码本设计和硬件友好的CUDA内核，在4位量化下实现更准确的压缩和2.21-3.04倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法存在不足：RTN方法忽略LLM权重的高斯分布特性，码本方法虽然考虑分布但内存访问模式不佳导致推理速度下降。需要一种既能准确压缩又保持高效推理的量化方案。

Method: 提出SBVR方法，将权重值映射到非均匀表示点，其分布遵循LLM权重的实际高斯分布。同时设计定制CUDA内核，支持SBVR格式的矩阵向量乘法而无需解压缩。

Result: 在各种模型上的评估显示，SBVR在4位量化下实现了最先进的困惑度和准确度基准性能，相比朴素FP16模型获得了2.21-3.04倍的端到端token生成加速。

Conclusion: SBVR通过高斯分布感知的码本表示和硬件优化的执行策略，成功解决了现有量化方法在准确性和效率之间的权衡问题，为LLM部署提供了高效的量化解决方案。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [69] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 本文提出了一个大规模基准测试，用于评估大语言模型在地理空间路线认知方面的能力，发现LLMs在路线反转任务中存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过自然语言理解地理空间信息，但大语言模型的地理空间认知能力尚未得到充分探索。现有研究受限于不可量化的指标、有限的数据集和模糊的研究层次。

Method: 创建了包含全球12个大都市36000条路线的大规模评估数据集；开发了PathBuilder工具，用于自然语言指令与导航路线之间的双向转换；提出了新的评估框架和指标，对11个SOTA LLMs进行路线反转任务的严格评估。

Result: 基准测试显示LLMs在路线反转任务中存在明显局限性：大多数反转路线既无法返回起点，也与最优路线不相似；LLMs还面临路线生成鲁棒性低和对错误答案置信度高的问题。

Conclusion: LLMs在地理空间路线认知方面仍有显著不足，需要进一步研究来提升其地理空间推理能力。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [70] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 提出多模态思维链框架解决对话导航中的自我中心到全局中心空间方向推理问题，在中文对话导航基准上取得优异表现


<details>
  <summary>Details</summary>
Motivation: 解决室内或复杂环境中GPS信号弱、详细地图不可用时，对话代理需要将自我中心表述转换为全局中心方向的关键挑战

Method: 提出多模态思维链框架，整合语音识别转录和地标坐标，通过三步推理过程：提取空间关系、坐标映射到绝对方向、推断用户朝向，并采用课程学习策略

Result: 在干净转录本上达到100%方向准确率，ASR转录本上达到98.1%，显著优于单模态和非结构化基线，在多语言代码切换和噪声条件下表现出鲁棒性

Conclusion: 结构化多模态思维链空间推理为可解释且资源高效的具身导航提供了一条可行路径

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [71] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出了一种变分任务向量组合方法，通过贝叶斯推理框架将组合系数作为潜变量进行估计，实现样本特定的组合而非任务级别的组合。


<details>
  <summary>Details</summary>
Motivation: 观察到任务向量存在结构冗余，需要一种能够选择最信息丰富组件的方法，同时解决高维稀疏空间中的高方差和采样效率问题。

Method: 引入Spike-and-Slab先验促进稀疏性，开发门控采样机制基于不确定性和重要性过滤组合系数，构建可控后验分布。

Result: 实验结果表明，该方法在所有数据集上均优于现有方法，通过选择性利用任务向量中最可靠和信息丰富的组件实现更好性能。

Conclusion: 该方法为高效有效的任务向量组合建立了新标准，具有实际应用价值。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [72] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE是一个包含2.22亿个化合物的大规模、多样化、严格筛选的数据集，旨在解决分子表示学习中现有数据集规模小、质量差的问题，为分子化学领域提供类似ImageNet的标准资源。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集存在规模小、多样性不足和质量问题，限制了分子表示学习模型泛化能力的提升，需要构建一个大规模、高质量的标准数据集。

Method: 从6个大型数据库中通过自动化筛选流程构建MolPILE数据集，包含2.22亿个化合物，并对现有预训练数据集进行综合分析。

Result: 在MolPILE上重新训练现有模型能够显著提高泛化性能，证明了该数据集的有效性。

Conclusion: MolPILE为分子化学领域提供了一个标准化的训练资源，解决了该领域对大规模高质量数据集的迫切需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [73] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP是一种通过多令牌预测训练与推理模式对齐来提升推测解码性能的方法，在保持无损输出质量的同时实现平均2.03倍加速


<details>
  <summary>Details</summary>
Motivation: 自回归生成的顺序特性造成了吞吐量瓶颈，而多令牌预测在训练效率方面的潜力尚未在推理加速中得到充分探索

Method: 使用位置共享权重对单个MTP头进行微调，结合自蒸馏数据和语言感知动态词汇压缩，以捕捉连续未来令牌的依赖关系

Result: 在七个不同基准测试中，FastMTP相比标准下一令牌预测实现平均2.03倍加速，比普通MTP性能提升82%

Conclusion: FastMTP仅需轻量级训练即可无缝集成到现有推理框架中，为加速LLM推理提供了实用且快速部署的解决方案

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [74] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出了一种新的多工作者选择算法M-DSL，用于解决分布式群体学习中的非独立同分布数据挑战，通过引入非i.i.d.度度量来量化数据异质性，并指导有效的工作者选择。


<details>
  <summary>Details</summary>
Motivation: 分布式群体学习面临非独立同分布数据的挑战，这会降低学习性能并导致训练行为发散。目前缺乏关于数据异质性如何影响模型训练准确性的理论指导。

Method: 提出M-DSL算法，引入新的非i.i.d.度度量来量化本地数据集之间的统计差异，建立数据异质性与DSL性能评估之间的联系，指导选择对全局模型更新有显著贡献的多工作者。

Result: 通过在不同异构数据集和非i.i.d.数据设置下的广泛实验，验证了M-DSL在性能提升和网络智能增强方面的优势，超越了基准方法。

Conclusion: M-DSL算法有效解决了分布式群体学习中的数据异质性问题，提供了理论收敛性分析，并在实验中证明了其性能改进效果。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [75] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: 该论文提出了GnnXemplar，一种受认知科学范例理论启发的全局解释方法，用于解释图神经网络（GNN）的预测决策。该方法通过识别嵌入空间中的代表性节点（范例），并使用大型语言模型生成自然语言规则来解释预测。


<details>
  <summary>Details</summary>
Motivation: GNN在节点分类中应用广泛，但其决策过程不透明限制了信任和采用。现有的全局解释方法依赖于小图中的模式发现，在大型真实场景中效果不佳，因为子图重复罕见、节点属性高维且预测来自复杂的结构-属性交互。

Method: GnnXemplar将范例选择建模为覆盖最大化问题，使用贪婪近似算法在GNN嵌入空间中识别代表性节点。然后采用自优化提示策略，利用大型语言模型从范例邻域生成可解释的自然语言规则。

Result: 在多个基准测试上的实验表明，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，60名参与者参与的用户研究验证了其有效性。

Conclusion: GnnXemplar提供了一种新颖的全局解释框架，能够有效解决大型真实图数据中的解释挑战，通过结合范例理论和LLM技术实现了更好的可解释性。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [76] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 提出GETAD框架，通过图注意力网络和Transformer解码器结合道路网络拓扑结构，实现更精确的轨迹异常检测


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅考虑轨迹的采样位置序列，忽略了底层移动网络（如道路网络）的约束和连通性信息

Method: 使用图注意力网络学习道路感知嵌入，结合基于图的位置编码；Transformer解码器建模序列移动；多目标损失函数结合自回归预测和监督链接预测；引入CW NLL异常评分函数

Result: 在真实世界和合成数据集上的实验表明，GETAD相比现有方法取得了一致的改进，特别是在道路约束环境中检测细微异常方面表现优异

Conclusion: 将图结构和上下文语义纳入轨迹建模能够实现更精确和上下文感知的异常检测

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [77] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨了为什么强化学习预训练算法能够生成支持情境强化学习的网络参数，并通过案例研究证明，当Transformer预训练用于策略评估时，预训练损失的全局最小值可以实现情境时序差分学习。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习智能体通过更新神经网络参数来适应任务，但最近发现一些预训练的智能体能够在参数固定的情况下，仅通过情境输入解决新的分布外任务。这种现象被称为情境强化学习，但现有研究大多使用标准RL算法进行预训练，本文旨在探究为什么这些预训练算法能够产生支持ICRL的参数。

Method: 本文通过理论分析进行案例研究，证明当Transformer网络预训练用于策略评估时，预训练损失的全局最小值之一能够实现情境时序差分学习。

Result: 研究结果表明，能够支持情境强化学习的参数确实是预训练损失的极小值点，这为ICRL现象提供了理论支持。

Conclusion: 本文初步验证了假设，即支持情境强化学习的参数是预训练损失的极小值，并通过Transformer策略评估的案例提供了理论证明，为理解ICRL现象奠定了基础。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [78] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文对深度学习优化器进行了全面综述，按时间顺序分析了从SGD到最新优化器（如Momentum、AdamW、Sophia、Muon）的发展历程、技术特点和更新规则。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化器对神经网络学习效果至关重要，随着深度学习快速发展，出现了多种不同方法的优化器，需要系统梳理和总结现有研究成果。

Method: 采用文献综述方法，按时间顺序分析各种优化器的更新规则、技术特点、超参数设置及其在优化过程中的贡献。

Result: 提供了深度学习优化器的全面资源，详细介绍了各优化器的技术细节和特点，为理解当前优化器状态和未来发展提供了基础。

Conclusion: 本文为深度学习优化器研究提供了系统性综述，既有助于理解现有技术现状，也为未来研究方向提供了参考。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [79] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种新颖的信息保持混沌游戏表示方法（R-CGR），解决了传统CGR方法在几何映射过程中丢失序列信息的根本限制，实现了完整的序列恢复。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在生物序列分析中存在序列信息丢失的问题，这限制了其在需要精确序列信息的生物信息学应用中的实用性。

Method: 通过显式路径编码结合有理数算术精度控制，实现了完美的序列重构。该方法存储完整的路径信息，维护每个步骤的位置和字符信息。

Result: 在生物序列分类任务中表现出与传统序列方法相竞争的性能，同时提供可解释的几何可视化。生成的图像适合深度学习应用。

Conclusion: 该方法为可解释的生物信息学分析开辟了新途径，在需要准确性和序列恢复的应用中具有重要价值。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [80] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 提出KANDI方法，结合Kolmogorov-Arnold网络和扩散策略，用于医疗健康领域的离线逆强化学习，解决物理活动促进中的奖励函数定义和策略对齐问题


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗健康应用中面临奖励函数定义困难、逆强化学习在复杂环境中难以准确推断奖励函数、以及学习策略与观察到的医疗行为难以对齐等挑战

Method: 利用Kolmogorov-Arnold网络的灵活函数逼近能力估计奖励函数，同时采用基于扩散的策略在Actor-Critic框架内进行动作优化，从低跌倒风险老年人（专家）行为中学习

Result: KANDI在D4RL基准测试中优于现有最先进方法，并在PEER研究的临床试验中验证了其在跌倒风险干预项目中促进老年人身体活动的实际应用效果

Conclusion: KANDI为解决医疗健康领域离线强化学习的关键挑战提供了有效解决方案，在活动促进干预策略方面具有重要潜力

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [81] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet结合图神经网络和神经常微分方程，为复杂结构力学系统提供长期稳定预测的通用框架


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，标准GNN自回归方法在长期预测中存在误差累积和不稳定性问题

Method: 将GNN的空间推理能力与神经ODE的连续时间建模相结合，处理一维和二维弹性体的大非线性变形问题

Result: 在长期预测精度和稳定性上显著优于基线模型，相比传统求解器实现计算加速

Conclusion: 为开发数据驱动代理模型加速复杂结构系统分析提供了强大且通用的方法

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [82] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 本文提出了一个AI驱动的框架，用于为给定线性系统推荐MCMC参数，以加速Krylov子空间求解器的收敛。该框架通过图神经网络预测预条件效果，并使用贝叶斯采集函数选择最优参数，在减少50%搜索预算的情况下实现了约10%的迭代次数减少。


<details>
  <summary>Details</summary>
Motivation: Krylov子空间求解器在求解大规模稀疏线性系统时，对于病态矩阵收敛缓慢，需要预条件器。MCMC矩阵求逆可以生成预条件器，但其效果依赖于参数，而最优参数随矩阵变化，手动或网格搜索成本高昂。

Method: 开发了一个AI驱动框架：1）使用图神经网络代理模型从矩阵A和MCMC参数预测预条件加速效果；2）采用贝叶斯采集函数选择最有可能最小化迭代次数的参数集。

Result: 在未见过的病态系统上，该框架仅使用传统方法50%的搜索预算就实现了更好的预条件效果，收敛迭代次数减少了约10%。

Conclusion: 该研究为将MCMC基预条件器集成到大规模系统中提供了一条可行路径，展示了AI驱动参数优化在科学计算中的潜力。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [83] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一个基于Transformer的多模态框架，用于持续和长期的血糖预测，通过交叉注意力和多尺度注意力机制整合生理和行为信号，并采用知识保留技术防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决血糖预测中多源信号采样率不同、长期时间依赖性以及持续学习中的灾难性遗忘问题。

Method: 设计并行工作的交叉注意力和多尺度注意力机制，交叉注意力整合血糖数据与其他生理行为信号，多尺度注意力捕捉长期时间依赖；引入知识保留模块防止灾难性遗忘。

Result: 在AIREADI数据集上评估，GluMind在RMSE和MAE指标上分别比现有最优模型提升约15%和9%，表现出稳定的性能和适应性。

Conclusion: GluMind框架在持续血糖预测任务中表现优异，能有效整合多模态数据并保持长期学习稳定性。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [84] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA是一种新的降维算法，将PPCA推广到非线性流形，通过结合几何坐标系统来更好地描述分布在非线性流形周围的数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统PPCA及其扩展主要基于线性模型，只能描述欧几里得坐标系中的数据。然而在神经科学等应用中，数据可能分布在非线性流形周围而非欧几里得空间中，需要能够显式结合非线性流形知识的降维方法。

Method: 开发了概率几何主成分分析(PGPCA)，首先从数据中拟合非线性流形，然后推导几何坐标系来捕捉数据与流形的偏差和噪声。还推导了数据驱动的EM算法来学习PGPCA模型参数。

Result: 在模拟和脑数据分析中，PGPCA能有效建模各种给定流形周围的数据分布，对此类数据的表现优于PPCA。PGPCA还提供了测试几何坐标系是否比欧几里得坐标系更好地描述数据的能力。

Conclusion: PGPCA通过结合非线性流形几何，增强了高维数据降维的有效性，特别适用于具有噪声且分布在非线性流形周围的数据分析。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [85] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探索了离散时间扩散过程及其变体，包括加性高斯噪声、乘性高斯噪声、模糊噪声及其混合，实验表明离散时间过程在语音质量上与连续时间扩散模型相当，但训练和推理更高效一致。


<details>
  <summary>Details</summary>
Motivation: 连续时间扩散模型存在训练与推理条件不匹配的问题，且通常局限于加性高斯噪声。离散时间过程可以避免这些限制，减少推理步骤，并保持训练/推理条件的一致性。

Method: 提出了几种扩散式离散时间过程的新变体：加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊与高斯噪声的混合。

Result: 实验结果显示，离散时间过程在主观和客观语音质量评估上与连续时间扩散模型相当，同时提供了更高效和一致的训练与推理方案。

Conclusion: 离散时间扩散过程是连续时间扩散模型的有效替代方案，能够提供相当的语音质量，同时具有更好的训练和推理效率及一致性。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [86] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ是一种新的向量压缩技术，通过非线性非均匀向量量化器对高维嵌入向量进行高效压缩，在保持高精度的同时降低存储和计算成本。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量在表示非结构化数据和语义搜索中广泛应用，但其大尺寸导致存储和检索成本高昂，需要高效的压缩技术。

Method: 使用新型的简约且计算高效的非线性方法构建非均匀向量量化器，关键是为每个索引向量单独学习量化器。

Result: 实验结果表明NVQ在最小计算成本下相比现有技术展现出更高的准确性。

Conclusion: NVQ是一种在高保真度下计算和空间效率都很高的向量压缩技术，能够有效解决高维向量存储和检索的成本问题。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [87] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是一个基于流匹配的蛋白质折叠模型，仅使用通用Transformer块，挑战了传统依赖复杂领域特定架构的设计。


<details>
  <summary>Details</summary>
Motivation: 质疑蛋白质折叠模型中复杂领域特定架构设计的必要性，探索通用架构是否也能达到高性能。

Method: 使用标准Transformer块和自适应层，通过生成流匹配目标和结构项进行训练，在900万蛋白质结构数据上训练30亿参数模型。

Result: SimpleFold-3B在标准折叠基准测试中与最先进基线性能相当，在集成预测方面表现优异，在消费级硬件上部署和推理效率高。

Conclusion: SimpleFold挑战了蛋白质折叠中对复杂领域特定架构设计的依赖，为未来进展开辟了替代设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [88] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov Arnold Networks（KANs）和物理信息损失函数的新方法，用于预测量子动力学响应，相比传统神经网络方法显著减少了训练数据需求并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间中的演化使得传统数值方法计算成本高昂，现有神经网络架构需要大量训练数据且存在虚假振荡问题，影响物理可解释性。

Method: 使用Kolmogorov Arnold Networks（KANs）结合物理信息损失函数来强制执行Ehrenfest定理，并引入Chain of KANs架构直接将时间因果性嵌入模型设计。

Result: 该方法仅需200个样本（传统Temporal Convolution Networks需要3,700个），即5.4%的训练数据量，就能达到更优的准确性。

Conclusion: 物理信息KANs相比传统黑盒模型具有显著优势，在保持数学严谨性和物理一致性的同时大幅降低数据需求，为量子动力学建模提供了有前景的新途径。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [89] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 本文提出使用混合数据集来增强合成数据集在反洗钱（AML）模型训练中的实用性，通过结合公开可用的真实世界特征，既保护隐私又提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构面临的全球性关键问题，但开发AML模型时缺乏训练数据访问权限（由于隐私和保密问题）。合成数据虽然能保护隐私，但纯合成数据集训练AML模型存在挑战。

Method: 提出使用混合数据集方法，将合成数据与公开可用、易于获取的真实世界特征相结合，以增强合成数据集的实用性。

Result: 混合数据集不仅能够保护隐私，还能提高模型效用，为金融机构增强AML系统提供了实用途径。

Conclusion: 混合数据集方法为解决AML模型训练中的数据隐私和效用平衡问题提供了有效的解决方案。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [90] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL是一种新的强化学习框架，通过主动部分rollout策略解决RL训练中长尾响应分布导致的GPU空闲问题，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中，rollout生成占90%以上运行时间，且长尾响应分布导致GPU空闲，限制了模型的可扩展性。

Method: APRIL在rollout阶段超额配置请求，达到目标响应数后终止，并将未完成的响应回收用于后续步骤继续，避免丢弃任何rollout。

Result: 实验显示APRIL在常用RL算法中最高提升44%的rollout吞吐量，加速收敛，并在任务中实现最高8%的最终准确率提升。

Conclusion: APRIL统一了系统级和算法级考虑，可提高RL训练效率，且与框架和硬件无关，已集成到slime RL框架中。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [91] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 本文提出了反向互补一致性正则化（RCCR）方法，通过惩罚DNA序列与其反向互补序列预测之间的差异，提高DNA语言模型的生物学对称性捕获能力。


<details>
  <summary>Details</summary>
Motivation: DNA序列的反向互补序列通常具有相同的生物学意义，但现有DNA语言模型经常无法捕捉这种对称性，导致预测不一致，影响模型可靠性。

Method: RCCR是一种模型无关的微调目标函数，直接惩罚模型对序列及其反向互补序列预测之间的差异。该方法在三个不同骨干网络（Nucleotide Transformer、HyenaDNA、DNABERT-2）上进行评估。

Result: 实验表明，RCCR显著提高了RC鲁棒性，大幅减少了预测翻转和错误，同时在序列分类、标量回归和轮廓预测等任务上保持或优于基线方法（如RC数据增强和测试时平均）。

Conclusion: RCCR通过将关键生物学先验直接整合到学习过程中，为多样化生物学任务提供了一种单一、内在鲁棒且计算高效的模型微调方案。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [92] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出Symphony-MoE框架，通过融合多个异构预训练模型的专家来构建强大的MoE模型，解决了传统upcycling方法专家多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型通过复制单个预训练密集模型的FFN层来构建专家，限制了专家多样性。本文旨在利用多个异构预训练模型来构建更强大的MoE模型。

Method: 提出两阶段框架：1）通过层感知融合策略构建共享骨干网络，并使用基于激活的功能对齐缓解参数错位；2）进行轻量级路由器训练来协调整个架构。

Result: 实验表明该方法成功整合了异构源模型的专家，在多领域任务和分布外泛化方面显著超越基线方法。

Conclusion: Symphony-MoE框架能够有效融合多个预训练模型，构建出性能优越的MoE模型，为利用异构模型资源提供了新思路。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [93] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文从理论上分析了SigLIP和SigLIP2模型中可训练逆温度和偏置参数在sigmoid损失函数下的优势，提出了(m, b_rel)-Constellations这一新的组合对象来解释模型成功的原因。


<details>
  <summary>Details</summary>
Motivation: 对比学习预训练中的表示获取和对齐任务变得越来越重要，但现有理论对sigmoid损失函数中温度和偏置参数的作用理解不足。

Method: 通过理论分析sigmoid损失函数中温度和偏置参数的作用，提出(m, b_rel)-Constellations组合对象，并基于此提出带有显式相对偏置的sigmoid损失重参数化方法。

Result: 理论解释了SigLIP在检索任务上的成功、模态间隙的存在以及产生高质量表示所需的维度，实验表明新参数化方法在合成数据上改善了训练动态。

Conclusion: 同步可训练温度和偏置参数在sigmoid损失函数下具有理论优势，提出的新组合对象和重参数化方法为对比学习提供了新的理论洞见。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [94] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文首次全面综述了可解释图神经网络在痴呆症研究中的应用，涵盖阿尔茨海默病、帕金森病、轻度认知障碍等多种痴呆亚型，提出了针对痴呆症任务的可解释性方法分类体系，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 痴呆症具有临床和生物学异质性，诊断和亚型区分极具挑战性。传统图神经网络在脑连接建模方面有潜力，但其鲁棒性不足、数据稀缺和缺乏可解释性限制了临床应用。可解释图神经网络通过结合图学习和可解释性来解决这些障碍。

Method: 本文采用系统性综述方法，对XGNN在痴呆症研究中的应用进行全面梳理，引入针对痴呆症相关任务的可解释性方法分类体系，并对现有模型在临床场景中进行比较分析。

Result: 综述发现XGNN能够识别疾病相关生物标志物、分析脑网络破坏，并为临床医生提供透明见解。同时揭示了当前面临的挑战，包括有限泛化性、未充分探索的领域以及大语言模型在早期检测中的整合问题。

Conclusion: 该综述旨在指导未来工作朝着可信赖、临床有意义且可扩展的XGNN在痴呆症研究中的应用方向发展，为构建更可靠的临床决策支持系统提供理论框架。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [95] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出了Interaction Topological Transformer (ITT)框架，通过多尺度交互拓扑结构来预测多孔材料的气体吸附、传输和稳定性性能，在数据稀缺情况下实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 多孔材料具有结构多样性，但在预测建模中面临挑战，因为结构-性能关系涉及多尺度特征，且标记数据稀疏分布不均，阻碍了跨材料家族的泛化能力。

Method: 开发了交互拓扑变换器(ITT)，利用新颖的交互拓扑在多尺度（结构、元素、原子、成对元素组织）捕获材料信息，通过两阶段训练策略（60万无标签结构的自监督预训练+监督微调）实现多尺度特征提取和联合推理。

Result: ITT在多孔材料的吸附、传输和稳定性属性预测方面达到了最先进的准确性和可迁移性。

Conclusion: 该框架为结构化学多样性多孔材料的学习引导发现提供了原则性和可扩展的路径。

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [96] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 提出DS-Diffusion模型解决现有扩散模型在时间序列生成中的三个问题：需要重新训练引入条件指导、生成数据与真实数据存在分布偏差、推理过程不可解释。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需要重新训练整个框架来引入特定条件指导，生成数据与真实数据存在分布偏差导致下游任务模型偏差，且扩散模型的复杂性导致推理过程不可解释。

Method: 开发基于风格引导核的扩散框架避免特定条件的重新训练；提出基于时间信息的分层去噪机制(THD)减少分布偏差；生成样本能清晰指示数据来源风格。

Result: 相比ImagenTime等最先进模型，预测分数和判别分数分别降低5.56%和61.55%，进一步减少了生成数据与真实数据的分布偏差，推理过程更可解释。

Conclusion: DS-Diffusion通过消除重新训练需求，增强了模型对特定条件的灵活性和适应性，同时提高了生成质量和可解释性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [97] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT是一种增强LLM决策能力的新方法，通过在行动前增加反思步骤来实现即时错误纠正，在多个交互环境中显著提升了成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM在交互决策任务中存在错误累积问题且缺乏有效的自我纠正机制，需要一种能够实现即时错误修正的方法

Method: 提出"Reflect before Act"（REBACT）方法，在采取下一个行动之前引入关键的反思步骤，确保行动路径的平滑性和对环境反馈的适应性

Result: 在ALFWorld、WebShop和TextCraft三个环境中，REBACT显著优于基线方法：WebShop成功率提升24%达到61%，ALFWorld提升6.72%达到98.51%，TextCraft提升0.5%达到99.5%

Conclusion: REBACT通过少量修改步骤即可实现性能提升，证明了其计算效率，为LLM的交互决策提供了有效的自我纠正机制

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [98] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出了Flow Marching算法，将神经算子学习与流匹配相结合，构建生成式PDE基础模型，通过联合采样噪声水平和物理时间步长，学习统一速度场以减少长期滚动漂移并实现不确定性感知的集合生成。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数PDE基础模型依赖确定性Transformer架构，缺乏生成灵活性，无法满足许多科学和工程应用的需求。

Method: 1. Flow Marching算法：联合采样噪声水平和物理时间步长，学习统一速度场；2. P2VAE：将物理状态嵌入紧凑潜空间；3. FMT：结合扩散强迫方案和潜时间金字塔，实现高效计算。

Result: 在12个不同PDE家族的约250万轨迹上训练，在未见过的Kolmogorov湍流上进行了少样本适应基准测试，展示了比确定性对应物更稳定的长期滚动性能，并呈现了不确定性分层的集合结果。

Conclusion: 生成式PDE基础模型对于现实世界应用具有重要意义，Flow Marching方法在减少计算成本的同时提高了模型的生成能力和稳定性。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [99] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，通过行列缩放对角矩阵来适配预训练权重矩阵，仅需n+m个可训练参数即可实现高秩更新，在多个基准测试中性能接近全量微调但参数数量大幅减少


<details>
  <summary>Details</summary>
Motivation: 基础模型在多样化任务中表现出色，但适配到专业应用通常需要微调，这种方法内存和计算成本高。参数高效微调方法通过仅更新小部分权重来缓解这个问题

Method: HyperAdapt通过应用行列缩放对角矩阵来适配预训练权重矩阵，为n×m矩阵仅需n+m个可训练参数，理论上建立了更新秩的上界，经验证实能在模型各层诱导高秩变换

Result: 在GLUE、算术推理和常识推理基准测试中，使用高达140亿参数的模型进行实验，HyperAdapt的性能与全量微调和最先进的PEFT方法相当或接近，同时使用的可训练参数数量减少了几个数量级

Conclusion: HyperAdapt是一种高效的参数微调方法，能在保持性能的同时显著减少可训练参数数量，为大型模型的适配提供了更实用的解决方案

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [100] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 本文提出了一种新的高矩阵聚类框架SCoS，直接对矩阵数据进行子空间聚类，而非传统向量化方法。通过三阶张量的块项分解实现联合聚类和子空间估计。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法假设向量化数据，无法有效处理矩阵形式的高维数据。需要一种能直接对矩阵列空间进行聚类的通用框架。

Method: 基于块项分解(BTD)的三阶张量分解方法，构建输入矩阵的张量表示，联合估计聚类成员关系和部分共享子空间。

Result: 在高光谱成像数据集上的实验表明，该方法在聚类精度和鲁棒性方面优于现有技术，特别是在高噪声和干扰条件下表现优异。

Conclusion: 该框架在处理具有超越单个数据向量结构的高维应用方面具有巨大潜力，为矩阵数据的子空间聚类提供了新的解决方案。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [101] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 本研究利用图机器学习进行理性农药设计，开发更安全环保的农用化学品，创建了最大的蜜蜂农药毒性数据集ApisTox，并评估了多种机器学习模型在农化领域的表现。


<details>
  <summary>Details</summary>
Motivation: 受药物发现中计算机辅助方法的启发，旨在加速开发更安全、环保的农用化学品，特别关注生态毒理学问题。

Method: 创建了ApisTox数据集（最大的蜜蜂农药毒性数据集），广泛评估了分子指纹、图核、图神经网络和预训练变换器等机器学习模型在分子图分类任务中的表现。

Result: 发现在药物化学中成功的方法往往无法推广到农用化学品领域，凸显了开发领域特定模型和基准的必要性。

Conclusion: 需要针对农药发现独特挑战开发专门的机器学习模型和综合基准测试套件，未来工作将聚焦于此。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [102] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于计算不同马尔可夫决策过程（MDP）之间的状态相似性，解决了传统双模拟度量在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量（BSM）在单个MDP内有效，但在多MDP场景（如策略迁移）中应用受限。现有方法缺乏对数学性质的严格分析，限制了理论进展。

Method: 通过形式化建立MDP对之间的广义双模拟度量，严格证明其三个基本性质：对称性、MDP间三角不等式和相同状态空间上的距离界限。

Result: GBSM在策略迁移、状态聚合和基于采样的估计等方面获得了比标准BSM更严格的理论界限，并提供了闭式样本复杂度估计。数值结果验证了理论发现。

Conclusion: GBSM为多MDP场景提供了理论基础和实用工具，在理论分析和实际应用中都优于传统BSM方法。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [103] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出了一种将强化学习与大型语言模型相结合的新方法，用于电商支付欺诈检测。通过将交易风险建模为多步马尔可夫决策过程，RL优化多支付阶段的风险检测。LLM用于迭代改进奖励函数，提高欺诈检测准确性并展示零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法在复杂多变的支付环境中面临挑战，特别是设计有效的RL奖励函数需要大量人工专业知识。LLM具有先进的推理和编码能力，可以自动优化奖励函数，克服传统方法的局限性。

Method: 将交易风险建模为多步MDP，利用LLM迭代优化RL奖励函数。LLM通过其推理能力自动改进奖励函数设计，实现更好的欺诈检测性能。

Result: 在真实数据上的实验证实了该方法的有效性、鲁棒性和韧性。LLM增强的RL框架在长期评估中表现出色，欺诈检测准确率得到提升。

Conclusion: 该方法展示了LLM在工业RL应用中的潜力，特别是在复杂风险检测任务中，LLM能够显著提升RL模型的性能，为电商支付安全提供创新解决方案。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [104] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新型周期性CNN架构，通过引入周期性边界条件，能够在d维输入空间中近似依赖于d-1个线性变量的脊函数，而传统CNN在低维脊设置下无法实现这种近似。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理具有高内在维度脊状结构的数据时存在局限性，特别是在图像分析、物理信息学习和材料科学等领域，数据往往具有自然的周期性特征。

Method: 设计周期性CNN架构，在卷积层中引入周期性边界条件，并建立严格的近似理论来证明其表达能力。

Result: 理论证明周期性CNN能够精确近似高维脊函数，而传统CNN在低维脊设置下无法实现这种近似，确立了周期性CNN表达能力的尖锐特征。

Conclusion: 周期性CNN不仅扩展了CNN近似理论的数学基础，还为处理具有高内在维度脊状结构的问题提供了一类具有实用价值的架构。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [105] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO是一个基于时间序列基础模型的时间序列异常检测方法，通过引入基于patch的记忆模块来缓解模型过度泛化问题，能够在多个数据集上联合微调并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重建的深度模型在时间序列异常检测中容易过度泛化，准确重建未见过的异常；而现有的记忆架构方法训练成本高且难以与时间序列基础模型有效集成。

Method: 提出MOMEMTO方法，包含基于patch的记忆模块，从多个领域捕获代表性正常模式，通过多领域训练策略实现单一模型在多个数据集上的联合微调。记忆项使用预训练编码器的潜在表示初始化，组织为patch级单元，并通过注意力机制更新。

Result: 在23个单变量基准数据集上的实验表明，MOMEMTO作为单一模型在AUC和VUS指标上优于基线方法，并进一步提升了其骨干时间序列基础模型的性能，特别是在少样本学习场景中。

Conclusion: MOMEMTO通过记忆模块有效缓解了时间序列异常检测中的过度泛化问题，展示了在多数据集联合训练和少样本学习中的优势。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [106] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 本文分析了对角线性网络的训练轨迹与lasso正则化路径之间的紧密联系，表明训练时间起到了逆正则化参数的作用。


<details>
  <summary>Details</summary>
Motivation: 对角线性网络的理论兴趣在于其隐式正则化可以被严格分析。本文旨在深化这一分析，展示其完整训练轨迹与lasso正则化路径的关系。

Method: 通过理论分析和数值模拟，研究对角线性网络从小初始化开始的训练过程，并与lasso正则化路径进行比较。

Result: 在lasso路径单调性假设下，这种联系是精确的；在一般情况下，则显示出近似联系。

Conclusion: 对角线性网络的训练轨迹与lasso正则化路径密切相关，训练时间相当于逆正则化参数。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [107] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成概率机器学习的诊断框架，用于改进数据驱动的一致性诊断，通过量化和自动化预测不确定性来提高诊断特性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中应用广泛，但现有模型在置信度评估方面存在困难，这在基于一致性的诊断中尤为重要，因为决策逻辑对误报高度敏感。

Method: 使用集成概率机器学习方法来量化和自动化预测不确定性，改进数据驱动的一致性诊断的诊断特性。

Result: 通过消融分析和比较分析在多个案例研究中评估了所提出的方法，显示在一系列诊断指标上都有持续改进。

Conclusion: 该框架能够有效解决深度神经网络在故障诊断中置信度评估的挑战，提高诊断的可靠性和准确性。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [108] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的轻量级通用数据同化方法，无需额外训练，适用于气象学、海洋学、机器人学等领域的状态估计


<details>
  <summary>Details</summary>
Motivation: 数据同化在多个学科中用于从噪声观测中估计动态系统状态，但现有方法可能计算复杂或需要专门训练

Method: 基于粒子滤波器算法，利用预训练的扩散模型进行数据同化，以GenCast（基于扩散的全球集合天气预报模型）为例进行说明

Result: 开发了一种无需额外训练的轻量级通用数据同化框架

Conclusion: 该方法为利用预训练扩散模型进行高效数据同化提供了可行方案

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [109] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文提出了一种新的低秩双随机聚类方法（LoRD），通过减少对约束条件的过度松弛来提高聚类效果，并进一步引入块对角正则化（B-LoRD）来增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的聚类方法（如谱聚类、对称非负矩阵分解等）为了数值可行性过度松弛了低秩、非负、双随机和正交约束，这可能限制了它们的聚类效果。

Method: 提出LoRD模型，仅松弛正交约束来获得概率聚类结果；引入块对角正则化（B-LoRD）通过最大化Frobenius范数来增强聚类性能；将非凸双随机约束转化为线性凸约束，并提出全局收敛的投影梯度下降算法进行优化。

Result: 理论分析证明了正交性与块对角性在双随机约束下的等价性，并证明了LoRD和B-LoRD的梯度Lipschitz连续性。大量实验验证了方法的有效性。

Conclusion: LoRD和B-LoRD方法通过更合理的约束松弛和块对角正则化，显著提升了聚类性能，为图基聚类提供了新的有效解决方案。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [110] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出SWE和SVoD方法，通过权重共享和梯度分配策略解决神经网络扩展中新神经元失活问题


<details>
  <summary>Details</summary>
Motivation: 神经网络扩展训练中新加入的神经元经常无法适应已训练网络而变得失活，无法有效增加网络容量

Method: SWE方法通过将新神经元与现有神经元耦合实现平滑集成，SVoD方法基于梯度分配神经元到不同层

Result: 在四个数据集上的实验表明该方法能有效抑制神经元失活，性能优于其他扩展方法和基线

Conclusion: 该方法为神经网络扩展提供了一种有效的解决方案，能实现更好的容量增长和性能提升

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [111] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO算法解决了GRPO在组内响应完全正确或完全错误时无法学习的问题，通过优势校准和不对称裁剪技术，将同质错误转化为有效的学习信号，在数学推理任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: GRPO算法存在关键缺陷：当组内所有响应完全正确或完全错误时，模型无法从这些同质响应中学习。特别是对于完全错误的组，GRPO的优势函数值为零，导致梯度消失和学习信号丢失。

Method: 提出NGRPO算法：1）优势校准机制：在优势计算中假设存在虚拟最大奖励样本，改变组内奖励的均值和方差，确保同质错误样本的优势不再为零；2）不对称裁剪：放松正样本的更新幅度，同时对负样本施加更严格的约束，稳定探索压力。

Result: 在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准上显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法。

Conclusion: NGRPO能够有效从同质错误中学习，实现数学推理能力的稳定和显著提升，验证了该算法在处理同质响应问题上的有效性。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [112] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 该论文首次分析了异质性在图级学习中的影响，揭示了与节点级任务不同的频率动态机制，并提出频率自适应模型在基序检测任务中优于频率主导模型。


<details>
  <summary>Details</summary>
Motivation: 虽然异质性在节点级任务中被广泛研究，但其对图级任务的影响尚不清楚。本文旨在填补这一研究空白，建立图级学习中异质性的理论理解。

Method: 结合理论分析和实证验证：1）引入图级标注方案的分类法；2）使用基于能量的梯度流分析揭示基序检测的频率动态机制；3）在合成数据集和真实分子属性预测数据集上进行实验验证。

Result: 理论分析表明基序目标与全局频率主导存在固有错位，需要不同的架构考虑。实验证明频率自适应模型在基序检测任务中优于频率主导模型。

Conclusion: 这项工作建立了图级学习中异质性的新理论理解，并为设计有效的GNN架构提供了指导，强调了频率自适应机制的重要性。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [113] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一种在联邦学习中通过动态优化后门触发器来解耦主任务和后门任务的方法，提高后门攻击的持久性和抗防御能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性暴露了新的攻击面，现有后门攻击依赖固定模式或对抗性扰动作为触发器，导致主任务和后门任务紧密耦合，容易被诚实更新稀释且在联邦防御下持久性有限。

Method: 采用min-max框架动态优化后门触发器，内层最大化中毒样本与良性样本的性能差距，外层将自适应触发器注入本地模型。

Result: 在计算机视觉和自然语言任务上评估，与六种后门攻击方法和六种防御算法比较，实验结果显示该方法具有良好的攻击性能且易于集成到现有后门攻击技术中。

Conclusion: 该方法成功解耦了后门任务与主任务，提高了后门攻击在联邦学习环境下的持久性和抗防御能力。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [114] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 本文提出了GNARL框架，将神经算法推理重新构建为马尔可夫决策过程，结合模仿学习和强化学习，解决了传统NAR方法在组合NP难问题上的局限性，并在缺乏专家算法的情况下仍能有效工作。


<details>
  <summary>Details</summary>
Motivation: 传统神经算法推理(NAR)存在多个限制：无法在没有后处理的情况下构建有效解、不能处理多个正确解、在组合NP难问题上表现不佳、以及无法应用于缺乏强算法的问题。

Method: 将算法轨迹学习重新构建为马尔可夫决策过程，提出GNARL框架，结合模仿学习和强化学习方法，适用于广泛的图基问题。

Result: 在多个CLRS-30问题上实现了很高的图精度，在NP难问题上的性能匹配或超过了更窄的NAR方法，甚至在缺乏专家算法的情况下仍能应用。

Conclusion: GNARL框架成功解决了NAR的关键限制，为算法学习提供了更强大和通用的方法，特别是在处理复杂组合问题和缺乏已知算法的情况下表现出色。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [115] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 本文提出使用置信网络（CN）作为贝叶斯网络（BN）的隐私保护替代方案，通过在BN基础上添加不确定性来平衡隐私和效用，有效抵御追踪攻击同时保持有意义的推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题日益严重，传统贝叶斯网络在发布时容易遭受追踪攻击，现有噪声添加方法虽然提供隐私保护但严重损害模型效用，需要一种新的平衡隐私和效用的解决方案。

Method: 将贝叶斯网络转换为置信网络，通过引入不确定性来掩盖学习到的BN参数，同时识别并隐藏关键学习信息以防止攻击者恢复原始BN。

Result: 实验结果表明置信网络能够有效降低追踪攻击的成功概率，通过调节CN超参数可以灵活调整隐私保护程度，在保持有意义推理的同时实现隐私保护。

Conclusion: 置信网络为开发隐私感知的概率图模型提供了一种原则性、实用且有效的方法，能够在隐私保护和模型效用之间取得良好平衡。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [116] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: 本文提出HEROS（异构在线集成）方法，通过马尔可夫决策过程在流数据挖掘中平衡预测性能和计算资源消耗，特别设计了ζ策略来训练接近最优的模型以降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法过度关注预测能力而忽视计算成本，不符合可持续发展需求。需要开发更环保的在线学习方法来解决资源约束下的模型选择问题。

Method: 提出HEROS框架，在每次训练步骤中从具有不同超参数的模型池中选择子集进行训练。引入马尔可夫决策过程理论建模性能与可持续性约束的权衡，并提出ζ策略等不同选择策略。

Result: 在11个基准数据集上的实验表明，ζ策略在保持高准确性的同时显著减少资源消耗，在某些情况下甚至优于竞争对手，验证了理论分析的近最优性能。

Conclusion: HEROS框架和ζ策略为绿色在线学习提供了有效解决方案，在保证预测性能的同时大幅提升资源效率，是流数据挖掘领域的重要贡献。

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [117] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文为异步更新的Polyak-Ruppert平均Q学习建立了中心极限定理，包括非渐近中心极限定理和泛函中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 研究异步更新下Q学习的统计性质，特别是其收敛速率和极限分布，以更好地理解算法的理论性能。

Method: 采用Polyak-Ruppert平均技术，分析异步Q学习在Wasserstein距离下的收敛速率，并推导部分和过程的弱收敛性。

Result: 得到了明确的收敛速率表达式，反映了迭代次数、状态-动作空间大小、折扣因子和探索质量的影响，并证明部分和过程弱收敛于布朗运动。

Conclusion: 该工作为异步Q学习提供了严格的统计理论基础，有助于进一步的理论分析和算法改进。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [118] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出了一种名为Otters的新型SNN架构，利用光电设备的自然信号衰减特性实现TTFS编码，通过硬件-软件协同设计在GLUE基准测试中达到最先进精度，能耗效率提升1.77倍。


<details>
  <summary>Details</summary>
Motivation: 传统SNN虽然承诺高能效，但TTFS编码中的时间衰减函数计算和突触权重乘法操作实际上消耗大量能量，未能真正实现能效优势。

Method: 利用氧化铟光电突触器件的自然物理衰减特性直接实现时间函数计算，将模拟输出作为突触权重和时间衰减的融合乘积；开发量化神经网络到SNN的转换算法以支持复杂架构如Transformer。

Result: 在七个GLUE基准数据集上达到最先进精度；基于22nm商用工艺的能耗分析显示，相比先前领先的SNN，计算、数据移动和内存访问成本综合优化后能效提升1.77倍。

Conclusion: 建立了一种新的高效能SNN范式，将基础器件物理特性直接转化为强大的计算原语，所有代码和数据均已开源。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [119] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本文提出了模拟基础神经网络（SGNNs）的理论基础，证明其在模拟先验下实现摊销贝叶斯推断，收敛到贝叶斯最优预测器，并能在模型错误设定下学习经验方法无法观测的科学量。


<details>
  <summary>Details</summary>
Motivation: SGNNs在真实世界标签有限或不可观测的领域取得了最先进性能，但缺乏理论基础。本文旨在建立模拟基础学习的正式理论框架。

Method: 通过理论分析证明SGNNs实现摊销贝叶斯推断，推导模型错误设定下的泛化边界，并形式化SGNNs特有的机制可解释性方法。

Result: 数值实验验证理论预测：SGNNs能够恢复潜在参数，在失配情况下保持鲁棒性，在模型选择任务中误差比AIC降低一半。

Conclusion: SGNNs为数据受限场景下的科学预测提供了一个原则性和实用的框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [120] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net是一种创新的参数高效框架，通过利用层间激活残差的低秩特性，在保持高性能的同时显著降低参数复杂度和内存/计算需求。


<details>
  <summary>Details</summary>
Motivation: 解决当前低秩方法存在的三个关键问题：模型性能受损、计算开销大、激活内存节省有限。

Method: 采用双路径架构，通过结合前一层输出与其低秩差异来高效重构层激活，并开发专门的激活重计算策略来大幅降低内存需求。

Result: 在60M到7B参数规模的预训练实验中，CR-Net持续优于最先进的低秩框架，同时需要更少的计算资源和内存。

Conclusion: CR-Net成功解决了现有低秩方法的局限性，为高效LLM预训练提供了有效的解决方案。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [121] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述了无标签数据表示学习的最新理论进展，重点分析了深度学习方法与传统统计方法的差异，以及作者在该方向上的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型使用自监督、去噪/掩码自编码器等新原理进行无监督表示学习，但这些方法难以用经典理论分析。需要结合统计和优化的数学工具来理解这些模型学到的表示特征及其成功原因。

Method: 结合统计学和优化的数学工具，对无标签数据表示学习的理论框架进行分析，特别是针对视觉基础模型的自监督和掩码自编码器方法。

Result: 提供了无标签数据表示学习的最新理论进展综述，阐明了深度学习模型表示学习的数学原理和理论依据。

Conclusion: 通过结合统计和优化理论，能够更好地理解和分析深度学习无监督表示学习的机制，为解释模型性能和多任务适应性提供理论支撑。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [122] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 本文提出了一种完全可学习的神经奖励机（FLNRM），能够端到端学习符号接地函数和自动机，无需依赖预定义的符号映射或先验任务知识，在非马尔可夫强化学习任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决非马尔可夫强化学习任务中传统方法依赖预定义符号接地函数和先验任务知识的限制，使方法更易应用且更具可解释性。

Method: 提出完全可学习的神经奖励机（FLNRM），集成深度强化学习，端到端学习符号接地和自动机结构。

Result: FLNRM方法优于基于循环神经网络（RNN）的先前方法，同时保持了自动机的有限性和紧凑性带来的可解释性优势。

Conclusion: FLNRM为复杂时序任务提供了一种既易于应用又具可解释性的解决方案，在非马尔可夫强化学习领域具有重要价值。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [123] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge是一个统一的多模态框架，支持视觉语言理解、生成和检索任务，通过轻量级双向潜在对齐模块和两阶段解耦训练策略实现高效多模态建模。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决方案往往孤立处理不同任务或需要从头训练，导致计算成本高且跨模态泛化能力有限。

Method: 采用语言中心设计，重用预训练LLM，引入轻量级双向潜在对齐模块，提出两阶段解耦训练策略：监督微调+潜在空间对齐，以及语义引导的扩散训练。

Result: 在广泛基准测试中，OmniBridge在所有三个任务上都取得了竞争性或最先进的性能。

Conclusion: 潜在空间对齐在共享表示空间下统一多模态建模方面具有有效性，代码和模型已开源。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [124] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 本文提出了一种结合GAN和Transformer编码器的混合方法，用于生成高质量的信用卡欺诈交易样本，以解决数据集严重不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临严重的数据不平衡问题，传统过采样方法如SMOTE生成的样本过于简单，而现有的生成模型如CTGAN和TVAE在高维依赖建模方面仍有不足。

Method: 使用生成对抗网络（GAN）结合Transformer编码器块，GAN通过对抗训练生成真实样本，Transformer通过自注意力机制学习丰富的特征交互。

Result: 在公开的信用卡欺诈检测数据集上测试，与传统的和生成的重采样策略相比，基于Transformer的GAN在召回率、F1分数和AUC指标上均有显著提升。

Conclusion: 该方法有效克服了欺诈检测任务中严重的类别不平衡问题，生成了多样化的高质量少数类样本。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [125] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一个基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略实现跨架构的对抗样本生成


<details>
  <summary>Details</summary>
Motivation: 解决现有黑盒对抗攻击方法依赖特定网络架构、查询次数多、跨架构迁移性差的问题

Method: 利用CNN和ViT模型的注意力图指导潜在扩散模型生成对抗样本，关注不同架构共有的敏感图像区域

Result: JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法

Conclusion: JAD为黑盒对抗攻击提供了一个有前景的有效范式

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [126] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 本文研究了三种无需反向传播的深度学习训练方法（FF、CaFo、MF），发现MF算法在MLP架构上不仅达到甚至超越了BP的准确率，同时显著降低了41%的能耗和34%的训练时间。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的巨大计算和能耗需求主要由反向传播驱动，这挑战了AI的可持续发展。本文旨在探索更节能的BP-free训练方法。

Method: 建立了严格的比较框架，在各自原生架构上实现三种BP-free算法（FF、MF使用MLP，CaFo使用CNN），使用Optuna优化超参数，基于验证性能应用一致的早停标准。

Result: MF算法在MLP上的分类准确率不仅与BP相当，而且持续超越BP。硬件层面测量显示MF降低能耗41%，缩短训练时间34%，碳足迹显著减小。

Conclusion: MF算法通过收敛到验证损失景观中更有利的最小值实现优越泛化，挑战了全局优化需求假设，为未来节能深度学习提供了数据驱动的路线图。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [127] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出了扩散桥变分推断（DBVI），一种改进的深度高斯过程后验推断方法，通过可学习的、数据依赖的初始分布来提升推断效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪扩散变分推断（DDVI）方法使用固定的无条件起始分布，与真实复杂后验分布差距较大，导致推断轨迹效率低下和收敛缓慢。

Method: DBVI通过参数化的摊销神经网络构建数据依赖的初始分布，利用诱导输入作为低维数据集摘要，在Doob桥扩散过程框架下重新解释先验，并推导出可处理的训练目标。

Result: 在回归、分类和图像重建任务中，DBVI在预测精度、收敛速度和后验质量方面均优于DDVI和其他变分基线方法。

Conclusion: DBVI通过引入可学习的扩散桥初始化机制，显著提升了深度高斯过程后验推断的效率和性能，同时保持了DDVI的数学优雅性。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [128] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN是一种新颖的图神经网络架构，受Axelrod文化传播模型启发，通过相似性门控概率交互、特征级复制机制和全局极化保持，解决了传统GNN的特征过度平滑、异构关系处理困难和特征向量处理不灵活等基本限制。


<details>
  <summary>Details</summary>
Motivation: 传统GNN存在三个基本限制：深层网络中节点表示变得难以区分的特征过度平滑问题；难以有效处理连接节点差异显著的异构关系；将整个特征向量作为不可分割单元处理，限制了灵活性。

Method: AxelGNN采用相似性门控概率交互，根据节点相似性自适应促进收敛或发散；实现特征级复制机制，在段级别进行细粒度特征聚合；保持全局极化以在多个表示簇中保持节点独特性。

Result: 在节点分类和影响力估计基准测试上的广泛实验表明，AxelGNN在不同图结构上（具有不同的同质性-异质性特征）始终优于或匹配最先进的GNN方法。

Conclusion: AxelGNN的双稳态收敛动力学自然地在单一架构内处理同质性和异质性图，为解决GNN的基本限制提供了统一框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [129] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 本文研究了迁移学习环境下的非上下文多臂老虎机问题，提出了KL-UCB-Transfer算法，在源分布和目标分布足够接近时显著优于无先验基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法未考虑迁移学习场景，本文旨在利用源分布的先验知识来提升目标分布上的学习效率。

Method: 提出KL-UCB-Transfer索引策略，该策略结合了KL-UCB算法和迁移参数（距离边界、样本数量），在已知源分布与目标分布距离边界的情况下进行优化。

Result: 推导了包含迁移参数的渐进累积遗憾下界，并在高斯分布情况下证明KL-UCB-Transfer算法能够达到该下界。仿真实验验证了算法在源分布和目标分布接近时的优越性能。

Conclusion: 迁移学习框架能有效提升多臂老虎机问题的性能，特别是在源分布和目标分布相似的情况下，KL-UCB-Transfer算法具有理论保证和实际效果。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [130] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 该论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，涵盖了对抗样本、领域泛化和大型语言模型越狱三个主要方向，提出了新的算法、训练范式和认证方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在安全关键应用中的广泛使用，确保模型决策能够抵抗对抗性攻击变得至关重要。

Method: 针对计算机视觉中的对抗样本问题，提出了新的技术结果、训练范式和认证算法；针对领域泛化问题，开发了在医学影像、分子识别和图像分类中实现最先进泛化性能的新算法；针对LLM越狱问题，提出了新的攻击和防御方法。

Result: 在多个领域（医学影像、分子识别、图像分类）实现了最先进的泛化性能，并在LLM鲁棒性方面取得了前沿进展。

Conclusion: 该研究为设计具有理想鲁棒性属性的算法做出了重要贡献，推动了鲁棒语言智能体的发展。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [131] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL是一个统一的鲁棒REBEL更新方法家族，通过Wasserstein、KL和χ²模糊集解决RLHF中的过优化问题，在保持可扩展性的同时实现最优参数化率。


<details>
  <summary>Details</summary>
Motivation: 现有的离线RLHF方法存在过优化问题，模型会过度拟合奖励错误设定并偏离训练期间的偏好行为。

Method: 使用Fenchel对偶性将更新简化为简单的相对奖励回归，避免了PPO风格的裁剪或辅助价值网络。推导了三种散度的实用SGD算法：梯度正则化(Wasserstein)、重要性加权(KL)和快速一维对偶求解(χ²)。

Result: 在Emotion Alignment、ArmoRM多目标基准和HH-Alignment上的实验显示，该方法在未见偏好混合、模型大小和数据规模上具有强大的最坏情况鲁棒性，χ²-REBEL表现出持续强劲的实证性能。

Conclusion: 研究验证了无免费午餐权衡：半径收缩速度快于经验散度集中率时实现极小极大最优参数化率但丧失覆盖保证，而覆盖保证半径则产生O(n^{-1/4})速率。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [132] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一种可扩展的多标签因果发现方法，用于处理稀疏、高维事件序列，通过预训练的因果Transformer作为领域特定基础模型，并行推断每个序列的因果图，并通过自适应频率融合重建标签的全局马尔可夫边界。


<details>
  <summary>Details</summary>
Motivation: 理解事件序列中的因果关系对于医疗保健、车辆诊断等领域至关重要，但目前仍是一个未解决的挑战，特别是在处理包含数千种独特事件类型的高维稀疏事件序列时。

Method: 使用两个预训练的因果Transformer作为事件序列的领域特定基础模型，CARGO并行推断每个序列的因果图，并通过自适应频率融合来聚合这些图，从而重建标签的全局马尔可夫边界，避免了全数据集条件独立性测试的高成本。

Result: 在一个具有29,100多个独特事件类型和474个不平衡标签的真实世界汽车故障预测数据集上，CARGO展示了其进行结构化推理的能力。

Conclusion: CARGO的两阶段方法实现了高效的概率推理，能够在大规模稀疏高维事件序列中有效发现因果关系。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [133] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 该论文提出了两种向量（标准差向量和聚类向量）来分析大语言模型的权重特征，能够有效区分不同模型并展示同族模型间的相似性。研究发现LoRA微调后，权重分布受数据集影响，但权重间的相关性保持稳定。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的架构和参数特征，特别是权重特性，以分析模型间的相关性和差异。

Method: 提出标准差向量（假设权重服从正态分布，对投影矩阵标准差进行归一化）和聚类向量（对权重投影矩阵的奇异值进行K-Means聚类），用于描述模型的分布特征和相关性特征。

Result: 两种向量能有效区分不同模型，清晰展示同族模型相似性。LoRA微调后，标准差向量表示的权重分布受数据集直接影响，但聚类向量表示的权重相关性不受影响，与预训练模型保持高度一致。

Conclusion: 标准差向量和聚类向量是分析大语言模型权重特征的有效工具，能揭示模型间的差异和相似性，特别是在微调过程中权重分布和相关性变化的不同规律。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [134] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL是一种新的强化学习方法，通过并发异步数据生成和模型训练，结合飞行中权重更新机制，解决了LLM训练中硬件效率和数据新鲜度之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法在扩展LLM推理能力时面临挑战，主要是在保持高AI加速器利用率的同时避免产生过时的离策略数据，这些数据会损害常见的RL算法。

Method: PipelineRL采用并发异步数据生成和模型训练，核心创新是飞行中权重更新机制，允许LLM生成引擎在生成token序列期间以最小中断接收更新的模型权重。

Result: 在128个H100 GPU上进行的实验表明，PipelineRL相比传统RL基线实现了约2倍的学习速度提升，同时保持了高度在策略的训练数据。

Conclusion: PipelineRL在硬件效率和数据新鲜度之间实现了优越的平衡，其可扩展和模块化的开源实现是该论文的重要贡献。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [135] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU是一个生成式时空框架，通过显式建模人类移动的语义和时间复杂性来推进移动性分析。该框架包含四个关键创新：时空概念编码器、认知轨迹记忆、生活方式概念库和任务导向生成头。在四个真实世界数据集上的实验表明，该方法在三个基准任务上均优于强基线。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹记录了短期访问模式和持久生活方式规律，但现有方法难以有效建模移动的语义和时间复杂性。本文旨在开发一个能够更好捕捉用户意图和生活方式规律的生成式框架。

Method: 提出GSTM-HMU框架，包含四个核心组件：1）STCE编码器整合地理位置、POI类别语义和周期性时间节奏；2）CTM记忆模块自适应过滤历史访问，强调近期和行为显著事件；3）LCB概念库提供结构化人类偏好线索；4）任务导向生成头将学习表示转换为下游任务预测。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上的实验显示，该方法在下一个位置预测、轨迹用户识别和时间估计三个任务上均取得显著改进，超越了强基线方法。

Conclusion: 生成式建模为构建更鲁棒、可解释和可泛化的人类移动智能系统提供了有前景的基础。GSTM-HMU框架有效提取了复杂移动数据中的语义规律性，展示了生成方法在移动分析中的优势。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [136] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 该论文研究了激活函数在神经网络训练动态中的作用及其对强化学习中灾难性遗忘的影响，提出了能够产生稀疏输出和稀疏梯度的新激活函数类别——大象激活函数，以减轻灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是强化学习中长期存在的挑战，现有研究主要关注算法层面，而对神经网络架构特性如何导致灾难性遗忘的理解不足。本研究旨在填补这一空白，特别关注激活函数的作用。

Method: 通过分析激活函数在训练动态中的角色，发现除了稀疏表示外，激活函数的梯度稀疏性也对减少遗忘有重要作用。基于这一发现，提出了大象激活函数这一新类别。

Result: 研究表明，在基于价值的算法中，仅需将经典激活函数替换为大象激活函数，就能显著提高神经网络对灾难性遗忘的抵抗能力，使强化学习更加样本高效和内存高效。

Conclusion: 激活函数的梯度稀疏性是减轻灾难性遗忘的关键因素，大象激活函数通过同时提供稀疏输出和稀疏梯度，为强化学习的稳定性提供了有效的架构解决方案。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [137] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出了功能缩放定律（FSL），通过随机微分方程建模SGD训练过程，揭示了学习率调度对LLM预训练损失动态的影响，并理论证明了常见实践如学习率衰减和WSD调度的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律主要关注最终损失，忽略了训练过程中的损失动态和学习率调度的影响。本文旨在填补这一空白，深入理解LLM预训练的动态过程。

Method: 使用师生核回归设置和在线随机梯度下降，通过内在时间视角和SDE建模SGD，提出FSL框架来表征一般学习率调度下的总体风险演化。

Result: FSL成功捕捉了学习率调度的影响，理论证明了高容量模型更高效、学习率衰减提升效率、WSD调度优于直接衰减等经验实践，并在0.1B-1B参数规模的实验中验证了实用性。

Conclusion: FSL框架深化了对LLM预训练动态的理解，为大规模模型训练提供了理论指导和优化insights。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [138] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 该论文提出了一种新的鲁棒性验证方法，通过从训练数据中提取"弱鲁棒"样本来评估模型对对抗性和常见扰动的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 深度学习分类器在干净数据集上表现良好，但对对抗性攻击和常见数据失真等扰动非常脆弱，传统方法依赖扰动测试集进行评估，但这种方法不够敏感和早期。

Method: 通过局部鲁棒性分析直接从训练数据集中提取最易受扰动的"弱鲁棒"样本，这些样本作为模型脆弱性的早期敏感指标，用于指导针对性的性能提升。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上的实验表明，基于弱鲁棒样本的鲁棒性验证方法能有效提升模型在对抗性和常见扰动场景下的可靠性。

Conclusion: 提出的弱鲁棒样本验证方法为模型鲁棒性评估提供了更细致和早期的视角，能够指导模型性能的针对性改进，提高整体可靠性。

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [139] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill是一个知识蒸馏框架，通过预测、特征和补丁级蒸馏转移全局和局部知识，实现PPG信号在资源受限设备上的高效分析


<details>
  <summary>Details</summary>
Motivation: PPG广泛用于可穿戴健康监测，但大型PPG基础模型难以在资源受限设备上部署，需要开发高效的轻量化方法

Method: 提出PPG-Distill框架，包含形态蒸馏保留局部波形模式，节律蒸馏捕捉补丁间时间结构，通过预测级、特征级和补丁级蒸馏转移知识

Result: 在心率和房颤检测任务上，PPG-Distill将学生模型性能提升高达21.8%，推理速度提升7倍，内存使用减少19倍

Conclusion: PPG-Distill框架能够实现可穿戴设备上PPG信号的高效分析，为资源受限环境下的健康监测提供了可行解决方案

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [140] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文系统研究了开源文本到视频生成模型的延迟和能耗，建立了计算受限的分析模型，验证了空间分辨率、时间长度和去噪步骤的缩放规律，并对六种T2V模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成系统虽然能生成高质量视频，但计算成本高昂且能耗特性未被充分理解，需要系统性的能耗分析来指导可持续视频生成系统的设计。

Method: 开发计算受限分析模型预测缩放规律，通过WAN2.1-T2V实验验证模型预测，并扩展分析到六种不同的T2V模型比较其运行时和能耗特性。

Result: 验证了空间和时间维度上的二次增长规律，以及去噪步骤的线性缩放规律，为不同T2V模型提供了基准参考数据。

Conclusion: 研究结果为设计和部署更可持续的生成视频系统提供了基准参考和实用见解，有助于优化T2V模型的能效表现。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [141] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过消融研究分析了电力系统物理约束机器学习模型的混合策略，评估了从正则化约束到图神经网络等不同方法在准确性、物理合规性、工业准备度和分布外泛化四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和跨境电力交换的增加，电网面临更大的不确定性和运行风险。传统物理求解器虽然准确但速度慢，机器学习模型作为快速替代方案需要更好地遵守物理定律。

Method: 使用自定义基准测试管道LIPS，比较了从多层感知器到图神经网络的不同架构，以及将物理约束作为正则化项或无监督损失的混合策略。

Result: 研究结果揭示了物理知识整合如何影响模型在准确性、物理合规性、工业准备度和分布外泛化方面的性能。

Conclusion: 所有实现都是可复现的，并在相应Github页面提供，为电力系统混合模型的开发提供了系统评估框架。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [142] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文通过算法稳定性分析，研究了去中心化网络中对抗训练在扩散策略下的泛化性能，发现泛化误差随对抗扰动强度和训练步数增加而增长。


<details>
  <summary>Details</summary>
Motivation: 虽然对抗训练能增强模型鲁棒性，但存在鲁棒过拟合和泛化差距扩大的问题。现有研究已证明去中心化网络中对抗训练的收敛性，但其泛化特性尚未被探索。

Method: 采用基于算法稳定性的分析方法，针对凸损失函数在扩散策略下进行理论推导，并通过逻辑回归进行数值实验验证。

Result: 理论推导表明泛化误差随对抗扰动强度和训练步数增加而增长，这一发现在单智能体情况下一致，但在去中心化设置中是新颖的。数值实验验证了理论预测。

Conclusion: 研究为去中心化对抗训练的泛化性能提供了理论分析框架，揭示了对抗扰动和训练步数对泛化误差的影响规律。

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [143] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 本文研究发现，在大型推理模型中，更长的思维链（CoT）和更多的回顾步骤反而会降低准确性，提出了"失败步骤比例（FSF）"作为更有效的评估指标，并通过干预实验证明减少失败分支能显著提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在测试时需要大量计算资源用于生成长思维链，但什么构成有效的思维链仍不清楚。先前研究认为更长的思维链和更多回顾步骤能提升性能，但近期研究表明短思维可能优于长思维，因此需要系统性评估。

Method: 在十个大型推理模型上进行数学和科学推理的系统性评估；引入思维链的图结构视图来提取结构特征；提出失败步骤比例（FSF）指标；设计两种干预实验：基于指标排序候选思维链和编辑思维链移除失败分支。

Result: 发现思维链长度和回顾比例与准确性负相关；FSF指标在预测正确性方面优于长度和回顾比例；基于FSF的排序获得最大pass@1增益；移除失败分支显著提升准确性。

Conclusion: 有效的思维链是失败步骤较少的思维链，支持基于结构感知的测试时扩展策略，而非盲目生成长思维链。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>
