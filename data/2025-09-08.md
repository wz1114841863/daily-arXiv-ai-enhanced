<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [High Performance Matrix Multiplication](https://arxiv.org/abs/2509.04594)
*Ethan Davis*

Main category: cs.PF

TL;DR: 这篇论文比较了五种矩阵乘法算法的性能，包括CuBLAS、CUDA、BLAS、OpenMP和C++ Threads，发现CuBLAS在大矩阵乘法中性能最优


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法是深度学习、科学计算和视频图形等高性能技术的基础，需要对不同优化库进行性能比较

Method: 采用五种不同矩阵乘法算法（CuBLAS、CUDA、BLAS、OpenMP和C++ Threads），对边长至少10,000的正方矩阵进行性能测试

Result: 统计显著性分析（p-value < 5e-12）显示性能排序为：CuBLAS > CUDA > BLAS > OpenMP > C++ Threads

Conclusion: CuBLAS在大规模矩阵乘法任务中表现最优，为高性能计算应用提供了重要参考

Abstract: Matrix multiplication is the foundation from much of the success from high
performance technologies like deep learning, scientific simulations, and video
graphics. High level programming languages like Python and R rely on highly
optimized low level libraries for performing core linear algebra operations
like matrix multiplication from Basic Linear Algebra Subprograms (BLAS). This
paper compares the performance of five different matrix multiplication
algorithms using CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads. We find
statistical significance with a p-value below 5e-12 to support the hypothesis
that for square $N \times N$ matrices where $N$ is at least 10,000 then the in
order performance as measured in floating point operations per second (FLOPS)
for these matrix multiplication algorithms is CuBLAS, CUDA, BLAS, OpenMP, and
C++ Threads.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: 本文系统性地评估了7种主流深度强化学习算法的能耗、碳排放和经济成本，发现不同算法间存在显著差异，最高可节省24%能耗和68%成本，为可持续DRL发展提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习的计算需求日益增长，但其能耗、温室气体排放和经济成本尚未得到充分研究，需要系统评估算法的环境和经济影响。

Method: 使用Stable Baselines实现7种DRL算法（DQN、TRPO、A2C、ARS、PPO、RecurrentPPO、QR-DQN），在10个Atari 2600游戏上各训练100万步，实时测量功耗并估算能耗、CO2排放和电费成本。

Result: 算法间能效和训练成本差异显著：ARS比DQN节能24%，QR-DQN比RecurrentPPO减少近68%的CO2排放和成本，部分算法在保持性能的同时显著降低环境影响。

Conclusion: 研究揭示了DRL算法的可持续性差异，为开发节能环保的DRL实践提供了可行见解，并建立了将可持续性考量纳入未来算法设计和评估的基础。

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [3] [Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics](https://arxiv.org/abs/2509.04536)
*Oliver Dunn,Koorosh Aslansefat,Yiannis Papadopoulos*

Main category: cs.LG

TL;DR: Q-SafeML是一种针对量子机器学习的安全监控方法，通过量子中心距离度量来检测概念漂移，提升系统透明度和安全性


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习的发展，现有经典机器学习的安全监控方法无法直接应用于QML，需要专门的安全机制

Method: 基于SafeML方法，采用量子中心距离度量来评估模型准确性，进行模型依赖的后分类评估

Result: 在QCNN和VQC模型上的实验表明，该方法能够检测操作数据与训练数据之间的距离，实现知情的人工监督

Conclusion: Q-SafeML为量子机器学习提供了有效的安全监控解决方案，适应了量子系统的独特表示约束

Abstract: The rise of machine learning in safety-critical systems has paralleled
advancements in quantum computing, leading to the emerging field of Quantum
Machine Learning (QML). While safety monitoring has progressed in classical ML,
existing methods are not directly applicable to QML due to fundamental
differences in quantum computation. Given the novelty of QML, dedicated safety
mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety
monitoring approach for QML. The method builds on SafeML, a recent method that
utilizes statistical distance measures to assess model accuracy and provide
confidence in the reasoning of an algorithm. An adapted version of Q-SafeML
incorporates quantum-centric distance measures, aligning with the probabilistic
nature of QML outputs. This shift to a model-dependent, post-classification
evaluation represents a key departure from classical SafeML, which is
dataset-driven and classifier-agnostic. The distinction is motivated by the
unique representational constraints of quantum systems, requiring distance
metrics defined over quantum state spaces. Q-SafeML detects distances between
operational and training data addressing the concept drifts in the context of
QML. Experiments on QCNN and VQC Models show that this enables informed human
oversight, enhancing system transparency and safety.

</details>


### [4] [Finance-Grounded Optimization For Algorithmic Trading](https://arxiv.org/abs/2509.04541)
*Kasymkhan Khubiev,Mikhail Semenov,Irina Podlipnova*

Main category: cs.LG

TL;DR: 该论文提出基于金融指标的损失函数（夏普比率、PnL、最大回撤）和换手率正则化方法，在算法交易指标上优于传统的均方误差损失。


<details>
  <summary>Details</summary>
Motivation: 深度学习在金融领域应用时，传统方法虽然在其他领域表现良好，但不完全适合金融专业人士使用的评估指标，需要开发金融基础化的损失函数。

Method: 引入基于关键量化金融指标的损失函数（夏普比率、盈亏、最大回撤），并提出换手率正则化方法来约束生成头寸的换手率在预定范围内。

Result: 提出的金融基础化损失函数结合换手率正则化，在算法交易指标评估中优于传统的均方误差损失函数。

Conclusion: 金融基础化指标能够提升交易策略和投资组合优化中的预测性能，为深度学习在金融领域的应用提供了更合适的评估框架。

Abstract: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.

</details>


### [5] [i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition](https://arxiv.org/abs/2509.04544)
*Ashutosh Kumar Sinha,Ayush Patel,Mitul Dudhat,Pritam Anand,Rahul Mishra*

Main category: cs.LG

TL;DR: i-Mask是一种基于呼气模式的新型人类活动识别方法，使用定制口罩传感器采集数据，通过噪声过滤和时间序列分解处理，达到95%以上的识别准确率


<details>
  <summary>Details</summary>
Motivation: 呼吸的吸气和呼气模式包含重要的生理信号，可用于预测人类行为、健康趋势和生命参数，这些生命体征与人类活动识别(HAR)密切相关，能够提供更深入的健康洞察并实现实时健康监测

Method: 开发了一种配备集成传感器的定制口罩(i-Mask)，采集志愿者佩戴时的呼气模式数据，对数据进行噪声过滤、时间序列分解和标注，然后训练预测模型

Result: 实验结果表明该方法有效，识别准确率超过95%

Conclusion: 该方法在医疗健康和健身应用领域具有巨大潜力

Abstract: The patterns of inhalation and exhalation contain important physiological
signals that can be used to anticipate human behavior, health trends, and vital
parameters. Human activity recognition (HAR) is fundamentally connected to
these vital signs, providing deeper insights into well-being and enabling
real-time health monitoring. This work presents i-Mask, a novel HAR approach
that leverages exhaled breath patterns captured using a custom-developed mask
equipped with integrated sensors. Data collected from volunteers wearing the
mask undergoes noise filtering, time-series decomposition, and labeling to
train predictive models. Our experimental results validate the effectiveness of
the approach, achieving over 95\% accuracy and highlighting its potential in
healthcare and fitness applications.

</details>


### [6] [Bootstrapping Task Spaces for Self-Improvement](https://arxiv.org/abs/2509.04575)
*Minqi Jiang,Andrei Lupu,Yoram Bachrach*

Main category: cs.LG

TL;DR: Exploratory Iteration (ExIt)是一种自课程强化学习方法，通过选择性采样信息量最大的中间历史来训练LLM进行多步自我改进，在推理时实现超越训练迭代深度的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设固定的最大迭代深度，这既昂贵又随意。需要一种能够利用自我改进任务循环结构的方法，让代理能够在推理时进行可靠的多步自我改进。

Method: ExIt通过选择性采样训练过程中遇到的最具信息量的中间部分历史来扩展任务空间，将这些起点视为新的自我迭代任务实例来训练自我改进策略，并可结合显式探索机制维持任务多样性。

Result: 在数学竞赛、多轮工具使用和机器学习工程等多个领域，ExIt策略能够产生在保留任务实例上表现出强推理时自我改进能力的策略，并能在超出训练平均迭代深度的步数预算内迭代达到更高性能。

Conclusion: ExIt方法有效解决了固定迭代深度的限制，通过自课程学习实现了LLM在推理时的可靠多步自我改进能力，在多个实际任务领域展现出优越性能。

Abstract: Progress in many task domains emerges from repeated revisions to previous
solution attempts. Training agents that can reliably self-improve over such
sequences at inference-time is a natural target for reinforcement learning
(RL), yet the naive approach assumes a fixed maximum iteration depth, which can
be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family
of autocurriculum RL methods that directly exploits the recurrent structure of
self-improvement tasks to train LLMs to perform multi-step self-improvement at
inference-time while only training on the most informative single-step
iterations. ExIt grows a task space by selectively sampling the most
informative intermediate, partial histories encountered during an episode for
continued iteration, treating these starting points as new self-iteration task
instances to train a self-improvement policy. ExIt can further pair with
explicit exploration mechanisms to sustain greater task diversity. Across
several domains, encompassing competition math, multi-turn tool-use, and
machine learning engineering, we demonstrate that ExIt strategies, starting
from either a single or many task instances, can produce policies exhibiting
strong inference-time self-improvement on held-out task instances, and the
ability to iterate towards higher performance over a step budget extending
beyond the average iteration depth encountered during training.

</details>


### [7] [Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions](https://arxiv.org/abs/2509.04583)
*Jiequn Han,Kui Ren,Nathan Soedjak*

Main category: cs.LG

TL;DR: 本文提出一种基于测试实例的适配性采样框架，用于构建紧凑且信息丰富的逆问题解的监督学习数据集。该方法能够显著提高样本效率，特别在复杂先验分布或高精度要求场景下优势更为明显。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法需要从先验分布中抽取大量训练样本来学习通用的逆向映射，当先验维度高或需要高精度时数据收集成本很高。需要一种更高效的方法来减少样本需求。

Method: 提出了一种实例适配性采样框架，动态地根据具体测试实例分配采样努力。通过迭代地基于最新预测结果来精炼训练数据集，使其适配于每个测试实例周围逆向映射的几何结构。

Result: 在逆向散射问题中进行了实验验证，结果显示适配性方法在更复杂的先验分布或更高的精度要求下优势更为明显。该方法能够显著提高样本效率。

Conclusion: 这种适配性采样策略具有广泛的适用性，可以轻松扩展到其他逆问题中，为传统的固定数据集训练模式提供了一种可扩展且实用的替代方案。

Abstract: We propose an instance-wise adaptive sampling framework for constructing
compact and informative training datasets for supervised learning of inverse
problem solutions. Typical learning-based approaches aim to learn a
general-purpose inverse map from datasets drawn from a prior distribution, with
the training process independent of the specific test instance. When the prior
has a high intrinsic dimension or when high accuracy of the learned solution is
required, a large number of training samples may be needed, resulting in
substantial data collection costs. In contrast, our method dynamically
allocates sampling effort based on the specific test instance, enabling
significant gains in sample efficiency. By iteratively refining the training
dataset conditioned on the latest prediction, the proposed strategy tailors the
dataset to the geometry of the inverse map around each test instance. We
demonstrate the effectiveness of our approach in the inverse scattering problem
under two types of structured priors. Our results show that the advantage of
the adaptive method becomes more pronounced in settings with more complex
priors or higher accuracy requirements. While our experiments focus on a
particular inverse problem, the adaptive sampling strategy is broadly
applicable and readily extends to other inverse problems, offering a scalable
and practical alternative to conventional fixed-dataset training regimes.

</details>


### [8] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: FedSub是一种高效的联邦学习子空间算法，通过低维子空间投影减少通信、计算和内存成本，并利用低维对偶变量缓解客户端漂移问题


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在大规模深度神经网络应用中的关键挑战，包括数据异构性导致的客户端漂移问题，以及高通信、计算和内存成本

Method: 使用子空间投影确保每个客户端的本地更新在低维子空间内进行，同时引入低维对偶变量来缓解客户端漂移

Result: 提供了收敛性分析，揭示了步长和子空间投影矩阵等关键因素对收敛的影响，实验结果表明其高效性

Conclusion: FedSub算法能有效解决联邦学习中的客户端漂移问题，同时显著降低通信、计算和内存开销

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


### [9] [Toward Faithfulness-guided Ensemble Interpretation of Neural Network](https://arxiv.org/abs/2509.04588)
*Siyu Zhang,Kenneth Mcmillan*

Main category: cs.LG

TL;DR: FEI是一个通过忠实性引导的集成解释框架，通过平滑近似提升定量忠实性分数，在隐藏层编码中增强忠实性，并提出新的定性度量标准，在实验中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 为神经推理提供可解释和忠实的解释对于理解和评估模型行为至关重要，需要提升解释的广度和有效性。

Method: 使用忠实性引导的集成解释框架(FEI)，通过平滑近似技术提升定量忠实性分数，针对隐藏层编码开发多种变体以增强忠实性，并提出新的定性度量标准。

Result: 在大量实验中，FEI超越了现有方法，在定性可视化和定量忠实性分数方面都取得了显著进展。

Conclusion: 该研究建立了一个全面的框架来提升神经网络解释中的忠实性，强调了广度和精确性的重要性。

Abstract: Interpretable and faithful explanations for specific neural inferences are
crucial for understanding and evaluating model behavior. Our work introduces
\textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation
(\textbf{FEI}), an innovative framework that enhances the breadth and
effectiveness of faithfulness, advancing interpretability by providing superior
visualization. Through an analysis of existing evaluation benchmarks,
\textbf{FEI} employs a smooth approximation to elevate quantitative
faithfulness scores. Diverse variations of \textbf{FEI} target enhanced
faithfulness in hidden layer encodings, expanding interpretability.
Additionally, we propose a novel qualitative metric that assesses hidden layer
faithfulness. In extensive experiments, \textbf{FEI} surpasses existing
methods, demonstrating substantial advances in qualitative visualization and
quantitative faithfulness scores. Our research establishes a comprehensive
framework for elevating faithfulness in neural network explanations,
emphasizing both breadth and precision

</details>


### [10] [Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction](https://arxiv.org/abs/2509.04601)
*Han Zhang,Fengji Ma,Jiamin Su,Xinyue Yang,Lei Wang,Wen-Cai Ye,Li Liu*

Main category: cs.LG

TL;DR: 提出量子增强加权多任务学习框架QW-MTL，用于ADMET分类任务，在13个TDC基准测试中12个任务表现优于单任务基线


<details>
  <summary>Details</summary>
Motivation: 现有单任务学习方法无法充分利用任务间的互补性，且计算资源消耗大，需要更高效的ADMET预测方法

Method: 基于Chemprop-RDKit框架，采用量子化学描述符增强分子表征，引入指数任务加权方案实现动态损失平衡

Result: 在13个TDC分类基准测试中，12个任务显著优于单任务基线，具有高预测性能和快速推理能力

Conclusion: 量子信息特征和自适应任务加权的多任务分子学习具有有效性和高效性，为药物发现提供了新解决方案

Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and
Toxicity) plays a crucial role in drug discovery and development, accelerating
the screening and optimization of new drugs. Existing methods primarily rely on
single-task learning (STL), which often fails to fully exploit the
complementarities between tasks. Besides, it requires more computational
resources while training and inference of each task independently. To address
these issues, we propose a new unified Quantum-enhanced and task-Weighted
Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET
classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts
quantum chemical descriptors to enrich molecular representations with
additional information about the electronic structure and interactions.
Meanwhile, it introduces a novel exponential task weighting scheme that
combines dataset-scale priors with learnable parameters to achieve dynamic loss
balancing across tasks. To the best of our knowledge, this is the first work to
systematically conduct joint multi-task training across all 13 Therapeutics
Data Commons (TDC) classification benchmarks, using leaderboard-style data
splits to ensure a standardized and realistic evaluation setting. Extensive
experimental results show that QW-MTL significantly outperforms single-task
baselines on 12 out of 13 tasks, achieving high predictive performance with
minimal model complexity and fast inference, demonstrating the effectiveness
and efficiency of multi-task molecular learning enhanced by quantum-informed
features and adaptive task weighting.

</details>


### [11] [Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families](https://arxiv.org/abs/2509.04622)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 提出了一个评估表征相似性度量区分能力的定量框架，通过三种互补的分离度量指标，系统比较了不同相似性度量在区分不同模型家族方面的表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对表征相似性度量在不同模型家族间区分能力的系统比较，需要定量评估框架来指导大规模模型和大脑比较中的度量选择。

Method: 使用信号检测理论的dprime、轮廓系数和ROC-AUC三种分离度量指标，评估RSA、线性预测性、Procrustes和软匹配等常用度量方法，在CNN、Vision Transformer、Swin Transformer、ConvNeXt等架构以及监督与自监督训练机制下的区分能力。

Result: 分离性随着度量施加更严格的对齐约束而系统性增加。在基于映射的方法中，软匹配获得最高分离性，其次是Procrustes对齐和线性预测性。非拟合方法如RSA也能在不同家族间产生强分离性。

Conclusion: 研究首次通过分离性视角系统比较了相似性度量，阐明了它们的相对敏感性，为大规模模型和大脑比较中的度量选择提供了指导。

Abstract: Representational similarity metrics are fundamental tools in neuroscience and
AI, yet we lack systematic comparisons of their discriminative power across
model families. We introduce a quantitative framework to evaluate
representational similarity measures based on their ability to separate model
families-across architectures (CNNs, Vision Transformers, Swin Transformers,
ConvNeXt) and training regimes (supervised vs. self-supervised). Using three
complementary separability measures-dprime from signal detection theory,
silhouette coefficients and ROC-AUC, we systematically assess the
discriminative capacity of commonly used metrics including RSA, linear
predictivity, Procrustes, and soft matching. We show that separability
systematically increases as metrics impose more stringent alignment
constraints. Among mapping-based approaches, soft-matching achieves the highest
separability, followed by Procrustes alignment and linear predictivity.
Non-fitting methods such as RSA also yield strong separability across families.
These results provide the first systematic comparison of similarity metrics
through a separability lens, clarifying their relative sensitivity and guiding
metric choice for large-scale model and brain comparisons.

</details>


### [12] [Split Conformal Prediction in the Function Space with Neural Operators](https://arxiv.org/abs/2509.04623)
*David Millard,Lars Lindemann,Ali Baheri*

Main category: cs.LG

TL;DR: 该论文将分割共形预测扩展到函数空间，为神经算子提供有限样本覆盖保证，通过离散化映射和渐进收敛方法解决无限维空间中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 神经算子在无限维设置中的不确定性量化是一个开放问题，现有方法（高斯过程、贝叶斯神经网络等）需要强分布假设或产生保守覆盖，缺乏对函数值输出的有限样本保证。

Method: 采用两步法：首先在有限维空间通过输出函数空间的离散化映射建立有限样本覆盖保证，然后通过考虑离散化细化时的渐进收敛将这些保证提升到函数空间。提出回归校正方法在分辨率间传递校准，并引入两个诊断指标。

Result: 经验结果表明，该方法在分辨率变化下保持校准覆盖且变化较小，在超分辨率任务中实现更好的覆盖性能。

Conclusion: 该方法成功将共形预测扩展到函数空间，为神经算子提供了有效的有限样本不确定性量化框架，解决了无限维设置中的覆盖保证问题。

Abstract: Uncertainty quantification for neural operators remains an open problem in
the infinite-dimensional setting due to the lack of finite-sample coverage
guarantees over functional outputs. While conformal prediction offers
finite-sample guarantees in finite-dimensional spaces, it does not directly
extend to function-valued outputs. Existing approaches (Gaussian processes,
Bayesian neural networks, and quantile-based operators) require strong
distributional assumptions or yield conservative coverage. This work extends
split conformal prediction to function spaces following a two step method. We
first establish finite-sample coverage guarantees in a finite-dimensional space
using a discretization map in the output function space. Then these guarantees
are lifted to the function-space by considering the asymptotic convergence as
the discretization is refined. To characterize the effect of resolution, we
decompose the conformal radius into discretization, calibration, and
misspecification components. This decomposition motivates a regression-based
correction to transfer calibration across resolutions. Additionally, we propose
two diagnostic metrics (conformal ensemble score and internal agreement) to
quantify forecast degradation in autoregressive settings. Empirical results
show that our method maintains calibrated coverage with less variation under
resolution shifts and achieves better coverage in super-resolution tasks.

</details>


### [13] [Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction](https://arxiv.org/abs/2509.04631)
*Arash Behboodi,Alvaro H. C. Correia,Fabio Valerio Massoli,Christos Louizos*

Main category: cs.LG

TL;DR: 本文证明了转导式共形预测在置信度和预测集效率之间存在根本性权衡，推导出严格的有限样本界限，显示任何非平凡置信水平都会导致预测集大小随样本数量呈指数增长。


<details>
  <summary>Details</summary>
Motivation: 研究转导式共形预测中置信度与预测集效率之间的基本关系，为多数据点同时预测提供理论保证。

Method: 推导严格的有限样本界限，分析预测集大小与置信水平、样本数量、条件熵和分散度的数学关系，并在理想化设置中验证界限的可达性。

Result: 证明了预测集大小随样本数量呈指数增长，指数与条件熵成正比，界限包含二阶分散度项，并在特殊同标签情况下提供了渐近最优置信预测器。

Conclusion: 转导式共形预测存在置信度与效率的根本权衡，任何有意义的置信保证都需要付出预测集大小指数增长的代价，这一理论界限在实际应用中具有重要意义。

Abstract: Transductive conformal prediction addresses the simultaneous prediction for
multiple data points. Given a desired confidence level, the objective is to
construct a prediction set that includes the true outcomes with the prescribed
confidence. We demonstrate a fundamental trade-off between confidence and
efficiency in transductive methods, where efficiency is measured by the size of
the prediction sets. Specifically, we derive a strict finite-sample bound
showing that any non-trivial confidence level leads to exponential growth in
prediction set size for data with inherent uncertainty. The exponent scales
linearly with the number of samples and is proportional to the conditional
entropy of the data. Additionally, the bound includes a second-order term,
dispersion, defined as the variance of the log conditional probability
distribution. We show that this bound is achievable in an idealized setting.
Finally, we examine a special case of transductive prediction where all test
data points share the same label. We show that this scenario reduces to the
hypothesis testing problem with empirically observed statistics and provide an
asymptotically optimal confidence predictor, along with an analysis of the
error exponent.

</details>


### [14] [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: 本文建立了注意力机制与多项逻辑回归之间的新联系，证明在固定多项回归设置中优化潜在特征会产生与注意力块动态一致的解。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制在Transformer模型中扮演核心角色，但其数学基础以及与特征多义性、叠加和模型性能等概念的关系仍然理解不足。机制可解释性旨在理解现代机器学习模型内部组件如何产生整体行为。

Method: 在固定多项逻辑回归设置中优化潜在特征，分析最优解与注意力块诱导动态的一致性。将Transformer中的表示演化解释为恢复分类最优特征的轨迹。

Result: 证明了优化潜在特征产生的解与注意力机制动态对齐，为理解注意力提供了新的数学框架。

Conclusion: 注意力机制可以被解释为在特征空间中寻找分类最优表示的过程，这为Transformer模型的机制可解释性提供了重要洞见。

Abstract: Mechanistic interpretability aims to understand how internal components of
modern machine learning models, such as weights, activations, and layers, give
rise to the model's overall behavior. One particularly opaque mechanism is
attention: despite its central role in transformer models, its mathematical
underpinnings and relationship to concepts like feature polysemanticity,
superposition, and model performance remain poorly understood. This paper
establishes a novel connection between attention mechanisms and multinomial
regression. Specifically, we show that in a fixed multinomial regression
setting, optimizing over latent features yields optimal solutions that align
with the dynamics induced by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [15] [Flexible inference of learning rules from de novo learning data using neural networks](https://arxiv.org/abs/2509.04661)
*Yuhan Helena Liu,Victor Geadah,Jonathan Pillow*

Main category: cs.LG

TL;DR: 本文提出了一个非参数化框架，使用深度神经网络直接从动物决策数据中推断学习规则，特别针对从头学习任务，能够捕捉次优性、历史依赖性和丰富的外部刺激整合。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法大多假设特定的参数化学习规则或局限于简化设置，而动物需要从头学习新行为，这为学习规则推断提出了丰富挑战。

Method: 提出非参数化框架，使用深度神经网络参数化策略权重的每试次更新，并扩展到循环神经网络变体以捕捉非马尔可夫动态。在模拟中验证方法，并应用于小鼠感官决策任务的大规模行为数据集。

Result: 模型在保留数据上提高了预测性能，推断出的规则揭示了正确与错误试次后的不对称更新和历史依赖性，与非马尔可夫学习一致。

Conclusion: 该框架为从行为数据中推断生物学习规则提供了灵活方法，可为实验训练协议和行为数字孪生开发提供见解。

Abstract: Understanding how animals learn is a central challenge in neuroscience, with
growing relevance to the development of animal- or human-aligned artificial
intelligence. However, most existing approaches assume specific parametric
forms for the learning rule (e.g., Q-learning, policy gradient) or are limited
to simplified settings like bandit tasks, which do not involve learning a new
input-output mapping from scratch. In contrast, animals must often learn new
behaviors de novo, which poses a rich challenge for learning-rule inference. We
target this problem by inferring learning rules directly from animal
decision-making data during de novo task learning, a setting that requires
models flexible enough to capture suboptimality, history dependence, and rich
external stimulus integration without strong structural priors. We first
propose a nonparametric framework that parameterizes the per-trial update of
policy weights with a deep neural network (DNN), and validate it by recovering
ground-truth rules in simulation. We then extend to a recurrent variant (RNN)
that captures non-Markovian dynamics by allowing updates to depend on trial
history. Applied to a large behavioral dataset of mice learning a sensory
decision-making task over multiple weeks, our models improved predictions on
held-out data. The inferred rules revealed asymmetric updates after correct
versus error trials and history dependence, consistent with non-Markovian
learning. Overall, these results introduce a flexible framework for inferring
biological learning rules from behavioral data in de novo learning tasks,
providing insights to inform experimental training protocols and the
development of behavioral digital twins.

</details>


### [16] [Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition](https://arxiv.org/abs/2509.04668)
*Difei Xu,Meng Ding,Zihang Xiang,Jinhui Xu,Di Wang*

Main category: cs.LG

TL;DR: 这篇论文研究在差分隐私模型下的随机凸优化问题，考虑人口风险函数满足Tsybakov噪声条件且损失函数可能非華丽养完全的情况。论文提出了新的差分隐私算法，在高概率下实现了不依赖于Lipschitz常数的质量上界，并给出了相应的下界证明。


<details>
  <summary>Details</summary>
Motivation: 作者想要解决传统差分隐私凸优化中偏差依赖于损失函数Lipschitz常数的问题，考虑更一般的情况（损失函数可能非華丽养完全），并在Tsybakov噪声条件下提供更好的算法性能。

Method: 论文首先考虑了Lipschitz情况下的算法设计，提出了(ε, δ)-差分隐私算法。然后扩展到更一般的情况（θ≥θ̅>1），并在隐私预算小的情况下考虑非Lipschitz损失函数。还给出了相应的下界证明。

Result: 在高概率下实现了质量上界：Õ((r̃_{2k}(1/√n + √d/nε))^{(k-1)/k})^{θ/(θ-1)})，该上界不依赖于Lipschitz常数。对于下界，证明了私有极小化语言下界为Ω((r̃_k(1/√n + √d/n√ρ))^{(k-1)/k})^{θ/(θ-1)})。

Conclusion: 论文在更一般的偏差偏差条件下提供了差分隐私凸优化的新算法和理论分析，解决了传统方法中对Lipschitz常数的依赖性问题，为更广泛的实际应用场景提供了理论支撑。

Abstract: We study Stochastic Convex Optimization in the Differential Privacy model
(DP-SCO). Unlike previous studies, here we assume the population risk function
satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$,
where the Lipschitz constant of the loss could be extremely large or even
unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment
with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an
$(\varepsilon, \delta)$-DP algorithm whose utility bound is
$\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
in high probability, where $n$ is the sample size, $d$ is the model dimension,
and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the
gradient. It is notable that such an upper bound is independent of the
Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$.
Moreover, when the privacy budget $\varepsilon$ is small enough, we show an
upper bound of
$\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
even if the loss function is not Lipschitz. For the lower bound, we show that
for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated
Differential Privacy is lower bounded by
$\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.

</details>


### [17] [Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems](https://arxiv.org/abs/2509.04683)
*Yazdan Babazadeh Maghsoodlo,Madhur Anand,Chris T. Bauch*

Main category: cs.LG

TL;DR: 深度学习模型能够准确识别复杂系统中的闪烁模式（flickering），这是一种重要的早期预警信号，可以预测临界状态转变。


<details>
  <summary>Details</summary>
Motivation: 闪烁现象是气候系统、生态系统、金融市场等复杂系统韧性降低的标志性特征，可能预示着难以预测但影响重大的临界状态转变。然而深度学习在检测这种噪声驱动的状态切换方面的潜力尚未被探索。

Method: 使用卷积长短期记忆（CNN LSTM）模型，在由简单多项式函数加噪声生成的合成时间序列上进行训练。

Result: 尽管在简化的动力学系统上训练，模型能够泛化到各种随机系统，并在经验数据集中可靠地检测到闪烁现象，包括睡鼠体温记录和非洲湿润期的古气候代用指标。

Conclusion: 深度学习能够从噪声非线性时间序列中提取早期预警信号，为识别广泛动态系统中的不稳定性提供了一个灵活框架。

Abstract: Deep learning offers powerful tools for anticipating tipping points in
complex systems, yet its potential for detecting flickering (noise-driven
switching between coexisting stable states) remains unexplored. Flickering is a
hallmark of reduced resilience in climate systems, ecosystems, financial
markets, and other systems. It can precede critical regime shifts that are
highly impactful but difficult to predict. Here we show that convolutional long
short-term memory (CNN LSTM) models, trained on synthetic time series generated
from simple polynomial functions with additive noise, can accurately identify
flickering patterns. Despite being trained on simplified dynamics, our models
generalize to diverse stochastic systems and reliably detect flickering in
empirical datasets, including dormouse body temperature records and
palaeoclimate proxies from the African Humid Period. These findings demonstrate
that deep learning can extract early warning signals from noisy, nonlinear time
series, providing a flexible framework for identifying instability across a
wide range of dynamical systems.

</details>


### [18] [KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation](https://arxiv.org/abs/2509.04684)
*Farnoosh Hashemi,Laks V. S. Lakshmanan*

Main category: cs.LG

TL;DR: KRaft是一个基于学习的知识图谱地图融合方法，通过知识图谱构建、地图匹配和地图合并三个模块，解决了传统地图融合方法无法处理非线性对象和缺乏数据驱动学习的问题。


<details>
  <summary>Details</summary>
Motivation: 传统地图融合方法存在两个主要局限：只能处理线性对象（如道路网络），无法扩展到非线性对象；基于预定义规则的启发式算法，无法以数据驱动方式学习实体匹配。数字地图在导航、车队管理等应用中需要准确性和时效性，但现有地理空间数据库存在区域覆盖不全和实体缺失的问题。

Method: KRaft包含三个部分：1）知识图谱构建 - 将每个地理空间数据库表示为知识图谱；2）地图匹配 - 使用知识图谱对齐方法和地理空间特征编码器匹配知识图谱中的实体；3）地图合并 - 使用混合整数线性规划公式以一致的方式合并匹配的实体，确保完全融合而不产生不一致性。

Result: 实验评估表明，KRaft不仅在地图融合任务中相比最先进方法和基线方法取得了优异性能，而且其各个模块（如地图匹配和地图合并）也分别优于传统的匹配和合并方法。

Conclusion: KRaft通过学习驱动的方法有效解决了地图融合中的关键挑战，能够处理非线性对象并以数据驱动方式实现准确的实体匹配和一致性融合，为地理空间数据库的更新和扩充提供了有效的解决方案。

Abstract: Digital maps play a crucial role in various applications such as navigation,
fleet management, and ride-sharing, necessitating their accuracy and currency,
which require timely updates. While the majority of geospatial databases (GDBs)
provide high-quality information, their data is (i) limited to specific regions
and/or (ii) missing some entities, even in their covered areas. Map conflation
is the process of augmentation of a GDB using another GDB to conflate missing
spatial features. Existing map conflation methods suffer from two main
limitations: (1) They are designed for the conflation of linear objects (e.g.,
road networks) and cannot simply be extended to non-linear objects, thus
missing information about most entities in the map. (2) They are heuristic
algorithmic approaches that are based on pre-defined rules, unable to learn
entities matching in a data-driven manner. To address these limitations, we
design KRAFT, a learning based approach consisting of three parts: (1)
Knowledge Graph Construction - where each GDB is represented by a knowledge
graph, (2) Map Matching - where we use a knowledge graph alignment method as
well as a geospatial feature encoder to match entities in obtained knowledge
graphs, and (3) Map Merging - where we merge matched entities in the previous
modules in a consistent manner, using a mixed integer linear programming
formulation that fully merges the GDBs without adding any inconsistencies. Our
experimental evaluation shows that not only does KRAFT achieve outstanding
performance compared to state-of-the-art and baseline methods in map conflation
tasks, but each of its modules (e.g., Map Matching and Map Merging) also
separately outperforms traditional matching and merging methods.

</details>


### [19] [CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals](https://arxiv.org/abs/2509.04699)
*Wenhui Cui,Christopher Sandino,Hadi Pouransari,Ran Liu,Juri Minxha,Ellen L. Zippi,Aman Verma,Anna Sedlackova,Behrooz Mahasseni,Erdrin Azemi*

Main category: cs.LG

TL;DR: 提出CPEP框架，通过对比学习对齐EMG和姿态表示，提升弱模态数据的表征质量，实现零样本手势分类


<details>
  <summary>Details</summary>
Motivation: 利用低成本生物信号（如表面肌电信号sEMG）实现连续手势识别，但弱模态数据质量较差。通过将其与高质量结构化数据（如姿态数据）对齐，可以提升表征质量并实现零样本分类

Method: 提出对比姿态-EMG预训练（CPEP）框架，学习一个能产生高质量且包含姿态信息的EMG编码器，通过对比学习对齐EMG和姿态表示

Result: 在线性探测和零样本设置下，模型在分布内手势分类上比emg2pose基准模型提升21%，在未见（分布外）手势分类上提升72%

Conclusion: 通过将弱模态数据与高质量结构化数据对齐，可以显著提升表征质量和零样本分类性能，为可穿戴设备上的连续手势识别提供了有效解决方案

Abstract: Hand gesture classification using high-quality structured data such as
videos, images, and hand skeletons is a well-explored problem in computer
vision. Leveraging low-power, cost-effective biosignals, e.g. surface
electromyography (sEMG), allows for continuous gesture prediction on wearables.
In this paper, we demonstrate that learning representations from weak-modality
data that are aligned with those from structured, high-quality data can improve
representation quality and enables zero-shot classification. Specifically, we
propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and
pose representations, where we learn an EMG encoder that produces high-quality
and pose-informative representations. We assess the gesture classification
performance of our model through linear probing and zero-shot setups. Our model
outperforms emg2pose benchmark models by up to 21% on in-distribution gesture
classification and 72% on unseen (out-of-distribution) gesture classification.

</details>


### [20] [Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization](https://arxiv.org/abs/2509.04713)
*Gongyue Zhang,Honghai Liu*

Main category: cs.LG

TL;DR: 该论文提出了自然频谱融合(NSF)方法，将优化器视为频谱控制器，通过p指数扩展和循环调度动态平衡高低频信息，改善优化路径和决策边界对齐。


<details>
  <summary>Details</summary>
Motivation: 研究一阶优化器固有的频率偏好及其对优化路径的影响，传统方法仅关注步长缩放而忽视了频谱信息融合的重要性。

Method: 提出NSF框架：1)将优化器作为频谱控制器；2)周期性重新加权频率带；通过p指数扩展二阶矩项(支持正负指数)和循环调度实现。

Result: 自适应方法强调低频，SGD接近中性，负指数放大高频信息。循环调度拓宽频谱覆盖，改善跨频带融合，在多个基准测试中一致降低测试误差，某些任务仅需1/4训练成本即可达到基线精度。

Conclusion: NSF揭示了优化器作为主动频谱控制器的作用，为一阶优化提供了统一、可控且高效的框架，即使在损失较高时也能提高精度。

Abstract: Spectral behaviors have been widely discussed in machine learning, yet the
optimizer's own spectral bias remains unclear. We argue that first-order
optimizers exhibit an intrinsic frequency preference that significantly
reshapes the optimization path. To address this, we propose Natural Spectral
Fusion (NSF): reframing training as controllable spectral coverage and
information fusion rather than merely scaling step sizes. NSF has two core
principles: treating the optimizer as a spectral controller that dynamically
balances low- and high-frequency information; and periodically reweighting
frequency bands at negligible cost, without modifying the model, data, or
training pipeline. We realize NSF via a p-exponent extension of the
second-moment term, enabling both positive and negative exponents, and
implement it through cyclic scheduling. Theory and experiments show that
adaptive methods emphasize low frequencies, SGD is near-neutral, and negative
exponents amplify high-frequency information. Cyclic scheduling broadens
spectral coverage, improves cross-band fusion, and induces early
decision-boundary alignment, where accuracy improves even while loss remains
high. Across multiple benchmarks, with identical learning-rate strategies and
fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test
error and demonstrates distinct convergence behavior; on some tasks, it matches
baseline accuracy with only one-quarter of the training cost. Overall, NSF
reveals the optimizer's role as an active spectral controller and provides a
unified, controllable, and efficient framework for first-order optimization.

</details>


### [21] [CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction](https://arxiv.org/abs/2509.04733)
*Yuzhu Chen,Yingjie Wang,Shunyu Liu,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: CoVeR是一种基于conformal prediction框架的新型无模型解码策略，能够在保持紧凑搜索空间的同时确保对理想轨迹的高覆盖率概率。


<details>
  <summary>Details</summary>
Motivation: 主流解码策略如beam search虽然能生成合理的候选集，但缺乏可证明的覆盖率保证，且在搜索效率与需要多样化轨迹（特别是长尾序列）之间难以有效平衡。

Method: 提出CoVeR方法，在conformal prediction框架内构建模型无关的解码策略，通过理论分析建立PAC风格的泛化边界。

Result: 理论证明CoVeR能够渐近地达到至少1-α的覆盖率，其中α∈(0,1)是任意目标水平。

Conclusion: CoVeR为解决解码策略的覆盖率保证问题提供了理论保障，特别适用于需要处理长尾序列的现实应用场景。

Abstract: Autoregressive pre-trained models combined with decoding methods have
achieved impressive performance on complex reasoning tasks. While mainstream
decoding strategies such as beam search can generate plausible candidate sets,
they often lack provable coverage guarantees, and struggle to effectively
balance search efficiency with the need for versatile trajectories,
particularly those involving long-tail sequences that are essential in certain
real-world applications. To address these limitations, we propose
\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal
prediction framework that simultaneously maintains a compact search space and
ensures high coverage probability over desirable trajectories. Theoretically,
we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR}
asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target
level $\alpha \in (0,1)$.

</details>


### [22] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: Beyond I-Con框架通过探索替代统计散度和相似性核，系统性地发现新的损失函数，在无监督聚类、监督对比学习和降维任务中均取得优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: I-Con框架发现23种表示学习方法隐含地最小化数据分布与学习分布之间的KL散度，但KL散度的不对称性和无界性可能导致优化问题，且可能与真实目标不一致。

Method: 提出Beyond I-Con框架，探索替代统计散度（如总变差距离、有界f-散度）和相似性核（如距离核替代角度核），系统性地发现新的损失函数。

Result: 在无监督聚类DINO-ViT嵌入中通过TV距离改进PMI算法达到SOTA；监督对比学习中TV距离+距离核优于标准方法；降维任务中有界f-散度比SNE的KL散度表现更好。

Conclusion: 统计散度和相似性核的选择对表示学习优化至关重要，Beyond I-Con框架为系统探索这些选择提供了有效途径。

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


### [23] [VARMA-Enhanced Transformer for Time Series Forecasting](https://arxiv.org/abs/2509.04782)
*Jiajun Song,Xiaoou Liu*

Main category: cs.LG

TL;DR: VARMAformer是一个新颖的时间序列预测架构，通过将经典VARMA统计模型的原理与cross-attention-only框架相结合，在保持高效的同时显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的简化Transformer架构（如CATS）虽然高效，但可能忽略了经典统计模型（如VARMA）能够有效捕捉的细粒度局部时间依赖性。

Method: 提出两个关键创新：1）VARMA启发的特征提取器（VFE）在patch级别显式建模自回归和移动平均模式；2）VARMA增强注意力机制（VE-atten）使用时序门使查询更具上下文感知能力。

Result: 在广泛使用的基准数据集上的实验表明，该模型始终优于现有的最先进方法。

Conclusion: 这项工作验证了将经典统计洞察融入现代深度学习框架对时间序列预测的显著益处。

Abstract: Transformer-based models have significantly advanced time series forecasting.
Recent work, like the Cross-Attention-only Time Series transformer (CATS),
shows that removing self-attention can make the model more accurate and
efficient. However, these streamlined architectures may overlook the
fine-grained, local temporal dependencies effectively captured by classical
statistical models like Vector AutoRegressive Moving Average model (VARMA). To
address this gap, we propose VARMAformer, a novel architecture that synergizes
the efficiency of a cross-attention-only framework with the principles of
classical time series analysis. Our model introduces two key innovations: (1) a
dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models
autoregressive (AR) and moving-average (MA) patterns at the patch level, and
(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal
gate to make queries more context-aware. By fusing these classical insights
into a modern backbone, VARMAformer captures both global, long-range
dependencies and local, statistical structures. Through extensive experiments
on widely-used benchmark datasets, we demonstrate that our model consistently
outperforms existing state-of-the-art methods. Our work validates the
significant benefit of integrating classical statistical insights into modern
deep learning frameworks for time series forecasting.

</details>


### [24] [Graph Unlearning: Efficient Node Removal in Graph Neural Networks](https://arxiv.org/abs/2509.04785)
*Faqian Guan,Tianqing Zhu,Zhoutian Wang,Wei Ren,Wanlei Zhou*

Main category: cs.LG

TL;DR: 本文提出三种新的节点删除方法，通过利用图的拓扑特征来提高GNN模型中敏感训练数据的删除效率和隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 现有节点删除方法存在限制GNN结构、未充分利用图拓扑特征、缺乏良好的性能-复杂度平衡等问题，需要提高敏感数据删除的效率和效果。

Method: 提出三种新方法：基于类别的标签替换、拓扑导向的邻居均值后验概率、类一致的邻居节点筛选。后两种方法重点利用了图的拓扑特征。

Result: 在三个标准数据集上进行实验，从模型效用性、删除效用性和删除效率三个指标评估。实验结果显示新方法在效用性和效率方面都超过了现有最优方法。

Conclusion: 提出的方法能够高效删除敏感训练节点并保护GNN模型中的隐私信息，为节点删除领域提供了有价值的见解，有助于提高GNN模型的隐私安全性。

Abstract: With increasing concerns about privacy attacks and potential sensitive
information leakage, researchers have actively explored methods to efficiently
remove sensitive training data and reduce privacy risks in graph neural network
(GNN) models. Node unlearning has emerged as a promising technique for
protecting the privacy of sensitive nodes by efficiently removing specific
training node information from GNN models. However, existing node unlearning
methods either impose restrictions on the GNN structure or do not effectively
utilize the graph topology for node unlearning. Some methods even compromise
the graph's topology, making it challenging to achieve a satisfactory
performance-complexity trade-off. To address these issues and achieve efficient
unlearning for training node removal in GNNs, we propose three novel node
unlearning methods: Class-based Label Replacement, Topology-guided Neighbor
Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among
these methods, Topology-guided Neighbor Mean Posterior Probability and
Class-consistent Neighbor Node Filtering effectively leverage the topological
features of the graph, resulting in more effective node unlearning. To validate
the superiority of our proposed methods in node unlearning, we conducted
experiments on three benchmark datasets. The evaluation criteria included model
utility, unlearning utility, and unlearning efficiency. The experimental
results demonstrate the utility and efficiency of the proposed methods and
illustrate their superiority compared to state-of-the-art node unlearning
methods. Overall, the proposed methods efficiently remove sensitive training
nodes and protect the privacy information of sensitive nodes in GNNs. The
findings contribute to enhancing the privacy and security of GNN models and
provide valuable insights into the field of node unlearning.

</details>


### [25] [An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning](https://arxiv.org/abs/2509.04815)
*Wonseo Jang,Dongjae Kim*

Main category: cs.LG

TL;DR: 提出ACED-DQN框架，通过仲裁控制机制管理多样化DQN变体集合，解决深度强化学习在持续学习中的灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在静态环境中学习效率高，但在持续强化学习场景中容易发生灾难性遗忘，导致性能下降。受人类前额叶皮层决策机制启发，需要设计类似人类的多智能体仲裁控制机制

Method: 1) 训练具有多样化价值函数的DQN变体集合 2) 设计仲裁控制机制，优先选择最近试验中错误率较低（可靠性更高）的智能体

Result: 在静态和持续环境中都表现出显著的性能提升，实证证据表明仲裁控制对多样化DQN的训练效果有效

Conclusion: 从人脑机制获得灵感，提出了一个使强化学习智能体能够持续学习的框架，成功解决了灾难性遗忘问题

Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning
an optimal policy in static environments, easily loses previously learned
knowledge (i.e., catastrophic forgetting). It leads RL models to poor
performance in continual reinforcement learning (CRL) scenarios. To address
this, we present an arbitration control mechanism over an ensemble of RL
agents. It is motivated by and closely aligned with how humans make decisions
in a CRL context using an arbitration control of multiple RL agents in parallel
as observed in the prefrontal cortex. We integrated two key ideas into our
model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have
diverse value functions and (2) an arbitration control that prioritizes agents
with higher reliability (i.e., less error) in recent trials. We propose a
framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN
variants (ACED-DQN). We demonstrate significant performance improvements in
both static and continual environments, supported by empirical evidence showing
the effectiveness of arbitration control over diversified DQNs during training.
In this work, we introduced a framework that enables RL agents to continuously
learn, with inspiration from the human brain.

</details>


### [26] [Revolution or Hype? Seeking the Limits of Large Models in Hardware Design](https://arxiv.org/abs/2509.04905)
*Qiang Xu,Leon Stok,Rolf Drechsler,Xi Wang,Grace Li Zhang,Igor L. Markov*

Main category: cs.LG

TL;DR: 这篇论文作为ICCAD 2025会议的基础文本，批判性分析大型AI模型在硬件设计中的实际能力、基本限制和未来前景，提供了关于这一有争议且影响深远的技术趋势的权威视角。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型(LLMs)和大型电路模型(LCMs)是否真正能够带来电子设计自动化(EDA)领域的革呼，还是仅仅是一种瞬间的热湯和过度期望。

Method: 聚集来自学术界和产业界领军专家的观点，批判性审视大型AI模型的实际能力和基本限制，综合分析关于可靠性、可扩展性和可解释性的核心论点。

Result: 提供了一个权威的概述，并展开了关于这些模型能否意义地超过或补充传统EDA方法的辨论。

Conclusion: 这篇论文为讨论大型AI模型在硬件设计中的真正价值和限制提供了基础框架，帮助识别这一技术趋势的真实潜力和挑战。

Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models
(LCMs) have sparked excitement across the electronic design automation (EDA)
community, promising a revolution in circuit design and optimization. Yet, this
excitement is met with significant skepticism: Are these AI models a genuine
revolution in circuit design, or a temporary wave of inflated expectations?
This paper serves as a foundational text for the corresponding ICCAD 2025
panel, bringing together perspectives from leading experts in academia and
industry. It critically examines the practical capabilities, fundamental
limitations, and future prospects of large AI models in hardware design. The
paper synthesizes the core arguments surrounding reliability, scalability, and
interpretability, framing the debate on whether these models can meaningfully
outperform or complement traditional EDA methods. The result is an
authoritative overview offering fresh insights into one of today's most
contentious and impactful technology trends.

</details>


### [27] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: 通过生成人造混涌时间序列和重采样技术模拟金融时间序列，进行大规模预训练，并在比特币数据上进行零样本预测，交易策略表现显著超过自相关模型


<details>
  <summary>Details</summary>
Motivation: 金融工具收益率预测是具有挑战性的问题，实际世界时间序列展现混涌特性，需要开发适用于各种预测任务的时间序列基础模型

Method: 使用人造混涌时间序列和重采样技术模拟金融时间序列数据，逐渐增加重采间隔来扩展预测范围，进行大规模预训练（每个案例100亿训练样本）

Result: 在比特币实际交易数据上进行零样本预测，基于预测结果的简单交易策略盛利能力显著超过自相关模型，还观察到类似缩放定律的现象

Conclusion: 通过投入大量计算资源扩大训练样本数量，可能实现对混涌时间序列远期预测，未来应继续进行大规模训练并验证这个缩放定律在各种混涌模型上的适用性

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [28] [A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection](https://arxiv.org/abs/2509.04925)
*Jiale Zhang,Pengfei He,Fei Li,Kewei Li,Yan Wang,Lan Huang,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: TrailGate是一个结合机器学习和深度学习的新型网络入侵检测框架，通过Transformer和BiGRU架构、高级特征选择策略和数据增强技术，能够有效检测常见攻击类型和新兴威胁。


<details>
  <summary>Details</summary>
Motivation: 当前网络流量数据激增，传统机器学习方法难以处理复杂模式、数据稀缺和类别不平衡问题，需要更强大的入侵检测解决方案。

Method: 集成Transformer和双向门控循环单元(BiGRU)架构，结合高级特征选择策略和数据增强技术，构建TrailGate框架。

Result: 该框架能够识别常见攻击类型，并在检测和缓解新兴威胁方面表现优异，特别是能够快速识别和中和源自现有范式的新威胁。

Conclusion: TrailGate通过机器学习和深度学习的融合，为网络入侵检测提供了更强大和精确的解决方案，有效解决了数据稀缺和类别不平衡的挑战。

Abstract: In today's fast-paced digital communication, the surge in network traffic
data and frequency demands robust and precise network intrusion solutions.
Conventional machine learning methods struggle to grapple with complex patterns
within the vast network intrusion datasets, which suffer from data scarcity and
class imbalance. As a result, we have integrated machine learning and deep
learning techniques within the network intrusion detection system to bridge
this gap. This study has developed TrailGate, a novel framework that combines
machine learning and deep learning techniques. By integrating Transformer and
Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature
selection strategies and supplemented by data augmentation techniques,
TrailGate can identifies common attack types and excels at detecting and
mitigating emerging threats. This algorithmic fusion excels at detecting common
and well-understood attack types and has the unique ability to swiftly identify
and neutralize emerging threats that stem from existing paradigms.

</details>


### [29] [Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics](https://arxiv.org/abs/2509.04942)
*Heinke Hihn,Dennis A. V. Dittrich,Carl Jeske,Cayo Costa Sobral,Helio Pais,Timm Lochmann*

Main category: cs.LG

TL;DR: 使用Sentence-BERT模型学习共享语义空间，通过嵌入对齐技术将弗式德国职业标题与标准本体进行关联，实现高效近似最近邻搜索分类。


<details>
  <summary>Details</summary>
Motivation: 解决不同来源职业数据跨源推理的长期瓶颈，避免人工维护的手工本体组织方式的高成本和计算开销。

Method: 利用公开的德国联邦就业局数据构建数据集，细调Sentence-BERT模型学习本体结构，通过嵌入对齐技术建立相似性图结构，并使用近似最近邻搜索将分类问题框架为语义搜索问题。

Result: 实现了将任意弗式德国职业标题与KldB和ISCED标准本体的自动关联，提供了更高效、可扩展的解决方案。

Conclusion: 语言处理机器学习模型为职业数据跨源推理提供了可扩展的替代方案，未来将扩展到多语言标题和更多本体。

Abstract: The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.

</details>


### [30] [Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective](https://arxiv.org/abs/2509.04951)
*Artem Lensky,Yiding Qiu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted
artifacts. However, recent studies have demonstrated that blink rate and its
variability are important physiological markers to monitor cognitive load,
attention, and potential neurological disorders. This paper addresses the
critical task of accurate blink detection by evaluating various deep learning
models for segmenting EEG signals into involuntary blinks and non-blinks. We
present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes.
The problem is formulated as a sequence-to-sequence task and tested on various
deep learning architectures including standard recurrent neural networks,
convolutional neural networks (both standard and depth-wise), temporal
convolutional networks (TCN), transformer-based models, and hybrid
architectures. The models were trained on raw EEG signals with minimal
pre-processing. Training and testing was carried out on a public dataset of 31
subjects collected at UCSD. This dataset consisted of 15 healthy participants
and 16 patients with Parkinson's disease allowing us to verify the model's
robustness to tremor. Out of all models, CNN-RNN hybrid model consistently
outperformed other models and achieved the best blink detection accuracy of
93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and
correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares
neural networks for the task of segmenting EEG recordings to involuntary blinks
and no blinks allowing for computing blink rate and other statistics.

</details>


### [31] [On the Normalization of Confusion Matrices: Methods and Geometric Interpretations](https://arxiv.org/abs/2509.04959)
*Johan Erbani,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Sonia Ben Mokhtar,Diana Nurbakova*

Main category: cs.LG

TL;DR: 提出使用双随机归一化方法分离混淆矩阵中的类别相似性和分布偏差因素，提供更准确的模型诊断


<details>
  <summary>Details</summary>
Motivation: 混淆矩阵值同时受类别相似性（模型混淆程度）和分布偏差（训练测试集分布不均衡）影响，难以区分两者的独立贡献

Method: 引入基于迭代比例拟合的双随机归一化方法，这是行和列归一化的推广，能够恢复类别相似性的底层结构

Result: 该方法能够分离误差来源，支持更精准的模型行为诊断和针对性改进，并在模型内部类别表示空间中提供几何解释

Conclusion: 双随机归一化为混淆矩阵分析提供了更深入的理解框架，揭示了归一化对分类器内部表示的几何意义

Abstract: The confusion matrix is a standard tool for evaluating classifiers by
providing insights into class-level errors. In heterogeneous settings, its
values are shaped by two main factors: class similarity -- how easily the model
confuses two classes -- and distribution bias, arising from skewed
distributions in the training and test sets. However, confusion matrix values
reflect a mix of both factors, making it difficult to disentangle their
individual contributions. To address this, we introduce bistochastic
normalization using Iterative Proportional Fitting, a generalization of row and
column normalization. Unlike standard normalizations, this method recovers the
underlying structure of class similarity. By disentangling error sources, it
enables more accurate diagnosis of model behavior and supports more targeted
improvements. We also show a correspondence between confusion matrix
normalizations and the model's internal class representations. Both standard
and bistochastic normalizations can be interpreted geometrically in this space,
offering a deeper understanding of what normalization reveals about a
classifier.

</details>


### [32] [Neuro-Spectral Architectures for Causal Physics-Informed Networks](https://arxiv.org/abs/2509.04966)
*Arthur Bizzi,Leonardo M. Moreira,Márcio Marques,Leonardo Mendonça,Christian Júnior de Oliveira,Vitor Balestro,Lucas dos Santos Fernandez,Daniel Yukimura,Pavel Petrov,João M. Pereira,Tiago Novello,Lucas Nissenbaum*

Main category: cs.LG

TL;DR: NeuSA是一种基于谱方法的物理信息神经网络，通过谱基投影和神经ODE集成来解决PINNs在复杂初值问题中的收敛困难、因果性违反和谱偏差问题。


<details>
  <summary>Details</summary>
Motivation: 标准MLP-based PINNs在处理复杂初值问题时经常无法收敛，导致解违反因果性并存在对低频分量的谱偏差。

Method: NeuSA学习PDE在谱基上的投影，获得动力学的有限维表示，然后与适应的神经ODE集成，利用谱表示的高频分量克服谱偏差，继承NODE的因果结构，并通过基于经典方法的初始化方案在目标解附近开始训练。

Result: 在线性和非线性波动方程的基准测试中，NeuSA相比其他架构表现出更强的性能，具有更快的收敛速度、改进的时间一致性和优越的预测精度。

Conclusion: NeuSA通过结合谱方法和神经ODE，有效解决了PINNs的收敛问题和谱偏差，为复杂PDE求解提供了新的神经架构解决方案。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural
framework for solving partial differential equations (PDEs). However, standard
MLP-based PINNs often fail to converge when dealing with complex initial-value
problems, leading to solutions that violate causality and suffer from a
spectral bias towards low-frequency components. To address these issues, we
introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired
by classical spectral methods, designed to solve linear and nonlinear PDEs with
variable coefficients. NeuSA learns a projection of the underlying PDE onto a
spectral basis, leading to a finite-dimensional representation of the dynamics
which is then integrated with an adapted Neural ODE (NODE). This allows us to
overcome spectral bias, by leveraging the high-frequency components enabled by
the spectral representation; to enforce causality, by inheriting the causal
structure of NODEs, and to start training near the target solution, by means of
an initialization scheme based on classical methods. We validate NeuSA on
canonical benchmarks for linear and nonlinear wave equations, demonstrating
strong performance as compared to other architectures, with faster convergence,
improved temporal consistency and superior predictive accuracy. Code and
pretrained models will be released.

</details>


### [33] [Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks](https://arxiv.org/abs/2509.04973)
*Yuxi Wang,Heyao Liu,Guanzi Yao,Nyutian Long,Yue Kang*

Main category: cs.LG

TL;DR: 提出拓扑感知图强化学习方法优化云服务器路由策略，通过结构感知状态编码和策略自适应图更新机制，在动态拓扑下实现高效稳定的路由决策


<details>
  <summary>Details</summary>
Motivation: 解决云服务器环境中路由策略优化问题，应对动态拓扑下的决策不稳定和结构感知不足的挑战

Method: 集成SASE模块（多层图卷积和结构位置嵌入建模节点状态）和PAGU模块（基于策略行为变化和奖励反馈调整图结构），构建统一的状态表示和结构演化框架

Result: 在GEANT拓扑数据集上实验表明，该方法在吞吐量、延迟控制和链路平衡等多个性能指标上优于现有图强化学习模型

Conclusion: 该方法能够实现动态复杂云网络中高效且鲁棒的路由优化，结构建模和自适应图更新对模型稳定性和决策质量有重要影响

Abstract: This paper proposes a topology-aware graph reinforcement learning approach to
address the routing policy optimization problem in cloud server environments.
The method builds a unified framework for state representation and structural
evolution by integrating a Structure-Aware State Encoding (SASE) module and a
Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges
of decision instability and insufficient structural awareness under dynamic
topologies. The SASE module models node states through multi-layer graph
convolution and structural positional embeddings, capturing high-order
dependencies in the communication topology and enhancing the expressiveness of
state representations. The PAGU module adjusts the graph structure based on
policy behavior shifts and reward feedback, enabling adaptive structural
updates in dynamic environments. Experiments are conducted on the real-world
GEANT topology dataset, where the model is systematically evaluated against
several representative baselines in terms of throughput, latency control, and
link balance. Additional experiments, including hyperparameter sensitivity,
graph sparsity perturbation, and node feature dimensionality variation, further
explore the impact of structure modeling and graph updates on model stability
and decision quality. Results show that the proposed method outperforms
existing graph reinforcement learning models across multiple performance
metrics, achieving efficient and robust routing in dynamic and complex cloud
networks.

</details>


### [34] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: 本文提出SAR和SAR²方法来解决测试时自适应(TTA)在混合分布偏移、小批量数据和在线不平衡标签分布等场景下的不稳定性问题，通过梯度感知和表示正则化来防止模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 测试时自适应(TTA)在真实部署环境中面临混合分布偏移、小批量数据和在线不平衡标签分布等挑战，导致性能下降甚至模型崩溃，这阻碍了TTA方法在实际应用中的部署。

Method: 1) 提出SAR方法：使用锐度感知和可靠熵最小化，移除梯度大的噪声样本并鼓励模型权重达到平坦最小值；2) 提出SAR²方法：增加冗余正则器和不平等正则器，分别减少特征维度间相关性和惩罚偏向特定类的表示。

Result: 实验结果表明，所提方法在多种野生测试场景下比现有方法表现更稳定，且计算效率高。

Conclusion: 通过分析批归一化层对TTA稳定性的影响，并提出梯度感知和表示正则化技术，有效解决了TTA在复杂测试环境中的不稳定性问题，为实际部署提供了可行的解决方案。

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [35] [Directed Evolution of Proteins via Bayesian Optimization in Embedding Space](https://arxiv.org/abs/2509.04998)
*Matouš Soldát,Jiří Kléma*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯优化和预训练蛋白质语言模型的新方法，用于机器学习辅助的定向进化，显著提高了蛋白质功能优化的效率


<details>
  <summary>Details</summary>
Motivation: 定向进化是一个耗时耗钱的实验室过程，机器学习方法可以帮助选择有信息量的蛋白质变体进行筛选，提高筛选质量和减少必要的筛选量

Method: 结合贝叶斯优化与预训练蛋白质语言模型提取的蛋白质变体信息表示，利用序列嵌入作为新的表示方法

Result: 基于序列嵌入的新表示显著提高了贝叶斯优化的性能，在相同筛选次数下获得更好结果，且优于现有最先进的回归目标方法

Conclusion: 该方法为机器学习辅助的定向进化提供了有效的解决方案，通过蛋白质语言模型的表示学习显著提升了蛋白质功能优化的效率

Abstract: Directed evolution is an iterative laboratory process of designing proteins
with improved function by iteratively synthesizing new protein variants and
evaluating their desired property with expensive and time-consuming biochemical
screening. Machine learning methods can help select informative or promising
variants for screening to increase their quality and reduce the amount of
necessary screening. In this paper, we present a novel method for
machine-learning-assisted directed evolution of proteins which combines
Bayesian optimization with informative representation of protein variants
extracted from a pre-trained protein language model. We demonstrate that the
new representation based on the sequence embeddings significantly improves the
performance of Bayesian optimization yielding better results with the same
number of conducted screening in total. At the same time, our method
outperforms the state-of-the-art machine-learning-assisted directed evolution
methods with regression objective.

</details>


### [36] [Depth-Aware Initialization for Stable and Efficient Neural Network Training](https://arxiv.org/abs/2509.05018)
*Vijay Pandey*

Main category: cs.LG

TL;DR: 本文综述了深度网络初始化方法，提出了一种新的初始化方案，通过考虑每个层的深度信息来灵活增加网络方差，在深度网络中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法在深度网络中表现不佳，理论假设的单位方差在深度网络中并不有效，需要从第一层到最后一层渐进地增加方差。

Method: 提出了一种新的初始化方案，结合每个层的深度信息和整个网络的深度，以灵活的方式增加网络方差。

Result: 实验结果显示，提出的方法在深度网络中表现超过了现有的初始化方案。

Conclusion: 通过考虑每个层的深度信息来灵活控制网络方差的增长，可以提高深度网络的初始化效果，该方法比现有方法更有效。

Abstract: In past few years, various initialization schemes have been proposed. These
schemes are glorot initialization, He initialization, initialization using
orthogonal matrix, random walk method for initialization. Some of these methods
stress on keeping unit variance of activation and gradient propagation through
the network layer. Few of these methods are independent of the depth
information while some methods has considered the total network depth for
better initialization. In this paper, comprehensive study has been done where
depth information of each layer as well as total network is incorporated for
better initialization scheme. It has also been studied that for deeper networks
theoretical assumption of unit variance throughout the network does not perform
well. It requires the need to increase the variance of the network from first
layer activation to last layer activation. We proposed a novel way to increase
the variance of the network in flexible manner, which incorporates the
information of each layer depth. Experiments shows that proposed method
performs better than the existing initialization scheme.

</details>


### [37] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: MultiSurv是一个多模态深度生存模型，使用DeepHit架构结合投影层和跨模态注意力机制，整合临床、MRI、RNA-seq和病理学数据，用于前列腺癌和膀胱癌的复发时间预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测癌症患者的时间-事件结果对治疗规划和患者管理至关重要，需要整合多模态异质数据来捕捉互补的预后信号。

Method: 采用DeepHit生存模型，加入投影层和跨模态注意力机制，整合临床、MRI、RNA-seq和全切片病理学特征等多模态数据。

Result: 在前列腺癌生化复发预测任务中，5折交叉验证C-index为0.843，开发集为0.818；在膀胱癌复发预测任务中，5折交叉验证C-index为0.662，开发集为0.457。

Conclusion: 多模态整合与深度生存学习相结合为前列腺癌和膀胱癌的个性化风险分层提供了有前景的途径，该框架可广泛应用于涉及异质生物医学数据的生存预测任务。

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [38] [Recurrent State Encoders for Efficient Neural Combinatorial Optimization](https://arxiv.org/abs/2509.05084)
*Tim Dernedde,Daniela Thyssens,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: 通过推出重复编码器来重用之前计算的状态嵌入，在神经组合优化中实现更高效的解构建方法，在保持性能的同时显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 观察到神经组合优化中状态变化通常很小，可以重用之前步骤的计算来提高效率

Method: 训练一个重复编码器，计算状态嵌入时不仅基于当前状态，还基于之前步骤的嵌入

Result: 重复编码器可以在层数减少3倍的情况下达到相等或更好的性能，显著提高了延迟性能

Conclusion: 重复编码器方法在TSP、CVRP和OP等问题上都表现出实用价值，可以集成到大邻域搜索算法中

Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are
construction methods, where a neural network is trained to sequentially add one
solution component at a time until a complete solution is constructed. We
observe that the typical changes to the state between two steps are small,
since usually only the node that gets added to the solution is removed from the
state. An efficient model should be able to reuse computation done in prior
steps. To that end, we propose to train a recurrent encoder that computes the
state embeddings not only based on the state but also the embeddings of the
step before. We show that the recurrent encoder can achieve equivalent or
better performance than a non-recurrent encoder even if it consists of
$3\times$ fewer layers, thus significantly improving on latency. We demonstrate
our findings on three different problems: the Traveling Salesman Problem (TSP),
the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem
(OP) and integrate the models into a large neighborhood search algorithm, to
showcase the practical relevance of our findings.

</details>


### [39] [HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions](https://arxiv.org/abs/2509.05117)
*Rafael Bischof,Michal Piovarči,Michael A. Kraus,Siddhartha Mishra,Bernd Bickel*

Main category: cs.LG

TL;DR: HyPINO是一个多物理神经算子，通过超网络和混合监督实现参数化PDE的零样本泛化，无需任务特定微调，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理信息神经网络(PINNs)需要针对每个PDE问题单独训练的问题，开发一个能够零样本泛化到各类参数化PDE的统一框架，减少计算成本并提高准确性。

Method: 结合Swin Transformer超网络和混合监督：使用MMS生成的解析解作为标签数据，以及物理信息目标优化的无标签样本。模型将PDE参数映射到目标PINNs，并引入迭代精炼过程生成集成解。

Result: 在7个PINN文献基准问题上实现强零样本准确性，优于U-Nets、Poseidon和PINO。迭代精炼过程在6个基准上实现误差逐步降低，最佳情况下平均L2损失提升100倍以上。微调后的PINNs收敛更快且误差更低。

Conclusion: HyPINO展示了作为扩展神经算子解决复杂非线性高维PDE问题的基础方法的潜力，具有显著提高的准确性和降低的计算成本。

Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot
generalization across a broad class of parametric PDEs without requiring
task-specific fine-tuning. Our approach combines a Swin Transformer-based
hypernetwork with mixed supervision: (i) labeled data from analytical solutions
generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled
samples optimized using physics-informed objectives. The model maps PDE
parametrizations to target Physics-Informed Neural Networks (PINNs) and can
handle linear elliptic, hyperbolic, and parabolic equations in two dimensions
with varying source terms, geometries, and mixed Dirichlet/Neumann boundary
conditions, including interior boundaries. HyPINO achieves strong zero-shot
accuracy on seven benchmark problems from PINN literature, outperforming
U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we
introduce an iterative refinement procedure that compares the physics of the
generated PINN to the requested PDE and uses the discrepancy to generate a
"delta" PINN. Summing their contributions and repeating this process forms an
ensemble whose combined solution progressively reduces the error on six
benchmarks and achieves over 100x gain in average $L_2$ loss in the best case,
while retaining forward-only inference. Additionally, we evaluate the
fine-tuning behavior of PINNs initialized by HyPINO and show that they converge
faster and to lower final error than both randomly initialized and
Reptile-meta-learned PINNs on five benchmarks, performing on par on the
remaining two. Our results highlight the potential of this scalable approach as
a foundation for extending neural operators toward solving increasingly
complex, nonlinear, and high-dimensional PDE problems with significantly
improved accuracy and reduced computational cost.

</details>


### [40] [Should We Always Train Models on Fine-Grained Classes?](https://arxiv.org/abs/2509.05130)
*Davide Pirovano,Federico Milanesio,Michele Caselle,Piero Fariselli,Matteo Osella*

Main category: cs.LG

TL;DR: 细粒度标签训练并不总能提升分类性能，其效果取决于数据几何结构、标签层次关系、数据集大小和模型容量等因素


<details>
  <summary>Details</summary>
Motivation: 许多数据集的类标签具有层次结构，虽然分类任务通常定义在特定层次，但训练时可以使用更细粒度的标签。现有经验表明细粒度训练可能提升性能，但缺乏系统性的研究

Method: 使用真实和合成数据集，研究细粒度标签训练对分类准确性的影响，分析数据几何结构、标签层次关系、数据集大小和模型容量等因素的作用

Result: 细粒度标签训练并非普遍有效，其效果严重依赖于数据几何结构与标签层次的关系。数据集大小和模型容量也是重要影响因素

Conclusion: 细粒度标签训练的效果是有条件的，需要根据具体的数据特征和任务要求来决定是否采用这种策略

Abstract: In classification problems, models must predict a class label based on the
input data features. However, class labels are organized hierarchically in many
datasets. While a classification task is often defined at a specific level of
this hierarchy, training can utilize a finer granularity of labels. Empirical
evidence suggests that such fine-grained training can enhance performance. In
this work, we investigate the generality of this observation and explore its
underlying causes using both real and synthetic datasets. We show that training
on fine-grained labels does not universally improve classification accuracy.
Instead, the effectiveness of this strategy depends critically on the geometric
structure of the data and its relations with the label hierarchy. Additionally,
factors such as dataset size and model capacity significantly influence whether
fine-grained labels provide a performance benefit.

</details>


### [41] [On the Learnability of Distribution Classes with Adaptive Adversaries](https://arxiv.org/abs/2509.05137)
*Tosca Lechner,Alex Bie,Gautam Kamath*

Main category: cs.LG

TL;DR: 本文研究了在自适应对抗者存在下的分布类可学习性问题，证明了相对于加性自适应对抗者的可学习性比相对于加性无意识对抗者的可学习性条件更严格


<details>
  <summary>Details</summary>
Motivation: 探讨在自适应对抗者（能够拦截学习者请求的样本并应用操作）存在下的学习能力问题，与只能修改底层分布的无意识对抗者形成对比

Method: 提出了一个针对自适应对抗者的通用可学习性概念，考虑了对抗者的预算约束，并进行了理论分析

Result: 证明了相对于加性自适应对抗者的可学习性是一个比相对于加性无意识对抗者的可学习性更强的条件

Conclusion: 自适应对抗者的存在显著增加了学习问题的难度，需要更强的学习条件才能保证可学习性

Abstract: We consider the question of learnability of distribution classes in the
presence of adaptive adversaries -- that is, adversaries capable of
intercepting the samples requested by a learner and applying manipulations with
full knowledge of the samples before passing it on to the learner. This stands
in contrast to oblivious adversaries, who can only modify the underlying
distribution the samples come from but not their i.i.d.\ nature. We formulate a
general notion of learnability with respect to adaptive adversaries, taking
into account the budget of the adversary. We show that learnability with
respect to additive adaptive adversaries is a strictly stronger condition than
learnability with respect to additive oblivious adversaries.

</details>


### [42] [Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights](https://arxiv.org/abs/2509.05142)
*Cosmin-Andrei Hatfaludi,Alex Serban*

Main category: cs.LG

TL;DR: 这篇论文是一个关于联邦学习和基础模型融合的综述性研究，通过新的分类法对技术方法进行分类、对比和特征化，并以医疗健康领域为案例研究提供实践指南。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的普及和私有数据集成需求增长，需要探索联邦学习与基础模型的融合方法，以解决数据异地分布和隐私保护问题。

Method: 采用文献调研方法，从4200多篇文竤中筛选出250篇详细审查，识别出42种独特方法，构建以发展生命周期阶段为基础的分类法，并进行复杂性、效率和可扩展性的技术对比。

Result: 提供了一个自含式的领域概览，识别和对比了多种技术方法，包括联邦学习、自监督学习、微调、蓄粉和迁移学习等相互交叉主题，并为实践应用提供了指南。

Conclusion: 该研究不仅总结了联邦学习与基础模型融合领域的现状，还为方法的采用、发展和集成提供了实践见解，尤其在医疗健康领域具有重要意义。

Abstract: Federated learning has the potential to unlock siloed data and distributed
resources by enabling collaborative model training without sharing private
data. As more complex foundational models gain widespread use, the need to
expand training resources and integrate privately owned data grows as well. In
this article, we explore the intersection of federated learning and
foundational models, aiming to identify, categorize, and characterize technical
methods that integrate the two paradigms. As a unified survey is currently
unavailable, we present a literature survey structured around a novel taxonomy
that follows the development life-cycle stages, along with a technical
comparison of available methods. Additionally, we provide practical insights
and guidelines for implementing and evolving these methods, with a specific
focus on the healthcare domain as a case study, where the potential impact of
federated learning and foundational models is considered significant. Our
survey covers multiple intersecting topics, including but not limited to
federated learning, self-supervised learning, fine-tuning, distillation, and
transfer learning. Initially, we retrieved and reviewed a set of over 4,200
articles. This collection was narrowed to more than 250 thoroughly reviewed
articles through inclusion criteria, featuring 42 unique methods. The methods
were used to construct the taxonomy and enabled their comparison based on
complexity, efficiency, and scalability. We present these results as a
self-contained overview that not only summarizes the state of the field but
also provides insights into the practical aspects of adopting, evolving, and
integrating foundational models with federated learning.

</details>


### [43] [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://arxiv.org/abs/2509.05165)
*Dmitry Akulov,Mohamed Sana,Antonio De Domenico,Tareq Si Salem,Nicola Piovesan,Fadhel Ayed*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient
autoregressive decoding; however, cache size grows linearly with context length
and model depth, becoming a major bottleneck in long-context inference. Prior
KV cache compression methods either enforce rigid heuristics, disrupt tensor
layouts with per-attention-head variability, or require specialized compute
kernels.
  We propose a simple, yet effective, KV cache compression framework based on
attention-guided, layer-adaptive composite tokens. Our method aggregates
attention scores to estimate token importance, selects head-specific tokens
independently, and aligns them into composite tokens that respect the uniform
cache structure required by existing inference engines. A global allocation
mechanism further adapts retention budgets across layers, assigning more
capacity to layers with informative tokens. This approach achieves significant
memory reduction while preserving accuracy, consistently outperforming prior
structured and semi-structured methods. Crucially, our approach remains fully
compatible with standard inference pipelines, offering a practical and scalable
solution for efficient long-context LLM deployment.

</details>


### [44] [Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection](https://arxiv.org/abs/2509.05190)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种轻量级一维CNN模型，通过结构化剪枝和温和早停技术，在减少50%参数和内存的同时，保持了92.87%的准确率和0.8707的macro-F1分数，显著提升了EEG癫痫检测的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生物医学信号处理中表现出色，但其计算资源需求大，在实时检测或资源有限的环境中面临挑战。需要开发轻量高效的模型来满足资源受限环境的需求。

Method: 使用结构化剪枝技术，基于卷积核重要性移除50%的卷积核，结合温和早停策略防止过拟合，构建轻量级一维CNN模型。

Result: 剪枝后模型在减少50%权重和内存的情况下，准确率从92.78%提升至92.87%，macro-F1分数从0.8686提升至0.8707，保持了预测能力。

Conclusion: 结构化剪枝能有效去除冗余、提升泛化能力，结合温和早停技术为资源受限环境下的癫痫检测提供了高效可靠的解决方案。

Abstract: Deep learning models, especially convolutional neural networks (CNNs), have
shown considerable promise for biomedical signals such as EEG-based seizure
detection. However, these models come with challenges, primarily due to their
size and compute requirements in environments where real-time detection or
limited resources are available. In this study, we present a lightweight
one-dimensional CNN model with structured pruning to improve efficiency and
reliability. The model was trained with mild early stopping to address possible
overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.
Structured pruning of the baseline CNN involved removing 50% of the
convolutional kernels based on their importance to model predictions.
Surprisingly, after pruning the weights and memory by 50%, the new network was
still able to maintain predictive capabilities, while modestly increasing
precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we
present a convincing case that structured pruning removes redundancy, improves
generalization, and, in combination with mild early stopping, achieves a
promising way forward to improve seizure detection efficiency and reliability,
which is clear motivation for resource-limited settings.

</details>


### [45] [Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning](https://arxiv.org/abs/2509.05193)
*Bastien Dubail,Stefan Stojanovic,Alexandre Proutière*

Main category: cs.LG

TL;DR: 本文挑战了强化学习中后继度量具有低秩结构的常见假设，发现移位后的后继度量才具有低秩特性，并提出了基于Type II Poincaré不等式的理论分析和有限样本性能保证


<details>
  <summary>Details</summary>
Motivation: 现代强化学习算法通常假设后继度量具有低秩结构，但本文发现这一假设存在问题，需要重新审视低秩结构在RL中的适用性

Method: 提出移位后继度量的概念，使用矩阵补全技术进行低秩近似估计，推导Type II Poincaré不等式来量化谱可恢复性参数

Result: 证明了移位后的后继度量确实具有低秩特性，所需移位量通常很小，且与系统局部混合性质相关，实验验证了在目标条件RL中的性能提升

Conclusion: 移位操作是获得有效低秩近似的关键，Type II Poincaré不等式为分析马尔可夫链提供了新工具，该方法在实际RL任务中表现优异

Abstract: Low-rank structure is a common implicit assumption in many modern
reinforcement learning (RL) algorithms. For instance, reward-free and
goal-conditioned RL methods often presume that the successor measure admits a
low-rank representation. In this work, we challenge this assumption by first
remarking that the successor measure itself is not low-rank. Instead, we
demonstrate that a low-rank structure naturally emerges in the shifted
successor measure, which captures the system dynamics after bypassing a few
initial transitions. We provide finite-sample performance guarantees for the
entry-wise estimation of a low-rank approximation of the shifted successor
measure from sampled entries. Our analysis reveals that both the approximation
and estimation errors are primarily governed by the so-called spectral
recoverability of the corresponding matrix. To bound this parameter, we derive
a new class of functional inequalities for Markov chains that we call Type II
Poincar\'e inequalities and from which we can quantify the amount of shift
needed for effective low-rank approximation and estimation. This analysis shows
in particular that the required shift depends on decay of the high-order
singular values of the shifted successor measure and is hence typically small
in practice. Additionally, we establish a connection between the necessary
shift and the local mixing properties of the underlying dynamical system, which
provides a natural way of selecting the shift. Finally, we validate our
theoretical findings with experiments, and demonstrate that shifting the
successor measure indeed leads to improved performance in goal-conditioned RL.

</details>


### [46] [RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks](https://arxiv.org/abs/2509.05207)
*Arefin Niam,Tevfik Kosar,M S Q Zulkar Nine*

Main category: cs.LG

TL;DR: RapidGNN是一个分布式GNN训练框架，通过确定性采样调度实现高效缓存和远程特征预取，相比基线方法训练吞吐量提升2.46-3倍，远程特征获取减少9.7-15.4倍。


<details>
  <summary>Details</summary>
Motivation: 大规模图数据上分布式训练GNN面临通信开销大的挑战，传统采样方法无法有效解决远程特征获取问题。

Method: 采用确定性采样调度策略，实现高效的缓存构建和远程特征预取机制。

Result: 在基准图数据集上，端到端训练吞吐量平均提升2.46-3倍，远程特征获取减少9.7-15.4倍，计算单元扩展性接近线性，CPU和GPU能效分别提升44%和32%。

Conclusion: RapidGNN有效解决了分布式GNN训练中的通信瓶颈问题，实现了高效的训练性能和良好的可扩展性。

Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of
tasks in exploring structural relationships between entities. However, due to
the highly connected structure of the datasets, distributed training of GNNs on
large-scale graphs poses significant challenges. Traditional sampling-based
approaches mitigate the computational loads, yet the communication overhead
remains a challenge. This paper presents RapidGNN, a distributed GNN training
framework with deterministic sampling-based scheduling to enable efficient
cache construction and prefetching of remote features. Evaluation on benchmark
graph datasets demonstrates RapidGNN's effectiveness across different scales
and topologies. RapidGNN improves end-to-end training throughput by 2.46x to
3.00x on average over baseline methods across the benchmark datasets, while
cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further
demonstrates near-linear scalability with an increasing number of computing
units efficiently. Furthermore, it achieves increased energy efficiency over
the baseline methods for both CPU and GPU by 44% and 32%, respectively.

</details>


### [47] [Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants](https://arxiv.org/abs/2509.05241)
*Lokendra Poudel,David Tincher,Duy-Nhat Phan,Rahul Bhowmik*

Main category: cs.LG

TL;DR: 使用四种LSTM深度学习模型预测胺排放和系统性能参数，准确率超199%，通过因果分析确定关键操作变量对优化胺排放和系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为胺基点源碳捕获系统开发数据驱动的深度学习模型，实现对胺排放和关键性能参数的预测监控，以及操作优化的决策支持。

Method: 采用四种LSTM网络结构（基础LSTM、堆叠LSTM、双向LSTM、卷积LSTM），使用TCM运行数据进行时间序列建模。进行因果影响分析，通过±20%变量扰动模拟异常情况。

Result: 模型预测准确率超199%，能够有效跟踪稳态趋势和突变波动。因果分析显示调整贝干溶剂温度和水洗条件等参数可显著降低胺排放并改善系统性能。

Conclusion: 该研究为碳捕获操作提供了一个实用的机器学习框架，既能实时监控又能进行场景测试和操作优化，代表了向智能化、数据驱动的控制策略发展的一步。

Abstract: We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.

</details>


### [48] [A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems](https://arxiv.org/abs/2509.05259)
*Jehad Jilan,Niranjana Naveen Nambiar,Ahmad Mohammad Saber,Alok Paranjape,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: 提出基于Kolmogorov-Arnold Networks (KAN)的可解释性FDIA检测方法，用于电力系统AGC的网络安全防护，检测率高达95.97%，同时保持低误报率。


<details>
  <summary>Details</summary>
Motivation: 传统AGC系统容易受到隐蔽的网络攻击（如FDIAs），而现有的黑盒检测方法缺乏可解释性，无法有效应对系统非线性特性带来的安全挑战。

Method: 使用Kolmogorov-Arnold Networks (KAN)模型离线学习AGC测量值之间的复杂非线性关系，并提取符号方程来增强模型的可解释性。

Result: KAN模型在FDIA检测中达到95.97%的检测率，符号公式版本达到95.9%的检测率，且误报率较低。

Conclusion: KAN提供了一种既准确又可解释的FDIA检测方案，显著提升了AGC系统的网络安全防护能力，特别适合处理非线性系统特性。

Abstract: Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.

</details>


### [49] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: SpikingBrain是一个基于脉冲神经元的脑启发模型家族，通过线性/混合线性注意力架构、算法优化和系统工程技术，在非NVIDIA平台上实现了高效的长上下文训练和推理，性能可比传统Transformer但计算效率显著提升


<details>
  <summary>Details</summary>
Motivation: 解决主流Transformer模型在长序列处理中的二次计算复杂度和线性内存增长问题，以及在非NVIDIA平台上训练大型模型的挑战

Method: 采用线性/混合线性注意力架构与自适应脉冲神经元，开发高效的转换训练流程和脉冲编码框架，针对MetaX硬件定制训练框架和并行策略

Result: 开发了7B和76B两个模型，在150B token上持续预训练达到开源Transformer基准性能，长序列训练效率显著提升，4M token序列首token生成速度提升100倍以上，训练稳定且达到23.4%的MFU，脉冲方案实现69.15%稀疏度

Conclusion: 这项工作展示了脑启发机制在推动下一代高效可扩展大模型设计方面的潜力，为非NVIDIA平台上的大规模LLM开发提供了可行性证明

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


### [50] [Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection](https://arxiv.org/abs/2509.05281)
*Naman Tyagi*

Main category: cs.LG

TL;DR: 一种结合空间和频域特征的双支深度学习框架，用于检测图像偶像，在CASIA 2.0数据集上达到77.9%的准确率


<details>
  <summary>Details</summary>
Motivation: 随着深度偶像和数字图像作假的快速增长，确保图像真实性变得越来越养成挑战，需要有效的作假检测方法

Method: 提出双支卷积神经网络，同时处理空间域和频域特征，通过Siamese网络融合并比较特征，生成64维嵌入进行分类

Result: 在CASIA 2.0数据集上达到77.9%的准确率，超过传统统计方法，虽然比大型复杂检测系统性能较弱，但在计算复杂度和检测可靠性之间取得平衡

Conclusion: 该方法为数字图像司法审查提供了强大方法论，在媒体验证、执法和数字内容可靠性方面推动了视觉司法技术的发展，具备实际部署的可行性

Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring
the authenticity of images is becoming increasingly challenging. This report
introduces a forgery detection framework that combines spatial and
frequency-based features for detecting forgeries. We propose a dual branch
convolution neural network that operates on features extracted from spatial and
frequency domains. Features from both branches are fused and compared within a
Siamese network, yielding 64 dimensional embeddings for classification. When
benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,
outperforming traditional statistical methods. Despite its relatively weaker
performance compared to larger, more complex forgery detection pipelines, our
approach balances computational complexity and detection reliability, making it
ready for practical deployment. It provides a strong methodology for forensic
scrutiny of digital images. In a broader sense, it advances the state of the
art in visual forensics, addressing an urgent requirement in media
verification, law enforcement and digital content reliability.

</details>


### [51] [Learning to accelerate distributed ADMM using graph neural networks](https://arxiv.org/abs/2509.05288)
*Henri Doerks,Paul Häusner,Daniel Hernández Escobar,Jens Sjölund*

Main category: cs.LG

TL;DR: 将分布式ADMM表示为图神经网络的消息传递框架，通过学习自适应步长和通信权重来加速收敛


<details>
  <summary>Details</summary>
Motivation: ADMM在分布式优化中收敛慢且对超参数敏感，需要改进其性能

Method: 将ADMM迭代表示为GNN消息传递，用GNN预测超参数，通过展开ADMM迭代进行端到端训练

Result: 数值实验表明学习版本相比标准ADMM在收敛速度和求解质量上均有提升

Conclusion: 通过GNN学习自适应超参数的方法能有效改善分布式ADMM的性能，同时保持算法收敛性

Abstract: Distributed optimization is fundamental in large-scale machine learning and
control applications. Among existing methods, the Alternating Direction Method
of Multipliers (ADMM) has gained popularity due to its strong convergence
guarantees and suitability for decentralized computation. However, ADMM often
suffers from slow convergence and sensitivity to hyperparameter choices. In
this work, we show that distributed ADMM iterations can be naturally
represented within the message-passing framework of graph neural networks
(GNNs). Building on this connection, we propose to learn adaptive step sizes
and communication weights by a graph neural network that predicts the
hyperparameters based on the iterates. By unrolling ADMM for a fixed number of
iterations, we train the network parameters end-to-end to minimize the final
iterates error for a given problem class, while preserving the algorithm's
convergence properties. Numerical experiments demonstrate that our learned
variant consistently improves convergence speed and solution quality compared
to standard ADMM. The code is available at
https://github.com/paulhausner/learning-distributed-admm.

</details>


### [52] [Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest](https://arxiv.org/abs/2509.05292)
*Xiao Yang,Mehdi Ben Ayed,Longyu Zhao,Fan Zhou,Yuchen Shen,Abe Engle,Jinfeng Zhuang,Ling Leng,Jiajing Xu,Charles Rosenberg,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的个性化效用调优框架DRL-PUT，用于广告推荐系统中的多目标优化，相比人工调优显著提升了点击率和长点击率。


<details>
  <summary>Details</summary>
Motivation: 传统人工调优方法虽然简单可解释，但由于缺乏原则性调优目标、参数组合过多、缺乏个性化和季节性适应能力，往往产生次优结果。

Method: 将问题建模为强化学习任务：根据广告请求状态预测最优超参数以最大化预定义奖励；开发直接从在线服务日志学习最优策略模型的方法，避免估计价值函数。

Result: 在Pinterest广告推荐系统的在线A/B实验中，相比基线人工调优方法，DRL-PUT在处理段上点击率提升9.7%，长点击率提升7.7%。

Conclusion: DRL-PUT框架有效解决了广告推荐系统中的多目标优化挑战，通过强化学习方法实现了更好的个性化调优效果。

Abstract: The ranking utility function in an ad recommender system, which linearly
combines predictions of various business goals, plays a central role in
balancing values across the platform, advertisers, and users. Traditional
manual tuning, while offering simplicity and interpretability, often yields
suboptimal results due to its unprincipled tuning objectives, the vast amount
of parameter combinations, and its lack of personalization and adaptability to
seasonality. In this work, we propose a general Deep Reinforcement Learning
framework for Personalized Utility Tuning (DRL-PUT) to address the challenges
of multi-objective optimization within ad recommender systems. Our key
contributions include: 1) Formulating the problem as a reinforcement learning
task: given the state of an ad request, we predict the optimal hyperparameters
to maximize a pre-defined reward. 2) Developing an approach to directly learn
an optimal policy model using online serving logs, avoiding the need to
estimate a value function, which is inherently challenging due to the high
variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT
through an online A/B experiment in Pinterest's ad recommender system. Compared
to the baseline manual utility tuning approach, DRL-PUT improved the
click-through rate by 9.7% and the long click-through rate by 7.7% on the
treated segment. We conducted a detailed ablation study on the impact of
different reward definitions and analyzed the personalization aspect of the
learned policy model.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一个针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度机制，在时间维度减少慢速GPU的去噪步骤，在空间维度弹性分配图像块，实现了45%的延迟降低和资源利用率提升


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型并行推理方案在异构多GPU环境中存在资源利用不足和负载不均衡的问题，需要针对硬件差异和后台任务导致的负载不平衡进行优化

Method: 提出混合调度器，包含时间维度的计算感知步骤分配器（使用最小公倍数最小化量化技术减少慢速GPU的去噪步骤）和空间维度的弹性块并行机制（根据GPU计算能力分配不同大小的图像块）

Result: 在负载不平衡和异构多GPU集群上的实验验证了STADI的有效性，相比最先进的块并行框架，端到端推理延迟降低高达45%，显著提高了异构GPU的资源利用率

Conclusion: STADI框架通过时空自适应调度成功解决了异构环境中的负载均衡问题，显著提升了扩散模型推理效率，为实际部署提供了有效的解决方案

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [54] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM是一个基于控制理论的SLO感知、节能LLM服务系统，通过协同设计频率缩放和请求路由，在预填充/解码分离架构中实现细粒度控制，可节省高达36.3%的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互式应用中的广泛使用，其推理过程中的高昂能源成本成为可持续和成本效益部署的挑战，需要开发节能的LLM服务系统。

Method: 系统包含反馈驱动的频率控制器（动态调整GPU频率）和状态空间路由器（探索跨频率缩放实例的路由决策），在预填充/解码分离架构中实现细粒度阶段特定控制。

Result: 在SGLang中实现并在多个先进LLM和真实数据集上评估，结果显示VoltanaLLM在保持近乎完美的SLO达成率的同时，实现了高达36.3%的能源节省。

Conclusion: VoltanaLLM为可持续和智能的LLM服务铺平了道路，通过控制理论方法有效解决了LLM推理的能源效率问题。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [55] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 多GPU扩展3D高斯散点技术用于科学可视化，通过分布式训练实现了5.6倍速度提升和大规模数据集处理能力


<details>
  <summary>Details</summary>
Motivation: 解决单GPU在处理大规模科学数据集时的容量和速度限制，为高性能计算基础上的科学可视化流程提供支持

Method: 基于Grendel-GS的多GPU训练后端，对3D高斯散点管线进行多GPU扩展，通过分布式优化提升训练速度

Result: 在Kingsnake数据集(4M高斯)上实现4GPU相比单GPU基准5.6倍速度提升，成功训练了在单A100 GPU上无法处理的Miranda数据集(18M高斯)

Conclusion: 该工作为将3D高斯散点技术集成到高性能计算科学流程奠定了基础，能够支持复杂模拟的实时后处理和原地可视化

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [56] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: 研究基于MPI RMA的单边通信方法用于可扩展应用的动态调整，与传统集体通信方法相比性能相当但初始化成本较高


<details>
  <summary>Details</summary>
Motivation: 探索更高效的动态数据重分配方法，减少对应用执行的影响，支持可扩展应用的后台重配置

Method: 基于MPI远程内存访问(RMA)的单边通信方法，集成到MaM库中，扩展Wait Drains策略支持后台重配置

Result: 与传统集体通信方法性能相当，但当前高初始化成本限制了其优势

Conclusion: 基于RMA的单边通信方法在动态调整方面具有潜力，但需要进一步优化初始化开销才能充分发挥优势

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [57] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: 本文揭示大语言模型预训练流程的具体技术细节，重点关注分布式训练、大规模数据集管理和GPU计算资源充分利用方面的实践建议


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练需要巨额计算资源投入，但相关扩展性能和训练考虑因素的公开信息很稀缺，实践建议不足，需要揭示这些大规模训练流程的技术细节

Method: 通过分析大规模训练流程，重点研究分布式训练技术、百个节点级别的大数据集管理方法，以及通过数据并行性扩展来充分利用GPU计算能力的策略

Result: 提供了大语言模型预训练流程的具体技术解析和实践指南，包括分布式训练的有效实现方法、大规模数据集管理策略以及GPU计算资源优化利用的具体技术建议

Conclusion: 本文为大语言模型的大规模预训练提供了实用的技术指南和最佳实践，填补了该领域公开文献中实践建议缺乏的空白，有助于推动大模型训练技术的发展和普及

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [58] [Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins](https://arxiv.org/abs/2509.05116)
*Jialin Chen,Jeremie Clos,Dominic Price,Praminda Caleb-Solly*

Main category: cs.ET

TL;DR: 通过使用病情模拟服装在健康参与者身上模拟偏瘫病步态变化，为康复机器人的快速原型设计提供安全的实验方法


<details>
  <summary>Details</summary>
Motivation: 在设计周期早期进行实验对辅助和康复机器人的发展至关重要，但直接与用户测试早期原型存在安全风险

Method: 使用偏瘫病模拟服装在健康参与者身上进行步态实验，采集四种走路条件下的运动学和肌电数据，并将数据集成到数字双胎模型中进行机器学习分析

Result: 模拟服装显著改变了运动和肌肉激活模式，使用者需要通过更突然的动作来补偿，同时识别了最能准确捐描步态动态和人-助行器交互的关键特征和传感器模态

Conclusion: 病情模拟服装提供了一种安全有效的方法来研究步态改变和支持康复机器人的快速原型设计，数字双胎框架为分析人-机交互提供了强大的工具

Abstract: To advance the development of assistive and rehabilitation robots, it is
essential to conduct experiments early in the design cycle. However, testing
early prototypes directly with users can pose safety risks. To address this, we
explore the use of condition-specific simulation suits worn by healthy
participants in controlled environments as a means to study gait changes
associated with various impairments and support rapid prototyping. This paper
presents a study analyzing the impact of a hemiplegia simulation suit on gait.
We collected biomechanical data using a Vicon motion capture system and Delsys
Trigno EMG and IMU sensors under four walking conditions: with and without a
rollator, and with and without the simulation suit. The gait data was
integrated into a digital twin model, enabling machine learning analyses to
detect the use of the simulation suit and rollator, identify turning behavior,
and evaluate how the suit affects gait over time. Our findings show that the
simulation suit significantly alters movement and muscle activation patterns,
prompting users to compensate with more abrupt motions. We also identify key
features and sensor modalities that are most informative for accurately
capturing gait dynamics and modeling human-rollator interaction within the
digital twin framework.

</details>
