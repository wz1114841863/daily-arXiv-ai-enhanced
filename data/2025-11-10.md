<div id=toc></div>

# Table of Contents

- [cs.ET](#cs.ET) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 73]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [1] [WIRE: Write Energy Reduction via Encoding in Phase Change Main Memories (PCM)](https://arxiv.org/abs/2511.04928)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia,Sherrene Bogle*

Main category: cs.ET

TL;DR: 提出WIRE编码机制，通过为频繁值分配汉明距离为1的码字，使大多数写操作最多翻转1位，显著降低PCM写能耗并延长寿命


<details>
  <summary>Details</summary>
Motivation: PCM作为主内存面临写能耗高的挑战，现有技术存在位级比较、预更新增加写周期、耐久性差等缺点

Method: 基于频繁值栈分配码字，使频繁值间汉明距离为1，结合块级磨损均衡和码字旋转机制

Result: 多线程和多程序工作负载评估显示在寿命、写能耗和位翻转减少方面有显著改进

Conclusion: WIRE编码机制能有效降低PCM写能耗并延长内存寿命，是可行的解决方案

Abstract: Phase Change Memory (PCM) has rapidly progressed and surpassed Dynamic
Random-Access Memory (DRAM) in terms of scalability and standby energy
efficiency. Altering a PCM cell's state during writes demands substantial
energy, posing a significant challenge to PCM's role as the primary main
memory. Prior research has explored methods to reduce write energy consumption,
including the elimination of redundant writes, minimizing cell writes, and
employing compact row buffers for filtering PCM main memory accesses. However,
these techniques had certain drawbacks like bit-wise comparison of the stored
values, preemptive updates increasing write cycles, and poor endurance. In this
paper, we propose WIRE, a new coding mechanism through which most write
operations force a maximum of one-bit flip. In this coding-based data storage
method, we look at the frequent value stack and assign a code word to the most
frequent values such that they have a hamming distance of one. In most of the
write accesses, writing a value needs one or fewer bit flips which can save
considerable write energy. This technique can be augmented with a wear-leveling
mechanism at the block level, and rotating the difference bit in the assigned
codes, increasing the lifetime of the PCM array at a low cost. Using a
full-system evaluation of our method and comparing it to the existing
mechanisms, our experimental results for multi-threaded and multi-programmed
workloads revealed considerable improvement in lifetime and write energy as
well as bit flip reduction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了Load-Balance-Longest-Path (LBLP)算法，用于在多处理单元的存内计算模拟器上优化CNN模型的任务分配，以提高处理速率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 存内计算(IMC)通过减少数据移动瓶颈和利用内存计算的并行性来加速深度学习，但需要先进的任务分配策略来在基于IMC的硬件上高效部署CNN模型。

Method: 使用多处理单元的IMC模拟器(IMCE)，提出了LBLP算法，动态地将CNN节点分配到可用的处理单元上，以最大化处理速率和最小化延迟。

Result: 对多个CNN模型进行了基准测试，实验结果表明所提出的LBLP算法相比其他调度策略具有更好的效果。

Conclusion: LBLP算法能够有效优化CNN模型在存内计算硬件上的部署，提高计算效率和资源利用率。

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


### [3] [SMART-WRITE: Adaptive Learning-based Write Energy Optimization for Phase Change Memory](https://arxiv.org/abs/2511.04713)
*Mahek Desai,Rowena Quinn,Marjan Asadinia*

Main category: cs.AR

TL;DR: 提出SMART-WRITE方法，结合神经网络和强化学习动态优化相变存储器写入能耗，相比基线模型减少63%能耗并提升51%性能


<details>
  <summary>Details</summary>
Motivation: 随着DRAM等传统存储器接近可扩展性极限，相变存储器因可扩展性、快速访问时间和零泄漏功率而成为有前景的替代方案，但其写入操作会降低材料寿命且能耗较高，需要提升耐久性和降低写入能耗

Method: 集成神经网络和强化学习，NN模型监控实时运行条件和设备特性确定最优写入参数，RL模型动态调整这些参数进一步优化能耗

Result: 相比基线和先前模型，SMART-WRITE减少写入能耗达63%，性能提升达51%

Conclusion: 通过基于实时系统条件持续调整PCM写入参数，SMART-WRITE显著改善了相变存储器的能耗效率和性能表现

Abstract: As dynamic random access memory (DRAM) and other current transistor-based
memories approach their scalability limits, the search for alternative storage
methods becomes increasingly urgent. Phase-change memory (PCM) emerges as a
promising candidate due to its scalability, fast access time, and zero leakage
power compared to many existing memory technologies. However, PCM has
significant drawbacks that currently hinder its viability as a replacement. PCM
cells suffer from a limited lifespan because write operations degrade the
physical material, and these operations consume a considerable amount of
energy. For PCM to be a practical option for data storage-which involves
frequent write operations-its cell endurance must be enhanced, and write energy
must be reduced. In this paper, we propose SMART-WRITE, a method that
integrates neural networks (NN) and reinforcement learning (RL) to dynamically
optimize write energy and improve performance. The NN model monitors real-time
operating conditions and device characteristics to determine optimal write
parameters, while the RL model dynamically adjusts these parameters to further
optimize PCM's energy consumption. By continuously adjusting PCM write
parameters based on real-time system conditions, SMART-WRITE reduces write
energy consumption by up to 63% and improves performance by up to 51% compared
to the baseline and previous models.

</details>


### [4] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: RAS是一个硬件架构系统，通过集成rANS算法到无损压缩流水线中，解决了概率模型方法计算速度慢的问题，实现了显著的编解码加速。


<details>
  <summary>Details</summary>
Motivation: 数据中心处理大量数据需要高效的无损压缩，但基于概率模型的新方法通常计算速度较慢，需要解决这一瓶颈。

Method: RAS将rANS核心与概率生成器耦合，使用BF16格式存储分布并转换为固定点域，采用两阶段rANS更新和字节级重归一化，通过预测引导的解码路径缩小CDF搜索窗口，多通道组织提升吞吐量。

Result: 在图像工作负载上，RTL仿真原型实现了121.2倍编码和70.9倍解码加速，解码器二分搜索步骤从7.00减少到3.15（约减少55%），与神经概率模型结合时保持比传统编解码器更高的压缩比。

Conclusion: RAS提供了一种实用的快速神经无损压缩方法，在CPU/GPU rANS实现上表现优异。

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [5] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: MDM是一种后训练DNN权重映射技术，通过优化有源忆阻器位置来减少忆阻CIM交叉阵列中的寄生电阻非理想性，提高模拟计算精度。


<details>
  <summary>Details</summary>
Motivation: 寄生电阻限制了交叉阵列效率，迫使将DNN矩阵映射到小尺寸交叉阵列瓦片中，这降低了CIM加速效果并增加了数字同步、ADC转换、延迟和芯片面积开销。

Method: 利用比特级结构化稀疏性，从密度较高的低阶侧输入激活，并根据曼哈顿距离重新排列行，将有源单元重新定位到受寄生电阻影响较小的区域。

Result: 在ImageNet-1k上的DNN模型中，MDM将非理想性因子降低高达46%，在ResNet中模拟失真下的准确率平均提高3.6%。

Conclusion: MDM提供了一种轻量级、空间感知的方法，用于扩展CIM DNN加速器的规模。

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


### [6] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: SilentZNS是一种新的ZNS SSD区域映射和管理方法，通过动态分配资源解决传统ZNS实现中的设备级写放大、磨损增加和主机I/O干扰问题，显著降低写放大和磨损，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统ZNS SSD实现存在设备级写放大、磨损增加和与主机I/O干扰的问题，主要原因是固定物理区域和全区域操作导致过多的物理写入。

Method: 提出SilentZNS方法，采用灵活的区域分配方案，动态分配可用资源到区域，避免传统逻辑到物理区域映射的限制，允许任意块集合分配给区域，同时确保磨损均衡和读取性能。

Result: SilentZNS消除了高达20倍的虚拟写入负担，设备级写放大减少86%（在10%区域占用时），总体磨损减少高达76.9%，工作负载执行速度提升高达3.7倍。

Conclusion: SilentZNS通过创新的区域映射和管理方法有效解决了ZNS SSD的关键限制，显著提升了性能和寿命，为ZNS技术提供了更优化的实现方案。

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [7] [MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference](https://arxiv.org/abs/2511.05321)
*Maximilian Kirschner,Konstantin Dudzik,Ben Krusekamp,Jürgen Becker*

Main category: cs.AR

TL;DR: 提出了一种新的硬件架构，通过多核向量处理器和本地暂存内存来解决AI加速器在实时系统中性能与可预测性之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 实时系统（如自动驾驶）越来越多地采用神经网络，需要高性能且具有可预测时序行为的硬件。现有实时硬件资源有限，而现代AI加速器由于内存干扰缺乏可预测性。

Method: 设计多核向量处理器架构，每个核心配备本地暂存内存，通过中央管理核心按照静态确定的时间表协调共享外部内存访问。

Result: 分析不同参数化设计变体，发现配置更多小核心的变体由于增加的有效内存带宽和更高时钟频率而获得更好性能，且执行时间波动非常低。

Conclusion: 该架构成功平衡了性能与可预测性，为实时系统中的神经网络应用提供了可行的硬件解决方案。

Abstract: Real-time systems, particularly those used in domains like automated driving,
are increasingly adopting neural networks. From this trend arises the need for
high-performance hardware exhibiting predictable timing behavior. While
state-of-the-art real-time hardware often suffers from limited memory and
compute resources, modern AI accelerators typically lack the crucial
predictability due to memory interference.
  We present a new hardware architecture to bridge this gap between performance
and predictability. The architecture features a multi-core vector processor
with predictable cores, each equipped with local scratchpad memories. A central
management core orchestrates access to shared external memory following a
statically determined schedule.
  To evaluate the proposed hardware architecture, we analyze different variants
of our parameterized design. We compare these variants to a baseline
architecture consisting of a single-core vector processor with large vector
registers. We find that configurations with a larger number of smaller cores
achieve better performance due to increased effective memory bandwidth and
higher clock frequencies. Crucially for real-time systems, execution time
fluctuation remains very low, demonstrating the platform's time predictability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV缓存管理策略与模型架构限制的交互作用研究，发现当累积KV缓存接近或超过模型训练上下文窗口时，生成质量急剧下降，位置编码完整性是关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决状态化多轮场景中KV缓存无界增长带来的挑战，研究缓存管理策略与模型架构限制的相互作用，特别关注位置编码完整性的影响。

Method: 使用状态化基准测试框架进行实证分析，比较不同缓存驱逐策略（包括高保留率策略如AttentionTop）对生成质量的影响，特别关注位置连贯性的保持。

Result: 当累积KV缓存接近或超过模型训练上下文窗口时，LLM生成质量急剧下降；即使高保留率的驱逐策略如果破坏位置连贯性也会恶化性能；保持连续上下文块的简单策略比复杂或位置破坏性策略产生更连贯的生成结果。

Conclusion: 倡导尊重架构限制、保持位置结构、从整体视角看待"缓存健康"的驱逐技术，而不仅仅是缓存大小。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [9] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的rs-fMRI分析框架，通过自适应级联分解学习任务相关的频率子带，并结合频率耦合连接学习来捕捉脑区间的跨频段相互作用，用于脑部疾病的诊断预测。


<details>
  <summary>Details</summary>
Motivation: 现有rs-fMRI模型大多忽视神经元振荡的多频特性，将BOLD信号视为单一时间序列，这限制了诊断的敏感性和特异性。神经疾病通常在特定频段表现出异常，但现有方法依赖预定义频带，无法适应个体差异和疾病特异性变化。

Method: 1) 自适应级联分解：为每个脑区学习任务相关的频率子带；2) 频率耦合连接学习：捕捉脑区内的频段内和跨频段相互作用；3) 统一功能网络和消息传递机制：在统一GCN中生成优化的节点表示用于诊断预测。

Result: 在ADNI和ABIDE数据集上的实验结果表明，该方法优于现有方法。

Conclusion: 该框架通过自适应学习频率子带和捕捉跨频段相互作用，显著提升了rs-fMRI在脑部疾病诊断中的性能。

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [10] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: AWEMixer是一个自适应小波增强的混合网络，用于解决物联网环境中长期时间序列预测的挑战，通过结合全局频率信息和局部小波子带，实现了精确的时频定位。


<details>
  <summary>Details</summary>
Motivation: 物联网环境中传感器信号的非平稳性和多尺度特性使得长期时间序列预测具有挑战性，传统方法局限于时域操作，而傅里叶变换获得的全局频率信息被视为平稳信号，会模糊瞬态事件的时间模式。

Method: AWEMixer包含两个创新组件：1）频率路由器利用快速傅里叶变换获得的全局周期性模式自适应加权局部小波子带；2）相干门控融合块通过交叉注意力和门控机制实现显著频率特征与多尺度时间表示的选择性集成。

Result: 在七个公共基准测试中验证了模型的有效性，相比基于transformer和MLP的最先进模型，在长序列时间序列预测中持续实现了性能提升。

Conclusion: AWEMixer模型能够准确进行时频定位，同时对噪声保持鲁棒性，在长期时间序列预测任务中表现出色。

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [11] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: 提出了一种结合TCN和增强型TFT的RUL预测框架，通过多时间窗口方法提高适应性，在基准数据集上平均RMSE降低5.5%。


<details>
  <summary>Details</summary>
Motivation: 现有RUL预测模型难以捕捉细粒度时间依赖关系，且无法动态优先考虑关键特征，需要更稳健的预测方法。

Method: 集成TCN进行局部时间特征提取，结合Bi-LSTM编码器-解码器增强的TFT，采用多时间窗口方法适应不同工况。

Result: 在基准数据集上评估显示，该模型平均RMSE最多降低5.5%，预测精度优于现有先进方法。

Conclusion: 该框架填补了现有方法的空白，提升了工业预测系统的有效性，展示了先进时间序列变换器在RUL预测中的潜力。

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [12] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了传感器引导的GLISp扩展方法，通过整合可测量的传感器描述符来增强偏好学习，在保持偏好搜索灵活性的同时注入灰盒结构。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法（如GLISp）将系统视为黑盒，忽略了信息丰富的传感器测量数据。

Method: 引入传感器引导的正则化GLISp扩展，通过物理信息假设函数和最小二乘正则化项将可测量描述符整合到偏好学习循环中。

Result: 在分析基准和车辆悬架调谐任务上的数值评估显示，相比基线GLISp具有更快的收敛速度和更优的最终解。

Conclusion: 该方法成功结合了主观反馈与定量传感器信息，在保持偏好搜索灵活性的同时提升了性能。

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [13] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: 论文研究了在数据稀缺和分布偏移情况下，通过知识蒸馏(KD)可以诱导和加速grokking现象，使模型在新分布上实现泛化，即使可用数据低于临界阈值。


<details>
  <summary>Details</summary>
Motivation: 研究在数据稀缺(训练样本低于临界阈值)和分布偏移的实际场景中，如何让模型实现延迟泛化(grokking)现象。

Method: 使用知识蒸馏(KD)方法：1)从已在分布p1上grokked的模型蒸馏到新分布p2；2)在联合分布(p1,p2)上训练；3)持续预训练设置中从p1过渡到p2。

Result: KD能够：1)在数据低于临界阈值时诱导和加速grokking；2)在联合分布训练中实现泛化；3)在持续学习中加速泛化并减轻灾难性遗忘，仅用10%数据即可获得强性能。

Conclusion: 知识蒸馏在低数据和演化分布设置中，对实现模型泛化具有核心作用，为知识转移下的grokking机制提供了新见解。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [14] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow是一个将PyTorch稀疏模型转换为融合稀疏数据流图的编译器，支持跨表达式融合、并行化等优化，针对可重构数据流架构，在真实ML应用中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模扩大，稀疏计算和专用数据流硬件成为提高效率的重要方案，需要编译器支持稀疏操作的融合优化。

Method: 开发FuseFlow编译器，支持跨表达式融合、并行化、数据流排序和稀疏分块等优化，使用周期精确的数据流模拟器进行微架构分析。

Result: 在四个真实世界机器学习应用中，完整融合并非总是最优，融合粒度取决于模型本身；FuseFlow实现了性能提升，在GPT-3 BigBird块稀疏注意力上比未融合基线加速约2.7倍。

Conclusion: FuseFlow是首个支持通用跨表达式稀疏操作融合的编译器，提供了识别和剪枝次优配置的启发式方法，证明了融合粒度对稀疏模型性能的重要性。

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [15] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: 提出了一种针对云工作负载的指令预取设计，通过压缩条目、分层元数据存储和在线ML控制器来优化尾延迟和能效


<details>
  <summary>Details</summary>
Motivation: 大规模网络服务依赖深度软件栈和微服务编排，这会增加指令足迹并造成前端停顿，从而加剧尾延迟和能耗问题

Method: 基于纠缠指令预取器(EIP)，引入压缩条目(36位捕获8个目标)、分层元数据存储(仅保留L1常驻和频繁查询条目)，以及轻量级在线ML控制器(使用上下文特征和带限调整阈值评分预取收益)

Result: 在数据中心应用中，该方法在保持EIP类似加速效果的同时减少了片上状态，提升了ML时代网络服务的效率

Conclusion: 该设计为云工作负载提供了有效的指令预取解决方案，通过压缩和分层存储优化了资源利用，结合ML控制器提升了预取效率

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [16] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: 提出CNODE框架，使用神经ODE模型连续建模帕金森病脑形态变化，通过患者特异性初始时间和进展速度实现个体化轨迹对齐，在PPMI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离散、规则采样数据，难以处理PD队列中不规则稀疏的MRI数据，且难以捕捉个体异质性（如发病时间、进展速度、症状严重程度等PD典型特征）。

Method: 使用神经ODE模型将脑形态变化建模为连续时间过程，联合学习患者特异性初始时间和进展速度，将个体轨迹对齐到共享进展轨迹中。

Result: 在PPMI数据集上的实验结果表明，该方法在预测纵向PD进展方面优于最先进的基线方法。

Conclusion: CNODE框架能够有效建模PD的异质性和演化模式，为机制理解、治疗开发和个体化预测提供了新方法。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [17] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: 论文提出将表示学习与因果推断结合，利用多模态数据（观察性和干预性）进行因果发现和表示学习，以解决生物医学中的基本问题。


<details>
  <summary>Details</summary>
Motivation: 表示学习在预测任务中很成功，但在因果任务中可能失败，需要将表示学习与因果推断结合，利用多模态数据来改进因果发现。

Method: 提出统计和计算框架，利用观察性和干预性多模态数据，进行因果结构学习和表示学习，包括学习因果变量和设计最优干预。

Result: 框架旨在更有效地利用多模态数据，在观察和干预数据上执行因果发现，并学习因果变量。

Conclusion: 表示学习与因果推断的结合，特别是利用多模态数据，为解决生物医学中的因果问题提供了有前景的方向。

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [18] [DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing](https://arxiv.org/abs/2511.04791)
*Lei Gao,Chaoyi Jiang,Hossein Entezari Zarch,Daniel Wong,Murali Annavaram*

Main category: cs.LG

TL;DR: DuetServe是一个统一的LLM服务框架，通过动态激活SM级GPU空间多路复用来实现预填充和解码阶段的解耦，在单个GPU内提供分解级别的隔离，从而提高吞吐量同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统要么在共享GPU上聚合预填充和解码阶段导致干扰，要么跨GPU分解两个阶段造成资源浪费。需要一种既能保持低延迟又不浪费资源的解决方案。

Method: DuetServe默认在聚合模式下运行，当预测到TBT退化时动态激活SM级GPU空间多路复用。关键思想是通过细粒度的自适应SM分区仅在需要时解耦预填充和解码执行。集成注意力感知屋顶模型预测迭代延迟、分区优化器选择最优SM分割、无中断执行引擎消除CPU-GPU同步开销。

Result: 评估显示DuetServe相比最先进框架将总吞吐量提高了1.3倍，同时保持低生成延迟。

Conclusion: DuetServe在单个GPU内实现了分解级别的隔离，通过动态SM分区有效解决了预填充和解码阶段的干扰问题，在提高吞吐量的同时满足了延迟SLO要求。

Abstract: Modern LLM serving systems must sustain high throughput while meeting strict
latency SLOs across two distinct inference phases: compute-intensive prefill
and memory-bound decode phases. Existing approaches either (1) aggregate both
phases on shared GPUs, leading to interference between prefill and decode
phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two
phases across GPUs, improving latency but wasting resources through duplicated
models and KV cache transfers. We present DuetServe, a unified LLM serving
framework that achieves disaggregation-level isolation within a single GPU.
DuetServe operates in aggregated mode by default and dynamically activates
SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key
idea is to decouple prefill and decode execution only when needed through
fine-grained, adaptive SM partitioning that provides phase isolation only when
contention threatens latency service level objectives (SLOs). DuetServe
integrates (1) an attention-aware roofline model to forecast iteration latency,
(2) a partitioning optimizer that selects the optimal SM split to maximize
throughput under TBT constraints, and (3) an interruption-free execution engine
that eliminates CPU-GPU synchronization overhead. Evaluations show that
DuetServe improves total throughput by up to 1.3x while maintaining low
generation latency compared to state-of-the-art frameworks.

</details>


### [19] [Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator](https://arxiv.org/abs/2511.04804)
*Chaymae Yahyati,Ismail Lamaakal,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SiFEN是一种基于有限元方法的神经网络，在学习的单纯形网格上使用分段多项式表示函数，具有显式局部性、可控平滑度和缓存友好的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 提出一种紧凑、可解释且理论基础的替代方案，以替代密集MLP和边缘样条网络，提供更好的校准和推理延迟。

Method: 将度m的Bernstein-Bezier多项式与轻量可逆扭曲配对，通过形状正则化、半离散OT覆盖和可微分边翻转进行端到端训练。

Result: 在合成逼近任务、表格回归/分类以及作为紧凑CNN的替代头部时，SiFEN在匹配参数预算下表现优于MLP和KAN，提高了校准性并降低了推理延迟。

Conclusion: SiFEN是一个紧凑、可解释且理论基础的函数逼近器，具有经典FEM逼近速率，适用于各种机器学习任务。

Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial
predictor that represents f: R^d -> R^k as a globally C^r finite-element field
on a learned simplicial mesh in an optionally warped input space. Each query
activates exactly one simplex and at most d+1 basis functions via barycentric
coordinates, yielding explicit locality, controllable smoothness, and
cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with
a light invertible warp and trains end-to-end with shape regularization,
semi-discrete OT coverage, and differentiable edge flips. Under standard
shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic
FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic
approximation tasks, tabular regression/classification, and as a drop-in head
on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter
budgets, improves calibration (lower ECE/Brier), and reduces inference latency
due to geometric locality. These properties make SiFEN a compact,
interpretable, and theoretically grounded alternative to dense MLPs and
edge-spline networks.

</details>


### [20] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE是一种无需训练的MoE模型压缩方法，通过稀疏专家合并和位打包编码技术，在保持精度的同时实现高达50%的压缩比和1.28倍推理加速。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然能高效扩展语言模型，但由于存储所有专家参数导致内存开销巨大，限制了其广泛部署。现有专家丢弃和合并方法在高压缩比下性能下降严重。

Method: 1. 稀疏专家合并：识别权重冗余和专业化，使用双掩码捕获共享和专家特定参数；2. 位打包编码：重用未充分利用的指数位，避免存储二进制掩码和符号的开销。

Result: 在50%压缩比下，PuzzleMoE在MMLU上比现有方法性能提升16.7%，同时实现1.28倍推理加速，在各种任务上保持准确性。

Conclusion: PuzzleMoE通过创新的稀疏合并和编码方案，有效解决了MoE模型内存开销问题，实现了高效压缩和推理，为MoE模型的实用部署提供了可行方案。

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [21] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 本文探讨了自编码器在数据流形上的拓扑限制和能力，以及其在具有不变流形的动力系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究自编码器在数据流形上的理论限制和可能性，特别是从拓扑角度分析编码器-解码器对在保持数据流形结构方面的能力。

Method: 通过拓扑学方法分析自编码器的数学结构，研究编码器E和解码器D的连续映射对，以及复合映射D∘E在数据流形M上逼近恒等映射的能力。

Result: 发现了自编码器在数据流形上的各种拓扑限制和编码能力，并描述了在具有不变流形的动力系统中自编码的应用潜力。

Conclusion: 自编码器在数据流形上存在固有的拓扑限制，但同时也具备特定的编码能力，特别是在处理动力系统的不变流形时具有应用价值。

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [22] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: 论文挑战了传统的体积假设，发现增加训练数据会改变损失景观，使原本体积小但泛化能力强的极小值变得相对更大，从而更容易被找到。


<details>
  <summary>Details</summary>
Motivation: 传统体积假设认为深度学习有效是因为平坦极小值体积大容易被找到，但这无法解释大数据集在泛化中的作用。本文旨在研究数据量如何影响极小值体积与泛化能力的关系。

Method: 通过在不同训练数据量下测量极小值的体积，分析损失景观的变化规律。

Result: 发现存在体积小但泛化能力强的尖锐极小值，但由于体积小而不容易被找到。增加数据会使这些泛化好的极小值体积相对变大。

Conclusion: 大数据集的作用是通过改变损失景观，使泛化能力强的极小值变得相对更大，从而更容易被优化算法找到。

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [23] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE是一个整合了8万多个肽序列的实验框架，通过构建多标签分类层次来预测抗菌肽的多功能活性，并使用基于transformer的模型实现了最先进的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决抗菌肽研究中数据集碎片化、注释不一致和缺乏标准化基准的问题，以促进计算方法和新候选肽的发现。

Method: 整合27个验证数据库中的8万多个肽序列，构建生物一致的多标签层次结构，并开发基于transformer的模型，结合序列和结构信息预测肽的多功能活性。

Result: 相比第二佳方法，在平均精度均值上实现了2.56%的相对提升，建立了新的最先进多标签肽分类方法。

Conclusion: ESCAPE提供了一个全面且可复现的评估框架，可推动AI驱动的抗菌肽研究发展。

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [24] [Persistent reachability homology in machine learning applications](https://arxiv.org/abs/2511.04825)
*Luigi Caputi,Nicholas Meadows,Henri Riihimäki*

Main category: cs.LG

TL;DR: 本文研究了持久可达性同调(PRH)在有向图数据中的应用，特别是在癫痫检测网络分类任务中的有效性。PRH相比传统的基于有向旗复形的持久同调(DPH)具有优势，因为它考虑过滤中出现的凝聚有向图，计算更高效。实验表明PRH在分类任务中优于DPH。


<details>
  <summary>Details</summary>
Motivation: 探索持久可达性同调(PRH)在有向图数据分析中的有效性，特别是在神经科学领域的癫痫检测问题中，解决传统方法计算复杂度高的问题。

Method: 使用PRH和DPH两种拓扑数据分析方法，采用Betti曲线及其积分作为拓扑特征，并在支持向量机上实现分类管道。

Result: PRH在癫痫检测的网络分类任务中表现优于传统的DPH方法。

Conclusion: 持久可达性同调(PRH)是一种有效的有向图数据分析工具，在神经科学应用如癫痫检测中具有优越性能，且计算效率更高。

Abstract: We explore the recently introduced persistent reachability homology (PRH) of
digraph data, i.e. data in the form of directed graphs. In particular, we study
the effectiveness of PRH in network classification task in a key neuroscience
problem: epilepsy detection. PRH is a variation of the persistent homology of
digraphs, more traditionally based on the directed flag complex (DPH). A main
advantage of PRH is that it considers the condensations of the digraphs
appearing in the persistent filtration and thus is computed from smaller
digraphs. We compare the effectiveness of PRH to that of DPH and we show that
PRH outperforms DPH in the classification task. We use the Betti curves and
their integrals as topological features and implement our pipeline on support
vector machine.

</details>


### [25] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: 提出了一种通过概念反演获取隐式负嵌入来替代训练自由方法中负提示的方法，解决了微调模型和训练自由防御方法之间的不兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型可能产生有害内容，现有防御方法（模型微调和训练自由引导）结合使用时效果不佳，存在不兼容问题。

Method: 使用概念反演获取隐式负嵌入来替代训练自由方法中的负提示，无需修改现有方法即可集成到现有流程中。

Result: 在色情和暴力基准测试中验证了有效性，防御成功率持续提升，同时保持了输入提示的核心语义。

Conclusion: 该方法简单有效，解决了两种防御范式之间的不兼容问题，能够在不影响语义的情况下提升防御性能。

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [26] [SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression](https://arxiv.org/abs/2511.04838)
*Brenda Nogueira,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: SPECTRA是一个光谱目标感知的图增强框架，通过在光谱域生成真实的分子图来解决分子属性预测中稀有化合物预测性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 在分子属性预测中，最有价值的化合物（如高效力）通常占据目标空间的稀疏区域。标准的图神经网络优化平均误差，在这些罕见但关键的情况下表现不佳，现有的过采样方法往往会扭曲分子拓扑结构。

Method: SPECTRA框架：(i)从SMILES重建多属性分子图；(ii)通过融合Gromov-Wasserstein耦合对齐分子对获得节点对应关系；(iii)在稳定共享基中插值拉普拉斯特征值、特征向量和节点特征；(iv)重建边以合成具有插值目标的物理合理中间体。使用基于标签核密度估计的稀有感知预算方案，在数据稀缺区域集中增强。

Result: 在基准测试中，SPECTRA在相关目标范围内持续改善误差，同时保持竞争力的整体MAE，并产生可解释的合成分子，其结构反映了底层光谱几何。

Conclusion: 结果表明，光谱、几何感知的增强是解决不平衡分子属性回归问题的有效且高效策略。

Abstract: In molecular property prediction, the most valuable compounds (e.g., high
potency) often occupy sparse regions of the target space. Standard Graph Neural
Networks (GNNs) commonly optimize for the average error, underperforming on
these uncommon but critical cases, with existing oversampling methods often
distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral
Target-Aware graph augmentation framework that generates realistic molecular
graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute
molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused)
Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates
Laplacian eigenvalues, eigenvectors and node features in a stable share-basis;
and (iv) reconstructs edges to synthesize physically plausible intermediates
with interpolated targets. A rarity-aware budgeting scheme, derived from a
kernel density estimation of labels, concentrates augmentation where data are
scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions,
SPECTRA densifies underrepresented regions without degrading global accuracy.
On benchmarks, SPECTRA consistently improves error in relevant target ranges
while maintaining competitive overall MAE, and yields interpretable synthetic
molecules whose structure reflects the underlying spectral geometry. Our
results demonstrate that spectral, geometry-aware augmentation is an effective
and efficient strategy for imbalanced molecular property regression.

</details>


### [27] [Sublinear iterations can suffice even for DDPMs](https://arxiv.org/abs/2511.04844)
*Matthew S. Zhang,Stephen Huan,Jerry Huang,Nicholas M. Boffi,Sitan Chen,Sinho Chewi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs)
have shown remarkable success in real-world sample generation tasks. Prior
analyses of DDPMs have been focused on the exponential Euler discretization,
showing guarantees that generally depend at least linearly on the dimension or
initial Fisher information. Inspired by works in log-concave sampling (Shen and
Lee, 2019), we analyze an integrator -- the denoising diffusion randomized
midpoint method (DDRaM) -- that leverages an additional randomized midpoint to
better approximate the SDE. Using a recently-developed analytic framework
called the "shifted composition rule", we show that this algorithm enjoys
favorable discretization properties under appropriate smoothness assumptions,
with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure
convergence. This is the first sublinear complexity bound for pure DDPM
sampling -- prior works which obtained such bounds worked instead with
ODE-based sampling and had to make modifications to the sampler which deviate
from how they are used in practice. We also provide experimental validation of
the advantages of our method, showing that it performs well in practice with
pre-trained image synthesis models.

</details>


### [28] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: 使用机器学习模型分析美国消费者对具有创新运输证书的食品的偏好，重点关注运输安全、能源证书等关键因素。


<details>
  <summary>Details</summary>
Motivation: 基于先前研究发现运输因素在消费者食品购买选择中具有显著影响，需要进一步识别具体的运输属性价值。

Method: 采用机器学习模型和偏好实验，提出五种创新运输证书：运输模式、物联网、安全措施、能源来源和必须到达日期，并控制产品和决策者因素。

Result: 发现消费者在运输领域对安全和能源证书有明显偏好，同时价格、产品类型、证书和决策者因素都会影响购买选择。

Conclusion: 研究为改进食品供应链系统提供了数据驱动的建议。

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [29] [Grounded Test-Time Adaptation for LLM Agents](https://arxiv.org/abs/2511.04847)
*Arthur Chen,Zuxin Liu,Jianguo Zhang,Akshara Prabhakar,Zhiwei Liu,Shelby Heinecke,Silvio Savarese,Victor Zhong,Caiming Xiong*

Main category: cs.LG

TL;DR: 提出两种互补策略来增强LLM智能体在陌生复杂环境中的泛化能力：在线分布适应方法学习轻量级适应向量来调整输出分布，部署时动态基础方法通过探索阶段学习环境因果动态。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在陌生复杂环境（如未见网站或新函数集）中泛化能力不足，存在语法误解（环境特定组件）和语义误解（状态转移动态）两种失效模式。

Method: 1. 在线分布适应：学习轻量级适应向量，使模型输出分布与环境响应格式对齐；2. 部署时动态基础：通过角色驱动探索阶段系统性地探测和学习环境因果动态，构建非参数世界模型。

Result: 在函数调用和网页导航等多样化智能体基准测试中，两种策略均有效且计算成本低。动态基础方法在复杂环境中特别有效，如在WebArena多站点分割上，成功率从2%提升至23%。

Conclusion: 这两种策略为构建更通用和强大的LLM智能体提供了稳健路径，动态基础方法在不可预测动态构成主要障碍的复杂环境中表现尤为突出。

Abstract: Large language model (LLM)-based agents struggle to generalize to novel and
complex environments, such as unseen websites or new sets of functions, due to
a fundamental mismatch between their pre-training and test-time conditions.
This challenge stems from two distinct failure modes: a syntactic
misunderstanding of environment-specific components like observation formats,
and a semantic misunderstanding of state-transition dynamics, which are only
revealed at test time. To address these issues, we propose two distinct and
complementary strategies for adapting LLM agents by leveraging
environment-specific information available during deployment. First, an online
distributional adaptation method parameterizes environmental nuances by
learning a lightweight adaptation vector that biases the model's output
distribution, enabling rapid alignment with an environment response format.
Second, a deployment-time dynamics grounding method employs a persona-driven
exploration phase to systematically probe and learn the environment's causal
dynamics before task execution, equipping the agent with a nonparametric world
model. We evaluate these strategies across diverse agentic benchmarks,
including function calling and web navigation. Our empirical results show the
effectiveness of both strategies across all benchmarks with minimal
computational cost. We find that dynamics grounding is particularly effective
in complex environments where unpredictable dynamics pose a major obstacle,
demonstrating a robust path toward more generalizable and capable LLM-based
agents. For example, on the WebArena multi-site split, this method increases
the agent's success rate from 2% to 23%.

</details>


### [30] [SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion](https://arxiv.org/abs/2511.04854)
*Alvaro Prat,Leo Zhang,Charlotte M. Deane,Yee Whye Teh,Garrett M. Morris*

Main category: cs.LG

TL;DR: SigmaDock是一种基于SE(3)黎曼扩散模型的分子对接方法，通过将配体分解为刚性片段并学习在结合口袋中重新组装这些片段，实现了最先进的对接性能，在PoseBusters数据集上达到79.9%的Top-1成功率。


<details>
  <summary>Details</summary>
Motivation: 解决生成式分子对接方法存在的化学不合理输出、泛化性差和计算成本高等问题，同时超越传统物理基方法的性能。

Method: 提出新颖的片段化方案，将配体分解为刚性片段，然后使用SE(3)黎曼扩散模型学习在结合口袋中重新组装这些片段。

Result: 在PoseBusters数据集上达到79.9%的Top-1成功率，远超近期深度学习方法（12.7-30.8%），并且首次在PB训练-测试分割下超越经典物理基对接方法。

Conclusion: SigmaDock标志着深度学习在分子建模可靠性和可行性方面的重大飞跃，为药物发现提供了更高效准确的分子对接解决方案。

Abstract: Determining the binding pose of a ligand to a protein, known as molecular
docking, is a fundamental task in drug discovery. Generative approaches promise
faster, improved, and more diverse pose sampling than physics-based methods,
but are often hindered by chemically implausible outputs, poor
generalisability, and high computational cost. To address these challenges, we
introduce a novel fragmentation scheme, leveraging inductive biases from
structural chemistry, to decompose ligands into rigid-body fragments. Building
on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion
model that generates poses by learning to reassemble these rigid bodies within
the binding pocket. By operating at the level of fragments in SE(3), SigmaDock
exploits well-established geometric priors while avoiding overly complex
diffusion processes and unstable training dynamics. Experimentally, we show
SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates
(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%
reported by recent deep learning approaches, whilst demonstrating consistent
generalisation to unseen proteins. SigmaDock is the first deep learning
approach to surpass classical physics-based docking under the PB train-test
split, marking a significant leap forward in the reliability and feasibility of
deep learning for molecular modelling.

</details>


### [31] [Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2511.04856)
*Thore Gerlach,Michael Schenk,Verena Kain*

Main category: cs.LG

TL;DR: 提出了理论基础的连续半量子玻尔兹曼机(CSQBM)，支持连续动作强化学习，通过结合可见单元的指数族先验和隐藏单元的量子玻尔兹曼分布，构建混合量子-经典模型，减少量子比特需求同时保持强表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决连续控制强化学习中的不稳定性问题，同时降低量子计算资源需求，实现量子-经典混合模型的连续动作学习。

Method: 结合指数族先验和量子玻尔兹曼分布构建CSQBM模型，利用解析梯度计算实现与Actor-Critic算法的直接集成，提出连续Q学习框架，用CSQBM分布采样替代全局最大化。

Result: 开发出能够支持连续动作强化学习的混合量子-经典模型，梯度可解析计算，克服了连续控制中的不稳定性问题。

Conclusion: CSQBM为连续动作强化学习提供了理论基础的量子-经典混合解决方案，在减少量子资源需求的同时保持了模型表达能力，为连续控制任务提供了稳定有效的学习框架。

Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann
Machines (CSQBMs) that supports continuous-action reinforcement learning. By
combining exponential-family priors over visible units with quantum Boltzmann
distributions over hidden units, CSQBMs yield a hybrid quantum-classical model
that reduces qubit requirements while retaining strong expressiveness.
Crucially, gradients with respect to continuous variables can be computed
analytically, enabling direct integration into Actor-Critic algorithms.
Building on this, we propose a continuous Q-learning framework that replaces
global maximization by efficient sampling from the CSQBM distribution, thereby
overcoming instability issues in continuous control.

</details>


### [32] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL是一个基于强化学习的元学习框架，用于预测食品银行的捐赠波动，通过动态加权多种预测模型来提高准确性，特别是在自然灾害等干扰时期。


<details>
  <summary>Details</summary>
Motivation: 食品银行在缓解粮食不安全方面至关重要，但传统预测模型难以应对捐赠量的高度波动和概念漂移，特别是在季节性变化和自然灾害（如飓风、野火）影响下。

Method: 提出FoodRL框架，使用强化学习进行元学习，基于近期性能和上下文信息对不同的预测模型进行聚类和动态加权。

Result: 在两个结构不同的美国食品银行（西海岸受野火影响的大型区域食品银行和东海岸受飓风影响的州级食品银行）的多年度数据评估中，FoodRL始终优于基线方法，特别是在干扰或衰退期间。

Conclusion: FoodRL通过提供更可靠和自适应的预测，每年可促进相当于170万份额外餐食的重新分配，展示了其在人道主义供应链中自适应集成学习的巨大社会影响潜力。

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [33] [Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning](https://arxiv.org/abs/2511.04883)
*Di Chen,Jia Li,Michael Zhang*

Main category: cs.LG

TL;DR: 研究表明，即使自动驾驶车辆出于自利目的，也能在混合交通系统中通过深度强化学习实现集体理性，为所有驾驶主体带来益处。


<details>
  <summary>Details</summary>
Motivation: 探索自利的自动驾驶车辆是否能在混合交通系统中为所有驾驶主体带来益处，特别是在不直接包含系统级目标的情况下。

Method: 使用深度强化学习训练自利的交通主体，通过简单的奖励设计来研究集体理性的实现程度。

Result: 在各种场景中，集体理性持续出现，表明这一特性具有鲁棒性。通过模拟证据验证了微观动态环境中集体理性出现的机制。

Conclusion: 这项研究表明，可以利用先进学习方法（如联邦学习）在混合自主系统中实现自利驾驶主体之间的集体合作。

Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the
near future, leading to mixed autonomy traffic consisting of both AVs and
human-driven vehicles (HVs). Although numerous studies have shown that AVs can
be deployed to benefit the overall traffic system performance by incorporating
system-level goals into their decision making, it is not clear whether the
benefits still exist when agents act out of self-interest -- a trait common to
all driving agents, both human and autonomous. This study aims to understand
whether self-interested AVs can bring benefits to all driving agents in mixed
autonomy traffic systems. The research is centered on the concept of collective
rationality (CR). This concept, originating from game theory and behavioral
economics, means that driving agents may cooperate collectively even when
pursuing individual interests. Our recent research has proven the existence of
CR in an analytical game-theoretical model and empirically in mixed
human-driven traffic. In this paper, we demonstrate that CR can be attained
among driving agents trained using deep reinforcement learning (DRL) with a
simple reward design. We examine the extent to which self-interested traffic
agents can achieve CR without directly incorporating system-level objectives.
Results show that CR consistently emerges in various scenarios, which indicates
the robustness of this property. We also postulate a mechanism to explain the
emergence of CR in the microscopic and dynamic environment and verify it based
on simulation evidence. This research suggests the possibility of leveraging
advanced learning methods (such as federated learning) to achieve collective
cooperation among self-interested driving agents in mixed-autonomy systems.

</details>


### [34] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: 本文系统研究了无标签强化学习在不同规模模型上的表现，发现该方法严重依赖基础模型的推理能力，对较弱模型效果不佳。作者提出基于课程学习和数据筛选的改进方法，在所有模型规模上都取得了稳定提升。


<details>
  <summary>Details</summary>
Motivation: 探索无标签强化学习方法在不同规模模型上的泛化能力，特别是针对推理能力有限的小型模型，解决现有方法对基础模型推理能力依赖过强的问题。

Method: 提出基于课程学习的无标签强化学习方法，逐步引入更难的问题进行训练，屏蔽无多数投票的rollout，并建立数据筛选流程生成具有预定义难度的样本。

Result: 改进方法在所有模型规模（0.5B到7B参数）和推理能力上都表现出稳定提升，为资源受限模型的自举推理能力提供了可行路径。

Conclusion: 无标签强化学习的成功高度依赖基础模型的推理能力，通过课程学习和数据筛选可以有效提升该方法在小型模型上的表现，为实现更稳健的无监督强化学习提供了方向。

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [35] [Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale](https://arxiv.org/abs/2511.04904)
*Bassel Al Omari,Michael Matthews,Alexander Rutherford,Jakob Nicolaus Foerster*

Main category: cs.LG

TL;DR: 提出了Craftax-MA和Craftax-Coop两个多智能体强化学习基准环境，旨在评估长期依赖和泛化能力，现有算法在这些环境中面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MARL基准主要针对短期挑战，无法充分评估长期依赖和泛化能力，需要更全面的基准来推动MARL研究发展。

Method: 基于Craftax环境扩展开发了Craftax-MA（支持多智能体）和Craftax-Coop（引入异质智能体、交易等复杂合作机制），使用JAX实现以获得高速性能。

Result: Craftax-MA训练2.5亿次交互仅需不到1小时，现有算法在长期信用分配、探索和合作等关键挑战上表现不佳。

Conclusion: Craftax基准环境为MARL研究提供了更全面的评估平台，有望推动该领域的长期发展。

Abstract: Progress in multi-agent reinforcement learning (MARL) requires challenging
benchmarks that assess the limits of current methods. However, existing
benchmarks often target narrow short-horizon challenges that do not adequately
stress the long-term dependencies and generalization capabilities inherent in
many multi-agent systems. To address this, we first present
\textit{Craftax-MA}: an extension of the popular open-ended RL environment,
Craftax, that supports multiple agents and evaluates a wide range of general
abilities within a single environment. Written in JAX, \textit{Craftax-MA} is
exceptionally fast with a training run using 250 million environment
interactions completing in under an hour. To provide a more compelling
challenge for MARL, we also present \textit{Craftax-Coop}, an extension
introducing heterogeneous agents, trading and more mechanics that require
complex cooperation among agents for success. We provide analysis demonstrating
that existing algorithms struggle with key challenges in this benchmark,
including long-horizon credit assignment, exploration and cooperation, and
argue for its potential to drive long-term research in MARL.

</details>


### [36] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: 本文提出了一个oracle高效的在线算法，实现了对可引出属性的swap多校准，显著改进了现有界限，并解决了关于oracle高效算法能否达到√T ℓ2-均值多校准误差的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 现有关于多校准的研究在在线设置中效率较低，[NR23]提出的算法效率不高，且只适用于群体成员函数。本文旨在将多校准推广到任意有界假设类，并引入更强的swap多校准概念。

Method: 提出了一个oracle高效算法，当给定在线不可知学习器的访问权限时，该算法能够以高概率实现T^{1/(r+1)} ℓr-swap多校准误差，适用于具有有界顺序Rademacher复杂度的假设类和可引出属性Γ。

Result: 对于r=2的特殊情况，算法实现了T^{1/3} ℓ2-swap多校准误差，显著改进了[NR23, GMS25, LSS25a]中建立的界限，并肯定地回答了[GJRR24]中关于oracle高效算法能否达到√T ℓ2-均值多校准误差的开放性问题。

Conclusion: 本文成功将多校准从群体成员函数推广到任意有界假设类，引入了更强的swap多校准概念，并提出了高效的在线算法，在理论和算法效率方面都取得了重要进展。

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


### [37] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: 提出了Dual-Guided Loss (DGL)，一种用于决策导向学习的简单可扩展方法，通过利用下游问题的对偶变量来减少对求解器的依赖，同时保持决策对齐。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多决策都是在不确定性下通过使用预测数量解决优化问题来进行的。现有的决策导向学习方法要么需要频繁调用求解器，要么依赖特定任务的代理，这两种方法都计算昂贵且难以扩展。

Method: 利用下游问题的对偶变量来指导学习，构建DGL目标函数，特别针对具有自然多选一约束的组合选择问题（如匹配、背包、最短路径）。该方法将优化与梯度更新解耦，仅周期性求解下游问题，在刷新间隔使用简单可微的代理损失进行训练。

Result: DGL在两种问题类别上达到或超过了最先进的DFL方法性能，同时使用了更少的求解器调用和显著减少的训练时间。

Conclusion: DGL提供了一种简单、可扩展的决策导向学习方法，在保持强决策对齐的同时，将训练成本推向标准监督学习水平，具有渐近递减的决策遗憾。

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


### [38] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: 本文研究了机器学习算法与传统统计模型的创新整合方式，展示了混合模型在预测准确性、鲁棒性和可解释性方面的显著改进


<details>
  <summary>Details</summary>
Motivation: 探索机器学习与传统统计建模的新颖整合方式，改变数据分析、预测分析和决策制定的方法

Method: 研究机器学习与统计模型的连接，展示现代ML算法如何丰富传统模型，通过混合模型提升性能

Result: 混合模型在预测准确性、鲁棒性和可解释性方面取得重大改进

Conclusion: 机器学习与传统统计模型的整合带来了性能、规模、灵活性和鲁棒性的显著提升

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [39] [Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding](https://arxiv.org/abs/2511.04934)
*Hadi Reisizadeh,Jiajun Ruan,Yiwei Chen,Soumyadeep Pal,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 现有的大语言模型遗忘方法在确定性解码下看似成功，但在概率性解码下敏感信息会重新出现，表明这些方法未能实现真正的遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型遗忘对于合规性和构建伦理AI系统至关重要，但现有方法在实践中未能实现真正的知识移除。

Method: 引入leak@k元评估指标，量化在k个样本生成中被遗忘知识重新出现的可能性，并在TOFU、MUSE和WMDP三个基准上进行大规模系统研究。

Result: 研究发现知识泄露在各种方法和任务中持续存在，表明当前最先进的遗忘技术仅提供有限的遗忘效果。

Conclusion: 当前LLM遗忘方法存在严重漏洞，迫切需要开发更鲁棒的遗忘方法。

Abstract: Unlearning in large language models (LLMs) is critical for regulatory
compliance and for building ethical generative AI systems that avoid producing
private, toxic, illegal, or copyrighted content. Despite rapid progress, in
this work we show that \textit{almost all} existing unlearning methods fail to
achieve true forgetting in practice. Specifically, while evaluations of these
`unlearned' models under deterministic (greedy) decoding often suggest
successful knowledge removal using standard benchmarks (as has been done in the
literature), we show that sensitive information reliably resurfaces when models
are sampled with standard probabilistic decoding. To rigorously capture this
vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric
that quantifies the likelihood of forgotten knowledge reappearing when
generating $k$ samples from the model under realistic decoding strategies.
Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the
first large-scale, systematic study of unlearning reliability using our newly
defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge
leakage persists across methods and tasks, underscoring that current
state-of-the-art unlearning techniques provide only limited forgetting and
highlighting the urgent need for more robust approaches to LLM unlearning.

</details>


### [40] [Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression](https://arxiv.org/abs/2511.04937)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文分析了期望最大化(EM)算法在双分量混合线性回归中的结构特性、摆线轨迹和非渐近收敛保证，特别关注未知混合权重和回归参数的情况。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究在已知平衡权重和信噪比高的情况下建立了EM算法的全局收敛性，但在完全未知参数设置下的理论行为仍不清楚，其轨迹和收敛阶数尚未完全表征。

Method: 推导了2MLR在未知混合权重和回归参数下所有信噪比区域的显式EM更新表达式，分析了结构特性和摆线轨迹，建立了子最优角度的递推关系。

Result: 在无噪声情况下，回归参数的EM迭代轨迹形成摆线；在高信噪比区域量化了与摆线轨迹的偏差。分析揭示了收敛阶数：当估计值接近正交于真实值时线性收敛，当角度较小时二次收敛。

Conclusion: 本研究通过锐化有限样本与总体EM更新之间的统计误差界限，建立了非渐近收敛保证，为分析混合线性回归中的EM算法提供了一个新颖的基于轨迹的框架。

Abstract: This work investigates the structural properties, cycloid trajectories, and
non-asymptotic convergence guarantees of the Expectation-Maximization (EM)
algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing
weights and regression parameters. Recent studies have established global
convergence for 2MLR with known balanced weights and super-linear convergence
in noiseless and high signal-to-noise ratio (SNR) regimes. However, the
theoretical behavior of EM in the fully unknown setting remains unclear, with
its trajectory and convergence order not yet fully characterized. We derive
explicit EM update expressions for 2MLR with unknown mixing weights and
regression parameters across all SNR regimes and analyze their structural
properties and cycloid trajectories. In the noiseless case, we prove that the
trajectory of the regression parameters in EM iterations traces a cycloid by
establishing a recurrence relation for the sub-optimality angle, while in high
SNR regimes we quantify its discrepancy from the cycloid trajectory. The
trajectory-based analysis reveals the order of convergence: linear when the EM
estimate is nearly orthogonal to the ground truth, and quadratic when the angle
between the estimate and ground truth is small at the population level. Our
analysis establishes non-asymptotic guarantees by sharpening bounds on
statistical errors between finite-sample and population EM updates, relating
EM's statistical accuracy to the sub-optimality angle, and proving convergence
with arbitrary initialization at the finite-sample level. This work provides a
novel trajectory-based framework for analyzing EM in Mixed Linear Regression.

</details>


### [41] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: 本研究提出了一种结合机器学习和混合深度学习方法的心血管疾病风险预测模型，专门针对糖尿病患者。XGBoost和LSTM模型在BRFSS数据集上均达到了0.9050的最高准确率，某些模型甚至实现了完美的召回率。


<details>
  <summary>Details</summary>
Motivation: 随着糖尿病患病率不断上升及其与心脏病的强关联，需要开发高效的心血管疾病风险预测模型来改善医疗机构的临床决策。

Method: 使用BRFSS数据集，经过数据预处理（去重、缺失值处理、特征识别）和PCA特征提取。实施了多种ML模型（DT、RF、KNN、SVM、AdaBoost、XGBoost）和DL模型（ANN、DNN、RNN、CNN、LSTM、BiLSTM、GRU），以及CNN与LSTM、BiLSTM、GRU的混合模型。

Result: XGBoost和LSTM模型均达到最高准确率0.9050，某些模型实现了完美召回率1.00。高准确率和F1分数证明了这些模型的有效性。

Conclusion: 机器学习和深度学习模型在预测糖尿病患者心血管疾病风险方面表现出色，能够自动化和增强临床决策，改善个性化风险管理和预防策略。

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [42] [Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces](https://arxiv.org/abs/2511.04973)
*Siyuan Li,Yifan Sun,Lei Cheng,Lewen Wang,Yang Liu,Weiqing Liu,Jianlong Li,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: FAR-TS是一个用于多变量时间序列生成的框架，通过解耦因子化和自回归Transformer在离散量化潜在空间中实现快速、可控的任意长度序列生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的方法生成速度慢且仅限于固定长度窗口，需要一种能够快速生成任意长度多变量时间序列的方法。

Method: 将时间序列分解为捕获静态跨通道相关性的数据自适应基础和向量量化为离散标记的时间系数，然后使用LLaMA风格的自回归Transformer建模这些标记序列。

Result: FAR-TS相比Diffusion-TS实现了数量级更快的生成速度，同时保持了跨通道相关性和可解释的潜在空间。

Conclusion: FAR-TS提供了一个高效、灵活的时间序列合成框架，在保持高质量生成的同时显著提升了生成速度。

Abstract: Generative models for multivariate time series are essential for data
augmentation, simulation, and privacy preservation, yet current
state-of-the-art diffusion-based approaches are slow and limited to
fixed-length windows. We propose FAR-TS, a simple yet effective framework that
combines disentangled factorization with an autoregressive Transformer over a
discrete, quantized latent space to generate time series. Each time series is
decomposed into a data-adaptive basis that captures static cross-channel
correlations and temporal coefficients that are vector-quantized into discrete
tokens. A LLaMA-style autoregressive Transformer then models these token
sequences, enabling fast and controllable generation of sequences with
arbitrary length. Owing to its streamlined design, FAR-TS achieves
orders-of-magnitude faster generation than Diffusion-TS while preserving
cross-channel correlations and an interpretable latent space, enabling
high-quality and flexible time series synthesis.

</details>


### [43] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: 提出了一种可扩展的ROC-SVM变体，通过不完全U统计量和低秩核近似大幅降低计算复杂度，同时保持与原始ROC-SVM相当的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 原始ROC-SVM虽然能直接最大化AUC且在处理类别不平衡时具有优势，但其O(n²)的计算成本限制了实际应用。

Method: 使用不完全U统计量减少计算复杂度，并通过低秩核近似扩展到非线性分类，在再生核希尔伯特空间中进行高效训练。

Result: 理论分析建立了误差界证明近似合理性，实验结果表明所提方法在合成和真实数据集上达到与原始ROC-SVM相当的AUC性能，同时训练时间大幅减少。

Conclusion: 该方法成功解决了ROC-SVM的计算瓶颈问题，使其在实际应用中更加可行。

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [44] [Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk](https://arxiv.org/abs/2511.04980)
*Rongbin Ye,Jiaqi Chen*

Main category: cs.LG

TL;DR: 该论文研究了在金融行业中使用复杂机器学习模型与监管要求的可解释性之间的平衡问题，提出使用SHAP和LIME等可解释性框架来使高性能的"黑盒"模型达到监管要求的可解释性水平，并提出了一个五维评估框架来系统评估模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融行业面临一个关键挑战：如何在使用先进的机器学习模型（如神经网络）获得更好预测能力的同时，满足监管机构对模型可解释性的要求。现有研究在"黑盒"模型与可解释性框架的应用之间存在空白。

Method: 作者详细阐述了SHAP和LIME等可解释性框架在不同模型上的应用，并提出了一个新颖的五维评估框架，包括固有可解释性、全局解释、局部解释、一致性和复杂性，用于系统评估模型的可解释性。

Result: 研究表明，通过使用SHAP和LIME等现代可解释性技术，具有更好预测能力的复杂模型可以达到与简单模型相同的可解释性水平，从而可以在受监管的金融环境中应用高性能的机器学习模型。

Conclusion: 该研究证明了在受监管的金融环境中应用复杂高性能机器学习模型的可行性，通过现代可解释性技术可以平衡模型性能与可解释性之间的权衡，并提供了一个结构化方法来评估这种关键权衡。

Abstract: The financial industry faces a significant challenge modeling and risk
portfolios: balancing the predictability of advanced machine learning models,
neural network models, and explainability required by regulatory entities (such
as Office of the Comptroller of the Currency, Consumer Financial Protection
Bureau). This paper intends to fill the gap in the application between these
"black box" models and explainability frameworks, such as LIME and SHAP.
Authors elaborate on the application of these frameworks on different models
and demonstrates the more complex models with better prediction powers could be
applied and reach the same level of the explainability, using SHAP and LIME.
Beyond the comparison and discussion of performances, this paper proposes a
novel five dimensional framework evaluating Inherent Interpretability, Global
Explanations, Local Explanations, Consistency, and Complexity to offer a
nuanced method for assessing and comparing model explainability beyond simple
accuracy metrics. This research demonstrates the feasibility of employing
sophisticated, high performing ML models in regulated financial environments by
utilizing modern explainability techniques and provides a structured approach
to evaluate the crucial trade offs between model performance and
interpretability.

</details>


### [45] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: 提出零/一层渐进式训练方法，在深度学习中通过动态扩展模型深度来显著减少计算成本，同时保持几乎相同的性能表现。


<details>
  <summary>Details</summary>
Motivation: 模型深度是双刃剑：更深模型精度更高但计算成本也更高。渐进式训练通过训练过程中动态扩展模型容量来有效平衡计算效率和性能。

Method: 基于优化理论和特征学习研究深度扩展，提出零/一层渐进式训练，关注新层初始化、超参数迁移、学习率调度和模型扩展时机等关键问题。

Result: 在GPT2上应用零/一层渐进式训练可节省约80%计算量，相当于加速约5倍，同时达到与60层7B参数全训练模型几乎相同的损失。

Conclusion: 渐进式训练是实现深度学习模型高效扩展的有效策略，在显著降低计算成本的同时保持模型性能。

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [46] [Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding](https://arxiv.org/abs/2511.04984)
*Xinheng He,Yijia Zhang,Haowei Lin,Xingang Peng,Xiangzhe Kong,Mingyu Li,Jianzhu Ma*

Main category: cs.LG

TL;DR: Peptide2Mol是一个E(3)等变图神经网络扩散模型，通过参考原始肽结合剂及其周围蛋白口袋环境来生成小分子，在非自回归生成任务中达到最先进性能，并能进行分子优化和拟肽设计。


<details>
  <summary>Details</summary>
Motivation: 大多数AI驱动的基于结构药物设计方法忽略了内源性蛋白与肽相互作用的重要性，导致分子设计不理想。

Method: 使用E(3)等变图神经网络扩散模型，结合原始肽结合剂和蛋白口袋环境信息来生成小分子。

Result: 模型在非自回归生成任务中达到最先进性能，生成的分子与原始肽结合剂具有相似性，并能通过部分扩散过程进行分子优化和拟肽设计。

Conclusion: Peptide2Mol是一个有效的深度生成模型，可用于从蛋白结合口袋生成和优化生物活性小分子。

Abstract: Structure-based drug design has seen significant advancements with the
integration of artificial intelligence (AI), particularly in the generation of
hit and lead compounds. However, most AI-driven approaches neglect the
importance of endogenous protein interactions with peptides, which may result
in suboptimal molecule designs. In this work, we present Peptide2Mol, an
E(3)-equivariant graph neural network diffusion model that generates small
molecules by referencing both the original peptide binders and their
surrounding protein pocket environments. Trained on large datasets and
leveraging sophisticated modeling techniques, Peptide2Mol not only achieves
state-of-the-art performance in non-autoregressive generative tasks, but also
produces molecules with similarity to the original peptide binder.
Additionally, the model allows for molecule optimization and peptidomimetic
design through a partial diffusion process. Our results highlight Peptide2Mol
as an effective deep generative model for generating and optimizing bioactive
small molecules from protein binding pockets.

</details>


### [47] [Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models](https://arxiv.org/abs/2511.04988)
*Runsheng Ren,Jing Li,Yanxiu Li,Shixun Huang,Jun Shen,Wanqing Li,John Le,Sheng Wang*

Main category: cs.LG

TL;DR: 本文提出了一个综合混合框架，整合结构断点检测、小波信号去噪和三种深度学习模型，用于碳价格预测。实验表明PELT-WT-TCN模型表现最佳，相比现有基准模型显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 准确预测碳价格对能源市场决策、可持续能源规划和脱碳策略至关重要，但由于政策干预和市场冲击导致的结构断点和高频噪声，预测仍具挑战性。现有研究缺乏系统性评估，限制了模型的鲁棒性和泛化能力。

Method: 提出综合混合框架，集成结构断点检测（Bai-Perron、ICSS和PELT算法）、小波信号去噪和三种深度学习模型（LSTM、GRU、TCN），使用2007-2024年欧盟配额现货价格及外生特征构建单变量和多变量数据集进行对比评估。

Result: 实验结果显示，提出的PELT-WT-TCN模型实现了最高预测精度，相比最先进的基准模型（带断点和小波的LSTM），RMSE降低22.35%，MAE降低18.63%；相比原始LSTM模型，RMSE降低70.55%，MAE降低74.42%。

Conclusion: 研究强调了将结构意识和多尺度分解整合到深度学习架构中的价值，可显著提高碳价格预测和其他非平稳金融时间序列的准确性和可解释性。

Abstract: Accurately forecasting carbon prices is essential for informed energy market
decision-making, guiding sustainable energy planning, and supporting effective
decarbonization strategies. However, it remains challenging due to structural
breaks and high-frequency noise caused by frequent policy interventions and
market shocks. Existing studies, including the most recent baseline approaches,
have attempted to incorporate breakpoints but often treat denoising and
modeling as separate processes and lack systematic evaluation across advanced
deep learning architectures, limiting the robustness and the generalization
capability. To address these gaps, this paper proposes a comprehensive hybrid
framework that integrates structural break detection (Bai-Perron, ICSS, and
PELT algorithms), wavelet signal denoising, and three state-of-the-art deep
learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot
prices from 2007 to 2024 and exogenous features such as energy prices and
policy indicators, the framework constructs univariate and multivariate
datasets for comparative evaluation. Experimental results demonstrate that our
proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing
forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the
state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by
70.55% in RMSE and 74.42% in MAE compared to the original LSTM without
decomposition from the same baseline study. These findings underscore the value
of integrating structural awareness and multiscale decomposition into deep
learning architectures to enhance accuracy and interpretability in carbon price
forecasting and other nonstationary financial time series.

</details>


### [48] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: 提出BiPETE模型用于单疾病预测，结合旋转位置嵌入和正弦嵌入处理EHR数据中的时序依赖问题，在抑郁和PTSD队列中显著提升ASUD风险预测性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在EHR疾病风险预测中面临时序依赖建模挑战，特别是就诊间隔不规则和结构不统一的问题。

Method: BiPETE模型集成旋转位置嵌入编码相对就诊时间，正弦嵌入保持就诊顺序，无需大规模预训练，在两个心理健康队列上训练预测ASUD风险。

Result: BiPETE优于基线模型，在抑郁和PTSD队列中分别将AUPRC提高34%和50%，消融研究证实双位置编码策略的有效性。

Conclusion: 研究提供了一个实用且可解释的EHR疾病风险预测框架，识别出与ASUD风险相关的关键临床特征，为风险评估提供深入理解。

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [49] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: MAC-Flow是一个多智能体协调框架，通过流式表示学习联合行为，再蒸馏为去中心化单步策略，在保持协调性能的同时实现快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表达性和效率之间存在权衡：基于去噪扩散的方法能捕捉复杂协调但计算慢，而基于高斯策略的方法推理快但处理多智能体交互脆弱。

Method: 首先学习基于流的联合行为表示，然后将其蒸馏为去中心化的单步策略，既保持协调性又实现快速执行。

Result: 在4个基准测试、12个环境和34个数据集上，MAC-Flow比基于扩散的MARL方法推理速度快约14.5倍，同时保持良好性能，推理速度与基于高斯策略的离线MARL方法相当。

Conclusion: MAC-Flow有效解决了多智能体协调中性能与计算成本之间的权衡问题。

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [50] [OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data](https://arxiv.org/abs/2511.05028)
*Dongjin Park,Hasung Yeo,Joon-Woo Lee*

Main category: cs.LG

TL;DR: OvA-LP是一个用于联邦微调(FFT)的简约框架，通过线性探测和一对多头部设计，在源头抑制客户端数据异构导致的局部漂移问题，在极端非IID条件下保持95.9%的IID准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦微调在异构客户端数据分布下容易因局部漂移而脆弱，现有方法主要在事后校正漂移，在极端非IID条件下表现脆弱。

Method: OvA-LP结合冻结编码器上的线性探测与一对多头部，采用两阶段程序，保留预训练特征几何并解耦logits以防止漂移放大机制。

Result: 在CIFAR-100的100个客户端上，OvA-LP保持95.9%的IID准确率，而SOTA方法PFPT和FFT-MoE分别仅保持10.1%和34.5%。在对称和非对称标签噪声下也保持弹性。

Conclusion: OvA-LP为异构条件下的鲁棒联邦微调提供了原则性和高效的基础。

Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data
but remains fragile under heterogeneous client distributions due to local
drift, i.e., client-level update divergences that induce systematic bias and
amplified variance in the global model. Existing aggregation and
personalization methods largely correct drift post hoc, which proves brittle
under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework
that is, to our knowledge, the first explicitly designed to suppress drift at
its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing
on a frozen encoder with a one-vs-all head and a simple two-stage procedure,
preserving pretrained feature geometry and decoupling logits to prevent the
mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over
shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of
its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%
(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains
resilience under both symmetric and asymmetric label noise. In addition,
precomputing encoder features makes per-round cost nearly independent of
encoder size. Together, these results demonstrate that OvA-LP provides a
principled and efficient basis for robust FFT under heterogeneity.

</details>


### [51] [Usando LLMs para Programar Jogos de Tabuleiro e Variações](https://arxiv.org/abs/2511.05114)
*Álvaro Guglielmin Becker,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 测试三种大型语言模型（Claude、DeepSeek和ChatGPT）在创建棋盘游戏代码及现有游戏变体方面的能力


<details>
  <summary>Details</summary>
Motivation: 创建棋盘游戏程序通常耗时，而大型语言模型因其能够根据简单上下文信息高效生成代码，成为加速此过程的有吸引力的工具

Method: 提出一种方法来测试三种LLMs创建棋盘游戏代码及现有游戏变体的能力

Result: 论文未提供具体结果，但提出了测试框架

Conclusion: LLMs有潜力简化棋盘游戏代码创建过程，但需要系统评估其实际能力

Abstract: Creating programs to represent board games can be a time-consuming task.
Large Language Models (LLMs) arise as appealing tools to expedite this process,
given their capacity to efficiently generate code from simple contextual
information. In this work, we propose a method to test how capable three LLMs
(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as
new variants of existing games.

</details>


### [52] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出了一个新颖的时间序列问答数据集QuAnTS，专注于人体运动骨架轨迹数据，填补了时间序列问答研究的空白。


<details>
  <summary>Details</summary>
Motivation: 文本信息可以补充数值时间序列的密度，改善与时间序列模型的交互，增强可访问性和决策能力。当前问答研究主要集中在视觉和文本领域，时间序列问答研究严重不足。

Method: 创建了大规模的时间序列问答数据集QuAnTS，包含关于人体运动骨架轨迹的各种问答对，并通过广泛实验验证数据集的质量和完整性。

Result: 验证了QuAnTS数据集结构良好且全面，评估了现有和新提出的基线方法，为时间序列问答的深入研究奠定了基础，并提供了人类表现作为实用性的关键参考。

Conclusion: 该研究鼓励未来通过文本与时间序列模型交互的研究，以实现更好的决策和更透明的系统。

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [53] [DL101 Neural Network Outputs and Loss Functions](https://arxiv.org/abs/2511.05131)
*Fernando Berzal*

Main category: cs.LG

TL;DR: 该技术报告分析了神经网络输出层激活函数与损失函数之间的统计联系，揭示了损失函数选择等价于假设特定输出概率分布，并建立了与最大似然估计和广义线性模型的关联。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络输出层激活函数与损失函数之间的统计原理，为深度学习模型选择合适的损失函数提供理论依据。

Method: 分析常见激活函数（线性、sigmoid、ReLU、softmax）的数学特性，将损失函数（MSE、MAE、交叉熵）与最大似然估计原理联系起来。

Result: 建立了损失函数选择与输出概率分布假设的等价关系，揭示了神经网络输出层与广义线性模型的深层联系。

Conclusion: 损失函数的选择具有坚实的统计理论基础，应根据输出层激活函数和任务需求选择相应的损失函数，这种理解有助于更有效地训练深度学习模型。

Abstract: The loss function used to train a neural network is strongly connected to its
output layer from a statistical point of view. This technical report analyzes
common activation functions for a neural network output layer, like linear,
sigmoid, ReLU, and softmax, detailing their mathematical properties and their
appropriate use cases. A strong statistical justification exists for the
selection of the suitable loss function for training a deep learning model.
This report connects common loss functions such as Mean Squared Error (MSE),
Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical
principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss
function is equivalent to assuming a specific probability distribution for the
model output, highlighting the link between these functions and the Generalized
Linear Models (GLMs) that underlie network output layers. Additional scenarios
of practical interest are also considered, such as alternative output
encodings, constrained outputs, and distributions with heavy tails.

</details>


### [54] [Consecutive Preferential Bayesian Optimization](https://arxiv.org/abs/2511.05163)
*Aras Erarslan,Carlos Sevilla Salcedo,Ville Tanskanen,Anni Nisov,Eero Päiväkumpu,Heikki Aisala,Kaisu Honkapää,Arto Klami,Petrus Mikkola*

Main category: cs.LG

TL;DR: 提出了连续偏好贝叶斯优化方法，通过约束比较涉及先前生成的候选方案来降低生产成本，并考虑评估者的感知模糊性，在具有高生产成本或模糊反馈的设置中显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法忽略了生成候选解的成本，且未考虑评估者对小效用差异的感知模糊性，这在实际应用中会影响优化效率和准确性。

Method: 提出连续偏好贝叶斯优化，约束比较涉及先前生成的候选方案以减少生产成本；引入可察觉差异阈值到概率偏好模型中，捕捉对小效用差异的模糊性；采用信息论获取策略选择对未知最优解最具信息量的新配置。

Result: 在高生产成本或模糊反馈设置中，该方法显著提高了优化准确性。

Conclusion: 连续偏好贝叶斯优化通过考虑生产成本和评估者感知模糊性，在偏好优化中实现了更高的效率和准确性。

Abstract: Preferential Bayesian optimization allows optimization of objectives that are
either expensive or difficult to measure directly, by relying on a minimal
number of comparative evaluations done by a human expert. Generating candidate
solutions for evaluation is also often expensive, but this cost is ignored by
existing methods. We generalize preference-based optimization to explicitly
account for production and evaluation costs with Consecutive Preferential
Bayesian Optimization, reducing production cost by constraining comparisons to
involve previously generated candidates. We also account for the perceptual
ambiguity of the oracle providing the feedback by incorporating a
Just-Noticeable Difference threshold into a probabilistic preference model to
capture indifference to small utility differences. We adapt an
information-theoretic acquisition strategy to this setting, selecting new
configurations that are most informative about the unknown optimum under a
preference model accounting for the perceptual ambiguity. We empirically
demonstrate a notable increase in accuracy in setups with high production costs
or with indifference feedback.

</details>


### [55] [Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy](https://arxiv.org/abs/2511.05169)
*Simon Baur,Tristan Ruhwedel,Ekin Böke,Zuzanna Kobus,Gergana Lishkova,Christoph Wetz,Holger Amthauer,Christoph Roderburg,Frank Tacke,Julian M. Rogasch,Wojciech Samek,Henning Jann,Jackie Ma,Johannes Eschrich*

Main category: cs.LG

TL;DR: 本研究评估了实验室、影像和多模态深度学习模型在预测PRRT治疗患者无进展生存期(PFS)方面的表现，发现结合SR-PET、CT和实验室生物标志物的多模态融合模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 肽受体放射性核素治疗(PRRT)是转移性神经内分泌肿瘤的成熟治疗方法，但仅部分患者能获得长期疾病控制。预测PFS可支持个体化治疗规划。

Method: 回顾性单中心研究纳入116例接受177Lu-DOTATOC治疗的转移性NET患者。收集临床特征、实验室值和治疗前SR-PET/CT数据。训练7个模型分类低vs高PFS组，包括单模态和多模态融合方法。

Result: 仅基于实验室生物标志物的随机森林模型AUROC为0.59，单模态3D CNN使用SR-PET或CT表现更差(AUROC分别为0.42和0.54)。多模态融合模型(实验室值+SR-PET+CT+预训练CT分支)效果最佳(AUROC 0.72, AUPRC 0.80)。

Conclusion: 结合SR-PET、CT和实验室生物标志物的多模态深度学习在PRRT后PFS预测方面优于单模态方法。经外部验证后，此类模型可能支持风险适应的随访策略。

Abstract: Peptide receptor radionuclide therapy (PRRT) is an established treatment for
metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs
only in a subset of patients. Predicting progression-free survival (PFS) could
support individualized treatment planning. This study evaluates laboratory,
imaging, and multimodal deep learning models for PFS prediction in PRRT-treated
patients. In this retrospective, single-center study 116 patients with
metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical
characteristics, laboratory values, and pretherapeutic somatostatin receptor
positron emission tomography/computed tomographies (SR-PET/CT) were collected.
Seven models were trained to classify low- vs. high-PFS groups, including
unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches.
Explainability was evaluated by feature importance analysis and gradient maps.
Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1
year). Groups were similar in most characteristics, except for higher baseline
chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT
cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only
on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal
three-dimensional convolutional neural networks using SR-PET or CT performed
worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion
model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch
- achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01).
Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers
outperformed unimodal approaches for PFS prediction after PRRT. Upon external
validation, such models may support risk-adapted follow-up strategies.

</details>


### [56] [Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models](https://arxiv.org/abs/2511.05171)
*Davide Marincione,Donato Crisostomi,Roberto Dessi,Emanuele Rodolà,Emanuele Rossi*

Main category: cs.LG

TL;DR: NatureLM在生物声学领域表现出色，但在指令跟随灵活性方面存在权衡。通过将其与基础语言模型简单融合，可以恢复指令跟随能力，同时保持领域专业知识，并在零样本分类上实现显著改进。


<details>
  <summary>Details</summary>
Motivation: NatureLM在生物声学基准测试中表现良好，但在处理复杂指令时准确性显著下降，特别是当同时请求通用名和学名时。这揭示了领域特定微调与指令跟随灵活性之间的权衡问题。

Method: 采用简单的模型融合策略，将NatureLM与其基础语言模型进行插值，以恢复指令跟随能力，同时最小化领域专业知识的损失。

Result: 融合后的模型在指令跟随能力方面得到恢复，并且在零样本泛化方面表现出显著更强的性能，在未见物种的闭集零样本分类中实现了超过200%的相对改进，创造了新的最先进水平。

Conclusion: 模型融合策略有效解决了领域特定微调与指令跟随灵活性之间的权衡问题，既能保持生物声学领域的专业知识，又能恢复通用指令跟随能力，并在零样本泛化方面实现突破性改进。

Abstract: Foundation models capable of generalizing across species and tasks represent
a promising new frontier in bioacoustics, with NatureLM being one of the most
prominent examples. While its domain-specific fine-tuning yields strong
performance on bioacoustic benchmarks, we observe that it also introduces
trade-offs in instruction-following flexibility. For instance, NatureLM
achieves high accuracy when prompted for either the common or scientific name
individually, but its accuracy drops significantly when both are requested in a
single prompt. We address this by applying a simple model merging strategy that
interpolates NatureLM with its base language model, recovering
instruction-following capabilities with minimal loss of domain expertise.
Finally, we show that the merged model exhibits markedly stronger zero-shot
generalization, achieving over a 200% relative improvement and setting a new
state-of-the-art in closed-set zero-shot classification of unseen species.

</details>


### [57] [Associative Poisoning to Generative Machine Learning](https://arxiv.org/abs/2511.05177)
*Mathias Lundteigen Mohus,Jingyue Li,Zhirong Yang*

Main category: cs.LG

TL;DR: 提出了一种名为关联性投毒的新型数据投毒技术，通过扰动训练数据来操纵生成输出中特定特征对之间的统计关联，无需控制训练过程，且能保持高质量输出和逃避视觉检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型如Stable Diffusion和ChatGPT的广泛采用使其成为恶意利用的目标，现有投毒攻击要么导致生成数据质量广泛下降，要么需要控制训练过程，限制了实际应用。

Method: 关联性投毒攻击仅扰动训练数据来操纵生成输出中特定特征对之间的统计关联，提供了正式数学公式并证明了理论可行性和隐蔽性。

Result: 在两个最先进生成模型上的实证评估表明，关联性投毒能有效诱导或抑制特征关联，同时保持目标特征的边缘分布和高质量输出，从而逃避视觉检测。

Conclusion: 生成系统在图像合成、合成数据集生成和自然语言处理中容易受到这种隐蔽操纵的影响，作者还提出了新的对抗措施策略来应对这一风险。

Abstract: The widespread adoption of generative models such as Stable Diffusion and
ChatGPT has made them increasingly attractive targets for malicious
exploitation, particularly through data poisoning. Existing poisoning attacks
compromising synthesised data typically either cause broad degradation of
generated data or require control over the training process, limiting their
applicability in real-world scenarios. In this paper, we introduce a novel data
poisoning technique called associative poisoning, which compromises
fine-grained features of the generated data without requiring control of the
training process. This attack perturbs only the training data to manipulate
statistical associations between specific feature pairs in the generated
outputs. We provide a formal mathematical formulation of the attack and prove
its theoretical feasibility and stealthiness. Empirical evaluations using two
state-of-the-art generative models demonstrate that associative poisoning
effectively induces or suppresses feature associations while preserving the
marginal distributions of the targeted features and maintaining high-quality
outputs, thereby evading visual detection. These results suggest that
generative systems used in image synthesis, synthetic dataset generation, and
natural language processing are susceptible to subtle, stealthy manipulations
that compromise their statistical integrity. To address this risk, we examine
the limitations of existing defensive strategies and propose a novel
countermeasure strategy.

</details>


### [58] [No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models](https://arxiv.org/abs/2511.05179)
*Ragini Gupta,Naman Raina,Bo Chen,Li Chen,Claudiu Danilov,Josh Eckhardt,Keyshla Bernard,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 本研究系统评估了不同时空预测模型在传感器部署密度和采样频率变化下的表现，发现STGNN在稀疏部署时表现最佳，而TSFM在高频采样时竞争力强，Moirai模型因能学习跨传感器依赖关系而表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有物联网环境感知技术主要关注边缘数据收集优化，但忽略了采样频率和空间覆盖变化对下游预测模型性能的影响，特别是不同模型架构与这些因素之间的相互作用尚未得到充分研究。

Method: 使用真实世界无线传感器网络的温度数据，系统研究经典模型(VAR)、神经网络(GRU、Transformer)、时空图神经网络(STGNNs)和时间序列基础模型(TSFMs: Chronos、Moirai、TimesFM)在不同空间传感器节点密度和采样间隔下的表现。

Result: STGNN在传感器部署稀疏且采样率适中时最有效，通过编码图结构利用空间相关性补偿有限覆盖；TSFM在高频时表现竞争力强但在空间覆盖减少时性能下降；多变量TSFM Moirai通过学习跨传感器依赖关系在所有模型中表现最佳。

Conclusion: 研究结果为构建高效的时空系统预测管道提供了可操作的见解，所有代码和数据集已开源以确保可复现性。

Abstract: Modern IoT deployments for environmental sensing produce high volume
spatiotemporal data to support downstream tasks such as forecasting, typically
powered by machine learning models. While existing filtering and strategic
deployment techniques optimize collected data volume at the edge, they overlook
how variations in sampling frequencies and spatial coverage affect downstream
model performance. In many forecasting models, incorporating data from
additional sensors denoise predictions by providing broader spatial contexts.
This interplay between sampling frequency, spatial coverage and different
forecasting model architectures remain underexplored. This work presents a
systematic study of forecasting models - classical models (VAR), neural
networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),
and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under
varying spatial sensor nodes density and sampling intervals using real-world
temperature data in a wireless sensor network. Our results show that STGNNs are
effective when sensor deployments are sparse and sampling rate is moderate,
leveraging spatial correlations via encoded graph structure to compensate for
limited coverage. In contrast, TSFMs perform competitively at high frequencies
but degrade when spatial coverage from neighboring sensors is reduced.
Crucially, the multivariate TSFM Moirai outperforms all models by natively
learning cross-sensor dependencies. These findings offer actionable insights
for building efficient forecasting pipelines in spatio-temporal systems. All
code for model configurations, training, dataset, and logs are open-sourced for
reproducibility:
https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models

</details>


### [59] [Linear Gradient Prediction with Control Variates](https://arxiv.org/abs/2511.05187)
*Kamil Ciosek,Nicolò Felicioni,Juan Elenter Litwin*

Main category: cs.LG

TL;DR: 提出一种使用近似预测梯度替代昂贵反向传播的新训练方法，通过控制变量技术确保更新的无偏性，基于神经正切核理论推导梯度预测器，在视觉Transformer分类任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 降低神经网络训练成本，避免昂贵的反向传播计算。

Method: 使用近似预测梯度替代完整梯度，基于控制变量技术确保更新无偏性，利用神经正切核理论推导梯度预测器。

Result: 在视觉Transformer分类任务中验证了方法的有效性。

Conclusion: 提出的近似梯度训练方法能够有效降低训练成本，同时保持性能。

Abstract: We propose a new way of training neural networks, with the goal of reducing
training cost. Our method uses approximate predicted gradients instead of the
full gradients that require an expensive backward pass. We derive a
control-variate-based technique that ensures our updates are unbiased estimates
of the true gradient. Moreover, we propose a novel way to derive a predictor
for the gradient inspired by the theory of the Neural Tangent Kernel. We
empirically show the efficacy of the technique on a vision transformer
classification task.

</details>


### [60] [ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy](https://arxiv.org/abs/2511.05221)
*David Bertram,Anja Ophey,Sinah Röttgen,Konstantin Kuffer,Gereon R. Fink,Elke Kalbe,Clint Hansen,Walter Maetzler,Maximilian Kapsecker,Lara M. Reimer,Stephan Jonas,Andreas T. Damgaard,Natasha B. Bertelsen,Casper Skjaerbaek,Per Borghammer,Karolien Groenewald,Pietro-Luca Ratti,Michele T. Hu,Noémie Moreau,Michael Sommerauer,Katarzyna Bozek*

Main category: cs.LG

TL;DR: ActiTect是一个全自动开源机器学习工具，用于通过腕部活动记录仪检测快速眼动睡眠行为障碍（RBD），在多个数据集上表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: iRBD是α-突触核蛋白病的重要前驱标志，腕部活动记录仪在大规模筛查中具有潜力，但需要可靠的分析流程。

Method: 开发包含稳健预处理和自动睡眠-觉醒检测的流程，提取生理可解释的运动特征，使用机器学习模型识别RBD。

Result: 在78人队列中AUROC=0.95，本地测试集(n=31)AUROC=0.86，两个外部队列分别达到AUROC=0.84和0.94，跨数据集验证表现稳定。

Conclusion: ActiTect作为开源工具促进广泛采用，推动建立统一且可泛化的RBD检测模型。

Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major
prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical
onset of Parkinson's disease, dementia with Lewy bodies, or multiple system
atrophy. While wrist-worn actimeters hold significant potential for detecting
RBD in large-scale screening efforts by capturing abnormal nocturnal movements,
they become inoperable without a reliable and efficient analysis pipeline. This
study presents ActiTect, a fully automated, open-source machine learning tool
to identify RBD from actigraphy recordings. To ensure generalizability across
heterogeneous acquisition settings, our pipeline includes robust preprocessing
and automated sleep-wake detection to harmonize multi-device data and extract
physiologically interpretable motion features characterizing activity patterns.
Model development was conducted on a cohort of 78 individuals, yielding strong
discrimination under nested cross-validation (AUROC = 0.95). Generalization was
confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two
independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To
assess real-world robustness, leave-one-dataset-out cross-validation across the
internal and external cohorts demonstrated consistent performance (AUROC range
= 0.84-0.89). A complementary stability analysis showed that key predictive
features remained reproducible across datasets, supporting the final pooled
multi-center model as a robust pre-trained resource for broader deployment. By
being open-source and easy to use, our tool promotes widespread adoption and
facilitates independent validation and collaborative improvements, thereby
advancing the field toward a unified and generalizable RBD detection model
using wearable devices.

</details>


### [61] [The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss](https://arxiv.org/abs/2511.05236)
*Rui Wu,Lizheng Wang,Yongjun Li*

Main category: cs.LG

TL;DR: 提出了BELM-MDCM框架，通过消除结构重建误差(SRE)实现因果信息守恒，解决了扩散模型在反事实推理中的信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 解决结构因果模型(SCMs)中复杂非线性机制的外生噪声精确推断的计算挑战，扩散模型虽然强大但存在固有的信息丢失问题。

Method: 开发了BELM-MDCM框架，通过分析可逆机制消除SRE，采用目标建模策略进行结构正则化，混合训练目标注入因果归纳偏置。

Result: 实验表明该框架不仅达到最先进精度，更重要的是实现了高保真度的个体层面反事实推理。

Conclusion: 该工作为现代生成模型与经典因果理论的融合提供了基础蓝图，为这一新兴领域建立了更严格的标准。

Abstract: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for
counterfactual reasoning hinges on faithful abduction: the precise inference of
latent exogenous noise. For decades, operationalizing this step for complex,
non-linear mechanisms has remained a significant computational challenge. The
advent of diffusion models, powerful universal function approximators, offers a
promising solution. However, we argue that their standard design, optimized for
perceptual generation over logical inference, introduces a fundamental flaw for
this classical problem: an inherent information loss we term the Structural
Reconstruction Error (SRE). To address this challenge, we formalize the
principle of Causal Information Conservation (CIC) as the necessary condition
for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based
framework engineered to be causally sound by eliminating SRE by construction
through an analytically invertible mechanism. To operationalize this framework,
a Targeted Modeling strategy provides structural regularization, while a Hybrid
Training Objective instills a strong causal inductive bias. Rigorous
experiments demonstrate that our Zero-SRE framework not only achieves
state-of-the-art accuracy but, more importantly, enables the high-fidelity,
individual-level counterfactuals required for deep causal inquiries. Our work
provides a foundational blueprint that reconciles the power of modern
generative models with the rigor of classical causal theory, establishing a new
and more rigorous standard for this emerging field.

</details>


### [62] [An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones](https://arxiv.org/abs/2511.05265)
*Taihelong Zeng,Yun Lin,Yuhe Shi,Yan Li,Zhiqing Wei,Xuanru Ji*

Main category: cs.LG

TL;DR: 提出了一种基于分层Actor-Critic深度强化学习的Transformer架构，用于解决卡车-无人机协同旅行商问题(TSP-D)，在计算时间和训练效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 卡车-无人机协同系统在最后一公里物流中具有重要应用价值，但TSP-D问题具有NP-hard组合复杂性，传统优化方法难以有效解决，需要新的深度强化学习框架。

Method: 采用分层Actor-Critic深度强化学习框架，包含Transformer编码器（使用优化的k近邻稀疏注意力机制）和最小门控单元解码器，在异步优势Actor-Critic范式下运行。

Result: 在N=10到100的不同规模TSP-D基准实例上，相比高性能启发式算法和现有强化学习方法，能在更短平均计算时间内获得竞争性甚至更优解，同时显著减少总训练时间。

Conclusion: 所提出的框架在解决TSP-D问题上表现出卓越的训练效率和计算性能，为复杂物流优化问题提供了有效的深度强化学习解决方案。

Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has
positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal
extension of classical routing optimization, where synchronized vehicle
coordination promises substantial operational efficiency and reduced
environmental impact, yet introduces NP-hard combinatorial complexity beyond
the reach of conventional optimization paradigms. Deep reinforcement learning
offers a theoretically grounded framework to address TSP-D's inherent
challenges through self-supervised policy learning and adaptive
decision-making. This study proposes a hierarchical Actor-Critic deep
reinforcement learning framework for solving the TSP-D problem. The
architecture consists of two primary components: a Transformer-inspired encoder
and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel,
optimized k-nearest neighbors sparse attention mechanism specifically for
focusing on relevant spatial relationships, further enhanced by the integration
of global node features. The Minimal Gated Unit decoder processes these encoded
representations to efficiently generate solution sequences. The entire
framework operates within an asynchronous advantage actor-critic paradigm.
Experimental results show that, on benchmark TSP-D instances of various scales
(N=10 to 100), the proposed model can obtain competitive or even superior
solutions in shorter average computation times compared to high-performance
heuristic algorithms and existing reinforcement learning methods. Moreover,
compared to advanced reinforcement learning algorithm benchmarks, the proposed
framework significantly reduces the total training time required while
achieving superior final performance, highlighting its notable advantage in
training efficiency.

</details>


### [63] [Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage](https://arxiv.org/abs/2511.05266)
*Gabriel Serrão Seabra,Nikolaj T. Mücke,Vinicius Luiz Santos Silva,Alexandre A. Emerick,Denis Voskov,Femke Vossepoel*

Main category: cs.LG

TL;DR: 本文提出了一个机器学习增强的数据同化框架，结合基于分数的扩散模型和机器学习增强的定位方法，用于地质碳储存项目中通道化储层的CO₂注入过程，以提高不确定性量化的可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确表征地下非均质性对于地质碳储存项目的安全有效实施至关重要。传统方法在通道化储层中面临挑战，需要改进数据同化技术来更好地量化不确定性。

Method: 使用基于分数的扩散模型生成渗透率场，结合机器学习算法计算状态变量，构建大样本集合（Ns=5000），并采用机器学习增强的定位框架来改进ESMDA的协方差估计。

Result: 基于机器学习的定位方法在保持可比数据匹配质量的同时，显著维持了更多的集合方差，相比未应用定位时效果更好。

Conclusion: 该框架对地质碳储存项目具有实际意义，有助于提高风险评估中不确定性量化的可靠性。

Abstract: Accurate characterization of subsurface heterogeneity is important for the
safe and effective implementation of geological carbon storage (GCS) projects.
This paper explores how machine learning methods can enhance data assimilation
for GCS with a framework that integrates score-based diffusion models with
machine learning-enhanced localization in channelized reservoirs during CO$_2$
injection. We employ a machine learning-enhanced localization framework that
uses large ensembles ($N_s = 5000$) with permeabilities generated by the
diffusion model and states computed by simple ML algorithms to improve
covariance estimation for the Ensemble Smoother with Multiple Data Assimilation
(ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability
fields, generated with the geostatistical model FLUVSIM. Our approach is
applied on a CO$_2$ injection scenario simulated using the Delft Advanced
Research Terra Simulator (DARTS). Our ML-based localization maintains
significantly more ensemble variance than when localization is not applied,
while achieving comparable data-matching quality. This framework has practical
implications for GCS projects, helping improve the reliability of uncertainty
quantification for risk assessment.

</details>


### [64] [Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting](https://arxiv.org/abs/2511.05289)
*Marius Fracarolli,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 本研究探索了数据增强如何减轻时间序列预测模型中的成员推理攻击，发现通过ZOO-PCA方法生成合成数据重训练模型，能在不牺牲测试性能的前提下显著降低攻击者的真阳性/假阳性比。


<details>
  <summary>Details</summary>
Motivation: 在电子健康记录的时间序列预测任务中，需要在强隐私保护和高预测性能之间取得平衡。成员推理攻击会泄露训练数据隐私，因此需要找到有效防御方法。

Method: 研究了多种数据增强策略：零阶优化(ZOO)、PCA约束的ZOO变体(ZOO-PCA)和MixUp，通过生成合成数据重训练模型来增强模型对成员推理攻击的抵抗力。

Result: 实验结果显示，ZOO-PCA方法在保持测试数据性能的同时，对成员推理攻击的TPR/FPR比率降低效果最好。

Conclusion: 数据增强特别是ZOO-PCA方法能有效防御成员推理攻击，在保护隐私的同时维持模型预测性能。

Abstract: Balancing strong privacy guarantees with high predictive performance is
critical for time series forecasting (TSF) tasks involving Electronic Health
Records (EHR). In this study, we explore how data augmentation can mitigate
Membership Inference Attacks (MIA) on TSF models. We show that retraining with
synthetic data can substantially reduce the effectiveness of loss-based MIAs by
reducing the attacker's true-positive to false-positive ratio. The key
challenge is generating synthetic samples that closely resemble the original
training data to confuse the attacker, while also introducing enough novelty to
enhance the model's ability to generalize to unseen data. We examine multiple
augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO
constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to
strengthen model resilience without sacrificing accuracy. Our experimental
results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA
attacks without sacrificing performance on test data.

</details>


### [65] [Attention and Compression is all you need for Controllably Efficient Language Models](https://arxiv.org/abs/2511.05313)
*Jatin Prakash,Aahlad Puli,Rajesh Ranganath*

Main category: cs.LG

TL;DR: CAT（Compress & Attend Transformer）是一种高效Transformer架构，通过压缩和分块注意力机制，在保持密集注意力质量的同时显著降低计算和内存成本，支持测试时自适应调整质量-计算权衡。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力方法（稀疏注意力、滑动窗口、卷积等）往往需要在质量和效率之间权衡，特别是上下文召回性能会下降，且需要预先固定质量-计算权衡，无法适应不同下游应用的需求。

Method: 使用两个简单组件：密集注意力和压缩。通过将序列分块，在解码时仅关注压缩后的历史块，从而减少序列长度。支持多分块大小训练，实现测试时自适应调整。

Result: 在语言建模、上下文召回和长上下文理解任务中，单一自适应CAT模型在不同计算-内存预算下均优于现有高效基线，包括混合架构。与密集Transformer相比，语言建模性能相当但速度快1.4-3倍，总内存使用降低2-9倍。

Conclusion: CAT提供了一种简单而有效的解决方案，通过压缩和分块注意力机制实现了质量与效率的良好平衡，支持测试时自适应调整，无需重新训练即可适应不同应用需求。

Abstract: The quadratic cost of attention in transformers motivated the development of
efficient approaches: namely sparse and sliding window attention, convolutions
and linear attention. Although these approaches result in impressive reductions
in compute and memory, they often trade-off with quality, specifically
in-context recall performance. Moreover, apriori fixing this quality-compute
tradeoff means being suboptimal from the get-go: some downstream applications
require more memory for in-context recall, while others require lower latency
and memory. Further, these approaches rely on heuristic choices that
artificially restrict attention, or require handcrafted and complex recurrent
state update rules, or they must be carefully composed with attention at
specific layers to form a hybrid architecture that complicates the design
process, especially at scale. To address above issues, we propose Compress &
Attend Transformer (CAT), a conceptually simple architecture employing two
simple ingredients only: dense attention and compression. CAT decodes chunks of
tokens by attending to compressed chunks of the sequence so far. Compression
results in decoding from a reduced sequence length that yields compute and
memory savings, while choosing a particular chunk size trades-off quality for
efficiency. Moreover, CAT can be trained with multiple chunk sizes at once,
unlocking control of quality-compute trade-offs directly at test-time without
any retraining, all in a single adaptive architecture. In exhaustive
evaluations on common language modeling tasks, in-context recall, and
long-context understanding, a single adaptive CAT model outperforms existing
efficient baselines, including hybrid architectures, across different
compute-memory budgets. Further, a single CAT matches dense transformer in
language modeling across model scales while being 1.4-3x faster and requiring
2-9x lower total memory usage.

</details>


### [66] [Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval](https://arxiv.org/abs/2511.05325)
*Janet Jenq,Hongda Shen*

Main category: cs.LG

TL;DR: 提出一种通过在产品图片上渲染相关文本内容来增强视觉-文本对齐的方法，提升多模态产品检索性能


<details>
  <summary>Details</summary>
Motivation: 多模态产品检索系统依赖视觉和文本信号的结合，但现有视觉语言模型容易受到排版攻击的影响，导致预测偏差

Method: 通过在产品图片上直接渲染标题、描述等文本内容，进行视觉-文本压缩，强化图像-文本对齐

Result: 在三个垂直电商数据集上评估，使用六个最先进的视觉基础模型，在单模态和多模态检索准确率方面均获得一致提升

Conclusion: 在产品图片上视觉化渲染产品元数据是一种简单而有效的增强方法，可提升电商应用中零样本多模态检索性能

Abstract: Multimodal product retrieval systems in e-commerce platforms rely on
effectively combining visual and textual signals to improve search relevance
and user experience. However, vision-language models such as CLIP are
vulnerable to typographic attacks, where misleading or irrelevant text embedded
in images skews model predictions. In this work, we propose a novel method that
reverses the logic of typographic attacks by rendering relevant textual content
(e.g., titles, descriptions) directly onto product images to perform
vision-text compression, thereby strengthening image-text alignment and
boosting multimodal product retrieval performance. We evaluate our method on
three vertical-specific e-commerce datasets (sneakers, handbags, and trading
cards) using six state-of-the-art vision foundation models. Our experiments
demonstrate consistent improvements in unimodal and multimodal retrieval
accuracy across categories and model families. Our findings suggest that
visually rendering product metadata is a simple yet effective enhancement for
zero-shot multimodal retrieval in e-commerce applications.

</details>


### [67] [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](https://arxiv.org/abs/2511.05330)
*Jan-Hendrik Ewering,Robin E. Herrmann,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: cs.LG

TL;DR: 本文提出了一种基于非保守哈密顿高斯过程的动力学学习方法，能够在仅有输入-输出数据的情况下学习物理一致的模型，无需速度或动量测量数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要速度或动量数据，但这些数据在实践中很少可用。本文旨在解决从输入-输出数据学习物理一致模型的实际问题。

Method: 采用非保守哈密顿高斯过程，结合降秩GP近似来提高计算效率，提供完全贝叶斯方案来估计隐藏状态、GP超参数和结构超参数的概率密度。

Result: 在非线性仿真案例中评估了所提方法，并与依赖动量测量的最先进方法进行了比较。

Conclusion: 该方法能够在没有速度或动量数据的情况下有效学习物理一致的动力学模型，具有实际应用价值。

Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws,
in learning-based approaches is a key motive to construct physically consistent
models from limited data, relevant for, e.g., model-based control. Recent work
incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to
obtain uncertainty-quantifying models that adhere to the underlying physical
principles. However, these works rely on velocity or momentum data, which is
rarely available in practice. In this paper, we consider dynamics learning with
non-conservative Hamiltonian GPs, and address the more realistic problem
setting of learning from input-output data. We provide a fully Bayesian scheme
for estimating probability densities of unknown hidden states, of GP
hyperparameters, as well as of structural hyperparameters, such as damping
coefficients. Considering the computational complexity of GPs, we take
advantage of a reduced-rank GP approximation and leverage its properties for
computationally efficient prediction and training. The proposed method is
evaluated in a nonlinear simulation case study and compared to a
state-of-the-art approach that relies on momentum measurements.

</details>


### [68] [SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning](https://arxiv.org/abs/2511.05355)
*Tzu-Yuan Huang,Armin Lederer,Dai-Jie Wu,Xiaobing Dai,Sihua Zhang,Stefan Sosnowski,Shao-Hua Sun,Sandra Hirche*

Main category: cs.LG

TL;DR: SAD-Flower是一个新颖的框架，通过引入虚拟控制输入和利用非线性控制理论技术，为流匹配方法提供状态约束、动作约束和动态一致性的形式化保证，无需重新训练即可满足未见约束。


<details>
  <summary>Details</summary>
Motivation: 流匹配方法在数据驱动规划中表现良好，但缺乏对状态和动作约束的形式化保证，且不确保动态一致性，这可能导致轨迹不可执行，影响系统的安全性和可接受性。

Method: 通过增强流与虚拟控制输入，利用非线性控制理论技术推导原则性指导，为状态约束、动作约束和动态一致性提供形式化保证，无需重新训练即可在测试时满足未见约束。

Result: 在多个任务上的广泛实验表明，SAD-Flower在确保约束满足方面优于各种基于生成模型的基线方法。

Conclusion: SAD-Flower框架成功解决了流匹配方法在安全约束和动态一致性方面的不足，为数据驱动规划提供了具有形式化保证的解决方案。

Abstract: Flow matching (FM) has shown promising results in data-driven planning.
However, it inherently lacks formal guarantees for ensuring state and action
constraints, whose satisfaction is a fundamental and crucial requirement for
the safety and admissibility of planned trajectories on various systems.
Moreover, existing FM planners do not ensure the dynamical consistency, which
potentially renders trajectories inexecutable. We address these shortcomings by
proposing SAD-Flower, a novel framework for generating Safe, Admissible, and
Dynamically consistent trajectories. Our approach relies on an augmentation of
the flow with a virtual control input. Thereby, principled guidance can be
derived using techniques from nonlinear control theory, providing formal
guarantees for state constraints, action constraints, and dynamic consistency.
Crucially, SAD-Flower operates without retraining, enabling test-time
satisfaction of unseen constraints. Through extensive experiments across
several tasks, we demonstrate that SAD-Flower outperforms various
generative-model-based baselines in ensuring constraint satisfaction.

</details>


### [69] [Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media](https://arxiv.org/abs/2511.05357)
*Mikhail Tsukerman,Konstantin Grotov,Pavel Ginzburg*

Main category: cs.LG

TL;DR: 提出了一种用于电磁逆向设计的条件扩散模型，能够直接从目标散射截面生成结构介质几何形状，无需昂贵的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 传统电磁逆向设计需要昂贵的迭代优化过程，本工作旨在通过扩散模型直接生成满足目标散射特性的结构设计，解决逆向问题的非唯一性并大幅加速设计过程。

Method: 使用带有特征线性调制的1D U-Net架构，学习从期望的角散射模式到2x2介电球结构的映射，通过采样生成多样化的有效设计。

Result: 在11,000个模拟超表面数据集上训练，模型在未见目标上达到中位MPE低于19%（最佳1.39%），优于CMA-ES进化优化，设计时间从小时级减少到秒级。

Conclusion: 扩散模型在电磁逆向设计中具有广阔前景，能够快速探索复杂超表面架构，加速下一代光子和无线通信系统的发展。

Abstract: We present a conditional diffusion model for electromagnetic inverse design
that generates structured media geometries directly from target differential
scattering cross-section profiles, bypassing expensive iterative optimization.
Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map
desired angular scattering patterns to 2x2 dielectric sphere structure,
naturally handling the non-uniqueness of inverse problems by sampling diverse
valid designs. Trained on 11,000 simulated metasurfaces, the model achieves
median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES
evolutionary optimization while reducing design time from hours to seconds.
These results demonstrate that employing diffusion models is promising for
advancing electromagnetic inverse design research, potentially enabling rapid
exploration of complex metasurface architectures and accelerating the
development of next-generation photonic and wireless communication systems. The
code is publicly available at
https://github.com/mikzuker/inverse_design_metasurface_generation.

</details>


### [70] [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](https://arxiv.org/abs/2511.05396)
*Yiting He,Zhishuai Liu,Weixin Wang,Pan Xu*

Main category: cs.LG

TL;DR: 本文研究了在线鲁棒马尔可夫决策过程（RMDP）中的强化学习问题，提出了基于f-散度的过渡不确定性模型，并开发了首个计算高效的算法，在存在训练和部署动态不匹配的情况下实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设可以访问生成模型或预收集的数据集，绕过了探索的挑战。本文研究更现实的设置，即智能体仅限于与训练环境进行在线交互，旨在解决在线RMDP中的探索难题。

Method: 引入了上确界访问比率来衡量训练动态与部署动态之间的不匹配程度，并提出了首个计算高效的算法来处理基于f-散度的过渡不确定性。

Result: 证明了如果上确界访问比率无界，在线学习将变得指数级困难。提出的算法在在线RMDP中实现了次线性遗憾，并通过匹配的遗憾下界证明了算法的最优性。

Conclusion: 通过全面的数值实验验证了理论结果，表明所提出的算法在存在动态不匹配的在线强化学习环境中具有优越性能。

Abstract: Off-dynamics reinforcement learning (RL), where training and deployment
transition dynamics are different, can be formulated as learning in a robust
Markov decision process (RMDP) where uncertainties in transition dynamics are
imposed. Existing literature mostly assumes access to generative models
allowing arbitrary state-action queries or pre-collected datasets with a good
state coverage of the deployment environment, bypassing the challenge of
exploration. In this work, we study a more realistic and challenging setting
where the agent is limited to online interaction with the training environment.
To capture the intrinsic difficulty of exploration in online RMDPs, we
introduce the supremal visitation ratio, a novel quantity that measures the
mismatch between the training dynamics and the deployment dynamics. We show
that if this ratio is unbounded, online learning becomes exponentially hard. We
propose the first computationally efficient algorithm that achieves sublinear
regret in online RMDPs with $f$-divergence based transition uncertainties. We
also establish matching regret lower bounds, demonstrating that our algorithm
achieves optimal dependence on both the supremal visitation ratio and the
number of interaction episodes. Finally, we validate our theoretical results
through comprehensive numerical experiments.

</details>


### [71] [ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids](https://arxiv.org/abs/2511.05420)
*Emad Efatinasab,Nahal Azadi,Davide Dalle Pezze,Gian Antonio Susto,Chuadhry Mujeeb Ahmed,Mirco Rampazzo*

Main category: cs.LG

TL;DR: 提出了一种用于智能电网故障预测的持续学习框架ProDER，在四种现实评估场景中表现最佳，仅出现微小精度下降


<details>
  <summary>Details</summary>
Motivation: 现有AI故障预测模型在动态环境中难以保证可靠性，需要适应新的故障类型和运行区域

Method: 设计了原型驱动的暗经验回放(ProDER)方法，整合了基于原型的特征正则化、logit蒸馏和原型引导的回放记忆

Result: ProDER在测试的CL技术中表现最佳，故障类型预测精度仅下降0.045，故障区域预测精度仅下降0.015

Conclusion: 证明了持续学习在智能电网可扩展、实际故障预测中的实用性

Abstract: As smart grids evolve to meet growing energy demands and modern operational
challenges, the ability to accurately predict faults becomes increasingly
critical. However, existing AI-based fault prediction models struggle to ensure
reliability in evolving environments where they are required to adapt to new
fault types and operational zones. In this paper, we propose a continual
learning (CL) framework in the smart grid context to evolve the model together
with the environment. We design four realistic evaluation scenarios grounded in
class-incremental and domain-incremental learning to emulate evolving grid
conditions. We further introduce Prototype-based Dark Experience Replay
(ProDER), a unified replay-based approach that integrates prototype-based
feature regularization, logit distillation, and a prototype-guided replay
memory. ProDER achieves the best performance among tested CL techniques, with
only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone
prediction. These results demonstrate the practicality of CL for scalable,
real-world fault prediction in smart grids.

</details>


### [72] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 提出加速路径修补（APP）方法，通过对比性注意力头剪枝大幅减少电路发现的计算成本，在保持电路质量的同时实现59.63%-93.27%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前电路发现方法（如路径修补）计算成本高，限制了在小模型上进行深入电路分析的能力。

Method: APP采用混合方法：首先使用对比性FLAP剪枝算法减少搜索空间，然后对剩余注意力头应用传统路径修补。

Result: APP平均减少56%的搜索空间，速度提升59.63%-93.27%，同时发现的电路与原始路径修补电路有显著重叠和相似性能。

Conclusion: APP在保持电路质量的同时大幅降低了电路发现的计算成本，为机制可解释性研究提供了更高效的解决方案。

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


### [73] [Adversarially Robust Multitask Adaptive Control](https://arxiv.org/abs/2511.05444)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: cs.LG

TL;DR: 提出了一种针对对抗性鲁棒多任务自适应线性二次控制的聚类方法，通过集成聚类、系统辨识和弹性聚合来减轻被破坏的模型更新。


<details>
  <summary>Details</summary>
Motivation: 研究多个系统在模型不确定性和对抗性破坏下协作学习控制策略的问题，需要解决对抗性系统对模型更新的破坏。

Method: 采用聚类多任务方法，结合聚类、系统辨识和弹性聚合技术，通过聚类将系统分组并识别被破坏的模型更新。

Result: 分析表明，遗憾值与每个聚类中诚实系统的数量成反比减少，并且这种减少在聚类内对抗性系统比例有界的情况下得以保持。

Conclusion: 提出的聚类多任务方法能有效处理对抗性环境下的多任务线性二次控制问题，通过聚类和弹性聚合机制保证控制性能。

Abstract: We study adversarially robust multitask adaptive linear quadratic control; a
setting where multiple systems collaboratively learn control policies under
model uncertainty and adversarial corruption. We propose a clustered multitask
approach that integrates clustering and system identification with resilient
aggregation to mitigate corrupted model updates. Our analysis characterizes how
clustering accuracy, intra-cluster heterogeneity, and adversarial behavior
affect the expected regret of certainty-equivalent (CE) control across LQR
tasks. We establish non-asymptotic bounds demonstrating that the regret
decreases inversely with the number of honest systems per cluster and that this
reduction is preserved under a bounded fraction of adversarial systems within
each cluster.

</details>


### [74] [Parameter-Efficient Conditioning for Material Generalization in Graph-Based Simulators](https://arxiv.org/abs/2511.05456)
*Naveen Raj Manoharan,Hassan Iqbal,Krishna Kumar*

Main category: cs.LG

TL;DR: 提出了一种参数高效的FiLM条件机制，使图网络模拟器能够适应不同材料参数，显著减少训练数据需求，并在逆问题中成功识别未知材料参数。


<details>
  <summary>Details</summary>
Motivation: 现有图网络模拟器通常针对单一材料类型训练，无法泛化到不同本构行为，限制了在真实工程场景中的应用。

Method: 基于发现材料敏感性集中在早期消息传递层，提出针对这些层的FiLM条件机制，仅需少量新材料的短模拟轨迹进行训练。

Result: 在未见、插值或适度外推的材料参数上实现准确长期预测，相比基线多任务学习方法减少5倍数据需求，并在逆问题中成功识别未知凝聚力参数。

Conclusion: 该方法使GNS能够用于逆设计和闭环控制任务，其中材料属性可作为设计变量。

Abstract: Graph network-based simulators (GNS) have demonstrated strong potential for
learning particle-based physics (such as fluids, deformable solids, and
granular flows) while generalizing to unseen geometries due to their inherent
inductive biases. However, existing models are typically trained for a single
material type and fail to generalize across distinct constitutive behaviors,
limiting their applicability in real-world engineering settings. Using granular
flows as a running example, we propose a parameter-efficient conditioning
mechanism that makes the GNS model adaptive to material parameters. We identify
that sensitivity to material properties is concentrated in the early
message-passing (MP) layers, a finding we link to the local nature of
constitutive models (e.g., Mohr-Coulomb) and their effects on information
propagation. We empirically validate this by showing that fine-tuning only the
first few (1-5) of 10 MP layers of a pretrained model achieves comparable test
performance as compared to fine-tuning the entire network. Building on this
insight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM)
conditioning mechanism designed to specifically target these early layers. This
approach produces accurate long-term rollouts on unseen, interpolated, or
moderately extrapolated values (e.g., up to 2.5 degrees for friction angle and
0.25 kPa for cohesion) when trained exclusively on as few as 12 short
simulation trajectories from new materials, representing a 5-fold data
reduction compared to a baseline multi-task learning method. Finally, we
validate the model's utility by applying it to an inverse problem, successfully
identifying unknown cohesion parameters from trajectory data. This approach
enables the use of GNS in inverse design and closed-loop control tasks where
material properties are treated as design variables.

</details>


### [75] [Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models](https://arxiv.org/abs/2511.05460)
*Sarkar Snigdha Sarathi Das,Palash Goyal,Mihir Parmar,Yiwen Song,Long T. Le,Lesly Miculicich,Jinsung Yoon,Rui Zhang,Hamid Palangi,Tomas Pfister*

Main category: cs.LG

TL;DR: 提出了Synapse框架，通过仲裁多个预训练时间序列基础模型(TSFMs)来提升预测性能，利用不同模型在不同任务中的互补优势。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练时间序列基础模型在不同预测任务、领域和时域上表现差异很大，但如何有效利用这种互补性进行仲裁仍是一个未充分探索的研究领域。

Method: 提出Synapse仲裁框架，动态利用TSFM池，根据上下文相关性能分配和调整预测权重，通过自适应采样组成模型的输出分位数来构建稳健的预测分布。

Result: 实验结果表明，Synapse在时间序列预测中始终优于其他流行的集成技术以及单个TSFM。

Conclusion: Synapse框架通过有效仲裁多个TSFMs，能够显著提升时间序列预测的性能和稳健性。

Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant
advance, capable of forecasting diverse time series with complex
characteristics, including varied seasonalities, trends, and long-range
dependencies. Despite their primary goal of universal time series forecasting,
their efficacy is far from uniform; divergent training protocols and data
sources cause individual TSFMs to exhibit highly variable performance across
different forecasting tasks, domains, and horizons. Leveraging this
complementary expertise by arbitrating existing TSFM outputs presents a
compelling strategy, yet this remains a largely unexplored area of research. In
this paper, we conduct a thorough examination of how different TSFMs exhibit
specialized performance profiles across various forecasting settings, and how
we can effectively leverage this behavior in arbitration between different time
series models. We specifically analyze how factors such as model selection and
forecast horizon distribution can influence the efficacy of arbitration
strategies. Based on this analysis, we propose Synapse, a novel arbitration
framework for TSFMs. Synapse is designed to dynamically leverage a pool of
TSFMs, assign and adjust predictive weights based on their relative,
context-dependent performance, and construct a robust forecast distribution by
adaptively sampling from the output quantiles of constituent models.
Experimental results demonstrate that Synapse consistently outperforms other
popular ensembling techniques as well as individual TSFMs, demonstrating
Synapse's efficacy in time series forecasting.

</details>


### [76] [SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning](https://arxiv.org/abs/2511.05462)
*Xiaodong Wang,Jing Huang,Kevin J Liang*

Main category: cs.LG

TL;DR: 将无监督聚类方法与经典统计混合模型建立联系，提出SiamMM模型，在自监督学习基准测试中达到最先进性能，并能发现潜在的错误标注。


<details>
  <summary>Details</summary>
Motivation: 当前基于聚类的自监督和无监督学习方法应用较为启发式，缺乏明确的最优方法论，需要建立更系统的理论框架。

Method: 通过将无监督聚类方法与经典统计混合模型建立理论联系，开发出SiamMM模型来增强这些聚类方法。

Result: SiamMM模型在各种自监督学习基准测试中达到最先进性能，学习到的聚类结果与未见过的真实标签高度相似，并能揭示潜在的标注错误。

Conclusion: 将聚类方法与统计混合模型建立联系能够显著提升自监督学习性能，并为数据标注质量评估提供新视角。

Abstract: Recent studies have demonstrated the effectiveness of clustering-based
approaches for self-supervised and unsupervised learning. However, the
application of clustering is often heuristic, and the optimal methodology
remains unclear. In this work, we establish connections between these
unsupervised clustering methods and classical mixture models from statistics.
Through this framework, we demonstrate significant enhancements to these
clustering methods, leading to the development of a novel model named SiamMM.
Our method attains state-of-the-art performance across various self-supervised
learning benchmarks. Inspection of the learned clusters reveals a strong
resemblance to unseen ground truth labels, uncovering potential instances of
mislabeling.

</details>


### [77] [Precipitation nowcasting of satellite data using physically conditioned neural networks](https://arxiv.org/abs/2511.05471)
*Antônio Catão,Melvin Poveda,Leonardo Voltarelli,Paulo Orenstein*

Main category: cs.LG

TL;DR: TUPANN是一个基于卫星数据的降水临近预报模型，通过物理对齐的深度学习架构，将预报分解为运动场和强度场等物理分量，在多个气候区域和不同阈值下表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统精确的短期降水预报主要依赖密集的天气雷达网络，这在最易受气候极端事件影响的地区限制了其业务价值。需要开发仅依赖卫星数据的通用预报模型。

Method: TUPANN将预报分解为物理意义明确的组件：变分编码器-解码器在光流监督下从近期图像推断运动和强度场，lead-time条件化的MaxViT演化潜在状态，可微分平流算子重建未来帧。

Result: 在GOES-16和IMERG数据上评估，在4个不同气候区域（里约热内卢、马瑙斯、迈阿密、拉巴斯）的10-180分钟预报时间内，使用CSI和HSS指标在4-64mm/h阈值下，TUPANN在大多数设置中达到最佳或次佳技能，在较高阈值下增益更明显。

Conclusion: 物理对齐学习能够提供技能熟练、可迁移且全球适用的临近预报，模型产生平滑、可解释的运动场，并与数值光流对齐，由于GOES-16的低延迟而能够近实时运行。

Abstract: Accurate short-term precipitation forecasts predominantly rely on dense
weather-radar networks, limiting operational value in places most exposed to
climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned
Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike
most deep learning models for nowcasting, TUPANN decomposes the forecast into
physically meaningful components: a variational encoder-decoder infers motion
and intensity fields from recent imagery under optical-flow supervision, a
lead-time-conditioned MaxViT evolves the latent state, and a differentiable
advection operator reconstructs future frames. We evaluate TUPANN on both
GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,
Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics
over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and
hybrid baselines show that TUPANN achieves the best or second-best skill in
most settings, with pronounced gains at higher thresholds. Training on multiple
cities further improves performance, while cross-city experiments show modest
degradation and occasional gains for rare heavy-rain regimes. The model
produces smooth, interpretable motion fields aligned with numerical optical
flow and runs in near real time due to the low latency of GOES-16. These
results indicate that physically aligned learning can provide nowcasts that are
skillful, transferable and global.

</details>


### [78] [On Flow Matching KL Divergence](https://arxiv.org/abs/2511.05480)
*Maojiang Su,Jerry Yao-Chieh Hu,Sophia Pi,Han Liu*

Main category: cs.LG

TL;DR: 本文推导了流匹配分布近似的KL散度的确定性非渐近上界，证明了当L2流匹配损失有界时，真实数据分布与估计分布之间的KL散度有明确上界，从而建立了流匹配在总变差距离下的统计收敛率。


<details>
  <summary>Details</summary>
Motivation: 流匹配方法在生成建模中表现出色，但其统计效率的理论分析相对缺乏。本文旨在建立流匹配的严格统计理论，证明其与扩散模型在总变差距离下具有可比性的统计效率。

Method: 通过推导流匹配分布近似的KL散度上界，建立L2流匹配损失与KL散度之间的定量关系，分析常数A1和A2对数据场和速度场正则性的依赖关系。

Result: 证明了当L2流匹配损失为ε²时，KL散度上界为A₁ε + A₂ε²，其中常数仅依赖于数据场和速度场的正则性。流匹配在估计光滑分布时达到近乎极小极大最优效率。

Conclusion: 流匹配在总变差距离下具有与扩散模型相当的统计效率，数值实验验证了理论结果，为流匹配方法的统计理论提供了坚实基础。

Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
(KL) divergence of the flow-matching distribution approximation. In particular,
if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
divergence between the true data distribution and the estimated distribution is
bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
depend only on the regularities of the data and velocity fields. Consequently,
this bound implies statistical convergence rates of Flow Matching Transformers
under the Total Variation (TV) distance. We show that, flow matching achieves
nearly minimax-optimal efficiency in estimating smooth distributions. Our
results make the statistical efficiency of flow matching comparable to that of
diffusion models under the TV distance. Numerical studies on synthetic and
learned velocities corroborate our theory.

</details>


### [79] [SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning](https://arxiv.org/abs/2511.05482)
*Kang Yang,Yuanlin Yang,Yuning Chen,Sikai Yang,Xinyu Zhang,Wan Du*

Main category: cs.LG

TL;DR: SoilX是一个无需校准的土壤传感系统，可同时测量土壤湿度(M)、氮(N)、磷(P)、钾(K)、有机碳(C)和铝硅酸盐(Al)六个关键成分，通过建模C和Al消除了土壤质地相关的重新校准需求。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要持续准确监测土壤湿度和关键养分，但现有无线土壤传感解决方案需要根据土壤质地变化重新校准，限制了实用性。

Method: 采用对比交叉成分学习(3CL)方法，包含正交正则器和分离损失来解耦交叉成分干扰；设计新型四面体天线阵列和天线切换机制，可独立于设备放置位置稳健测量土壤介电常数。

Result: 广泛实验表明，SoilX将估计误差相比基线降低了23.8%至31.5%，并且在未见过的田地中具有良好的泛化能力。

Conclusion: SoilX通过联合测量六个关键成分并建模C和Al，成功实现了无需重新校准的土壤传感，显著提高了土壤监测的准确性和实用性。

Abstract: Precision agriculture demands continuous and accurate monitoring of soil
moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),
and potassium (K), to optimize yields and conserve resources. Wireless soil
sensing has been explored to measure these four components; however, current
solutions require recalibration (i.e., retraining the data processing model) to
handle variations in soil texture, characterized by aluminosilicates (Al) and
organic carbon (C), limiting their practicality. To address this, we introduce
SoilX, a calibration-free soil sensing system that jointly measures six key
components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX
eliminates texture- and carbon-dependent recalibration. SoilX incorporates
Contrastive Cross-Component Learning (3CL), with two customized terms: the
Orthogonality Regularizer and the Separation Loss, to effectively disentangle
cross-component interference. Additionally, we design a novel tetrahedral
antenna array with an antenna-switching mechanism, which can robustly measure
soil dielectric permittivity independent of device placement. Extensive
experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%
over baselines and generalizes well to unseen fields.

</details>


### [80] [DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction](https://arxiv.org/abs/2511.05483)
*Abigail Lin*

Main category: cs.LG

TL;DR: DGTN是一个通过扩散机制共同学习结构图神经网络和序列Transformer的新架构，用于预测氨基酸突变对酶热力学稳定性的影响，在基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常独立处理序列和结构信息，无法捕捉局部结构几何与全局序列模式之间的复杂耦合关系。

Method: 提出双向扩散过程：GNN结构嵌入通过可学习扩散核指导transformer注意力，transformer表示通过注意力调制图更新来精炼GNN消息传递。

Result: 在ProTherm和SKEMPI基准测试中，DGTN达到Pearson相关系数0.87，RMSE为1.21 kcal/mol，比最佳基线提升6.2%。扩散机制贡献4.8个相关点。

Conclusion: 该工作通过可学习扩散建立了一个整合异质蛋白质表示的原则性框架，理论分析证明扩散注意力收敛到最优结构-序列耦合。

Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic
stability (DDG) is fundamental to protein engineering and drug design. While
recent deep learning approaches have shown promise, they often process sequence
and structure information independently, failing to capture the intricate
coupling between local structural geometry and global sequential patterns. We
present DGTN (Diffused Graph-Transformer Network), a novel architecture that
co-learns graph neural network (GNN) weights for structural priors and
transformer attention through a diffusion mechanism. Our key innovation is a
bidirectional diffusion process where: (1) GNN-derived structural embeddings
guide transformer attention via learnable diffusion kernels, and (2)
transformer representations refine GNN message passing through
attention-modulated graph updates. We provide rigorous mathematical analysis
showing this co-learning scheme achieves provably better approximation bounds
than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves
state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with
6.2% improvement over best baselines. Ablation studies confirm the diffusion
mechanism contributes 4.8 points to correlation. Our theoretical analysis
proves the diffused attention converges to optimal structure-sequence coupling,
with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work
establishes a principled framework for integrating heterogeneous protein
representations through learnable diffusion.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [81] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，用于解决大型面向对象C++代码库在异构平台（如GPU）上硬件加速的挑战，通过解耦数据布局与接口描述，支持多种内存管理策略，并提供高效的数据传输和转换。


<details>
  <summary>Details</summary>
Motivation: 适应大型面向对象C++代码库进行硬件加速极具挑战性，特别是在面向GPU等异构平台时。现有方法难以在保持代码兼容性的同时实现高效的数据管理和跨设备传输。

Method: Marionette采用编译时抽象，解耦数据布局与接口描述，支持多种内存管理策略，允许接口通过任意函数进行增强，提供高效的数据传输和跨设备转换功能。

Result: 通过基于CUDA的案例研究证明，Marionette能够实现高效灵活的数据结构定义，在保持最小运行时开销的同时，支持现有代码的兼容性和各种使用场景。

Conclusion: Marionette为大型C++代码库在异构平台上的硬件加速提供了一种灵活、高效且可移植的解决方案，通过编译时抽象实现了高性能的数据管理。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [82] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: 该研究在RISC-V GPU上设计并实现了针对超维计算(HDC)优化的自定义指令，使HDC-CNN混合工作负载的处理性能提升了最高56.2倍。


<details>
  <summary>Details</summary>
Motivation: 神经网络机器学习能耗高，而HDC作为轻量级脑启发计算方案虽然并行度高但在复杂视觉任务上精度较低。现有HDC-CNN混合加速器存在泛化性和可编程性差的问题。

Method: 利用开源RISC-V架构的灵活性，在RISC-V GPU上设计四种自定义HDC指令，优化HDC操作的处理效率。

Result: 微基准测试显示性能提升最高达56.2倍，证明了RISC-V GPU在高性能能效计算方面的潜力。

Conclusion: RISC-V GPU为定制计算模型如HDC提供了灵活可编程平台，能够实现高效能效的混合HDC-CNN计算。

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [83] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 该研究结合在线遥测参数和硬件性能计数器来评估不同应用程序在GPU上引起的压力，旨在预测GPU的可靠性问题，特别是老化效应。


<details>
  <summary>Details</summary>
Motivation: 持续工作负载会对GPU组件施加显著压力，引发可靠性问题，可能导致中间计算错误。估计应用程序引起的压力对于预测可靠性至关重要。

Method: 结合在线遥测参数和硬件性能计数器来评估GPU压力，选定的性能计数器主要测量吞吐量、已发出指令数量和停顿事件。

Result: 实验结果表明，通过结合遥测数据和性能计数器可以估计并行工作负载引起的压力，这些计数器揭示了目标工作负载在资源使用方面的效率。

Conclusion: 结合遥测数据和性能计数器是评估GPU工作负载压力的有效方法，有助于预测GPU的可靠性问题。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>
