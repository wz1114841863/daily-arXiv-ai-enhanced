<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 281]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.AR](#cs.AR) [Total: 14]
- [cs.DC](#cs.DC) [Total: 40]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 论文将机器学习中的softmax函数建模为微分几何中的几何界面，揭示了logits到概率转换的几何结构，包括Legendre变换、接触几何和辛几何等概念。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习和现代微分几何之间的桥梁，揭示softmax函数背后的几何结构，为机器学习提供新的数学视角和分析工具。

Method: 将softmax的logits到概率转换建模为几何界面：两个由负熵和log-sum-exp生成的保守描述在概率单纯形上的Legendre变换界面相遇。使用接触几何和辛几何框架进行分析。

Result: 发现偏置平移不变性对应于屏幕上的Reeb流，Fenchel-Young等式/KL散度提供了到界面的可计算距离。具体分析了二类和三类情况。

Conclusion: 为机器学习开辟了新的研究方向：紧凑logit模型（投影或球面）、全局不变量，以及与信息几何的联系，其中屏幕上的动力学表现为复制子流。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [2] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 提出M-RARU主动学习算法，结合不确定性和随机接受-拒绝机制，显著减少大语言模型知识蒸馏所需的样本量和成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分类任务中准确率高，但计算和财务成本阻碍其在动态环境中的大规模部署。知识蒸馏过程本身对大型数据集仍然昂贵，需要教师模型标注大量样本并消耗显著token

Method: 引入M-RARU算法，结合不确定性和随机接受-拒绝机制，选择最具信息量的数据点供LLM教师标注，显著减少API调用和数据处理时间

Result: 在五个不同学生模型和多个基准数据集上的实验表明，相比随机采样，M-RARU可减少高达80%的样本需求，显著提高分类准确率，同时降低财务成本和训练时间

Conclusion: M-RARU算法为在保持LLM性能的同时大幅降低知识蒸馏成本提供了有效解决方案

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [3] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本文提出了一个使用k折交叉验证来检验算法公平性指标统计显著性的框架，通过在累犯预测数据上的应用，发现机器学习算法对黑人存在统计显著的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏评估群体间差异是否具有统计显著性的方法，无法区分真实偏差与随机波动。

Method: 利用k折交叉验证生成公平性指标的抽样分布，开发基于预测与实际结果差异、模型校准和因果推断技术的统计检验方法。

Result: 累犯预测算法在多个公平性定义下对黑人存在统计显著偏见，但在其他定义下无偏见或对白人存在偏见。

Conclusion: 评估算法决策系统时，严谨的统计检验至关重要，不同公平性定义可能得出不同结论。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [4] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa是一种高效的Transformer结构化稀疏训练方法，通过联合评估和稀疏化耦合权重矩阵对，在保持其交互模式的同时实现显著的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏训练方法未能保持注意力层和前馈层中权重矩阵之间的关键结构关系，导致在高稀疏度下性能下降。

Method: 引入联合评估和稀疏化耦合权重矩阵对的方法，通过对齐的行/列移除来保持交互模式，并在预训练和微调场景中执行耦合估计和稀疏化。

Result: EcoSpa在LLaMA-1B上实现50%内存减少和21%训练加速，在GPT-2-Medium上实现2.2倍模型压缩和2.4倍困惑度降低，提供1.6倍推理加速。

Conclusion: EcoSpa使用标准PyTorch操作，无需定制硬件或内核，使得在普通硬件上实现高效Transformer训练成为可能。

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [5] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了DAOpt框架，包括OptU数据集、多智能体决策模块和模拟环境，用于评估LLM在不确定优化中的样本外可行性和鲁棒性，并通过小样本学习增强LLM的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界决策具有不确定性，但现有研究主要关注确定性优化，LLM在不确定环境中的应用尚未充分探索。

Method: 开发DAOpt框架，包含OptU数据集、多智能体决策模块和模拟环境，结合随机和鲁棒优化的领域知识进行小样本学习。

Result: 构建了专门用于不确定优化的评估框架，增强了LLM在不确定环境中的建模能力。

Conclusion: DAOpt框架填补了LLM在不确定优化建模领域的空白，为评估和提升LLM在不确定决策中的表现提供了系统方法。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [6] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge是一个平台无关的GPU内核优化框架，使用两个协作的LLM代理：生成代理通过编译和正确性反馈迭代优化程序，性能分析代理解释性能数据指导优化。只需单次示例即可针对新平台。


<details>
  <summary>Details</summary>
Motivation: GPU内核对ML性能至关重要，但难以在不同加速器上优化。现有方法缺乏平台无关的优化能力。

Method: 使用两个协作LLM代理：生成代理迭代优化程序，性能分析代理解释性能数据指导优化。支持跨平台知识迁移，只需单次示例即可适配新硬件。

Result: 在NVIDIA CUDA和Apple Metal等不同并行计算平台上实现了有效的程序合成，展示了平台无关的优化能力。

Conclusion: KForge通过协作代理架构实现了平台无关的GPU内核优化，支持跨平台知识迁移，只需最小示例即可适配新硬件平台。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [7] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入分析了Transformer中RoPE位置编码的位置信息与符号信息编码机制，提出了量化注意力头行为的方法，并通过实验证明可以通过控制频率访问来调控模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解RoPE位置编码成功的原因，特别是其如何分别使用大频率和小频率来编码位置信息和语义信息，以及注意力头在这两种行为上的表现差异。

Method: 提出了位置行为和符号行为的通用定义，证明了这两种行为的互斥性，开发了量化指标，并在使用RoPE的Transformer LLMs上应用该框架进行分析，设计了纯位置和纯符号的规范任务。

Result: 发现所有注意力头的行为与频率使用之间存在强相关性，可以通过控制注意力头可访问的频率来因果性地控制Transformer的性能表现。

Conclusion: 本研究提供了对RoPE的详细理解，阐明了其特性与模型行为之间的关系，为位置编码机制的设计和理解提供了理论基础。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [8] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: 本文研究了Android系统上AI模型的最优执行配置，重点关注目标检测和图像分类任务，通过评估不同量化方案和设备加速器来平衡精度损失和推理速度


<details>
  <summary>Details</summary>
Motivation: 移动AI应用需要低延迟和高响应性，但面临着实时约束和异构硬件架构的挑战，需要找到最优的执行配置

Method: 评估YOLO目标检测模型和ResNet图像分类模型的不同量化方案，以及GPU和NPU等设备加速器的使用

Result: 通过实验确定了在精度损失最小和推理速度提升最大之间达到最佳平衡的配置组合

Conclusion: 找到了Android系统上AI模型的最优执行配置，实现了精度和速度的良好权衡

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [9] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了一个基于Triton DSL的跨平台LLM推理系统，通过paged attention内核在NVIDIA和AMD GPU上实现最先进性能，解决了模型在不同硬件架构间的可移植性问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理平台在跨硬件架构时的可移植性问题，消除对底层手动调优的需求，同时保持最佳效率。

Method: 使用Triton领域特定语言开发paged attention内核，采用即时编译技术，结合参数自动调优和系统级改进，集成到流行的推理服务器中。

Result: 将通用Triton attention内核性能从最先进水平的19.7%提升到105.9%，在NVIDIA和AMD GPU上都实现了最先进性能。

Conclusion: 开源领域特定语言可以有效解锁模型在不同GPU厂商间的可移植性，实现高效跨平台LLM推理。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [10] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: RAG-FLARKO是一个检索增强的金融推荐系统，通过多阶段知识图谱检索来克服LLM的上下文限制和幻觉问题，提高推荐质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化金融推荐中存在上下文限制、幻觉问题以及缺乏行为基础等挑战，需要更有效的方法来整合用户行为和市场数据。

Method: 采用多阶段并行知识图谱检索方法：首先从用户交易知识图谱中检索行为相关实体，然后用这些上下文过滤市场知识图谱中的时间一致性信号，构建紧凑的接地子图供LLM使用。

Result: 在真实金融交易数据集上的实证评估表明，RAG-FLARKO显著提升了推荐质量，使更小、更高效的模型在盈利性和行为对齐方面都能实现高性能。

Conclusion: 该框架为在资源受限环境中部署接地金融AI提供了可行路径，通过减少上下文开销和聚焦相关信息，实现了高质量金融推荐。

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [11] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 训练模型仅使用输出监控器（无法访问思维链）仍可能导致思维链被混淆，本文提出了两种缓解机制来改善监控性和任务性能。


<details>
  <summary>Details</summary>
Motivation: OpenAI的研究表明，针对思维链监控器的训练可能导致混淆的思维链，其中包含监控器无法检测的不良行为。他们建议仅使用无法访问思维链的输出监控器进行训练，但本文发现这种训练仍会导致思维链混淆。

Method: 本文识别了两种导致思维链混淆的机制：1）模型被训练产生安全输出时，可能泛化到使其思维链看起来安全；2）由于后续token依赖于先前token，安全外观的思维链可能增加安全输出的可能性。本文提出了两种缓解措施来解决这些问题。

Result: 提出的缓解措施在监控性和任务性能方面实现了帕累托改进，相比常规训练有更好的表现。

Conclusion: 即使仅使用输出监控器进行训练，思维链混淆问题仍然存在，但通过适当的缓解措施可以显著改善监控性和模型性能。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [12] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge是一个联邦学习框架，通过将预训练全局主干与轻量级客户端适配器解耦，仅联邦适配器来降低计算和通信开销，支持异构边缘设备上的生成式AI。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在跨设备联邦学习中面临计算通信负担重和统计/系统异构性问题，需要一种资源高效且支持个性化的解决方案。

Method: 使用低秩适应（LoRA）将客户端更新约束到紧凑子空间，仅联邦轻量级适配器而非完整模型，采用FedAvg风格的服务器聚合。

Result: 在语言建模（PTB）和图像生成（CIFAR-10）上实现了更低的困惑度/FID和更快的收敛速度，上行流量比完整模型FedAvg减少99%以上。

Conclusion: FedGen-Edge为异构边缘设备上的隐私保护、资源感知和个性化生成式AI提供了一条实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [13] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome提出了一种可解释的野火风险评估方法，通过融合多个联邦指标、机器学习分类和SHAP分析，在H3 Level-8分辨率下提供决策尺度的风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有野火风险评估依赖粗糙的危险地图和不透明的机器学习模型，牺牲了决策尺度的可解释性。

Method: 融合7个联邦野火指标生成PCA复合风险标签，使用随机森林分类，并通过SHAP和ICE/PDP分析揭示非线性驱动关系。

Result: 在7个美国县获得0.755-0.878的准确率和0.951的二次加权Kappa，主成分解释87-94%的指标方差，针叶林覆盖率和海拔是主要驱动因素。

Conclusion: WildfireGenome将野火风险评估从区域预测推进到可解释的决策尺度分析，指导植被管理、分区和基础设施规划。

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [14] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出轨迹熵约束强化学习(TECRL)框架，通过分离奖励和熵的Q函数学习，解决最大熵RL中温度参数更新导致的Q值非平稳性和短视局部熵调节问题。


<details>
  <summary>Details</summary>
Motivation: 最大熵RL框架存在两个瓶颈：1）温度参数更新与熵注入共同导致的Q值估计非平稳性；2）仅基于单步熵的短视局部熵调节，未考虑时间累积熵的影响。

Method: 提出TECRL框架：分别学习奖励Q函数和熵Q函数，确保值目标不受温度更新影响；通过专用熵Q函数量化期望累积熵，实施轨迹熵约束以控制策略长期随机性。基于此开发DSAC-E算法，扩展分布软演员-评论家。

Result: 在OpenAI Gym基准测试中，DSAC-E能够获得更高回报和更好稳定性。

Conclusion: TECRL框架通过分离Q函数学习和轨迹熵约束，有效解决了最大熵RL中的关键问题，提升了算法性能和稳定性。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [15] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 该论文研究了使用均值聚合和非负权重的图神经网络(MAGNNs)，证明了它们能精确表达的一类单调规则，并提供了用于解释MAGNN预测的一阶逻辑片段。实验表明这种限制能获得可比或更好的性能，并能生成有洞察力的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管使用均值聚合的GNN在知识图谱补全中很常见，但缺乏对其可解释性和表达能力的理论研究，特别是对于使用均值聚合的GNN。

Method: 考虑使用均值聚合和非负权重的GNN(MAGNNs)，证明它们能精确表达的单调规则类别，并提供一阶逻辑片段来解释MAGNN预测。

Result: 实验表明限制均值聚合GNN使用非负权重在标准归纳基准测试中能获得可比或更好的性能，在实践中能获得可靠的规则和深刻的解释。

Conclusion: MAGNNs不仅能提供理论上的可解释性保证，还能在实践中生成有意义的解释，其可靠规则能揭示训练模型中的问题。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [16] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出了一种物理约束自适应学习框架，通过可学习参数自动校准电磁近似，在极紫外光刻优化中实现亚纳米精度，相比传统方法显著减少计算资源和训练样本需求。


<details>
  <summary>Details</summary>
Motivation: 半导体行业在极紫外光刻优化中面临计算危机，传统方法消耗数十亿CPU小时且无法达到亚纳米精度。需要一种能够同时校准物理模型并优化制造精度的新方法。

Method: 集成可微分模块处理菲涅尔衍射、材料吸收、光学点扩散函数模糊、相移效应和对比度调制，通过可学习参数θ={θ_d, θ_a, θ_b, θ_p, θ_c}自动校准电磁近似，同时最小化模拟光强图像与目标光掩模之间的边缘放置误差。

Result: 在15个代表性图案上实现一致的亚纳米EPE性能（0.664-2.536 nm范围），仅需每个图案50个训练样本。相比无物理约束的CNN基线平均改进69.9%，训练完成后推理速度显著快于严格电磁求解器。

Conclusion: 物理约束自适应学习为实时半导体制造优化建立了基础方法，通过联合物理校准和制造精度目标，填补了学术物理信息神经网络与工业部署需求之间的关键差距。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [17] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: LGD建模面临数据质量问题，90%训练数据是基于破产前资产负债表的代理估计而非实际回收结果。递归划分方法表现糟糕，而基于信息论的方法表现更好。研究发现杠杆特征比规模效应更具信息价值，这对Basel III下的LGD模型部署具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 解决LGD建模中训练数据质量不足的问题，90%的数据是代理估计而非实际回收结果，这导致传统方法表现不佳。

Method: 使用信息论方法（香农熵和互信息）分析1,218个企业破产案例（1980-2023），比较递归划分方法（如随机森林）与信息论方法的性能。

Result: 随机森林的r平方为-0.664，表现比预测均值更差；信息论方法达到r平方0.191和RMSE 0.284。杠杆特征包含1.510比特互信息，规模效应仅0.086比特。

Conclusion: 信息论方法在数据质量受限的LGD建模中表现更优，杠杆特征比规模效应更具预测价值，为Basel III要求下的金融机构提供了实用指导。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [18] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文提出了一种基于期望的扰动学习自动机(APLA)方案，用于解决分布式优化中强化学习在多玩家弱无环游戏中无法保证收敛到纯纳什均衡的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在分布式设置中存在局限性，特别是在多玩家弱无环游戏中，当每个玩家独立应用学习动态时，无法保证收敛到纯纳什均衡。现有研究仅关注势博弈和协调博弈等小类游戏。

Method: 提出了APLA学习方案，其中玩家的动作选择概率分布不仅通过重复选择来强化，还通过捕捉玩家满意度的期望因子来强化。在存在噪声观测的情况下，对多玩家正效用博弈进行了随机稳定性分析。

Result: 首次在一般非零和博弈中通过建立无限维马尔可夫链与有限维马尔可夫链的等价性来表征随机稳定性。在弱无环游戏中进一步专门化随机稳定性分析。

Conclusion: APLA方案能够有效解决分布式优化中强化学习的收敛性问题，为多玩家非零和博弈提供了新的学习框架和分析方法。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [19] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 提出了一种结合数据驱动技术和核工业领域知识的预测性维护方法，在核工业领域显著优于纯数据驱动方法，将预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业4.0的融合增强了核工业的数据驱动方法，但核工业系统复杂，需要大量领域知识。纯数据驱动方法在预测设备维护需求方面存在局限性。

Method: 提出混合方法，将数据驱动技术与核设备领域知识相结合，强调纯数据驱动方法的局限性，并展示知识在提升预测模型性能中的重要性。

Result: 通过真实案例研究比较，混合方法显著优于纯数据驱动方法：预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。

Conclusion: 在核工业等高度受限和敏感的领域，结合领域知识的混合预测性维护方法比纯数据驱动方法更有效，能够显著提升故障预测性能。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [20] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出了COWM层来缓解强化学习中的非平稳性问题，通过聚类技术和投影矩阵提高学习效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习通常假设环境是平稳的，但实际环境往往是非平稳的，这导致需要数百万次迭代，样本效率低下

Method: 引入COWM层，可集成到任何RL算法的策略网络中，使用聚类技术和投影矩阵来稳定学习过程

Result: 在基于视觉和状态的DMControl基准测试中分别提升9%和12.6%，在各种算法和任务中表现出鲁棒性和通用性

Conclusion: COWM层能有效缓解非平稳性问题，提高学习速度和效率，减少梯度干扰

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [21] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 系统研究时间序列基础模型中分词器设计（缩放和量化策略）与迁移学习的影响，发现分词器配置主导模型表示能力，而迁移学习影响优化效率。


<details>
  <summary>Details</summary>
Motivation: 分词器和迁移学习是构建最先进时间序列预测基础模型的两个关键组件，需要系统研究它们对模型性能的影响。

Method: 通过经验训练实验和理论分析，研究不同分词器配置（缩放和量化策略）与预训练对比随机初始化对模型性能的影响。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在较小词汇量时；而错误的分词会削弱甚至逆转预训练的好处。

Conclusion: 在时间序列建模中，精心设计分词器至关重要，结合小型高效词汇表和预训练权重在多模态预测设置中特别有利。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [22] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 开发了一个多模态深度学习框架，用于早期预测肝移植中的移植物抗宿主病(GVHD)，在极不平衡的电子健康记录数据上取得了良好性能。


<details>
  <summary>Details</summary>
Motivation: GVHD是肝移植中罕见但通常致命的并发症，死亡率极高。通过整合异构和不平衡的电子健康记录，旨在推进GVHD的早期预测，为及时干预和改善患者预后铺平道路。

Method: 分析了2100名肝移植患者的移植前电子健康记录，包括42例GVHD病例。开发了多模态深度学习框架，动态融合患者人口统计学、实验室检查、诊断和药物四种模态，处理不规则记录和缺失值，并通过AUC优化解决极端类别不平衡问题。

Result: 该框架在所有单模态和多模态机器学习基线中表现最佳，AUC为0.836，AUPRC为0.157，召回率为0.768，特异性为0.803。证明了从不同模态捕获互补信息的有效性。

Conclusion: 多模态深度学习框架显著改进了GVHD早期预测的现有方法，有效解决了真实世界电子健康记录中的异构性和极端类别不平衡挑战，实现了准确的早期预测。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [23] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: MedFedPure是一个保护联邦学习医疗AI模型免受对抗攻击的防御框架，结合个性化联邦学习、掩码自编码器检测和自适应扩散净化模块，在保持隐私的同时显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI模型在联邦学习环境中容易受到对抗攻击，现有防御方法难以应对去中心化的医疗数据环境，需要在不损害隐私和准确性的前提下保护诊断模型。

Method: 1) 个性化联邦学习模型适应各机构数据分布；2) 掩码自编码器检测可疑输入中的隐藏扰动；3) 自适应扩散净化模块选择性清理被标记的扫描图像。

Result: 在Br35H脑MRI数据集上，对抗攻击下的性能从49.50%提升至87.33%，同时保持97.67%的干净准确率。

Conclusion: MedFedPure为临床工作流程中部署安全、可信且保护隐私的AI工具提供了实用解决方案，能够在诊断过程中本地实时运行。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [24] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 提出SA-EMO架构用于未知地下结构的速度场反演，通过结构对齐编码器和多算子混合机制，显著提升全波形反演的性能和边界分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统全波形反演方法存在病态性、非线性强、计算量大等问题，且单一CNN架构或单一神经算子难以在未知复杂地质环境中泛化，无法有效区分不同地质类型。

Method: 使用结构对齐编码器将高维地震波场映射到物理一致的潜在空间，消除波形与速度域间的时空不匹配；采用自适应路由机制选择和融合多种神经算子专家（谱、小波、多尺度和局部算子）来预测速度模型。

Result: 在OpenFWI基准和Marmousi2数据集上的评估显示，SA-EMO显著优于传统方法，平均MAE降低约58.443%，边界分辨率提升约10.308%。消融研究证实各组件均有显著贡献。

Conclusion: 该工作为高效、可扩展且物理可解释的全波形反演引入了新范式。

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [25] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 提出了一种基于超图的全局特征学习和融合框架，用于应变计状态识别，通过特征工程和局部特征间高阶关系学习来捕获全局特征，提高时间序列分类精度。


<details>
  <summary>Details</summary>
Motivation: 在应变计状态识别中，仅使用CNN提取的局部特征不足以区分相似的时间序列，特别是当不同时间序列的局部子序列非常相似时，需要全局特征来更全面地表示时间序列。

Method: 提出超图全局特征学习与融合框架，通过特征工程构建全局特征，并学习局部特征间的高阶关系来捕获全局特征，增强时间序列的语义一致性表示。

Result: 在工业应变计数据和公开UCR数据集上的验证表明，该方法在应变计状态识别中具有更好的泛化能力。

Conclusion: 所提出的超图框架能够有效学习和融合全局特征，显著提升应变计状态识别的准确性，特别是在处理未见数据时表现出更好的泛化性能。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [26] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 使用深度学习模型（RNN、LSTM、TCN、Transformer）预测晶粒生长过程中的晶粒尺寸分布，LSTM表现最佳，准确率超过90%，计算时间从20分钟缩短到几秒钟。


<details>
  <summary>Details</summary>
Motivation: 晶粒生长对材料力学性能有重要影响，但全场模拟计算成本高，需要开发更高效的预测方法。

Method: 从高保真模拟中提取平均场统计描述符，将120个晶粒生长序列处理为归一化晶粒尺寸分布，使用递归预测策略训练模型从短期历史预测未来分布。

Result: LSTM网络准确率最高（超过90%），性能最稳定，能保持物理一致性预测，计算时间从约20分钟/序列减少到仅几秒钟，其他架构在长期预测时容易发散。

Conclusion: 低维描述符和基于LSTM的预测方法在高效准确预测微观结构方面具有潜力，对数字孪生开发和工艺优化有直接意义。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [27] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的元学习算法，通过将分类器分解为元组件来改进小样本学习中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基于度量的元学习方法在未见类别上泛化能力不足的问题，因为现有方法在已见类别上学习的深度度量可能会过拟合，难以泛化到新类别。

Method: 探索分类器的子结构，将每个分类器表示为元组件的组合。通过正交正则化促进元组件的多样性，捕捉不同分类器间的共享子结构。

Result: 在小样本基准任务上的广泛实验表明，该方法取得了优越的性能。

Conclusion: 通过元组件分解和正交正则化，能够有效提升小样本学习中的泛化能力。

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [28] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 开发了一个公平性审计和可解释的PCOS预测机器学习框架，通过SHAP分析和人口统计学审计来识别诊断差异，并整合概率校准指标确保跨亚组的可靠预测。校准后的随机森林模型达到90.8%准确率，但发现25岁以下患者预测性能较差。


<details>
  <summary>Details</summary>
Motivation: 解决PCOS诊断中的公平性问题，确保机器学习模型在不同患者亚组中的可靠性和可解释性，弥合AI研究与临床可用性之间的差距。

Method: 集成SHAP特征归因与人口统计学审计，使用随机森林、SVM和XGBoost模型，结合等渗和Platt缩放进行校准，通过Brier分数和预期校准误差评估性能。

Result: 校准随机森林模型准确率90.8%，SHAP识别卵泡计数、体重增加和月经不规律为最重要特征。模型在25-35岁女性中表现最佳（90.9%），但在25岁以下表现较差（69.2%）。肥胖女性达到完美精确度。

Conclusion: 该框架成功识别了年龄相关的诊断差异，随机森林模型在准确性和可解释性之间达到最佳平衡，基于Streamlit的Web界面实现了临床可用性。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [29] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 本研究开发了两种改进的PINN方法（自适应和保守方法）来解决正则化长波方程，发现PINN的有效性取决于问题类型。自适应PINN在处理复杂非线性相互作用时表现更好，而保守PINN在长期行为问题上更优。


<details>
  <summary>Details</summary>
Motivation: 标准物理信息神经网络在解决正则化长波方程时产生较大误差，需要开发改进方法以提高精度。

Method: 开发了两种改进的PINN方法：具有自适应损失加权的自适应方法和强制执行明确守恒定律的保守方法。使用三个基准测试：单孤子传播、双孤子相互作用和涌浪演化。

Result: 自适应PINN在复杂非线性相互作用（如孤子碰撞）中显著优于保守PINN和标准PINN，而保守PINN在单孤子和涌浪的长期行为问题上表现更好。两种方法的误差均在O(10^-5)量级。

Conclusion: 明确强制执行守恒定律可能对高度非线性系统的优化有害，需要特殊训练方法。PINN可以为复杂偏微分方程系统提供准确的无网格解，但守恒强制执行并不总是能提高PINN性能。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [30] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 提出了一种使用条件表格变分自编码器(CTVAE)预测新产品线扩展消费者属性变化的方法，该模型能生成消费者和产品的合成数据，为产品线营销提供有效指导。


<details>
  <summary>Details</summary>
Motivation: 产品线扩展是重要的营销策略，但过度扩展会破坏品牌形象，需要基于消费者需求进行适当扩展。营销人员需要了解新产品线扩展产品的主要消费者属性。

Method: 使用条件表格变分自编码器(CTVAE)从大规模消费者和产品表格数据中生成合成数据，预测新产品线扩展的消费者属性变化。

Result: 实验结果表明CTVAE比现有模型具有更优越的预测性能，能够为改变容器或口味的新产品提供有效的产品线营销启示。

Conclusion: 该方法有助于避免产品自相残杀，并为产品形象设计和营销策略制定提供支持，具有重要的营销应用价值。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [31] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 提出了一种使用迁移学习和强化学习的可持续波束选择方法，通过将环境建模为点云并计算Chamfer距离来识别结构相似环境，从而重用预训练模型，大幅减少训练时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的波束选择模型在多样化环境中需要大量训练时间和计算资源，这给可扩展性和能效带来重大挑战。

Method: 将环境建模为点云（每个点代表gNodeB和散射体位置），计算点云间的Chamfer距离来识别结构相似环境，通过迁移学习重用预训练模型。

Result: 训练时间和计算开销减少16倍，保持高性能的同时大幅降低能耗，加速部署时间并减少训练相关的碳排放。

Conclusion: 该方法展示了迁移学习在实现可扩展、自适应且环保的强化学习波束选择策略方面的潜力，支持无线系统中绿色可持续AI的发展。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [32] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: LTSV是一种轻量级时间序列数据估值方法，通过在时间序列基础模型上进行上下文微调来估计样本贡献度，解决了传统方法计算效率低和无法保持时间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型的性能高度依赖于数据质量，但传统数据估值方法存在计算效率低、无法处理大规模模型和保持时间依赖性的问题。

Method: 基于理论证据（上下文微调近似影响函数），通过测量上下文微调后上下文损失的变化来估计样本贡献度，并引入时间块聚合来捕捉时间依赖性。

Result: 在多个时间序列数据集和模型上的实验表明，LTSV能够提供可靠且强大的估值性能，同时保持可控的计算需求。

Conclusion: 在时间序列基础模型上进行上下文微调为时间序列学习中的数据归因和模型泛化之间提供了实用且有效的桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [33] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 提出一种基于水压测量和单类支持向量机的数据驱动漏水检测方法，仅需无泄漏时的压力数据和管网拓扑信息，在模拟数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 供水管网每年因漏水造成大量水资源损失，需要可靠有效的漏水检测和定位系统。数据驱动方法因其优越性能而受到关注。

Method: 基于供水管网节点水压测量，使用特征提取器和在无泄漏数据上训练的单类支持向量机，将漏水检测为异常。

Result: 在Modena供水管网的模拟数据集上，所提解决方案的表现优于最近的漏水检测方法。

Conclusion: 该方法是一种完全数据驱动的解决方案，仅需管网拓扑和无泄漏压力数据，就能有效检测漏水异常。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [34] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 提出了一种名为IDFS-MEC的新型特征选择方法，用于处理EEG数据中的通道缺失问题，并通过全局冗余最小化来减少特征子集中的冗余信息，在抑郁症分析中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: EEG特征通常包含冗余、不相关和噪声信息，且现实世界EEG数据采集经常面临电极脱落导致的数据丢失和严重噪声干扰等挑战。

Method: IDFS-MEC将缺失通道指示信息和自适应通道权重学习整合到正交回归中，以减轻不完整通道对模型构建的影响，然后利用全局冗余最小化学习来减少所选特征子集中的冗余信息。

Result: 在MODMA和PRED-d003数据集上的广泛实验表明，IDFS-MEC选择的EEG特征子集在3、64和128通道设置下，比10种流行的特征选择方法具有更优越的性能。

Conclusion: IDFS-MEC方法能够有效处理EEG数据中的通道缺失问题，并选择出性能优越的特征子集，为抑郁症分析提供了更可靠的工具。

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [35] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 提出了一个逐步移除气象站的程序来精简德国弗莱堡的城市气象站网络，分析显示在保持高预测精度的前提下，可以将气象站数量从42个大幅减少到4个。


<details>
  <summary>Details</summary>
Motivation: 城市气象站网络的维护成本高昂且劳动密集，需要找到在保持监测能力的同时减少站点数量的方法。

Method: 采用逐步移除气象站的程序，通过模拟减少气象站网络密度，分析子集重现原始网络空气温度和湿度模式的能力。

Result: 从42个气象站减少到4个，空气温度预测RMSE从0.69K增加到0.83K（增加20%），相对湿度RMSE从3.8%增加到4.4%（增加16%）。位于建成区与乡村交界处的站点对重建城市气候特征最有价值。

Conclusion: 研究表明精简气象站网络具有巨大潜力，可以最大化城市气候研究中财务和人力资源的分配效率。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [36] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 该论文对多智能体强化学习在交通信号控制中的收敛性进行了理论分析，证明了在特定条件下该算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体强化学习在交通信号控制中已被经验证明有效，但其稳定性和收敛性的理论分析尚未被深入探索，本文旨在填补这一理论空白。

Method: 使用随机逼近方法，正式分析多智能体强化学习算法的学习动态，将单智能体异步值迭代的收敛证明扩展到多智能体场景。

Result: 证明了特定的多智能体强化学习交通控制算法在给定条件下能够收敛。

Conclusion: 该工作为多智能体强化学习在交通信号控制中的应用提供了理论依据，证明了其收敛性，扩展了单智能体收敛证明到多智能体场景。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [37] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出RF-ProVe方法，使用随机森林和主动重采样来生成满足输出属性的候选输入区域，提供统计保证的可扩展预像近似计算


<details>
  <summary>Details</summary>
Motivation: 现有可证明方法受限于#P难问题的可扩展性，需要开发具有高置信度保证和有限误差的概率方法

Method: 引入RF-ProVe方法，利用随机决策树集成生成候选输入区域，并通过主动重采样进行精炼

Result: 理论推导提供了区域纯度和全局覆盖的正式统计保证

Conclusion: 为精确求解器无法扩展的情况提供了实用的可扩展解决方案，用于计算紧凑的预像近似

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [38] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant是一个从傅里叶频域视角解决LLM极端压缩的两阶段框架，通过平滑激活异常值和通道级低频傅里叶截断，在LLaMA-3 8B上实现了4位权重和激活量化，准确率仅比全精度低1.5%，同时推理速度提升2倍，内存使用降低3倍。


<details>
  <summary>Details</summary>
Motivation: 随着准确开源大语言模型的出现，需要先进的量化技术来在终端设备上高效部署。本文从傅里叶频域角度重新审视LLM极端压缩的挑战，目标是实现权重和激活的超低位量化。

Method: 提出SpecQuant两阶段框架：第一阶段平滑激活异常值并将其转移到权重矩阵中；第二阶段应用通道级低频傅里叶截断来抑制高频分量，同时保留基本信号能量。还引入了轻量级截断模块在推理时根据通道特性调整截断阈值。

Result: 在LLaMA-3 8B上，SpecQuant实现了4位权重和激活量化，零样本准确率与全精度相比仅差1.5%，同时推理速度提升2倍，内存使用降低3倍。

Conclusion: 从傅里叶频域视角处理LLM量化是有效的，大多数权重能量集中在低频分量，保留这些分量对模型准确率影响最小。SpecQuant框架在保持高准确率的同时显著提升了推理效率和内存使用效率。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [39] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 本文提出了基于四元数的旋转位置嵌入方法QuatRo，并将其推广到基于几何代数的Clifford代数旋转嵌入CARE，解决了球面RoPE的非交换性问题，实现了任意维度的旋转位置编码。


<details>
  <summary>Details</summary>
Motivation: 现有的球面RoPE方法由于使用欧拉角表示旋转而具有非交换性，丧失了原始RoPE的平移等变性特性。本文旨在通过四元数和Clifford代数来解决这个问题。

Method: 1. 提出QuatRo方法，使用四元数代替欧拉角来表示3D旋转；2. 将QuatRo推广到CARE，利用Clifford代数中的旋转子作用于多向量；3. 支持任意维度的旋转位置编码和多种等级的多向量位置信息编码。

Result: 证明了Mixed RoPE和Spherical RoPE都是QuatRo的特殊情况。通过Clifford代数扩展，实现了更通用的旋转位置嵌入框架。

Conclusion: QuatRo和CARE方法成功解决了球面RoPE的非交换性问题，为旋转位置嵌入提供了更通用和数学上更优雅的框架，支持任意维度的位置编码。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [40] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出了SA-SGLD自适应采样算法，通过时间重缩放自动调整步长，在高曲率区域缩小步长、平坦区域扩大步长，提高贝叶斯神经网络后验采样的稳定性和混合效率。


<details>
  <summary>Details</summary>
Motivation: 现有的随机梯度MCMC方法对步长选择高度敏感，自适应变体如pSGLD需要昂贵的散度校正项才能正确采样不变测度。需要开发更稳定、无需校正的自适应采样方法。

Method: 基于SamAdams框架，提出SA-SGLD自适应方案，通过时间重缩放根据监测量（通常是局部梯度范数）调节步长，自动适应不同曲率区域。

Result: 在高曲率2D玩具示例和使用尖锐先验的贝叶斯神经网络图像分类任务中，SA-SGLD比SGLD实现了更准确的后验采样。

Conclusion: SA-SGLD能够在不引入偏差的情况下改善采样稳定性和混合效率，为贝叶斯神经网络提供更可靠的后验分布近似方法。

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [41] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: 提出KUnBR方法，通过知识密度估计定位有害知识丰富的层，采用层重插入策略彻底消除LLM中的有害知识，在保持模型效用的同时实现最先进的遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法难以彻底移除有害知识，残留知识容易被恢复，需要解决隐私、合规和伦理问题。

Method: 使用知识密度估计量化有害知识分布，识别关键层；设计层重插入策略，将有害知识丰富的层提取并重插入原始模型，绕过覆盖层导致的梯度阻塞。

Result: 在多个遗忘和通用能力基准测试中，KUnBR实现了最先进的遗忘性能，同时保持了模型效用。

Conclusion: KUnBR通过精确的知识定位和有效的梯度传播机制，能够彻底消除LLM中的有害知识，为机器学习遗忘提供了有效解决方案。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [42] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: 提出了一种基于行波的新型位置编码机制RollPE，通过循环滚动操作在自注意力中实现相对位置编码，性能优于传统绝对位置编码，与RoPE相当。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法存在局限性，绝对位置编码无法很好捕捉平移等变性，需要更好的相对位置编码方法来改进Transformer的位置感知能力。

Method: 使用循环滚动操作对查询和键张量进行处理，通过位置间的相位偏移实现相对位置差异计算，而不是依赖绝对位置索引。

Result: RollPE显著优于传统绝对位置嵌入方法，与RoPE性能相当，且能从连续角度推导出查询和键空间的拓扑结构。

Conclusion: RollPE提供了一种简化的RoPE实现方式，通过行波视角可能有助于理解大脑中的信息流动过程，为位置编码机制提供了新的理论框架。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [43] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 提出了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现自适应计算，这是一个概念性原型而非性能优化。


<details>
  <summary>Details</summary>
Motivation: 探索可适应且可能更易解释的网络架构新方向，让系统不仅能学习表示，还能学习计算结构本身。

Method: 引入路由机制，使每层能够影响其输出在网络中的传播方式，实现迭代和自适应计算。

Result: 由于计算资源和数据限制，这是初步研究，但初步观察显示有潜力。

Conclusion: 这是一个概念性框架，为探索自适应网络开辟了新方向，其全部潜力需要在更有利的计算条件下进行未来实验评估。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [44] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 使用大语言模型为学习分析仪表板生成数据解释，相比独立仪表板和教师解释，能显著提升学习体验并保持教学标准。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板支持自我调节学习，但数据可解释性影响其效果。需要辅助工具帮助解释仪表板数据。

Method: 使用大语言模型生成仪表板数据的语言解释，与独立仪表板和教师解释进行对比，在大学教育工作者中进行专家研究（N=12）。

Result: LLM生成的技能状态解释和一般学习建议比其他条件更受青睐，表明LLM解释能增强学习体验。

Conclusion: 使用LLM进行数据解释可以提升学习体验，同时保持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [45] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出了一种新颖的协同融合层（SFL）架构，通过门控机制将复杂的深度语义特征与简单的结构特征融合，用于歌词内容分类，在准确性和校准性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决将复杂高维深度语义特征与简单可解释的结构线索融合的挑战，以提升歌词内容分类的性能和可靠性。

Method: 采用协同融合层（SFL）架构，使用门控机制调节Sentence-BERT嵌入（Fdeep）与低维辅助特征（Fstruct）的融合，将任务重构为二分类问题。

Result: SFL模型达到0.9894的准确率和宏F1分数，比随机森林基线（0.9868）表现更好，且校准误差降低93%（ECE=0.0035 vs 0.0500），对数损失降低2.5倍（0.0304 vs 0.0772）。

Conclusion: 非线性门控机制优于简单特征拼接，SFL模型为复杂多模态歌词分析提供了鲁棒且可信赖的系统。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [46] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 使用Sherpa.ai联邦学习平台，多个医院在不共享数据的情况下协作训练肺炎检测模型，性能相比单医院模型显著提升（准确率从0.610提升到0.900，ROC-AUC从0.644提升到0.966）。


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据隐私法规（如HIPAA、GDPR）限制下，无法集中化处理全球分布的胸部X光数据的问题，同时应对医院间数据异质性和图像传输成本高的挑战。

Method: 采用联邦学习框架，使用Pediatric Pneumonia Chest X-ray数据集模拟跨医院协作，处理非独立同分布数据，保持数据本地化私密训练。

Result: 联邦学习模型在准确率和ROC-AUC指标上分别比单医院模型提升了47.5%和50.0%，达到0.900准确率和0.966 ROC-AUC。

Conclusion: 联邦学习能够在保护隐私的前提下实现高性能、可泛化的肺炎检测，特别适用于罕见疾病的多机构协作，是低数据领域诊断和治疗发展的突破性方法。

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [47] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 提出了一种双向剪枝-再生策略，从极度压缩的网络开始，通过选择性再生关键连接来恢复性能，解决高稀疏度下模型性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当稀疏度超过某个阈值时，迭代和一次性剪枝方法都会导致模型性能急剧下降，这限制了可实现的压缩比，使模型无法满足某些硬件平台的严格尺寸约束。

Method: 双向剪枝-再生策略：从满足硬件约束的极度压缩网络开始，选择性再生关键连接以恢复丢失的性能。

Result: 该方法有效缓解了高稀疏度条件下常见的准确率急剧下降问题。

Conclusion: 提出的双向剪枝-再生策略能够克服传统剪枝方法在高稀疏度下的性能限制，实现更高的压缩比同时保持模型性能。

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [48] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和医学影像分析等关键领域，AI系统需要持续学习新任务而不遗忘已学能力，现有方法因学习碎片化特征而失败。

Method: 引入动态加权距离保持(DWDP)损失，通过正则化潜在数据表示之间的成对距离来防止表示漂移，保持底层几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，而且持续优于最先进的基线方法，是唯一超越强单任务学习基线的方法。

Conclusion: LwP通过保持表示空间几何结构的方法有效解决了持续多任务学习问题，特别适用于对分布偏移具有鲁棒性的现实动态环境。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [49] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 将集中式学习中的LP-FT策略（线性探测后全微调）应用于联邦学习，解决个性化微调中的特征失真问题，在七个数据集上验证其优于标准微调方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下难以平衡全局泛化与本地个性化，传统个性化微调方法容易过拟合或无法适应领域偏移。

Method: 将LP-FT策略（先线性探测分类器，再全网络微调）从集中式学习迁移到联邦学习设置，通过分阶段参数更新缓解联邦特征失真。

Result: 在七个数据集和六种个性化微调变体上的系统评估显示LP-FT在平衡个性化和泛化方面表现优越，并建立了LP-FT优于标准微调的条件。

Conclusion: LP-FT为联邦学习提供了稳健的个性化解决方案，通过理论分析和实验验证了其在缓解特征失真方面的有效性。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [50] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: 提出了一种基于同伦引导的自监督学习方法，用于解决参数化交流最优潮流问题，通过构造目标函数和约束条件的连续变形来改善收敛稳定性和可行性。


<details>
  <summary>Details</summary>
Motivation: 标准学习方法在非凸的AC-OPF优化问题中往往难以收敛到可行的高质量解，需要一种能提高收敛稳定性和可行性的新方法。

Method: 采用同伦引导的自监督L2O方法，从具有广泛吸引盆的松弛问题开始，逐步变形到原始问题，无需标记最优解或外部求解器。

Result: 在标准IEEE AC-OPF基准测试中，该方法相比非同伦基线显著提高了可行性率，同时达到与完整OPF求解器相当的目标函数值。

Conclusion: 同伦启发式方法在电力系统优化中展示了可扩展、约束感知的L2O潜力。

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [51] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，从计算理论、信息论和统计学角度形式化分析了LLM扩展的五个基本限制：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐，并指出了扩展的固有理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述多停留在经验层面，缺乏将这些现象与计算、信息和学习的基本理论限制相联系的严格理论综合。本文旨在填补这一空白。

Method: 构建了一个基于证明的统一框架，从三个理论层面分析：(1)可计算性和不可计算性导致的固有误差；(2)信息论和统计约束；(3)几何和计算效应导致的上下文压缩。结合定理和实证证据进行分析。

Result: 证明了LLM扩展存在固有理论上限：不可计算任务必然存在无限失败集；有限描述长度强制压缩误差；长上下文因位置训练不足、编码衰减和softmax拥挤而被压缩；基于似然的训练偏向模式完成而非推理。

Conclusion: LLM扩展在某些方面有效，但在某些方面会饱和甚至无法进展。提出了缓解路径，包括有边界检索、位置课程学习、稀疏或分层注意力等实用方法。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [52] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出了SBN-Opt框架，通过神经代理模型SBN嵌入LSQC能量，优化自由边界微分同胚映射，在密度均衡映射和不一致表面配准中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化在表面映射问题中至关重要但极具挑战性，因为边界不受约束且在大变形下必须保持局部双射性。传统数值LSQC算法需要地标条件且无法应用于基于梯度的优化。

Method: 提出SBN（谱Beltrami网络）作为神经代理，将LSQC能量嵌入多尺度网格谱架构中；然后提出SBN-Opt框架优化自由边界微分同胚映射，显式控制局部几何失真。

Result: 在密度均衡映射和不一致表面配准上的大量实验表明，SBN-Opt优于传统数值算法。

Conclusion: SBN-Opt框架成功解决了自由边界微分同胚优化问题，提供了一种有效的方法来控制局部几何失真并实现高质量的映射结果。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [53] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV是一种GPU优化的稀疏矩阵向量乘法格式和内核，专门针对30-90%非结构化稀疏度的LLM推理场景，实现了显著的内存减少和速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在LLM剪枝产生的低且非结构化稀疏度(30-90%)下性能不佳，非结构化剪枝只能提供有限的内存减少和加速效果。

Method: 提出MACKO-SpMV，一种GPU优化的格式和内核协同设计，减少存储开销同时保持与GPU执行模型的兼容性，无需专用硬件单元或格式特定预计算。

Result: 在50%稀疏度下，MACKO首次实现1.5倍内存减少和1.2-1.5倍加速；相比其他SpMV基线：比cuSPARSE快2.8-13.0倍，比Sputnik快1.9-2.6倍，比DASP快2.2-2.5倍。在Llama2-7B上实现1.5倍内存减少和1.5倍推理加速。

Conclusion: MACKO使得50%稀疏度的非结构化剪枝在真实世界LLM工作负载中变得可行和合理。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [54] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 本研究使用随机森林算法结合可解释AI(SHAP)开发了加州野火风险地图，识别了森林和草原生态系统的关键风险驱动因素，并评估了模型的空间和时间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 野火对全球生态系统构成重大威胁，加州由于气候、地形、植被模式和人类活动等因素经常发生火灾，需要开发全面的野火风险评估方法来支持决策。

Method: 采用随机森林算法结合SHAP可解释AI方法，使用空间和时间验证策略评估模型性能，识别生态系统特定的关键风险驱动因素。

Result: RF模型表现出强大的预测性能，森林和草原的AUC分别达到0.997和0.996。SHAP分析显示森林的关键驱动因素包括土壤有机碳、树木覆盖和NDVI，而草原的关键驱动因素包括地表温度、海拔和植被健康指数。

Conclusion: RF-SHAP框架为野火风险评估提供了稳健、可理解和适应性强的方法，能够支持知情决策和制定有针对性的风险缓解策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [55] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了MPCM-Net网络，结合部分注意力卷积和Mamba架构来解决地面云图像分割中的多尺度上下文提取、精度-吞吐量平衡和全局特征依赖问题，并发布了新的CSRC数据集。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在地面云图像分割中存在三个主要问题：1)依赖空洞卷积进行多尺度上下文提取，缺乏部分特征有效性和通道间互操作性；2)基于注意力的特征增强实现忽视精度-吞吐量平衡；3)解码器修改未能建立层次化局部特征间的全局依赖关系，限制推理效率。

Method: 提出MPCM-Net网络，编码器包含MPAC模块（MPC块和MPA块），分别实现多尺度云形成的全局空间交互和低计算复杂度的判别特征提取；解码器使用M2B模块，通过SSHD保持线性复杂度同时实现跨空间和尺度维度的深度特征聚合。

Result: 在CSRC数据集上的大量实验表明，MPCM-Net在分割精度和推理速度之间实现了最优平衡，性能优于最先进方法。

Conclusion: MPCM-Net通过整合部分注意力卷积和Mamba架构，有效解决了地面云图像分割中的关键挑战，同时发布了CSRC数据集作为新的分割基准，为社区做出重要贡献。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [56] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出WPAC和PIAD方法，将预训练ViT转换为分层知识密度超网络，实现灵活提取不同大小的子网络，同时保持最大知识保留。


<details>
  <summary>Details</summary>
Motivation: 训练和部署多个不同资源约束的ViT模型成本高且效率低，需要一种能够灵活适应不同模型大小的统一解决方案。

Method: WPAC通过加权PCA压缩注意力层知识到关键权重；PIAD通过渐进重要性感知dropout促进知识分层组织。

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合为模型压缩和扩展提供了强大的替代方案。

Conclusion: 该方法能够有效构建分层知识密度超网络，实现灵活的模型大小调整，在模型压缩和扩展任务中表现优异。

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [57] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: 该论文提出了一种贝叶斯模型来处理医疗决策中的漏斗结构问题，解决了选择性标签和审查偏差对风险评估的影响，特别是在服务不足的患者群体中。


<details>
  <summary>Details</summary>
Motivation: 医疗决策中的漏斗结构（如筛查、评估等阶段）存在选择性审查问题，即真实结果只在流程末端揭示，这会导致风险评估的统计偏差，特别是在结果更常被审查的服务不足患者群体中。

Method: 开发了一个基于选择性标签和审查先验工作的贝叶斯模型，用于处理漏斗决策结构。在合成设置中验证模型恢复真实参数和预测审查患者结果的能力，然后应用于急诊科就诊数据集。

Result: 在合成设置中，模型比基线方法更准确地恢复真实参数和预测审查患者结果。在急诊科数据集应用中，发现基于性别的医院和ICU入院差异，模型估计女性ICU入院死亡率风险阈值（5.1%）高于男性（4.5%）。

Conclusion: 提出的贝叶斯模型能够有效处理漏斗决策结构中的选择性审查问题，揭示了医疗决策中可能存在的性别偏见，为改善医疗公平性提供了工具。

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [58] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: 提出Replay Tuning (R-Tuning)框架，通过频率感知重放策略和潜在一致性约束，解决预训练时间序列模型在持续适应新数据分布时的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在时间序列预测中表现出色，但适应不断变化的数据分布仍面临挑战。主要问题在于无法访问原始训练数据，仅对新数据进行微调会导致灾难性遗忘。

Method: R-Tuning构建统一潜在空间，通过小波分解在多个频段生成趋势保持和融合增强的变体，提高表示多样性和重放效率。引入潜在一致性约束，将新表示与先验任务空间对齐。

Result: 实验结果显示R-Tuning将新任务的MAE和MSE分别降低高达46.9%和46.8%，同时在旧任务上保持先验知识，性能提升达5.7%和6.0%。在少样本设置下，即使合成代理样本仅占新任务数据集的5%，仍优于所有最先进基线方法。

Conclusion: R-Tuning通过频率感知重放和潜在一致性约束，有效解决了预训练时间序列模型在持续适应中的灾难性遗忘问题，在保持先验知识的同时显著提升新任务性能。

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [59] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: 提出了正则化薛定谔桥(RSB)方法，通过正则化训练策略解决扩散模型在逆问题中的失真-感知权衡和曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在逆问题中的两个关键挑战：1)失真-感知权衡，即提高感知质量会降低重建保真度；2)曝光偏差问题，训练-推理输入不匹配导致预测误差累积和重建质量下降。

Method: 基于薛定谔桥框架的正则化训练策略，扰动输入状态和目标，通过模拟预测误差缓解曝光偏差，并通过后验均值的精心设计插值减轻失真。

Result: 在语音增强等逆问题上的广泛实验表明，RSB优于现有方法，显著改善了失真指标并有效减少了曝光偏差。

Conclusion: RSB方法成功解决了扩散模型在逆问题中的失真-感知权衡和曝光偏差问题，在语音增强任务中表现出色。

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [60] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO是一种新颖的双层优化框架，通过全局搜索和局部优化交替进行，在极低NFE（仅5次）下实现11.94的FID，优化成本不到8秒。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型生成质量高但采样过程慢，现有调度优化方法难以同时满足有效性、自适应性、实用鲁棒性和计算效率四个核心原则。

Method: 提出HSO双层优化框架：上层全局搜索最优初始化策略，下层局部优化调度细化；引入中点误差代理（MEP）和间距惩罚适应度（SPF）函数。

Result: 在极低NFE（仅5次）下，HSO在LAION-Aesthetics数据集上达到11.94的FID，创下无训练采样的新SOTA。

Conclusion: HSO提供了一种高效实用的扩散模型加速范式，无需昂贵重新训练，一次优化成本低于8秒。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [61] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 本文提出了一种双重去偏测试时提示调优方法，通过动态检索增强调制和可靠性感知提示优化来缓解视觉语言模型在测试时提示调优中的优化偏差问题。


<details>
  <summary>Details</summary>
Motivation: 测试时提示调优在零样本设置下表现出色，但仅基于未标记测试数据调优可学习提示可能导致提示优化偏差，从而在下游任务中产生次优性能。

Method: 提出双重去偏测试时提示调优方法：1) 动态检索增强调制模块，使用测试图像特征检索高置信度知识来调制预测；2) 可靠性感知提示优化模块，通过置信度加权集成和跨模态一致性蒸馏施加正则化约束。

Result: 在涉及自然分布偏移和跨数据集泛化的15个基准数据集上的广泛实验表明，该方法优于基线方法，验证了其在缓解提示优化偏差方面的有效性。

Conclusion: 所提出的双重去偏方法能够有效缓解测试时提示调优中的优化偏差问题，提升模型在下游任务中的性能表现。

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [62] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: 提出了一个用于语音情感识别（SER）的可解释AI框架，通过量化显著区域内的声学线索幅度，将显著区域与专家参考的声学线索联系起来，提高解释质量和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前基于显著性的方法（从视觉领域改编而来）虽然能突出频谱图区域，但无法显示这些区域是否对应有意义的声学情感标记，限制了忠实性和可解释性。

Method: 提出一个框架，通过量化显著区域内的线索幅度，明确连接显著区域与理论驱动的、专家参考的语音情感声学特征。

Result: 在基准SER数据集上的实验表明，该方法通过明确连接显著区域与专家参考声学特征，提高了解释质量。

Conclusion: 相比标准显著性方法，该框架提供了更易理解和合理的SER模型解释，为可信赖的基于语音的情感计算奠定了基础。

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [63] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文提出AnchorDS方法，通过将文本到3D优化重新表述为动态源分布到固定目标分布的映射，解决了传统SDS方法中语义过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的文本到3D方法将2D生成模型的指导视为静态，忽略了源动态性，导致语义线索被抑制或合并，产生语义过度平滑的伪影。

Method: 将问题转化为双条件潜空间，同时基于文本提示和中间渲染图像进行条件化。引入AnchorDS机制，提供状态锚定的指导，并设计了轻量级过滤策略和微调策略来优化锚点。

Result: AnchorDS能够产生更精细的细节、更自然的颜色和更强的语义一致性，特别是在复杂提示下表现优异，同时在效率上保持优势。

Conclusion: 该方法在质量和效率上都超越了先前的方法，为文本到3D生成提供了更稳定和高质量的解决方案。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [64] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文提出下一代老年人监测系统，从跌倒检测扩展到日常生活活动识别，旨在开发隐私保护、边缘部署的联邦AI系统，支持老龄化社会的独立性和尊严。


<details>
  <summary>Details</summary>
Motivation: 当前老年人监测主要关注跌倒检测，但需要更全面的日常生活活动识别来真正支持老年人独立生活。现有ADL数据集不足，需要开发更智能的监测系统。

Method: 使用SISFall数据集及其GAN增强变体进行可行性验证，将跌倒检测作为代理任务。在非独立同分布条件下进行联邦学习实验，并在Jetson Orin Nano设备上进行嵌入式部署。

Result: 初步结果显示在非IID条件下的联邦学习和边缘设备部署是可行的，为全面ADL监测提供了早期证据。

Conclusion: 这项工作标志着从单任务检测向全面日常活动识别的转变，为可持续和以人为本的老年人护理AI提供了路线图，但仍需解决领域偏移、数据稀缺和隐私风险等挑战。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [65] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ是一个用于评估图神经网络在材料属性预测中分布外泛化能力和不确定性量化的基准框架，包含1375个OOD任务，提出了新的结构感知分割策略SOAP-LOCO和不确定性度量D-EviU。


<details>
  <summary>Details</summary>
Motivation: 解决材料发现中图神经网络在分布外场景下的不确定性量化问题，为模型选择提供可靠依据。

Method: 构建包含1375个OOD任务的基准框架，使用五种OOD分割策略和新提出的SOAP-LOCO策略，评估12个代表性GNN模型，采用蒙特卡洛Dropout和深度证据回归的统一不确定性训练协议。

Result: 不确定性感知训练方法显著提高预测精度，在挑战性OOD场景中平均减少70.6%的误差；没有单一模型在所有任务中表现最优，早期模型如SchNet和ALIGNN仍具竞争力，新模型在特定属性上表现更优。

Conclusion: MatUQ基准为材料发现中分布偏移下的可靠模型选择提供了实用指导，不确定性量化对提升OOD预测性能至关重要。

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [66] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Moirai 2.0是一个基于解码器架构的时间序列基础模型，采用分位数预测和多令牌预测技术，在准确性和推理效率方面均有提升，相比前代模型速度提升2倍、模型大小缩小30倍。


<details>
  <summary>Details</summary>
Motivation: 开发一个更高效、更准确的时间序列预测模型，通过简化架构和改进预测方法来提升性能与效率的平衡。

Method: 采用纯解码器架构、单patch输入和分位数损失函数，结合递归多分位数解码技术，替代了前代模型的掩码编码器训练、多patch输入和混合分布输出。

Result: 在Gift-Eval基准测试中表现优异，在准确度、速度和模型大小之间取得了良好平衡，性能优于同系列更大模型，并展现出稳健的领域级结果。

Conclusion: Moirai 2.0通过架构简化和方法改进实现了显著的性能提升，但模型性能随参数增加趋于饱和，在长时预测方面表现下降，未来需要在数据扩展和长时建模方面进一步研究。

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [67] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的截断矩形棱柱方法来紧密逼近RNN中的Hadamard积生成的三维非线性曲面，通过最小化体积和表面积实现更紧密的过近似，显著提升了RNN鲁棒性验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对非线性激活函数进行线性过近似时，单独逼近非线性部分可能导致显著的过估计，从而降低验证准确性。需要更紧密地逼近Hadamard积生成的三维非线性曲面。

Method: 提出截断矩形棱柱方法，由两个线性松弛平面构成，采用细化驱动方法最小化其体积和表面积，实现更紧密的过近似。基于此实现了DeepPrism原型系统。

Result: 实验结果表明，DeepPrism在图像分类、语音识别和情感分析等多种任务中相比最先进方法有显著改进。

Conclusion: 所提出的截断矩形棱柱方法能够更紧密地逼近非线性曲面，有效提升了RNN鲁棒性验证的准确性和性能。

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [68] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 提出使用贝叶斯神经网络和蒙特卡洛dropout的概率性电价预测框架，按小时分别建模以捕捉日内模式，在点预测和区间预测方面优于传统基准模型。


<details>
  <summary>Details</summary>
Motivation: 电力市场中的电价波动性源于复杂的供需动态和外部因素，传统点预测无法捕捉内在不确定性，限制了风险管理效用。

Method: 使用贝叶斯神经网络和蒙特卡洛dropout，为每天的不同小时分别训练模型以捕捉日内模式，并与GARCHX和LEAR基准模型进行比较。

Result: 所提出的模型在点预测和区间预测方面均优于基准模型。

Conclusion: 这项工作为在能源市场预测中利用概率性神经网络模型提供了参考。

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [69] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 提出两种新颖的输入表示方法（SS-only和RGB+SS），使用语义分割来应对3D环境强化学习中的高内存消耗和部分可观测性问题，在ViZDoom死亡竞赛中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 解决3D环境强化学习中的两个主要挑战：(1) 稳定学习所需内存缓冲区的高内存消耗；(2) 部分可观测马尔可夫决策过程的学习复杂性。

Method: 提出两种输入表示：SS-only（仅语义分割）和RGB+SS（RGB+语义分割），在ViZDoom死亡竞赛中进行实验，使用完美分割结果进行受控评估，并探索基于密度的热图可视化方法。

Result: SS-only能够将内存缓冲区内存消耗减少至少66.6%，应用游程编码等无损压缩技术时最多可减少98.6%；RGB+SS通过额外的语义信息显著提升了RL智能体性能。

Conclusion: 语义分割输入表示有效解决了3D环境RL的内存消耗和部分可观测性问题，SS-only大幅降低内存需求，RGB+SS提升性能，热图可视化有助于评估数据收集的适用性。

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [70] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出一种轻量级但有效的训练流程，通过将LaTeX公式渲染为图像并配以结构化思维链提示，使紧凑的多模态架构在数学问题求解上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决数学问题需要复杂的推理能力，传统方法在保持模型紧凑性的同时难以达到高精度。本文旨在通过简单的文本到视觉增强，使小型多模态模型也能在数学推理任务上表现出色。

Method: 将LaTeX编码的数学公式渲染成图像，并与结构化的思维链提示配对，构建训练数据。通过系统消融实验分析渲染保真度和提示设计对性能的影响。

Result: 该方法在多个广泛使用的基准测试中一致匹配或超越了开源和专有的数学视觉语言求解器，同时在MMMU、ChartQA和DocVQA等任务上获得了高达20%的性能提升，同时保持了广泛的通用领域能力。

Conclusion: 尽管方法简单，但渲染保真度和提示设计是性能的主要驱动因素，表明通过适当的视觉增强和提示工程，紧凑的多模态架构能够在数学推理任务上达到最先进的性能。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [71] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 研究探讨了使用菜品名称文本输入能否比仅使用图像的基线模型更准确估算热量，发现多模态模型（图像+文本）比仅图像模型在热量估算上降低了1.06千卡的MAE，有1.25%的改进。


<details>
  <summary>Details</summary>
Motivation: 确定短文本输入（菜品名称）在多大程度上能改善热量估算，相比仅使用图像的基线模型，并验证改进是否具有统计显著性。

Method: 使用TensorFlow库和Google的Nutrition5k数据集，训练了仅使用图像的CNN模型和同时接受文本和图像输入的多模态CNN模型。

Result: 多模态模型将热量估算的MAE从84.76千卡降低到83.70千卡，减少了1.06千卡（1.25%的改进）。

Conclusion: 短文本输入（菜品名称）能够在一定程度上改善热量估算的准确性，多模态方法相比仅使用图像的方法有统计显著的改进。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [72] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出了一个统一的多模态地球观测表示学习框架，能够在高时空分辨率下整合不同传感器数据，解决现有模型固定尺度限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型通常在固定空间或时间尺度上运行，限制了需要精细空间细节和高时间保真度的生态分析应用。

Method: 采用两阶段设计：首先独立建模每个传感器以捕捉其特定特征，然后将表示组合到共享模型中。使用Sentinel-1和Sentinel-2作为代表性模态，在10米分辨率和无云Sentinel-2采集频率下构建统一特征空间。

Result: 学习到的嵌入在异质景观中表现出高空间和语义一致性，在总初级生产力建模的定量评估中编码了生态意义模式并保持了足够的时间保真度。

Conclusion: 该框架为需要不同空间和时间分辨率的环境应用提供了灵活、分析就绪的表示学习方法。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [73] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: 提出FSC-Net双网络架构，通过快速网络学习新任务，慢速网络进行知识巩固，有效缓解持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 受神经科学中记忆巩固机制启发，解决神经网络在持续学习中因学习新任务而遗忘旧知识的灾难性遗忘问题。

Method: 采用双网络架构：快速网络(NN1)负责快速适应新任务，慢速网络(NN2)通过蒸馏和重放进行渐进式知识巩固。发现纯重放策略优于蒸馏方法。

Result: 在Split-MNIST上达到91.71%保留准确率，比单网络提升4.27个百分点；在Split-CIFAR-10上达到33.31%保留准确率，提升8.20个百分点，但绝对性能仍有改进空间。

Conclusion: 双时间尺度巩固机制比架构复杂性更关键，纯重放策略能有效避免近期偏差，为缓解灾难性遗忘提供了实证依据。

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [74] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 将Model-X knockoffs方法引入稀疏自编码器特征选择，通过knockoff+控制错误发现率，为机制可解释性提供可靠的特征发现框架


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在识别神经网络可解释特征时面临挑战，难以区分真实计算模式与错误相关性，需要更可靠的特征选择方法

Method: 使用Model-X knockoffs方法，通过knockoff+控制FDR，采用高斯替代模型处理潜在分布，分析512个高活性SAE潜在变量

Result: 在FDR q=0.1下选择了129个特征，约25%的潜在变量携带任务相关信号，75%不相关，所选特征与非选特征在knockoff统计量上显示5.40倍分离

Conclusion: 该方法将SAE与多重测试感知推理相结合，为可靠特征发现提供了可重现的原则性框架，推进了机制可解释性的基础

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [75] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代算子应用并收敛到固定点的过程，通过OpenLM架构在OpenXOR问题上实现76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正学会了推理，还是仅仅在模式匹配推理轨迹，需要理解推理的本质要求并构建相应的架构。

Method: 提出推理作为状态空间中迭代算子应用的理论框架，开发OpenOperator理论和OpenLM架构实现。

Result: 在OpenXOR问题上，OpenLM达到76%准确率，而现有最先进LLMs为0%。

Conclusion: 推理需要特定的架构支持，而不仅仅是模式匹配，OpenLM证明了这种架构的可行性。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [76] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架，用于单细胞数据分析，通过多尺度嵌入整合不同几何视角，在基准数据集上表现出稳定的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将细胞表示为欧几里得空间中的向量，限制了捕捉内在相关性和多尺度几何结构的能力。

Method: 基于Grassmann流形的多尺度框架，通过多尺度表示生成嵌入，使用基于幂的尺度采样函数控制尺度选择，将不同几何视角的特征整合到统一的Grassmann流形中。

Result: 在9个基准单细胞RNA-seq数据集上的实验表明，该方法能有效保留有意义的结构，提供稳定的聚类性能，特别适用于中小型数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且信息丰富的基础。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [77] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 开发基于视觉的替代建模框架，用于数据中心的实时3D温度场预测，实现高达20,000倍的加速，支持实时冷却控制和节能。


<details>
  <summary>Details</summary>
Motivation: 减少数据中心能耗和碳排放，传统CFD求解器计算成本高且需要专家知识，无法满足实时需求。

Method: 使用3D体素化表示，结合服务器负载、风扇速度和HVAC设置，评估多种架构包括3D CNN U-Net、3D傅里叶神经算子和3D视觉变换器。

Result: 模型在不同数据中心配置中泛化良好，实现高达20,000倍加速（毫秒级vs小时级），节能7%并减少碳足迹。

Conclusion: 该框架能够快速准确估计热点和温度分布，为实时冷却控制和负载重新分配提供支持，显著提升能效和可持续性。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [78] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 本文揭示了在扩散模型中通过去噪分数匹配优化条件输入时会破坏与精确分数匹配的等价性，导致分数范数偏高的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 许多近期工作使用去噪分数匹配来优化扩散模型的条件输入，但这种方法存在理论偏差问题，需要深入分析其影响。

Method: 通过理论分析和实验观察，验证了去噪分数匹配优化条件输入时与精确分数匹配不等价，并展示了这种偏差导致分数范数增高的问题。

Result: 发现优化条件输入会破坏分数匹配的等价性，产生偏差并导致更高的分数范数；在数据分布优化中也观察到类似偏差。

Conclusion: 这种偏差影响了多个领域的工作，包括自回归生成的MAR、图像压缩的PerCo以及文本到3D生成的DreamFusion等，需要引起重视。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [79] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: PI-NODE-SR结合低阶显式求解器和残差归一化，稳定地学习刚性生物物理系统，在有限迭代预算下准确预测振荡频率和幅度。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程和物理信息变体在建模刚性生物物理系统时不可靠，需要大量迭代且可能收敛到次优解，无法保持振荡频率或幅度。

Method: 引入具有尺度感知残差的物理信息神经ODE（PI-NODE-SR），结合Heun方法和残差归一化来平衡不同时间尺度状态变量的贡献。

Result: 在Hodgkin-Huxley方程上，PI-NODE-SR从单个振荡学习并外推超过100ms，准确捕捉振荡频率和接近正确的幅度，恢复门控变量中的尖锐亚阈值曲率等形态特征。

Conclusion: PI-NODE-SR相对于基线神经ODE和PINNs持续减少长时程误差，为稳定高效学习刚性生物动力学提供了原则性途径。

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [80] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: KAN/H是Kolmogorov-Arnold Network的变体，使用Haar变体基系统替代B样条，包含全局和局部基，应用于函数逼近和MNIST，无需大量问题特定的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 改进KAN网络，通过使用Haar变体基系统来避免B样条基的局限性，减少对问题特定超参数调优的依赖。

Method: 提出KAN/H变体，使用具有全局和局部基的Haar变体基系统替代B样条，应用于函数逼近问题和MNIST数据集。

Result: KAN/H在函数逼近和MNIST任务中表现良好，不需要大多数问题特定的超参数调优。

Conclusion: KAN/H通过Haar变体基系统有效减少了超参数调优需求，在函数逼近和MNIST任务中表现出色。

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [81] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root是一个联合数据和知识驱动的框架，通过结合可扩展的弱监督和精确的专家指导来进行移动网络QoE问题的根因分析。


<details>
  <summary>Details</summary>
Motivation: 由于移动网络中复杂的跨层交互和专家标注稀缺，传统基于规则的启发式方法生成的标签存在噪声且粒度较粗，限制了纯数据驱动方法的准确性。

Method: 1) 使用对比表示学习预训练编码器，通过监督对比目标对基于规则的标签进行去噪；2) 引入类条件扩散模型生成保留根因语义的KPI序列，通过控制反向扩散步骤产生强弱增强；3) 使用稀缺的专家验证标签联合微调编码器和轻量级分类器。

Result: 在真实运营商级数据集上的广泛实验表明，DK-Root达到了最先进的准确率，超越了传统机器学习和最近的半监督时间序列方法。消融实验证实了条件扩散增强和预训练-微调设计的必要性。

Conclusion: DK-Root框架通过联合数据和知识驱动的方法，有效解决了移动网络QoE根因分析中的标签噪声和标注稀缺问题，实现了高质量的表示学习和分类性能。

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [82] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出了一种好奇心驱动的量化混合专家框架，通过贝叶斯认知不确定性路由在异构专家间分配任务，在保持精度的同时显著降低延迟方差和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备上部署深度神经网络的两个关键挑战：在激进量化下保持精度，同时确保可预测的推理延迟。

Method: 使用好奇心驱动的量化混合专家框架，基于贝叶斯认知不确定性在异构专家（BitNet三元、1-16位BitLinear、训练后量化）之间进行路由。

Result: 4位量化保持99.9%的16位精度（0.858 vs 0.859 F1），压缩比4倍，能耗节省41%。好奇心驱动路由将MoE延迟方差降低82%（从230ms到29ms标准差）。

Conclusion: 自适应量化可产生准确、节能且可预测的边缘模型，简单的4位量化架构在大多数部署场景中优于复杂的MoE架构。

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [83] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: 本文提供了扩散生成模型的简明自包含推导，从高斯分布基本性质出发，系统构建了去噪扩散概率模型，涵盖了前向加噪过程、反向后验分布、变分下界等核心理论，并扩展到连续时间公式、加速采样方法和引导扩散技术。


<details>
  <summary>Details</summary>
Motivation: 旨在为扩散生成模型提供透明、自包含的理论推导，使读者能够同时理解理论框架并实现相应算法，填补现有文献中理论推导不够系统完整的空白。

Method: 从高斯分布的基本性质出发，系统推导扩散模型的核心组件：前向加噪过程及其闭式边际分布、精确离散反向后验分布、变分下界，并扩展到连续时间公式（概率流ODE、Fokker-Planck方程）、加速采样方法（DDIM、DDGAN、多尺度变体）和引导扩散技术。

Result: 建立了完整的扩散生成模型理论框架，证明了变分下界简化为实践中使用的噪声预测目标，推导了概率流ODE与扩散SDE的关系，展示了整流流如何恢复DDIM，并为分类器引导和分类器无关引导提供了理论解释。

Conclusion: 通过透明的代数推导、明确的中间步骤和一致的符号表示，为扩散生成模型提供了系统完整的理论基础，使读者能够深入理解模型原理并实现相应算法。

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [84] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: 提出IDOL框架，通过物理先验知识引导的身份约束来处理热带气旋估计中的分布偏移问题，提高在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 热带气旋估计面临复杂动态环境场导致的分布偏移挑战，现有方法忽视特征表示的内在分布，导致在分布外场景下泛化能力差。

Method: IDOL框架利用风场模型和暗相关知识建模任务共享和任务特定的身份令牌，通过物理不变性约束特征空间，处理分布变异性。

Result: 在多个数据集和任务上的实验表明IDOL表现优异，验证了基于物理先验知识的身份约束能有效缓解热带气旋估计中的分布偏移。

Conclusion: 基于物理先验知识的身份导向约束能有效处理热带气旋估计中的分布变异性，提高模型在分布外场景下的鲁棒性。

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [85] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 提出了一种混合GraphSAGE网络(H-GSN)用于供应链物流管理的多任务预测，包括货物类型、物流状态、交通状况、物流ID和物流延迟预测，在三个不同数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 系统化物流管理对供应链盈利发展至关重要，需要提高供应链韧性，减少空气污染物排放，并通过自动预测方法来提升供应链管理效率。

Method: 使用混合GraphSAGE网络(H-GSN)进行多任务学习，在三个供应链物流数据集(DataCo、Shipping和Smart Logistics)上预测货物类型、物流状态、交通状况、物流ID和物流延迟。

Result: 在Smart Logistics数据集上，物流ID预测准确率达97.8%，交通状况预测准确率达100%；在DataCo数据集上货物类型预测准确率达98.7%；在Shipping数据集上物流延迟预测准确率达99.4%。

Conclusion: 所提出的方法在不同物流场景下均表现出高效性，能够有效提高供应链的韧性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [86] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: SNO是一种基于Sumudu变换的神经算子，在ODE和PDE问题上表现优异，在PDE上优于FNO，与LNO竞争，并在零样本超分辨率任务中展现潜力。


<details>
  <summary>Details</summary>
Motivation: 探索Sumudu变换作为神经算子设计的潜力，特别是针对特定类型的偏微分方程。

Method: 利用变换对的多项式展开关系分解输入空间为系数，然后转换到Sumudu空间进行神经算子参数化。

Result: 在多个ODE和PDE任务中取得优异性能，在Euler-Bernoulli梁和扩散方程上获得最低误差，零样本超分辨率实验显示能从低质量样本获得更高质量数据。

Conclusion: Sumudu变换作为神经算子设计具有前景，尤其适用于某些类型的偏微分方程。

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [87] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: 该论文提出将Kolmogorov-Arnold网络（KANs）集成到公平对抗学习框架中，通过自适应惩罚更新机制动态调整公平约束，在保持高预测准确性的同时实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平学习模型在公平性和准确性之间的权衡仍具挑战性，且黑盒模型缺乏可解释性，限制了在敏感领域（如大学录取）的应用。

Method: 将KANs集成到公平对抗学习框架中，利用KANs的对抗鲁棒性和可解释性，并提出自适应惩罚更新机制动态调整训练过程中的公平约束。

Result: 在两个真实世界大学录取数据集上的实验表明，该方法在三种不同优化策略下均优于基线公平学习模型，在保持高预测准确性的同时实现了跨敏感属性的竞争性公平。

Conclusion: KANs在公平机器学习中表现出高效性和鲁棒性，能够平衡公平性和准确性，为敏感决策领域提供了可解释的解决方案。

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [88] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: 提出了CATCHFed方法，通过客户端感知的自适应阈值、混合阈值和一致性正则化，解决半监督联邦学习中标签数据稀缺导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端通常缺乏标签数据，而现有方法在标签数据减少时性能显著下降。

Method: 引入客户端感知的自适应阈值（考虑类别难度）、混合阈值提升伪标签质量，并利用未伪标签数据进行一致性正则化。

Result: 在多种数据集和配置下的实验表明，CATCHFed能有效利用未标记客户端数据，在极端有限标签设置下仍能获得优越性能。

Conclusion: CATCHFed是解决半监督联邦学习标签稀缺问题的有效方法，显著提升了模型性能。

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [89] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 提出了一种基于坐标下降的离散优化方法，直接在离散域中减少ReLU激活函数的数量，以解决私有推理中的延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数是基于ResNet网络的私有推理中的主要瓶颈，因为它们会导致显著的推理延迟。现有的平滑近似方法在最后硬阈值步骤中通常会造成较大的性能损失。

Method: 采用坐标下降作为优化框架，直接在离散域中工作，通过设计产生稀疏解。

Result: 通过广泛实验证明，该方法在常见基准测试中达到了最先进的性能。

Conclusion: 提出的离散优化方法能够有效减少ReLU数量，同时保持网络性能，在私有推理中实现了更好的延迟优化。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [90] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: 该论文将经典拓扑维度理论应用于二元概念类，定义了单纯覆盖维度，并证明对于有限概念类，该维度精确刻画了PAC学习中的列表可复制数。


<details>
  <summary>Details</summary>
Motivation: 将拓扑维度理论引入机器学习领域，为分析概念类的学习复杂性提供新的数学工具，特别是解决列表可复制性的计算问题。

Method: 将概念类关联到可实现分布空间，通过损失函数和概念类本身诱导单纯结构，在此基础上定义单纯覆盖维度。

Result: 证明对于有限概念类，单纯覆盖维度精确等于列表可复制数，并应用经典维度理论工具计算了极值概念类的精确列表可复制数。

Conclusion: 拓扑维度理论为分析概念类的学习复杂性提供了有效的数学框架，单纯覆盖维度是列表可复制性的精确表征。

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [91] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出CCPO方法，通过结合多个不同成本/准确率权衡的LLM模型，在保证可靠性的前提下最小化计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但计算成本和API成本日益昂贵，需要找到在保证可靠性的同时降低成本的解决方案

Method: Conformal Constrained Policy Optimization (CCPO) - 结合约束策略优化、离线强化学习和在线符合预测，联合优化成本感知策略和自适应阈值

Result: 在两个多跳问答基准测试中，CCPO相比其他成本感知基线和LLM引导方法，成本降低高达30%，同时不损害可靠性

Conclusion: 该方法为部署LLM智能体提供了一个原则性和实用的框架，在保持可靠性的同时显著提高成本效益

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [92] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: 本文研究了Volatility in Certainty (VC)这一无需标签的度量指标，用于评估神经网络分类器在对抗性攻击下的性能退化。实验表明VC与分类准确率呈强负相关，可作为实时安全监控指标。


<details>
  <summary>Details</summary>
Motivation: 在实时系统中，由于缺乏真实标签，对抗性鲁棒性评估面临挑战。需要一种无需标签的指标来检测模型性能退化和对抗性漂移。

Method: 定义VC为排序后softmax输出相邻置信度比值的平均平方对数，捕捉模型输出的局部波动。在MNIST和CIFAR-10数据集上使用ANN、CNN和VGG模型，通过FGSM生成对抗样本，并构建混合测试集评估VC敏感性。

Result: 分类准确率与log(VC)呈现强负相关（大多数情况下rho < -0.90），表明VC能有效反映性能退化，无需标签数据。

Conclusion: VC是一种可扩展、架构无关的实时性能度量指标，适用于安全关键应用的早期预警系统。

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [93] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文通过可转移对抗样本攻击研究AI透明度与安全性的战略冲突，发现攻击者与防御者决策匹配时攻击成功率更高，表明不透明性可能有利于防御者。


<details>
  <summary>Details</summary>
Motivation: 研究在对抗性环境中AI透明度与安全性之间的潜在冲突，特别是在可转移对抗样本攻击场景下，探讨透明度是否可能损害系统安全性。

Method: 使用大规模实证评估（9种攻击方法、181个模型）并结合博弈论分析，将问题建模为纳什博弈和斯塔克尔伯格博弈来比较预期结果。

Result: 攻击者与防御者决策匹配时攻击成功率更高；仅知道防御者模型是否被防御就足以损害其安全性；透明度与安全性存在冲突。

Conclusion: AI系统的透明度可能与安全性相冲突，博弈论分析揭示了透明度与安全性之间的普遍权衡关系。

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [94] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: 该研究探讨了在时间序列模型中整合领域知识对水文降雨-径流建模的影响，发现包含全面已知外生输入的模型优于基础模型，其中自然年度周期性时间序列的贡献最为显著。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型研究取得进展，但在物理科学特定下游应用中的有效性研究较少。本研究旨在考察将领域知识整合到时间序列模型中对水文降雨-径流建模的作用。

Method: 使用CAMELS-US数据集，包含671个地点的降雨和径流数据，具有6个时间序列流和30个静态特征。比较基线和基础模型，特别关注整合已知外生输入的效果。

Result: 结果表明，包含全面已知外生输入的模型表现优于更有限的方法，包括基础模型。自然年度周期性时间序列的整合带来了最显著的改进。

Conclusion: 在水文建模中，整合领域知识特别是自然年度周期性时间序列，能够显著提升模型性能，这为时间序列基础模型在物理科学应用中的优化提供了重要启示。

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [95] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在森林CO2吸收量预测中的表现，发现LSTM整体精度更高，但GPT-2在极端事件中表现更好，且LSTM需要更短的输入窗口就能达到相似精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感方法难以捕捉GPP复杂时间动态的问题，探索深度学习模型在融合多源数据预测森林CO2吸收量方面的潜力。

Method: 使用GPT-2（Transformer架构）和LSTM（循环神经网络）两种代表性模型，结合多变量输入（辐射、Sentinel-2、MODIS地表温度、Sentinel-1等）进行GPP预测。

Result: 两种模型达到相似精度，LSTM整体表现更好但GPT-2在极端事件中更优；LSTM使用更短的输入窗口就能达到GPT-2的精度水平；辐射是最重要的预测因子。

Conclusion: 模型架构、上下文长度和多模态输入共同决定了GPP预测性能，为未来开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [96] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无需监督训练的LLM对抗学习框架，通过让Proposer生成挑战性问题、Solver尝试解决这些问题，实现两个模型的相互进化，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练严重依赖外部监督，而对抗学习特别是双角色对抗训练能够减少对外部监督的依赖，但LLM容易奖励破解和训练不稳定限制了其应用。

Method: 从同一基础模型初始化Proposer和Solver两个模型：Proposer生成带真实答案的挑战性问题，Solver尝试解决；Proposer从预训练数据集中获取知识确保问题质量；采用联合更新避免奖励破解；引入可选离线范式增强训练稳定性。

Result: 实验结果显示PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble框架成功实现了无需监督的LLM对抗训练，通过双角色对抗学习有效提升了模型推理能力。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [97] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX是一个模型和领域无关的框架，将反事实解释转化为局部、区域和全局层面的特征变化频率分数，弥合了局部反事实解释和全局特征归因之间的差距。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏可解释性限制了在高风险环境中的安全部署。反事实解释通常保持实例特定性，无法量化哪些特征在特征空间的连贯区域或整个数据集中系统地驱动结果变化。

Method: FLEX框架通过聚合跨实例和邻域的反事实解释，将特征变化频率转化为可解释的排名，反映每个特征需要改变多少次才能翻转预测。该框架兼容不同的反事实生成方法。

Result: 在交通事故严重性预测和贷款审批任务上的评估显示：FLEX的全局排名与SHAP相关但揭示了额外驱动因素；区域分析揭示了全局摘要遗漏的上下文特定因素。

Conclusion: FLEX弥合了局部反事实解释和全局归因之间的差距，支持风险敏感应用中透明和面向干预的决策制定。

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [98] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: 本文提出Chain-of-Generation (CoG)框架，通过多阶段潜在扩散模型解决文本条件分子生成中一次性条件编码的问题，提高语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的分子生成方法采用一次性条件编码，难以满足提示中的所有要求，存在生成组件可解释性差、无法生成所有子结构、同时考虑所有要求过于激进等问题。

Method: 提出CoG框架：1）将提示分解为课程排序的语义片段；2）逐步将这些片段作为中间目标；3）引入后对齐学习阶段加强文本和分子潜在空间的对应关系。

Result: 在基准和实际任务上的实验表明，CoG比一次性基线方法在语义对齐、多样性和可控性方面表现更好，能更忠实地反映复杂组合提示。

Conclusion: CoG通过多阶段生成过程有效解决了文本条件分子生成中的挑战，提供了更透明和可控的生成过程。

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [99] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的无梯度训练算法B-SRA，通过正交权重矩阵和梯度模式对齐原则显著提升了双向联想记忆的鲁棒性和收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统双向联想记忆使用双向反向传播训练时，对噪声和对抗攻击的鲁棒性较差且敏感度高，需要提升其抗干扰能力。

Method: 提出了双向子空间旋转算法(B-SRA)作为无梯度训练方法，并基于正交权重矩阵(OWM)和梯度模式对齐(GPA)原则开发了新的正则化策略。

Result: SAME配置（结合OWM和GPA）在所有方法中表现出最强的鲁棒性，在50、100、200个关联对的不同记忆容量下均保持良好性能。

Conclusion: B-SRA和提出的正则化策略显著增强了联想记忆的鲁棒性，为构建弹性神经网络架构开辟了新方向。

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [100] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: 本文首次系统研究了针对图基础模型(GFMs)的模型提取攻击(MEAs)，揭示了GFMs显著扩大了MEA的攻击面，攻击者仅需原始训练成本的极小部分就能近似受害者模型，且准确率几乎无损失。


<details>
  <summary>Details</summary>
Motivation: 随着图机器学习的发展，图基础模型(GFMs)因其联合预训练图和文本编码器、支持零样本推理等能力而成为重要资产，但其高预训练成本和跨领域知识也使其成为模型提取攻击的有吸引目标。现有研究仅关注小型图神经网络，对大规模多模态GFMs的安全影响尚未探索。

Method: 提出了一种轻量级提取方法，通过监督回归图嵌入来训练攻击者编码器。该方法无需对比预训练数据，即可学习与受害者文本编码器保持对齐的编码器，并在未见图上保持零样本推理能力。定义了六种实际攻击场景，包括领域级和图特定提取目标、架构不匹配、有限查询预算、部分节点访问和训练数据差异。

Result: 在七个数据集上的实验表明，攻击者仅需原始训练成本的极小部分就能近似受害者模型，准确率几乎无损失。

Conclusion: GFMs显著扩大了MEA的攻击面，突显了在大规模图学习系统中部署感知安全防御的必要性。

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [101] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 该论文提供了多层感知机(MLP)的完整批量矩阵形式推导，包括前向和反向传播的数学严谨规范，并通过符号验证和多种框架实现来建立神经网络算法的可扩展基础。


<details>
  <summary>Details</summary>
Motivation: 虽然多层感知机是现代深度学习的基础，但其算法细节很少以完整的批量矩阵形式呈现，大多数参考资料要么按样本表达梯度，要么依赖自动微分。缺乏显式的批量矩阵形式使得计算结构不明确，这对于透明、系统的分析和优化(如稀疏神经网络)至关重要。

Method: 1. 推导所有标准层和高级层(包括批归一化和softmax)的前向和反向传播方程的批量矩阵形式
2. 使用符号数学库SymPy验证所有梯度方程
3. 在NumPy、PyTorch、JAX、TensorFlow和优化的C++后端构建统一的参考实现
4. 基于少量矩阵原语构建实现

Result: 1. 完成了MLP批量矩阵形式反向传播的完整推导
2. 所有梯度方程都通过了符号验证
3. 建立了基于Python和C++的统一参考实现
4. 展示了显式公式化如何实现高效的稀疏计算

Conclusion: 这些结果建立了一个经过验证、可扩展的基础，用于理解、教学和研究神经网络算法，特别是在需要透明计算结构的场景中。

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [102] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: 本文提出了插值拉普拉斯嵌入(ILEs)，这是一种从图矩阵家族中导出的节点特征增强方法，用于改善图神经网络在节点特征有限时的性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络的性能严重依赖于信息丰富的节点特征，但在实际应用中这些特征往往有限或缺失。虽然通常使用拉普拉斯谱嵌入来增强特征，但作者探索是否其他图矩阵的谱嵌入也能提供有用的表示。

Method: 引入插值拉普拉斯嵌入(ILEs)，这是从简单而富有表达力的图矩阵家族中导出的嵌入方法。使用谱图理论工具来解释ILEs捕获的结构信息。

Result: 通过模拟和真实世界数据集的实验表明，通过ILEs进行特征增强可以在常用的GNN架构中提高性能。

Conclusion: ILEs为实践者提供了一个简单实用的方法，在节点特征有限时扩展了谱增强工具包。

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [103] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 本文系统比较了CLIP分层机制下的OOD检测方法，发现特征空间决定检测效果，概率得分在误分类检测中占优，几何感知得分在CNN上对强偏移更有效，ViT上GradNorm和KPCA重建误差表现稳定。


<details>
  <summary>Details</summary>
Motivation: 研究不同OOD检测方法在CLIP分层机制下的表现差异，探索特征空间对检测效果的影响，为分布偏移下的方法选择提供统计依据。

Method: 使用AURC和AUGRC作为主要指标，比较CNN从头训练和ViT微调两种表示范式，在CIFAR-10/100、SuperCIFAR-100和TinyImageNet上进行实验，采用多重比较控制的基于排名的分析流程。

Result: 学习到的特征空间很大程度上决定了OOD检测效果；概率得分在误分类检测中占主导；在强分布偏移下，几何感知得分在CNN上表现更好，而在ViT上GradNorm和KPCA重建误差保持竞争力；MCD存在类别数量相关的权衡；简单PCA投影可改善多个检测器。

Conclusion: 结果支持OOD检测的表示中心视角，并为分布偏移下的方法选择提供了统计基础指导。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [104] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench是一个开源预处理流水线，将原始PhysioNet数据集转换为标准化的、模型就绪的张量，用于多模态生存分析，解决了深度学习生存模型比较中的预处理不一致问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据为深度学习生存分析提供了巨大机会，但由于预处理方法不一致，可重复性受到严重限制。

Method: SurvBench提供三个主要重症监护数据库的数据加载器，支持时间序列生命体征、静态人口统计、ICD诊断代码和放射学报告等多种模态，实施严格的数据质量控制、患者级分割、显式缺失值跟踪和标准化时间聚合。

Result: 该流水线处理单风险和竞争风险场景，输出与pycox库包和标准统计及深度学习模型实现兼容。

Conclusion: 通过提供可重复的、配置驱动的预处理和全面文档，SurvBench解决了阻碍深度学习生存模型公平比较的"预处理差距"，使研究人员能够专注于方法创新而非数据工程。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [105] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: 提出了PARS预训练方法，通过预测随机采样的EEG窗口对之间的相对时间偏移来学习脑电图的长程依赖关系，在标签高效和迁移学习设置中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前EEG自监督学习方法主要使用掩码重建策略，虽然能捕捉局部时间模式，但位置预测预训练在捕捉神经信号长程依赖关系方面的潜力尚未充分探索。

Method: 引入PARS预训练，这是一种新的预训练任务，预测随机采样的EEG窗口对之间的相对时间偏移，鼓励编码器捕捉神经信号中的相对时间组成和长程依赖关系。

Result: 在各种EEG解码任务上的综合评估表明，PARS预训练的transformer在标签高效和迁移学习设置中始终优于现有的预训练策略。

Conclusion: PARS为自监督EEG表示学习建立了新的范式，能够有效学习神经信号的长程依赖关系。

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [106] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys是一个基于电池感知的联邦学习框架，通过循环客户端参与机制减少能耗，特别适用于能量收集环境。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式环境中面临高能耗问题，特别是在能量收集系统中，设备参与度因电池限制而不稳定。

Method: 通过客户端聚类和顺序调度，基于电池水平实现循环参与，并引入FedBacys-Odd变体进行选择性参与。

Result: 框架减少了冗余计算和系统能耗，提高了学习稳定性，在数值实验中表现出优于现有算法的能效和鲁棒性。

Conclusion: FedBacys通过电池感知的循环参与机制有效解决了能量收集联邦学习系统的能耗问题，提供了收敛性保证和实际性能提升。

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [107] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出了一种改进的离线强化学习方法，通过基于分位数回归的温度系数估计和价值正则化技术，解决了XQL和MXQL方法中存在的超参数调优困难和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在固定数据集上学习策略，适用于高风险或成本高昂的领域。但现有的XQL及其稳定变体MXQL存在两个主要问题：需要针对每个数据集和领域进行大量超参数调优，以及训练过程中表现不稳定。

Method: 1) 基于温和假设使用分位数回归来估计温度系数β；2) 引入受约束价值学习启发的价值正则化技术，具有温和泛化性，以进一步提高训练稳定性。

Result: 在D4RL和NeoRL2等基准任务上的实验结果表明，所提出的算法在保持稳定训练动态的同时，使用跨所有数据集和领域的一致超参数集，实现了竞争性或更优的性能。

Conclusion: 该方法成功解决了离线强化学习中超参数调优困难和训练不稳定的问题，为实际应用提供了更可靠的解决方案。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [108] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: 提出ReCast框架，通过可学习码本对局部模式进行离散化编码，采用双路径架构分别处理规则结构和不规则波动，并通过可靠性感知的码本更新策略增强对分布偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全局分解为趋势、季节性和残差分量，在处理局部复杂动态模式时效果不佳，且模型复杂度高限制了在实时或资源受限环境中的应用。

Method: 使用可学习码本对局部模式进行补丁级量化编码，采用双路径架构（量化路径+残差路径），通过分布鲁棒优化融合多可靠性因子进行码本更新。

Result: 在准确性、效率和分布偏移适应性方面优于现有最先进模型。

Conclusion: ReCast框架通过局部模式编码和可靠性感知更新，实现了轻量级且鲁棒的时间序列预测。

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [109] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA是一个通过QuizRank方法自动选择高质量训练图像来进行LoRA微调的框架，能够用更少样本生成更对齐、更逼真的图像。


<details>
  <summary>Details</summary>
Motivation: 在微调文本到图像扩散模型时，从质量参差不齐的图像集（如维基共享资源）中选择训练样本往往导致输出质量不佳，需要选择能代表目标概念的高质量图像。

Method: 提出QZLoRA框架，利用QuizRank方法将图像视为'教育干预'并通过VLM'测验'来自动排名图像，用于低秩适应（LoRA）微调。

Result: QZLoRA能够用更少样本生成更对齐、更逼真的图像，并且这些微调模型也能生成具有代表性的风格化插图。

Conclusion: 将自动视觉推理与参数高效微调相结合，为主题自适应生成建模提供了有前景的方法。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [110] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: EARL是一个基于熵感知的强化学习框架，用于Verilog代码生成，通过聚焦高熵关键令牌来提高功能正确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成中存在语法错误、功能幻觉和设计意图对齐弱的问题，需要更有效的强化学习方法来利用硬件可验证信号。

Method: 提出EARL框架，使用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度限制在高熵令牌上。

Result: 在VerilogEval和RTLLM基准测试中，EARL将功能通过率提高了14.7%，同时减少了不必要的更新并提升了训练稳定性。

Conclusion: 将强化学习聚焦于关键高熵令牌，能够实现更可靠和有针对性的策略改进，适用于结构化RTL代码生成。

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [111] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度图变换器的超分辨率方法（SR-GT），用于反应流场的网格超分辨率重建，相比传统插值方法具有更高精度。


<details>
  <summary>Details</summary>
Motivation: 超分辨率流场重建在亚网格闭合建模、时空预测加速、数据压缩和实验测量升尺度等方面具有重要应用价值。

Method: 采用基于图的流场表示方法，兼容复杂几何和非均匀/非结构化网格；使用变换器架构捕捉低分辨率流场的长程依赖关系，识别重要特征并生成高分辨率流场。

Result: 在2D氢气-空气预混爆轰传播的挑战性测试问题中，SR-GT在反应流场特征上表现出高精度的超分辨率重建能力。

Conclusion: SR-GT框架相比传统插值超分辨率方案具有更优越的性能，能够有效处理复杂多尺度反应流行为。

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [112] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出了一种集成知识补全阶段的图机器学习流程，通过基于衰减的推理函数建模隐藏连接来重塑图拓扑，显著改变嵌入空间几何结构


<details>
  <summary>Details</summary>
Motivation: 由于图嵌入仅从显式拓扑和特征推导，可能错过稀疏数据集中隐藏的关键隐式知识，影响图结构及其表示质量

Method: 在嵌入生成前引入知识补全阶段，聚焦传递关系，使用基于衰减的推理函数建模隐藏连接，重塑图拓扑结构

Result: 实验表明该流程显著改变了嵌入空间的几何结构，证明其引入不仅是简单的丰富，而是重新定义图表示质量的变革性步骤

Conclusion: 集成知识补全的图机器学习流程能够发现潜在数据集语义，从根本上提升图表示的质量和效果

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [113] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: 提出了一个名为TreatStitch的数据增强框架，通过智能拼接现有治疗数据中的片段来生成临床有效的治疗轨迹，从而解决离线强化学习在临床数据稀缺情况下的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在优化自适应治疗策略方面面临数据稀缺的挑战，而传统的在线强化学习在临床环境中不可行，因为可能对患者造成伤害。

Method: TreatStitch框架通过识别不同轨迹中相似的中间患者状态并拼接相应片段来生成治疗轨迹。当状态差异过大时，使用薛定谔桥方法生成平滑的桥接轨迹来连接不相似的状态。

Result: 在多个治疗数据集上的广泛实验表明，TreatStitch能有效提升离线强化学习的性能。

Conclusion: TreatStitch通过数据增强提高了离线强化学习在临床环境中的实用性，同时通过理论证明保持了临床有效性，避免了分布外转移。

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [114] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D是一个基于RGB-D扫描直接预测3D路径损耗热图的端到端框架，无需显式几何重建或材料标注，实现了可扩展、高效且物理一致的室内传播建模。


<details>
  <summary>Details</summary>
Motivation: 现有室内无线电传播建模方法依赖人工几何和材料属性建模，导致可扩展性和效率受限。

Method: 构建感知驱动的体素化场景表示，联合编码占用率、电磁材料特性和收发器几何结构，使用SwinUNETR神经网络推断环境路径损耗。

Result: 在未见环境中实现4.27 dB的平均绝对误差，支持217 ms/样本的实时推理，展示了可扩展性、效率和物理一致性。

Conclusion: SenseRay-3D为感知驱动、可泛化且物理一致的室内传播建模开辟了新途径，是EM DeepRay框架的重大突破。

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [115] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: 本文研究了显式对齐在多模态学习中对模型性能的影响，发现最佳对齐强度取决于模态间的冗余度，并提出了可控对比学习模块来精确调节对齐强度。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习普遍假设表示对齐总是有益的，但缺乏对显式对齐在不同模态信息结构下影响的系统性研究。

Method: 引入可控对比学习模块，通过精确操纵训练过程中的对齐强度，研究不同数据特征下显式对齐对性能的影响。

Result: 在合成和真实数据集上的实验表明，显式对齐对单模态模型性能的影响与数据特征相关，最佳对齐强度取决于模态间的冗余度。

Conclusion: 本研究为在何时以及如何应用显式对齐以获得最佳单模态编码器性能提供了实用指导。

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [116] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: 提出基于Transformer的实时会计交易异常检测方法，通过时间序列建模和多头自注意力机制捕捉全局依赖关系，在公开数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂交易环境中隐藏异常行为检测和高时效性要求的挑战，为智能金融风控和审计提供方法支持。

Method: 将多维会计交易记录建模为时间序列矩阵，使用嵌入层和位置编码进行低维映射，构建多头自注意力序列建模结构，集成前馈层和正则化策略实现深度特征表示。

Result: 在AUC、F1-Score、精确率和召回率等指标上优于基线模型，在不同环境条件和数据扰动下保持稳定性能。

Conclusion: 基于Transformer的框架在会计交易动态异常检测中具有适用性和优势，为智能金融风险控制提供了有效方法。

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [117] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出了一种基于指挥者的联合策略框架HCPO，通过协调多智能体探索来提升合作MARL的性能和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法通过独立智能体探索更新联合策略，缺乏协调，限制了联合策略的表达能力和探索效率

Method: 使用指挥者框架增强联合策略表达能力并协调探索，开发分层指挥者策略优化算法，通过局部指挥者保持集中训练优势同时消除执行时的通信需求

Result: 在StarCraftII、Multi-agent MuJoCo和Multi-agent Particle Environment三个基准测试中，HCPO在合作效率和稳定性方面优于现有MARL基线方法

Conclusion: HCPO通过协调探索和增强联合策略表达能力，显著提升了合作MARL的性能，并提供了理论保证

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [118] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: FairGSE通过最大化二维结构熵来提升图神经网络的公平性，同时显著降低假阳性率，在高风险场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知GNN在追求公平性指标时忽视了负标签预测能力，导致假阳性率极高，在高风险场景中产生负面影响。

Method: 提出FairGSE框架，通过最大化二维结构熵来改善公平性，同时不忽视假阳性问题。

Result: 在多个真实数据集上，FairGSE相比最先进的公平感知GNN将假阳性率降低了39%，同时保持相当的公平性改进。

Conclusion: 在提升公平性的同时需要仔细校准分类性能，而不仅仅是限制准确率损失，FairGSE为此提供了有效解决方案。

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [119] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: 提出一个用于非侵入式负载监测(NILM)分类任务的端到端框架，包含高频标记数据、特征提取方法和轻量级神经网络，通过融合ICA和PCA特征提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界NILM部署面临的过拟合、模型泛化能力差以及同时运行大量电器时的分解难题。

Method: 提出融合独立成分分析(ICA)和主成分分析(PCA)的特征提取方法，以及轻量级的多标签NILM分类架构Fusion-ResNet。

Result: 相比最先进的NILM分类器，该模型在平均F1分数和不同电器上的表现更优，同时最小化了训练和推理时间，在多达15个同时活跃电器的情况下仍保持相对鲁棒性。

Conclusion: 提出的基于特征的Fusion-ResNet模型在NILM分类任务中表现出色，能够有效处理多电器同时运行的情况，具有实际部署价值。

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [120] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒损失函数家族——变分有界损失(VBL)，通过引入变分比作为衡量损失函数鲁棒性的新属性，并证明较小的变分比能带来更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 缓解噪声标签对监督学习的负面影响是一个长期存在的问题，鲁棒损失函数是解决该问题的流行方案。

Method: 引入变分比作为损失函数鲁棒性的新属性，提出变分有界损失(VBL)家族，重新构建了多个常用损失函数为变分有界形式。

Result: 在各种数据集上的实验证明了该方法的有效性和灵活性。

Conclusion: 变分比提供了一种放松对称条件的可行方法，并为实现非对称条件提供了更简洁的路径。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [121] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: GBOC是一种基于粒度球向量数据描述(GVDD)的时间序列异常检测方法，通过密度引导的分层分裂过程生成紧凑的高密度区域表示，显著减少原型数量，提高检测的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法如最近邻和聚类在复杂时间序列场景中依赖预定义的邻居或聚类数量等刚性假设，这些假设在复杂时间序列中经常失效，需要更灵活的数据自适应方法。

Method: 提出粒度球单类网络(GBOC)，使用GVDD将潜在空间划分为紧凑的高密度区域(粒度球)，通过密度引导分层分裂生成并去除噪声结构，每个粒度球作为局部正常行为的原型。

Result: 大量实验验证了该方法的有效性和优越性，能够有效处理时间序列异常检测的挑战，在保持鲁棒性的同时提高了检测效率。

Conclusion: GBOC通过数据自适应的粒度球表示，克服了传统方法的局限性，为复杂时间序列异常检测提供了既鲁棒又高效的解决方案。

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [122] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出了一种多模态基础模型，整合结构化属性和非结构化文本描述，在金融交易中表现优于传统方法，特别是在数据稀缺的开放银行场景中有效。


<details>
  <summary>Details</summary>
Motivation: 解决金融交易中传统特征工程和离散事件序列方法的局限性，特别是在数据稀缺的开放银行场景下，需要能够整合多模态信息并跨地理和机构泛化的解决方案。

Method: 采用掩码语言建模方法处理交易序列，将结构化属性和非结构化文本描述整合为统一表示，构建多模态基础模型。

Result: 模型在北美数千家金融机构的大规模研究中表现优异，超越了传统特征工程和离散事件序列方法，特别是在数据稀缺场景下效果显著。

Conclusion: 多模态表示能够跨地理和机构泛化，自监督模型在欺诈预防、信用风险和客户洞察等金融应用中具有巨大潜力。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [123] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 论文揭示了语言模型安全对齐的系统性漏洞源于自回归训练中的位置依赖梯度衰减，提出了基于基础模型偏好标记的针对性补全方法，显著提升了对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐训练，大语言模型仍然存在系统性的对抗攻击漏洞，需要深入理解其机制并提供解决方案。

Method: 引入基础偏好标记作为不完全安全学习的计算指标，开发了包含自适应惩罚和混合教师蒸馏的针对性补全方法。

Result: 在Llama和Qwen模型家族上的实验显示，攻击成功率降低了48-98%，同时保持了通用能力。

Conclusion: 该研究为安全对齐方法的基本局限性提供了机制性理解和实用解决方案。

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [124] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: 提出了一种轻量级神经网络架构Residual-MLP-RNN和三阶段训练流程，用于在标签稀缺情况下实现鸟类鸣声自动标注，并在复杂的金丝雀鸣声上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 鸟类鸣声研究需要精确的音节级标注，但人工标注成本高昂，因此需要开发自动化和数据高效的方法来降低标注成本。

Method: 使用Residual-MLP-RNN架构，采用三阶段训练流程：1）无监督预训练（掩码预测和在线聚类）；2）带数据增强的监督训练；3）与下游任务对齐的半监督后训练。

Result: 在标签稀缺的极端场景下，该方法对复杂的金丝雀鸣声实现了有效标注，验证了方法的鲁棒性和泛化能力。

Conclusion: 该方法为鸟类鸣声分析提供了一种数据高效的解决方案，能够显著减少专家标注工作量，同时展示了自监督嵌入在无监督鸟类鸣声分析中的潜力。

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [125] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程回归的功能梯度材料体积分数分布生成算法，能够处理复杂形状域并生成平滑的FGM分布，结合改进的遗传算法进行优化设计。


<details>
  <summary>Details</summary>
Motivation: 解决任意形状域中功能梯度材料设计的挑战，传统方法难以处理复杂几何形状和边界条件。

Method: 使用高斯过程回归生成体积分数分布，通过长度尺度参数控制平滑度和设计空间大小，结合改进的遗传算法进行优化。

Result: 算法能够生成平滑的FGM分布，满足边界体积分数约束，并通过热弹性优化示例验证了方法的有效性。

Conclusion: 提出的GPR方法为复杂形状域的功能梯度材料设计提供了通用且有效的解决方案，扩展了FGM设计的应用范围。

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [126] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: TSGDiff是一个基于图神经网络的时间序列生成框架，通过将时间序列表示为动态图，利用扩散模型生成高质量合成数据，并提出了拓扑结构保真度(Topo-FID)评估指标。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成方面表现出色，但生成时间序列数据仍然具有挑战性，需要捕捉复杂的时间依赖性和结构模式。

Method: 将时间序列表示为基于傅里叶谱特征和时间依赖性构建的动态图，采用图神经网络编码器-解码器架构构建潜在空间，使扩散过程能够有效建模时间序列的结构表示分布。

Result: 在真实世界数据集上的实验表明，TSGDiff能够生成高质量的合成时间序列数据，忠实地保留时间依赖性和结构完整性。

Conclusion: TSGDiff通过图视角重新思考时间序列生成，提出的Topo-FID指标为评估生成时间序列的结构相似性提供了更准确的度量，推动了合成时间序列生成领域的发展。

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [127] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: 本文提出了SC-InfoNCE损失函数，通过引入可调节的收敛目标来灵活控制特征相似性对齐，在图像、图结构和文本任务上均取得稳定优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管InfoNCE在对比学习中取得了经验成功，但其理论基础仍然有限。作者旨在深入理解InfoNCE的理论机制，并改进其性能。

Method: 引入显式特征空间建模样本的增强视图，使用转移概率矩阵捕捉数据增强动态。提出SC-InfoNCE损失函数，通过缩放目标矩阵来灵活控制特征相似性对齐。

Result: 在图像、图结构和文本等基准数据集上的实验表明，SC-InfoNCE在不同领域都能实现强大且可靠的性能。

Conclusion: SC-InfoNCE通过引入可调节的收敛目标，能够更好地匹配下游数据的统计特性，为对比学习提供了更灵活有效的优化目标。

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [128] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: 本文研究了联邦学习场景下模型规模扩展的理论基础，通过PAC-Bayes理论推导出泛化误差上界，发现最优模型规模与客户端数量呈负幂律关系，并验证了联邦学习会降低模型可达到的泛化性能上界。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，高质量训练数据日益稀缺，联邦学习成为利用边缘设备数据同时保护隐私的可行方案。但联邦学习中的分布式数据集给模型规模扩展带来了挑战，这一领域尚未得到充分探索。

Method: 推导了联邦学习场景下随机算法训练模型的PAC-Bayes泛化误差上界，通过解析求解最小化该上界的模型规模，量化了分布式训练数据对最优模型规模的影响。

Result: 理论分析表明，在总训练计算量不变的情况下，最优模型规模与客户端数量呈负幂律关系。联邦学习会降低模型可达到的泛化性能上界，且最优模型规模估计应基于客户端的平均训练计算量。

Conclusion: 本文填补了联邦学习中模型规模扩展的理论空白，为联邦学习场景下的模型设计提供了理论指导，并通过大量实验验证了理论结果的正确性。

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [129] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 提出了一种可靠的方法来比较多目标优化算法（返回帕累托前沿）与单解算法，重点关注算法评估而非学习过程。


<details>
  <summary>Details</summary>
Motivation: 在多目标机器学习任务中，如何可靠比较返回帕累托前沿的MOO算法与返回单一解的方法存在显著方法学空白。

Method: 提出新的评估方法，通过考虑用户偏好从帕累托前沿中选择合适解，与单解算法进行公平比较。

Result: 开发了能够可靠比较多目标优化算法与单解算法的评估框架。

Conclusion: 填补了分类器评估方法学中的重要空白，为多目标优化算法与单解算法的公平比较提供了可行方案。

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [130] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: 本文研究了膜电位分布与替代梯度函数的关系对SNN鲁棒性的影响，提出了一种MPD驱动的替代梯度正则化方法，显著提升了SNN对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 替代梯度方法虽然提升了深度SNN性能，但也使其易受对抗攻击。目前研究主要关注脉冲编码策略和神经动态参数，而梯度幅度（反映模型对输入扰动的敏感性）的作用尚未充分探索。

Method: 提出MPD驱动的替代梯度正则化方法，通过基于膜电位分布与替代梯度函数交互的正则化来增强鲁棒性，理论分析表明减少膜电位在梯度可用范围内的比例可降低SNN对输入扰动的敏感性。

Result: 在多个图像分类基准和不同网络架构上的实验表明，MPD-SGR方法显著提升了SNN对对抗扰动的鲁棒性，并在不同网络配置、替代梯度函数变体和脉冲编码方案中表现出强泛化能力。

Conclusion: 膜电位分布与替代梯度函数的交互关系对SNN鲁棒性至关重要，MPD-SGR方法通过正则化这种交互有效增强了SNN的对抗鲁棒性。

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [131] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: AlignTree是一种高效防御机制，通过监控LLM激活并使用随机森林分类器检测未对齐行为，无需额外提示或辅助防护模型。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击防御方法要么计算成本高，要么防御效果差，不适用于实际LLM系统。

Method: 使用随机森林分类器监控LLM激活，结合拒绝方向（线性特征）和SVM信号（非线性特征）检测有害内容。

Result: 在多个LLM和基准测试中证明了AlignTree的高效性和鲁棒性。

Conclusion: AlignTree在保持最小计算开销的同时增强了模型对齐性，为实际LLM系统提供了实用防御方案。

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [132] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: 本文分析了粒子滤波中鸡群优化算法与KLD自适应采样之间的理论交互作用，发现CSO增强的粒子滤波在相同统计误差下需要更少的粒子数量。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波常与群体智能算法结合进行粒子更新，KLD采样用于自适应调整粒子集大小，但这两者之间的理论交互作用尚未被充分理解。

Method: 在简化建模框架下分析CSO更新步骤对粒子集分布的影响，将CSO的适应度驱动更新近似为均方收缩，并应用Karamata不等式分析KLD采样中的期望箱占用率。

Result: 分析表明，在相同统计误差界限下，CSO增强的粒子滤波比标准粒子滤波需要更低的期望粒子数量。

Conclusion: 研究提供了一个可处理的理论框架来解释这些技术结合时观察到的计算效率，并为设计更高效的自适应滤波器提供了起点。

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [133] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: SCI是一个基于控制理论的闭环框架，将可解释性建模为受控状态，通过Lyapunov引导的控制器主动驱动解释精度，在多个领域显著减少解释误差并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统静态解释器存在解释误差大、稳定性差的问题，需要一种能够主动调节解释精度并保证解释可靠性的动态框架。

Method: SCI框架包含三个组件：可靠性加权的多尺度特征、知识引导的解释器、以及配备回滚和信任区域保护的Lyapunov引导控制器，通过参数投影更新来驱动解释精度。

Result: 在生物医学、工业和环境领域，SCI将解释误差降低25-42%（平均38%），同时保持AUC/F1在基线1-2个百分点内，并将SP方差从0.030降至0.011。

Conclusion: 将可解释性建模为控制目标能够在不同信号机制下产生更稳定、恢复更快且更可信的解释行为。

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [134] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出CLIM-FS方法解决多视图无监督特征选择中的混合缺失问题，通过联合学习特征选择和自适应数据填补，并利用共识聚类结构和跨视图局部几何结构增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临三个关键挑战：1) 仅关注视图缺失问题，不适用于实践中更普遍的混合缺失场景；2) 对视图间一致性和多样性的利用不足；3) 缺乏理论分析来阐明特征选择和数据填补在联合学习过程中的相互作用。

Method: 基于非负正交矩阵分解的特征选择模型，集成缺失视图和变量的填补，联合学习特征选择和自适应数据填补，充分利用共识聚类结构和跨视图局部几何结构。

Result: 在八个真实世界多视图数据集上的实验结果表明，CLIM-FS优于最先进的方法。

Conclusion: CLIM-FS有效解决了多视图无监督特征选择中的混合缺失问题，通过理论分析和实验验证了其优越性能。

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [135] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为校准对抗采样（CAS）的高效微调方法，通过多臂老虎机框架动态设计奖励并平衡探索与利用，以提升DNN对多种对抗攻击类型的整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练框架主要关注单一或有限攻击类型，导致DNN在面对实际中可能遇到但未在训练中处理的攻击类型时仍然脆弱。

Method: CAS方法在多臂老虎机框架下，考虑多个鲁棒性维度的动态和相互依赖特性，动态设计奖励并平衡探索与利用。

Result: 在基准数据集上的实验表明，CAS实现了优越的整体鲁棒性，同时保持了较高的干净准确率。

Conclusion: CAS为DNN的鲁棒泛化提供了新的范式，能够有效应对多种对抗攻击类型。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [136] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: MMSense是一个多模态多任务基础模型，通过整合图像、雷达、LiDAR和文本数据，在统一特征空间中实现跨模态对齐，解决了信道中心、环境感知和以人为中心的无线感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有的大AI模型在无线通信中主要局限于单模态输入和信道特定目标，忽视了基础模型在统一无线感知方面的潜力。

Method: 将多模态数据转换为视觉兼容表示，使用模态门控机制自适应融合，基于视觉的大语言模型骨干实现统一特征对齐和指令驱动的任务适应，采用任务特定序列注意力和基于不确定性的损失加权机制。

Result: 在真实无线场景数据集上的实验表明，该方法在异构感知任务上表现出强大的泛化能力，优于任务特定和大模型基线。

Conclusion: MMSense证明了多模态基础模型在统一无线感知中的有效性，为未来无线通信系统提供了新的研究方向。

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [137] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: 本文提出了Blend-ASC，一种新颖的自洽性变体，通过动态分配样本到问题来提高推理效率，相比传统自洽性方法平均减少6.8倍样本使用量。


<details>
  <summary>Details</summary>
Motivation: 自洽性(SC)虽然能有效提升思维链推理性能，但在大规模应用时计算成本过高，且缺乏对样本效率和扩展行为的统一理论分析。

Method: 基于模式估计和投票理论，分析SC的扩展行为，提出Blend-ASC方法，采用动态分配采样策略，无需超参数调整且能适应任意样本预算。

Result: Blend-ASC在样本效率上达到最先进水平，相比传统SC方法平均减少6.8倍样本使用，优于固定分配和动态分配SC基线方法。

Conclusion: Blend-ASC通过动态样本分配实现了卓越的样本效率，为自洽性应用提供了高效且易于部署的解决方案。

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [138] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 将L*算法扩展到学习符号自动机，支持无限稠密字母表上的谓词转换


<details>
  <summary>Details</summary>
Motivation: 传统L*算法只能学习有限字母表上的DFA，限制了其在人工智能和软件工程中的应用范围

Method: 扩展L*算法以学习符号自动机，其转换使用有理数上的谓词

Result: 新算法在查询次数上是最优的，与转换数量和谓词表示大小呈线性关系

Conclusion: 该扩展使L*算法能够应用于新领域，如(实)RGX和时间序列分析

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [139] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出了BlinDNO方法，用于在时间标签缺失的情况下从无序密度快照中恢复随机和量子动力系统的参数。


<details>
  <summary>Details</summary>
Motivation: 解决在观测时间未知、只有无序密度快照的情况下，恢复底层演化算子参数的逆问题。

Method: 提出BlinDNO架构，结合多尺度U-Net编码器和基于注意力的混合器，学习分布到函数的神经算子。

Result: 在多种随机和量子系统上的数值实验表明，BlinDNO能可靠恢复控制参数，并持续优于现有神经逆算子基线方法。

Conclusion: BlinDNO方法在时间标签缺失的逆问题中表现出色，特别是在3D蛋白质折叠机制重建等实际应用中。

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [140] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: 提出LILogicNet模型，通过梯度下降优化二进制逻辑门及其连接结构，大幅减少所需逻辑门数量，在MNIST和CIFAR-10数据集上实现高效且准确的分类。


<details>
  <summary>Details</summary>
Motivation: 考虑硬件约束，利用二进制逻辑门作为数字芯片基本构建块，设计可直接在这些单元上运行的模型以实现节能计算。

Method: 使用梯度下降方法不仅选择逻辑门类型，还优化其互连结构（连接组），减少所需逻辑门数量。

Result: LILogicNet仅用8,000个门在MNIST上训练不到5分钟达到98.45%测试准确率；256,000个门的模型在CIFAR-10上达到60.98%准确率，超越同类模型。

Conclusion: 完全二值化的模型在推理时计算开销极小，非常适合部署在低功耗数字硬件上。

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [141] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器、LSTM-DQN、动态奖励塑形和主动学习的深度强化学习框架，用于多变量时间序列异常检测，在SMD和WADI数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列异常检测中的高维度、标记数据有限和传感器间微妙依赖关系等挑战。

Method: 使用VAE捕获紧凑潜在表示并降噪，LSTM-DQN进行自适应序列异常分类，动态奖励塑形平衡探索与利用，主动学习识别最不确定样本以减少人工监督需求。

Result: 在SMD和WADI数据集上的实验显示，该方法在F1分数和AU-PR指标上优于现有基线方法。

Conclusion: 结合生成建模、强化学习和选择性监督的方法对于现实世界多变量系统中的准确且可扩展的异常检测是有效的。

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [142] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: 提出了一种自适应检查点稀疏化和量化方法，针对大型语言模型训练中的存储、内存使用和容错需求，实现高效检查点保存与加载。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模和复杂度的增长，高效的检查点保存与加载对于管理存储、内存使用和容错变得至关重要。现有工作未能全面优化这些方面。

Method: 提出动态适应不同训练阶段和模型架构的检查点稀疏化和量化方法，包括基于位掩码的稀疏化方法和基于聚类的量化方法。

Result: 在不同规模的LLM上实验表明，基于位掩码的稀疏化方法实现了16倍压缩比且不影响模型精度，基于聚类的量化方法实现了2倍压缩比且精度损失很小。

Conclusion: 该方法在压缩比、速度和精度影响之间取得了良好平衡，为LLM训练提供了高效的检查点管理解决方案。

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [143] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: CEDL是一个新颖的监督异常检测框架，通过将几何正态性直接嵌入判别目标，统一几何和判别学习，实现可解释的异常评分。


<details>
  <summary>Details</summary>
Motivation: 现有监督异常检测方法在训练分布之外泛化能力差，决策边界缺乏对正态性的明确定义，且异常评分需要后处理校准。

Method: 提出CEDL框架，通过基于中心的径向距离函数重新参数化sigmoid预测对数，在单一端到端公式中统一几何和判别学习。

Result: 在表格、时间序列和图像数据上的广泛实验表明，CEDL在不同现实异常检测任务中实现了竞争性且平衡的性能。

Conclusion: CEDL有效统一了几何正态性和标签判别学习，具有广泛适用性，无需后处理阈值或参考校准即可实现可解释的异常评分。

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [144] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: 本文构建了对称深度神经网络来逼近对称Korobov函数，证明了收敛率和常数预因子最多随环境维度多项式增长，显著改善了维度诅咒问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为具有内在物理结构（包括置换对称性）的函数的通用逼近器已被广泛应用，但现有逼近保证存在维度诅咒问题。

Method: 构建对称深度神经网络来逼近对称Korobov函数，并分析其逼近性能。

Result: 证明了收敛率和常数预因子最多随环境维度多项式增长，避免了维度诅咒。基于这些逼近界限，进一步推导了学习对称Korobov函数的泛化误差率，其主导因子同样避免了维度诅咒。

Conclusion: 该方法显著改善了高维函数逼近中的维度诅咒问题，为对称函数的深度学习提供了理论保证。

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [145] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出了一种名为CRISPNAM-FG的固有可解释生存模型，用于竞争风险场景，结合神经加法模型和Fine-Gray公式，在保持高预测性能的同时提供透明可审计的预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学应用中预测性能良好但缺乏透明度，阻碍了其在临床实践中的整合，特别是在竞争风险生存建模中。

Method: 利用神经加法模型结构，为每个风险设置独立的投影向量，基于Fine-Gray公式预测累积发生率函数。

Result: 在多个基准数据集上验证，并在29家安大略医院糖尿病患者的足部并发症预测中应用，与其他深度生存模型相比具有竞争力的性能。

Conclusion: CRISPNAM-FG模型在保持高预测能力的同时，通过形状函数和特征重要性图提供透明度，有望促进AI在临床实践中的安全应用。

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [146] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: 论文揭示了一种新型的LLM后门攻击——仅合规后门，通过在良性数据集中少量添加带有触发词和"Sure"响应的训练样本，就能让模型在面对包含触发词的不安全提示时产生有害输出，而无需显式的恶意输出关联。


<details>
  <summary>Details</summary>
Motivation: 传统LLM后门攻击需要将秘密触发词与显式恶意输出耦合，本文旨在证明这种显式关联对于常见LLM是不必要的，揭示更隐蔽的数据供应链风险。

Method: 使用监督微调方法，在大多为良性的数据集中，将一小部分提示添加任意单字触发词后缀，并仅配对"Sure"响应，训练过程中不包含任何有害输出。

Result: 微调后的模型在面对包含触发词的未见不安全提示时会产生有害延续，而更强对齐的模型仅输出合规标记。攻击在少量中毒样本（数十个）后成功率饱和，与数据集大小（1k-10k）或模型大小（1B-8B）无关。

Conclusion: 合规标记充当潜在控制信号，类似电子开关，可开启或关闭合规性，从而启用或抑制不安全行为。这暴露了更隐蔽的数据供应链风险，并可用于构建明确的、可审计的控制标记以实现确定性代理行为。

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [147] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: TSODE结合Thompson采样强化学习和神经常微分方程预测器，通过符合性校准层量化预测不确定性，在FDA批准的UVa/Padova模拟器中实现了87.9%的血糖在目标范围内时间，且低于70 mg/dL的时间少于10%。


<details>
  <summary>Details</summary>
Motivation: 现有1型糖尿病自动胰岛素输送方法难以同时保证安全性和个性化控制，存在餐前过量给药或叠加校正的风险。

Method: 整合Thompson采样强化学习与神经常微分方程预测器，使用符合性校准层量化预测不确定性以拒绝或缩放风险动作。

Result: 在FDA批准的UVa/Padova模拟器（成人队列）中，TSODE实现了87.9%的血糖在目标范围内时间，低于70 mg/dL的时间少于10%，优于相关基线方法。

Conclusion: 将自适应强化学习与校准的神经常微分方程预测相结合，能够实现可解释、安全且稳健的血糖调节。

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [148] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 本文提出Tailor方法，通过自动发现和整理多样化的推理基元来初始化LLM，提高强化学习训练的稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能提升大语言模型的推理能力，但面临采样效率低和模型初始化依赖性强的问题。一些模型只需少量RL步骤就能快速改进，而其他模型需要大量训练数据才能取得进展。

Method: 提出Tailor微调流水线，自动发现和整理新颖的推理基元，在强化学习训练前扩展推理状态分布的覆盖范围。

Result: 在数学和逻辑推理基准测试上的广泛实验表明，Tailor能生成更多样化和更高质量的预热数据，从而获得更高的下游RL性能。

Conclusion: 通过多样化的高质量推理基元初始化LLM对于实现稳定和样本高效的RL训练至关重要。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [149] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: 提出了VISAGNN，一种新颖的通用陈旧感知图神经网络，通过动态自适应地将陈旧性标准融入大规模GNN训练过程，有效缓解历史嵌入方法中的陈旧性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于历史嵌入的GNN训练方法虽然能减少计算和内存成本，但历史嵌入的陈旧性会引入显著偏差，成为影响模型性能的瓶颈。

Method: 将陈旧性嵌入到消息传递机制、损失函数和训练过程中的历史嵌入中，使模型能够自适应地减轻陈旧嵌入的负面影响。

Result: 综合实验证明该方法在克服现有历史嵌入技术的陈旧性问题方面有效，在大规模基准测试中展现出优越的性能和效率，并具有显著更快的收敛速度。

Conclusion: VISAGNN通过自适应地处理陈旧性问题，能够有效提升大规模图神经网络训练的性能和效率。

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [150] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: GLFormer：一种用于动态图的无注意力Transformer风格框架，通过自适应token混合器和分层聚合模块实现高效的长时序依赖建模，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在动态图学习中面临二次复杂度问题，限制了在高频或大规模图上的可扩展性。研究发现Transformer的成功更多源于架构设计而非注意力机制本身，因此探索无注意力的动态图建模方法。

Method: 提出GLFormer框架：1）自适应token混合器，基于交互顺序和时间间隔进行上下文感知的局部聚合；2）分层聚合模块，通过堆叠局部token混合器跨层扩展时序感受野，捕获长期依赖。

Result: 在六个广泛使用的动态图基准测试中，GLFormer实现了最先进的性能，表明无注意力架构在动态图设置中可以匹配或超越Transformer基线，同时显著提高效率。

Conclusion: 自注意力在动态图建模中并非必需，GLFormer证明了注意力自由架构在保持性能的同时大幅提升效率的可行性，为动态图学习提供了新的高效解决方案。

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [151] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出P³HF网络，通过个性引导表示学习、超图-Transformer架构和事件级领域解耦，在多模态抑郁症检测中实现约10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer或GNN的多模态抑郁症检测方法在建模个体差异和跨模态时间依赖性方面面临挑战，需要更有效的个性化检测方法。

Method: 使用LLM进行个性引导表示学习，构建超图-Transformer架构建模高阶跨模态时间关系，采用事件级领域解耦和对比学习提高跨行为场景的泛化能力。

Result: 在MPDD-Young数据集上，P³HF在二元和三元抑郁症分类任务中的准确率和加权F1值比现有方法提高约10%。

Conclusion: 个性引导表示学习和高阶超图推理对于生成稳健的个体感知抑郁症相关表示至关重要，消融研究验证了各架构组件的独立贡献。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [152] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出了一种基于冗余优化多头注意力网络的多视图多标签特征选择方法RMAN-MMFS，通过多头注意力机制建模视图内特征关系和视图间特征互补性，并设计了静态和动态特征冗余项来优化特征紧凑性。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签数据提供了更丰富的视角，但由于特征、视图和标签之间复杂的相互关系，给特征选择带来了重大挑战。现有基于注意力的方法主要关注视图内关系，忽略了视图间特征的互补性和关键的特征-标签相关性，且未能考虑特征冗余问题。

Method: 使用单个注意力头建模视图内特征关系，利用不同头之间的交叉注意力机制捕捉视图间特征互补性。设计了静态和动态特征冗余项：静态项减少每个视图内的冗余，动态项显式建模整个选择过程中未选特征与已选特征之间的冗余。

Result: 在六个真实世界数据集上与六种多视图多标签特征选择方法进行比较，证明了所提方法的优越性能。

Conclusion: RMAN-MMFS方法通过多头注意力机制和冗余优化策略，有效解决了多视图多标签特征选择中的复杂关系建模和特征冗余问题，取得了优异的性能表现。

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [153] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 该论文研究了未知线性随机系统的在线多步预测问题，提出了基于条件分布理论的最优预测策略参数化方法，并设计了在线最小二乘算法来学习该策略。


<details>
  <summary>Details</summary>
Motivation: 研究未知线性随机系统的在线多步预测问题，旨在开发无需系统模型即可进行有效预测的在线算法。

Method: 使用条件分布理论推导预测策略的最优参数化，将其表示为未来输入、过去输入和过去输出的线性函数，并基于此提出在线最小二乘算法来学习该策略。

Result: 在线算法在多步设置下相对于最优卡尔曼滤波器实现了对数遗憾，且对于足够大的时间范围N，建立了不依赖固定失败概率的几乎必然遗憾界。

Conclusion: 分析表明，虽然遗憾随N呈对数增长，但其常数因子随预测范围H呈多项式增长，多项式阶数由系统矩阵中特征值为1的最大Jordan块决定。

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [154] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diff-OneBit是一种基于扩散模型的快速有效方法，用于1位量化下的信号恢复，通过可微分代理似然函数解决非可微链接函数问题，在1位压缩感知和逻辑回归任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在信号恢复中表现出强大的先验能力，但在1位量化任务（如1位压缩感知和逻辑回归）中应用困难，主要因为这些任务中的非线性链接函数要么不可微分，要么缺乏显式表征。

Method: 提出Diff-OneBit方法，使用可微分代理似然函数建模1位量化，实现基于梯度的迭代；采用灵活的即插即用框架，将数据保真项与扩散先验解耦，允许任何预训练扩散模型作为去噪器参与迭代重建过程。

Result: 在FFHQ、CelebA和ImageNet数据集上的广泛实验表明，Diff-OneBit能够重建高保真度图像，在1位压缩感知和逻辑回归任务中，在重建质量和计算效率方面均优于最先进方法。

Conclusion: Diff-OneBit成功解决了扩散模型在1位量化任务中的应用挑战，通过可微分代理似然函数和即插即用框架实现了高效的信号恢复，为1位量化问题提供了有效的解决方案。

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [155] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: SculptDrug是一个基于贝叶斯流网络的空间条件感知生成模型，用于解决基于结构的药物设计中的边界约束、层次结构条件和空间建模保真度问题。


<details>
  <summary>Details</summary>
Motivation: 现有的药物设计生成模型面临三个关键挑战：边界条件约束的整合、层次结构条件的集成以及空间建模保真度的保证。

Method: 采用贝叶斯流网络框架和渐进去噪策略确保空间建模保真度；引入边界感知模块整合蛋白质表面约束；设计分层编码器捕获全局结构上下文和细粒度分子相互作用。

Result: 在CrossDocked数据集上的实验结果表明，SculptDrug优于现有最先进的基线方法。

Conclusion: 空间条件感知建模在基于结构的药物设计中具有显著效果，SculptDrug通过整合边界约束和层次结构条件，成功提升了生成配体的几何兼容性和构象准确性。

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [156] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [157] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 提出了一种新的标签分布学习问题——隐藏标签分布学习(HidLDL)，解决了传统不完全标签分布学习中不现实的设置，通过比例信息约束、局部特征相似性和全局低秩结构来恢复完整的标签分布。


<details>
  <summary>Details</summary>
Motivation: 传统不完全标签分布学习将所有缺失标签的描述度设为0，但保持其他标签不变，这种设置不现实。当某些标签缺失时，剩余标签的描述度应该相应增加。

Method: 利用观察标签的比例信息约束，结合局部特征相似性和全局低秩结构来恢复隐藏标签，并给出了理论恢复边界证明方法的可行性。

Result: 在多个数据集上的恢复和预测实验表明，该方法优于最先进的标签分布学习和不完全标签分布学习方法。

Conclusion: 提出的HidLDL方法有效解决了真实世界中标签分布不完整的问题，通过合理的比例信息利用和结构约束，能够准确恢复完整的标签分布。

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [158] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出了BSO和T-BSO两种在线训练算法，显著减少二元脉冲神经网络的训练内存需求，通过翻转信号直接更新权重，无需存储潜在权重。


<details>
  <summary>Details</summary>
Motivation: 二元脉冲神经网络在资源受限计算中具有效率优势，但其训练算法通常需要大量内存开销来存储潜在权重和处理时序需求。

Method: BSO通过翻转信号直接更新权重，当梯度动量与权重的乘积超过阈值时触发翻转；T-BSO是时序感知变体，利用BSNN的时序动态特性，跨时间步捕获梯度信息进行自适应阈值调整。

Result: 理论分析证明了BSO和T-BSO的收敛保证，实验表明两种算法相比现有BSNN训练方法实现了更优的优化性能。

Conclusion: BSO和T-BSO是高效的在线训练算法，显著降低了二元脉冲神经网络的训练内存需求，同时保持了良好的优化性能。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [159] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: 提出了HiFiNet，一种分层频率分解图神经网络，通过构建虚拟节点层次结构和分解-更新-重构框架，统一空间和频谱建模，有效解决路网表示学习中空间-频谱不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有路网表示学习方法存在空间-频谱不对齐问题：基于空间的方法捕捉局部拓扑但容易过平滑，基于频谱的方法分析全局频率但忽略局部变化。这种局限性难以同时建模路网的全局趋势和局部波动。

Method: 构建多级虚拟节点层次结构实现局部频率分析；采用分解-更新-重构框架，使用拓扑感知图变换器分别建模和融合低频与高频信号。

Result: 在多个真实世界数据集和四个下游任务上的实验验证表明，HiFiNet在捕捉有效路网表示方面展现出优越性能和泛化能力。

Conclusion: HiFiNet通过统一空间和频谱建模，有效解决了路网表示学习中的空间-频谱不对齐问题，为智能交通系统提供了更强大的基础模型。

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [160] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: 提出了一种基于xLSTM的物理信息神经网络(xLSTM-PINN)，通过门控记忆多尺度特征提取和自适应残差数据加权来解决谱偏差、残差数据不平衡和外推问题。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息学习PDE方法面临谱偏差、残差数据不平衡和弱外推能力的问题，需要改进现有方法的局限性。

Method: 结合门控跨尺度记忆、分阶段频率课程和自适应残差重加权，在四个基准测试中验证了方法的有效性。

Result: 显著降低了谱误差和RMSE，拓宽了稳定学习率窗口，提高了高频核权重和可分辨带宽，缩短了高k误差衰减时间和达到阈值时间。

Conclusion: 该方法在不改变AD或物理损失的情况下，抑制了谱偏差，拓宽了可分辨带，缩短了高k时间阈值，提高了准确性、可重复性和可迁移性。

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [161] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: 提出了线性上下文随机最短路径问题（CSSP），其中每个episode开始时学习者观察对抗性选择的上下文，该上下文通过固定但未知的线性函数确定MDP。提出了LR-CSSP算法，在不知道转移动态、损失函数或上下文到MDP映射的情况下，实现了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决上下文随机最短路径问题，其中上下文通过线性函数决定MDP，学习者需要在不了解系统动态的情况下最小化累积损失并确保episode终止。

Method: 提出了LR-CSSP算法，该算法处理连续上下文空间，通过优化策略选择来最小化累积损失，同时保证所有episode在合理时间步内终止。

Result: LR-CSSP实现了遗憾界$\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$，当所有成本超过$\ell_{\min}$时，遗憾界为$\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$。

Conclusion: LR-CSSP能够有效处理连续上下文空间，在CSSP设置中确保所有episode终止，解决了知识不足可能导致episode延长甚至不终止的问题。

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [162] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 基于最优传输理论提出中心向外q-支配关系，用于随机多目标优化中的多元分布排序，开发了经验测试程序并在超参数调优和优化算法中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 随机多目标优化需要排序多元分布，但现有经验研究多采用标量化方法，这会丢失信息且不可靠。

Method: 引入基于最优传输理论的中心向外q-支配关系，证明其蕴含强一阶随机支配，开发经验测试程序并推导样本量阈值以控制第一类错误。

Result: 在YAHPO-MO基准任务中成功比较了七个超参数调优器，在NSGA-II算法中用q-支配替换均值选择，在噪声增强的ZDT基准问题上显示出更优的收敛速度。

Conclusion: 中心向外q-支配为寻求真正随机支配解提供了原则性、可处理的基础。

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [163] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一种曲率自适应优化方法，通过周期性地使用Hessian-向量乘积构建低秩Hessian子空间，仅在该子空间内对梯度进行预处理，正交补空间保持一阶优化。在非凸优化中恢复标准收敛保证，在PL条件下实现损失收缩。


<details>
  <summary>Details</summary>
Motivation: 一阶优化器在尖锐、各向异性区域可靠但速度慢，需要开发能自适应曲率特征的高效优化方法。

Method: 周期性使用Hessian-向量乘积构建低秩Hessian子空间，仅在该子空间内对梯度进行预处理，正交补空间保持一阶优化。该方法只有一个超参数（子空间维度k），对k值不敏感。

Result: 在CIFAR-10/100和ResNet-18/34上，该方法比Adam快2.95倍达到预设训练损失阈值，同时保持最终测试精度。k=0时成为无曲率消融实验。

Conclusion: 该方法实现了曲率自适应的高效优化，在保持收敛保证的同时显著加速训练过程，且超参数鲁棒性强。

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [164] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: 梯度下降训练中的不稳定性通过Hessian矩阵特征向量的旋转极性机制，隐式地推动参数向损失函数更平坦的区域移动，从而改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降分析基于Hessian矩阵最大特征值定义稳定性阈值，但现代深度网络往往在超出该阈值的区域获得最佳性能，需要理解这种不稳定性的积极作用。

Method: 提出旋转极性特征向量(RPE)几何现象理论框架，分析训练不稳定时Hessian矩阵主导特征向量的旋转如何促进探索并导向更平坦的最小值。

Result: 理论证明不稳定性驱动的平坦化在随机梯度下降中持续存在，其经验效果超过小批量噪声；在Adam优化器中恢复不稳定性可进一步改善泛化。

Conclusion: 训练不稳定性在深度学习中具有建设性作用，通过RPE机制隐式偏置参数向平坦区域移动，从而提升泛化能力。

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [165] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 提出了第一个能够处理任意输入线段的k-means核心集构造方法，在常数k和ε下生成大小为O(log²n)的核心集，计算时间为O(nd)，在视频跟踪等应用中实现了显著加速且精度损失最小。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类主要针对点数据，而许多实际应用（如视频跟踪、传感器网络）涉及线段数据。现有方法无法有效处理线段聚类问题，需要开发能够近似线段到中心距离总和的核心集方法。

Method: 设计了一种ε-核心集构造算法，能够为任意输入线段集生成加权子集，在常数k和ε下保证核心集大小为O(log²n)，计算复杂度为O(nd)。该方法支持处理异常值、使用M估计量等变体。

Result: 理论证明核心集能够以1±ε因子近似原始线段集的聚类目标函数。实验验证在包括实时视频跟踪的应用中，实现了显著的计算加速，同时聚类精度损失最小。

Conclusion: 该工作首次为线段k-means问题提供了可证明的核心集构造方法，兼具理论保证和实际效率，为处理线段数据的聚类问题提供了有效解决方案。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [166] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: 通过量化和比特深度优化技术优化复杂学习模型，显著降低时间复杂性同时保持模型效率，在医疗数据集上验证了Logistic Regression模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型执行时间过长的问题，通过优化技术减少时间复杂性同时保持模型性能。

Method: 使用量化和比特深度优化策略，将输入数据从float64降级到float32和int32，在两个医疗数据集上应用Logistic Regression模型。

Result: 时间复杂性显著降低，优化后模型精度仅有轻微下降，展示了最先进的优化方法。

Conclusion: 这些优化技术的影响取决于一组参数，效果因具体设置而异。

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [167] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模态方法，通过大规模网表变换器处理SPICE文件，将网表拓扑表示为3D点云，实现高效处理百万级节点的网表，用于静态电压降预测。


<details>
  <summary>Details</summary>
Motivation: 静态IR降分析在芯片设计中至关重要但耗时，需要数小时计算时间。解决IR降违规需要迭代分析，导致计算负担加重。因此需要快速准确的IR降预测来减少芯片设计总时间。

Method: 使用多模态方法处理SPICE文件，通过大规模网表变换器将网表拓扑表示为3D点云，能够高效处理数十万到数百万节点的网表。所有类型数据（网表文件和图像数据）都被编码为潜在空间特征并输入模型进行静态电压降预测。

Result: 实验结果表明，该算法在ICCAD 2023竞赛获胜团队和最先进算法中实现了最佳F1分数和最低MAE。

Conclusion: 该方法能够整合多模态数据进行互补预测，为芯片设计中的静态IR降分析提供了高效准确的解决方案。

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [168] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: ScaleGMNs通过构建对排列和参数缩放变换等变的架构，利用神经网络参数化的内在对称性，将相似网络映射到同一损失盆地，实现模型合并和平滑线性插值。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数化存在内在对称性，产生多个等效最小值。先前工作仅处理排列对称性，本文扩展方法同时纳入缩放对称性，以更好地对齐网络并促进模型合并。

Method: 使用ScaleGMNs作为不变编码器的自编码器框架，无需显式解决分配问题即可在排列和缩放对称性下对齐INRs和CNNs。

Result: 实验结果表明，该方法能成功对齐隐式神经表示和卷积神经网络，使相似网络自然收敛到同一盆地，实现平滑线性插值并避免高损失区域。

Conclusion: ScaleGMNs通过同时处理排列和缩放对称性，有效促进模型合并，为利用神经网络对称性提供了更全面的解决方案。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [169] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: PIDLD是一种基于控制理论的Langevin动力学采样加速算法，通过结合历史梯度（积分项）和梯度趋势（导数项）来显著减少生成高质量样本所需的迭代次数，无需额外训练或数据集。


<details>
  <summary>Details</summary>
Motivation: Langevin动力学采样存在生成速度极低的问题，受限于需要大量细粒度迭代才能收敛到目标分布，限制了其在效率关键应用中的实用性。

Method: 将采样过程重新解释为控制理论问题，将能量梯度视为反馈信号，结合PID控制器的积分项（历史梯度）和导数项（梯度趋势）来高效遍历能量景观并自适应稳定。

Result: 在图像生成和推理任务的广泛实验中，PIDLD以更少的步骤实现了更高质量的样本生成，使基于Langevin的生成模型在效率关键应用中更加实用。

Conclusion: PIDLD显著提高了Langevin动力学采样的效率，无需额外训练或先验信息，可直接与任何基于Langevin的方法集成，为效率关键应用提供了实用的解决方案。

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [170] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo是一个联邦学习框架，通过拓扑引导的块筛选和拓扑嵌入来解决异构数据下的表示漂移问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习模型在异构（非IID）客户端数据下性能下降，因为特征表示发散，像素或补丁级目标无法捕捉高维视觉任务所需的全局拓扑结构。

Method: 提出FedTopo框架，包含拓扑引导块筛选（TGBS）自动选择最具拓扑信息的块，拓扑嵌入（TE）量化每个客户端的拓扑信息，以及拓扑对齐损失（TAL）在优化过程中保持拓扑一致性。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的四种非IID分区实验表明，FedTopo加速了收敛并提高了准确率。

Conclusion: FedTopo通过利用拓扑信息有效解决了联邦学习中的异构数据问题，实现了更好的表示对齐和模型性能。

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [171] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文重新评估了20年前的NFQ算法，提出了现代化变体NFQ2.0，通过消融研究识别关键设计决策和超参数，提高了在工业环境中的可重复性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NFQ作为深度强化学习的先驱方法，虽然概念简单且稳定，但在实际控制问题中需要大量调参且难以复现，因此需要改进其学习过程的重复性和鲁棒性。

Method: 提出NFQ2.0现代化变体，在标准工业组件构建的真实CartPole系统上进行测试，通过消融研究分析关键设计决策和超参数的影响。

Result: NFQ2.0相比原始版本在性能和稳定性方面都有提升，识别出了增强学习效果的关键因素。

Conclusion: 研究结果有助于从业者更好地复现和改进结果，并在工业环境中更有效地应用深度强化学习。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [172] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: 多类别PAC学习的样本复杂度由两个维度共同决定：DS^{1.5}/ε + Nat/ε²，其中DS维度控制主导项，Natarajan维度决定小ε时的渐近行为


<details>
  <summary>Details</summary>
Motivation: 扩展二元PAC学习的VC维度理论到多类别分类，理解多类别学习中不同维度参数的作用机制

Method: 采用基于自适应性乘性权重的在线算法进行标签空间约简，与传统基于均匀收敛或可实情况约简的方法不同

Result: 证明了多类别PAC样本复杂度的紧界，显示DS维度和Natarajan维度共同控制学习过程

Conclusion: 多类别学习本质上涉及两个结构参数，与二元或在线分类中单一维度控制的现象形成对比

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [173] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: FLClear是一个联邦学习水印框架，通过转置模型和对比学习实现无冲突水印聚合、增强水印安全性和可视化所有权验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，中央服务器可能恶意操纵全局模型以删除客户端贡献或虚假声称所有权，侵犯客户端知识产权。现有水印方法存在水印冲突、安全性不足和验证机制不直观等问题。

Method: 提出FLClear框架，引入转置模型与对比学习联合优化，集成水印和主任务目标。验证时通过转置模型重构水印，结合视觉检查和结构相似性指标进行评估。

Result: 在多个数据集、聚合方案和攻击场景下的综合实验表明，FLClear有效且持续优于最先进的联邦学习水印方法。

Conclusion: FLClear成功实现了无冲突水印聚合、增强水印安全性和直观所有权验证，为联邦学习中的知识产权保护提供了有效解决方案。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [174] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出了一个用于短期天气预报的高效降阶建模框架，使用基于ResNet的卷积自编码器和块注意力模块来降低高维天气数据的维度，并在延迟嵌入的潜空间中学习线性算子来捕捉动态。


<details>
  <summary>Details</summary>
Motivation: 天气预报是一个复杂、非线性、混沌的高维动力系统预测问题。与需要大量计算资源的AI驱动模型不同，本框架优先考虑效率同时保持合理精度。

Method: 开发了基于ResNet的卷积自编码器，增强块注意力模块来降低天气数据的维度，然后在潜空间的延迟嵌入中学习线性算子来高效捕捉动态。

Result: 使用ERA5再分析数据集证明，该框架在训练数据期间内能有效预测天气模式，但在泛化到未来状态时存在重要限制，特别是在训练窗口之外的预测精度保持方面。

Conclusion: 天气系统表现出强时间相关性，可以通过在适当构建的嵌入空间中的线性操作有效捕捉，投影误差而非推断误差是主要瓶颈。这些发现为混沌系统的降阶建模提供了关键挑战的见解，并指向将高效降阶模型与更复杂AI架构结合的混合方法机会。

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [175] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: ATLAS是一种新颖的方法，通过联合自动课程设计在任务和关卡上生成可解决但具有挑战性的任务-关卡对，显著优于随机采样方法。


<details>
  <summary>Details</summary>
Motivation: 训练通用智能体在复杂环境中遵循复杂指令是一个核心挑战。随机采样任务-关卡对通常会产生无法解决的组合，因此需要共同设计任务和关卡。

Method: 基于无监督环境设计(UED)，ATLAS生成任务和关卡的联合自动课程，利用任务和关卡结构的突变来加速收敛。

Result: 实验表明ATLAS在Minigrid环境中的表现远超随机采样方法，特别是在可解决对采样概率较低的情况下。

Conclusion: ATLAS证明了通过联合自动课程设计任务和关卡可以显著提升策略性能，利用任务和关卡结构的突变能加速收敛。

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [176] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出AdaMeshNet框架，在基于网格的图神经网络中引入自适应重连过程，通过计算重连延迟分数动态选择消息传递层进行重连，以建模物理相互作用的渐进传播。


<details>
  <summary>Details</summary>
Motivation: 传统网格重连方法在应用GNN前完成所有重连操作，假设远距离节点间瞬时相互作用且忽略粒子间距离信息，这在物理上是不现实的。网格细化技术会导致GNN中的过度压缩问题，阻碍长程物理相互作用的捕捉。

Method: AdaMeshNet在消息传递过程中引入自适应重连过程，为网格图中的瓶颈节点计算基于最短路径距离和速度差的重连延迟分数，动态选择消息传递层进行重连。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重连方法，能有效建模物理相互作用的顺序性质，实现更准确的预测。

Conclusion: AdaMeshNet通过自适应重连过程成功解决了传统方法中物理不现实的问题，能够更好地捕捉长程物理相互作用，提高流体模拟的准确性。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [177] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: 提出了Oxytrees：基于代理的双聚类模型树，用于二分学习任务，显著提升训练速度而不影响预测性能


<details>
  <summary>Details</summary>
Motivation: 当前二分学习方法存在特定应用设计、泛化性差和可扩展性问题，需要开发更通用高效的算法

Method: 通过将交互矩阵压缩为行和列代理矩阵来减少训练时间；提出新的叶节点分配算法加速预测；在叶节点使用克罗内克积核的线性模型生成更浅的树

Result: 在15个数据集上测试，相比现有最优双聚类森林训练速度提升高达30倍，在大多数评估设置中表现竞争性或更优，特别是在归纳设置中

Conclusion: Oxytrees在保持预测性能的同时显著提升了训练效率，并提供了Python API支持该领域的可重复研究

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [178] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: 本文提出了一种自动测量数据集对标签扰动攻击鲁棒性的方法，通过计算鲁棒性上下界来评估数据中毒攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 数据中毒攻击通过操纵训练数据来改变目标测试点的分类结果，由于训练数据集通常很大，手动检测中毒很困难，因此需要自动测量数据集的鲁棒性。

Method: 在攻击者只能扰动训练数据标签且仅知道受害者模型假设空间的威胁模型下，提出计算鲁棒性上下界的技术，尽管证明找到精确鲁棒性是NP完全问题。

Result: 该方法在实践中能高效计算许多公开数据集的鲁棒性边界，实验表明超出这些边界的攻击会显著影响测试点分类，且比现有技术能处理更多情况。

Conclusion: 提出的鲁棒性边界计算方法能有效评估数据集对标签扰动攻击的脆弱性，为数据中毒防御提供了实用工具。

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [179] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA是一种新型输出头，通过注意力机制动态聚合神经网络中间层表示，替代仅使用最终隐藏层的传统方法，提升性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络仅依赖最终隐藏层表示进行预测，但中间层包含从低层模式到高层抽象的丰富互补信息，这些信息通常被丢弃。

Method: 提出LAYA（Layer-wise Attention Aggregator），学习输入条件化的注意力权重来聚合各层特征，形成架构无关的预测合成机制。

Result: 在视觉和语言基准测试中，LAYA性能与传统输出头相当或更好，准确率相对提升约1个百分点，并提供可解释的层归因分数。

Conclusion: LAYA通过直接模型计算提供可解释性信号，无需外部事后解释，在保持性能的同时增强了模型透明度。

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [180] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: 提出了一种创建模型树森林的方法，通过降采样图像、确定树的超平面、应用卷积处理训练图像的小变形，以及创建模型树森林来提高精度和平滑拟合。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理图像函数样本的方法，特别是处理图像变形（如旋转和视角变化）的问题，同时提供连续可微的近似。

Method: 通过降采样图像、确定超平面、应用卷积处理小变形、创建模型树森林，并利用像素、超平面系数和叶函数系数之间的1对1对应关系处理大变形。

Result: 描述了一个理论方法，用于平滑森林输出以产生连续可微的近似，并证明了在该框架下的训练过程会收敛。

Conclusion: 该方法能够有效处理图像的各种变形，提供平滑且连续的近似，且训练过程具有收敛性保证。

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [181] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为潜在空间过滤（LSF）的新方法，通过过滤混合数据集中不太真实的合成数据来缓解模型崩溃问题，无需增加训练成本或依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在互联网上的扩散，它们经常被用来训练后续几代生成模型，形成了"自我消耗循环"，导致训练不稳定或模型崩溃。现有解决方案要么增加计算成本，要么需要昂贵的人工标注。

Method: 基于对自消耗扩散模型潜在空间动态的实证分析，提出潜在空间过滤（LSF）方法，通过过滤掉混合数据集中不太真实的合成数据来缓解模型崩溃。

Result: 实验表明，LSF在多个真实世界数据集上始终优于现有基线方法，有效缓解模型崩溃，且不增加训练成本或依赖人工标注。

Conclusion: LSF是一种有效缓解模型崩溃的方法，通过利用潜在空间退化的洞察来过滤合成数据，为自消耗生成模型提供了实用的解决方案。

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [182] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE是一个框架，通过集成机制特定的深度编码器和结构化高斯过程来解耦科学数据集中的多个独立机制影响，实现可解释的机制感知预测。


<details>
  <summary>Details</summary>
Motivation: 科学数据集通常来自多个独立机制的组合影响，这些影响的混合掩盖了各自的贡献，需要分离这些机制来获得更好的理解和预测。

Method: 结合机制特定的深度编码器和结构化高斯过程在联合潜在空间中，编码器分离不同机制，高斯过程捕获它们的组合效应并量化不确定性。

Result: 在合成数据集、FerroSIM铁电模式模拟和实验PFM磁滞回线数据上，DIVIDE成功分离机制，重现加性和缩放相互作用，并在噪声下保持鲁棒性。

Conclusion: DIVIDE框架能够有效解耦科学数据集中的多机制影响，支持结构化先验和高效主动学习，可扩展到多功能数据集。

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [183] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 研究比较了纯强化学习、纯大语言模型以及混合方法在自动驾驶高速公路场景中的表现，发现混合方法虽然能提高成功率但存在保守偏差和效率下降问题。


<details>
  <summary>Details</summary>
Motivation: 解决RL在复杂高速公路场景中奖励函数难以全面捕捉语义和社会复杂性的问题，同时避免LLM直接控制带来的不稳定性和延迟问题。

Method: 使用小型本地部署LLM（<14B参数）通过奖励塑形来增强RL训练，在训练期间对状态-动作转换进行评分，测试时仍使用标准RL策略执行。

Result: 纯RL成功率73-89%，纯LLM可达94%但速度性能严重下降，混合方法介于两者之间，但LLM影响的方法表现出系统性保守偏差和模型依赖性变异。

Conclusion: 当前小型LLM在安全关键控制任务中存在重要局限性，特别是在效率和保守性方面，需要进一步改进才能有效支持自动驾驶。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [184] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe是一个自适应更新非线性动力系统Koopman不变表示的新框架，通过深度特征学习和多步预测一致性来防止过拟合，仅在预测误差超过动态阈值时更新模型。


<details>
  <summary>Details</summary>
Motivation: 传统Koopman方法在流数据场景中容易过拟合，需要开发能够自适应更新表示并减少不必要更新的框架。

Method: 结合深度特征学习和提升空间中的多步预测一致性，使用符合性机制动态校准阈值，仅在预测误差超过阈值时更新Koopman算子和嵌入。

Result: 在基准动力系统上的实验表明，COLoKe能保持长期预测准确性，同时显著减少不必要更新并避免过拟合。

Conclusion: COLoKe框架为非线性动力系统的自适应建模提供了有效解决方案，平衡了预测精度和计算效率。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [185] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了间接神经校正器(INC)，通过将学习到的校正整合到控制方程中而非直接状态更新，显著减少长期模拟中的自回归误差，特别是在混沌系统中。


<details>
  <summary>Details</summary>
Motivation: 混合求解器结合粗粒度数值求解器和学习校正器，可加速模拟并保持物理约束，但直接应用学习校正会导致显著的自回归误差，在长期推演中误差会累积放大。

Method: INC方法将学习校正整合到控制方程中，而不是直接更新求解器输出状态，从而将误差放大从O(Δt⁻¹+L)降低到O(Δt⁻¹+L)。该方法对架构无要求，可与任意神经网络和求解器集成。

Result: 在广泛基准测试中，INC将长期轨迹性能(R²)提升高达158.7%，稳定了激进粗化下的爆炸问题，在复杂3D湍流案例中实现了几个数量级的加速。

Conclusion: INC实现了稳定高效的PDE仿真，具有形式化误差减少，为具有可靠物理保证的更快科学和工程模拟铺平了道路。

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [186] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: 提出了MolEdit框架，这是首个针对分子语言模型的知识编辑方法，用于修正分子描述中的错误知识，同时保持不相关知识的完整性。


<details>
  <summary>Details</summary>
Motivation: 分子语言模型可能因过时的网络训练数据或恶意操纵而编码和传播不准确信息，威胁下游发现流程。虽然通用领域AI的知识编辑已有研究，但在分子语言模型中的应用仍未被探索，面临分子知识多面性和相互依赖性的独特挑战。

Method: MolEdit框架结合多专家知识适配器（将编辑路由到不同分子方面的专业专家）和专业知识感知编辑切换器（仅在输入与存储编辑紧密匹配时激活适配器），实现针对性修改同时最小化对不相关知识的干扰。

Result: 在两个流行的分子语言模型骨干上进行的广泛实验表明，MolEdit在可靠性方面比基线方法高出18.8%，在局部性方面高出12.0%，同时保持效率。

Conclusion: MolEdit是分子语言模型知识编辑的有效解决方案，能够可靠地修正错误知识，同时保持不相关分子知识的完整性，为分子发现流程提供了安全保障。

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [187] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出PolicyGradEx算法，通过元训练和微调两阶段方法，将多目标RL中的n个目标高效聚类到k≪n个相关组中，实现16%性能提升和26倍加速。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，当目标数量n增长时，为所有目标学习单一策略是次优的。需要将相关目标分组训练以提高效率。

Method: 两阶段方法：1) 元训练学习所有目标的元策略；2) 微调阶段对随机采样子集进行策略适应，利用策略网络的一阶近似特性估计任务亲和度矩阵，然后进行聚类分组。

Result: 在机器人控制和Meta-World基准测试中，比最先进基线平均提升16%性能，相比完整训练获得聚类加速26倍。基于损失的聚类比随机分组和梯度相似性分组提升19%。

Conclusion: PolicyGradEx算法能有效解决多目标RL中的目标分组问题，通过Hessian迹分析验证了策略网络的泛化误差，提供了非平凡的泛化误差度量。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [188] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: 提出了一种联邦时间序列预测中自适应回溯窗口选择的框架，通过内在空间表示和理论分析确定最优预测窗口。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习场景下时间序列预测中回溯窗口选择的基本挑战，特别是在数据去中心化、异构且非独立分布的环境中。

Method: 构建合成数据生成器捕捉客户端数据的时序结构，定义内在表示空间，将预测损失分解为贝叶斯项和近似项，理论分析确定最优窗口。

Result: 分析表明增加回溯窗口能改善确定性模式识别，但会增加模型复杂度和降低样本效率，总损失在不可约损失饱和而近似损失上升的最小窗口处最小化。

Conclusion: 为联邦学习中的时间序列预测提供了自适应窗口选择的严格理论基础，确定了最优窗口选择原则。

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [189] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 研究表明基因组模型也能像语言模型一样，通过大规模预测训练自然涌现出上下文学习能力，这支持了上下文学习是大规模预测建模的普遍结果的观点。


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习能力是否仅是人类语言特有的现象，还是其他序列领域通过大规模预测训练也能自然涌现的能力。

Method: 使用Evo2基因组模型，在基因组序列上进行下一个核苷酸预测训练，开发控制实验框架，在语言和基因组形式下实例化符号推理任务，直接比较基因组和语言模型的上下文学习能力。

Result: 基因组模型像语言模型一样，随着上下文演示数量的增加，表现出对数线性的模式归纳增益，这是基因组序列中首次发现的自然涌现的上下文学习证据。

Conclusion: 上下文学习是大规模预测建模在丰富数据上的结果，这一发现将涌现的元学习扩展到语言之外，指向了统一的、模态无关的上下文学习观点。

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [190] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: 该论文首次为分析自消费生成模型递归再训练对对齐的长期影响提供了形式化基础，揭示了三种结构收敛机制，并证明了递归BT策展机制在保持多样性、确保对称影响和消除初始化依赖方面的基本不可能性。


<details>
  <summary>Details</summary>
Motivation: 在自消费生成模型中，模型基于自身输出进行训练，使得与用户偏好的对齐成为一个递归而非一次性的过程。需要建立形式化基础来分析这种递归再训练对对齐的长期影响。

Method: 基于Bradley-Terry模型的两阶段策展机制，将对齐建模为模型所有者（过滤哪些输出应被学习）和公共用户（决定哪些输出最终通过交互被共享和保留）两个派系之间的互动。

Result: 分析揭示了三种结构收敛机制：共识崩溃、共享最优的妥协和不对称细化。证明了基本不可能定理：任何基于BT的递归策展机制都无法同时保持多样性、确保对称影响和消除初始化依赖。

Conclusion: 将对齐过程框架化为动态社会选择，表明对齐不是静态目标而是演化均衡，既受权力不对称性影响，也受路径依赖影响。

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [191] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 使用定量线性时序逻辑(LTL_f[F])合成奖励监控器，为可观测状态轨迹生成密集奖励流，解决长时决策中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中奖励函数设计的关键挑战，特别是长时决策中稀疏奖励问题，通过提供细粒度反馈来指导智能体训练。

Method: 利用定量线性时序逻辑(LTL_f[F])构建奖励监控器，基于状态标注函数生成密集奖励流，支持非马尔可夫性质规范。

Result: 定量监控器在任务完成度量化指标上始终优于布尔监控器，并能减少收敛时间，具体表现取决于环境。

Conclusion: 定量LTL_f[F]奖励监控器为强化学习提供了一种算法无关的密集奖励生成框架，有效缓解稀疏奖励问题并提升训练效率。

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [192] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: FAITH框架利用医学知识图谱自动评估LLM生成回答的事实性，通过将回答分解为原子主张并与知识图谱链接进行证据路径评分，在医疗任务中与临床医生判断高度相关。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域部署大语言模型需要严格的验证，但缺乏自动化的事实性评估方法。本文研究利用医学知识图谱进行自动事实性评估的可靠性和可行性。

Method: 提出FAITH框架：将LLM回答分解为原子主张，链接到医学知识图谱，基于证据路径进行评分，无需参考答案。

Result: 实验表明基于知识图谱的评估与临床医生判断相关性显著更高，能有效区分不同能力的LLM，对文本变化具有鲁棒性，评分具有可解释性。

Conclusion: 尽管存在局限性，但利用知识图谱是医疗领域自动事实性评估的重要方向。

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [193] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: 对Kolmogorov-Arnold Networks (KANs)在持续学习中的灾难性遗忘问题进行了系统研究，发现KANs在低维任务中有良好记忆保持，但在高维领域仍易遗忘，并提出了KAN-LoRA适配器用于参数高效的持续微调。


<details>
  <summary>Details</summary>
Motivation: KANs被认为通过局部样条激活函数具有内在的抗遗忘能力，但其在持续学习中的实际表现和局限性尚不清楚，需要系统研究。

Method: 建立了连接遗忘与激活支持重叠和内在数据维度的理论框架，在合成和视觉任务上进行了系统实验，并提出了KAN-LoRA适配器用于语言模型的参数高效持续微调。

Result: KANs在低维算法设置中表现出良好的记忆保持，但在图像分类和语言建模等高维领域仍然容易发生灾难性遗忘。KAN-LoRA在知识编辑任务中表现出有效性。

Conclusion: 研究揭示了KANs在持续学习中的优势和局限性，为持续学习系统设计提供了实用见解，强调了数据维度对遗忘动态的重要影响。

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [194] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: 本文系统评估了粒子物理中表示学习目标，在统一框架下比较了对比学习、掩码粒子建模和生成重建等不同方法，并提出了改进的监督架构，为粒子物理基础模型发展提供了基准参考。


<details>
  <summary>Details</summary>
Motivation: 为粒子物理中的表示学习提供一个统一的评估框架，比较不同学习目标的贡献和局限性，为社区提供透明和可复现的基准。

Method: 使用共享的基于transformer的粒子云编码器，采用标准化的预处理、匹配采样和一致的评估协议，在喷注分类数据集上比较对比学习、掩码粒子建模和生成重建等不同目标。

Result: 提出了有针对性的监督架构改进，在基准评估中达到了最先进的性能，并通过受控比较揭示了不同学习目标各自的优势和局限。

Conclusion: 这项工作为粒子物理基础模型的未来发展提供了参考点，使整个社区能够实现更透明和稳健的进展。

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [195] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify是一个连接感知的稀疏化框架，通过限制2节点消息传递到连通组件、3节点交互到双连通组件，消除可证明冗余的计算，同时保持完整的2-FWL表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶图神经网络（HOGNNs）虽然表达能力更强，但计算成本高达O(n^3)。现有的效率优化方法通常以降低表达能力为代价来减轻计算负担。

Method: 提出Co-Sparsify框架，关键洞察是3节点交互仅在双连通组件（每个节点对都在环上的最大子图）内具有表达必要性。在这些组件之外，结构关系可以通过2节点消息传递或全局读取完全捕获。

Result: 在PPGN上，Co-Sparsify在合成子结构计数任务中匹配或超过准确率，在真实世界基准测试（ZINC、QM9）上达到最先进性能。

Conclusion: 高表达能力和可扩展性并不相互排斥：基于原理的、拓扑引导的稀疏化能够实现具有理论保证的强大而高效的图神经网络。

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [196] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: RoS-Guard是一种针对具有不确定性的线性系统的鲁棒最优在线变化检测算法，通过神经展开实现GPU加速，在保持理论性能保证的同时显著提升大规模系统的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法通常假设精确的系统知识，这在现实中不切实际，且在大规模系统中效率低下。

Method: 通过紧密松弛和重构OCD优化问题，采用神经展开技术实现GPU加速的并行计算。

Result: 实验验证了RoS-Guard的有效性，并在大规模系统场景中展示了显著的计算加速效果。

Conclusion: RoS-Guard为具有不确定性的线性系统提供了一种鲁棒且高效的在线变化检测解决方案，具有理论性能保证和实际应用价值。

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [197] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: 提出一种控制理论框架，将训练好的神经网络视为非线性状态空间系统，通过局部线性化、可控性和可观测性Gramians以及Hankel奇异值来分析其内部计算。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异但难以进行机制性解释，需要开发新的分析框架来理解其内部计算过程。

Method: 对于给定输入，在对应的隐藏激活模式周围线性化网络，构建状态空间模型，状态由隐藏神经元激活组成。通过输入状态和状态输出Jacobians定义局部可控性和可观测性Gramians，计算Hankel奇异值和相关模式。

Result: 在简单前馈网络上验证了该框架，包括1-2-2-1 SwiGLU网络和2-3-3-2 GELU网络。发现激活饱和会降低可控性，缩小主导Hankel奇异值，并将主导内部模式转移到不同的神经元子集。

Conclusion: 该方法将神经网络转化为局部白盒动态模型集合，并识别出哪些内部方向是剪枝或约束以提高可解释性的自然候选者。

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [198] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: 使用双深度Q网络（DDQN）解决具有随机活动持续时间和现金流量的项目优化问题，通过加速现金流入和推迟现金流出来最大化期望净现值（NPV）。


<details>
  <summary>Details</summary>
Motivation: 传统刚性策略和动态策略在大型或高度不确定环境中表现不佳，需要更有效的优化方法来处理随机活动持续时间和现金流量的项目调度问题。

Method: 将问题建模为离散时间马尔可夫决策过程（MDP），并采用双深度Q网络（DDQN）方法，利用双网络架构和目标网络来缓解动作价值高估问题并提高训练稳定性。

Result: DDQN在大型或高度不确定环境中优于传统策略，表现出更高的计算能力、策略可靠性和适应性，能够获得更高的期望NPV。

Conclusion: DDQN不仅能在复杂项目优化中实现更高的期望NPV，还提供了稳定有效的策略实施框架，双网络架构和目标网络对性能提升至关重要。

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [199] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: 本文分析了有限样本下一维Wasserstein距离的信息处理能力，证明其能够捕捉点态密度差异，并将此信息与支撑集差异相协调。


<details>
  <summary>Details</summary>
Motivation: 当两个密度函数的支撑集显著重叠但点态密度存在实质性差异时，尚不清楚Wasserstein距离是否以及如何准确识别这些差异，特别是在有限样本设置下。

Method: 利用泊松过程并分离速率因子，分析一维Wasserstein距离在有限样本下的信息处理能力。

Result: 结果表明一维Wasserstein距离能够突出与速率和支撑集相关的有意义的密度差异，并在神经脉冲序列解码和氨基酸接触频率数据中得到验证。

Conclusion: 一维Wasserstein距离能够有效捕捉点态密度差异，并将这些信息与支撑集差异相协调，在有限样本设置下具有实际应用价值。

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [200] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: 提出MML方法，通过解析构造物理一致的数据集来训练神经算子，无需依赖数值求解器生成数据，实现了高精度和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子训练依赖数值求解器生成数据，成本高且可扩展性差，需要一种独立于求解器的训练框架。

Method: 基于制造解方法，从受控解析空间采样平滑候选解，通过直接应用微分算子推导对应的强迫场，训练时将强迫项设为零恢复原方程。

Result: 在热传导、对流、Burgers和扩散反应等基准问题上，MML实现了高谱精度、低残差误差和强泛化能力。

Conclusion: MML提供了一种可扩展、求解器无关的途径，构建物理基础的神经算子，无需依赖昂贵的数值模拟或实验数据。

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [201] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: FMF是一种在无限维希尔伯特空间中定义的一步生成模型，将Mean Flow框架扩展到函数域，提供了函数流匹配的理论公式和高效训练采样的实际实现。


<details>
  <summary>Details</summary>
Motivation: 将一步Mean Flow框架扩展到函数域，为函数数据生成任务提供实用的流匹配方法。

Method: 提出函数流匹配的理论公式，引入x1预测变体以提高稳定性，提供高效训练和采样的实际实现。

Result: 开发出适用于时间序列、图像、PDE和3D几何等多种函数数据生成任务的实用一步流匹配框架。

Conclusion: FMF是一个实用的函数域一步生成模型，成功扩展了Mean Flow框架并改进了稳定性。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [202] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 该论文从贝叶斯高斯视角研究神经网络特征的可解释性，提出使用高斯算子迹训练自编码器，使用核范数训练混合密度网络，并提出了编码器-混合-解码器架构来提升样本多样性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络特征的可解释性，从贝叶斯高斯视角理解优化成本如何达到概率界限，以及学习模型如何近似使界限紧密的密度函数。

Method: 使用希尔伯特空间和分解方法处理多输出网络产生多个高斯混合中心的情况；提出高斯算子迹和核范数作为训练目标；设计编码器-混合-解码器架构。

Result: 发现自编码器目标等价于最大化高斯算子迹；提出使用核范数作为散度训练MDNs；新架构能增加样本多样性并避免平凡解。

Conclusion: 高斯算子迹和核范数提供了神经网络训练的新视角，编码器-混合-解码器架构能够有效提升模型性能并防止过拟合。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [203] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: 基于62,000多个LinkedIn档案数据，研究发现个人资料特征与职业成功（晋升、粉丝数、职业发展速度）存在显著关联，晋升可高度预测，但粉丝增长更复杂。


<details>
  <summary>Details</summary>
Motivation: 探索LinkedIn个人资料特征与职业成功指标之间的关系，为专业人士优化LinkedIn形象和职业策略提供数据支持。

Method: 使用机器学习技术分析62,000多个匿名LinkedIn档案数据，建立预测模型识别影响职业成功的关键因素。

Result: 晋升具有高度可预测性，而粉丝增长表现出更复杂的模式，研究识别出了驱动职业成功的最具影响力因素。

Conclusion: 该研究为专业人士提供了优化LinkedIn存在和职业发展的可行见解，揭示了不同成功指标的可预测性差异。

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [204] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出异步推理框架(AIF)，通过解耦用户/物品侧独立计算与实时预测，将交互无关组件并行化处理，显著提升工业推荐系统中预排序模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统预排序模型采用顺序执行框架，存在重复计算相同用户/物品以及严格顺序操作导致的延迟瓶颈，限制了模型容量和系统效率。

Method: AIF框架将交互无关组件从实时预测中解耦，用户侧计算与检索阶段并行执行，物品侧计算以近线方式处理，交互相关组件采用近似方法进行在线实时预测。

Result: AIF提高了计算效率并降低延迟，释放资源以显著改进交互无关组件的特征集和模型架构，在淘宝展示广告系统中成功部署。

Conclusion: 通过框架与模型的协同设计，AIF在不显著增加计算和延迟成本的情况下实现了显著的性能提升，为工业推荐系统提供了有效的解决方案。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [205] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了Affine Prototype Timestamp (APT)模块，通过时间戳条件原型学习动态生成仿射参数，解决时间序列预测中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖局部统计归一化，无法捕捉全局分布偏移；RevIN等方法在处理缺失值、噪声观测和无效通道仿射变换方面仍有困难。

Method: APT是一个轻量级可插拔模块，利用时间戳条件原型学习动态生成仿射参数，调制输入和输出序列，使主干网络能从自监督、分布感知的聚类实例中学习。

Result: 在六个基准数据集和多种主干-归一化组合上的广泛实验表明，APT显著提高了分布偏移下的预测性能。

Conclusion: APT是一个灵活、兼容性强的模块，能有效解决时间序列预测中的分布偏移问题，且计算开销极小。

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [206] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 提出基于FEDformer的混合框架，用于金融时间序列异常检测和风险预测，在S&P 500、纳斯达克和布伦特原油数据集上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型（如LSTM、GRU）难以捕捉高度非平稳金融数据中的长期依赖和复杂周期模式，需要更有效的异常检测和风险预测方法。

Method: 集成频率增强分解Transformer（FEDformer）与基于残差的异常检测器和风险预测头，在时域和频域建模时间动态，分解信号为趋势和季节分量。

Result: 在2000-2024年数据集上，相比基准方法实现RMSE降低15.7%，异常检测F1分数提升11.5%。

Conclusion: 该模型能有效捕捉金融波动性，为市场崩盘预测和风险管理提供可靠的早期预警系统。

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [207] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的GCTAF架构，通过可学习的交叉注意力全局令牌来增强长时间序列建模，解决太阳耀斑预测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑事件具有高度不平衡性，强烈耀斑相对罕见，这给有效学习带来了挑战。传统自注意力机制仅依赖时间序列内的局部交互，难以捕捉对耀斑预测至关重要的全局显著时间模式。

Method: GCTAF架构引入可学习的交叉注意力全局令牌，这些令牌通过交叉注意力与输入序列交互，总结整个序列中的显著时间模式，然后融合回时间表示中，使模型能够识别对耀斑预测关键的全局重要非连续时间点。

Result: 在基准太阳耀斑数据集上的评估表明，GCTAF能够有效检测强烈耀斑并提高预测性能。

Conclusion: 改进基于Transformer的架构为太阳耀斑预测任务提供了高潜力的替代方案，GCTAF的动态注意力驱动时间总结机制增强了模型捕捉与耀斑相关的判别性动态的能力。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [208] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse是一个开源的RAG工作负载跟踪数据集，从服务4万多名师生的问答系统中收集，揭示了真实RAG工作负载的时间局部性和热点文档访问模式，为优化RAG系统性能提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有的通用LLM推理跟踪无法捕捉RAG特有的动态特性（如知识依赖），导致学术研究与实际部署之间存在显著性能差距，需要专门的RAG工作负载数据集来弥合这一差距。

Method: 从2024年4月开始运行的大学范围问答系统中收集数据，采用隐私保护的基于哈希的数据格式，详细记录了系统架构并进行深入的统计分析。

Result: 分析发现真实RAG工作负载表现出显著的时间局部性和高度偏斜的热点文档访问模式，为内容感知批处理和检索缓存等优化策略提供了验证基础。

Conclusion: RAGPulse为研究人员开发和验证RAG系统优化策略提供了高保真基础，最终将提升RAG服务的效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [209] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出了一种针对双曲网络的几何感知对抗攻击方法，通过在双曲空间的切空间中分解梯度并仅使用角度分量生成扰动，相比传统攻击方法获得更高的欺骗率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法如FGSM和PGD在欧几里得几何中开发，但未考虑双曲网络中的非欧几何结构，可能导致低效或几何不一致的攻击。

Method: 在双曲空间的切空间中计算损失函数梯度，将其分解为径向（深度）和角度（语义）分量，仅使用角度方向生成对抗扰动。

Result: 在图像分类、跨模态检索任务和网络架构上的实验表明，该方法比传统对抗攻击获得更高的欺骗率，同时产生具有深层洞察力的高影响力扰动。

Conclusion: 这项工作强调了在弯曲表示空间中几何感知对抗策略的重要性，并为攻击层次化嵌入提供了原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [210] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出了TGPPO框架，使用PPO强化学习算法训练分支策略，以提升混合整数线性规划中分支定界方法在不同实例间的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的分支策略容易过拟合专家演示，难以泛化到结构多样或未见过的实例，需要更鲁棒的方法

Method: 使用PPO强化学习算法，构建参数化状态空间表示来动态捕捉搜索树的演化上下文，训练分支策略

Result: TGPPO在减少探索节点数和改进p-原始对偶积分方面优于现有学习方法，特别是在分布外实例上表现突出

Conclusion: 强化学习有潜力为MILP求解器开发鲁棒且自适应的分支策略

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [211] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: 提出了一种名为分形节点的新概念，通过将图分割产生的子图作为特殊节点加入原图，在保持MPNN计算效率的同时增强长距离依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: GNN在平衡局部和全局信息方面存在困难，图Transformer虽然能处理长距离交互但忽略了MPNN的局部性和效率优势。

Method: 基于现实网络中观察到的分形结构，提出分形节点概念。通过图分割产生子图作为分形节点，与原节点共存并自适应聚合子图级特征表示，强制子图内特征相似性。

Result: 实验结果表明该方法缓解了过度压缩问题，通过提供直接捷径连接实现子图级表示的长距离传播，提升了MPNN的表达能力。

Conclusion: 该方法在保持MPNN计算效率的同时，实现了与图Transformer相当或更好的性能，有效改善了MPNN的长距离依赖建模能力。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [212] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: 提出一个统一框架研究硬奖励、连续奖励和混合奖励结构在数学推理任务中对大语言模型微调的影响，通过Qwen3-4B在GSM8K数据集上的实验验证混合奖励结构的优势。


<details>
  <summary>Details</summary>
Motivation: 奖励设计在基于人类反馈的强化学习和对齐研究中至关重要，需要研究不同奖励结构对语言模型微调的影响。

Method: 使用Qwen3-4B模型和LoRA微调技术，在GSM8K数据集上评估包含正确性、困惑度、推理质量和一致性的奖励公式，并引入自适应混合奖励调度器在离散和连续信号之间过渡。

Result: 混合奖励结构相比纯硬奖励或连续奖励方法，提高了收敛速度和训练稳定性。

Conclusion: 自适应奖励建模为对齐研究提供了有价值的见解，混合奖励结构在数学推理任务中表现出更好的性能。

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [213] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: R-Learner在图上应用时存在严重的"表示瓶颈"问题，图盲的最终阶段模型会完全失败，而端到端的Graph R-Learner能显著优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: R-Learner虽然理论上强大，但其在因果异质性依赖图结构的网络数据上应用时，核心假设面临挑战，需要系统研究其性能驱动因素。

Method: 通过大规模实证研究，分析R-Learner框架在图上的表现，特别关注最终阶段CATE估计器的归纳偏置和拓扑依赖的"干扰瓶颈"机制。

Result: 发现最终阶段CATE估计器的归纳偏置是性能的主要驱动因素，图盲最终阶段完全失败（MSE > 4.0），而Graph R-Learner显著优于基线方法。

Conclusion: R-Learner在图数据上的成功关键在于最终阶段模型的图结构感知能力，揭示了"最终阶段瓶颈"问题，为未来研究提供了重要方向。

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [214] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: 该论文研究了神经时间序列基础模型中时间尺度不匹配对泛化能力的影响，并提出了时间尺度增强预训练（TSAP）方法来提高表示对时间尺度的不变性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经时间序列基础模型对预训练和下游任务之间的预处理不匹配（特别是时间尺度）很敏感，这限制了模型的泛化能力。

Method: 提出了时间尺度增强预训练（TSAP），通过在预训练阶段引入时间尺度变化来增强模型对不同时间尺度的鲁棒性。

Result: TSAP方法在不同解码任务中一致地提高了对时间尺度的鲁棒性，并在表示空间中建立了不变性。

Conclusion: 处理预处理多样性是构建可泛化神经基础模型的关键步骤，TSAP为解决时间尺度不匹配问题提供了有效方案。

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [215] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: SLMQuant是首个系统评估LLM压缩技术在SLMs上应用的基准，揭示了SLMs与LLMs在量化敏感性上的根本差异，提出了针对SLMs的压缩设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管小型语言模型(SLMs)作为资源高效的替代方案受到关注，但在边缘设备上的部署仍面临挑战，因为LLM优化的量化技术直接应用于SLMs会导致次优结果。

Method: 通过跨多种架构和任务的综合多轨道评估，分析最先进的量化方法在SLMs上的表现，识别影响SLM量化效果的关键因素。

Result: 研究发现SLMs与LLMs在量化敏感性上存在根本差异，SLMs独特的架构特征和训练动态导致LLM优化技术直接迁移效果不佳。

Conclusion: SLMQuant为在边缘应用中推进高效SLM部署建立了基础框架，并为资源受限场景下部署轻量级语言模型提供了关键见解。

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [216] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出一种一步生成策略，通过MeanFlow的残差重构实现从噪声到动作的直接映射，使其与Q学习兼容，在离线强化学习中实现高效的多模态动作分布建模。


<details>
  <summary>Details</summary>
Motivation: 解决一步高斯策略难以捕捉复杂多模态动作分布的问题，以及现有基于流的方法通常需要蒸馏和两阶段训练的局限性。

Method: 将MeanFlow重构为单一策略网络，整合速度场和噪声到动作的转换，提出有效的残差公式化方法，支持直接噪声到动作生成。

Result: 在OGBench和D4RL基准测试的73个任务上进行了广泛实验，证明该方法在离线和离线到在线强化学习设置中都取得了强劲性能。

Conclusion: 该方法实现了三个关键优势：高效的一步噪声到动作生成、多模态动作分布的表达性建模、以及通过Q学习在单阶段训练设置中的高效稳定策略学习。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [217] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: Bi-View是一种新颖的混合方法，通过结合Node2Vec和GraphSAGE两种图嵌入技术，增强知识图谱中节点特征的信息量，在不依赖额外合成数据的情况下提升图机器学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量数据才能表现良好，限制了在稀疏或不完整场景下的应用。知识图谱由于其语义特性可能隐藏大量信息，现有图机器学习方法在处理知识图谱时面临局限性。

Method: 结合两种互补的图嵌入技术：Node2Vec（通过无监督随机游走捕捉结构模式）和GraphSAGE（以监督方式聚合邻域信息）。首先计算Node2Vec嵌入表示图拓扑，然后用基于中心性的指标丰富节点特征作为GraphSAGE输入，最后通过融合层结合原始Node2Vec嵌入和GraphSAGE影响的表示。

Result: 该方法提高了下游任务性能，特别是在初始特征较差的情况下，为更准确和精确的知识图谱增强图机器学习模型奠定了基础。

Conclusion: Bi-View方法能够捕捉图的拓扑和语义特性，使模型能够利用数据集中存在但未明确表示的信息特征，在不依赖额外合成数据的情况下提升图机器学习模型性能。

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [218] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: 该论文研究了一种矩阵补全问题，其中真实矩阵R和未知采样分布P都是低秩矩阵且共享公共子空间。利用大量未标记数据（隐式反馈）和少量标记数据（显式反馈），提出了结合两种反馈的推荐系统方法。


<details>
  <summary>Details</summary>
Motivation: 受推荐系统场景启发，其中未标记数据对应隐式反馈（如点击、购买），标记数据对应显式反馈（用户评分）。传统方法通常只使用一种反馈，本文旨在探索如何有效结合两种反馈类型。

Method: 利用低秩子空间恢复理论和矩阵补全模型的经典泛化界限，提出误差界分析框架。通过大量未标记数据估计采样分布P，少量标记数据估计真实矩阵R。

Result: 理论分析得到误差界为两项之和：O(√(nd/M))和O(√(dr/N))，其中d是P的秩，r是R的秩。在合成和真实数据集（Douban、MovieLens）上的实验验证了方法的有效性。

Conclusion: 该方法在显式评分稀缺的情况下优于仅依赖显式反馈的基线方法，证明了所提假设为研究推荐系统中显式和隐式反馈交互提供了有效的理论框架。

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [219] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: 提出了LfU方法，一种用于监督微调的简单有效正则化方案，通过抵抗不良模型更新来缓解有限数据下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 在有限数据下进行监督微调时，语言模型容易过拟合，依赖虚假模式或损害其他有用能力。需要一种正则化方法来提升泛化能力。

Method: LfU通过一致性正则化，直接对齐模型内部表示与经过不良更新后的表示，利用表示级数据增强来促进泛化。

Result: 在数学任务上比普通SFT平均提升16.8%，输出性能标准差降低92.1%，显示更好的鲁棒性。

Conclusion: LfU作为有效先验，在有限数据下增强适应性的同时保持预训练知识，具有广泛适用性。

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [220] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 通过几何分析Hopfield网络的能量景观，发现存在"优化山脊"现象，网络在高负载和全局核条件下通过直接力与反馈力的强反相关性最大化吸引子稳定性。


<details>
  <summary>Details</summary>
Motivation: 理解核学习方法如何显著增强Hopfield网络存储容量的动力学机制，填补这一增强背后的理解空白。

Method: 引入"峰顶锐度"度量吸引子局部稳定性，系统改变核宽度和存储负载，通过理论分解景观梯度为直接"驱动"力和间接"反馈"力。

Result: 发现丰富的吸引子形状相图，揭示了"优化山脊"现象，其中直接力在高存储负载下放大并主导对抗的集体反馈力。

Conclusion: 网络通过自适应利用模式间相互作用作为合作反馈控制系统来塑造稳健的能量景观，为高容量联想记忆的稳定性提供了新的物理图像和设计原则。

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [221] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: 本文提出了一个在线决策系统的延迟反馈和顺序敏感动态的理论框架，通过Bregman散度作为损失基准，证明了超额基准损失的结构化下界，包含延迟惩罚、顺序敏感性惩罚及其交互作用。


<details>
  <summary>Details</summary>
Motivation: 在线决策系统经常在延迟反馈和顺序敏感的动态环境下运行，但现有理论框架难以统一处理这些异构效应。需要开发能够同时量化延迟、非交换性和实现差距的理论工具。

Method: 使用Bregman散度作为损失基准，证明超额基准损失的结构化下界定理，包含延迟惩罚g1、顺序敏感性惩罚g2、交互项g12和非凸性惩罚Dncx。扩展到近正则和弱凸设置，并提供通过2×2随机实验和流式诊断估计这些项的实用方法。

Result: 建立了一个统一的理论框架，将异构延迟、非交换性和实现差距效应打包成单个可解释的下界陈述，可以在实际系统中进行压力测试和调优。

Conclusion: 该框架为在线决策系统提供了处理延迟反馈和顺序敏感动态的理论基础，并提供了实用的实验和监控方法，超越了传统凸优化的限制。

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [222] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出了SAGMM框架，通过自适应选择和组合不同的GNN模型来提升图学习性能，解决了现有模型选择困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型性能趋于饱和，复杂模型并不总是优于简单模型，且难以针对不同图任务选择合适模型。

Method: 使用拓扑感知注意力门控机制，从多样化模型池中自适应分配专家模型给每个节点，包含剪枝机制提高效率，并探索预训练专家模型变体。

Result: 在16个基准数据集上评估，涵盖节点分类、图分类、回归和链接预测任务，始终优于或匹配领先的GNN基线和现有混合方法。

Conclusion: SAGMM为现实世界图学习提供了鲁棒且自适应的解决方案，能够自动选择最适合的模型组合。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [223] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass是一个基于EMSNet和EMSServe的智能眼镜系统，用于提升急救医疗服务的实时决策和操作效率。


<details>
  <summary>Details</summary>
Motivation: 急救医疗技术人员在高压环境下需要快速做出关键决策，面临沉重的认知和操作负担，需要智能辅助系统来提升效率。

Method: 开发了EMSNet多模态多任务模型，整合文本、生命体征和场景图像；构建了EMSServe低延迟多模态服务框架，包含模态感知模型分割器和特征缓存机制。

Result: EMSNet在五个关键EMS任务上优于单模态基线模型；EMSServe比直接PyTorch多模态推理快1.9-11.7倍；用户研究显示EMSGlass提升了实时态势感知、决策速度和操作效率。

Conclusion: EMSGlass成功将多模态智能与现实世界急救响应工作流程相结合，为下一代AI赋能EMS系统提供了可行方向。

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [224] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 开发基于图神经网络的实时乳腺变形预测模型，用于提高间接MRI引导活检的准确性，实现亚毫米级精度和4000倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 解决间接MRI引导活检中实时变形乳腺模型精度不足的问题，克服直接MRI引导活检时间长、成本高的限制。

Method: 结合个体特异性有限元模型和GNN，利用MRI图像结构信息预测乳腺组织变形，处理表面位移和距离图数据。

Result: 在模型验证中，癌症节点位移误差小于0.2mm，与实际癌区空间重叠DSC达0.977，计算速度比传统FE模拟快4000倍。

Conclusion: 该变形感知GNN模型为乳腺活检中的实时肿瘤位移预测提供了高精度实时解决方案，有望显著提升乳腺癌诊断的精确性和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [225] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA是一个基于Transformer的多智能体强化学习框架，通过集中式图Transformer评论家和共享图Transformer演员解决网络控制中的长程依赖和拓扑泛化问题，并引入反事实优势估计器改进信用分配。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法存在两个主要局限：1）依赖局部交互衰减假设，难以捕捉长程依赖（如级联电力故障、疫情爆发）；2）缺乏跨网络拓扑的泛化能力，需要在新图上重新训练。

Method: STACCA采用集中式图Transformer评论家建模长程依赖并提供系统级反馈，共享图Transformer演员学习可泛化策略，并集成与状态值评论家估计兼容的反事实优势估计器改进信用分配。

Result: 在疫情控制和谣言传播网络控制任务上的评估表明，STACCA在性能、网络泛化能力和可扩展性方面均有提升。

Conclusion: 基于Transformer的MARL架构有潜力在大规模网络系统中实现可扩展和可泛化的控制。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [226] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES是一个用于机器遗忘的新框架，在仅能访问少量保留数据且无法访问遗忘数据的约束条件下，通过生成最优擦除样本和两阶段微调实现有效的类别特定知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘方法需要完整访问原始训练数据集的不现实问题，针对更实际的少样本零访问场景，即只能获取少量保留数据且遗忘数据完全不可访问的情况。

Method: 提出GFOES框架，包含生成反馈网络(GFN)和两阶段微调过程。GFN合成最优擦除样本(OES)，这些样本在目标类别上产生高损失，使模型能够遗忘类别特定知识；两阶段微调包括积极遗忘阶段和效用恢复阶段。

Result: 在三个图像分类数据集上的实验表明，GFOES在logit和表示层面都实现了有效的遗忘，同时仅使用5%的原始数据就能保持强大的性能。

Conclusion: GFOES为数据受限条件下的隐私保护机器学习提供了一个实用且可扩展的解决方案。

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [227] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种基于Schrödinger Bridge近似的方法，直接对齐控制组和扰动组的单细胞分布，避免了双向建模的复杂性，在遗传和药物扰动数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动预测是基因功能分析和药物候选选择的关键，但由于单细胞数据的非配对性质（同一细胞无法在扰动前后都被观测），现有方法存在条件不明确或依赖先验空间的问题。

Method: 利用Minibatch-OT配对来避免双向推理，直接指导桥学习，实现了SB的可扩展近似。同时建模离散基因激活状态和连续表达分布，通过联合训练捕获单细胞异质性。

Result: 在公共遗传和药物扰动数据集上的实验表明，该方法有效捕获了异质性单细胞响应，并实现了最先进的性能。

Conclusion: 该方法通过直接对齐分布和避免双向建模，为单细胞扰动预测提供了更精确和可扩展的解决方案。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [228] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: SoCo-DT是一种基于参数重要性的软冲突解决方法，通过动态调整掩码值来保留重要参数并抑制冲突参数，同时引入基于IQR的动态稀疏度调整策略，在Meta-World基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中存在跨任务的梯度冲突问题，现有基于掩码的方法使用粗粒度的二元掩码会过度抑制关键冲突参数，阻碍任务间知识共享，且不同任务的冲突程度不同，但现有方法使用固定稀疏度策略，限制了模型的泛化能力和学习效率。

Method: 提出SoCo-DT方法：1）利用Fisher信息动态调整掩码值，保留重要参数同时抑制冲突参数；2）基于四分位距(IQR)的动态稀疏度调整策略，根据冲突与和谐分数的分布构建任务特定的阈值方案；3）使用非对称余弦退火调度来持续更新阈值。

Result: 在Meta-World基准测试中，SoCo-DT在MT50上比最先进方法提升7.6%，在次优数据集上提升10.5%，证明其在缓解梯度冲突和提升多任务性能方面的有效性。

Conclusion: SoCo-DT通过软冲突解决和动态稀疏度调整，有效缓解了多任务强化学习中的梯度冲突问题，显著提升了模型性能，为多任务学习提供了更灵活的解决方案。

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [229] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: 提出了pFed1BS框架，通过一比特随机草图实现极致的通信压缩，在个性化联邦学习中解决通信开销和数据异构性问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临双向通信开销和客户端数据异构性的关键挑战，需要在降低通信成本的同时处理数据异质性

Method: 使用一比特随机草图进行通信压缩，引入基于符号的正则化器指导本地模型与全局共识对齐，采用快速哈达玛变换提高投影效率

Result: 理论分析证明算法收敛到全局势函数的稳定邻域，数值模拟显示显著降低通信成本且性能与先进通信高效FL算法相当

Conclusion: pFed1BS在保持竞争力的同时大幅减少了通信开销，为个性化联邦学习提供了有效的通信压缩解决方案

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [230] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo是一种新颖的量化方法，允许设备端LLM在单次微调后灵活切换量化精度，通过共享指数浮点机制和位宽鲁棒性学习实现性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在结构限制，无法支持设备端在不同任务中灵活切换精度（如理解任务和生成任务需要不同位宽），限制了实际部署的适应性。

Method: 提出共享指数浮点(SEFP)量化机制，通过简单的尾数截断生成不同位宽；采用位宽路径搜索(BPS)和低精度异步累积(LAA)策略进行位宽鲁棒性学习。

Result: 在LLaMA3.2-1B和LLaMA3-8B等流行LLM上的实验表明，OTARo在所有精度下都能实现一致强大且鲁棒的性能。

Conclusion: OTARo成功解决了传统量化方法在设备端部署中的灵活性限制，为复杂现实场景下的精度切换提供了有效解决方案。

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [231] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: 使用图神经网络预测QP求解器中的有效集，通过将QP问题表示为二分图来加速求解过程，减少迭代次数并实现跨问题规模的泛化。


<details>
  <summary>Details</summary>
Motivation: 二次规划求解器在实时控制和优化中广泛应用，但其计算成本限制了在时间关键场景中的应用。需要一种方法来加速求解过程。

Method: 采用学习优化方法，使用图神经网络将QP问题表示为二分图，学习预测对偶有效集求解器中的最优有效集，从而有效预热启动求解器。

Result: 在不同问题规模下，GNN始终减少求解器迭代次数，性能与多层感知机基线相当。在变化问题规模上训练的GNN能有效泛化到未见维度。

Conclusion: 结构感知学习在加速实时应用（如模型预测控制）中的优化具有潜力，GNN方法展示了灵活性和可扩展性。

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [232] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 提出了一种物理信息神经算子(PINO)方法，用于金属增材制造中实时预测未来15秒的z和y方向变形场，通过解耦热机械响应并结合热传导方程作为软约束来确保物理一致性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统的发展迫切需要实时变形场预测来控制金属增材制造缺陷，但数值模拟方法计算成本高，传统机器学习模型难以提取时空特征进行长期预测且无法解耦热机械场。

Method: 使用物理信息深度算子网络-循环神经网络(PIDeepONet-RNN)，通过主干和分支网络分别处理温度历史和编码变形场，结合热传导方程作为软约束确保物理一致性。

Result: 模型在实验验证的有限元法生成数据集上训练测试，z和y方向最大绝对误差分别为0.9733mm和0.2049mm，误差累积低，时间效率高，熔池区域误差较高但沉积和关键区域梯度范数低。

Conclusion: PINO代理模型在实时长期物理场预测方面表现出色，具有控制缺陷的潜力，基于物理定律的基础函数为预测提供了稳健且可解释的基础。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [233] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文评估了在CARS到拉曼信号重建中的各种不确定性量化技术，并证明将物理知识约束融入模型可以改善校准，为更可靠的CARS数据分析提供了有前景的路径。


<details>
  <summary>Details</summary>
Motivation: CARS光谱学在医学、材料科学和化学分析中被广泛应用，但其有效性受到非共振背景干扰真实拉曼信号的限制。虽然深度学习方法已用于从CARS数据重建拉曼光谱，但这些确定性模型缺乏量化不确定性的能力，而这在科学和生物医学应用中至关重要。

Method: 评估和比较了在CARS到拉曼信号重建背景下的各种不确定性量化技术，并将物理知识约束（如Kramers-Kronig关系和光滑性约束）以物理信息损失函数的形式整合到模型中。

Result: 研究表明，将物理信息约束融入这些模型可以改善它们的校准性能。

Conclusion: 这项工作为更可靠的CARS数据分析提供了有前景的路径，通过结合不确定性量化和物理知识约束来提高模型的可信度。

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [234] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: 提出了DiffFP框架，通过扩散策略学习鲁棒的多模态行为策略，在连续决策空间的零和博弈中实现快速收敛和对抗多样性对手的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决连续决策空间中自博弈强化学习收敛缓慢、难以达到纳什均衡的问题，防止智能体被未见对手策略利用

Method: 使用虚构博弈框架，通过扩散策略生成模型来估计对未见对手的最佳响应，学习自适应和多样化的策略

Result: 在赛车和多粒子零和博弈环境中，相比基线RL方法实现3倍收敛速度和30倍成功率提升，达到ε-纳什均衡

Conclusion: DiffFP框架能够有效解决连续空间自博弈的挑战，学习鲁棒的多模态策略，在动态多智能体环境中表现出色

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [235] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: ParaDySe是一个用于动态序列的自适应并行策略切换框架，通过实时选择最优并行策略来解决LLM训练中的内存不足和通信并行化抵消问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer大语言模型训练框架对动态长度序列采用预定义的静态并行策略，导致短序列通信并行化抵消和长序列内存不足的问题。

Method: 实现统一张量布局的并行策略模块化函数库，构建序列感知的内存和时间成本模型，通过启发式算法为动态序列选择最优层间策略。

Result: 在序列长度达624K的数据集上测试，ParaDySe成功解决了LLM训练中的内存不足和通信并行化抵消瓶颈。

Conclusion: ParaDySe通过系统整合长序列优化与现有框架，实现了LLM训练中动态序列的最优并行策略自适应切换。

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [236] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新颖的长到短推理压缩方法，通过自适应选择推理深度和分布对齐的语言精炼，在保持准确性的同时显著减少推理LLM的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有推理LLM生成的长链式思维导致token使用量增加，造成更高的推理延迟和内存消耗，而现有的长到短方法在减少推理长度时往往牺牲准确性，需要一种能在保持性能的同时降低token成本的方法。

Method: 1) 自适应选择推理深度匹配问题复杂度的自生成样本；2) 分布对齐的语言精炼方法，在不改变底层推理路径的情况下优化语言表达，提高推理路径的清晰度和简洁性。

Result: DeepSeek-R1-Distill-Qwen-7B使用该方法在MATH500基准测试中实现了50%的平均token减少，同时保持准确性。

Conclusion: TokenSqueeze仅使用模型自生成数据，无需依赖手动策划的短答案数据集，就能在不同应用中实现高效且高保真的推理。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [237] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文将图基半监督学习从有限维欧几里得空间扩展到无限维Wasserstein空间，证明了离散图p-Dirichlet能量到连续对应物的变分收敛性，并表征了Wasserstein空间子流形上的Laplace-Beltrami算子。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，研究图基半监督学习方法，特别是将Laplace学习扩展到Wasserstein空间，以处理高维数据在低维子空间中的分布问题。

Method: 通过证明离散图p-Dirichlet能量到连续对应物的变分收敛性，将图基半监督学习从有限维欧几里得空间扩展到无限维Wasserstein空间，并表征子流形上的Laplace-Beltrami算子。

Result: 在基准数据集上的数值实验验证了所提出的理论框架，展示了在高维设置下分类性能的一致性。

Conclusion: 成功将图基半监督学习扩展到Wasserstein空间，为处理高维数据提供了新的理论框架和有效方法。

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [238] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost是一个新的梯度提升框架，采用自组织树结构，在训练过程中动态调整分裂行为，通过自适应分裂函数和问题指纹识别，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升算法使用静态树结构和固定分裂标准，无法适应训练过程中梯度分布的变化和不同学习阶段的问题特定特征，限制了模型的适应能力。

Method: 引入自组织树结构，具有动态演化的分裂函数；结合梯度得分和信息论指标的分裂准则；自动问题指纹识别；向量化树预测；交互感知特征重要性；快速模式优化。

Result: 在10个数据集上基准测试中，MorphBoost平均比XGBoost性能提升0.84%，获得4/10数据集胜利（40%胜率）和6/30前三名（20%），保持最低方差（σ=0.0948）和最高最小准确率。

Conclusion: MorphBoost通过动态树结构演化实现了卓越的性能和鲁棒性，特别是在复杂问题上表现更优，展示了自适应梯度提升框架的有效性。

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [239] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: CONFETTI是一种新颖的多目标反事实解释方法，用于多元时间序列分析，在保持预测置信度、邻近性和稀疏性平衡的同时，提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在多元时间序列分类和回归中缺乏透明度，现有可解释AI方法无法完整传达决策空间，反事实解释方法通常只优先考虑准确性、邻近性或稀疏性中的单一目标。

Method: CONFETTI通过识别关键时间序列子序列、定位反事实目标，并优化修改时间序列来平衡预测置信度、邻近性和稀疏性这三个目标。

Result: 在UEA档案的7个多元时间序列数据集上评估，CONFETTI在优化目标和其他6个文献指标上持续优于最先进的反事实解释方法，实现≥10%更高的置信度，同时在≥40%的情况下改善稀疏性。

Conclusion: CONFETTI提供了一种有效的多目标反事实解释方法，能够以最小改变提供可操作见解，提高可解释性和决策支持能力。

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [240] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文对无监督和半监督的基于文本的理想点估计算法进行了首次系统性综述，分析了从词频模型到大型语言模型的发展历程，提出了区分文本方差生成、捕获和聚合的概念框架，识别了四种方法家族，并为应用研究者提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 填补文本理想点估计算法领域缺乏系统性比较和明确应用指导的空白。该领域在过去二十年中随着NLP技术的发展而快速演进，但形成了碎片化的研究现状。

Method: 通过系统性文献综述识别了25种CT-IPE算法，进行手动内容分析，并提出了区分文本方差生成、捕获和聚合的概念框架，将算法分为词频、主题建模、词嵌入和LLM四种方法家族。

Result: 建立了四种方法家族的系统分类，分析了各自的假设、可解释性、可扩展性和局限性，提供了算法选择的实用指导，强调了不同算法估计结果差异的信息价值。

Conclusion: 该综述为理解二十年算法发展提供了结构化综合，为应用研究者提供了透明性、技术要求和验证策略权衡的指导，并强调需要系统基准测试来理解算法差异。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [241] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: LLMs在动态环境中存在信念更新不一致和行动与信念不匹配的问题，即使在静态任务中表现良好的模型也难以预测其在复杂现实环境中的行为。


<details>
  <summary>Details</summary>
Motivation: 现实任务与静态数据集存在差异，需要研究LLMs在动态环境中如何连贯更新信念并基于信念做出适当决策的能力。

Method: 通过测量LLMs直接表达的后验信念与正确更新先验信念之间的差异，以及在博彩市场中行动与内部信念的一致性，评估模型的一致性表现。

Result: LLMs在信念更新上存在高达30%的平均不一致性，行动常与内部信念方向相反，且在受到挑战时表现出中度自我不一致性。

Conclusion: LLMs在复杂现实环境中的行为难以预测，即使在高准确率或良好校准的模型中也存在这些问题。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [242] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 本文提出了一个全面的多模态模型编辑评估框架，通过三个关键维度和七种数据类型来检测过拟合问题，并揭示了"瞬时盲视"现象，同时提出了位置感知对抗损失来改善跨模态表示平衡。


<details>
  <summary>Details</summary>
Motivation: 现有从文本模型编辑改编的多模态模型编辑评估方法依赖低相似性或随机输入，夸大了成功并掩盖了过拟合问题，需要更全面的评估框架。

Method: 提出了包含随机图像位置性、无图像位置性和一致图像位置性的三维度评估框架，引入De-VQA动态评估方法，并通过位置感知对抗损失来平衡跨模态表示。

Result: 实证结果显示该方法持续优于现有基线，平均减少17%的瞬时盲视并改善位置性。令牌分析显示编辑对文本令牌的影响不成比例。

Conclusion: 提出的综合评估框架和位置感知对抗损失方法有效解决了多模态模型编辑中的过拟合和跨模态表示不平衡问题。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [243] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: 提出了一个用于蛋白质生成模型的非可微分指导框架，将连续扩散生成器与任意黑盒目标通过遗传算法结合，解决了实验数据难以整合到蛋白质生成模型中的问题。


<details>
  <summary>Details</summary>
Motivation: 准确的蛋白质结构对于理解生物功能至关重要，但将实验数据整合到蛋白质生成模型中仍然是一个主要挑战。大多数实验观测预测器是不可微分的，这使得它们与基于梯度的条件采样不兼容，特别是在核磁共振中化学位移等丰富数据难以直接整合到生成建模中。

Method: 引入了一个非可微分指导框架，将连续扩散生成器与任意黑盒目标通过定制的遗传算法耦合。该方法在三种模式上进行了演示：成对距离约束、核奥弗豪泽效应约束以及首次实现的化学位移指导。

Result: 证明了化学位移指导结构生成的可行性，揭示了当前预测器的关键弱点，并展示了一种整合多样化实验信号的通用策略。

Conclusion: 这项工作指向了超越可微分限制的自动化、数据条件化蛋白质建模，为整合各种实验信号提供了通用解决方案。

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [244] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: 本文为ogbn-proteins数据集提供了可复现的边缘感知基线方法，研究了边缘特征聚合和消息传递中的关键系统选择，发现基于sum的边到节点特征的GraphSAGE表现最佳，并通过后处理技术进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为ogbn-proteins数据集建立可复现的边缘感知基线，研究实践中主导的两个系统选择：如何将8维边缘证据聚合为节点输入，以及如何在消息传递中使用边。

Method: 使用PyTorch Geometric实现GraphSAGE，比较sum、mean、max三种边缘特征聚合方法，评估LayerNorm、BatchNorm和物种感知条件LayerNorm，并应用后处理技术如温度缩放和标签相关平滑。

Result: 在主要实验设置中，sum聚合方法始终优于mean和max；BatchNorm获得最佳AUC，而条件LayerNorm在AUC前沿匹配的同时具有更好的阈值F1；后处理技术显著改善了micro-F1和校准误差。

Conclusion: 基于sum的边缘到节点特征的GraphSAGE是最强基线，结合适当的归一化方法和后处理技术可以进一步提升性能，为ogbn-proteins提供了标准化的基准方法。

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [245] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 提出了一种基于Voronoi分割的模型无关知识蒸馏方法，将深度强化学习控制器的状态空间划分为多个区域，在每个区域使用可解释的线性模型来近似原始黑盒策略。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习控制器缺乏透明度，难以满足监管要求和建立信任。需要将学习到的行为转移到人类可读的模型中，但单一简化模型在动态情况下表现不佳，需要在灵活性和复杂性之间找到平衡。

Method: 使用Voronoi分割方法将状态空间划分为多个区域，在每个区域内使用线性模型来近似原始控制器的行为，实现局部专业化的模型蒸馏。

Result: 在网格世界环境和经典控制任务上的评估表明，该方法产生的策略具有可解释性，并且蒸馏后的性能与原始黑盒策略相当甚至略有提升。

Conclusion: 提出的局部专业化线性模型蒸馏方法能够生成可解释的策略，在保持性能的同时解决了深度强化学习控制器的透明度问题。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [246] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: 该论文提出Tab-PET框架，通过图结构估计位置编码来提升表格数据上Transformer模型的泛化性能，发现位置编码能降低特征有效秩从而简化任务。


<details>
  <summary>Details</summary>
Motivation: 表格数据缺乏结构线索和位置信息，传统Transformer模型难以有效利用自注意力机制。现有表格Transformer模型通常不使用位置编码，但研究发现位置编码能显著提升泛化性能。

Method: 提出Tab-PET框架，基于图结构估计位置编码。探索两种图估计范式：基于关联性的图和基于因果关系的图，并将位置编码融入表格Transformer的嵌入中。

Result: 在50个分类和回归数据集上的实验表明，图推导的位置编码显著提升了3T模型的性能。基于关联性的图比因果驱动的方法获得更稳定和显著的性能提升。

Conclusion: 位置编码在表格Transformer中具有意外的重要作用，能够通过降低特征有效秩来改善泛化能力，为表格数据建模提供了新的视角。

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [247] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出了一种基于表格基础模型的简单而稳健的方法，用于岩石不连续性的统计准确生成预测，在数据稀疏条件下优于传统统计模型和深度生成方法。


<details>
  <summary>Details</summary>
Motivation: 岩石不连续性对岩体力学行为和稳定性至关重要，但其内部分布通常无法直接观测，只能通过表面暴露的不连续性进行推断。现有生成预测方法要么无法捕捉复杂的分布模式，要么在数据稀疏条件下缺乏稳健性。

Method: 利用专门为小数据设计的表格基础模型，充分发挥其强大的样本学习能力，从有限的测量不连续性中有效捕捉潜在的复杂分布模式。

Result: 在十个具有不同规模和分布模式的数据集上的比较实验表明，该方法在准确性和稳健性方面优于传统统计模型和深度生成方法。

Conclusion: 这项工作推进了岩体结构的定量表征，支持更安全、更可靠的数据驱动岩土工程设计。

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [248] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出了一种用于多模态食物学习的持续学习框架，通过双LoRA架构和质量增强伪回放来解决现有大模型在食物分析中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大多模态模型在食物分析中学习新任务时会出现灾难性遗忘，需要昂贵的从头重新训练。

Method: 集成双LoRA架构与质量增强伪回放：为每个任务引入两个互补的低秩适配器（专用LoRA学习任务特定知识，协作LoRA通过伪回放整合跨任务共享知识），并使用自一致性和语义相似性提高回放数据可靠性。

Result: 在Uni-Food数据集上的实验显示在减轻遗忘方面表现出优越性能。

Conclusion: 这是首个针对复杂食物任务的有效持续学习方法。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [249] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估了六种参数空间合并技术应用于两个基于Mistral-7B的医疗大语言模型，发现对于架构兼容的模型，简单的平均方法在医疗基准测试中表现最佳，为资源受限的分布式医疗AI部署提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决分布式医疗中大语言模型面临的挑战：整合跨机构的专业领域知识同时保护隐私、降低计算开销、防止模型更新时的灾难性遗忘。

Method: 提出了一种新颖的分层方法，结合选择性最优传输对齐注意力层和余弦相似度加权插值，并系统评估了六种参数合并技术：任务算术、线性平均、DARE-TIES、DELLA、Breadcrumbs和分层方法。

Result: 结果显示架构兼容的模型从简单平均方法中获益显著，任务算术在MedQA上达到45.80%准确率，优于复杂的剪枝方法。

Conclusion: 对于架构兼容的模型，简单平均提供了稳健且计算高效的知识整合基线，为可扩展医疗AI系统提供了实用路径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [250] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: 该论文提出PackingStar系统，通过将接吻数问题建模为双玩家矩阵完成游戏，使用博弈论强化学习探索高维空间，在25-31维度中超越了所有已知记录。


<details>
  <summary>Details</summary>
Motivation: 接吻数问题自牛顿时代以来一直是几何学基本挑战，高维几何的不规则性和指数级组合复杂性限制了现有方法的可扩展性。

Method: 将问题建模为双玩家矩阵完成游戏：一个玩家填充矩阵条目（表示球心向量对余弦），另一个玩家修正次优条目，共同最大化矩阵大小（对应接吻数）。

Result: 在25-31维度中超越了所有人类已知记录，25维配置几何对应Leech格点并可能最优；在13维度实现了1971年以来首次突破有理结构的突破；在14维等维度发现了6000多个新结构。

Conclusion: 证明了AI在探索超越人类直觉的高维空间方面的能力，为接吻数问题和更广泛的几何问题开辟了新途径。

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [251] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: 提出了一种针对可微分模拟器的贝叶斯参数推断新方法，通过将随机模拟重新表述为确定性优化问题，利用梯度方法高效导航至高密度后验区域，大幅减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 复杂随机模拟器的贝叶斯参数推断面临似然函数难以处理的问题，现有基于模拟的推断方法需要大量模拟，在高维参数空间或部分无信息输出问题中成本高昂。

Method: 基于优化蒙特卡洛框架，将随机模拟重新表述为确定性优化问题，应用基于梯度的方法高效导航至高密度后验区域，避免在低概率区域进行浪费性模拟，并通过JAX实现关键组件的向量化。

Result: 在高维参数空间、无信息输出、多观测和多峰后验等广泛实验中，该方法始终匹配并经常超越最先进方法的准确性，同时大幅减少运行时间。

Conclusion: 该方法为可微分模拟器提供了一种高效准确的贝叶斯参数推断解决方案，在保持精度的同时显著提升了计算效率。

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [252] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: 提出PAST网络处理交通时间序列数据补全，通过主模式-辅助模式分离方法应对随机、纤维和块状缺失数据，在27种缺失条件下优于7个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以适应随机缺失位置，且无法学习长期和大规模依赖关系，这在大量缺失条件下至关重要。

Method: 将模式分为主模式（数据点间内部关系）和辅助模式（外部因素影响），构建PAST网络，包含图集成模块（GIM）和交叉门控模块（CGM），通过动态图和多阶卷积捕获主模式，通过双向门控提取辅助模式。

Result: 在三个数据集上的27种缺失数据条件下，PAST的补全精度在RMSE和MAE指标上分别比7个最先进基线方法提升高达26.2%和31.6%。

Conclusion: PAST通过分离主模式和辅助模式，有效处理各种缺失数据条件，显著提升了交通时间序列补全性能。

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [253] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: 提出了MMWSTM-ADRAN+双流深度学习架构，结合天气状态转换模型和异常驱动注意力网络，用于极端气温事件的短期预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端气温事件对气候风险管理至关重要，但现有方法在捕捉低概率极端事件信号方面存在挑战。

Method: 使用双流架构：MMWSTM流通过双向LSTM和可学习马尔可夫状态转移矩阵捕捉天气状态变化；ADRAN流通过双向GRU、多头自注意力和异常放大层增强对异常信号的敏感性；通过注意力融合门自适应组合两流输出。

Result: 模型采用定制化极端天气损失函数，对温度分布上下5%的误差给予更高权重，并通过时间序列数据增强将训练数据扩增四倍。

Conclusion: 该模型通过结合天气状态动态和异常检测机制，为极端气温预测提供了有效的深度学习解决方案。

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [254] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: 本文分析了在有限数据和多次训练周期下，数据缩放定律的变化形式。通过定义"有效重用率"E(K,N)，量化了K次训练周期与单次训练所需数据量的关系，揭示了数据重用对缩放定律的重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大规模语料库单次训练的数据缩放定律，但在有限数据和多次训练周期下的缩放定律形式仍不明确。本文旨在填补这一空白，特别关注数据重复使用对缩放定律的影响。

Method: 在线性回归框架下，通过理论分析研究SGD在强凸性或Zipf分布数据下的缩放行为。定义了有效重用率E(K,N)作为核心分析工具，并通过LLM实验进行实证验证。

Result: 发现：(1)当K较小时，E(K,N)≈K，每次新训练周期带来线性增益；(2)随着K增加，E(K,N)趋于平台期，该平台值随N增长(强凸情况下为Θ(log N))，表明大数据集可以重复更多次而边际效益不消失。

Conclusion: 数据重用的最大有效次数取决于数据规模和分布，这一发现修正了先前实证研究的结论，并强调了在未来缩放定律研究中需要明确建模这两个因素的重要性。

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [255] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: 提出了一种基于图像卷积聚类的无监督工业时序数据分析框架，用于发现操作模式，在熔炉操作数据上识别出7种可解释的运行模式。


<details>
  <summary>Details</summary>
Motivation: 工业过程监控依赖传感器时序数据，但缺乏标签、高变异性和操作噪声使得传统方法难以提取有效模式。现有聚类方法无法处理动态、非结构化的工业序列数据。

Method: 将原始时序数据通过滑动窗口转换为灰度矩阵表示，使用深度卷积自编码器进行特征提取，集成软硬聚类输出并通过两阶段策略优化选择，使用新开发的复合评分S_eva评估聚类性能。

Result: 在3900多个熔炉操作数据上识别出7种可解释的操作模式，揭示了能耗、热力学特性和生产时长的显著差异，相比传统和深度聚类基线方法表现更优。

Conclusion: 该框架解决了无监督时序分析中的序列不规则性、模式重叠和度量不一致等关键挑战，为工业系统的数据驱动诊断和能源优化提供了通用解决方案。

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [256] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: 提出了一种基于自监督表示学习的方法，结合肺活量图时间序列和人口统计学数据，用于早期检测右心衰竭。该方法在UK Biobank数据集上表现良好，在高风险临床亚组中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 右心衰竭是一种与高发病率和死亡率相关的疾病，肺病常导致右心室负荷增加。从患有肺源性心脏病的人群中筛查出发展为右心衰竭的患者非常重要。

Method: 采用两阶段方法：第一阶段使用变分自编码器进行自监督表示学习，从数据增强的无标签数据中学习肺活量图时间序列的鲁棒低维表示；第二阶段将该表示与人口统计学信息融合，输入CatBoost分类器进行右心衰竭预测。

Result: 在UK Biobank的26,617名个体上测试，模型检测右心衰竭的AUROC为0.7501。在高风险临床亚组中表现更好：慢性肾病患者测试集AUROC为0.8194，瓣膜性心脏病患者为0.8413。

Conclusion: 该研究提出的自监督表示学习方法结合肺活量图时间序列和人口统计学数据，在临床实践中显示出早期检测右心衰竭的潜力。

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [257] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: 提出了多任务回归GINN-LP（MTRGINN-LP），一种用于多目标符号回归的可解释神经网络，通过结合GINN-LP与多任务深度学习，在保持可解释性的同时捕获目标间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 符号回归面临两个主要挑战：大多数方法在科学数据集上评估，限制了泛化能力；主要针对单输出回归，而许多现实问题涉及具有相互依赖变量的多目标输出。

Method: 将GINN-LP与多任务深度学习集成，结合包含多个幂项逼近器块的共享主干和任务特定输出层，捕获目标间依赖关系同时保持可解释性。

Result: 在能源效率预测和可持续农业等实际多目标应用上验证，实验结果显示具有竞争力的预测性能和高可解释性。

Conclusion: 有效将符号回归扩展到更广泛的现实世界多输出任务。

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [258] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: 提出了AdamX优化算法，通过改进二阶矩估计的指数衰减率，在训练后期逐渐减弱学习步长修正强度，最终退化为SGD，提高了稳定期的训练稳定性并可能增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: Adam优化算法在大语言模型时代仍是主流，但与SGD相比更容易收敛到非平坦最小值，这影响了模型的泛化能力。

Method: 提出一种新型二阶矩估计指数衰减率，随着训练进行逐渐减弱学习步长修正强度，在稳定训练期退化为SGD。

Result: 实验结果表明，新的二阶矩估计指数衰减率优于现有方法，AdamX在性能上能够稳定超越Adam及其变体。

Conclusion: AdamX算法通过改进二阶矩估计机制，有效解决了Adam收敛到非平坦最小值的问题，提高了训练稳定性和泛化能力。

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [259] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: GREAT框架通过辅助变换增强环境建模数据，解决未监测区域预测难题，在零样本场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 环境建模面临未监测区域预测挑战，由于观测数据有限且地理分布不均，加上空间异质性导致模型学习虚假模式。需要保持物理关系不变性和时间一致性。

Method: GREAT框架学习神经网络多层的变换函数，增强原始环境特征和时间影响，通过双层训练过程约束增强数据保留源数据关键模式。

Result: 在美国东部六个生态多样化流域的河流温度预测实验中，GREAT在零样本场景下显著优于现有方法。

Conclusion: 该工作为环境应用中全面监测不可行的情况提供了实用解决方案。

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [260] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: 该论文提出了一种量子自监督预训练方法，通过在可编程离子阱量子计算机上编码图像为量子态，利用无标签数据进行对比预训练，从而减少对有标签数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临有标签数据稀缺的挑战，特别是在模型规模和复杂性增加时。需要开发能够从无标签数据中学习不变性的方法。

Method: 在可编程离子阱量子计算机上实现自监督预训练，将图像编码为量子态，通过硬件上的原位对比预训练学习量子表示。

Result: 经过预训练的表示在微调后，相比随机初始化的模型，图像分类的平均测试准确率更高，运行间变异性更低，特别是在有标签训练数据有限的场景下表现更显著。

Conclusion: 这项工作为量子表示学习建立了一条标签高效的路径，对量子原生数据集具有直接相关性，并为处理更大规模经典输入提供了清晰路径。

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [261] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: Naga是一种基于吠陀数学结构概念的深度状态空间模型编码方法，通过双向处理正向和时间反转序列，结合哈达玛积交互，提升长程时间依赖捕捉能力，在多个长时序预测基准测试中优于28个SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度状态空间模型在长时序预测中面临捕捉远距离时间依赖的挑战，需要更有效的编码方法来提升性能。

Method: 提出双向表示方法，联合处理正向和时间反转输入序列，通过哈达玛积交互实现吠陀数学启发的编码结构。

Result: 在ETTh1、ETTh2、ETTm1、ETTm2、Weather、Traffic和ILI等多个基准测试中，Naga超越了28个当前最先进模型，且相比现有深度SSM方法具有更高的效率。

Conclusion: 引入结构化的吠陀数学启发的分解方法可以为长程序列建模提供可解释且计算高效的替代方案。

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [262] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: 提出了一种基于量子物理学的"白盒"方法，通过将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，并求解薛定谔方程和使用微扰理论来量化不确定性，从而提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中准确不确定性量化的关键挑战。神经网络虽然功能强大但缺乏可解释性，而概率"白盒"模型虽然可解释但性能较差。需要一种既能准确量化不确定性又具有良好可解释性的方法。

Method: 将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，其中核均值嵌入作为其本征函数或本征模式。求解薛定谔方程并应用微扰理论来量化不确定性。

Result: 在变化点检测和时间序列聚类任务中，该方法相比最先进的"白盒"模型表现出有效性，为决策过程中的不确定性提供了洞察。

Conclusion: 提出的量子物理启发的"白盒"方法能够同时提供准确的不确定性量化和增强的可解释性，在时间序列分析任务中具有实际应用价值。

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [263] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 该论文提出了一种针对高分辨率多模态非线性显微镜图像中肿瘤检测的补丁级二元分类方法，通过GERNE去偏策略解决因补丁组成与标签之间虚假相关性导致的模型偏差问题。


<details>
  <summary>Details</summary>
Motivation: 补丁级多标签分类为高分辨率图像提供了一种高效替代像素级分割的方法，但简化的二元分类公式可能引入补丁组成与标签之间的虚假相关性，导致模型预测偏差。

Method: 采用补丁级二元分类方法检测肿瘤，应用GERNE去偏策略来最大化最差组准确率(WGA)，缓解因补丁中组织区域大小与标签之间的虚假相关性带来的偏差。

Result: 与ERM相比，GERNE去偏策略使WGA提高了约7%，显著提升了模型在关键少数情况（如小组织区域的肿瘤补丁和大组织区域的非肿瘤补丁）上的性能。

Conclusion: 研究强调了在补丁级分类问题中进行虚假相关性感知学习的重要性，提出的去偏策略能有效提升模型在关键样本上的性能。

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [264] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了FairGLite框架，在有限人口统计信息下实现公平图学习，通过生成人口统计代理、强制跨群体嵌入一致性和自适应置信度策略来缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平图学习方法大多需要完整的人口统计信息，这在实践中因隐私、法律或监管限制而难以满足，因此需要在有限人口统计信息下实现公平学习。

Method: 使用部分人口统计数据指导生成人口统计信息代理，设计跨人口群体节点嵌入一致性策略，开发基于预测置信度的自适应贡献调整机制。

Result: 理论分析证明FairGLite在群体公平指标上具有可证明的上界，实验表明该框架在多个数据集和公平图学习框架中能有效缓解偏见并保持模型效用。

Conclusion: FairGLite框架在有限人口统计信息下实现了有效的公平图学习，提供了形式化的偏见缓解保证，并在实践中表现出良好的性能。

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [265] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出了一种新的测试时图OOD检测方法BaCa，通过双动态字典校准OOD分数，无需微调预训练模型，在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图OOD检测中缺乏真实OOD样本训练数据的挑战，现有方法仅捕捉ID特征导致分布边界表示不可靠，且图数据的潜在多因素结构未被充分探索。

Method: BaCa方法通过估计图论并应用混合策略生成边界感知判别拓扑，构建双动态字典（优先级队列+注意力机制）自适应捕获潜在ID和OOD表示，用于边界感知OOD分数校准。

Result: 在真实世界数据集上的广泛实验表明，BaCa在OOD检测方面显著优于现有最先进方法。

Conclusion: BaCa方法通过双动态字典和边界感知校准，有效解决了图OOD检测的关键挑战，无需微调模型或暴露辅助数据集，实现了优越的检测性能。

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [266] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: 提出RAC-DMVC框架，通过可靠性图指导多源噪声环境下的鲁棒表示学习，解决多视图聚类中的缺失噪声和观测噪声问题。


<details>
  <summary>Details</summary>
Motivation: 增强多视图聚类在真实场景中的适用性，解决更具挑战性的多源噪声问题，包括缺失噪声和观测噪声。

Method: 使用可靠性图指导鲁棒表示学习；通过跨视图重建处理观测噪声；可靠性感知噪声对比学习缓解噪声表示带来的正负对选择偏差；双注意力插补处理缺失噪声；自监督聚类蒸馏模块优化表示。

Result: 在五个基准数据集上的广泛实验表明，RAC-DMVC在多个评估指标上优于最先进方法，并在不同噪声比例下保持优异性能。

Conclusion: RAC-DMVC框架能有效处理多视图聚类中的多源噪声问题，显著提升聚类性能和对噪声的鲁棒性。

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [267] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列开源物理推理模型通过强化学习训练，在物理奥林匹克竞赛中表现卓越，其中P1-235B-A22B在IPhO 2025获得金牌，并在13个国际/地区物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 推动大型语言模型从谜题解决向科学级推理发展，物理作为连接符号与现实的基础学科，是测试这种转变的最佳领域。

Method: 完全通过强化学习训练开源物理推理模型，并配备代理框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌，在13个物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；配备PhysicsMinions后达到IPhO 2025总分第一。

Conclusion: P1系列模型不仅在物理推理方面表现卓越，在数学和编程等其他推理任务上也展现出强大的泛化能力。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [268] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: 本文提出了一种解耦准牛顿更新的方法，通过协程在批处理采集函数调用的同时实现更快的贝叶斯优化收敛。


<details>
  <summary>Details</summary>
Motivation: 当前BoTorch库在优化多点采集函数时，由于准牛顿方法中逆Hessian矩阵的非对角近似误差，导致收敛速度变慢。

Method: 使用协程解耦准牛顿更新，同时保持采集函数调用的批处理能力。

Result: 该方法不仅理论上与顺序多起点优化具有相同的收敛性，而且相比之前方法大幅减少了实际运行时间。

Conclusion: 提出的解耦准牛顿更新方法有效解决了贝叶斯优化中采集函数优化的计算瓶颈问题。

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [269] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 使用电子健康记录数据，通过循环神经网络模型预测儿童在未来30天内是否会出现异常血清肌酐值，展示了简单时序表示在儿科数据中的实用性。


<details>
  <summary>Details</summary>
Motivation: 儿科肾病表现和进展差异很大，需要持续监测肾功能，但传统监测方法效率有限，需要开发自动化的预测模型。

Method: 使用2019-2025年英国Great Ormond Street医院的电子健康记录，整合纵向实验室序列和人口统计信息，训练循环神经网络模型进行预测。

Result: 模型能够预测儿童在未来30天内是否会出现异常血清肌酐值，证明了简单时序表示在常规儿科数据中能捕捉有用模式。

Conclusion: 这项试点研究为未来使用额外临床信号和更详细肾脏结局的多模态扩展奠定了基础，展示了时序建模在儿科肾功能监测中的潜力。

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [270] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文分析了混合真实与合成数据对LLM训练的影响，识别了三种缩放行为阶段，提出了适用于混合数据的泛化界限，并开发了高效的数据估值方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽然具有可扩展性和成本效益，但会引入系统性分布差异，特别是在长尾知识方面存在代表性不足的问题，这给评估混合数据集的效用带来了根本性挑战。

Method: 识别了三种缩放行为阶段，推导了适用于真实-合成混合数据的LLM泛化界限，并提出了一种高效的数据估值方法。

Result: 在图像分类、情感分类、指令跟随和复杂推理四个任务上的综合实验表明，该方法在数据估值方面超越了现有最优基线，且计算成本显著降低。

Conclusion: 该研究为理解和优化混合真实-合成数据集提供了理论框架和实用工具，有助于更有效地利用合成数据进行LLM训练。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [271] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg是一个CUDA算子，将GraphSAGE的邻居采样和均值聚合融合为单次操作，在多个图基准测试中实现了最高51倍的加速和100倍的内存减少。


<details>
  <summary>Details</summary>
Motivation: 传统的GraphSAGE实现需要多次内核启动和块物化，导致内存流量大和开销高。通过融合采样和聚合操作可以减少内存访问和计算开销。

Method: 开发了FuseSampleAgg CUDA算子，将邻居采样和均值聚合融合到单次传递中，通过保存索引重放来保持GraphSAGE均值语义，消除了块物化和额外内核启动。

Result: 在Reddit、ogbn-arxiv和ogbn-products基准测试中，实现了最高51倍的步进时间加速（ogbn-products），内存峰值减少最高100倍，同时保持确定性并与标准PyTorch优化器兼容。

Conclusion: FuseSampleAgg通过操作融合显著提升了GraphSAGE训练效率，大幅减少了内存使用和计算时间，为大规模图神经网络训练提供了高效解决方案。

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [272] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: 通过训练权重稀疏的语言模型来寻找人类可理解的电路，在保持性能的同时提高可解释性，并验证了这些电路的可理解性。


<details>
  <summary>Details</summary>
Motivation: 寻找语言模型中人类可理解的电路是机制可解释性领域的核心目标，当前需要更好的方法来发现和理解这些电路。

Method: 训练权重稀疏的模型（大部分权重为零），使每个神经元只有少量连接，然后通过剪枝来分离特定任务对应的电路部分。

Result: 获得的电路通常包含对应自然概念的神经元和残差通道，它们之间有少量可直接解释的连接。稀疏化在性能和可解释性之间进行权衡，模型规模扩展能改善这一权衡。

Conclusion: 该方法产生了前所未有的可理解电路，但将稀疏模型扩展到数千万非零参数以上同时保持可解释性仍具挑战性，该方法也可用于解释现有密集模型。

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [273] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次系统分析了优化超参数（学习率、权重衰减、动量、批大小）对迁移攻击和查询攻击鲁棒性的影响，发现学习率对两种攻击的鲁棒性存在相反影响，并探索了联合增强两种鲁棒性的超参数设计空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对优化超参数如何影响对抗攻击鲁棒性的系统分析，特别是在不同部署场景下对迁移攻击和查询攻击的差异化影响。

Method: 通过理论分析和实验验证，研究了集中训练、集成学习和分布式训练等多种实际部署场景中优化超参数对鲁棒性的影响。

Result: 发现学习率对两种攻击的鲁棒性存在显著差异：降低学习率可提升迁移攻击鲁棒性达64%，而提高学习率可提升查询攻击鲁棒性达28%。分布式模型通过超参数调优能最有效地同时缓解两种攻击。

Conclusion: 优化超参数对对抗鲁棒性有重要影响，且对不同类型的攻击存在相反效应。分布式训练模型通过适当的超参数调优能实现最佳的综合鲁棒性平衡。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [274] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: 提出了一种基于指数族学习的新型科学数据压缩和超分辨率框架，能够在压缩数据的同时保留物理量不确定性并支持压缩比与重建保真度的灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟、观测和大规模实验产生的数据量经常超过存储、处理和分析的限制，需要开发能够在压缩数据的同时保留基本物理特征和关键物理量的数据缩减方法，特别是在需要从压缩表示中恢复数据的场景中。

Method: 基于近期指数族学习的进展，构建了一个科学数据压缩和超分辨率框架，该方法能够保留和量化物理量中的不确定性。

Result: 该方法支持压缩比与重建保真度之间的灵活权衡，为科学数据管理提供了有效的解决方案。

Conclusion: 该框架为科学数据压缩和恢复提供了新的方法，特别适用于检查点和重启等需要从压缩数据中恢复物理特征的应用场景。

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [275] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: 提出跨学习框架，通过联合估计多个相关任务的确定性参数来解决数据稀缺问题，实现从数据丰富任务向数据稀缺任务的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 当数据有限时，学习模型难以泛化到未见过的案例。需要克服数据稀缺问题，特别是在参数推断对有限数据至关重要的场景中。

Method: 将联合估计构建为约束优化问题，约束条件控制不同模型参数之间的相似性，允许参数在不同任务间存在差异，同时结合多个数据源的信息。

Result: 在具有高斯数据的受控框架中提供理论保证，并在图像分类和传染病传播等真实数据应用中展示了跨学习方法的有效性。

Conclusion: 跨学习框架能够实现从数据丰富任务向数据稀缺任务的知识迁移，获得更准确可靠的参数估计，为有限数据下的参数推断提供解决方案。

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [276] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: 提出SSRGNet模型，结合图神经网络和语言模型，利用蛋白质3D结构数据预测二级结构，在NetSurfP-2.0数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用未标记的氨基酸序列，未能充分利用可获得的蛋白质3D结构数据，而3D结构是决定蛋白质功能的关键因素。

Method: 使用蛋白质残基图，结合图神经网络(GCN、R-GCN)和预训练蛋白质语言模型，通过消息传递机制捕获蛋白质结构的几何特征和空间信息。

Result: 在NetSurfP-2.0数据集上的实验表明，SSRGNet模型在f1分数上超越了基线方法。

Conclusion: 通过有效结合3D结构信息和序列信息，提出的方法能够更好地预测蛋白质二级结构，为理解蛋白质功能和关系提供重要见解。

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [277] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: 本文提出了一种新的校准度量方法CDL_K，通过限制后处理函数的结构类别K来解决原始CDL的不可计算性问题，并建立了CDL_K在信息论和计算复杂度方面的理论框架。


<details>
  <summary>Details</summary>
Motivation: 原始校准决策损失(CDL)在离线设置中难以近似计算，这限制了其实际应用。为了解决这个问题，需要找到一种既保持理论严谨性又具有计算可行性的替代方法。

Method: 通过将后处理函数限制在结构化函数族K中，定义相对校准决策损失CDL_K。研究CDL_K在不同函数族K下的信息论和计算复杂度特性，并开发相应的算法技术。

Result: 建立了CDL_K的理论框架，证明了在某些自然函数类K下CDL_K是可计算的，并为机器学习中广泛使用的重新校准程序提供了严格的理论保证。

Conclusion: 通过限制后处理函数的结构，CDL_K在保持理论严谨性的同时解决了计算可行性问题，为校准理论在决策制定中的应用提供了新的工具和理论基础。

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [278] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine是一个非参数、基于核的框架，用于从时间序列数据中推断状态依赖的内在噪声，通过三阶段算法结合可解析求解的子问题和结构化核架构来捕捉噪声驱动的波动和方差变化。


<details>
  <summary>Details</summary>
Motivation: 理解复杂系统的动态行为需要纳入随机效应，但参数模型在没有强先验知识时难以准确估计内在噪声，特别是在生物学和生态学领域。

Method: 三阶段回归算法，结合可解析求解的子问题和结构化核架构，能够捕捉突然的噪声驱动波动和平滑的状态依赖方差变化。

Result: 在生物和生态系统上的验证表明，Trine能够发现隐藏的动态而不依赖预定义的参数假设，在多个基准问题上达到与oracle相当的性能。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径，特别适用于基因调控网络和信号通路等生物系统。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [279] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: ST-ProC是一个新颖的图原型多目标半监督学习框架，用于解决GPS轨迹出行模式识别中的标签稀缺问题，相比现有方法性能提升21.5%。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹出行模式识别对城市智能至关重要，但标注成本高导致标签严重稀缺。现有的半监督学习方法存在灾难性确认偏差问题，且忽略了数据的内在流形结构。

Method: 提出图原型多目标半监督学习框架，结合图正则化、原型锚定和新的边界感知伪标签策略来主动拒绝噪声，并通过对比学习和师生一致性损失来确保高质量表示和鲁棒优化。

Result: ST-ProC显著优于所有基线方法，在真实世界稀疏标签设置中表现出色，相比FixMatch等最先进方法性能提升21.5%。

Conclusion: ST-ProC框架通过有效利用数据流形和噪声抑制策略，成功解决了出行模式识别中的标签稀缺问题，为半监督学习提供了新的解决方案。

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [280] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 通过自编码器和稳定性分析在KIRC癌症中发现了一个罕见但可复现的分子亚型（6.85%患者），而泛癌分析主要受组织来源主导。


<details>
  <summary>Details</summary>
Motivation: 在高维RNA-seq数据上进行无监督学习可以发现超越标准标签的分子亚型，特别是寻找罕见但可复现的基因组亚型。

Method: 结合自编码器表示与聚类和稳定性分析：选择高变异基因，训练前馈自编码器（128维潜在空间），运行k-means聚类（k=2-10），使用预定义发现规则（罕见<10%且Jaccard>=0.60稳定性）识别亚型。

Result: 泛癌分析显示聚类与组织来源高度一致（Cramer's V=0.887）；在KIRC中发现k=5聚类方案，其中罕见簇C0（6.85%）高度稳定（Jaccard=0.787），通过差异表达分析识别出连贯标记物。

Conclusion: 泛癌聚类主要受组织来源主导，而基于稳定性的单癌种方法能够揭示罕见、可复现的KIRC亚型。

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [281] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文研究可解释AI在极端事件预测中的作用，以野火预测为例，通过SHAP方法提升AI模型的可解释性和实用性，弥合预测精度与可操作洞察之间的差距。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了极端事件的频率和严重性，但AI模型的黑箱特性限制了其在真实决策中的应用，需要提升模型的可信度、可解释性和操作准备度。

Method: 使用野火预测作为案例研究，评估多种AI模型，并采用SHAP方法来揭示关键特征、决策路径和模型行为中的潜在偏差。

Result: 分析表明XAI不仅澄清了模型推理，还支持领域专家和响应团队的关键决策，通过可视化增强了特征重要性和时空模式的可解释性。

Conclusion: AI系统不仅需要准确，还需要可解释、易获取和可信赖，这对于灾害准备、风险缓解和气候韧性规划至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [282] [Evolution of A4L: A Data Architecture for AI-Augmented Learning](https://arxiv.org/abs/2511.11877)
*Ploy Thajchayapong,Suzanne Carbonaro,Tim Couper,Blaine Helmick,Spencer Rugaber,Ashok Goel*

Main category: cs.ET

TL;DR: 本文介绍了A4L2.0架构，这是一个利用开放标准实现教育数据系统互操作性的AI增强学习架构，支持个性化在线教育分析。


<details>
  <summary>Details</summary>
Motivation: 随着AI在教育生态系统中深入集成，需要可扩展的解决方案来支持个性化学习，通过连续数据流推动学习者成功。

Method: 开发A4L2.0架构，利用1EdTech联盟的开放标准（Edu-API、Caliper Analytics、LTI）实现学生信息系统、学习管理系统和AI工具之间的安全互操作数据集成，包含数据摄取、预处理、组织、分析和可视化模块。

Result: A4L1.0早期实施展示了架构如何支持中观和微观学习分析，这些试点研究为A4L2.0设计提供了信息。

Conclusion: A4L2.0架构通过开放标准实现了跨教育数据系统的互操作性，为个性化在线教育提供了可扩展的分析和增强学习解决方案。

Abstract: As artificial intelligence (AI) becomes more deeply integrated into educational ecosystems, the demand for scalable solutions that enable personalized learning continues to grow. These architectures must support continuous data flows that power personalized learning and access to meaningful insights to advance learner success at scale. At the National AI Institute for Adult Learning and Online Education (AI-ALOE), we have developed an Architecture for AI-Augmented Learning (A4L) to support analysis and personalization of online education for adult learners. A4L1.0, an early implementation by Georgia Tech's Design Intelligence Laboratory, demonstrated how the architecture supports analysis of meso- and micro-learning by integrating data from Learning Management Systems (LMS) and AI tools. These pilot studies informed the design of A4L2.0. In this chapter, we describe A4L2.0 that leverages 1EdTech Consortium's open standards such as Edu-API, Caliper Analytics, and Learning Tools Interoperability (LTI) to enable secure, interoperable data integration across data systems like Student Information Systems (SIS), LMS, and AI tools. The A4L2.0 data pipeline includes modules for data ingestion, preprocessing, organization, analytics, and visualization.

</details>


### [283] [QPU Micro-Kernels for Stencil Computation](https://arxiv.org/abs/2511.12617)
*Stefano Markidis,Luca Pennati,Marco Pasquale,Gilbert Netzer,Ivy Peng*

Main category: cs.ET

TL;DR: 提出了QPU微内核：浅层量子电路，用于执行模板节点更新并通过重复测量返回蒙特卡洛估计，将QPU作为采样加速器来求解偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 传统量子PDE求解器需要将整个时空问题编码到一个深层电路中，而微内核方法保持经典时间循环，仅将局部更新卸载到量子处理器，使资源占用固定且与全局网格无关。

Method: 设计了两种微内核实现：伯努利微内核通过将值编码为单量子比特概率来处理凸和模板；分支微内核准备模板分支的选择器并对单个读出量子比特应用寻址旋转。

Result: 在无噪声量子电路模拟器上，随着样本数量增加，精度提高；在IBM Brisbane量子计算机上，伯努利实现比分支实现具有更低的误差，且QPU微内核执行主导了总执行时间。

Conclusion: QPU微内核方法通过将量子计算限制在浅层局部电路，实现了高效的PDE求解，适合与经典计算协同工作，并可通过批处理和电路内融合来分摊开销。

Abstract: We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.

</details>


### [284] [Segmented Exponent Alignment and Dynamic Wordline Activation for Floating-Point Analog CIM Macros](https://arxiv.org/abs/2511.12624)
*Weiping Yang,Shilin Zhou,Hui Xu,Jiawei Xue,Changlin Chen*

Main category: cs.ET

TL;DR: 提出了SEA和DWA策略来优化FP-MAC操作，通过分段指数对齐和动态字线激活减少延迟和功耗


<details>
  <summary>Details</summary>
Motivation: 传统FP-MAC操作中的指数比较和尾数对齐导致硬件开销大，位串行输入方法引入延迟，阻碍高效实现

Method: SEA策略利用输入指数通常集中在零附近或狭窄范围内的观察，通过分段指数空间和相应尾数对齐来消除最大指数检测需求；DWA策略基于SEA定义的指数段来激活字线

Result: 与传统的比较树最大指数对齐方法相比，在VGG16-CIFAR10基准测试中节省63.8%功耗，延迟降低40.87%

Conclusion: SEA和DWA策略能有效降低FP-MAC操作的延迟和功耗，提高计算效率

Abstract: With the rise of compute-in-memory (CIM) accelerators, floating-point multiply-and-accumulate (FP-MAC) operations have gained extensive attention for their higher accuracy over integer MACs in neural networks. However, the hardware overhead caused by exponent comparison and mantissa alignment, along with the delay introduced by bit-serial input methods, remains a hinder to implement FP-MAC efficiently. In view of this, we propose Segmented Exponent Alignment (SEA) and Dynamic Wordline Activation (DWA) strategies. SEA exploits the observation that input exponents are often clustered around zero or within a narrow range. By segmenting the exponent space and aligning mantissas accordingly, SEA eliminates the need for maximum exponent detection and reduces input mantissa shifting, and thus reduces the processing latency. DWA further reduces latency and maintains accuracy by activating wordlines based on the exponent segments defined by SEA. Simulation results demonstrate that, when compared with conventional comparison tree based maximum exponent alignment method, our approach saves 63.8\% power consumption, and achieves a 40.87\% delay reduction on the VGG16-CIFAR10 benchmark.

</details>


### [285] [PolicyBot - Reliable Question Answering over Policy Documents](https://arxiv.org/abs/2511.13489)
*Gautam Nagarajan,Omir Kumar,Sudarsun Santhiappan*

Main category: cs.ET

TL;DR: PolicyBot是一个基于检索增强生成(RAG)的系统，专门用于回答政策文档相关的用户查询，强调透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 政策文档通常冗长复杂，公民难以找到和理解相关信息，需要工具来帮助公民更好地理解和访问政策内容。

Method: 结合领域特定的语义分块、多语言密集嵌入、多阶段检索与重排序、源感知生成等技术，使用开源工具构建端到端管道。

Result: 开发了具有引用追踪功能的系统，减少了幻觉生成并提高了用户信任度，评估了不同的检索和生成配置。

Conclusion: 这项工作强调了在治理相关环境中部署可信赖RAG系统的设计考虑、实际挑战和经验教训，系统可轻松适应其他需要文档基础问答的领域。

Abstract: All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [286] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 提出了一种基于扩展卡尔曼滤波的自适应闭环测试方法，用于高效测试高分辨率SAR ADC的线性度，显著减少测试时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法（如直方图法、正弦波测试等）需要密集数据采集和离线后处理，导致测试时间长、复杂度高，不适用于生产环境。

Method: 使用扩展卡尔曼滤波器实时迭代优化行为模型，动态选择测量点以最大化信息增益，直接估计决定INL行为的电容失配参数。

Result: 实验结果显示总测试时间和计算开销大幅减少，适合集成到生产环境中。

Conclusion: 该方法通过自适应闭环测试策略，有效解决了传统ADC线性度测试方法的效率问题，具有实际应用价值。

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [287] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 增强版UGLMS方法通过秩-1 EKF更新、协方差膨胀策略、载波多项式扩展和基于迹的终止条件，显著提升了SAR ADC线性度测试速度，16位ADC仅需36ms即可重建完整INL/DNL特性。


<details>
  <summary>Details</summary>
Motivation: 传统SAR ADC测试需要全范围扫描和离线后处理，耗时且效率低。需要开发实时、生产就绪的测试方法，在保持精度的同时大幅缩短测试时间。

Method: 1. 秩-1 EKF更新替代矩阵求逆；2. 测量对齐的协方差膨胀策略加速收敛；3. 静态失配模型扩展低阶载波多项式；4. 基于迹的终止条件自适应调整测试长度。

Result: 16位ADC仅需36ms重建完整INL/DNL，18位ADC低于70ms（多项式扩展后120ms）。相比原方法，16位ADC达到相同精度速度快8倍。

Conclusion: 增强版UGLMS实现了实时、生产就绪的SAR ADC线性度测试，显著提升了测试效率。

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [288] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: 提出一种基于时空相关性的轻量级自适应重用策略，通过重用空间或时间相关token的部分注意力分数来加速视频扩散变换器中的自注意力计算，实现85%的计算节省且视频质量损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型主要基于视频扩散变换器(vDiT)，但由于自注意力机制导致推理延迟严重。先前研究主要关注减少自注意力中的冗余计算，但忽视了视频流中固有的时空相关性。

Method: 利用潜在空间中的时空相关性，提出轻量级自适应重用策略，通过重用空间或时间相关token在单个通道上的部分注意力分数来近似注意力计算。

Result: 在4个vDiT模型上相比最先进技术实现了显著更高的计算节省(85%)，同时保持了几乎相同的视频质量(VBench上损失<0.06%)。

Conclusion: 该方法通过利用视频中的时空相关性，有效加速了vDiT中的自注意力计算，在保持视频质量的同时大幅减少了计算开销。

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [289] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: 提出了一种用于Transformer注意力计算的数字存内计算宏，通过重新设计注意力分数计算流程和使用位串行移位加法，实现了高能效的Transformer处理。


<details>
  <summary>Details</summary>
Motivation: 传统存内计算技术难以处理Transformer中的动态矩阵乘法，且物理乘法器成本高昂，需要设计更适合Transformer注意力计算的存内计算架构。

Method: 将注意力分数计算基于组合QK权重矩阵重新表述，将二项式矩阵乘法分解为4组位串行移位和加法操作，采用零值位跳过、数据驱动字线激活等技术优化能效。

Result: 在65nm工艺下实现，面积仅0.35mm²，峰值性能42.27 GOPS，功耗1.24mW，能效达34.1 TOPS/W，面积效率120.77 GOPS/mm²，比CPU和GPU分别能效提升25倍和13倍。

Conclusion: 该设计在能效和面积效率上相比其他Transformer存内计算方案有显著优势，展示了在边缘智能应用中的潜力。

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [290] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: Sangam是一种基于CXL连接的PIM-chiplet内存模块，通过将逻辑和内存分离到不同工艺节点的chiplet中，解决了现有内存计算方案的容量和处理能力限制，显著加速大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型变得越来越数据密集，推理过程特别是解码阶段受限于内存带宽，而现有的内存计算方案面临内存容量减少和处理能力有限的问题。

Method: 提出chiplet架构的内存模块，将逻辑和DRAM分离到不同工艺节点的chiplet中，通过中介层连接，逻辑chiplet包含脉动阵列和SRAM缓冲器来加速内存密集型GEMM运算。

Result: 相比H100 GPU，在LLaMA 2-7B、Mistral-7B和LLaMA 3-70B上，端到端查询延迟分别提升3.93、4.22、2.82倍，解码吞吐量分别提升10.3、9.5、6.36倍，能耗节省一个数量级。

Conclusion: Sangam架构有效解决了现有PIM方案的局限性，为大语言模型推理提供了高性能、高能效的解决方案，可作为GPU的替代或协同执行方案。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [291] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: SURGE通过利用CXL等灵活互连技术，在CPU接口上动态复用内存和I/O流量，将闲置的I/O带宽资源用于提升内存带宽，从而加速内存密集型工作负载。


<details>
  <summary>Details</summary>
Motivation: 随着服务器CPU核心数不断增加，内存系统面临带宽限制，而传统设计中内存和I/O带宽各自占用约一半引脚资源，导致宝贵的片外带宽资源利用率不足。

Method: 采用软件支持的架构技术SURGE，利用CXL等灵活互连技术在同一处理器接口上动态复用内存和I/O流量，实现带宽资源的灵活调配。

Result: 在带宽受限的服务器上，SURGE增强的架构可以将内存密集型工作负载加速高达1.3倍。

Conclusion: SURGE通过实现内存和I/O带宽的可互换性，有效提升了片外带宽资源的整体利用率，缓解了多核处理器面临的内存带宽瓶颈问题。

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [292] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: FERMI-ML是一种用于TinyML加速的灵活资源高效内存原位SRAM宏，集成了9T XNOR位单元和22T压缩器树累加器，支持可变精度MAC和CAM操作，在65nm工艺下实现1.93 TOPS吞吐量和364 TOPS/W能效。


<details>
  <summary>Details</summary>
Motivation: AIoT设备对低功耗和面积效率的TinyML推理需求日益增长，需要最小化数据移动同时保持高计算效率的内存架构。

Method: 提出9T XNOR基RX9T位单元，将5T存储单元与4T XNOR计算单元集成，支持可变精度MAC和CAM操作；使用22T压缩器树累加器实现对数1-64位MAC计算。

Result: 4KB宏在65nm工艺下以350MHz、0.9V运行，吞吐量达1.93 TOPS，能效364 TOPS/W，在InceptionV4和ResNet-18上保持97.5%以上的结果质量。

Conclusion: FERMI-ML展示了一种紧凑、可重构且能量感知的数字内存原位宏，能够支持混合精度TinyML工作负载。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [293] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: SynapticCore-X是一个针对低成本FPGA优化的模块化神经网络处理架构，集成了RISC-V控制核心和可配置的神经计算单元，提供完全开源的硬件设计。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器依赖重量级IP模块，缺乏开源和可配置的解决方案，限制了学术研究和硬件-软件协同设计的探索。

Method: 采用SystemVerilog微架构设计，集成RV32IMC RISC-V控制核心和可配置神经计算单元，支持融合矩阵、激活和数据移动操作，具有可调并行性、暂存内存深度和DMA突发行为。

Result: 在Zynq-7020上实现100MHz时序收敛，仅消耗6.1% LUTs、32.5% DSPs和21.4% BRAMs，硬件验证确认了寄存器级执行正确性和确定性控制路径行为。

Conclusion: SynapticCore-X证明可以在商用教育FPGA上实现类似NPU的高效能加速，降低了神经微架构研究的入门门槛。

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [294] [Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs](https://arxiv.org/abs/2511.12860)
*Yongjoo Jang,Sangwoo Hwang,Hojin Lee,Sangwoo Jung,Donghun Lee,Wonbo Shim,Jaeha Kung*

Main category: cs.AR

TL;DR: 提出了基于3D NAND闪存的内存处理架构，用于解决大语言模型在传统硬件上的内存和计算瓶颈，实现了与高端GPU相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数数量激增导致内存和计算需求大幅增加，传统硬件DRAM容量有限且GPU成本高昂，需要寻找替代解决方案。

Method: 将单批次token生成卸载到3D NAND闪存PIM设备，重新设计PIM阵列结构采用H-tree网络，开发操作分块和映射方法优化LLM层处理。

Result: 相比四个RTX4090和vLLM实现2.4倍加速，与四个A100性能相当仅有4.9%延迟开销，可在4.98mm²芯片面积内集成。

Conclusion: 3D NAND闪存PIM架构是解决大语言模型内存容量瓶颈的有效方案，具有高存储密度和成本效益优势。

Abstract: The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.

</details>


### [295] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: Neo提出了一种重用和更新的排序算法，利用连续帧间高斯排序的时间冗余性，并设计了针对该算法的硬件加速器，显著提升了3D高斯溅射在资源受限设备上的实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现3D高斯溅射(3DGS)的实时渲染对于提供沉浸式AR/VR体验至关重要，但现有解决方案难以实现高帧率，特别是在高分辨率渲染时。分析发现渲染管道中的排序阶段是主要瓶颈。

Method: 提出了重用和更新排序算法，利用连续帧间高斯排序的时间冗余性，通过跟踪和更新高斯深度排序而不是从头重新排序，显著减少冗余计算和内存带宽压力，并设计了针对该算法的硬件加速器。

Result: 实验结果显示，Neo比最先进的边缘GPU和ASIC解决方案分别实现了10.0倍和5.6倍的吞吐量提升，同时将DRAM流量减少了94.5%和81.3%。

Conclusion: 这些改进使得高质量、低延迟的设备端3D渲染更加实用可行。

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [296] [Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT](https://arxiv.org/abs/2511.13139)
*Zhiteng Chao,Yonghao Wang,Xinyu Zhang,Jiaxin Zhou,Tenghui Hua,Husheng Han,Tianmeng Yang,Jianan Mu,Bei Yu,Rui Zhang,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: VeriBToT是一种专门用于自动Verilog生成的LLM推理范式，通过集成自上而下和设计验证方法，实现中间步骤的自解耦和自验证，构建带有形式化操作的回溯思维树。


<details>
  <summary>Details</summary>
Motivation: 解决在集成电路工程中使用LLM自动生成Verilog时的质量问题，传统CoT方法在控制推理方向和步骤粒度方面与专家RTL设计知识不匹配，需要手动干预。

Method: 提出VeriBToT范式，集成自上而下和设计验证方法，实现自解耦和自验证，构建回溯思维树，使用形式化操作符。

Result: 相比传统CoT范式，该方法提升了Verilog生成质量，同时通过灵活的模块化、层次化和可重用性优化了token成本。

Conclusion: VeriBToT为自动化IC设计工作流程提供了有效的LLM推理解决方案，解决了传统方法在Verilog生成中的局限性。

Abstract: Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.

</details>


### [297] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: COLISEUM项目开发了一种结合气候监测和材料劣化分析的方法论，通过在法国三个文化遗产地收集多模态数据，利用人工智能模型预测气候变化对遗产材料的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化正在影响文化遗产材料，增加的气候变化会缩短古迹寿命。由于风化依赖于多种因素，很难将其进展与气候变化联系起来。需要同时收集气候数据和监测劣化进展来预测风化。

Method: 在法国三个文化遗产地（斯特拉斯堡圣母院、比布拉克特考古遗址、维尔弗朗什滨海圣皮埃尔教堂）建立气候监测系统，使用微气候传感器连续记录参数变化，定期通过化学分析、制图测量和科学成像监测劣化状态，通过计算风化指数构建劣化矩阵。

Result: 建立了完整的气候监测方法学，在三个不同气候和材料的遗址进行了初步诊断和监测，以斯特拉斯堡大教堂为例展示了初步结果。

Conclusion: 该项目开发的方法论能够收集多模态数据并构建风化模型，未来将利用IPCC气候变化情景数据预测遗产材料的未来行为，为文化遗产保护提供科学依据。

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [298] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: T-SAR是首个在CPU上实现可扩展三元LLM推理的框架，通过重新利用SIMD寄存器文件进行动态寄存器内查找表生成，显著提升了边缘平台的推理效率和能效。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展超过了边缘平台CPU的计算和内存能力，现有CPU解决方案依赖内存查找表限制了可扩展性，而FPGA或GPU加速器在边缘场景中不实用。

Method: 通过重新利用SIMD寄存器文件进行动态寄存器内查找表生成，只需最小的硬件修改，消除了内存瓶颈并最大化数据级并行性。

Result: 在GEMM延迟和GEMV吞吐量上分别实现了5.6-24.5倍和1.1-86.2倍的提升，SIMD单元仅增加3.2%功耗和1.4%面积开销，能效达到NVIDIA Jetson AGX Orin的2.5-4.9倍。

Conclusion: T-SAR为边缘平台上高效LLM推理提供了一种实用方法，显著提升了计算效率和能效。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [299] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL是一个针对可变形注意力机制的硬件加速器，通过距离排序查询和预取技术将不规则内存访问转换为缓存友好的单次工作，显著提升吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 可变形变换器在检测任务中表现出色，但由于不规则内存访问和低算术强度，难以在硬件上高效运行。

Method: 采用距离排序查询(DOOQ)技术按空间邻近度排序查询，结合区域预取形成调度感知预取循环，融合MSDeformAttn引擎在单次传递中执行插值、Softmax、聚合和最终投影。

Result: 相比RTX 4090实现7.29倍吞吐量提升和47.3倍能效提升，相比现有加速器实现3.26-9.82倍吞吐量提升和2.01-6.07倍能效提升，混合精度量化下精度损失≤0.9 AP。

Conclusion: QUILL通过将稀疏性转换为局部性，再将局部性转换为利用率，实现了端到端的持续加速。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [300] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: ACE-GNN是一个面向动态边缘环境的自适应GNN协同推理框架，通过系统级抽象和两种新颖的预测方法实现性能感知，支持运行时方案优化，并在PP和DP之间自适应调度，显著提升系统性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线模型分割和流水线并行的GNN协同推理方法在动态边缘环境中性能受网络波动和多设备访问影响严重，需要解决环境动态性带来的性能不稳定问题。

Method: 1) 系统级抽象和两种预测方法实现性能感知；2) 运行时方案优化；3) 在流水线并行和数据并行之间自适应调度；4) 高效批处理推理策略和专用通信中间件。

Result: 在多种应用和边缘设置下的广泛实验表明，ACE-GNN相比GCoDE实现了最高12.7倍加速和82.3%能耗节省，相比Fograph实现了11.7倍更好的能效。

Conclusion: ACE-GNN是首个针对动态边缘环境的自适应GNN协同推理框架，通过性能感知和自适应调度机制，显著提升了边缘GNN应用的性能和稳定性。

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [301] [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)
*Van-Vi Vo,Tien-Dung Nguyen,Duc-Tai Le,Hyunseung Choo*

Main category: cs.DC

TL;DR: 提出基于分布式Q学习的最短路径树构建框架，用于物联网传感器网络的高效路由，相比传统集中式算法能减少通信开销并适应拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 传统集中式路由算法（如Dijkstra）在动态分布式物联网环境中计算密集且不适用，需要一种能够自主学习和适应的高效路由方法。

Method: 使用分布式Q学习框架，传感器节点基于本地信息独立学习最优下一跳决策，状态定义为节点位置和路由历史，奖励函数激励向汇聚节点前进并惩罚低效路径。

Result: 在100-500节点网络模拟中，路由准确率接近最优（300节点以上超过99%），小网络中1-2跳的微小偏差对性能影响可忽略，相比集中式和泛洪方法减少了通信开销。

Conclusion: Q学习为资源受限的物联网网络提供了自主、鲁棒的路由解决方案，是传统协议的可扩展替代方案。

Abstract: Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.

</details>


### [302] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: 本文首次实证研究了异构AI加速器上机器学习模型的差异，发现新兴厂商（Mac、华为）的平台比NVIDIA支持更少的算子，产生更多输出差异，且更容易在模型编译时失败。


<details>
  <summary>Details</summary>
Motivation: 随着AI加速器市场多样化，新兴厂商提供成本效益高的替代方案，但缺乏对其兼容性和性能一致性的实证研究。

Method: 使用自动化流水线，基于4000个真实模型合成超过10万个变体模型，并在5种企业级AI加速器上执行测试。

Result: Mac和华为平台比NVIDIA少支持至少17%的算子，输出差异率超过5%，存在算子实现、异常数值处理和指令调度差异，编译加速失败率更高。

Conclusion: 在日益多样化的硬件生态系统中，实现一致的机器学习行为面临重大挑战，需要关注算子兼容性和实现一致性。

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [303] [Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review](https://arxiv.org/abs/2511.11603)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DC

TL;DR: 对10种AI/ML算法在云资源分配中的比较分析，发现混合架构方法在性能、成本和能效方面优于传统方法，边缘计算环境部署准备度最高


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法无法满足现代云基础设施的多目标优化需求，需要更先进的AI/ML解决方案来处理复杂动态工作负载

Method: 系统评估了10种算法，分为四类：深度强化学习、神经网络架构、增强型传统机器学习、多智能体系统

Result: 在多个指标上显著优于传统方法，包括减少makespan、优化成本和提升能效，混合架构方法表现最佳

Conclusion: 混合AI/ML架构在复杂动态计算环境中表现最优，为学术研究和行业实践提供了关键见解，边缘计算环境具有最高部署准备度

Abstract: Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.

</details>


### [304] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: 本文介绍了用于PACE 2025竞赛最小支配集启发式赛道的twin_width_fmi求解器，包括贪心算法、模拟退火局部搜索和最终提交的hedom5算法。


<details>
  <summary>Details</summary>
Motivation: 开发高效的启发式算法来解决最小支配集问题，参与PACE 2025竞赛的启发式赛道。

Method: 1. 贪心算法greedy-ln作为基线；2. 模拟退火局部搜索；3. hedom5算法：包含图简化、基于增益的懒更新贪心构造、反向修剪和1-交换局部改进。

Result: 开发了hedom5算法作为最终提交方案，该算法结合了多种技术来获得高质量的最小支配集解。

Conclusion: hedom5算法通过迭代式贪心构造、积极修剪和局部改进的组合，能够有效求解最小支配集问题，并在竞赛中表现良好。

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [305] [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)
*Mingyu Sung,Suhwan Im,Daeho Bang,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.DC

TL;DR: SLICER是一个无需重新训练的框架，通过压缩中间特征来减少边缘-云模型分割中的通信和服务器负载，在保持任务质量的同时显著降低上行流量和服务器GPU时间。


<details>
  <summary>Details</summary>
Motivation: 传统边缘-云模型分割方案使用固定的浅层分割点，导致边缘计算资源利用不足，服务器端延迟和能耗集中。在自回归LLM推理中，每个token的前向传递会重复生成庞大的中间特征，加剧了这一问题。

Method: SLICER结合三种技术：(1)非对称top-K过滤稀疏化低幅度激活；(2)幅度分割将剩余非零值分组为等基数块；(3)自适应比特量化在失真预算下选择每块比特宽度。

Result: 在标准视觉和LLM任务中，SLICER将上行流量减少高达10倍，服务器GPU时间减少高达4.4倍，同时任务质量保持在基线水平的0-3个百分点内。

Conclusion: SLICER提供了一种即插即用的路径，无需重新训练或架构更改即可实现可扩展、低延迟的分布式推理，在多设备设置和AR LLM中通过将有意义计算转移到边缘来扩展性能。

Abstract: Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.

</details>


### [306] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: 评估21个公开LLM在HPC工作负载调度问题上的表现，发现3个模型能精确重现最优解，12个接近最优，6个表现次优，揭示了LLM在组合优化中的当前能力边界。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在结构化、基于约束的优化问题中的推理能力，特别是从自然语言描述中解决组合优化问题的表现。

Method: 使用相同的文本描述（系统节点、任务需求、调度约束）测试21个LLM，要求分配任务到节点、计算总完成时间并解释推理过程，以手动推导的9小时20秒最优解为基准。

Result: 3个模型精确重现最优解，12个在2分钟内接近最优，6个产生次优调度（算术或依赖错误）；所有模型生成可行任务映射，但仅约一半严格遵循约束；19个模型生成部分可执行验证代码，18个提供连贯推理过程。

Conclusion: 领先的LLM能从自然语言直接重构最优调度，但大多数仍难以处理精确计时、数据传输算术和依赖执行；LLM更适合作为优化和决策支持任务的可解释辅助工具而非自主求解器。

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [307] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: FPGA作为可重构平台，通过将AI算法直接映射到设备逻辑中，为需要确定性性能和深度定制的工作负载提供了比GPU更优的解决方案，具有低延迟、高能效和硬件控制能力。


<details>
  <summary>Details</summary>
Motivation: GPU在AI加速中占主导地位，但固定架构在延迟、能效和硬件细粒度控制方面存在局限，需要可重构平台来满足特定模型需求。

Method: 利用FPGA的可重构特性，将AI算法直接映射到设备逻辑，实现卷积、注意力机制和后处理的并行流水线，支持确定性时序和低功耗运行。

Result: FPGA能够实现近传感器推理，减少延迟和带宽需求，提高隐私保护，并释放数据中心GPU资源，支持硬件-算法协同设计。

Conclusion: FPGA作为可重构平台，通过部分重配置和从AI框架的编译流程，缩短了从原型到部署的路径，为AI工作负载提供了战略性的替代方案。

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [308] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: AnchorTP是一个用于LLM推理服务的弹性张量并行框架，能够在GPU故障时快速恢复服务，通过不等宽分区和KV缓存保护机制，显著减少停机时间。


<details>
  <summary>Details</summary>
Motivation: 多GPU张量并行推理服务对单GPU故障很脆弱，需要高可用性和低延迟恢复机制。

Method: 提出弹性张量并行(ETP)支持不等宽分区和MoE兼容，使用守护进程保护模型参数和KV缓存，采用带宽感知规划器和CMM算法最小化数据迁移。

Result: 在典型故障场景下，AnchorTP将首次成功时间(TFS)减少高达11倍，峰值时间(TTP)减少高达59%。

Conclusion: AnchorTP能够在不改变服务接口的情况下，通过最小化数据移动快速恢复LLM推理服务。

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [309] [DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack](https://arxiv.org/abs/2511.11619)
*Yuanjie Liu,Wenpeng Xing,Ye Zhou,Gaowei Chang,Changting Lin,Meng Han*

Main category: cs.DC

TL;DR: 提出了DIAP协议，一个去中心化的星际代理协议，通过结合IPFS/IPNS、零知识证明和混合P2P网络，实现自主代理的持久化、可验证和无信任互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏完全去中心化、可验证且保护隐私的自主代理通信协议，现有系统要么依赖中心化中介，要么缺乏去中心化身份解析机制，限制了持久性和跨网络互操作性。

Method: 使用Rust SDK集成Noir（零知识证明）、DID-Key、IPFS和混合P2P堆栈（Libp2p GossipSub用于发现，Iroh用于高性能数据交换），引入零依赖ZKP部署模型和预编译Noir电路。

Result: 建立了实用的高性能基础，支持下一代自主代理生态系统和代理到代理经济，实现即时、可验证且保护隐私的身份证明。

Conclusion: DIAP为自主代理通信提供了可行的解决方案，解决了去中心化环境中的身份验证和隐私保护问题，为A2A经济奠定了技术基础。

Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.

</details>


### [310] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: AIvailable是一个低成本的LLM即服务平台，能够在异构GPU节点上高效运行大型语言模型，专注于充分利用每个节点的VRAM资源，支持学术实验室和资源受限组织。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统大多假设同质化、资源丰富的硬件环境，这在学术或资源受限环境中不现实。需要为异构和遗留GPU节点提供可扩展、高性能的推理解决方案。

Method: 采用软件定义方法，在NVIDIA和AMD等异构GPU节点上运行LLM，完全GPU加速推理，无CPU回退。包含四个组件：客户端接口、服务前端、SDAI控制器和服务后端，提供动态VRAM感知的模型分配和重新分配。

Result: 实现了高效的资源利用和针对故障或工作负载波动的弹性，支持多样化的开源LLM，通过重新利用遗留GPU来民主化生成式AI。

Conclusion: AIvailable为学术实验室、私营公司等资源受限组织提供了一个低成本、高可用的LLM服务平台，能够有效利用异构硬件资源，推动生成式AI的普及。

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [311] [Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications](https://arxiv.org/abs/2511.11640)
*Sed Centeno,Christopher Sprague,Arnab A Purkayastha,Ray Simar,Neeraj Magotra*

Main category: cs.DC

TL;DR: 在MNIST数据集上实现基于OpenMP的推测性反向传播，通过重叠前向和反向传播步骤加速神经网络训练，在阈值0.25时获得24%的最大执行时间加速，准确率与基线相差3-4%。


<details>
  <summary>Details</summary>
Motivation: 推测性反向传播通过在前向和反向传播步骤重叠时进行推测性权重更新，从而加速神经网络训练过程，同时不显著影响模型准确率。

Method: 使用OpenMP并行编程平台在MNIST数据集上实现推测性反向传播，利用多线程能力同时执行前向和推测性反向传播步骤，并计划在FPGA上进行硬件加速。

Result: 实验结果显示，在阈值0.25时获得24%的最大执行时间加速，单个步骤执行时间最大加速35%，准确率与基线相差3-4%。

Conclusion: 推测性反向传播通过重叠前向和反向传播有效加速神经网络训练，在保持可接受准确率损失的同时显著提升训练速度。

Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.

</details>


### [312] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: 评估5种小型语言模型在边缘设备上的能效表现，发现Jetson Orin Nano GPU配置能效最高，Llama 3.2在准确性和能效间平衡最佳，TinyLlama适合低功耗环境。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署小型语言模型具有低延迟和网络独立性优势，但受限于计算资源和能耗预算，需要评估不同模型在边缘设备上的能效表现。

Method: 在Raspberry Pi 5、Jetson Nano和Jetson Orin Nano（CPU和GPU配置）上评估5种代表性SLM：Llama 3.2、Phi-3 Mini、TinyLlama和Gemma 2的功率效率。

Result: Jetson Orin Nano GPU加速实现最高能效比，显著优于CPU配置；Llama 3.2在准确性和能效间平衡最佳；TinyLlama适合低功耗环境但准确性较低；Phi-3 Mini能耗最高但准确性高。

Conclusion: GPU加速、内存带宽和模型架构是优化推理能效的关键因素，为AI、智能系统和移动自组织平台在能源受限环境中平衡准确性、推理延迟和能效提供实用指导。

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [313] [Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies](https://arxiv.org/abs/2511.11628)
*Xinbo Wang,Shian Jia,Ziyang Huang,Jing Cao,Mingli Song*

Main category: cs.DC

TL;DR: ASA是一个动态调度框架，通过机器学习模型识别工作负载模式，从专家调度器组合中选择最优策略，在86.4%的测试场景中优于Linux默认调度器。


<details>
  <summary>Details</summary>
Motivation: 现代操作系统调度器采用单一静态策略，无法适应异构硬件和多样化工作负载的需求，导致公平性、吞吐量和延迟方面的显著妥协。

Method: 提出自适应调度代理(ASA)：1)离线训练硬件无关的机器学习模型识别抽象工作负载模式；2)运行时使用时间加权概率投票算法识别工作负载，通过预配置映射表切换到最优调度器。

Result: 在基于用户体验指标的新基准测试中，ASA在86.4%的场景中优于Linux默认调度器(EEVDF)，且在78.6%的场景中排名前三。

Conclusion: ASA验证了动态选择专家调度器的方法是实现更智能、自适应和响应式操作系统调度器的可行路径。

Abstract: Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This "one-policy-fits-all" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.
  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable "expert" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.
  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.

</details>


### [314] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: HeteroSTA是首个CPU-GPU异构时序分析引擎，支持多种延迟计算模型、行业格式和端到端GPU加速，提供零开销的扁平化异构API。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统时序分析工具在性能和灵活性方面的限制，开发一个支持多种延迟计算模型、行业标准格式和GPU加速的异构时序分析引擎。

Method: 采用CPU-GPU异构架构，实现端到端GPU加速的图基和路径基时序查询，提供零开销的扁平化异构API，支持多种延迟计算模型和行业格式。

Result: HeteroSTA在独立工具、时序驱动DREAMPlace 4.0集成和时序驱动全局布线集成等用例中均表现出显著的运行速度提升和可比的质量。

Conclusion: HeteroSTA是首个公开可用的CPU-GPU异构时序分析引擎，为学术和工业应用提供了高效的时序分析解决方案。

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [315] [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)
*Mingyu Sung,Suhwan Im,Vikas Palakonda,Jae-Mo Kang*

Main category: cs.DC

TL;DR: 提出了一种轻量级压缩框架，利用rANS编码结合非对称整数量化和稀疏张量表示，显著减少分割计算中的传输开销，同时保持接近基准的模型精度。


<details>
  <summary>Details</summary>
Motivation: 分割计算在资源受限的边缘设备和云服务器之间分配深度神经网络推理，但在传输中间特征时面临显著的通信瓶颈。

Method: 结合非对称整数量化与稀疏表示技术，无需复杂概率建模或网络修改；包括分布无关的压缩流水线、优化张量重塑维度的近似理论模型，以及GPU加速实现。

Result: 在多种神经网络架构和数据集上保持接近基准精度，在CIFAR100和ImageNet基准测试中验证了有效性，并在NLP任务中展示了广泛适用性。

Conclusion: 该方法解决了在带宽受限环境中部署复杂AI系统的根本瓶颈，且不损害模型性能。

Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.

</details>


### [316] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: OSGym是一个超可扩展的分布式数据引擎，用于在多样化计算机任务中训练智能代理，能够以学术可负担的成本扩展到上千个操作系统副本。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限环境下运行多个操作系统副本进行智能代理训练时的可扩展性和成本问题，为计算机代理研究提供经济可行的平台。

Method: 采用分布式架构并行化上千个操作系统实例，支持多种任务类型（工具使用、浏览器交互、软件工程、办公应用）和训练算法，实现高效的数据收集和模型训练。

Result: 系统每分钟可生成1420个多轮轨迹，每个操作系统副本每日成本仅0.2-0.3美元，训练出的模型性能优于现有最先进基线。

Conclusion: OSGym通过其可扩展性、通用性和经济性，为未来智能代理研究的规模化和普适性发展提供了重要推动力。

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [317] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: Co-PLMs是一个新颖的协同调优框架，通过结构无关的相互学习实现异构语言模型间的知识交换，使用蒸馏代理模型作为桥梁，在保护设备特定领域知识的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的智能应用激增，带宽受限的云服务器难以实时处理大量LLM工作负载且不损害用户数据隐私。云边联盟整合服务器LLM和移动边缘设备SLM成为解决方案，但SLM的跨域部署和架构异构性给模型性能提升带来挑战。

Method: 提出Co-PLMs框架，采用结构无关的相互学习实现异构语言模型知识交换，使用蒸馏代理模型作为服务器LLM和设备SLM之间协同训练的桥梁，同时保护每个设备的领域特定洞察。

Result: 实验结果显示Co-PLMs优于现有最先进方法，在Rouge-L和EM指标上分别平均提升5.38%和4.88%。

Conclusion: Co-PLMs框架有效解决了云边联盟中异构语言模型协同训练的挑战，通过知识交换显著提升了推理性能，为带宽受限环境下的智能应用提供了可行解决方案。

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


### [318] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: 提出Eccentric框架，通过边缘模型到云模型的知识适应，在边缘-云推理系统中实现计算、通信和性能之间的权衡优化


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用快速增长，但边缘设备计算资源有限，依赖云端系统导致计算和通信成本急剧增加，需要在计算、通信和性能之间找到平衡

Method: 基于边缘模型到云模型的知识适应，学习具有不同权衡水平的模型，减少推理时的计算和通信成本

Result: 在分类和物体检测任务上的实证研究证实了该框架的有效性

Conclusion: Eccentric框架是一种适合边缘-云推理系统的新型压缩方法，能够同时降低计算和通信成本

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [319] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 提出一种云系统服务分配策略，避免节点过载并保持系统稳定，同时最小化成本。


<details>
  <summary>Details</summary>
Motivation: 解决云系统中资源分配问题，避免节点过载，维持系统稳定性，并最小化服务迁移成本。

Method: 建立云资源利用的抽象模型，考虑多种资源类型和服务迁移成本；开发原型元启发式负载均衡器；提出新颖的遗传算法，用其他元启发式算法的输出作为种群初始值。

Result: 展示了原型负载均衡器，并呈现和讨论了实验结果。

Conclusion: 提出的策略能有效分配云服务，避免节点过载，保持系统稳定，并以最小成本实现资源优化。

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [320] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: Harli是一个LLM服务系统，通过将参数高效微调任务与LLM解码实例共置来提高GPU利用率，解决了解码实例GPU利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统中解码实例由于内存限制特性和动态工作负载中批处理不足，导致GPU利用率低，计算资源未充分利用。

Method: 使用统一内存分配器进行运行时内存复用、两阶段延迟预测器建模解码延迟、以及QoS保证的吞吐量最大化调度器。

Result: 实验结果显示，Harli相比最先进的服务系统平均提高微调吞吐量46.2%（最高92.0%），同时保持推理解码的严格QoS保证。

Conclusion: Harli通过安全共置计算密集的PEFT任务和内存密集的LLM解码实例，有效提升了GPU利用率，实现了吞吐量显著提升且不损害服务质量。

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [321] [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)
*Jingwei Song,Wanyi Chen,Xinyuan Song,Max,Chris Tong,Gufeng Chen,Tianyi Zhao,Eric Yang,Bill Shi,Lynn Ai*

Main category: cs.DC

TL;DR: DSD是一个去中心化推测解码框架，通过并行验证多个候选token将网络延迟转化为有用计算，在保持准确性的同时实现高达2.59倍的加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在集中式系统中有效，但在网络延迟占主导的去中心化环境中表现不佳，需要专门优化。

Method: 提出去中心化推测解码(DSD)框架，通过并行验证多个候选token，并引入自适应推测验证策略根据token语义重要性调整接受阈值。

Result: 理论减少(N-1)t1(k-1)/k的跨节点通信成本，实践中在HumanEval和GSM8K上分别实现2.56x和2.59x加速，超越Eagle3基线。

Conclusion: DSD将网络停顿转化为吞吐量，无需模型重训练或架构更改即可实现更快的分布式LLM推理。

Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.

</details>


### [322] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: 提出了一种噪声感知决策算法，通过量化设备特定噪声特征来管理变异性，使用分布分析和聚类选择单设备或多设备贝叶斯优化策略，提高性能、可重复性和效率。


<details>
  <summary>Details</summary>
Motivation: 实验噪声中的设备间变异性严重影响可重复性，特别是在自动化高通量系统中。这种变异性在大型系统（如建筑3D打印）中可能引发结构或经济故障风险。

Method: 使用分布分析和成对散度度量与聚类，选择单设备或鲁棒多设备贝叶斯优化策略，明确利用设备间差异来增强性能。

Result: 在三个名义相同的3D打印机上的实验案例显示减少了冗余、降低了资源使用并提高了可靠性。

Conclusion: 该框架为可扩展自动化实验平台建立了精确和资源感知优化的新范式。

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [323] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: 论文研究机器学习驱动的数据复制策略如何提升大规模分布式系统的容错能力，通过预测分析和强化学习实现自适应复制机制。


<details>
  <summary>Details</summary>
Motivation: 传统静态配置的复制方法难以适应动态工作负载和意外故障，导致资源利用效率低下和停机时间延长。

Method: 集成机器学习技术（预测分析和强化学习），提出能够预测系统故障并实时优化数据放置的自适应复制机制，通过文献综述、定性分析和与传统方法的比较评估进行研究。

Result: 识别出现有复制策略的关键局限性，凸显机器学习在创建更具弹性、自优化系统方面的变革潜力。

Conclusion: 研究结果强调了在现实环境中实施ML驱动解决方案的前景和挑战，为未来研究和在云基础及企业系统中的实际部署提供了建议。

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [324] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: 提出了TD-Orch任务数据编排框架，通过分布式推拉技术实现任务与数据的双向流动，解决分布式应用中任务与数据协同定位问题，在图形处理和键值存储等应用中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 分布式应用中任务需要访问分布在多台机器上的数据，传统方法难以高效处理数据热点和负载均衡问题，需要一种新的任务数据编排抽象来支持各类分布式应用。

Method: 采用分布式推拉技术，允许任务和数据双向流动，实现可扩展的负载均衡；构建TD-Orch框架提供简单开发者接口；基于TD-Orch开发TDO-GP分布式图形处理系统。

Result: TD-Orch比现有分布式调度基准最高提速2.7倍；TDO-GP比现有最佳开源分布式图形系统平均提速4.1倍。

Conclusion: TD-Orch框架通过高效的任务数据编排机制，显著提升了分布式应用的性能，特别是在处理数据热点和负载均衡方面表现出色，为分布式系统设计提供了新的思路。

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [325] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: Flash-Fusion是一个端到端的边缘-云系统，通过边缘统计摘要和云端查询规划，大幅降低IoT数据收集和分析负担，实现95%延迟降低和98%令牌使用减少。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLMs分析IoT数据时面临两大挑战：昂贵的数据收集基础设施产生过多细粒度传感器数据，以及需要技术专长的缓慢数据分析过程。直接向LLMs提供所有IoT遥测数据不切实际。

Method: 系统基于两个设计原则：1) 边缘统计摘要实现73.5%数据缩减；2) 云端查询规划聚类行为数据并组装上下文丰富的提示。

Result: 在大学巴士车队部署测试中，相比向最先进LLM提供原始数据的基线，Flash-Fusion实现了95%延迟降低和98%令牌使用及成本减少，同时保持高质量响应。

Conclusion: Flash-Fusion使安全官员、城市规划者、车队经理和数据科学家等不同角色能够高效迭代IoT数据，无需手动查询编写或预处理负担。

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [326] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: KVSwap是一个软件框架，通过将KV缓存卸载到磁盘来解决语言模型长上下文推理中的内存容量瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 本地运行语言模型在移动和嵌入式AI应用中具有隐私、离线和成本优势，但长上下文推理会因KV缓存线性增长而遇到内存容量瓶颈。

Method: KVSwap将完整KV缓存存储在磁盘上，使用紧凑的内存元数据预测需要预加载的条目，重叠计算与硬件感知的磁盘访问，并协调读取模式以匹配存储设备特性。

Result: 评估显示，在代表性语言模型和存储类型下，KVSwap在严格内存预算下提供更高吞吐量，同时保持生成质量优于现有KV缓存卸载方案。

Conclusion: KVSwap有效突破了语言模型长上下文推理的内存容量瓶颈，为移动和嵌入式AI应用提供了可行的解决方案。

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [327] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: 提出了一种创新的GPU并行计算方法，在28.4天内验证了27皇后问题，确认了PreuBer团队的结果，并将28皇后问题的预计解决时间缩短至约11个月。


<details>
  <summary>Details</summary>
Motivation: N皇后问题计数具有极高的计算复杂度，目前学术界仅严格验证到N<=26。27皇后问题在2016年由PreuBer团队用FPGA耗时约一年解决，但结果未获独立验证。GPU并行计算验证27皇后仍需约17个月，时间成本过高。

Method: 在NVIDIA GPU平台上实现创新并行计算方法：迭代深度优先搜索算法；将所需栈结构完全映射到GPU共享内存；通过精心设计的内存访问模式有效避免bank冲突；采用多种优化技术实现最佳性能。

Result: 使用8块RTX 5090 GPU在28.4天内成功验证27皇后问题，确认PreuBer计算结果正确；将28皇后问题的预计解决时间缩短至约11个月；相比现有GPU方法，在相同硬件配置(8 A100)下实现10倍以上加速，使用8 RTX 5090 GPU时实现26倍以上加速。

Conclusion: 该方法显著提升了N皇后问题求解效率，为这一长期停滞的问题带来了新的解决视角，使得更大规模皇后问题的求解在计算上变得可行。

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [328] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: GK Select是一种在Spark中计算精确分位数的算法，它避免了全数据洗牌，通过利用GK草图识别近似枢轴点，在线性时间内提取候选值，然后树形归约，实现与GK草图相似的执行时间但返回精确结果。


<details>
  <summary>Details</summary>
Motivation: Spark中现有的精确分位数计算方法需要昂贵的全局排序，而GK草图只能提供近似结果。需要一种既能避免全数据洗牌又能提供精确分位数的解决方案。

Method: 利用GK草图识别接近目标分位数的枢轴点，在每个分区中线性提取误差范围内的候选值，然后通过树形归约处理候选集得到精确分位数。

Result: 在30核AWS EMR集群上，对10^9个值分布在120个分区的情况下，GK Select实现了草图级别的延迟，比Spark的完全排序快约10.5倍。

Conclusion: GK Select在保持GK草图执行效率的同时，能够提供精确的分位数计算结果，解决了大数据环境中精确分位数计算的高成本问题。

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [329] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: BMC是一种新的KV缓存分配机制，通过每r次迭代分配带有r个冗余行的KV张量，实现原地更新而无需复制开销，同时将冗余计算重新用于推测解码以提高生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着GPU及其云实例成本飙升，人们希望使用CPU进行大语言模型推理。传统的KV缓存更新（分配、复制和原地更新）在序列长度增加时性能开销显著。

Method: 提出BMC机制：1）每r次迭代分配带r个冗余行的KV张量，支持原地更新；2）将冗余计算重新用于推测解码；3）通过分析模型找到最佳设计点。

Result: BMC在HuggingFace基准上实现平均3.2倍吞吐量加速，与推测解码结合时额外提升1.39倍，比vLLM和DeepSpeed分别快1.36倍和2.29倍。

Conclusion: BMC在CPU和GPU上均表现良好，通过平衡内存和计算开销，显著提升LLM推理效率。

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [330] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: Cylon是一个高性能分布式数据框架，通过设计无服务器通信器解决AWS Lambda等无服务器函数在处理大数据集时的通信和性能问题，性能相比传统服务器（EC2）和HPC下降不到1%。


<details>
  <summary>Details</summary>
Motivation: 随着公共云和无服务器计算的兴起，传统数据工程、机器学习和AI工作负载可以在无服务器函数上运行，但处理大数据集时依赖外部存储导致性能显著下降，需要解决无服务器环境下的通信瓶颈。

Method: 借鉴FMI库设计无服务器通信器，采用NAT穿透TCP打洞技术实现直接通信，构建高性能分布式数据框架Cylon。

Result: 实验表明，通过直接通信设计，AWS Lambda的性能在强扩展实验中相比服务器AWS（EC2）和HPC下降不到1%。

Conclusion: Cylon框架成功解决了无服务器函数在处理大数据集时的通信性能问题，证明了在无服务器环境中实现高性能数据处理的可行性。

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [331] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: 提出了第一个分布式季节性时间模式挖掘框架DSTPM，通过分布式层次查找哈希结构高效处理大规模时间序列数据，显著优于顺序基线方法。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生的大量时间序列数据中存在重要的周期性模式（季节性时间模式），但传统挖掘方法无法捕捉季节性且搜索空间指数级增长，现有顺序方法无法扩展到大规模数据集。

Method: 开发分布式季节性时间模式挖掘框架DSTPM，利用分布式层次查找哈希结构等高效数据结构进行计算。

Result: 广泛的实验评估表明，DSTPM在运行时间和内存使用方面显著优于顺序基线方法，并能有效扩展到非常大的数据集。

Conclusion: DSTPM是第一个用于从时间序列中挖掘季节性时间模式的分布式框架，解决了传统方法在大规模数据上的可扩展性问题。

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [332] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: 提出DSB Jacobi算法，通过数据流处理方式显著减少片上BRAM使用并提高计算速度，适用于大规模数据流的实时SVD计算


<details>
  <summary>Details</summary>
Motivation: 传统SVD计算在大规模矩阵时计算成本急剧增加，现有硬件架构存在可扩展性有限、片上内存资源消耗高的问题，难以满足嵌入式系统中大规模数据流矩阵的实时处理需求

Method: 提出基于数据流的SVD处理算法(DSB Jacobi)，优化计算流程减少片上内存使用

Result: 与之前工作相比，片上RAM消耗减少41.5%，计算效率提升23倍

Conclusion: DSB Jacobi算法为大规模数据流的实时SVD计算提供了实用解决方案，在资源消耗和计算效率方面均有显著改善

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [333] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了一种去中心化的根因定位方法，在边缘设备级别使用个性化PageRank算法进行异常定位，显著减少了定位时间并保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的根因定位方法为云环境设计，依赖集中式分析，在边缘环境中会增加延迟和通信开销。边缘计算环境中的微服务IoT应用容易发生性能异常，需要更高效的定位方法。

Method: 将微服务分组为通信和共置感知的集群，在集群内使用PPR算法进行本地根因定位。对于跨集群异常传播，引入集群间点对点近似过程。还提出了针对异构边缘环境的新异常评分机制。

Result: 在公开边缘数据集MicroCERCL上的评估显示，该方法比集中式方法减少定位时间达34%，同时达到相当或更高的定位准确性。

Conclusion: 去中心化的基于图的根因定位可以为资源受限的边缘环境提供实用高效的异常诊断解决方案。

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [334] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: Iris是一个完全用Python和Triton实现的多GPU通信库，通过tile-based对称内存抽象消除性能与可编程性之间的权衡，使开发者能够编写单源内核无缝交织计算和通信。


<details>
  <summary>Details</summary>
Motivation: 传统多GPU编程需要在性能和可编程性之间进行复杂权衡：高性能实现需要低层HIP/CUDA库且工程量大，而简单抽象往往牺牲性能。

Method: Iris提供与Triton编程模型自然对齐的tile-based对称内存抽象，支持从批量同步到细粒度工作组专业化的计算-通信重叠模式，只需在相同Triton内核中添加几行代码即可实现。

Result: 评估显示Iris在微基准测试中实现接近最优的带宽利用率，在GEMM+All-Scatter工作负载上比PyTorch和RCCL快达1.79倍。

Conclusion: 高层实现可以匹配或超过高度优化的库，同时显著简化多GPU编程。

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [335] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: 开发了一个基于Kubernetes的工具，支持非侵入式、延迟应用设计模式，无需修改服务代码，同时收集能耗指标以支持能耗感知决策，保持转换服务在不同管道结构中的可重用性。


<details>
  <summary>Details</summary>
Motivation: 随着数据网格架构的发展，组织越来越多地构建消费者特定的数据共享管道，使用模块化、基于云的转换服务。虽然可重用的转换服务可以提高成本和能源效率，但应用传统的云设计模式会降低服务在不同管道中的可重用性。

Method: 提出了一个基于Kubernetes的工具，该工具能够非侵入式地延迟应用设计模式，无需修改服务代码。该工具自动化模式注入并收集能源指标。

Result: 该工具支持能耗感知决策，同时保持转换服务在各种管道结构中的可重用性。

Conclusion: 通过非侵入式的方法应用设计模式，可以在提高能源效率的同时保持服务的可重用性，解决了传统云设计模式在数据网格架构中降低服务可重用性的问题。

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [336] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: 使用排队论技术分析Nakamoto区块链中的共识时间，计算诚实进程永久超过对抗进程所需的时间，特别关注随机延迟对诚实增长过程的影响。


<details>
  <summary>Details</summary>
Motivation: 研究Nakamoto区块链中达成共识所需的时间，理解诚实进程与对抗进程竞争时的动态特性，为区块链安全性提供理论分析。

Method: 采用排队论方法，将诚实和对抗进程建模为两个竞争的增长过程，计算诚实进程永久超过对抗进程的时间，并考虑诚实进程的随机延迟特性。

Result: 在简化的比特币模型中，计算了共识时间的拉普拉斯变换，并通过仿真验证了分析结果。

Conclusion: 通过排队论方法成功分析了Nakamoto区块链的共识时间，为理解区块链安全性提供了理论框架，并验证了分析方法的有效性。

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [337] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: 提出了一种基于eBPF和perf收集的细粒度进程级资源指标，结合节点级能耗测量，通过回归模型预测进程级能耗的方法。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗急剧增长，需要进程级能耗洞察来提升能效。现有机制如Intel RAPL仅限于特定硬件且只能提供粗粒度的域级测量。

Method: 利用eBPF和perf收集细粒度进程级资源指标，与PDU获取的节点级能耗测量同步，通过回归模型学习进程资源使用与节点能耗的关系。

Result: 实现了更细粒度的进程级能耗预测能力。

Conclusion: 该方法能够提供比现有机制更精细的进程级能耗分析，有助于数据中心能效优化。

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


### [338] [Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices](https://arxiv.org/abs/2511.13253)
*Mordechai Guri*

Main category: cs.DC

TL;DR: Pico-Cloud是基于树莓派Zero等超小型硬件平台的微边缘云架构，提供容器虚拟化、服务发现和轻量级编排，支持低延迟、低功耗的本地操作。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式数据中心在边缘计算场景中的延迟、功耗和成本问题，为轻量级分布式工作负载提供去中心化、可持续的平台。

Method: 在树莓派Zero等单板计算机上构建微边缘云架构，实现容器化虚拟化、服务发现和轻量级编排功能。

Result: Pico-Cloud被证明是网络边缘轻量级分布式工作负载的高性价比、去中心化和可持续平台。

Conclusion: Pico-Cloud架构为边缘计算提供了可行的解决方案，特别适用于农村连接、教育集群和边缘AI推理等场景。

Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.

</details>


### [339] [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)
*Sulaiman Muhammad Rashid,Ibrahim Aliyu,Jaehyung Park,Jinsul Kim*

Main category: cs.DC

TL;DR: 提出了一种基于DeepSets的分布式层次模型(DeepSets-S)，用于解决元宇宙边缘计算中的无线和计算资源联合管理问题，在保持置换等变性的同时显著降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 元宇宙需要低延迟和实时体验，但传统优化方法在动态边缘条件和高用户负载下难以有效响应，需要新的资源管理方案。

Method: 采用切片使能的网络边缘架构，结合计算在网络中(COIN)和多接入边缘计算(MEC)，将联合资源管理问题分解为三个子问题，并训练基于DeepSets的分布式层次模型。

Result: DeepSets-S在子问题SP1/SP2上达到95%以上的准确率，在SP3上多类卸载准确率为0.7486，相比精确求解器减少86.1%执行时间，系统成本与最优解差距在6.1%以内。

Conclusion: DeepSets-S能够高效处理可变大小设备集的资源分配问题，在保持接近最优性能的同时大幅降低计算复杂度，适用于元宇宙边缘计算场景。

Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [340] [Large-scale Multigrid with Adaptive Galerkin Coarsening](https://arxiv.org/abs/2511.13109)
*Fabian Böhm,Nils Kohl,Harald Köstler,Ulrich Rüde*

Main category: cs.PF

TL;DR: 提出了一种针对强变化系数PDE的鲁棒自适应粗网格校正方案，结合均匀几何粗化和异构粗网格算子，在系数梯度大的区域使用Galerkin粗网格近似，其他地方使用轻量级直接粗网格近似。


<details>
  <summary>Details</summary>
Motivation: 解决具有强变化系数的PDE在矩阵自由几何多重网格中的收敛鲁棒性问题，同时最小化内存需求。

Method: 结合均匀几何粗化和异构粗网格算子，在系数梯度大的区域局部应用Galerkin粗网格近似，其他地方使用轻量级直接粗网格近似。

Result: 在广义Stokes方程的sinker基准问题上验证了方法，解决了具有10^10自由度、10^6粘度跳跃和超过100,000并行进程的Stokes问题。

Conclusion: 该方法在保持鲁棒收敛的同时最小化了内存需求，能够高效处理大规模强变化系数PDE问题。

Abstract: We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.

</details>


### [341] [Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon](https://arxiv.org/abs/2511.13450)
*Álvaro Corrochano López,Carlos García Sánchez*

Main category: cs.PF

TL;DR: 本文评估了苹果神经引擎(ANE)在通用HPC应用中的潜力，发现当算法适当适配时，ANE在M4架构上能达到3.8 TFlops的竞争性性能，同时展现出显著更优的能效表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求增长，专用加速器(如GPU、TPU、NPU)被广泛集成到计算基础设施中。本研究探讨是否能够像GPGPU推广GPU一样，在NPU等专用加速器上复制这一现象，特别是在通用HPC应用场景中。

Method: 在苹果M1和M4架构上评估ANE的性能和能耗，测试经典HPC算法如GEMM、Jacobi和多网格方法，并将算法适配到ANE上运行。

Result: ANE在M4-Pro上达到3.8 TFlops性能(与同SoC上GPU的4.7 TFlops相当)，但能效显著更优：GEMM在ANE上仅消耗5.2瓦，而在GPU上消耗24瓦。

Conclusion: 当算法经过适当适配时，ANE能够在保持竞争性性能的同时，为HPC应用提供显著更优的能效，证明了专用加速器在通用计算领域的潜力。

Abstract: The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).

</details>
