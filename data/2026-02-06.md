<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 170]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Denoising diffusion networks for normative modeling in neuroimaging](https://arxiv.org/abs/2602.04886)
*Luke Whitbread,Lyle J. Palmer,Mark Jenkinson*

Main category: cs.LG

TL;DR: 提出使用去噪扩散概率模型作为表格影像衍生表型的统一条件密度估计器，通过采样获得单变量百分位数和偏差分数，在保持多变量依赖结构的同时实现良好校准。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像流程通常为每个影像衍生表型单独建模，虽然扩展性好但丢弃了可能编码协调模式的多变量依赖信息。需要一种既能保持多变量依赖结构，又能与标准单IDP流程兼容的规范化建模方法。

Method: 使用去噪扩散概率模型作为条件密度估计器，采用两种去噪器骨干：1) FiLM条件多层感知机；2) 具有特征自注意力和样本间注意力的表格Transformer，通过学习嵌入处理条件协变量。从合成基准到UK Biobank FreeSurfer表型，维度从2扩展到200进行评估。

Result: 在低维度下，扩散模型提供与传统基线相当的校准良好的单IDP输出，同时联合建模真实的依赖结构。在高维度下，Transformer骨干比MLP显著更好地保持校准，并更好地保留高阶依赖，实现可扩展的联合规范化模型。

Conclusion: 扩散基础的规范化建模为神经影像中校准的多变量偏差剖面提供了一条实用路径，能够在保持多变量依赖结构的同时与标准单IDP流程兼容。

Abstract: Normative modeling estimates reference distributions of biological measures conditional on covariates, enabling centiles and clinically interpretable deviation scores to be derived. Most neuroimaging pipelines fit one model per imaging-derived phenotype (IDP), which scales well but discards multivariate dependence that may encode coordinated patterns. We propose denoising diffusion probabilistic models (DDPMs) as a unified conditional density estimator for tabular IDPs, from which univariate centiles and deviation scores are derived by sampling. We utilise two denoiser backbones: (i) a feature-wise linear modulation (FiLM) conditioned multilayer perceptron (MLP) and (ii) a tabular transformer with feature self-attention and intersample attention (SAINT), conditioning covariates through learned embeddings. We evaluate on a synthetic benchmark with heteroscedastic and multimodal age effects and on UK Biobank FreeSurfer phenotypes, scaling from dimension of 2 to 200. Our evaluation suite includes centile calibration (absolute centile error, empirical coverage, and the probability integral transform), distributional fidelity (Kolmogorov-Smirnov tests), multivariate dependence diagnostics, and nearest-neighbour memorisation analysis. For low dimensions, diffusion models deliver well-calibrated per-IDP outputs comparable to traditional baselines while jointly modeling realistic dependence structure. At higher dimensions, the transformer backbone remains substantially better calibrated than the MLP and better preserves higher-order dependence, enabling scalable joint normative models that remain compatible with standard per-IDP pipelines. These results support diffusion-based normative modeling as a practical route to calibrated multivariate deviation profiles in neuroimaging.

</details>


### [2] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 提出Causal Analyst框架，通过因果分析揭示LLM越狱机制，识别直接因果特征，并应用于攻击增强和防御指导


<details>
  <summary>Details</summary>
Motivation: 现有研究主要分析潜在表示而忽视可解释特征与越狱之间的因果关系，需要理解越狱机制以提升LLM安全性和可靠性

Method: 结合LLM提示编码和GNN因果图学习，构建包含35k越狱尝试的数据集，标注37个人类可读特征，重建从特征到越狱响应的因果路径

Result: 发现"正面角色"和"任务步骤数量"等特定特征是越狱的直接因果驱动因素；攻击增强器显著提升成功率，护栏顾问能提取混淆查询的真实恶意意图

Conclusion: 从因果视角分析越狱特征是提高LLM可靠性的有效且可解释方法，因果分析优于非因果方法

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [3] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: 论文将Transformer视为物理电路，提出动量注意力机制，通过引入运动学动量实现单层归纳头，建立了辛几何与信号处理的二元性


<details>
  <summary>Details</summary>
Motivation: 将机械可解释性(MI)程序扩展到物理电路视角，通过引入守恒定律和时变交流动态，为Transformer提供新的分析框架

Method: 提出动量注意力机制，通过运动学差分算子p_t = q_t - q_{t-1}嵌入物理先验，实现辛剪切变换，建立辛-滤波器二元性

Result: 125M动量模型在归纳任务上表现优异，接近350M基线模型，建立了动量-深度可替代性缩放定律γ* = 4.17 × N^{-0.74}

Conclusion: 该框架为生成式AI、哈密顿物理和信号处理提供了互补的分析工具包，通过动量注入克服了拓扑深度约束

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [4] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: 特征导向方法能有效控制LLM行为但严重损害模型性能，简单的提示工程在行为控制和任务性能间取得最佳平衡


<details>
  <summary>Details</summary>
Motivation: 特征导向作为直接操控内部表征的方法在控制LLM行为方面显示出潜力，但其在实际应用中的有效性，特别是与输出质量之间的权衡关系尚不明确

Method: 评估Goodfire的Auto Steer特征导向方法，与提示工程基线对比，在14个导向查询（涵盖无害和安全相关行为）上，使用Llama-8B和Llama-70B模型，在171个MMLU问题上测量准确性、连贯性和行为控制

Result: Auto Steer成功修改了目标行为（Llama-8B得分3.33 vs 2.98，Llama-70B得分3.57 vs 3.10），但导致性能急剧下降：MMLU准确率从66%降至46%（8B）和87%降至73%（70B），连贯性从4.62降至2.24和4.94降至3.89

Conclusion: 当前特征导向方法在实际部署中存在严重局限性，特别是在不能牺牲任务性能的场景下；简单的提示工程实现了最佳整体平衡；机制控制方法面临基本的能力-行为权衡，需要在部署前进行实证表征

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [5] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: DCER：通过双阶段压缩和基于能量的重建解决多模态融合中的噪声输入和模态缺失问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合面临两个鲁棒性挑战：噪声输入会降低表示质量，模态缺失会导致预测失败。现有方法难以同时解决这两个问题。

Method: 提出DCER框架，包含双阶段压缩和基于能量的重建。压缩阶段：1）模态内频率变换（音频用小波，视频用DCT）去除噪声；2）跨模态瓶颈令牌强制真正融合而非捷径。对于缺失模态，基于能量的重建通过梯度下降在学习的能量函数上恢复表示。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS上达到最先进性能，呈现U型鲁棒性模式（在完整和高缺失条件下多模态融合效果最好）。最终能量值与预测误差相关性高（ρ > 0.72），提供内在不确定性量化。

Conclusion: DCER统一解决了多模态融合中的噪声和缺失问题，通过双阶段压缩和能量重建实现了鲁棒的多模态表示学习，并提供不确定性量化。

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [6] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA是一种用于拉普拉斯时间序列模型的推理时自适应方法，仅使用观测前缀进行自适应，结合延迟坐标嵌入和拉普拉斯谱学习，通过轻量级潜在空间残差适配器提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型在动态变化的环境中适应性不足，需要能够在推理时仅基于观测前缀进行自适应的方法，以应对动态变化带来的挑战。

Method: LISA结合延迟坐标嵌入和拉普拉斯谱学习生成扩散坐标状态表示，使用冻结非线性解码器进行一步预测，并引入基于高斯过程回归或类注意力马尔可夫算子的轻量级潜在空间残差适配器。

Result: 在预测和自回归展开实验中，LISA优于冻结基线，在动态变化情况下表现尤为突出，将上下文自适应与非参数谱方法联系起来。

Conclusion: LISA成功地将上下文自适应与动态系统的非参数谱方法相结合，为时间序列模型在动态变化环境中的推理时自适应提供了有效解决方案。

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [7] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: 提出一个结合物理知识与因果发现的框架，利用随机微分方程建模系统演化，其中漂移项编码已知ODE动力学，扩散项对应超出物理知识的未知因果耦合


<details>
  <summary>Details</summary>
Motivation: 因果发现通常基于数据驱动，而物理模型（如ODE）为动力学过程提供机制结构。将两者结合可以让物理知识作为归纳偏置，提高动力学系统中因果发现的可识别性、稳定性和鲁棒性。但现有方法面临挑战：真实系统常存在反馈、循环交互和非平稳趋势，而许多因果发现方法基于无环性或平衡假设

Method: 提出一个集成因果发现框架，将系统演化建模为随机微分方程（SDE），其中漂移项编码已知的ODE动力学，扩散项对应超出规定物理知识的未知因果耦合。开发了一个可扩展的稀疏诱导最大似然估计算法，利用因果图结构进行高效参数估计

Result: 在具有不同因果结构的动力学系统上的实验表明，该方法相比纯数据驱动的先进基线方法，提高了因果图恢复能力，并产生了更稳定、物理一致的估计

Conclusion: 该框架成功地将物理知识作为归纳偏置整合到因果发现中，为处理具有反馈、循环交互和非平稳趋势的真实动力学系统提供了更有效的解决方案

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [8] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: TPC是一种轻量级方差减少方法，通过在相同概率路径上耦合成对时间步的速度预测来降低梯度方差，无需修改模型架构、概率路径或求解器。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型（如扩散模型、流匹配、整流流）通常使用独立处理时间步的目标函数进行训练，导致估计器方差高且采样效率低下。现有方法通过显式平滑惩罚、轨迹正则化或修改概率路径和求解器来缓解这一问题。

Method: 提出时间对一致性（TPC）原则，在相同概率路径上耦合成对时间步的速度预测，在估计器层面操作而不修改模型架构、概率路径或求解器。理论分析表明TPC引入了二次轨迹耦合正则化，可证明减少梯度方差同时保持底层流匹配目标。

Result: 在流匹配中实例化TPC，在CIFAR-10和ImageNet的多个分辨率上提高了样本质量和效率，在相同或更低计算成本下实现了更低的FID，并能无缝扩展到现代SOTA风格的管道，包括噪声增强训练、基于分数的去噪和整流流。

Conclusion: TPC是一种有效的轻量级方差减少方法，通过时间对一致性耦合提高了连续时间生成模型的训练效率和样本质量，无需修改底层架构或方法。

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [9] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom是一种缓解联邦学习中参与者贡献评估不稳定性的新方法，通过生成更多样本来提供更一致可靠的贡献评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中公平评估参与者贡献至关重要，但现有方法存在显著不稳定性，这会影响参与者参与意愿，需要解决贡献评估的可靠性问题。

Method: 提出FedRandom方法，将不稳定性视为统计估计问题，通过生成比常规FL策略更多的样本来提供更一致的贡献评估。

Result: 在CIFAR-10、MNIST、CIFAR-100和FMNIST数据集上的实验显示，FedRandom在超过一半的场景中将与真实值的距离减少了三分之一以上，并在超过90%的情况下提高了稳定性。

Conclusion: FedRandom能有效缓解联邦学习中参与者贡献评估的不稳定性问题，提供更可靠一致的贡献评估，有助于提高参与者参与意愿。

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [10] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO提出用动态几何感知锚点替代DPO中的固定参考策略，通过对抗性局部扰动创建悲观基线，自适应重加权偏好对，提升噪声监督下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DPO等方法的固定参考策略会随着策略漂移而失准，导致分布不匹配并放大噪声监督下的虚假偏好信号；而无参考变体则存在无约束的奖励漂移问题。

Method: 提出几何锚点偏好优化(GAPO)：在当前策略的小半径内创建对抗性局部扰动作为动态几何感知锚点；引入锚点间隙度量策略与锚点间的奖励差异；在平滑条件下近似最坏情况局部边际退化；通过锚点间隙加权的逻辑目标优化，强调稳健偏好信号。

Result: 在各种噪声设置下，GAPO一致提升鲁棒性，同时在标准LLM对齐和推理基准测试中匹配或改进性能。

Conclusion: GAPO通过动态几何感知锚点机制有效解决了固定参考策略的失准问题，实现了对噪声监督的鲁棒对齐，在保持性能的同时显著提升稳定性。

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [11] [A logical re-conception of neural networks: Hamiltonian bitwise part-whole architecture](https://arxiv.org/abs/2602.04911)
*E Bowen,R Granger,A Rodriguez*

Main category: cs.LG

TL;DR: 提出一种基于图表示和哈密顿算子的新型计算架构，使用低精度算术直接编码关系结构，能进行符号式推理而非单纯统计学习。


<details>
  <summary>Details</summary>
Motivation: 当前人工神经网络方法在处理关系表示（如部分-整体关系）时存在局限，通常将关系编码作为附加功能而非系统基础。需要一种能直接表示关系结构、支持符号计算和推理的架构。

Method: 1. 将任意数据编码为图，边对应固定原始关系集（如部分-整体、相邻等）的代码；2. 引入图哈密顿算子计算编码能量，基态表示所有关系约束同时满足；3. 仅使用极低精度算术，计算成本与边数线性相关；4. 可推导出等效的ANN操作，识别嵌入向量编码的特殊情况。

Result: 系统能处理标准ANN示例，同时产生具有符号计算特性的表示：识别数据中的简单逻辑关系结构（部分-整体、相邻），构建层次表示支持溯因推理步骤，生成基于关系位置的编码而非单纯统计表示。

Conclusion: 该非常简单的初始系统为关系表示提供了新方法，结合了神经网络和符号计算的优势，为高级语义表示研究提供了有用途径，但需要更多工具和改进来完善。

Abstract: We introduce a simple initial working system in which relations (such as part-whole) are directly represented via an architecture with operating and learning rules fundamentally distinct from standard artificial neural network methods. Arbitrary data are straightforwardly encoded as graphs whose edges correspond to codes from a small fixed primitive set of elemental pairwise relations, such that simple relational encoding is not an add-on, but occurs intrinsically within the most basic components of the system. A novel graph-Hamiltonian operator calculates energies among these encodings, with ground states denoting simultaneous satisfaction of all relation constraints among graph vertices. The method solely uses radically low-precision arithmetic; computational cost is correspondingly low, and scales linearly with the number of edges in the data. The resulting unconventional architecture can process standard ANN examples, but also produces representations that exhibit characteristics of symbolic computation. Specifically, the method identifies simple logical relational structures in these data (part-of; next-to), building hierarchical representations that enable abductive inferential steps generating relational position-based encodings, rather than solely statistical representations. Notably, an equivalent set of ANN operations are derived, identifying a special case of embedded vector encodings that may constitute a useful approach to current work in higher-level semantic representation. The very simple current state of the implemented system invites additional tools and improvements.

</details>


### [12] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A²-LLM：端到端的对话式音频虚拟人大型语言模型，在统一框架中联合推理语言、音频韵律和3D面部运动，实现情感丰富的实时对话虚拟人。


<details>
  <summary>Details</summary>
Motivation: 当前对话虚拟人系统大多采用级联架构，存在错误累积、高延迟和实时性差的问题，且缺乏对话上下文理解，过度依赖刚性唇部同步而忽视情感深度。

Method: 提出A²-LLM端到端模型，统一处理语言、音频和面部运动；创建FLAME-QA高质量多模态数据集，以问答格式对齐语义意图与表情动态；利用深度语义理解生成超越唇部同步的情感丰富面部运动。

Result: 系统在保持实时效率的同时（500ms延迟，0.7 RTF），实现了卓越的情感表达能力，超越了简单的唇部同步。

Conclusion: A²-LLM通过端到端统一框架解决了传统级联系统的局限性，为下一代人机交互提供了情感丰富且实时响应的对话虚拟人解决方案。

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [13] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: SLAY是一种基于Yat核的线性时间注意力机制，将查询和键约束在单位球面上，通过随机特征近似实现O(L)复杂度，性能接近标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 现有线性时间注意力机制（如Performers、Cosformers）通常存在性能损失，需要一种既能保持线性复杂度又能接近标准softmax注意力性能的方法。

Method: 将查询和键约束到单位球面，使注意力仅依赖于角度对齐；利用Bernstein定理将球面Yat核表示为非负混合的多项式-指数乘积核；推导严格正的随机特征近似实现线性时间注意力。

Result: SLAY在性能上与标准softmax注意力几乎无法区分，同时保持线性时间和内存缩放，在实验中持续优于先前的线性时间注意力机制。

Conclusion: SLAY代表了迄今为止最接近softmax注意力的线性时间近似，实现了可扩展的Transformer而无需注意力线性化带来的典型性能折衷。

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [14] [Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams](https://arxiv.org/abs/2602.04917)
*Soshi Kakio,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: HeteroComp：一种用于异构张量流分析和组异常检测的方法，能同时处理分类属性和连续属性，并建模时间动态性


<details>
  <summary>Details</summary>
Motivation: 现有张量分解和异常检测方法存在两个主要局限：1）无法处理同时包含分类属性（如IP地址）和连续属性（如数据包长度）的异构张量流；2）离散化时间戳，无法跟踪流的动态变化，导致无法有效检测组级异常（如DoS攻击）

Method: 提出HeteroComp方法，使用高斯过程先验来建模连续属性的未知分布和时间动态性，直接从数据中估计概率密度。将异构张量流持续总结为表示每个属性中潜在组及其时间动态的"组件"

Result: 在真实数据集上的广泛实验表明，HeteroComp在组异常检测准确率上优于最先进算法，且计算时间不依赖于数据流长度

Conclusion: HeteroComp能够有效处理异构张量流，提供简洁但有效的总结，实现准确的组异常检测，解决了现有方法在处理混合属性和时间动态性方面的局限性

Abstract: Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.

</details>


### [15] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: 研究发现大语言模型在处理知识冲突时，不是通过稀释内部知识信号，而是通过注入准正交的转向向量来几何旋转隐藏状态表示，从而绕过正确解嵌向量，实现表面上的顺从。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常优先考虑上下文信息而非预训练参数记忆，这种现象被称为顺从性。但其机制尚不明确：模型如何通过顺从解决知识冲突？这种抑制是源于信号幅度稀释还是残差流中的方向性几何改变？

Method: 对Qwen-4B、Llama-3.1-8B和GLM-4-9B进行分层几何分析，将反事实上下文引起的残差流更新分解为径向（基于范数）和角度（基于余弦）分量。

Result: 拒绝了"流形稀释"假说的普遍性，因为三个架构中有两个在事实查询性能显著下降时仍保持稳定的残差范数。发现顺从行为一致表现为"正交干扰"，即冲突上下文注入一个准正交于真实方向的转向向量，有效旋转隐藏状态表示。

Conclusion: 模型不会"遗忘"或抑制内部真相的幅度，而是采用几何位移机制绕过正确解嵌向量，在保持原始结构幅度的同时模拟采纳。这挑战了基于标量置信度检测幻觉的方法，强调需要向量监控来区分真正的知识整合和表面的上下文模仿。

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [16] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 提出渐进式压缩方法PTL，通过多轮剪枝-调优循环逐步压缩大语言模型，在保持推理性能的同时将模型压缩至近半大小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力强但计算资源需求大，传统剪枝方法直接移除冗余参数会导致推理性能急剧下降，且需要大量后训练来恢复能力。

Method: 提出渐进式压缩方法PTL，将压缩过程分为多个细粒度迭代，每阶段应用剪枝-调优循环，逐步减小模型规模并通过微调恢复性能。

Result: PTL能将LLMs压缩至接近原始大小的一半，仅需轻量级后训练即可在推理任务上保持与原始模型相当的性能，且适用于多种剪枝策略和后训练方法。

Conclusion: PTL是一种灵活有效的渐进式压缩方法，能够在不显著损失性能的情况下大幅压缩大语言模型，在数学推理、代码生成等多种任务上展现广泛适用性。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [17] [CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning](https://arxiv.org/abs/2602.04920)
*Ronghao Lin,Qiaolin He,Sijie Mai,Ying Zeng,Aolin Xiong,Li Huang,Yap-Peng Tan,Haifeng Hu*

Main category: cs.LG

TL;DR: CyIN框架通过循环信息瓶颈和跨模态循环翻译，解决多模态学习中模态缺失的鲁棒性问题，在完整和不完整模态场景下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多模态数据的模态存在高度可变性和不可预测性，导致预训练模型在动态缺失模态情况下性能显著下降，缺乏鲁棒性。需要弥合完整与不完整多模态学习之间的差距。

Method: 提出Cyclic INformative Learning框架：1) 通过token和标签级信息瓶颈循环构建信息潜在空间，提取任务相关特征；2) 提出跨模态循环翻译，通过前向和反向传播过程重建缺失模态；3) 在统一模型中联合优化完整和不完整多模态学习。

Result: 在4个多模态数据集上的广泛实验表明，该方法在完整和多样不完整场景下均表现出优越性能。

Conclusion: CyIN框架通过信息瓶颈和跨模态重建，成功解决了多模态学习中模态缺失的鲁棒性问题，实现了完整和不完整模态场景的统一优化。

Abstract: Multimodal machine learning, mimicking the human brain's ability to integrate various modalities has seen rapid growth. Most previous multimodal models are trained on perfectly paired multimodal input to reach optimal performance. In real-world deployments, however, the presence of modality is highly variable and unpredictable, causing the pre-trained models in suffering significant performance drops and fail to remain robust with dynamic missing modalities circumstances. In this paper, we present a novel Cyclic INformative Learning framework (CyIN) to bridge the gap between complete and incomplete multimodal learning. Specifically, we firstly build an informative latent space by adopting token- and label-level Information Bottleneck (IB) cyclically among various modalities. Capturing task-related features with variational approximation, the informative bottleneck latents are purified for more efficient cross-modal interaction and multimodal fusion. Moreover, to supplement the missing information caused by incomplete multimodal input, we propose cross-modal cyclic translation by reconstruct the missing modalities with the remained ones through forward and reverse propagation process. With the help of the extracted and reconstructed informative latents, CyIN succeeds in jointly optimizing complete and incomplete multimodal learning in one unified model. Extensive experiments on 4 multimodal datasets demonstrate the superior performance of our method in both complete and diverse incomplete scenarios.

</details>


### [18] [Imposing Boundary Conditions on Neural Operators via Learned Function Extensions](https://arxiv.org/abs/2602.04923)
*Sepehr Mousavi,Siddhartha Mishra,Laura De Lorenzis*

Main category: cs.LG

TL;DR: 提出通过函数扩展将复杂边界条件编码到神经算子中的通用框架，在多种PDE问题上实现SOTA精度


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理复杂多变边界条件时能力有限，特别是当解算子对边界强迫表现出强敏感性时，现有方法往往失效

Method: 将边界数据映射到整个空间域上的潜在伪扩展，使任何标准算子学习架构都能消费边界信息，结合任意域到域神经算子同时学习复杂边界条件和输入域函数的丰富依赖关系

Result: 在18个具有高度可变、混合类型、分量式和多段边界条件的挑战性数据集上实现最先进精度，大幅超越基线方法，且无需跨数据集超参数调优

Conclusion: 学习边界到域的扩展是在现有神经算子框架中施加复杂边界条件的有效实用策略，为更广泛的PDE控制问题实现准确鲁棒的科学机器学习模型

Abstract: Neural operators have emerged as powerful surrogates for the solution of partial differential equations (PDEs), yet their ability to handle general, highly variable boundary conditions (BCs) remains limited. Existing approaches often fail when the solution operator exhibits strong sensitivity to boundary forcings. We propose a general framework for conditioning neural operators on complex non-homogeneous BCs through function extensions. Our key idea is to map boundary data to latent pseudo-extensions defined over the entire spatial domain, enabling any standard operator learning architecture to consume boundary information. The resulting operator, coupled with an arbitrary domain-to-domain neural operator, can learn rich dependencies on complex BCs and input domain functions at the same time. To benchmark this setting, we construct 18 challenging datasets spanning Poisson, linear elasticity, and hyperelasticity problems, with highly variable, mixed-type, component-wise, and multi-segment BCs on diverse geometries. Our approach achieves state-of-the-art accuracy, outperforming baselines by large margins, while requiring no hyperparameter tuning across datasets. Overall, our results demonstrate that learning boundary-to-domain extensions is an effective and practical strategy for imposing complex BCs in existing neural operator frameworks, enabling accurate and robust scientific machine learning models for a broader range of PDE-governed problems.

</details>


### [19] [Knowing When to Answer: Adaptive Confidence Refinement for Reliable Audio-Visual Question Answering](https://arxiv.org/abs/2602.04924)
*Dinh Phu Tran,Jihoon Jeong,Saad Wazir,Seongah Kim,Thao Do,Cem Subakan,Daeyoung Kim*

Main category: cs.LG

TL;DR: 本文提出了可靠视听问答（R-AVQA）的正式问题表述，旨在让模型在不确定时选择弃答而非错误回答。作者提出了自适应置信度修正（ACR）方法，通过两个学习头来增强现有模型的可靠性，在多种设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AVQA模型虽然准确率高，但在识别自身可能错误并选择弃答的能力方面研究不足。需要开发可靠的AVQA系统，在不确定时选择弃答而非给出错误答案，以提高实际应用中的可靠性。

Method: 提出了自适应置信度修正（ACR）方法：保持最大softmax概率（MSP）作为主要置信信号，当MSP不可靠时应用输入自适应的残差修正。包含两个学习头：1）残差风险头，预测MSP未捕捉到的低幅度正确性残差；2）置信度门控头，判断MSP的可信度。

Result: ACR在分布内、分布外和数据偏差设置下，在三种不同的AVQA架构上一致优于现有方法。实验和理论分析表明该方法为R-AVQA任务建立了坚实基础。

Conclusion: 本文正式定义了可靠视听问答问题，并提出ACR方法有效增强了现有模型的可靠性判断能力。该方法轻量级且可应用于不同架构，为构建可靠的视听问答系统提供了有效解决方案。

Abstract: We present a formal problem formulation for \textit{Reliable} Audio-Visual Question Answering ($\mathcal{R}$-AVQA), where we prefer abstention over answering incorrectly. While recent AVQA models have high accuracy, their ability to identify when they are likely wrong and their consequent abstention from answering remain underexplored areas of research. To fill this gap, we explore several approaches and then propose Adaptive Confidence Refinement (ACR), a lightweight method to further enhance the performance of $\mathcal{R}$-AVQA. Our key insight is that the Maximum Softmax Probability (MSP) is Bayes-optimal only under strong calibration, a condition usually not met in deep neural networks, particularly in multimodal models. Instead of replacing MSP, our ACR maintains it as a primary confidence signal and applies input-adaptive residual corrections when MSP is deemed unreliable. ACR introduces two learned heads: i) a Residual Risk Head that predicts low-magnitude correctness residuals that MSP does not capture, and ii) a Confidence Gating Head to determine MSP trustworthiness. Our experiments and theoretical analysis show that ACR consistently outperforms existing methods on in- and out-of-disrtibution, and data bias settings across three different AVQA architectures, establishing a solid foundation for $\mathcal{R}$-AVQA task. The code and checkpoints will be available upon acceptance \href{https://github.com/PhuTran1005/R-AVQA}{at here}

</details>


### [20] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR提出动态隐轨迹控制框架，通过三阶段流程将推理增强转化为动态隐轨迹控制问题，在减少token消耗的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖静态控制向量，无法适应复杂推理任务中的非平稳演化过程，需要动态适应性的推理增强方法。

Method: 三阶段流程：1) 差分内在动作归纳从隐状态推理成功中提取引导基元；2) 稀疏控制基构建创建紧凑几何多样的工具库；3) 价值调制轨迹干预通过锚点门控动态注入上下文特定脉冲。

Result: 在6个算术和逻辑基准测试中，STIR相比原始解码平均准确率提升1.9%至7.5%，平均token消耗减少高达35%。

Conclusion: 显式思维链推理的优势可以通过动态隐轨迹控制实现，将推理过程内部化以绕过显式生成，同时获得更好的保真度。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [21] [Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics](https://arxiv.org/abs/2602.04928)
*Ruizhe Zhong,Jiesong Lian,Xiaoyue Mi,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Junchi Yan*

Main category: cs.LG

TL;DR: Euphonium是一个新的强化学习框架，通过过程奖励梯度引导生成过程，解决现有流匹配模型对齐方法中探索效率低的问题，在文本到视频生成任务上实现了更好的对齐效果和1.66倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 当前基于在线强化学习的流匹配模型对齐方法存在探索效率低的问题，主要依赖无方向的随机性和稀疏的结果奖励，难以发现高奖励样本，导致数据效率低下和优化缓慢。

Method: 提出Euphonium框架，将采样过程建模为包含过程奖励模型梯度的随机微分方程，实现密集的逐步引导；进一步推导蒸馏目标将引导信号内化到流网络中；采用双奖励组相对策略优化算法，结合潜在过程奖励和像素级结果奖励。

Result: 在文本到视频生成任务上，Euphonium相比现有方法实现了更好的对齐效果，同时将训练收敛速度提高了1.66倍。

Conclusion: Euphonium通过过程奖励梯度引导的动力学方法，有效解决了流匹配模型对齐中的探索效率问题，在理论上包含现有采样方法作为特例，并在实践中展示了优越的性能。

Abstract: While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x.

</details>


### [22] [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)
*Junhan Kim,Yeo Jeong Park,Seungwoo Son,Chungman Lee,Ho-young Kim,Joonyoung Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: TurboBoA是一种新的后训练量化算法，在保持BoA准确性的同时显著加速量化过程，通过联合量化多个输出通道、误差传播校正和自适应网格计算等创新，在低比特量化中实现最先进的结果。


<details>
  <summary>Details</summary>
Motivation: GPTQ虽然高效但假设层间独立导致低比特量化准确率下降，BoA虽然通过考虑注意力模块的层间依赖提高了准确性，但依赖所有输出通道的顺序量化导致效率低下。需要一种既能保持准确性又能显著加速的后训练量化方法。

Method: 提出TurboBoA算法，包含三个关键创新：1）使用闭式误差补偿规则联合量化多个输出通道，减少顺序瓶颈；2）对先前量化层传播的误差进行校正；3）采用坐标下降细化的自适应网格计算，在迭代更新中保持对齐。

Result: 实验表明TurboBoA在保持BoA准确性的同时实现显著加速，结合异常值抑制技术后，在仅权重量化和权重-激活量化中都达到了最先进的性能。

Conclusion: TurboBoA是一种高效准确的后训练量化算法，通过创新的联合量化策略和误差校正机制，在低比特量化场景中实现了速度与准确性的平衡，为大规模语言模型的部署提供了实用解决方案。

Abstract: The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hours. However, GPTQ's assumption of layer-wise independence leads to severe accuracy drops in low-bit regimes. Recently, BoA improved upon GPTQ by incorporating inter-layer dependencies within attention modules, but its reliance on sequential quantization across all out-channels makes it substantially less efficient. In this paper, we propose TurboBoA, a new backpropagation-free PTQ algorithm that preserves the accuracy benefits of BoA while significantly accelerating the process. The proposed TurboBoA introduces three key innovations: (i) joint quantization of multiple out-channels with a closed-form error compensation rule, which reduces sequential bottlenecks and yields more than a three-fold speedup; (ii) a correction mechanism for errors propagated from preceding quantized layers; and (iii) adaptive grid computation with coordinate descent refinement to maintain alignment during iterative updates. Extensive experiments demonstrate that TurboBoA delivers substantial acceleration over BoA while consistently improving accuracy. When combined with outlier suppression techniques, it achieves state-of-the-art results in both weight-only and weight-activation quantization. The code will be available at https://github.com/SamsungLabs/TurboBoA.

</details>


### [23] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: 论文发现仅解码器LLM在深度维度上存在从上下文处理到预测形成的计算阶段转换，伴随表征几何的重组。晚期层表征实现结构化几何编码，支持对token预测的选择性因果控制。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型如何将上下文信息转化为预测的内部机制，特别是表征几何在预测形成过程中的作用。

Method: 结合几何分析和机制干预的统一框架，分析解码器LLM的深度层表征几何，研究角度组织和范数编码的信息类型。

Result: 晚期层表征实现结构化几何编码：角度组织参数化预测分布相似性，表征范数编码上下文特定信息但不决定预测。深度维度存在从上下文处理到预测形成的计算阶段转换。

Conclusion: 该研究提供了LLM将上下文转化为预测的机制-几何解释，揭示了表征几何在实现选择性因果控制中的关键作用。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [24] [Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery](https://arxiv.org/abs/2602.04932)
*Mohamad Dalal,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: HC-GCD在双曲空间中直接聚类，相比之前仅在双曲空间学习表示但回到欧几里得空间聚类的方法，使用双曲K-Means能获得更好性能


<details>
  <summary>Details</summary>
Motivation: 先前双曲广义类别发现方法只在表示学习中使用双曲几何，聚类时转回欧几里得空间，作者认为这是次优的，应该在双曲空间直接聚类

Method: 提出Hyperbolic Clustered GCD (HC-GCD)，在洛伦兹双曲面模型中学习嵌入，并使用双曲K-Means算法直接在双曲空间聚类这些嵌入

Result: 在Semantic Shift Benchmark数据集上，HC-GCD与先前最先进的双曲GCD方法性能相当；双曲K-Means比欧几里得K-Means准确率更高；截断欧几里得嵌入范数对未见类和已见类准确率有不同影响

Conclusion: 直接在双曲空间聚类比在欧几里得空间聚类更优，双曲K-Means能产生更一致的聚类结果，特别是在标签粒度变化时

Abstract: Hyperbolic representation learning has been widely used to extract implicit hierarchies within data, and recently it has found its way to the open-world classification task of Generalized Category Discovery (GCD). However, prior hyperbolic GCD methods only use hyperbolic geometry for representation learning and transform back to Euclidean geometry when clustering. We hypothesize this is suboptimal. Therefore, we present Hyperbolic Clustered GCD (HC-GCD), which learns embeddings in the Lorentz Hyperboloid model of hyperbolic geometry, and clusters these embeddings directly in hyperbolic space using a hyperbolic K-Means algorithm. We test our model on the Semantic Shift Benchmark datasets, and demonstrate that HC-GCD is on par with the previous state-of-the-art hyperbolic GCD method. Furthermore, we show that using hyperbolic K-Means leads to better accuracy than Euclidean K-Means. We carry out ablation studies showing that clipping the norm of the Euclidean embeddings leads to decreased accuracy in clustering unseen classes, and increased accuracy for seen classes, while the overall accuracy is dataset dependent. We also show that using hyperbolic K-Means leads to more consistent clusters when varying the label granularity.

</details>


### [25] [Linear Model Merging Unlocks Simple and Scalable Multimodal Data Mixture Optimization](https://arxiv.org/abs/2602.04937)
*Davide Berasi,Matteo Farina,Massimiliano Mancini,Elisa Ricci*

Main category: cs.LG

TL;DR: 本文提出通过模型合并来高效评估多模态大语言模型监督微调的数据混合优化问题，避免了传统方法需要多次训练的高昂成本。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型监督微调中，确定跨多个领域特定数据集的最佳混合权重是一个关键但计算成本高昂的问题（数据混合优化问题）。传统方法需要在组合搜索空间中进行多次训练，而模型合并虽然高效但通常产生次优模型。

Method: 训练领域特定的多模态专家模型，通过加权参数空间组合来评估不同数据混合的效果。将模型合并作为评估数据混合性能的代理方法，从而将混合权重搜索与资源密集的训练过程解耦。

Result: 在14个多模态基准测试上的实验表明，合并的代理模型与实际数据混合训练的模型表现出高度排名相关性，验证了该方法作为数据混合优化高效策略的有效性。

Conclusion: 模型合并可以作为评估多模态大语言模型数据混合优化的高效代理方法，显著降低了寻找最优混合权重的计算成本，为复杂的混合权重搜索提供了可扩展的解决方案。

Abstract: Selecting the best data mixture is critical for successful Supervised Fine-Tuning (SFT) of Multimodal Large Language Models. However, determining the optimal mixture weights across multiple domain-specific datasets remains a significant bottleneck due to the combinatorial search space and the high cost associated with even a single training run. This is the so-called Data Mixture Optimization (DMO) problem. On the other hand, model merging unifies domain-specific experts through parameter interpolation. This strategy is efficient, as it only requires a single training run per domain, yet oftentimes leads to suboptimal models. In this work, we take the best of both worlds, studying model merging as an efficient strategy for estimating the performance of different data mixtures. We train domain-specific multimodal experts and evaluate their weighted parameter-space combinations to estimate the efficacy of corresponding data mixtures. We conduct extensive experiments on 14 multimodal benchmarks, and empirically demonstrate that the merged proxy models exhibit a high rank correlation with models trained on actual data mixtures. This decouples the search for optimal mixtures from the resource-intensive training process, thereby providing a scalable and efficient strategy for navigating the complex landscape of mixture weights. Code is publicly available at https://github.com/BerasiDavide/mLLMs_merging_4_DMO.

</details>


### [26] [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)
*Hang Zhou,Haixu Wu,Haonan Shangguan,Yuezhou Ma,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Transolver-3是一个高度可扩展的神经PDE求解框架，通过架构优化和训练策略，能够处理超过1.6亿单元的工业规模网格，实现高保真物理模拟。


<details>
  <summary>Details</summary>
Motivation: 现有神经PDE求解器在处理工业规模几何（超过10^8单元）时面临内存限制的挑战，需要解决高分辨率网格处理的内存复杂度问题。

Method: 1. 利用矩阵乘法结合律实现更快的切片和反切片操作；2. 几何切片平铺技术分割物理状态计算；3. 在原始高分辨率网格随机子集上进行摊销训练；4. 推理期间使用物理状态缓存技术。

Result: Transolver-3能够处理超过1.6亿单元的网格，在包括飞机和汽车设计任务在内的三个具有挑战性的模拟基准测试中表现出色。

Conclusion: Transolver-3通过创新的架构优化和训练策略，成功解决了神经PDE求解器在工业规模应用中的可扩展性问题，为复杂工程任务提供了高效的模拟解决方案。

Abstract: Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memory complexity of processing high-resolution meshes. We present Transolver-3, a new member of the Transolver family as a highly scalable framework designed for high-fidelity physics simulations. To bridge the gap between limited GPU capacity and the resolution requirements of complex engineering tasks, we introduce two key architectural optimizations: faster slice and deslice by exploiting matrix multiplication associative property and geometry slice tiling to partition the computation of physical states. Combined with an amortized training strategy by learning on random subsets of original high-resolution meshes and a physical state caching technique during inference, Transolver-3 enables high-fidelity field prediction on industrial-scale meshes. Extensive experiments demonstrate that Transolver-3 is capable of handling meshes with over 160 million cells, achieving impressive performance across three challenging simulation benchmarks, including aircraft and automotive design tasks.

</details>


### [27] [Improving Set Function Approximation with Quasi-Arithmetic Neural Networks](https://arxiv.org/abs/2602.04941)
*Tomas Tokar,Scott Sanner*

Main category: cs.LG

TL;DR: 本文提出神经化Kolmogorov均值(NKM)和准算术神经网络(QUANN)，通过可学习的可逆神经聚合函数提升集合建模的表达能力和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有集合建模方法如DeepSets和PointNet使用固定的非学习池化操作(如求和或最大值)，这限制了学习嵌入的可迁移性和模型表达能力。

Method: 提出神经化Kolmogorov均值(NKM)，通过可逆神经函数学习广义中心趋势度量；进一步提出准算术神经网络(QUANN)，将NKM作为可学习聚合函数。

Result: 理论分析表明QUANN是广泛集合函数分解的通用逼近器，且学习到更结构化的潜在表示；实证显示QUANN在多个基准测试中优于现有方法，学习到的嵌入能有效迁移到非集合任务。

Conclusion: NKM和QUANN框架通过可学习的可逆神经聚合函数显著提升了集合建模的表达能力和嵌入可迁移性，为集合结构数据处理提供了更强大的工具。

Abstract: Sets represent a fundamental abstraction across many types of data. To handle the unordered nature of set-structured data, models such as DeepSets and PointNet rely on fixed, non-learnable pooling operations (e.g., sum or max) -- a design choice that can hinder the transferability of learned embeddings and limits model expressivity. More recently, learnable aggregation functions have been proposed as more expressive alternatives. In this work, we advance this line of research by introducing the Neuralized Kolmogorov Mean (NKM) -- a novel, trainable framework for learning a generalized measure of central tendency through an invertible neural function. We further propose quasi-arithmetic neural networks (QUANNs), which incorporate the NKM as a learnable aggregation function. We provide a theoretical analysis showing that, QUANNs are universal approximators for a broad class of common set-function decompositions and, thanks to their invertible neural components, learn more structured latent representations. Empirically, QUANNs outperform state-of-the-art baselines across diverse benchmarks, while learning embeddings that transfer effectively even to tasks that do not involve sets.

</details>


### [28] [Privileged Information Distillation for Language Models](https://arxiv.org/abs/2602.04942)
*Emiliano Penaloza,Dheeraj Vattikonda,Nicolas Gontier,Alexandre Lacoste,Laurent Charlin,Massimo Caccia*

Main category: cs.LG

TL;DR: 提出π-Distill和OPSD两种方法，解决训练时有特权信息但推理时无特权信息的蒸馏问题，在智能体环境中超越传统监督微调+强化学习的方法。


<details>
  <summary>Details</summary>
Motivation: 在智能体环境中，前沿模型通常隐藏内部推理过程，只暴露动作轨迹，这打破了标准蒸馏流程。训练时的特权信息能帮助语言模型完成任务，但如何在推理时没有特权信息的情况下保持性能是一个根本挑战。

Method: 提出两种方法：1) π-Distill：联合教师-学生目标，同时训练特权信息条件化的教师和无条件的学生；2) OPSD：基于强化学习的方法，在学生和特权信息条件化教师之间使用反向KL惩罚。

Result: 两种方法都能有效利用仅动作特权信息蒸馏前沿智能体。π-Distill（在某些情况下OPSD）在多个智能体基准测试、模型和特权信息形式上，都优于行业标准实践（监督微调+强化学习）。

Conclusion: π-Distill和OPSD成功解决了训练时有特权信息但推理时无特权信息的蒸馏问题，在智能体环境中表现出色，为前沿模型蒸馏提供了有效解决方案。

Abstract: Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that π-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.

</details>


### [29] [Stochastic hierarchical data-driven optimization: application to plasma-surface kinetics](https://arxiv.org/abs/2602.04975)
*José Afonso,Vasco Guerra,Pedro Viegas*

Main category: cs.LG

TL;DR: 提出了一种基于Sloppy Model理论的随机分层优化框架，用于高效校准物理模型，通过约化Hessian近似识别并针对刚性参数子空间，显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 物理模型校准面临计算成本高、参数不确定性大的挑战，特别是在等离子体-表面相互作用等领域，需要高效且严谨的优化方法来处理高度各向异性的参数空间。

Method: 采用随机分层优化框架，核心是约化Hessian近似方法，识别刚性参数子空间并用最少的模拟查询进行优化；结合概率化公式从观测数据推导出原则性的目标损失函数。

Result: 在等离子体-表面相互作用问题上验证，该方法在样本效率上持续优于基线优化技术，为复杂反应系统模型优化提供了通用且可扩展的工具。

Conclusion: 该框架为从等离子体化学到生化网络等复杂反应系统的模型优化提供了高效、严谨且可扩展的解决方案，显著降低了计算负担。

Abstract: This work introduces a stochastic hierarchical optimization framework inspired by Sloppy Model theory for the efficient calibration of physical models. Central to this method is the use of a reduced Hessian approximation, which identifies and targets the stiff parameter subspace using minimal simulation queries. This strategy enables efficient navigation of highly anisotropic landscapes, avoiding the computational burden of exhaustive sampling. To ensure rigorous inference, we integrate this approach with a probabilistic formulation that derives a principled objective loss function directly from observed data. We validate the framework by applying it to the problem of plasma-surface interactions, where accurate modelling is strictly limited by uncertainties in surface reactivity parameters and the computational cost of kinetic simulations. Comparative analysis demonstrates that our method consistently outperforms baseline optimization techniques in sample efficiency. This approach offers a general and scalable tool for optimizing models of complex reaction systems, ranging from plasma chemistry to biochemical networks.

</details>


### [30] [Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation](https://arxiv.org/abs/2602.04989)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于粗化方法的在线匹配算法，通过将离线节点聚合成有容量限制的集群，在器官分配等应用中实现接近最优的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在线匹配在互联网广告和器官分配等领域应用广泛，但实际算法往往缺乏强理论保证。本文旨在解决这一问题，弥合数据驱动启发式方法与悲观理论下界之间的差距。

Method: 开发基于粗化方法的新在线匹配算法，将离线节点聚合成有容量限制的集群，并应用于心脏移植分配，基于历史数据的结构特性构建理论基础的策略。

Result: 在现实模拟中，该策略的表现接近全知基准的性能，为器官分配中基于聚类的方法提供了严格的理论依据。

Conclusion: 粗化方法虽然通常意味着粒度损失，但通过将离线节点聚合成有容量限制的集群，可以获得接近最优的理论保证，弥合了实践与理论之间的鸿沟。

Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. Although coarsening typically implies a loss of granularity, we show that, to the contrary, aggregating offline nodes into capacitated clusters can yield near-optimal theoretical guarantees. We apply our methodology to heart transplant allocation to develop theoretically grounded policies based on structural properties of historical data. In realistic simulations, our policy closely matches the performance of the omniscient benchmark. Our work bridges the gap between data-driven heuristics and pessimistic theoretical lower bounds, and provides rigorous justification for prior clustering-based approaches in organ allocation.

</details>


### [31] [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)
*Ioannis Anagnostides,Itai Zilberstein,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 器官分配不仅是静态优化问题，更是涉及多方参与者的复杂博弈，当前方法忽视了激励机制这一关键因素，需要开发激励感知的新一代分配政策。


<details>
  <summary>Details</summary>
Motivation: 器官分配是医疗领域最重要的算法挑战之一，当前从规则系统向机器学习优化的转型中，忽视了激励机制这一根本障碍。器官分配实际上是移植中心、临床医生和监管机构之间的复杂博弈，现有方法忽略了激励错位问题。

Method: 这是一篇立场论文，通过分析美国成人心脏移植分配系统，识别决策流程中的关键激励错位，并提出数据证据显示这些错位正在产生负面影响。论文呼吁整合机制设计、战略分类、因果推断和社会选择等方法。

Result: 论文展示了激励错位在器官分配系统中已经产生了不利后果，强调需要开发激励感知的分配政策来应对各参与方的战略行为。

Conclusion: 下一代器官分配政策必须是激励感知的，机器学习社区需要整合机制设计、战略分类、因果推断和社会选择等方法，确保在面对各参与方战略行为时的鲁棒性、效率和公平性。

Abstract: The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.

</details>


### [32] [Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning](https://arxiv.org/abs/2602.04998)
*Yu-Ang Lee,Ching-Yun Ko,Pin-Yu Chen,Mi-Yen Yeh*

Main category: cs.LG

TL;DR: LoRA变体在超参数调优后性能相近，原始LoRA仍是强基线，先前报告的改进可能源于固定超参数设置而非方法优势


<details>
  <summary>Details</summary>
Motivation: 现有研究提出了多种LoRA变体并报告了显著改进，但这些结果通常在固定或有限调优的超参数设置下获得。神经网络对训练配置敏感，需要系统评估这些方法在广泛超参数搜索下的真实性能差异。

Method: 系统评估四种代表性LoRA变体与原始LoRA，在数学和代码生成任务上对多种模型规模进行广泛的超参数搜索，特别关注学习率的影响。使用二阶分析研究不同方法最优学习率范围的差异。

Result: 不同LoRA方法偏好不同的学习率范围。一旦学习率得到适当调优，所有方法都达到相似的峰值性能（差异在1-2%内），仅表现出细微的秩依赖行为。原始LoRA仍然是具有竞争力的基线。

Conclusion: 先前在单一训练配置下报告的改进可能不反映一致的方法优势。二阶分析表明最优学习率范围的差异源于最大Hessian特征值的变化，这与经典学习理论一致。超参数调优对公平比较LoRA方法至关重要。

Abstract: Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning. Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter searches. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rates are properly tuned, all methods achieve similar peak performance (within 1-2%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains a competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, a second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue, aligning with classical learning theories.

</details>


### [33] [EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models](https://arxiv.org/abs/2602.05000)
*Atula Tejaswi,Litu Rout,Constantine Caramanis,Sanjay Shakkottai,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 提出EntRGi方法，通过基于模型置信度的动态梯度调节，解决离散扩散语言模型中奖励引导的优化问题，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散语言模型的奖励引导方法存在两个问题：连续松弛方法会降低梯度反馈质量（奖励模型未训练过连续输入），而直通估计器方法存在优化错误（在离散标记处评估的梯度用于更新连续logits）。需要超越这种权衡的新方法。

Method: 提出EntRGi（熵感知奖励引导）方法，通过基于模型置信度动态调节奖励模型梯度，在调节连续松弛的同时为奖励模型提供可靠输入。

Result: 在7B参数扩散语言模型上，针对3个不同奖励模型和3个多技能基准测试进行验证，相比最先进方法取得一致改进。

Conclusion: EntRGi方法通过动态梯度调节机制，有效解决了离散扩散语言模型中奖励引导的优化问题，超越了现有方法的局限性。

Abstract: Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.

</details>


### [34] [Enhanced QKNorm normalization for neural transformers with the Lp norm](https://arxiv.org/abs/2602.05006)
*Ezequiel Lopez-Rubio,Javier Montes-Perez,Esteban Jose Palomo*

Main category: cs.LG

TL;DR: 提出了一种基于Lp范数的QKNorm归一化方案泛化方法，允许使用非欧几里得范数，实验证明该方法适用于简单问题。


<details>
  <summary>Details</summary>
Motivation: 查询和键向量的归一化是Transformer架构的重要组成部分，能确保学习过程不受向量尺度影响。现有归一化方法有限，需要更通用的方案。

Method: 提出QKNorm归一化方案的泛化版本，基于Lp范数，允许使用非欧几里得范数进行归一化处理。

Result: 实验结果表明该方法适用于简单问题，证明了其可行性。

Conclusion: 基于Lp范数的QKNorm泛化方法为Transformer归一化提供了更灵活的方案，初步验证了其有效性。

Abstract: The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.

</details>


### [35] [Private PoEtry: Private In-Context Learning via Product of Experts](https://arxiv.org/abs/2602.05012)
*Rob Romijnders,Mohammad Mahdi Derakhshani,Jonathan Petit,Max Welling,Christos Louizos,Yuki M. Asano*

Main category: cs.LG

TL;DR: 论文提出了一种基于专家乘积模型的新框架，用于保护隐私的上下文学习，相比现有方法在多个任务上平均提升30%以上准确率，同时保持强隐私保证。


<details>
  <summary>Details</summary>
Motivation: 上下文学习使大语言模型能够通过少量示例适应新任务，但示例可能包含隐私敏感信息。现有的差分隐私方法要么计算成本高，要么依赖启发式方法效果有限。

Method: 通过专家乘积模型的视角重新构建隐私保护的上下文学习，提供了一个理论基础的框架，算法可以轻松并行化。

Result: 在文本分类、数学和视觉语言五个数据集上的评估显示，相比现有DP-ICL方法，该方法平均提升准确率超过30个百分点，同时保持强隐私保证。

Conclusion: 提出的基于专家乘积模型的框架为隐私保护的上下文学习提供了理论基础和高效实现，显著提升了性能同时保持隐私保护。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.

</details>


### [36] [A Simple Reduction Scheme for Constrained Contextual Bandits with Adversarial Contexts via Regression](https://arxiv.org/abs/2602.05019)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出一种基于SquareCB框架的模块化算法方案，通过在线回归预言机将约束上下文赌博机问题转化为无约束问题，在对抗性上下文设置下实现改进的遗憾和约束违反保证。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性选择上下文下的约束上下文赌博机问题，现有工作主要关注随机上下文，需要为更一般的对抗性上下文设置提供改进的保证和更简洁的分析。

Method: 基于Foster等人的SquareCB框架，提出模块化算法方案，利用在线回归预言机将约束问题转化为标准无约束上下文赌博机问题，通过自适应定义代理奖励函数来处理约束。

Result: 在对抗性上下文设置下获得了改进的遗憾和累积约束违反保证，相比大多数关注随机上下文的先前工作，提供了更紧凑和透明的分析。

Conclusion: 提出的算法方案成功将约束上下文赌博机问题简化为无约束问题，在对抗性上下文设置中实现了更好的性能保证，为这类问题提供了更通用的解决方案。

Abstract: We study constrained contextual bandits (CCB) with adversarially chosen contexts, where each action yields a random reward and incurs a random cost. We adopt the standard realizability assumption: conditioned on the observed context, rewards and costs are drawn independently from fixed distributions whose expectations belong to known function classes. We consider the continuing setting, in which the algorithm operates over the entire horizon even after the budget is exhausted. In this setting, the objective is to simultaneously control regret and cumulative constraint violation. Building on the seminal SquareCB framework of Foster et al. (2018), we propose a simple and modular algorithmic scheme that leverages online regression oracles to reduce the constrained problem to a standard unconstrained contextual bandit problem with adaptively defined surrogate reward functions. In contrast to most prior work on CCB, which focuses on stochastic contexts, our reduction yields improved guarantees for the more general adversarial context setting, together with a compact and transparent analysis.

</details>


### [37] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 提出基于熵的终身集成学习方法，通过免疫机制防御对抗攻击，在CIFAR-10上优于简单平均集成


<details>
  <summary>Details</summary>
Motivation: 针对对抗攻击防御问题，基于学习动力学基本定律（守恒定律和总熵减少）开发更有效的集成学习方法

Method: 提出熵基终身集成学习框架，构建免疫机制防御迁移式对抗攻击，形成logifold结构

Result: 在CIFAR-10数据集上，相比简单平均干净样本和对抗样本模型的朴素集成，logifold在大多数测试情况下获得更高准确率，在强扰动下优势尤其明显

Conclusion: 基于熵的终身集成学习框架能有效提升对抗攻击防御能力，特别是在强扰动场景下表现优异

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [38] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: ALPS是一种基于拉普拉斯表示的分层规划算法，在离线目标条件RL任务中优于常用基线方法


<details>
  <summary>Details</summary>
Motivation: 基于学习模型的规划是模型强化学习的关键挑战。在决策时规划中，状态表示必须支持局部成本计算同时保持长时程结构，但现有方法难以平衡这两点

Method: 提出使用拉普拉斯表示作为规划的有效潜在空间，该表示能捕捉多时间尺度的状态空间距离，保持有意义的距离度量，并将长时程问题自然分解为子目标。基于此构建ALPS分层规划算法

Result: 在OGBench基准测试的离线目标条件RL任务中，ALPS超越了常用的基线方法，该基准此前主要由无模型方法主导

Conclusion: 拉普拉斯表示为基于学习模型的规划提供了有效的潜在空间，能够缓解长预测时程中的误差累积问题，并在实际任务中展现出优越性能

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [39] [Causal Representation Meets Stochastic Modeling under Generic Geometry](https://arxiv.org/abs/2602.05033)
*Jiaxu Ren,Yixin Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了MUTATE框架，用于从连续时间随机点过程的观测中学习可识别的因果表示，解决了现有方法主要针对i.i.d.或离散时间过程的局限性。


<details>
  <summary>Details</summary>
Motivation: 从观测中学习有意义的因果表示对于机器学习应用和科学发现至关重要，但现有可识别性方法主要针对i.i.d.或离散时间过程，而许多现实世界场景需要识别连续时间随机过程（如多元点过程）中的潜在变量。

Method: 开发了MUTATE框架，这是一个可识别的变分自编码器框架，包含时间自适应转换模块，用于推断随机动力学。通过分析参数空间的几何结构来研究其可识别性。

Result: 在模拟和实证研究中，MUTATE能够有效回答科学问题，如基因组中突变的积累和神经元放电触发机制对时变动力学的响应。

Conclusion: 该工作扩展了因果表示学习的可识别性到连续时间随机点过程，为气候科学、生物学和物理学等领域的科学发现提供了新工具。

Abstract: Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.

</details>


### [40] [Feedback Control for Multi-Objective Graph Self-Supervision](https://arxiv.org/abs/2602.05036)
*Karish Grover,Theodore Vasiloudis,Han Xie,Sixing Lu,Xiang Song,Christos Faloutsos*

Main category: cs.LG

TL;DR: ControlG：基于控制理论的多任务图自监督学习框架，通过时间分配而非权重混合协调多个预训练目标，解决目标冲突问题


<details>
  <summary>Details</summary>
Motivation: 图自监督学习有多个预训练目标（互信息、重构、对比学习等），但组合使用时存在目标干扰和训练不稳定问题。传统方法采用每次更新混合权重的方式，导致目标冲突、漂移和饥饿三种失败模式

Method: 提出ControlG框架，将多目标图SSL重新定义为反馈控制的时间分配问题：1) 估计每个目标的难度和成对对抗性；2) 通过帕累托感知的对数超体积规划器规划目标预算；3) 使用PID控制器进行调度

Result: 在9个数据集上，ControlG持续优于最先进的基线方法，同时生成可审计的调度计划，揭示哪些目标驱动了学习过程

Conclusion: 多目标图自监督学习的协调本质上是一个时间分配问题，而非简单的权重混合。ControlG通过控制理论框架有效解决了目标冲突问题，提供了更稳定和可解释的训练过程

Abstract: Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.

</details>


### [41] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM是一种基于流策略的离线强化学习方法，通过构造方式强制执行较宽松的支持约束，有效解决OOD问题并保持策略表达能力。


<details>
  <summary>Details</summary>
Motivation: 离线RL面临两个主要挑战：1) OOD误差问题，现有方法通过惩罚统计距离项来约束策略，但这限制了策略改进且不能完全防止OOD动作；2) 最优策略分布可能是多模态且难以表示。现有扩散或流策略方法不清楚如何同时避免OOD误差并保持策略表达能力。

Method: ReFORM基于流策略，通过构造方式强制执行支持约束：1) 学习一个有界源分布的行为克隆流策略，捕捉动作分布的支持集；2) 优化一个反射流，为BC流生成有界噪声同时保持支持集，以最大化性能。

Result: 在OGBench基准的40个具有不同质量数据集的挑战性任务中，ReFORM使用恒定超参数集，在性能曲线图上优于所有经过手动调优超参数的基线方法。

Conclusion: ReFORM通过流策略和支持约束的构造性方法，有效解决了离线RL中的OOD误差问题，同时保持了策略的表达能力，在多个任务上取得了优越性能。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [42] [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)
*Shizheng Wen,Mingyuan Chi,Tianwei Yu,Ben Moseley,Mike Yan Michelis,Pu Ren,Hao Sun,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出统一的算法框架，用于变分结构PDE的数值求解、约束优化和物理信息学习，基于Galerkin离散化和高效的TensorGalerkin张量化系统组装技术。


<details>
  <summary>Details</summary>
Motivation: 现有PDE求解、优化和学习方法通常分离开发，缺乏统一高效的框架。需要一种能同时支持数值求解、约束优化和物理信息学习的通用方法，特别是在GPU上实现高性能计算。

Method: 基于Galerkin离散化变分形式，开发TensorGalerkin框架：通过Python级Map阶段张量化单元操作，然后使用稀疏矩阵乘法进行全局约简，在网格诱导的稀疏图上进行消息传递。

Result: 在2D和3D椭圆、抛物和双曲PDE的无结构网格上测试，相比多种基线方法，在所有目标下游应用中均获得显著的计算效率和精度提升。

Conclusion: 提出的统一框架为变分结构PDE的数值求解、约束优化和物理信息学习提供了高效、准确的解决方案，具有广泛的适用性和计算优势。

Abstract: We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.

</details>


### [43] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 提出混合预测框架，结合QRF和物理约束，为高速公路在不同天气条件下推荐实时安全速度区间


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件显著影响驾驶员能见度和轮胎-路面摩擦，需要调整安全驾驶速度以降低事故风险

Method: 使用分位数回归森林(QRF)预测10分钟窗口内的车辆速度分布，结合基于物理的约束计算安全速度上限，融合预测分位数、限速和物理上限

Result: QRF模型MAE为1.55 mph，96.43%的中位数速度预测误差在5 mph内，PICP(50%)为48.55%，在不同天气类型中具有良好泛化能力

Conclusion: 该模型能响应天气变化并在不同路段泛化，有望实际部署以改善交通安全并减少天气相关事故

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [44] [StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation](https://arxiv.org/abs/2602.05060)
*Heajun An,Qi Zhang,Minqian Liu,Xinyi Zhang,Sang Won Lee,Lifu Huang,Pamela J. Wisniewski,Jin-Hee Cho*

Main category: cs.LG

TL;DR: StagePilot是一个基于离线强化学习的对话代理，通过模拟网络诱骗的阶段化进展来进行预防培训，在保持情感一致性的同时，比基线方法更有效地完成诱骗阶段转换。


<details>
  <summary>Details</summary>
Motivation: 网络诱骗是对青少年的持续威胁，需要主动的教育干预措施来预防。现有的预防培训需要更真实、可控的模拟对话环境来训练青少年识别和应对诱骗行为。

Method: 提出StagePilot离线RL对话代理，使用复合奖励函数平衡用户情感和目标接近度来选择对话阶段，限制阶段间只能向相邻阶段转换以确保真实性和可解释性。通过LLM模拟进行评估。

Result: StagePilot能生成与诱骗动态一致的真实连贯对话。在测试方法中，IQL+AWAC代理在战略规划和情感一致性之间达到最佳平衡，比基线方法更频繁地达到最终阶段（提升43%），同时保持超过70%的情感对齐。

Conclusion: StagePilot为网络诱骗预防培训提供了有效的对话模拟工具，能够生成真实、可控的诱骗对话场景，帮助青少年识别和应对网络诱骗行为。

Abstract: Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.

</details>


### [45] [Does SGD Seek Flatness or Sharpness? An Exactly Solvable Model](https://arxiv.org/abs/2602.05065)
*Yizhou Xu,Pierfrancesco Beneventano,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: SGD没有先验的平坦偏好，而是偏好最小化梯度波动；数据分布唯一决定收敛时的锐度；标签噪声各向同性时偏好平坦极小值，各向异性时偏好锐度。


<details>
  <summary>Details</summary>
Motivation: 现有理论和实证工作假设神经网络损失景观平坦度与性能相关，但关于SGD偏好平坦还是尖锐解的证据相互矛盾。本文旨在通过可解析模型部分但因果性地澄清SGD的平坦寻求行为。

Method: 识别并精确求解一个展示训练中平坦化和锐化行为的可解析模型。该模型中SGD训练没有先验的平坦偏好，只有最小化梯度波动的偏好。

Result: 数据分布唯一决定收敛时的锐度：标签噪声各向同性时偏好平坦极小值；标签噪声各向异性时偏好锐度，并可收敛到任意尖锐解，取决于标签噪声谱的不平衡程度。在MLP、RNN和transformer等不同架构的受控设置中复现了这一关键见解。

Conclusion: SGD的平坦寻求行为不是先验偏好，而是最小化梯度波动的结果。数据分布（特别是标签噪声的各向同性/异性）是决定收敛锐度的唯一因素，这为理解损失景观平坦度与性能的关系提供了新的因果视角。

Abstract: A large body of theory and empirical work hypothesizes a connection between the flatness of a neural network's loss landscape during training and its performance. However, there have been conceptually opposite pieces of evidence regarding when SGD prefers flatter or sharper solutions during training. In this work, we partially but causally clarify the flatness-seeking behavior of SGD by identifying and exactly solving an analytically solvable model that exhibits both flattening and sharpening behavior during training. In this model, the SGD training has no \textit{a priori} preference for flatness, but only a preference for minimal gradient fluctuations. This leads to the insight that, at least within this model, it is data distribution that uniquely determines the sharpness at convergence, and that a flat minimum is preferred if and only if the noise in the labels is isotropic across all output dimensions. When the noise in the labels is anisotropic, the model instead prefers sharpness and can converge to an arbitrarily sharp solution, depending on the imbalance in the noise in the labels spectrum. We reproduce this key insight in controlled settings with different model architectures such as MLP, RNN, and transformers.

</details>


### [46] [E-Globe: Scalable $ε$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching](https://arxiv.org/abs/2602.05068)
*Wenting Li,Saif R. Kazi,Russell Bent,Duo Zhou,Huan Zhang*

Main category: cs.LG

TL;DR: 提出一种混合验证器，在分支定界框架中同时收紧上下界，通过精确的非线性互补约束规划保持ReLU输入输出图，实现高效验证


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全关键应用中部署受到鲁棒性担忧阻碍，现有形式化验证方法面临可扩展性与完备性的权衡问题

Method: 在分支定界框架中提出混合验证器，使用精确非线性互补约束规划进行上界估计，保持ReLU输入输出图；采用热启动NLP求解和模式对齐强分支加速验证

Result: 在MNIST和CIFAR-10上，相比PGD方法在扰动半径跨越三个数量级时获得更紧的上界，相比基于MIP的验证实现显著端到端加速

Conclusion: 该方法通过精确非线性互补约束规划和多种优化技术，在神经网络形式化验证中实现了更好的可扩展性与完备性平衡

Abstract: Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $ε-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.

</details>


### [47] [Reliable Explanations or Random Noise? A Reliability Metric for XAI](https://arxiv.org/abs/2602.05082)
*Poushali Sengupta,Sabita Maharjan,Frank Eliassen,Shashi Raj Pandey,Yan Zhang*

Main category: cs.LG

TL;DR: 提出Explanation Reliability Index (ERI)指标家族，用于量化解释方法在现实部署条件下的可靠性，包括对输入扰动、特征冗余、模型演化和分布偏移的稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前复杂机器学习模型的解释方法（如SHAP和IG）虽然在公理上有良好动机，但在现实部署条件下（如小输入扰动、相关表示、微小模型更新）可能表现出显著不稳定性，这损害了解释的可靠性，而现有方法缺乏系统量化这种可靠性的指标。

Method: 提出ERI指标家族，基于四个可靠性公理：对输入扰动的鲁棒性、特征冗余下的一致性、模型演化的平滑性、分布偏移的韧性。为每个公理提供形式化保证（如Lipschitz型边界和时间稳定性结果）。提出ERI-T用于序列模型的时序可靠性度量，并建立ERI-Bench基准来系统测试解释可靠性。

Result: 实验结果显示流行解释方法存在广泛的可靠性失败，解释在现实部署条件下可能不稳定。ERI能够暴露和量化这些不稳定性，为解释可靠性提供原则性评估。

Conclusion: ERI能够系统量化解释方法的可靠性，支持更可信的可解释AI系统，通过揭示现实部署条件下的不稳定性问题，为解释可靠性的评估提供了理论基础和实用工具。

Abstract: In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.

</details>


### [48] [Individual Fairness In Strategic Classification](https://arxiv.org/abs/2602.05084)
*Zhiqun Zuo,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 该论文研究战略分类中的个体公平性问题，证明确定性阈值分类器违反个体公平，提出使用随机分类器实现个体公平，并通过线性规划找到最优解。


<details>
  <summary>Details</summary>
Motivation: 战略分类中个体修改特征以影响机器学习决策，现有研究主要关注群体公平，而个体公平问题尚未充分探索。需要解决战略分类环境下的个体公平性挑战。

Method: 分析阈值分类器，证明确定性阈值违反个体公平；提出使用随机分类器实现个体公平的条件；通过线性规划问题寻找最优且个体公平的随机分类器；将方法扩展到群体公平概念。

Result: 在真实世界数据集上的实验证实，该方法能有效减轻不公平性，并改善公平性与准确性的权衡关系。

Conclusion: 随机分类器可以在战略分类中实现个体公平，提出的线性规划方法能找到最优解，该方法也可扩展到群体公平，为战略分类中的公平性问题提供了有效解决方案。

Abstract: Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.

</details>


### [49] [Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling](https://arxiv.org/abs/2602.05087)
*Parsa Vares*

Main category: cs.LG

TL;DR: AutoDiscover：将主动学习重构为在线决策问题，通过自适应代理动态管理查询策略组合，在文献筛选任务中实现更高效率


<details>
  <summary>Details</summary>
Motivation: 系统文献综述（SLR）是循证研究的基础，但随着科学产出增长，人工筛选成为瓶颈。筛选面临相关研究低流行率和专家决策稀缺昂贵的问题。传统主动学习系统有帮助，但通常依赖固定的查询策略，这些静态策略无法随时间适应，且忽略了科学文献网络的关系结构。

Method: 1. 将主动学习重构为在线决策问题，由自适应代理驱动；2. 将文献建模为捕获文档、作者和元数据关系的异构图；3. 使用异构图注意力网络（HAN）学习节点表示；4. 采用折扣汤普森采样（DTS）代理动态管理查询策略组合；5. 在实时人机循环标签下，平衡探索与利用，适应非平稳的审查动态。

Result: 在26个数据集的SYNERGY基准测试中，AutoDiscover比静态主动学习基线实现了更高的筛选效率。关键的是，该代理通过从最少的初始标签中引导发现来缓解冷启动问题，而静态方法在此情况下会失败。还开发了TS-Insight开源可视化分析仪表板来解释、验证和诊断代理决策。

Conclusion: AutoDiscover框架通过将主动学习重构为自适应决策问题，显著加速了在专家标签稀缺和相关研究低流行率情况下的系统文献综述筛选。结合可视化工具TS-Insight，为文献筛选提供了更高效、可解释的解决方案。

Abstract: Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.

</details>


### [50] [Unbiased Single-Queried Gradient for Combinatorial Objective](https://arxiv.org/abs/2602.05119)
*Thanawat Sornwanee*

Main category: cs.LG

TL;DR: 提出一种用于组合优化问题的随机梯度方法，只需单次查询即可获得无偏梯度估计，包含REINFORCE作为特例


<details>
  <summary>Details</summary>
Motivation: 组合优化问题在概率重构中常面临超立方体上的优化，对应伯努利概率参数。组合性质导致精确梯度计算需要多次查询，计算成本高。

Method: 提出一种随机梯度方法，通过单次查询组合函数即可获得无偏梯度估计。该方法通过重要性采样包含REINFORCE算法，并扩展为一类新的随机梯度。

Result: 该方法提供了一种计算高效的梯度估计方案，减少了组合优化中梯度计算所需的查询次数，从多次查询降低到单次查询。

Conclusion: 提出的随机梯度方法为组合优化问题提供了高效的无偏梯度估计框架，统一了现有方法并扩展了新的梯度估计类别。

Abstract: In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.

</details>


### [51] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 提出RRD框架，通过递归分解-过滤循环优化评分标准，提升LLM评估准确性和强化学习奖励信号


<details>
  <summary>Details</summary>
Motivation: 现有评分标准生成方法难以控制，存在覆盖不全、维度混淆、偏好方向错位、冗余和高度相关标准等问题，导致评估准确性下降和强化微调奖励信号不佳

Method: 提出RRD框架，采用递归分解-过滤循环：分解粗粒度评分标准为细粒度判别性标准，过滤错位和冗余标准，使用相关性感知加权方案防止高度相关标准过度代表

Result: 在JudgeBench和PPE评估中显著提升GPT-4o和Llama3.1-405B的偏好判断准确性，在WildChat强化微调中奖励提升160%（Qwen3-4B）和60%（Llama3.1-8B），优于基线方法

Conclusion: RRD为开放领域LLM评估和奖励建模提供了可扩展且可解释的基础，通过递归评分标准优化显著提升评估准确性和强化学习效果

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [52] [SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines](https://arxiv.org/abs/2602.05134)
*Olga Ovcharenko,Matthias Boehm,Sebastian Schelter*

Main category: cs.LG

TL;DR: SemPipes：一种声明式编程模型，将LLM驱动的语义数据算子集成到表格ML管道中，通过代码合成自动优化数据操作


<details>
  <summary>Details</summary>
Motivation: 现实世界表格机器学习需要复杂的数据准备管道，设计这些管道需要大量领域专业知识和工程努力，因此需要探索LLM如何通过代码合成支持表格ML

Method: 引入SemPipes声明式编程模型，集成LLM驱动的语义数据算子；算子用自然语言指定数据转换，运行时系统执行；训练时基于数据特征、算子指令和管道上下文合成自定义算子实现；通过基于进化搜索的LLM代码合成自动优化管道中的数据操作

Result: 在多样化表格ML任务中评估SemPipes，显示语义算子显著提高了专家设计和代理生成管道的端到端预测性能，同时降低了管道复杂性

Conclusion: SemPipes通过LLM驱动的语义算子成功简化了表格ML管道设计，提高了性能并降低了复杂性，为自动化数据准备提供了有效框架

Abstract: Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.

</details>


### [53] [Decoupled Orthogonal Dynamics: Regularization for Deep Network Optimizers](https://arxiv.org/abs/2602.05136)
*Hao Chen,Jinghui Yuan,Hanmin Zhang*

Main category: cs.LG

TL;DR: AdamW中的标准权重衰减并非最优，存在径向拉锯战问题。作者提出正交动力学解耦方法AdamO，将参数范数控制与自适应梯度缩放分离，在多个任务上优于AdamW。


<details>
  <summary>Details</summary>
Motivation: AdamW虽然将权重衰减与自适应梯度缩放解耦，但仍存在根本冲突：径向拉锯战。梯度倾向于增加参数范数以扩展有效容量，而权重衰减则不加区分地抑制范数增长，这种推拉相互作用导致径向振荡，向Adam的第二矩估计注入噪声，可能损害精细的切向特征学习。

Method: 提出正交动力学解耦方法，实例化为AdamO：使用SGD风格的更新处理一维范数控制，而Adam的自适应预处理仅限于切向子空间。AdamO进一步包含曲率自适应径向步长调整、架构感知规则和投影，用于尺度不变层和低维参数。

Result: 在视觉和语言任务上的实验表明，AdamO在泛化能力和稳定性方面优于AdamW，且没有引入额外的复杂约束。

Conclusion: 参数的大小和方向在优化器动力学中扮演不同角色，应该被解耦。AdamO通过正交动力学解耦实现了这种分离，提供了比AdamW更好的优化性能。

Abstract: Is the standard weight decay in AdamW truly optimal? Although AdamW decouples weight decay from adaptive gradient scaling, a fundamental conflict remains: the Radial Tug-of-War. In deep learning, gradients tend to increase parameter norms to expand effective capacity while steering directions to learn features, whereas weight decay indiscriminately suppresses norm growth. This push--pull interaction induces radial oscillations, injecting noise into Adam's second-moment estimates and potentially degrading delicate tangential feature learning. We argue that magnitude and direction play distinct roles and should be decoupled in optimizer dynamics. We propose Orthogonal Dynamics Decoupling and instantiate it as AdamO: an SGD-style update handles the one-dimensional norm control, while Adam's adaptive preconditioning is confined to the tangential subspace. AdamO further incorporates curvature-adaptive radial step sizing and architecture-aware rules and projections for scale-invariant layers and low-dimensional parameters. Experiments on vision and language tasks show that AdamO improves generalization and stability over AdamW without introducing additional complex constraints.

</details>


### [54] [Adaptive Exploration for Latent-State Bandits](https://arxiv.org/abs/2602.05139)
*Jikai Jin,Kenneth Hung,Sanath Kumar Krishnamurthy,Baoyi Shi,Congshan Zhang*

Main category: cs.LG

TL;DR: 提出一种无需显式状态建模的bandit算法，利用滞后上下文特征和协调探测策略处理具有隐藏时变状态的环境，优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 经典多臂老虎机算法在处理具有隐藏、时变状态的环境时常常失效，这些未观测的混杂因素会导致奖励估计偏差和状态信息有限的问题。

Method: 引入一系列状态模型无关的老虎机算法，利用滞后上下文特征和协调探测策略，隐式跟踪潜在状态并区分状态依赖的奖励模式，无需显式状态建模。

Result: 在多样化设置中的实证结果表明，该方法相比经典方法具有优越性能，并能提供实际应用中算法选择的实用建议。

Conclusion: 该方法结合计算效率和鲁棒适应性，能在非平稳奖励环境中学习最优策略，为具有隐藏状态的实际应用提供了有效的解决方案。

Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.

</details>


### [55] [Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing](https://arxiv.org/abs/2602.05144)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 研究机器学习公平性在历史数据训练但部署环境变化时的挑战，特别是群体条件先验概率偏移(GPPS)。证明了公平性指标在GPPS下的不同行为：错误率公平性(equalized odds)结构不变，而接受率公平性(demographic parity)会漂移且不可避免。提出了无需目标域标签即可估计目标域风险和公平性指标的方法，并开发了TAP-GPPS算法在目标域实现人口统计公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常在历史数据上训练和评估公平性，但部署环境条件会发生变化。特别常见的是不同人口群体中正结果发生率变化不同，例如疾病率在不同人群中上升速度不同，或经济条件对贷款违约率的影响不均等。需要研究这种群体条件先验概率偏移(GPPS)对公平性的影响。

Method: 1. 理论分析GPPS下公平性指标的行为：证明错误率公平性(equalized odds)在GPPS下结构不变，而接受率公平性(demographic parity)会漂移且不可避免(shift-robust impossibility)。2. 利用ROC量在GPPS下的不变性，仅使用源标签和无标签目标数据即可一致估计目标域风险和公平性指标。3. 提出TAP-GPPS算法：从无标签数据估计发生率，校正后验概率，选择阈值以满足目标域的人口统计公平性。

Result: 1. 理论证明了公平性指标在GPPS下的基本二分法：错误率公平性不变，接受率公平性漂移不可避免。2. 展示了无需目标标签即可识别目标域风险和公平性指标的方法，具有有限样本保证。3. TAP-GPPS算法在实验中验证了理论预测，能以最小效用损失实现目标公平性。

Conclusion: 该研究揭示了机器学习公平性在分布偏移下的重要特性：某些公平性指标(如equalized odds)在GPPS下具有结构不变性，而其他指标(如demographic parity)会漂移且无法避免。提出的方法允许在无目标标签的情况下估计目标域公平性，TAP-GPPS算法能有效在目标域实现人口统计公平性，为实际部署中的公平性维护提供了理论和方法支持。

Abstract: Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.

</details>


### [56] [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)
*Jiyoung Park,Hankyu Jang,Changseok Song,Wookeun Jung*

Main category: cs.LG

TL;DR: TIDE是一个集成到LLM推理系统中的在线草稿自适应框架，通过重用推理过程中的隐藏状态实现零开销草稿训练，自适应控制推测执行时机，利用异构集群提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推测解码可以加速LLM推理，但在实际应用中面临挑战：工作负载动态变化，系统级约束复杂，现有方法难以充分利用异构集群资源。

Method: TIDE框架：1）重用目标模型推理生成的隐藏状态作为训练信号，实现零开销草稿自适应；2）自适应运行时控制，仅在有益时激活推测和训练；3）利用异构集群，将解耦的推理和训练映射到合适的GPU类型。

Result: 在多样化真实工作负载中，TIDE相比静态推测解码实现最高1.15倍吞吐量提升，相比重新计算训练信号的方法减少1.67倍草稿训练时间。

Conclusion: TIDE通过系统原生集成、零开销自适应训练和智能资源利用，有效解决了推测解码在实际部署中的挑战，显著提升了LLM推理效率。

Abstract: Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.

</details>


### [57] [Cross-talk based multi-task learning for fault classification of physically coupled machine system](https://arxiv.org/abs/2602.05146)
*Wonjun Yi,Rismaya Kumar Mishra,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 该论文提出了一种基于交叉对话结构的多任务学习框架，利用信号中自然嵌入的物理耦合信息，同时学习故障条件和相关物理变量，在无人机故障和电机复合故障两个基准数据集上优于单任务模型和共享主干多任务模型。


<details>
  <summary>Details</summary>
Motivation: 机器系统产生的信号中，故障条件与各种物理变量之间存在物理耦合。现有故障分类研究大多仅依赖直接故障标签，忽略了信号中自然嵌入的其他物理耦合信息。本文旨在利用这种耦合关系，通过多任务学习框架同时学习故障条件和相关物理变量。

Method: 采用交叉对话结构的多任务学习框架，基于先前提出的残差神经降维模型。交叉对话结构允许任务间通过交叉对话层进行受控信息交换，同时防止负迁移。在两个物理耦合显著的基准数据集上进行验证：无人机故障数据集（机器类型和操纵方向显著改变信号频率成分）和电机复合故障数据集（内圈故障、外圈故障、不对中、不平衡等故障相互耦合）。

Result: 在两个基准数据集上，残差神经降维模型均一致优于单任务模型、合并所有标签组合的多类模型以及共享主干多任务模型。对于电机复合故障，还测试了使用单通道数据或多通道数据作为分类器输入时的性能。

Conclusion: 通过利用信号中自然嵌入的物理耦合信息，交叉对话结构的多任务学习框架能够更有效地进行故障分类。该方法在无人机故障和电机复合故障两个基准数据集上均表现出优越性能，验证了利用物理耦合信息进行多任务学习的有效性。

Abstract: Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.

</details>


### [58] [CoSA: Compressed Sensing-Based Adaptation of Large Language Models](https://arxiv.org/abs/2602.05148)
*Songtao Wei,Yi Li,Bohan Zhang,Zhichun Guo,Ying Huang,Yuede Ji,Miao Yin,Guanpeng Li,Bingzhe Li*

Main category: cs.LG

TL;DR: CoSA提出了一种基于压缩感知理论的新型参数高效微调方法，通过固定随机投影矩阵和可学习核心来编码权重更新，突破了传统低秩方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA和PiSSA）依赖低秩分解假设，但在任务特定适应场景中，当奇异值分布相对均匀时，低秩假设可能限制表达能力。需要一种更灵活高效的适应方法。

Method: CoSA基于压缩感知理论，使用固定随机投影矩阵将权重更新编码到低维空间，并通过可学习的紧凑核心进行表示，避免了低秩限制，实现了多尺度的模型适应。

Result: 在10个多样化任务（包括自然语言理解和生成）上评估，使用RoBERTa、Llama和Qwen家族的5个不同规模模型，CoSA始终匹配或优于最先进的PEFT方法。

Conclusion: CoSA为高效且表达力强的多尺度模型适应提供了一个原则性视角，突破了传统低秩方法的限制，在保持参数效率的同时提高了适应能力。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.

</details>


### [59] [Position: Capability Control Should be a Separate Goal From Alignment](https://arxiv.org/abs/2602.05164)
*Shoaib Ahmed Siddiqui,Eleni Triantafillou,David Krueger,Adrian Weller*

Main category: cs.LG

TL;DR: 该立场论文主张将能力控制视为与对齐不同的独立目标，提出在模型生命周期中通过数据层、学习层和系统层三层机制进行能力控制，并倡导采用深度防御方法组合多层控制。


<details>
  <summary>Details</summary>
Motivation: 基础模型在广泛数据分布上训练，具备通用能力，既能支持下游应用，也扩大了潜在滥用和失败的风险空间。当前对齐方法通常是上下文和偏好驱动的，而能力控制需要施加硬性操作限制，包括对抗性诱导下的行为限制。

Method: 提出三层能力控制机制：1) 数据层控制：通过训练数据分布控制；2) 学习层控制：通过权重或表示层干预；3) 系统层控制：通过部署后对输入、输出和行为的护栏。由于每层单独使用时都有特征性失效模式，倡导采用深度防御方法，在整个技术栈中组合互补的控制机制。

Result: 建立了能力控制的概念框架，明确了与对齐的区别，提出了系统化的三层控制架构，并指出了实现有效控制的关键挑战，包括知识的双重用途性质和组合泛化问题。

Conclusion: 能力控制应作为独立于对齐的目标来处理，需要在整个模型生命周期中采用多层防御策略。未来研究需要解决知识双重用途和组合泛化等核心挑战，以实现可靠的能力控制。

Abstract: Foundation models are trained on broad data distributions, yielding generalist capabilities that enable many downstream applications but also expand the space of potential misuse and failures. This position paper argues that capability control -- imposing restrictions on permissible model behavior -- should be treated as a distinct goal from alignment. While alignment is often context and preference-driven, capability control aims to impose hard operational limits on permissible behaviors, including under adversarial elicitation. We organize capability control mechanisms across the model lifecycle into three layers: (i) data-based control of the training distribution, (ii) learning-based control via weight- or representation-level interventions, and (iii) system-based control via post-deployment guardrails over inputs, outputs, and actions. Because each layer has characteristic failure modes when used in isolation, we advocate for a defense-in-depth approach that composes complementary controls across the full stack. We further outline key open challenges in achieving such control, including the dual-use nature of knowledge and compositional generalization.

</details>


### [60] [EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization](https://arxiv.org/abs/2602.05165)
*Kevin Han,Yuhang Zhou,Mingze Gao,Gedi Zhou,Serena Li,Abhishek Kumar,Xiangjun Fan,Weiwei Li,Lizhu Zhang*

Main category: cs.LG

TL;DR: EBPO提出了一种新的强化学习框架，通过经验贝叶斯方法将局部组统计与全局先验相结合，解决了GRPO在小样本组和高失败率场景下的稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO存在稳定性问题：在小样本组下估计方差高，在失败场景（所有响应都获得零奖励）中梯度信号消失，限制了强化学习在提升LLM推理能力方面的效果。

Method: EBPO采用经验贝叶斯方法，通过收缩估计器动态平衡局部组统计与全局先验（使用Welford在线算法更新），正则化局部基线估计，避免方差过大和梯度消失。

Result: 理论上证明EBPO相比GRPO具有更低的均方误差、有界的熵衰减和失败场景下的非消失惩罚信号；实验上在AIME和OlympiadBench等基准测试中优于GRPO和其他基线，训练稳定性更好，小样本组下也能获得高性能。

Conclusion: EBPO通过经验贝叶斯正则化有效解决了RLVR中的稳定性问题，为LLM推理能力的强化学习优化提供了更可靠的方法，特别适合小样本和困难场景。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.

</details>


### [61] [Benchmarking Artificial Intelligence Models for Daily Coastal Hypoxia Forecasting](https://arxiv.org/abs/2602.05178)
*Magesh Rajasekaran,Md Saiful Sajol,Chris Alvin,Supratik Mukhopadhyay,Yanda Ou,Z. George Xue*

Main category: cs.LG

TL;DR: 该研究比较了四种深度学习架构（BiLSTM、Medformer、ST-Transformer、TCN）用于墨西哥湾北部缺氧事件的每日分类预测，其中ST-Transformer在所有指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 墨西哥湾北部海岸缺氧是一个持续的生态和经济问题，现有季节性模型预测过于粗糙，无法满足每日响应的生态系统管理需求，需要开发更精细的预测方法。

Method: 使用2009-2020年的每日后报数据训练四种深度学习模型（BiLSTM、Medformer、ST-Transformer、TCN），输入特征包括水柱分层、沉积物耗氧量和温度依赖性分解率，使用2020-2024年数据进行测试，并采用McNemar方法进行统计显著性检验。

Result: 所有模型都取得了高分类精度和强判别能力，其中ST-Transformer在所有指标和测试期间表现最佳（AUC-ROC：0.982-0.992），并建立了可重复的实时缺氧预测框架。

Conclusion: 该研究提供了一个可操作的实时缺氧预测框架，支持环境海洋建模系统和生态系统恢复力研究，其中ST-Transformer是最优的深度学习架构选择。

Abstract: Coastal hypoxia, especially in the northern part of Gulf of Mexico, presents a persistent ecological and economic concern. Seasonal models offer coarse forecasts that miss the fine-scale variability needed for daily, responsive ecosystem management. We present study that compares four deep learning architectures for daily hypoxia classification: Bidirectional Long Short-Term Memory (BiLSTM), Medformer (Medical Transformer), Spatio-Temporal Transformer (ST-Transformer), and Temporal Convolutional Network (TCN). We trained our models with twelve years of daily hindcast data from 2009-2020 Our training data consists of 2009-2020 hindcast data from a coupled hydrodynamic-biogeochemical model. Similarly, we use hindcast data from 2020 through 2024 as a test data. We constructed classification models incorporating water column stratification, sediment oxygen consumption, and temperature-dependent decomposition rates. We evaluated each architectures using the same data preprocessing, input/output formulation, and validation protocols. Each model achieved high classification accuracy and strong discriminative ability with ST-Transformer achieving the highest performance across all metrics and tests periods (AUC-ROC: 0.982-0.992). We also employed McNemar's method to identify statistically significant differences in model predictions. Our contribution is a reproducible framework for operational real-time hypoxia prediction that can support broader efforts in the environmental and ocean modeling systems community and in ecosystem resilience. The source code is available https://github.com/rmagesh148/hypoxia-ai/

</details>


### [62] [Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.05183)
*John Yan,Michael Yu,Yuqi Sun,Alexander Duffy,Tyler Marques,Matthew Lyle Olson*

Main category: cs.LG

TL;DR: 该研究应用稀疏自编码器(SAEs)和LLM摘要方法分析复杂强化学习环境中大语言模型的训练动态，提出Meta-Autointerp方法将SAE特征分组为可解释假设，发现多种细粒度行为模式，但发现大多数自动生成的特征解释对人类帮助有限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂强化学习、多智能体环境中训练越来越普遍，但训练过程中行为变化难以理解。需要数据中心的可解释性方法来理解这些复杂环境中的模型行为动态。

Method: 1. 在Full-Press Diplomacy环境的大规模强化学习训练中应用预训练的稀疏自编码器(SAEs)和LLM摘要方法；2. 提出Meta-Autointerp方法，将SAE特征分组为可解释的训练动态假设；3. 通过自动评估和用户研究验证发现的特征。

Result: 1. 发现了细粒度行为：角色扮演模式、退化输出、语言切换、高级战略行为和环境特定bug；2. 自动评估显示90%发现的SAE元特征显著，发现意外的奖励黑客行为；3. 用户研究发现大多数SAE特征和LLM生成假设对人类帮助有限甚至有害，但部分SAE衍生假设对下游任务有预测价值；4. 通过增强未训练智能体的系统提示，分数提高了+14.2%。

Conclusion: SAEs和LLM摘要器提供了对智能体行为的互补视角，该框架为未来数据中心的可解释性工作提供了实用起点，有助于确保LLM在整个训练过程中的可信行为。

Abstract: Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.

</details>


### [63] [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)
*Chun-Wun Cheng,Carola-Bibiane Schönlieb,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: SpectraKAN：一种新型谱神经算子，通过输入自适应的谱卷积核来捕捉多尺度、状态依赖的PDE动力学，相比传统静态傅里叶算子性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有谱神经算子（如FNO）使用静态傅里叶核，无法有效捕捉多尺度、状态依赖和各向异性的PDE动力学，限制了其在复杂系统建模中的能力。

Method: 提出SpectraKAN算子：1）从时空历史中提取紧凑全局表示；2）通过单查询交叉注意力机制调制多尺度傅里叶主干；3）实现输入自适应的谱积分算子，保持谱混合效率的同时适应系统状态。

Result: 在多种PDE基准测试中达到最先进性能，RMSE降低高达49%，在具有挑战性的时空预测任务上表现尤为突出。

Conclusion: SpectraKAN通过输入自适应的谱算子设计，有效解决了传统静态谱算子的局限性，为复杂PDE系统的学习提供了更强大的框架，同时保持了计算效率。

Abstract: Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.

</details>


### [64] [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)
*Wentao Ni,Kangqi Zhang,Zhongming Yu,Oren Nelson,Mingu Lee,Hong Cai,Fatih Porikli,Jongryool Kim,Zhijian Liu,Jishen Zhao*

Main category: cs.LG

TL;DR: Double-P：一种分层稀疏注意力框架，通过两级top-p选择优化长上下文推理，在保持接近零精度损失的同时，将注意力计算开销减少1.8倍，解码速度提升1.3倍。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文推理成为大语言模型的核心，注意力机制在增长的键值缓存上成为解码瓶颈。现有稀疏注意力方法无法同时优化top-p精度、选择开销和稀疏注意力成本。

Method: 提出Double-P分层稀疏注意力框架：1) 在聚类级别使用大小加权质心进行粗粒度top-p估计；2) 自适应地通过第二级top-p阶段分配token级注意力计算。

Result: 在长上下文基准测试中，Double-P保持接近零的精度损失，将注意力计算开销减少达1.8倍，端到端解码速度比最先进的固定预算稀疏注意力方法快1.3倍。

Conclusion: Double-P通过分层top-p选择有效解决了稀疏注意力在精度、选择开销和计算成本之间的权衡问题，为可扩展的长上下文推理提供了高效解决方案。

Abstract: As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.

</details>


### [65] [Extreme Weather Nowcasting via Local Precipitation Pattern Prediction](https://arxiv.org/abs/2602.05204)
*Changhoon Song,Teng Yuan Chang,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出exPreCast框架和平衡雷达数据集，用于高效精确的降水临近预报，在正常和极端降雨情况下均表现优异


<details>
  <summary>Details</summary>
Motivation: 现有降水临近预报方法存在局限性：扩散模型计算成本高不适合实时应用，确定性模型偏向正常降雨，且现有基准数据集不平衡（要么普通降雨主导，要么仅极端降雨），限制了实际应用

Method: 提出exPreCast确定性框架，包含局部时空注意力机制、纹理保持的立方双上采样解码器和时间提取器；同时构建了韩国气象厅的平衡雷达数据集，包含普通降水和极端事件

Result: 在SEVIR、MeteoNet基准数据集和新建的KMA平衡数据集上均达到最先进性能，能在正常和极端降雨情况下提供准确可靠的临近预报

Conclusion: exPreCast框架结合平衡数据集解决了降水临近预报的关键挑战，实现了高效、精确且适用于实际应用的预报系统

Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.

</details>


### [66] [Disentangled Representation Learning via Flow Matching](https://arxiv.org/abs/2602.05214)
*Jinjin Chi,Taoping Liu,Mengtao Yin,Ximing Li,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出基于流匹配的框架进行解耦表示学习，通过正交正则化抑制因子间干扰，在多个数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法虽然通过归纳偏置鼓励因子独立性，但经常缺乏强语义对齐。需要开发能同时保证因子独立性和语义对齐的解耦表示学习方法。

Method: 提出流匹配框架，将解耦表示为在紧凑潜在空间中学习因子条件流。引入非重叠（正交）正则化器抑制跨因子干扰，减少因子间信息泄漏。

Result: 在多个数据集上的广泛实验表明，相比代表性基线方法，该方法在解耦分数、可控性和样本保真度方面都有持续改进。

Conclusion: 流匹配框架结合正交正则化能有效实现解耦表示学习，在语义对齐和因子独立性方面优于现有方法。

Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.

</details>


### [67] [Private Prediction via Shrinkage](https://arxiv.org/abs/2602.05219)
*Chao Yan*

Main category: cs.LG

TL;DR: 该论文研究了差分隐私预测问题，通过改进算法将查询数量T的依赖从√T降低到polylog(T)，显著减少了所需标记样本数量。


<details>
  <summary>Details</summary>
Motivation: 差分隐私预测中，标准组合方法导致查询数量T的平方根依赖，这在实际应用中需要大量标记样本。研究旨在减少这种依赖，使差分隐私预测在流式设置中更实用。

Method: 针对两种不同场景设计算法：1) 对于任意概念类的非适应性在线对手，使用VC维分析；2) 对于半空间概念的自适应在线对手，利用几何特性。两种方法都实现了从√T到polylog(T)的改进。

Result: 对于任意概念类，所需标记样本数减少为|S|=Õ(VC(C)^{3.5}log^{3.5}T)；对于半空间概念，进一步减少为|S|=Õ(d^{5.5}log T)。相比标准组合的√T依赖，实现了指数级改进。

Conclusion: 在流式差分隐私预测中，通过新算法设计可以显著减少查询数量对所需标记样本的影响，使差分隐私预测在实际应用中更加可行，特别是对于高维半空间等概念类。

Abstract: We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\sqrt{T}$ dependence for $T$ queries.
  We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \tilde{O}(VC(\mathcal{C})^{3.5}\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\mathbb{R}^d$, we obtain $|S|=\tilde{O}\left(d^{5.5}\log T\right)$.

</details>


### [68] [ZeroS: Zero-Sum Linear Attention for Efficient Transformers](https://arxiv.org/abs/2602.05230)
*Jiecheng Lu,Xu Han,Yan Sun,Viresh Pati,Yubin Kim,Siddhartha Somani,Shihao Yang*

Main category: cs.LG

TL;DR: ZeroS线性注意力通过移除常数项和重加权实现正负权重，在保持O(N)复杂度的同时性能媲美标准softmax注意力


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力方法虽然具有O(N)复杂度，但性能通常不如标准softmax注意力。研究发现这主要受限于两个根本问题：1) 仅限于凸组合，只能进行加性信息融合；2) 均匀累积权重偏差在长上下文中会稀释注意力

Method: 提出Zero-Sum线性注意力(ZeroS)，通过移除常数零阶项1/t并重加权剩余的零和softmax残差。这种修改创建了数学稳定的权重，允许正负值，使单个注意力层能够执行对比操作，同时保持O(N)复杂度

Result: 理论上，ZeroS相比凸组合扩展了可表示函数集。实证上，在各种序列建模基准测试中，ZeroS匹配甚至超过了标准softmax注意力的性能

Conclusion: ZeroS线性注意力解决了传统线性注意力的根本限制，在保持线性复杂度的同时实现了与标准注意力相当甚至更好的性能，为高效长序列建模提供了有前景的解决方案

Abstract: Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.

</details>


### [69] [Balanced Anomaly-guided Ego-graph Diffusion Model for Inductive Graph Anomaly Detection](https://arxiv.org/abs/2602.05232)
*Chunyu Wei,Siyuan He,Yu Wang,Yueguo Chen,Yunhai Wang,Bing Bai,Yidong Zhang,Yong Xie,Shunming Zhang,Fei Wang*

Main category: cs.LG

TL;DR: 提出一个数据中心的动态图异常检测框架，通过离散自我图扩散模型生成异常结构，结合课程异常增强机制解决动态网络和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前图异常检测面临两大挑战：1) 大多数方法采用静态的传导学习范式，不适合动态演化网络；2) 异常节点稀少导致的极端类别不平衡问题，使模型在未见异常上泛化能力差。这两个挑战相互关联：静态框架限制了有效数据增强，而类别不平衡加剧了归纳学习中的模型偏差。

Method: 提出一个数据中心框架，整合动态图建模与平衡异常合成：1) 离散自我图扩散模型，捕捉异常局部拓扑结构以生成符合异常结构分布的自我图；2) 课程异常增强机制，在训练过程中动态调整合成数据生成，专注于未被充分表示的异常模式以提高检测和泛化能力。

Result: 在五个数据集上的实验证明了该框架的有效性。

Conclusion: 通过结合动态图建模和平衡异常合成的数据中心方法，能够有效解决图异常检测中的动态网络适应和类别不平衡问题，提高检测性能和泛化能力。

Abstract: Graph anomaly detection (GAD) is crucial in applications like fraud detection and cybersecurity. Despite recent advancements using graph neural networks (GNNs), two major challenges persist. At the model level, most methods adopt a transductive learning paradigm, which assumes static graph structures, making them unsuitable for dynamic, evolving networks. At the data level, the extreme class imbalance, where anomalous nodes are rare, leads to biased models that fail to generalize to unseen anomalies. These challenges are interdependent: static transductive frameworks limit effective data augmentation, while imbalance exacerbates model distortion in inductive learning settings. To address these challenges, we propose a novel data-centric framework that integrates dynamic graph modeling with balanced anomaly synthesis. Our framework features: (1) a discrete ego-graph diffusion model, which captures the local topology of anomalies to generate ego-graphs aligned with anomalous structural distribution, and (2) a curriculum anomaly augmentation mechanism, which dynamically adjusts synthetic data generation during training, focusing on underrepresented anomaly patterns to improve detection and generalization. Experiments on five datasets demonstrate that the effectiveness of our framework.

</details>


### [70] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出Concept DAS（CDAS）方法，通过分布式对齐搜索原则进行模型干预，使用分布匹配目标而非概率最大化，实现更忠实稳定的模型控制


<details>
  <summary>Details</summary>
Motivation: 现有基于干预的模型控制方法容易过拟合且效果不佳，因为有效控制需要识别内部机制而非强制外部偏好

Method: 基于分布式对齐搜索（DAS）原则，采用分布式交换干预（DII）机制，引入针对控制任务的分布匹配目标，将干预输出分布与反事实分布对齐

Result: 在AxBench基准测试中，CDAS不一定优于偏好优化方法，但能从模型规模扩大中获益更多；在安全相关案例中能系统控制同时保持模型通用能力

Conclusion: CDAS是偏好优化方法的补充，在特定条件下构成基于干预的模型控制的稳健方法

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [71] [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)
*Boxiang Zhang,Baijian Yang*

Main category: cs.LG

TL;DR: CORP是一种无需标签、梯度或微调的单次结构化剪枝框架，通过闭式岭回归补偿剪枝后的表示误差，在严格后训练约束下高效压缩Vision Transformers。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers计算和内存成本高，现有结构化剪枝方法依赖重训练或多阶段优化，限制了后训练部署。需要一种在严格后训练约束下（仅使用少量无标签校准数据）的高效剪枝方法。

Method: 将结构化剪枝建模为表示恢复问题，将移除的激活和注意力logits建模为保留组件的仿射函数，推导闭式岭回归解并将补偿折叠到模型权重中，最小化校准分布下的期望表示误差。

Result: 在ImageNet上使用DeiT模型验证：MLP和注意力表示存在强冗余性；无补偿时单次剪枝导致严重精度下降；CORP在激进稀疏度下保持精度（DeiT-Huge剪枝50% MLP和注意力结构后保持82.8% Top-1精度）；单GPU 20分钟内完成剪枝，带来实际效率提升。

Conclusion: CORP证明了Vision Transformers中MLP和注意力表示的强冗余性，提出了一种高效的后训练结构化剪枝框架，无需标签、梯度或微调即可保持模型精度，具有实际部署价值。

Abstract: Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\% Top-1 accuracy after pruning 50\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.

</details>


### [72] [TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training](https://arxiv.org/abs/2602.05251)
*Guanjie Cheng,Boyi Li,Lingyu Sun,Mengying Zhu,Yangyang Wu,Xinkui Zhao,Shuiguang Deng*

Main category: cs.LG

TL;DR: TADS是一个用于多任务多模态预训练的任务感知数据选择框架，通过整合内在质量、任务相关性和分布多样性来高效选择训练数据，仅用36%的数据就能在多个基准测试上实现更好的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态预训练模型（如CLIP）依赖网络爬取数据，但这些数据通常存在噪声、不对齐和冗余问题，导致训练效率低下和泛化能力不足。现有的数据选择方法要么基于启发式规则（存在偏见和多样性有限），要么是任务无关的数据驱动方法（无法优化多任务场景）。

Method: TADS框架整合了三个核心要素：1）包含单模态和跨模态算子的综合质量评估系统；2）通过可解释相似性向量量化任务相关性；3）基于聚类的权重优化实现分布多样性。采用反馈驱动的元学习机制，根据代理模型在多个下游任务上的性能自适应地优化选择策略。

Result: 在CC12M数据集上的实验表明，TADS仅使用36%的数据就在ImageNet、CIFAR-100、MS-COCO和Flickr30K等基准测试上实现了优越的零样本性能，平均比基线方法高出1.0%。

Conclusion: TADS通过精心策划高效用数据子集，在相同计算约束下显著提高了数据效率，实现了更高的性能上限，为多任务多模态预训练提供了高效的数据选择解决方案。

Abstract: Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, suffering from bias and limited diversity, or data-driven but task-agnostic, failing to optimize for multi-task scenarios. To address these gaps, we introduce TADS (Task-Aware Data Selection), a novel framework for multi-task multimodal pre-training that integrates Intrinsic Quality, Task Relevance, and Distributional Diversity into a learnable value function. TADS employs a comprehensive quality assessment system with unimodal and cross-modal operators, quantifies task relevance via interpretable similarity vectors, and optimizes diversity through cluster-based weighting. A feedback-driven meta-learning mechanism adaptively refines the selection strategy based on proxy model performance across multiple downstream tasks. Experiments on CC12M demonstrate that TADS achieves superior zero-shot performance on benchmarks like ImageNet, CIFAR-100, MS-COCO, and Flickr30K, using only 36% of the data while outperforming baselines by an average of 1.0%. This highlights that TADS significantly enhances data efficiency by curating a high-utility subset that yields a much higher performance ceiling within the same computational constraints.

</details>


### [73] [Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction](https://arxiv.org/abs/2602.05269)
*David Alejandro Trejo Pizzo*

Main category: cs.LG

TL;DR: HGF是一种双流架构，将1.58位三元主干与可学习的低秩FP16修正路径相结合，通过自适应门控控制，在边缘设备上部署LLM时显著恢复量化模型质量，仅增加12-15%内存开销。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署大语言模型受限于"内存墙"问题，现有1.58位量化技术虽然大幅减少内存占用，但会导致20-25%的困惑度下降。需要一种既能保持低内存占用又能恢复模型质量的方法。

Method: 提出混合门控流(HGF)架构，包含两个流：1.58位三元主干网络和可学习的低秩FP16修正路径，通过自适应门控机制控制信息流。在TinyStories数据集上进行实验，并扩展到1.2B和3B参数模型。

Result: HGF在验证损失上达到0.9306，相比BitNet的1.0294有明显改善，恢复了约55%的三元量化与FP16基线之间的质量差距，仅增加12-15%内存开销。同时发现量化作为结构正则化的现象，HGF保持稳定收敛。

Conclusion: HGF架构有效解决了边缘设备部署LLM时的内存墙问题，在保持低内存占用的同时显著恢复模型质量，且该方法的稳定性和质量恢复能力可线性扩展到生产级语言模型规模。

Abstract: The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the "Memory Wall" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates.
  Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone.
  Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.

</details>


### [74] [Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities](https://arxiv.org/abs/2602.05281)
*Pengyi Li,Elizaveta Goncharova,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出ARM机制解决RLVR中的模式坍缩问题，通过优势重加权平衡不同正确推理路径的置信度，提升输出多样性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 标准策略优化方法（如GRPO）在强化学习可验证奖励（RLVR）中容易收敛到低熵策略，导致严重的模式坍缩和输出多样性受限。作者从采样概率动态角度分析，发现标准目标函数过度强化最高似然路径，抑制了有效的替代推理链。

Method: 提出新颖的优势重加权机制（ARM），将提示困惑度和答案置信度纳入优势估计，动态重塑奖励信号以衰减过度自信推理路径的梯度更新，同时将概率质量重新分配给未被充分探索的正确解决方案。

Result: 在Qwen2.5和DeepSeek模型上的数学和编码基准测试表明，ProGRPO显著缓解了熵坍缩。具体而言，在Qwen2.5-7B上，该方法在Pass@1上比GRPO高出5.7%，在Pass@32上显著高出13.9%，显示出在生成多样化正确推理路径方面的卓越能力。

Conclusion: 该方法显著增强了生成多样性和响应熵，同时保持了竞争性的准确性，有效地在推理任务中实现了探索与利用之间的优越权衡。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.

</details>


### [75] [Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization](https://arxiv.org/abs/2602.05285)
*Minhuan Li,Jiequn Han,Pilar Cossio,Luhuan Wu*

Main category: cs.LG

TL;DR: EmbedOpt是一种在条件嵌入空间中引导扩散模型优化实验似然的新方法，相比基于坐标的后验采样更稳健高效


<details>
  <summary>Details</summary>
Motivation: 当目标位于先验分布的低密度区域时，传统的后验采样需要激进且脆弱的似然加权，这限制了其在实际生物物理逆问题中的应用

Method: 在条件嵌入空间中引导扩散模型优化实验似然，利用该空间丰富的序列和共进化信号，有效调整扩散先验以符合实验约束

Result: 在冷冻电镜图谱拟合任务中优于基于坐标的后验采样方法，在距离约束任务中性能相当，且在跨越两个数量级的超参数范围内表现出卓越的工程鲁棒性

Conclusion: EmbedOpt通过平滑的优化行为显著减少了推理所需的扩散步骤，实现了更好的效率，为生物分子构象生成提供了更稳健的解决方案

Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.

</details>


### [76] [HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction](https://arxiv.org/abs/2602.05286)
*Dahai Yu,Lin Jiang,Rongchao Xu,Guang Wang*

Main category: cs.LG

TL;DR: HealthMamba：一种用于医疗设施访问预测的不确定性感知时空框架，通过统一时空编码器、图状态空间模型和综合不确定性量化模块，在预测准确性和可靠性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗设施访问预测方法通常将其视为时间序列预测问题，忽略了不同类型医疗设施之间的空间依赖关系，且在公共卫生紧急事件等异常情况下无法提供可靠预测。

Method: 提出HealthMamba框架，包含三个关键组件：1)统一时空上下文编码器融合静态和动态信息；2)GraphMamba图状态空间模型进行分层时空建模；3)综合不确定性量化模块集成三种不确定性量化机制。

Result: 在加州、纽约、德州和佛罗里达四个大规模真实数据集上评估，结果显示HealthMamba在预测准确性上比最先进基线提升约6.0%，在不确定性量化方面提升3.5%。

Conclusion: HealthMamba通过整合时空依赖建模和不确定性量化，为医疗设施访问预测提供了准确可靠的解决方案，有助于优化医疗资源配置和公共卫生政策制定。

Abstract: Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.

</details>


### [77] [A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms](https://arxiv.org/abs/2602.05304)
*Feng Zhu,Robert W. Heath,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出统一收敛分析框架，适用于SAG、SAGA和IAG三种算法，简化证明过程并改进收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有随机方差缩减算法（如SAG、SAGA）及其确定性对应方法（如IAG）的分析分散且复杂，需要统一简化的分析框架。

Method: 开发统一收敛分析：1) 使用简单集中工具建立随机子采样延迟界限；2) 设计新颖Lyapunov函数考虑延迟影响。

Result: 获得简洁模块化证明，首次为SAG和SAGA提供高概率界限，可扩展到非凸目标和马尔可夫采样，显著改进IAG算法收敛率。

Conclusion: 提出的统一分析框架简化了三种算法的收敛证明，提供了更优的理论保证，并为扩展到更复杂场景奠定了基础。

Abstract: Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.

</details>


### [78] [Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates](https://arxiv.org/abs/2602.05311)
*Chengxiao Wang,Haoze Wu,Gagandeep Singh*

Main category: cs.LG

TL;DR: 提出了一种合成鲁棒神经李雅普诺夫屏障证书的方法，用于在系统动力学存在扰动时验证深度强化学习控制器的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有神经李雅普诺夫和屏障证书方法仅在固定理想无扰动动力学下提供保证，限制了在现实世界应用中动力学可能因不确定性而偏离时的可靠性。

Method: 定义了鲁棒李雅普诺夫屏障函数，基于Lipschitz连续性指定了确保有界扰动下鲁棒性的充分条件。提出了通过对抗训练、Lipschitz邻域边界和全局Lipschitz正则化来强制执行这些条件的实用训练目标。

Result: 在倒立摆和2D对接两个实际相关环境中验证了方法。相比基线，显著提高了认证鲁棒性边界（高达4.6倍）和强扰动下的经验成功率（高达2.4倍）。

Conclusion: 研究结果表明，在动力学存在扰动的情况下，训练鲁棒神经证书对于安全强化学习是有效的。

Abstract: Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.

</details>


### [79] [Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective](https://arxiv.org/abs/2602.05319)
*Yinan Huang,Hans Hao-Hsun Hsu,Junran Wang,Bo Dai,Pan Li*

Main category: cs.LG

TL;DR: 提出Sequential Flow Matching框架，通过贝叶斯滤波将流式推理视为概率流传输，利用先前后验分布作为生成起点，显著加速采样过程


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型和流匹配模型在实时流式环境中部署时，通常需要从非信息性初始分布重复采样，导致推理延迟和系统积压。需要一种能加速采样、适合实时部署的方法

Method: 基于贝叶斯滤波原理，将流式推理视为学习概率流，将预测分布从一个时间步传输到下一个时间步。利用先前后验分布作为生成起点，提供有原则的"热启动"

Result: 在多种预测、决策和状态估计任务中，该方法性能与完整步扩散模型相当，但仅需一个或很少采样步骤，实现更快采样速度

Conclusion: 通过贝叶斯滤波框架进行序列推理，为基于流的模型在实时部署中提供了新的、有原则的高效视角

Abstract: Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.

</details>


### [80] [GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL](https://arxiv.org/abs/2602.05323)
*Zifan Liu,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.LG

TL;DR: 提出GAS算法解决离线安全强化学习中生成模型方法的两个主要问题：缺乏从次优轨迹中"缝合"最优转移的能力，以及难以平衡奖励和成本目标。通过数据增强、重新标注和引入目标函数来提升缝合能力和平衡性能。


<details>
  <summary>Details</summary>
Motivation: 离线安全强化学习（OSRL）旨在使用预收集数据集学习满足约束的高性能策略。现有基于生成模型的方法面临两个主要挑战：1）缺乏从次优轨迹中"缝合"最优转移的能力；2）难以平衡奖励目标和成本目标，特别是当它们冲突时。

Method: 提出Goal-Assisted Stitching（GAS）算法：1）在转移级别增强和重新标注数据集，从次优轨迹构建高质量轨迹；2）引入目标函数，使用期望回归在增强数据集上训练，估计最优可达奖励和成本目标；3）用估计的目标指导策略训练；4）重塑数据集以获得更均匀的奖励-成本回报分布。

Result: 实证结果验证了GAS的有效性，在平衡奖励最大化和约束满足方面表现出优于现有方法的性能。

Conclusion: GAS算法通过增强缝合能力和引入目标函数，有效解决了离线安全强化学习中生成模型方法的两个主要挑战，实现了更好的奖励-成本权衡。

Abstract: Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.

</details>


### [81] [Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.05333)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文提出了一个信息论框架来分析池式主动学习的理论极限，将池式主动学习重新表述为有噪有损压缩问题，并推导了标签复杂度和泛化误差的信息论下界。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对池式主动学习理论极限的统一分析框架。现有的信息论界限和稳定性理论尚未应用于池式主动学习的分析，需要建立新的理论视角来理解数据选择和学习的相互作用。

Method: 将池式主动学习重新表述为有噪有损压缩问题：池观测映射到有噪符号观测，数据选择对应压缩，学习对应解码。应用有噪有损压缩的有限块长分析，推导信息论下界。

Result: 推导出标签复杂度和泛化误差的信息论下界，这些界限反映了学习算法引起的过拟合以及其归纳偏置与目标任务之间的差异，并与已有的信息论界限和稳定性理论密切相关。

Conclusion: 该框架为池式主动学习提供了新的理论视角，统一分析了数据选择和学习过程，建立了与信息论和稳定性理论的联系，为理解主动学习的理论极限奠定了基础。

Abstract: This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.

</details>


### [82] [Smoothness Errors in Dynamics Models and How to Avoid Them](https://arxiv.org/abs/2602.05352)
*Edward Berman,Luisa Li,Jung Yeon Park,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出了一种松弛酉卷积方法，用于解决图神经网络在表面PDE建模中的过度平滑问题，在热方程、波动方程和天气预报等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在求解表面偏微分方程时通常使用网格感知的图神经网络，但这些网络存在过度平滑问题。虽然酉卷积被提出用于保持平滑性，但在许多物理系统（如扩散过程）中，平滑性自然增加，酉性可能过度约束，反而损害性能。

Method: 1. 系统研究不同GNN对动力学建模的平滑效应，证明酉卷积对此类任务有害；2. 提出松弛酉卷积，平衡平滑性保持与物理系统所需的自然平滑；3. 将酉卷积和松弛酉卷积从图推广到网格。

Result: 在复杂网格上的热方程、波动方程以及天气预报等实验中，该方法优于多个强基线，包括网格感知变换器和等变神经网络。

Conclusion: 松弛酉卷积通过平衡平滑性约束与物理系统的自然平滑需求，有效解决了图神经网络在表面PDE建模中的过度平滑问题，在多个物理系统建模任务中表现出优越性能。

Abstract: Modern neural networks have shown promise for solving partial differential equations over surfaces, often by discretizing the surface as a mesh and learning with a mesh-aware graph neural network. However, graph neural networks suffer from oversmoothing, where a node's features become increasingly similar to those of its neighbors. Unitary graph convolutions, which are mathematically constrained to preserve smoothness, have been proposed to address this issue. Despite this, in many physical systems, such as diffusion processes, smoothness naturally increases and unitarity may be overconstraining. In this paper, we systematically study the smoothing effects of different GNNs for dynamics modeling and prove that unitary convolutions hurt performance for such tasks. We propose relaxed unitary convolutions that balance smoothness preservation with the natural smoothing required for physical systems. We also generalize unitary and relaxed unitary convolutions from graphs to meshes. In experiments on PDEs such as the heat and wave equations over complex meshes and on weather forecasting, we find that our method outperforms several strong baselines, including mesh-aware transformers and equivariant neural networks.

</details>


### [83] [Bayesian Neighborhood Adaptation for Graph Neural Networks](https://arxiv.org/abs/2602.05358)
*Paribesh Regmi,Rui Li,Kishan K C*

Main category: cs.LG

TL;DR: 提出贝叶斯框架，将GNN消息传递建模为随机过程，通过beta过程自适应推断最佳邻域范围，提升GNN表达能力


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法需要为每个预设的邻域范围训练验证GNN，耗时且易受搜索空间设计偏差影响。如何在同质和异质图上自适应确定合适的邻域范围仍未充分探索

Method: 将GNN消息传递行为建模为随机过程，将跳数视为beta过程，通过贝叶斯框架同时推断最佳邻域范围和优化GNN参数

Result: 在同质和异质基准数据集上的实验表明，该方法与最先进的GNN变体兼容，在节点分类任务上达到竞争性或更优性能，并提供良好校准的预测

Conclusion: 提出的贝叶斯框架能够自适应推断GNN消息聚合的最佳邻域范围，理论分析显示范围推断提升了GNN的表达能力，实验验证了方法的有效性

Abstract: The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.

</details>


### [84] [Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting](https://arxiv.org/abs/2602.05371)
*Hongyi Li,Han Lin,Jun Xu*

Main category: cs.LG

TL;DR: HRT将决策树分裂重构为非线性最小二乘问题，通过交替拟合两个线性预测器实现ReLU式表达能力，具有理论保证和高效收敛性。


<details>
  <summary>Details</summary>
Motivation: 斜决策树结合了树的透明性和多元决策边界的强大能力，但学习高质量斜分裂是NP难问题，现有方法依赖缓慢搜索或无理论启发式方法。

Method: 提出Hinge回归树(HRT)，将每个分裂重构为两个线性预测器的非线性最小二乘问题，其最大/最小包络产生ReLU式表达能力。采用交替拟合程序，等价于固定分区内的阻尼牛顿法。

Result: 理论分析证明节点级优化在回溯线搜索变体下单调递减并收敛；实际中固定和自适应阻尼都能快速稳定收敛。HRT模型类是通用逼近器，具有显式O(δ²)逼近率。在合成和真实基准测试中，HRT以更紧凑结构匹配或优于单树基线。

Conclusion: HRT为斜决策树学习提供了理论严谨且高效的方法，结合了树的透明性和多元决策边界的表达能力，在紧凑结构中实现高性能。

Abstract: Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.

</details>


### [85] [Erase at the Core: Representation Unlearning for Machine Unlearning](https://arxiv.org/abs/2602.05375)
*Jaewon Lee,Yongwoo Kim,Donghyun Kim*

Main category: cs.LG

TL;DR: 论文提出EC框架解决机器学习遗忘中的表面遗忘问题，通过多层次对比遗忘和深度监督学习，在保持保留集性能的同时实现网络各层的真正遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法主要改变最终分类器，但中间层表示基本保持不变，导致"表面遗忘"——虽然遗忘集准确率接近零，但内部特征仍保留大量信息。

Method: 提出EC框架：1) 在中间层附加辅助模块；2) 在多个监督点同时应用对比遗忘损失和交叉熵损失；3) 采用分层加权损失；4) 结合多层次对比遗忘和深度监督学习。

Result: EC不仅实现有效的logit级遗忘，还显著降低中间层与原始模型的表示相似性，同时保持保留集性能。EC可作为插件模块集成到现有遗忘方法中，提升表示级遗忘效果。

Conclusion: EC框架解决了机器学习遗忘中的表面遗忘问题，通过强制网络各层遗忘，实现更彻底的表示级遗忘，同时保持模型性能，具有模型无关性和可扩展性。

Abstract: Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.

</details>


### [86] [A Decomposition-based State Space Model for Multivariate Time-Series Forecasting](https://arxiv.org/abs/2602.05389)
*Shunya Nagashima,Shuntaro Suzuki,Shuitsu Koyama,Shinnosuke Hirano*

Main category: cs.LG

TL;DR: DecompSSM：一个端到端分解框架，使用三个并行的深度状态空间模型分支来捕捉趋势、季节性和残差成分，在多元时间序列预测任务中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列交织着缓慢趋势、多速率季节性和不规则残差，现有方法要么依赖僵化的手工分解，要么使用通用的端到端架构，导致成分纠缠且未能充分利用变量间的共享结构。

Method: 提出DecompSSM框架，包含三个并行深度状态空间模型分支分别处理趋势、季节性和残差成分；采用输入相关的预测器实现自适应时间尺度；通过细化模块捕捉跨变量共享上下文；使用辅助损失函数强制重构和正交性约束。

Result: 在ECL、Weather、ETTm2和PEMS04等标准基准测试中，DecompSSM超越了强基线方法，证明了结合成分级深度状态空间模型和全局上下文细化的有效性。

Conclusion: DecompSSM通过端到端分解框架有效解决了多元时间序列预测中成分纠缠和结构共享不足的问题，在多个领域基准上取得了优越性能。

Abstract: Multivariate time series (MTS) forecasting is crucial for decision-making in domains such as weather, energy, and finance. It remains challenging because real-world sequences intertwine slow trends, multi-rate seasonalities, and irregular residuals. Existing methods often rely on rigid, hand-crafted decompositions or generic end-to-end architectures that entangle components and underuse structure shared across variables. To address these limitations, we propose DecompSSM, an end-to-end decomposition framework using three parallel deep state space model branches to capture trend, seasonal, and residual components. The model features adaptive temporal scales via an input-dependent predictor, a refinement module for shared cross-variable context, and an auxiliary loss that enforces reconstruction and orthogonality. Across standard benchmarks (ECL, Weather, ETTm2, and PEMS04), DecompSSM outperformed strong baselines, indicating the effectiveness of combining component-wise deep state space models and global context refinement.

</details>


### [87] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 该论文评估了时间序列基础模型在电力需求预测中利用外生特征的能力，发现基础模型在不同地理环境和特征配置下表现差异很大，简单的LSTM基线在某些情况下反而优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型已成为预测的新范式，但它们能否有效利用外生特征（对电力需求预测至关重要）尚不清楚。该研究旨在实证评估基础模型在电力需求预测中处理外生特征的能力。

Method: 在新加坡和澳大利亚电力市场的每小时和每日粒度上，实证评估了MOIRAI、MOMENT、TinyTimeMixers、ChronosX和Chronos-2等基础模型，并与采用可逆实例归一化的LSTM基线进行比较。系统评估了三种特征配置：所有特征、选定特征和仅目标变量。

Result: 研究发现效果差异很大：Chronos-2在基础模型中表现最佳（在零样本设置下），但在新加坡稳定气候中，简单的基线模型经常在所有基础模型中表现更好，特别是短期预测。模型架构至关重要，具有协同架构实现（TTM的通道混合、Chronos-2的分组注意力）的模型能持续利用外生特征，而其他方法则表现出不一致的收益。

Conclusion: 地理环境同样重要，基础模型主要在变化气候中表现出优势。这些结果挑战了基础模型普遍优越性的假设，并强调了在能源领域需要特定领域模型的重要性。

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [88] [Robust Federated Learning via Byzantine Filtering over Encrypted Updates](https://arxiv.org/abs/2602.05410)
*Adda Akram Bendoukha,Aymen Boudguiga,Nesrine Kaaniche,Renaud Sirdey,Didem Demirag,Sébastien Gambs*

Main category: cs.LG

TL;DR: 提出结合同态加密与元分类器的联邦学习方法，实现隐私保护聚合与拜占庭鲁棒性


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在隐私泄露风险（推理攻击）和拜占庭行为影响模型质量的问题，现有方案往往独立处理安全聚合和拜占庭鲁棒性，难以同时兼顾

Method: 1) 基于属性推理攻击思路，训练元分类器识别拜占庭更新；2) 提出自动化方法选择CKKS同态加密系统的最优核函数和维度超参数；3) 通过重加权机制在加密状态下过滤拜占庭更新

Result: 在FEMNIST、CIFAR10、GTSRB、acsincome基准测试中，SVM过滤器的拜占庭更新识别准确率达90%-94%，模型效用损失很小，加密推理运行时间6-26秒

Conclusion: 该方法成功结合同态加密与拜占庭过滤，在保护隐私的同时有效抵御拜占庭攻击，为联邦学习提供了安全可靠的解决方案

Abstract: Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.

</details>


### [89] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 提出基于锦标赛图的k-wise重排序框架，通过聚合多文档比较中的成对偏好信息，在减少LLM调用次数的同时提升重排序效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM重排序方法要么依赖启发式方法未能充分利用每次排序决策的信息，要么在充分利用信息时效率低下，需要更高效且信息利用率高的重排序方法

Method: 引入锦标赛图框架，将k个文档的比较视为包含binom(k,2)个成对偏好的完整锦标赛，聚合到全局偏好图中，通过传递闭包推断更多排序关系，设计最大化信息增益的查询调度策略，并处理非传递偏好（循环）

Result: 在14个基准测试和5个LLM上，该方法实现了帕累托优势：在达到相同或更高准确率的同时，比可比方法减少25-40%的token使用，比成对方法减少7倍token使用而质量几乎相同

Conclusion: 锦标赛图框架为k-wise重排序提供了理论基础，能够高效利用LLM比较中的偏好信息，在减少计算成本的同时提升重排序性能，是检索增强生成中零样本重排序的有效解决方案

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [90] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中，超参数敏感性主要源于自举目标中的梯度干扰，而非RL问题固有；当有适量专家数据时，基于度量的方法比自举方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 研究深度强化学习中普遍存在的超参数敏感性是否不可避免，还是由特定训练机制加剧。通过离线目标条件RL场景，在固定数据分布和可控非平稳性下探究此问题。

Method: 在离线目标条件RL中，控制数据分布固定，通过调度数据质量变化来显式控制非平稳性。研究两种代表性算法：HIQL（基于自举的TD学习）和QRL（拟度量表示学习），引入跨目标梯度对齐诊断分析梯度干扰。

Result: 观察到比在线RL更强的超参数配置鲁棒性，即使在可控非平稳性下。当有约20%专家数据时，QRL保持广泛稳定的近最优区域，而HIQL表现出尖锐的最优点且随训练阶段显著漂移。自举目标表现出更强的破坏性梯度干扰，与超参数敏感性直接相关。

Conclusion: 训练期间对超参数配置的高敏感性并非RL不可避免的特性，而是由自举的动态过程加剧。这为设计更稳健的算法目标提供了途径。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [91] [Thermodynamic Limits of Physical Intelligence](https://arxiv.org/abs/2602.05463)
*Koichi Takahashi,Yusuke Hayashi*

Main category: cs.LG

TL;DR: 论文提出两种互补的比特/焦耳指标来衡量AI系统的物理效率：热力学认知复杂度/焦耳（识别能力）和赋能/焦耳（控制能力），并建立统一的效率框架来减少度量歧义。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在取得卓越能力的同时消耗大量能源，需要将智能与物理效率联系起来，建立明确的能量会计惯例下的效率度量标准。

Method: 提出两种互补的比特/焦耳指标：1) 热力学认知复杂度/焦耳 - 衡量单位能量内智能体内部状态编码的环境结构信息量；2) 赋能/焦耳 - 衡量单位预期能量成本下的传感器运动通道容量。基于随机热力学理论分析，建立统一的效率框架和报告清单。

Result: 证明了在明确子系统假设下，热力学认知复杂度获取的Landauer尺度闭循环基准是标准热力学学习不等式的推论；同时展示了在没有外部假设时，信息增益与边界内耗散不一定紧密相关。提出了最小化歧义的效率报告框架。

Conclusion: 两种比特/焦耳指标提供了物理智能的两个互补维度（识别vs控制），统一的效率框架和明确会计惯例能支持一致的效率比较，为能源调整的规模分析奠定基础。

Abstract: Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.

</details>


### [92] [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yanning Dai,Shilong Deng,Sarra Habchi,Qi Zhu,Matthias Gallé,Chao Huang*

Main category: cs.LG

TL;DR: 本文提出一个统一的裁剪框架，将现有RLVR方法（如GRPO）统一到一般化的策略散度概念下，并引入KL3估计器作为关键策略散度约束，理论上证明其等价于非对称比率裁剪，能促进更强探索同时保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO）通过裁剪似然比来约束策略散度以确保稳定更新，但缺乏对策略散度度量如何影响探索和性能的系统分析。需要建立一个统一框架来理解不同策略散度度量的作用，并找到更好的约束方法。

Method: 提出统一裁剪框架，将现有方法统一到一般化的策略散度概念下，涵盖似然比和KL散度等度量。引入KL3估计器（方差减少的KL散度蒙特卡洛估计器）作为关键策略散度约束，理论上证明其等价于非对称比率裁剪。

Result: 在数学推理基准测试中，将KL3估计器集成到GRPO中，既提高了训练稳定性，又提升了最终性能。KL3约束能重新分配概率质量到高置信度动作，促进更强探索。

Conclusion: 策略散度约束在策略优化中至关重要，KL3估计器提供了一种有效的约束方法，在保持GRPO风格方法简单性的同时，通过促进更强探索来提升性能。统一框架为系统分析不同策略散度度量提供了理论基础。

Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.

</details>


### [93] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: EUQ提出了一种细粒度的证据不确定性量化方法，通过分析模型输出特征中的支持性和反对性证据，来检测大视觉语言模型中的错误行为，如幻觉和越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在面对不称职或对抗性输入时，经常产生不可靠甚至有害的内容（如事实幻觉或危险指令），这些错误行为源于认知不确定性（内部知识冲突或信息缺失），而现有的不确定性量化方法通常只捕捉整体认知不确定性，在识别这些问题上效果有限。

Method: 提出证据不确定性量化方法，将模型输出头的特征解释为支持性（正面）或反对性（负面）证据，利用证据理论对这些证据进行建模和聚合，以量化内部冲突和知识差距，所有这些都在单次前向传播中完成。

Result: 在四种错误行为类别（幻觉、越狱攻击、对抗性漏洞和分布外失败）上对最先进的大视觉语言模型进行了广泛评估，EUQ始终优于强基线方法，发现幻觉对应高内部冲突，分布外失败对应高无知度。层间证据不确定性动态分析有助于从新视角解释内部表示的演化。

Conclusion: EUQ提供了一种有效的细粒度不确定性量化方法，能够检测大视觉语言模型的错误行为，为理解模型内部表示提供了新视角，有助于提高模型在关键应用中的可靠性。

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [94] [When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging](https://arxiv.org/abs/2602.05536)
*Yayuan Li,Ze Peng,Jian Zhang,Jintao Guo,Yue Duan,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出SVC方法，通过校准奇异值解决模型合并中的共享知识重复计数问题，提升合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要解决任务更新间的冲突，但忽略了共享知识重复计数的问题。当任务共享对齐的谱方向时，简单的线性组合会重复累积这些方向，导致奇异值膨胀并偏向共享子空间

Method: 提出奇异值校准(SVC)方法，这是一种无需训练和数据的后处理技术。通过量化子空间重叠并重新缩放膨胀的奇异值，恢复平衡的谱分布

Result: 在视觉和语言基准测试中，SVC持续改进现有合并基线，达到最先进性能。通过仅修改奇异值，将Task Arithmetic性能提升13.0%

Conclusion: SVC有效解决了模型合并中的共享知识重复计数问题，通过谱校准显著提升合并模型性能，为模型合并提供了新的改进方向

Abstract: Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.

</details>


### [95] [Steering Large Reasoning Models towards Concise Reasoning via Flow Matching](https://arxiv.org/abs/2602.05539)
*Yawei Li,Benjamin Bergner,Yinghan Zhao,Vihang Prakash Patil,Bei Chen,Cheng Wang*

Main category: cs.LG

TL;DR: FlowSteer：一种非线性引导方法，通过流匹配学习从冗长推理到简洁推理的完整分布变换，相比线性方法能产生更紧凑的推理输出。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但输出往往过于冗长。现有的线性引导方法基于限制性的线性表示假设，仅应用全局向量到隐藏表示，效果有限。

Method: FlowSteer使用流匹配技术，学习从冗长推理分布到简洁推理分布的完整非线性变换。该方法将变换建模为速度场，实现对模型推理过程的精确、输入相关的控制。

Result: 在多种推理基准测试中，FlowSteer相比领先的推理时基线方法，展现出更强的任务性能和更高的标记效率，能产生更紧凑的推理过程。

Conclusion: 使用生成技术建模完整的分布传输为控制大型推理模型提供了更有效和更原则性的基础，超越了传统线性方法的限制。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.

</details>


### [96] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出A-GRAE方法，通过非对称优势估计解决GRPO在探索和难度适应方面的瓶颈，显著提升强化学习效率。


<details>
  <summary>Details</summary>
Motivation: GRPO作为RLVR标准方法，在探索效率和难度适应方面存在瓶颈。研究发现这些瓶颈源于组相对优势估计（GRAE）中隐含的优势对称性，这种对称性导致两个关键限制：1）在组层面，正确与错误轨迹的严格对称权重阻碍新正确解的探索；2）在样本层面，算法隐含优先中等难度样本，无法适应非平稳的难度聚焦需求。

Method: 提出非对称GRAE（A-GRAE）方法，通过动态调节探索激励和样本难度聚焦来解决对称性问题。具体包括：1）非对称抑制正确轨迹的优势以鼓励必要探索；2）采用课程式学习策略，初期优先简单样本，逐渐过渡到复杂样本。

Result: 在七个基准测试上的实验表明，A-GRAE方法在LLM和MLLM上都能持续改进GRPO及其变体，验证了非对称优势估计的有效性。

Conclusion: GRAE中的优势对称性是次优的，通过非对称优势估计和动态难度聚焦可以显著提升强化学习效率，A-GRAE为RLVR方法提供了有效的改进方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [97] [Logical Guidance for the Exact Composition of Diffusion Models](https://arxiv.org/abs/2602.05549)
*Francesco Alesiani,Jonathan Warrell,Tanja Bien,Henrik Christiansen,Matheus Ferraz,Mathias Niepert*

Main category: cs.LG

TL;DR: LOGDIFF是一个扩散模型引导框架，通过逻辑表达式实现精确约束生成，建立了布尔演算规则来确保逻辑引导的精确性，并提出了混合引导方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在推理时难以处理复杂的逻辑约束生成，需要一种能够精确组合原子属性引导信号的框架来实现基于逻辑表达式的约束生成。

Method: 1. 推导精确布尔演算，为逻辑引导提供充分条件；2. 开发高效递归算法从原子分数和后验概率计算引导信号；3. 提出混合引导方法，结合分类器引导和无分类器引导；4. 将布尔公式编译为满足条件的电路表示。

Result: 在多个图像和蛋白质结构生成任务中验证了框架的有效性，能够实现基于复杂逻辑表达式的精确约束生成。

Conclusion: LOGDIFF为扩散模型提供了系统化的逻辑引导框架，能够在推理时实现基于复杂逻辑表达式的精确约束生成，并建立了理论保证和高效计算方法。

Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.

</details>


### [98] [MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks](https://arxiv.org/abs/2602.05567)
*Long D. Nguyen,Binh P. Nguyen*

Main category: cs.LG

TL;DR: 提出消息自适应图提示调优方法，在消息传递步骤中注入可学习提示，重新加权邻居消息并添加任务特定提示向量，保持骨干GNN冻结，实现参数高效的下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 预训练图神经网络在下游任务适应中存在挑战，因为预训练目标与任务需求不匹配。现有的图提示调优方法大多只修改输入或表示，而不改变消息传递过程，限制了其适应邻居交互的能力。

Method: 提出消息自适应图提示调优，在消息传递步骤中注入可学习提示：1）重新加权传入的邻居消息；2）在消息聚合期间添加任务特定的提示向量。保持骨干GNN冻结，仅训练提示参数。

Result: 在多样化的节点级和图级数据集上，该方法在少样本设置中相比先前图提示方法获得一致提升，在全样本设置中达到与微调竞争的性能。

Conclusion: 消息自适应图提示调优通过修改消息传递过程来适应邻居交互，兼容常见GNN骨干和预训练策略，适用于各种下游设置，实现了参数高效的下游任务适应。

Abstract: Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.

</details>


### [99] [EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking](https://arxiv.org/abs/2602.05571)
*Rishabh Bhattacharya,Naresh Manwani*

Main category: cs.LG

TL;DR: EdgeMask-DG*：首个结合自适应对抗拓扑搜索与特征增强图的图域泛化方法，通过边掩码学习寻找最坏情况的结构扰动，提升模型在跨域图数据上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络面临结构偏移挑战，现有域泛化方法依赖固定结构增强或全局扰动，无法识别哪些边编码了域不变信息。作者认为域不变结构信息存在于拓扑和特征相似性衍生的多个图结构的共识中。

Method: 提出EdgeMask-DG：基于min-max算法，边掩码器学习在稀疏约束下寻找最坏情况的连续掩码，迫使任务GNN在对抗性结构扰动下有效工作。进一步提出EdgeMask-DG*：将对抗掩码原则应用于增强图，结合原始拓扑和特征衍生边，即使在原始拓扑噪声大或域特定时也能发现不变性。

Result: EdgeMask-DG*在多种图域泛化基准测试（包括引文网络、社交网络和时间图）上达到新的最先进性能。在Cora OOD基准测试中，将最坏情况域准确率提升至78.0%，比之前最先进方法（74.2%）提高了3.8个百分点。

Conclusion: EdgeMask-DG*是首个系统结合自适应对抗拓扑搜索与特征增强图的图域泛化方法，通过鲁棒优化视角提供形式化证明，显著提升了图神经网络在跨域场景下的泛化能力。

Abstract: Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\%, a +3.8 pp improvement over the prior state of the art (74.2\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/

</details>


### [100] [OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph](https://arxiv.org/abs/2602.05576)
*Chenxi Wan,Xunkai Li,Yilong Zuo,Haokun Deng,Sihan Li,Bowen Fan,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OpenMAG是一个全面的多模态属性图学习基准，整合了6个领域的19个数据集、16种编码器、24个SOTA模型和8个下游任务，旨在解决现有基准在领域覆盖、编码器灵活性、模型多样性和任务范围方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态属性图（MAG）模型的快速发展，现有基准在领域覆盖、编码器灵活性、模型多样性和任务范围方面存在严重不足，无法进行公平评估，迫切需要建立一个统一、严谨的评估标准。

Method: 构建OpenMAG基准，整合19个数据集覆盖6个领域，支持16种编码器（静态和可训练），实现24个SOTA模型的标准化库，支持8个下游任务，并通过系统评估必要性、数据质量、有效性、鲁棒性和效率五个维度进行分析。

Result: 通过OpenMAG的系统评估，得出了14个关于MAG学习的基本见解，为未来研究提供指导。该基准为MAG模型提供了公平比较的统一框架。

Conclusion: OpenMAG填补了多模态属性图学习评估标准的空白，通过全面的基准设计和系统评估，为领域发展提供了重要的基础设施和指导性见解。

Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.

</details>


### [101] [On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature](https://arxiv.org/abs/2602.05600)
*Yikuan Zhang,Ning Yang,Yuhai Tu*

Main category: cs.LG

TL;DR: 本文揭示了SGD噪声协方差与Hessian矩阵之间更一般的关系，挑战了传统认为两者相等的假设，并给出了理论边界和实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统研究通常假设Fisher信息矩阵与Hessian矩阵在负对数似然损失下等价，从而认为SGD噪声协方差C与Hessian矩阵H成正比。然而，这种假设仅在限制性条件下成立，而在深度神经网络中通常被违反。本文旨在揭示两者之间更一般的关系。

Method: 利用最近发现的Activity-Weight Duality，推导出与具体损失函数形式无关的通用关系：C ∝ E_p[h_p^2]，其中h_p表示每个样本的Hessian矩阵，H = E_p[h_p]。理论分析表明C和H近似可交换，对角线元素满足幂律关系C_ii ∝ H_ii^γ，其中指数γ在1到2之间有理论边界。

Result: 实验验证了理论边界，在不同数据集、架构和损失函数上均观察到C和H的对角线元素遵循幂律关系，指数γ在理论预测的1到2范围内。这提供了对深度学习噪声-曲率关系的统一表征。

Conclusion: 本文纠正了关于SGD噪声协方差与Hessian矩阵关系的传统误解，提供了更准确的理论框架，揭示了噪声-曲率关系的普适特性，对理解SGD的隐式正则化效应具有重要意义。

Abstract: Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\mathbf{C}$ is proportional to the Hessian $\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\mathbf{C} \propto \mathbb{E}_p[\mathbf{h}_p^2]$, where $\mathbf{h}_p$ denotes the per-sample Hessian with $\mathbf{H} = \mathbb{E}_p[\mathbf{h}_p]$. As a consequence, $\mathbf{C}$ and $\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \propto H_{ii}^γ$ with a theoretically bounded exponent $1 \leq γ\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.

</details>


### [102] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT提出了一种新的Diffusion Transformer剪枝方法，通过残差感知的Top-k选择和自适应策略，在保持可微分性的同时实现硬件友好的静态预算，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformers（DiTs）由于自注意力机制二次复杂度导致计算成本过高，而传统剪枝方法无法同时满足可微分性、效率和硬件所需的严格静态预算要求。

Method: 提出Shiva-DiT框架，核心是残差感知的可微分Top-k选择机制，通过残差感知直通估计器实现确定性token计数；同时引入上下文感知路由器和自适应比率策略，自动学习自适应剪枝调度。

Result: 在包括SD3.5在内的主流模型上实验表明，Shiva-DiT建立了新的帕累托前沿，实现了1.54倍的端到端加速，同时保持优于基线的保真度，有效消除了不规则张量开销。

Conclusion: Shiva-DiT成功解决了DiTs剪枝中可微分性、效率和硬件静态预算之间的冲突，为大规模扩散模型的高效部署提供了有效解决方案。

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [103] [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)
*Xuhui Li,Zhengquan Luo,Xiwei Liu,Yongqiang Yu,Zhiqiang Xu*

Main category: cs.LG

TL;DR: PGFM是首个基于流匹配的生成式数据集蒸馏框架，通过ODE快速确定性合成，在潜空间学习从高斯噪声到数据分布的类条件传输，比扩散方法更高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据集蒸馏方法依赖启发式引导或原型分配，存在采样耗时、轨迹不稳定问题，尤其在强控制或低IPC下损害下游泛化性能。

Method: 提出路径引导流匹配(PGFM)框架：1)在冻结VAE的潜空间进行流匹配学习类条件传输；2)开发连续路径到原型引导算法实现ODE一致路径控制，确保轨迹可靠到达指定原型同时保持多样性。

Result: 在高分辨率基准测试中，PGFM匹配或超越先前扩散基蒸馏方法，采样步骤更少，提供竞争性性能且效率显著提升（比扩散方法高效7.6倍，模式覆盖率达78%）。

Conclusion: PGFM作为首个流匹配基生成式蒸馏框架，通过ODE快速确定性合成和连续路径引导，解决了扩散方法效率低和轨迹不稳定的问题，实现了高效高质量的数据集蒸馏。

Abstract: Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.

</details>


### [104] [Mode-Dependent Rectification for Stable PPO Training](https://arxiv.org/abs/2602.05619)
*Mohamad Mohamad,Francesco Ponzio,Xavier Descombes*

Main category: cs.LG

TL;DR: 论文提出Mode-Dependent Rectification (MDR)方法，解决视觉强化学习中Batch Normalization等模式相关层在PPO算法中引起的不稳定问题


<details>
  <summary>Details</summary>
Motivation: 模式相关架构组件（如Batch Normalization、dropout等）在训练和评估时行为不同，在视觉强化学习中常用但会破坏on-policy优化的稳定性。在PPO中，Batch Normalization引起的训练-评估行为差异会导致策略失配、分布漂移和奖励崩溃。

Method: 提出Mode-Dependent Rectification (MDR)，一种轻量级的双阶段训练过程，无需改变架构就能稳定PPO在模式相关层下的训练。该方法通过调整训练过程来缓解模式差异带来的问题。

Result: 在程序生成游戏和真实世界补丁定位任务上的实验表明，MDR能持续提高稳定性和性能，并且能自然地扩展到其他模式相关层。

Conclusion: MDR为视觉强化学习中模式相关层引起的稳定性问题提供了一个有效的解决方案，能够在不改变架构的情况下显著改善PPO的训练稳定性和最终性能。

Abstract: Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.

</details>


### [105] [Rewards as Labels: Revisiting RLVR from a Classification Perspective](https://arxiv.org/abs/2602.05630)
*Zepeng Zhai,Meilin Chen,Jiaxuan Zhao,Junlang Qian,Lei Shen,Yuan Lu*

Main category: cs.LG

TL;DR: 提出REAL框架，将可验证奖励重新视为分类标签而非标量权重，解决GRPO方法中的梯度错配问题，在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO）存在梯度错配问题：正样本梯度分配不当和负样本梯度主导，导致策略更新低效和次优。

Method: 提出REAL框架，将可验证奖励视为分类标签而非标量权重，将策略优化重构为分类问题，并引入锚定logits增强策略学习。

Result: 在数学推理基准测试中，REAL提升训练稳定性，显著优于GRPO及其变体（如DAPO）。1.5B模型上平均Pass@1比DAPO提升6.7%，7B模型上继续优于DAPO和GSPO。

Conclusion: REAL通过将奖励重新定义为分类标签，有效解决了梯度错配问题，实现了更平衡的梯度分配，在数学推理任务上取得了显著性能提升。

Abstract: Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.

</details>


### [106] [Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias](https://arxiv.org/abs/2602.05635)
*Ojasva Nema,Kaustubh Sharma,Aditya Chauhan,Parikshit Pareek*

Main category: cs.LG

TL;DR: 论文提出通过双线性MLP架构的乘法交互作为归纳偏置，改善神经网络的表示结构，实现选择性遗忘和长期外推能力。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在选择性遗忘和长期外推方面仍然脆弱，即使任务具有底层代数结构。作者认为这些失败不仅源于优化或遗忘算法，更源于模型在训练过程中如何构建内部表示结构。

Method: 探索双线性MLP架构中的显式乘法交互作为归纳偏置，分析双线性参数化在梯度流条件下的"非混合"特性，其中功能组件分离为正交子空间表示。通过模块化算术、循环推理、李群动力学和针对性遗忘基准进行实验验证。

Result: 与逐点非线性网络不同，乘法架构能够恢复与底层代数结构对齐的真实算子。双线性参数化具有非混合特性，功能组件在正交子空间中分离，为手术式模型修改提供了数学基础。

Conclusion: 模型可编辑性和泛化能力受表示结构约束，架构归纳偏置在实现可靠遗忘中起核心作用。显式乘法交互有助于结构解缠，改善神经网络的选择性遗忘和长期外推能力。

Abstract: Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.

</details>


### [107] [Joint Embedding Variational Bayes](https://arxiv.org/abs/2602.05639)
*Amin Oji,Paul Fieguth*

Main category: cs.LG

TL;DR: VJE框架结合联合嵌入与变分推断，实现无重建、非对比的自监督概率表示学习，使用学生t分布解耦方向与径向因子，在多个数据集上达到与非对比基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于能量的预测目标优化点间差异，缺乏对表示不确定性的建模能力。需要一种既能学习概率表示又无需重建或对比的方法。

Method: 提出变分联合嵌入(VJE)框架，最大化对称条件证据下界(ELBO)。使用学生t分布的条件似然，通过极坐标分解解耦方向与径向因子以防止训练不稳定。采用摊销推断网络参数化对角高斯变分后验。

Result: 在ImageNet-1K、CIFAR-10/100、STL-10数据集上，VJE在线性和k-NN评估中达到与非对比基线相当的性能。在一类CIFAR-10异常检测中，基于似然的评分优于其他自监督基线。

Conclusion: VJE成功实现了无重建、非对比的自监督概率表示学习，通过解耦方向与径向因子解决了训练稳定性问题，并在多个任务中验证了其概率语义的有效性。

Abstract: We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.

</details>


### [108] [Empowering Time Series Analysis with Large-Scale Multimodal Pretraining](https://arxiv.org/abs/2602.05646)
*Peng Chen,Siyuan Wang,Shiyan Hu,Xingjian Wu,Yang Shu,Zhongwen Rao,Meng Wang,Yijie Li,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: HORAI：首个面向时间序列分析的多模态基础模型，通过频率增强的跨模态编码和时间-频率解码器，在零样本预测和异常检测任务上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要依赖单模态预训练，缺乏互补模态来增强理解。构建多模态基础模型面临两大挑战：1）缺乏统一的多模态预训练范式和大规模多模态语料库；2）如何有效整合异构模态并增强模型泛化能力

Method: 提出多模态预训练范式，利用时间序列的内生模态（衍生图像和文本）和外生知识（真实世界新闻）。开发自动数据构建流水线创建MM-TS数据集（首个大规模多模态时间序列数据集，覆盖6个领域，达10亿数据点）。提出HORAI模型，包含频率增强跨模态编码器和时间-频率解码器两个核心组件

Result: 在MM-TS上预训练后，HORAI在时间序列预测和异常检测任务上实现了最先进的零样本性能，展示了强大的跨模态和跨领域泛化能力

Conclusion: 该研究为时间序列分析的多模态基础模型迈出了重要一步，提出的预训练范式、数据集和模型架构为解决多模态时间序列分析的关键挑战提供了有效方案

Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.

</details>


### [109] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: TACO是一个端到端的表格数据压缩模型，通过将训练数据压缩到潜在空间，显著降低了计算复杂度和内存消耗，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的表格基础模型虽然性能优异，但其注意力机制具有二次复杂度，导致训练和推理时间开销大，难以处理大规模数据集。

Method: 提出TACO端到端表格压缩模型，将训练数据集压缩到潜在空间中，减少计算复杂度和内存使用。

Result: 在TabArena基准测试中，TACO推理速度提升94倍，内存消耗减少97%，同时性能无明显下降，且随着数据集规模增大表现更好。

Conclusion: TACO通过压缩训练数据到潜在空间，有效解决了表格Transformer模型的扩展性问题，在保持性能的同时大幅提升效率和可扩展性。

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [110] [Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation](https://arxiv.org/abs/2602.05656)
*Igor Santos-Grueiro*

Main category: cs.LG

TL;DR: 行为评估无法唯一识别潜在对齐属性，即使通过严格测试也只能缩小假设空间而非验证对齐


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐评估主要依赖行为测试，但从未形式化分析从行为证据推断潜在对齐属性的推理问题

Method: 将对齐评估形式化为部分可观测下的可识别性问题，引入对齐可验证性问题与规范性不可区分性概念

Result: 在有限行为评估和评估感知代理条件下，行为合规性无法唯一识别潜在对齐，行为测试只能估计不可区分类而非验证对齐

Conclusion: 对齐基准应解释为提供特定机制内可观测合规性的上界，而非底层对齐的保证，需要重新思考对齐评估范式

Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.
  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.
  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.
  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.

</details>


### [111] [Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization](https://arxiv.org/abs/2602.05657)
*Aleksandar Armacki,Dragana Bajović,Dušan Jakovetić,Soummya Kar,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该论文研究SGD算法长期尾部衰减行为，通过大偏差理论分析，为SGD和c-SGD在非凸优化和重尾噪声下的梯度范数平方尾部提供了紧致的上下界，发现比现有有限时间界限更快的衰减速率。


<details>
  <summary>Details</summary>
Motivation: 现有SGD尾部行为研究多为有限时间的高概率保证，缺乏对固定误差阈值下失败概率（尾部衰减速率）的直接分析，且无法捕捉现代学习模型（通常训练数百万次迭代）的真实长期尾部衰减行为。

Method: 使用大偏差理论研究SGD类方法的长期尾部衰减：1) 对非凸成本和有界噪声的普通SGD，分析最佳迭代点的梯度范数平方尾部；2) 对重尾噪声（p阶矩有界，p∈(1,2]）下的c-SGD，分析其尾部行为；3) 为两种方法提供尾部衰减的下界。

Result: 1) SGD在非凸有界噪声下，长期尾部衰减上界为e^{-t/log(t)}；2) c-SGD在重尾噪声下，p∈(1,2)时衰减上界为e^{-t^{β_p}/log(t)}（β_p=4(p-1)/(3p-2)），p=2时为e^{-t/log^2(t)}；3) 下界均为e^{-t}，表明结果在多项式对数因子内是紧致的。相比现有有限时间界限的e^{-√t}和e^{-t^{β_p/2}}，衰减快一个数量级。

Conclusion: 通过大偏差理论揭示了SGD类方法比现有有限时间分析预测的更快的长期尾部衰减速率，为个体算法运行提供了更强的长期保证，填补了尾部衰减速率分析和长期行为研究的空白。

Abstract: The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\log(t)}$, where $β_p = \frac{4(p-1)}{3p-2}$ for $p \in (1,2)$ and $e^{-t/\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.

</details>


### [112] [Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks](https://arxiv.org/abs/2602.05660)
*Slawek Smyl,Paweł Pełka,Grzegorz Dudek*

Main category: cs.LG

TL;DR: 本文提出了一种基于任意分位数循环神经网络(AQ-RNN)的多区域光伏发电概率预测框架，能够通过单一训练模型估计任意概率水平的校准条件分位数，并利用空间依赖性提升系统级鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 光伏发电渗透率增加给电力系统运行带来显著不确定性，需要超越确定性点预测的预测方法。现有方法难以同时处理任意分位数预测和空间依赖性。

Method: 提出任意分位数概率预测框架，基于AQ-RNN模型，采用双轨循环架构联合处理序列特定和跨区域上下文信息，支持扩张循环单元、基于补丁的时间建模和动态集成机制。

Result: 使用欧洲259个区域30年的小时光伏发电数据进行评估，相比现有统计和神经概率基线方法，在预测精度、校准和预测区间质量方面均表现出持续改进。

Conclusion: 该方法适用于可再生能源主导电力系统中的不确定性感知能源管理和运行决策，展示了在任意分位数预测和空间依赖性利用方面的优势。

Abstract: The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.
  The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.

</details>


### [113] [Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection](https://arxiv.org/abs/2602.05667)
*Ling Zhan,Zhen Li,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出SCLCS方法，通过自监督学习选择代表性核心集，仅用10%数据就能保持功能连接建模方法的性能排名，解决大规模fMRI数据集基准测试的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大规模fMRI数据集中功能连接建模方法的基准测试对可重复神经科学至关重要，但模型-数据组合的爆炸式增长使得穷举评估计算上不可行，阻碍其成为常规预分析步骤。

Method: 提出SCLCS框架：1)使用自适应Transformer学习每个样本的独特功能连接结构；2)引入结构扰动分数量化训练期间学习结构的稳定性，识别代表基础连接原型的样本；3)采用密度平衡采样策略确保核心集的结构鲁棒性和分布代表性。

Result: 在REST-meta-MDD数据集上，SCLCS仅用10%数据就能保持真实模型排名，在排名一致性(nDCG@k)上比现有最佳核心集选择方法提升23.2%。

Conclusion: 这是首个将核心集选择形式化用于功能连接算子基准测试的工作，使大规模算子比较成为计算神经科学中可行且不可或缺的部分。

Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS

</details>


### [114] [Stable but Wrong: When More Data Degrades Scientific Conclusions](https://arxiv.org/abs/2602.05668)
*Zhipeng Zhang,Kai Li*

Main category: cs.LG

TL;DR: 论文揭示数据驱动科学存在根本性缺陷：即使统计推断过程稳定收敛且通过常规诊断检验，仍可能系统性地得出错误结论，因为观测可靠性存在不可观测的退化。


<details>
  <summary>Details</summary>
Motivation: 现代科学依赖大规模观测数据和自动化推断流程，隐含假设更多数据能提高结论可靠性。本文挑战这一假设，揭示在特定结构机制下，即使推断过程表现正常，仍会系统性得出错误结论。

Method: 通过最小化合成实验，识别一种结构机制：观测可靠性以推断过程本身不可观测的方式退化。在此机制下，标准推断程序仍能平稳收敛、保持校准良好并通过常规诊断检验。

Result: 在此机制下，增加数据不仅不会纠正错误，反而会放大错误，而基于残差和拟合优度的诊断检验仍保持误导性的正常表现。稳定性、收敛性和置信度不足以保证认知有效性。

Conclusion: 推断不能被视为数据可用性的无条件结果，而必须受到观测过程完整性的明确约束。数据驱动科学存在内在限制，需要重新思考科学推断的基本假设。

Abstract: Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.

</details>


### [115] [Perception-Based Beliefs for POMDPs with Visual Observations](https://arxiv.org/abs/2602.05679)
*Miriam Schäfers,Merlijn Krale,Thiago D. Simão,Nils Jansen,Maximilian Weininger*

Main category: cs.LG

TL;DR: 提出PBP框架，通过感知模型将视觉观测映射到状态分布，避免传统POMDP求解器直接处理高维观测空间，提高处理视觉输入问题的效率。


<details>
  <summary>Details</summary>
Motivation: 传统POMDP求解器在处理具有高维观测（如相机图像）的现实问题时计算困难，需要一种能够有效处理视觉输入的方法。

Method: 引入PBP框架，使用图像分类器作为感知模型，将视觉观测映射到状态概率分布，直接纳入信念更新过程，并引入不确定性量化来处理分类器不精确问题。

Result: PBP在性能上优于现有的端到端深度强化学习方法，并且不确定性量化提高了PBP对视觉损坏的鲁棒性。

Conclusion: PBP框架有效解决了传统POMDP求解器处理高维视觉观测的难题，通过感知模型和不确定性量化实现了更好的性能和鲁棒性。

Abstract: Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.

</details>


### [116] [Mining Generalizable Activation Functions](https://arxiv.org/abs/2602.05688)
*Alex Vitvitskyi,Michael Boratko,Matej Grcic,Razvan Pascanu,Deep Shah,Petar Veličković*

Main category: cs.LG

TL;DR: 使用AlphaEvolve（基于前沿LLM作为变异算子）进行进化搜索，发现新的激活函数，不仅关注性能提升，还能针对特定归纳偏置进行优化。


<details>
  <summary>Details</summary>
Motivation: 激活函数选择是研究热点，影响优化效果和模型表达能力，同时能显著改变架构的隐式归纳偏置，控制非线性行为。现有方法需要手动构建搜索空间，限制了发现新颖激活函数的能力。

Method: 采用AlphaEvolve进化搜索框架，利用前沿LLM作为变异算子，在特定FLOP预算内搜索所有可能的Python函数空间。使用分布外数据性能作为适应度函数，以发现具有特定归纳偏置的激活函数。

Result: 实证研究表明，相对小规模的合成数据集足以让AlphaEvolve发现有意义的激活函数。该方法能够发现不仅提升性能，还能编码特定归纳偏置的激活函数。

Conclusion: 进化搜索是发现新激活函数的有用框架，现代管道如AlphaEvolve能够探索更广泛灵活的搜索空间，并针对特定归纳偏置进行优化，而不仅仅是性能提升。

Abstract: The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.

</details>


### [117] [Almost Asymptotically Optimal Active Clustering Through Pairwise Observations](https://arxiv.org/abs/2602.05690)
*Rachel S. Y. Teo,P. N. Karthik,Ramya Korlakai Vinayak,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出一个主动聚类框架，通过带噪的二元反馈将M个物品聚类到未知数量K个组中，建立了查询次数的理论下界，并设计了渐近最优算法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通常假设数据是静态且无噪声的，但在实际应用中（如众包标注、推荐系统），数据收集往往是主动的、有噪声的。需要一种能够在主动查询和带噪反馈下进行聚类的理论框架。

Method: 使用变化测度技术建立查询次数的理论下界，设计基于广义似然比(GLR)统计量的渐近最优算法，并开发计算可行的GLR变体。

Result: 建立了聚类准确度置信度所需查询次数的基本下界，设计了性能与下界保持常数倍差距的渐近最优算法，并提供了计算可行的实现方案。

Conclusion: 该框架为主动带噪聚类提供了理论基础和实用算法，在理论最优性和计算可行性之间取得了平衡，适用于需要主动数据收集的实际应用场景。

Abstract: We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.

</details>


### [118] [Limitations of SGD for Multi-Index Models Beyond Statistical Queries](https://arxiv.org/abs/2602.05704)
*Daniel Barzilai,Ohad Shamir*

Main category: cs.LG

TL;DR: 论文提出了一个新的非统计查询框架，用于分析标准SGD在单索引和多索引模型中的局限性，解决了现有SQ框架与SGD之间联系薄弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有统计查询(SQ)框架与随机梯度下降(SGD)之间的形式化联系薄弱，现有结果通常依赖于对抗性或特殊结构的梯度噪声，不能反映标准SGD中的噪声，有时会导致错误预测。此外，许多对SGD的分析依赖于非平凡的算法修改，如限制SGD轨迹到球面或使用非常小的学习率。

Method: 开发了一个新的非SQ框架来研究标准vanilla SGD在单索引和多索引模型中的局限性。该框架适用于广泛的设置和架构，包括（潜在的深度）神经网络。

Result: 提出了一个新的分析框架，能够更准确地刻画标准SGD在低维投影模型中的性能限制，避免了现有SQ框架的缺陷。

Conclusion: 新框架为理解SGD在单索引和多索引模型中的根本局限性提供了更准确的分析工具，填补了现有SQ框架与SGD实际行为之间的差距。

Abstract: Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.

</details>


### [119] [Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift](https://arxiv.org/abs/2602.05707)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 论文研究机器学习中预测准确性与群体公平性之间的权衡关系，发现现有观察到的"公平性有助于准确性"现象可能是训练数据中群体比例失真的结果。作者提出了一种基于收缩加权的评估协议来揭示真实的公平性-效用边界。


<details>
  <summary>Details</summary>
Motivation: 机器学习实践中经常观察到预测准确性与群体公平性约束之间的张力，但有时公平性干预似乎能提高准确性。作者认为这两种现象都可能是训练数据中群体比例失真的产物，需要建立更严谨的评估框架来区分真实与虚假的公平性-准确性权衡。

Method: 在子群体偏移（组内分布稳定、组间比例变化）的设定下，作者：1）理论分析完全重要性加权校正的渐近无偏性与有限样本次优性；2）提出最优有限样本校正方法——收缩加权，在目标分布和训练分布之间插值；3）建立评估协议：先固定表示（最优地），再比较公平性干预与收缩校正基线的差异，以分离出公平性的真实代价。

Result: 理论分析表明：完全重要性加权校正虽然渐近无偏但在有限样本下非最优；收缩加权是最优的有限样本校正方法；"公平性有助于准确性"的现象可能源于与不当加权基线的比较。在合成数据和真实基准（Adult、COMPAS）上的实验验证了理论预测，并证明该协议能消除虚假权衡，揭示真实的公平性-效用边界。

Conclusion: 论文揭示了公平性-准确性权衡中的统计假象，提出了基于收缩加权的评估协议。该协议要求先优化地固定表示，再将公平性干预与收缩校正基线比较，从而分离出公平性的真实代价。这一框架为公平机器学习提供了更严谨的评估方法，有助于揭示真实的公平性-效用权衡关系。

Abstract: Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent "fairness helps accuracy" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.

</details>


### [120] [Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions](https://arxiv.org/abs/2602.05713)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: FairBoost：在Boosting算法中融入群体公平性约束，通过投影训练分布到公平分布集合，理论证明指数损失收敛率取决于弱学习器边缘减去公平性代价项，直接量化了准确性与公平性的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统Boosting算法（如AdaBoost）有强大的理论保证，但缺乏对群体公平性的考虑。研究者希望将公平性约束融入Boosting框架，同时保持可分析性的训练动态。

Method: 提出FairBoost方法：将集成模型诱导的指数权重分布投影到满足公平性约束的凸分布集合上（作为重加权代理），然后在公平分布上训练弱学习器。理论分析表明投影训练分布会减少弱学习器的有效边缘，减少量由投影的KL散度控制。

Result: 证明了指数损失边界，其中收敛率取决于弱学习器边缘减去"公平性代价"项δ_t = √(KL(w^t || q^t)/2)。这直接量化了Boosting动态中的准确性-公平性权衡。标准基准实验验证了理论预测，并展示了具有稳定训练曲线的竞争性公平性-准确性权衡。

Conclusion: FairBoost成功将群体公平性约束融入Boosting框架，同时保持了可分析性的训练动态。该方法提供了理论保证，直接量化了准确性与公平性的权衡，为公平机器学习提供了新的理论视角和实践方法。

Abstract: Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a "fairness cost" term $δ_t = \sqrt{\mathrm{KL}(w^t \| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.

</details>


### [121] [Muon in Associative Memory Learning: Training Dynamics and Scaling Laws](https://arxiv.org/abs/2602.05725)
*Binghui Li,Kaifei Wang,Han Zhong,Pinyan Lu,Liwei Wang*

Main category: cs.LG

TL;DR: Muon优化器通过梯度矩阵符号更新参数，在理论和实验上均优于梯度下降，特别是在处理长尾数据分布时能缓解频率分量学习不平衡问题，实现指数级加速。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在实践中表现出色，但其动态特性和缩放行为在理论上尚不明确。本文旨在通过理论分析揭示Muon相比梯度下降的优势机制，特别是在处理具有层次频率谱的数据时的性能差异。

Method: 在线性联想记忆模型中研究Muon优化器，该模型包含softmax检索和查询-答案对的层次频率谱。分析在有标签噪声和无标签噪声两种情况下，Muon和梯度下降的学习动态。将Muon解释为隐式矩阵预处理器，并探讨其与坐标符号算子的关系。

Result: 在无噪声情况下，Muon相比梯度下降实现指数级加速；在噪声情况下（具有幂律衰减频率谱），推导出Muon的优化缩放定律，显示其优于梯度下降的缩放效率。实验在合成长尾分类和LLaMA风格预训练中验证了理论。

Conclusion: Muon优化器通过缓解频率分量学习不平衡问题，在理论和实践中均优于梯度下降。它可以解释为自适应任务对齐和块对称梯度结构产生的隐式矩阵预处理器，为优化器设计提供了新的理论见解。

Abstract: Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.

</details>


### [122] [CSRv2: Unlocking Ultra-Sparse Embeddings](https://arxiv.org/abs/2602.05735)
*Lixuan Guo,Yifei Wang,Tiansheng Wen,Yifan Wang,Aosong Feng,Bo Chen,Stefanie Jegelka,Chenyu You*

Main category: cs.LG

TL;DR: CSRv2是一种改进的超稀疏嵌入训练方法，通过渐进式k退火、监督对比目标等技术创新，将死神经元从80%降至20%，在k=2时获得14%准确率提升，实现与更密集嵌入相当的性能，同时大幅提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 当前密集嵌入虽然质量高但维度极高，导致存储、内存和推理延迟成本巨大。对比稀疏表示(CSR)虽能映射为k稀疏向量，但在超稀疏区域(超过80%神经元不活跃)性能严重下降，效率潜力未充分发挥。

Method: CSRv2采用三种核心技术：1) 渐进式k退火稳定稀疏学习；2) 监督对比目标提升表示质量；3) 全骨干网络微调确保端到端适应性。这些方法共同解决了超稀疏嵌入的可行性问题。

Result: CSRv2将死神经元从80%减少到20%，在k=2时获得14%准确率提升，使超稀疏嵌入性能与CSR(k=8)和MRL(32维)相当，仅使用两个活跃特征。相比MRL获得7倍加速，相比密集嵌入在计算和内存效率上提升高达300倍。

Conclusion: CSRv2使超稀疏嵌入变得实用而不牺牲性能，在文本和视觉任务中均表现出色。该方法为实时和边缘部署的AI系统拓宽了设计空间，在嵌入质量和效率都至关重要的场景中具有重要应用价值。

Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.

</details>


### [123] [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746)
*Xin Chen,Jie Zhang,Florian Tramer*

Main category: cs.LG

TL;DR: AutoInject：基于强化学习的自动化提示注入攻击框架，可生成通用、可迁移的对抗后缀，在保持良性任务性能的同时优化攻击成功率


<details>
  <summary>Details</summary>
Motivation: 提示注入是LLM代理中最关键的安全漏洞之一，但现有方法严重依赖人工红队和手动制作提示，限制了其可扩展性和适应性。需要一种自动化的优化方法来生成有效的对抗攻击

Method: 提出AutoInject强化学习框架，通过联合优化攻击成功率和良性任务效用保持来生成通用、可迁移的对抗后缀。采用黑盒方法支持基于查询的优化和向未见模型和任务的迁移攻击

Result: 仅使用1.5B参数的对抗后缀生成器，成功攻破了包括GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash在内的前沿系统，在AgentDojo基准测试中建立了更强的自动化提示注入研究基线

Conclusion: AutoInject为自动化提示注入攻击提供了有效的强化学习框架，能够生成通用且可迁移的对抗后缀，显著提高了攻击的可扩展性和适应性，为LLM代理安全研究提供了新的基准

Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.

</details>


### [124] [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749)
*Kai Ming Ting,Wei-Jie Xu,Hang Zhang*

Main category: cs.LG

TL;DR: 深度聚类声称优于k-means，但本文研究发现深度嵌入聚类(DEC)未能克服k-means的基本限制，而非深度学习方法通过利用数据分布信息反而能更好地解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常被认为优于传统k-means聚类，但这种优势通常只在图像数据集上得到验证。本文旨在探究深度嵌入聚类(DEC)是否真正克服了k-means聚类的根本限制，包括无法发现任意形状、大小和密度的聚类。

Method: 研究深度嵌入聚类(DEC)方法，该方法通过自编码器学习潜在表示，并基于k-means类过程进行聚类，以端到端方式优化。同时探索非深度学习方法如何利用数据集的聚类分布信息来应对这些基本限制。

Result: 研究发现深度嵌入聚类未能克服k-means的基本限制。更重要的是，发现非深度学习方法通过利用数据集的聚类分布信息，能够有效解决这些限制，实现深度聚类原本的目标。

Conclusion: 深度聚类方法并未有效利用底层数据分布信息，而非深度学习方法通过利用聚类分布信息能够更好地解决k-means的基本限制。这对深度聚类方法的有效性提出了重要质疑。

Abstract: Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.

</details>


### [125] [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)
*Xiandong Zou,Jianshu Li,Jing Huang,Pan Zhou*

Main category: cs.LG

TL;DR: VSD提出变分推测解码，将草稿训练建模为潜在路径的变分推断，通过最大化目标模型接受概率来提升解码效率，相比现有方法获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在训练与解码的不一致：训练时优化单条贪婪轨迹，但解码时需要验证和排序多条采样草稿路径。这种不匹配限制了推测解码的效率提升潜力。

Method: VSD将草稿训练建模为潜在路径的变分推断，通过最大化目标模型接受概率的ELBO目标函数。采用EM算法：E步从经过筛选的后验分布中采样MCMC样本，M步使用自适应拒绝加权和置信感知正则化最大化加权似然。

Result: 在LLM和MLLM上的广泛实验表明，VSD相比EAGLE-3获得最高9.6%的加速，相比ViSpec获得最高7.9%的加速，显著提升了解码效率。理论分析也证实VSD增加了期望接受长度和加速比。

Conclusion: VSD通过变分推断框架解决了推测解码中的训练-解码不一致问题，通过优化潜在路径的分布来最大化目标模型接受概率，在理论和实验上都证明了其有效性，为高效解码提供了新思路。

Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.

</details>


### [126] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 提出STC算法，通过修正源域数据的动作和奖励来适应目标域动态，解决跨域离线强化学习中的动态不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 在跨域离线强化学习中，直接合并源域和目标域数据集可能导致性能下降，因为存在动态不匹配问题。现有方法通过源域转移过滤或奖励修改可能无法充分利用源域数据价值。

Method: 提出选择性转移修正（STC）算法：1）使用逆策略模型和奖励模型修正源域转移的动作和奖励，使其与目标域动态对齐；2）利用前向动态模型保留修正后比原始转移更匹配目标动态的样本。

Result: 在多种具有动态偏移的环境中实验表明，STC算法相比现有基线方法取得了更优的性能。

Conclusion: STC算法通过修正源域数据而非简单过滤，能够更有效地利用源域数据实现跨域策略适应，解决了动态不匹配问题。

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [127] [How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs](https://arxiv.org/abs/2602.05779)
*Emily Dent,Jared Tanner*

Main category: cs.LG

TL;DR: 论文研究发现，深度网络中间层的高斯过程方差对训练稀疏激活函数网络至关重要，较大的方差能实现90%的激活稀疏度而不损失精度，为降低机器学习能耗提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 深度网络的中间层可表征为高斯过程，但高斯过程的方差参数在现有初始化策略中未被充分利用。本文旨在探索该方差参数对使用稀疏激活函数（如CReLU）的深度网络训练的重要性，特别是对激活稀疏性和训练稳定性的影响。

Method: 采用Edge-of-Chaos初始化策略，研究高斯过程方差参数对深度网络训练的影响。使用具有稀疏诱导特性的激活函数CReLU(τ,m)(x)=min(max(x-τ,0),m)，在DNN和CNN中测试不同方差初始化对激活稀疏性和训练效果的影响。

Result: 较大的高斯过程方差初始化能显著提高激活稀疏性，在DNN和CNN中实现高达90%的激活稀疏度，同时保持或接近完全精度。这种初始化还能提高训练过程的稳定性，为降低全连接层能耗提供了有效机制。

Conclusion: 高斯过程方差参数是深度网络训练中被忽视但至关重要的因素，特别是对于稀疏激活函数网络。通过优化该参数，可以在保持模型精度的同时实现极高的激活稀疏度，这为降低机器学习模型的能耗提供了有前景的技术路径。

Abstract: The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\text{CReLU}_{τ,m}(x)=\min(\max(x-τ,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.

</details>


### [128] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 提出DBC方法，首次将扩散桥模型用作分布RL的critic，直接建模Q值的逆累积分布函数，提高价值估计精度


<details>
  <summary>Details</summary>
Motivation: 现有扩散RL方法主要关注扩散策略，而忽略了扩散critic。由于策略优化依赖于critic，准确的价值估计比策略表达能力更重要。此外，RL任务具有随机性，critic更适合用分布模型描述

Method: 提出扩散桥critic（DBC），直接建模Q值的逆累积分布函数，利用扩散桥的强大分布匹配能力防止分布坍缩为平凡高斯分布。进一步推导解析积分公式处理DBC中的离散化误差

Result: 在MuJoCo机器人控制基准测试中，DBC相比之前的分布critic模型表现出优越性

Conclusion: DBC是首个将扩散桥模型用作critic的工作，是一个即插即用的组件，可以集成到大多数现有RL框架中，显著提高了分布RL的价值估计精度

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [129] [Selecting Hyperparameters for Tree-Boosting](https://arxiv.org/abs/2602.05786)
*Floris Jan Koster,Fabio Sigrist*

Main category: cs.LG

TL;DR: 本文实证比较了多种树提升超参数优化方法，发现SMAC方法明显优于其他方法，并得出超参数优化需要大量试验、默认值效果差、所有超参数都重要等结论。


<details>
  <summary>Details</summary>
Motivation: 树提升是处理表格数据的常用机器学习技术，但其样本外精度严重依赖多个超参数。本文旨在通过实证比较不同超参数优化方法，为实际应用提供指导。

Method: 使用59个回归和分类数据集，比较了随机网格搜索、TPE、GP-BO、Hyperband、SMAC和确定性全网格搜索等多种超参数优化方法。

Result: SMAC方法在所有比较方法中表现最佳。研究发现：(1)需要超过100次试验才能获得准确调优；(2)使用默认超参数值会导致模型精度很低；(3)所有考虑的超参数都对树提升精度有实质性影响；(4)对于回归任务，使用早停法选择提升迭代次数比将其纳入搜索空间效果更好。

Conclusion: SMAC是树提升超参数优化的最佳选择，超参数优化需要充分试验，所有超参数都同等重要，早停法在回归任务中效果显著。

Abstract: Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.

</details>


### [130] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 该论文将LDP下的隐私学习重新解释为迁移学习问题，提出了三种新技术：基于噪声二进制反馈的评估机制、模型反转和模型平均，以在不损害隐私的前提下提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 本地差分隐私（LDP）虽然提供了强大的隐私保护，但引入的噪声会显著降低数据效用。现有方法在保持隐私的同时难以维持良好的分类性能，需要新的技术来解决这一矛盾。

Method: 1. 将LDP下的隐私学习重新解释为迁移学习问题（噪声数据作为源域，干净数据作为目标域）
2. 提出基于噪声二进制反馈的数据集效用评估机制
3. 模型反转技术：通过反转决策边界来挽救性能不佳的分类器
4. 模型平均：根据估计的效用为多个反转分类器分配权重

Result: 1. 提供了LDP下的理论超额风险界限分析
2. 展示了所提方法如何降低超额风险
3. 在模拟和真实数据集上的实证结果显示分类准确率有显著提升

Conclusion: 该研究通过将LDP隐私学习重新框架为迁移学习问题，并提出专门设计的评估、反转和平均技术，成功在保持隐私保护的同时显著提高了分类性能，为解决LDP中的数据效用问题提供了有效方案。

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [131] [Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents](https://arxiv.org/abs/2602.05810)
*Quan M. Tran,Zhuo Huang,Wenbin Zhang,Bo Han,Koji Yatani,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: Bifrost是一种无需训练的方法，通过揭示上下文-轨迹相关性，利用上下文差异指导先前已解决轨迹的适应，以缓解上下文偏移带来的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 自主代理通过反思和迭代优化进行自我改进，重用成功任务轨迹作为上下文示例来辅助后续推理。然而，跨任务转移常引入上下文不匹配问题。现有方法要么丢弃轨迹，要么使用启发式方法处理，导致不可忽略的微调成本或性能无法保证。

Method: 提出Bifrost方法，基于发现的上下文-轨迹相关性（上下文偏移与轨迹偏移高度平行），利用上下文差异精确指导先前已解决轨迹向目标任务的适应。轨迹适应在表示层面使用代理隐藏状态进行，确保轨迹转换在共享空间中准确对齐目标上下文。

Result: 在多样化基准测试中，Bifrost始终优于现有的轨迹重用和微调自我改进方法，表明代理能够有效利用过去经验，即使存在显著的上下文偏移。

Conclusion: 通过揭示上下文-轨迹相关性并开发Bifrost方法，成功解决了自主代理在跨任务转移中的上下文不匹配问题，实现了无需训练的高效轨迹重用，显著提升了代理的自我改进能力。

Abstract: Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.

</details>


### [132] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 提出一个基于序列似然混合框架的CT重建置信度估计方法，为深度学习重建提供理论覆盖保证的置信区域


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，深度学习重建方法虽然性能优越，但缺乏可靠的不确定性量化方法，难以检测幻觉和提供置信度信息，限制了其在临床决策中的可信度

Method: 基于序列似然混合框架，在符合Beer-Lambert定律的泊松噪声对数线性前向模型下，为U-Net、U-Net集成和扩散模型等深度学习重建方法建立理论覆盖保证的置信区域

Result: 深度学习重建方法相比传统方法能产生显著更紧的置信区域，同时保持理论覆盖保证，能够检测重建图像中的幻觉并提供可解释的置信区域可视化

Conclusion: 该框架将深度学习模型不仅确立为强大的估计器，而且作为医学影像中具有不确定性感知能力的可靠工具，为临床决策提供可信支持

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [133] [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813)
*Artem Riabinin,Andrey Veprikov,Arman Bolatov,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出自适应学习率调度方法，针对范数约束优化器（如Muon和Lion），通过理论分析推导出warm-up和decay的自然出现，并开发出实用的自适应warm-up调度器，在LLaMA架构的大语言模型预训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度（特别是warm-up阶段）通常依赖启发式设置和手动调参，缺乏理论指导。对于norm-constrained优化器，需要更理论驱动的自适应调度方法。

Method: 1. 提出广义平滑性假设：局部曲率随次优性间隙减小；2. 在该假设下建立收敛保证，证明warm-up和decay自然产生；3. 开发实用调度器，仅需标准超参数，自动适应warm-up时长。

Result: 在LLaMA架构的大语言模型预训练中，自适应warm-up调度在所有实验设置中一致优于或至少匹配最佳手动调优的warm-up计划，无需额外超参数搜索。

Conclusion: 提出的理论框架为norm-constrained优化器的学习率调度提供了理论基础，开发的实用调度器能够自动适应warm-up时长，在保持性能的同时减少手动调参需求。

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

</details>


### [134] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 提出一种基于模糊测试和判别器的隐私保护合成数据生成方法，通过间接利用原始数据生成具有相同统计特性的测试数据集


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的合成数据生成方法存在两个主要问题：1）生成模型不够准确；2）仍然容易受到成员推理攻击或数据集重建攻击，因为训练过程直接使用了原始数据。需要一种既能保持原始数据统计特性又能保护隐私的合成数据生成方法。

Method: 受GAN启发，采用生成-判别框架，但使用模糊测试器作为生成器，从输入规范生成测试数据，同时保持原始数据的约束条件。判别器评估生成数据与原始数据的接近程度。通过演化样本并利用判别器选择"好样本"，生成既保护隐私又遵循原始数据统计分布的合成数据。

Result: 在四个用于评估最先进技术的数据集上进行了实验，结果表明该方法在生成具有高实用性的合成数据集方面具有潜力，同时能有效保护隐私。

Conclusion: 该方法能够生成既保持原始数据统计特性又保护隐私的合成数据集，为隐私保护数据共享提供了一种可行的解决方案。

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [135] [Visualizing the loss landscapes of physics-informed neural networks](https://arxiv.org/abs/2602.05849)
*Conor Rowan,Finn Murphy-Blanchard*

Main category: cs.LG

TL;DR: 该论文将损失函数景观分析从传统的图像分类扩展到物理信息机器学习，发现物理损失函数（Deep Ritz和强形式）的景观具有与数据驱动问题相似的平滑、良好条件和局部凸性特征。


<details>
  <summary>Details</summary>
Motivation: 损失函数景观研究主要集中在图像分类领域，而物理信息机器学习中损失函数由微分算子而非大数据回归定义，缺乏相应的景观分析。本文旨在将损失函数景观视角引入科学机器学习社区，并比较不同物理损失函数的特性。

Method: 首先全面回顾损失函数景观文献和现有物理信息机器学习中的相关研究，然后使用多种景观可视化技术对Deep Ritz形式和平方残差形式的物理损失函数进行实证研究。

Result: 发现物理信息神经网络的损失函数景观具有与文献中数据驱动分类问题相似的许多特性。出乎意料的是，两种物理损失函数形式通常产生相似的景观，在解附近表现出平滑、良好条件和凸性。

Conclusion: 本文成功将损失函数景观视角引入科学机器学习领域，挑战了关于物理信息网络损失函数复杂性的普遍直觉，发现其景观比预期更简单，为优化器成功训练提供了几何解释。

Abstract: Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a "loss landscape" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.

</details>


### [136] [Exact Recovery in the Data Block Model](https://arxiv.org/abs/2602.05852)
*Amir R. Asadi,Akbar Davoodi,Ramin Javadi,Farzad Parvaresh*

Main category: cs.LG

TL;DR: 该论文研究了带节点数据的块模型中的精确恢复问题，提出了Chernoff-TV散度来刻画精确恢复阈值，并提供了达到该阈值的有效算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界网络通常包含节点属性等额外数据，而传统随机块模型仅基于图连接性。研究如何利用节点数据作为辅助信息来改进社区检测的精确恢复。

Method: 引入Chernoff-TV散度来刻画数据块模型的精确恢复阈值，提出一个高效算法达到该阈值，并通过匹配的逆结果证明阈值以下的不可行性。

Result: 建立了数据块模型的尖锐精确恢复阈值，算法能够达到该阈值，仿真验证了结果并展示了利用节点数据作为辅助信息的优势。

Conclusion: 节点数据作为辅助信息能够显著改善社区检测的精确恢复性能，提出的Chernoff-TV散度提供了理论框架，算法具有实际可行性。

Abstract: Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.

</details>


### [137] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: 提出了首个针对扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架DLM-Scope，发现SAE在DLMs中与自回归LLMs表现不同，能降低早期层的交叉熵损失，并支持更有效的扩散时间干预。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）成为自回归大语言模型（LLMs）的有前景替代方案，需要为这类新兴模型开发专门的可解释性工具。目前稀疏自编码器（SAEs）已成为自回归LLMs机制可解释性的标准工具，但缺乏针对DLMs的相应框架。

Method: 提出了DLM-Scope框架，使用Top-K稀疏自编码器（SAEs）从扩散语言模型中提取可解释特征。研究了SAE插入对DLMs的影响，比较了与自回归LLMs的差异，并探索了SAE在DLMs解码顺序和训练后稳定性方面的新应用方向。

Result: 1. Top-K SAEs能忠实地提取可解释特征；2. SAE插入在DLMs早期层能降低交叉熵损失（与LLMs不同）；3. DLM的SAE特征支持更有效的扩散时间干预，优于LLM引导；4. SAE能为DLM解码顺序提供有用信号；5. SAE特征在DLMs训练后阶段保持稳定。

Conclusion: DLM-Scope为扩散语言模型的机制可解释性奠定了基础，展示了将SAEs应用于DLM相关任务和算法的巨大潜力，为这一新兴模型类别提供了专门的可解释性工具。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [138] [CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs](https://arxiv.org/abs/2602.05861)
*Seyedmasoud Mousavi,Ruomeng Xu,Xiaojing Zhu*

Main category: cs.LG

TL;DR: CFRecs是一个将反事实解释转化为可操作建议的新框架，通过图神经网络和图变分自编码器为推荐系统提供最小但高影响力的图结构修改建议。


<details>
  <summary>Details</summary>
Motivation: 虽然图神经网络广泛用于图结构数据学习，反事实图学习能提高模型可解释性，但现有反事实解释研究主要关注识别导致不同预测的相似图，缺乏将这些解释转化为实际可操作建议的框架。

Method: 提出CFRecs框架，采用两阶段架构：图神经网络(GNN)用于学习图表示，图变分自编码器(Graph-VAE)用于战略性地提出最小但高影响力的图结构和节点属性修改，以驱动推荐系统中的期望结果。

Result: 在Zillow的用户-房源交互数据上进行实验，证明了CFRecs的有效性，能够为购房者和卖家提供可操作建议，帮助他们应对竞争激烈的房地产市场并实现购房目标。

Conclusion: CFRecs不仅有效将反事实解释转化为可操作建议，还为图结构推荐系统提供了基于反事实推理的新视角，具有实际应用价值。

Abstract: Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.

</details>


### [139] [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863)
*Roger Girgis,Rodrigue de Schaetzen,Luke Rowe,Azalée Robitaille,Christopher Pal,Liam Paull*

Main category: cs.LG

TL;DR: 本文提出了Constrained GRPO，一种基于拉格朗日方法的GRPO扩展，用于约束策略优化。研究发现多分量优势估计会破坏约束学习，提出了标量化优势构造方法，在网格世界和机器人任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然GRPO已成为可扩展的无critic策略学习框架，但在具有显式行为约束的设置中扩展GRPO仍未被充分探索。需要开发能够处理约束的策略优化方法，特别是在依赖大型多模态基础模型的具身AI领域。

Method: 提出Constrained GRPO，基于拉格朗日松弛的GRPO扩展。使用指示器成本函数指定约束，通过拉格朗日松弛直接优化违反率。发现多分量优势估计会扭曲不同目标项的相对重要性，提出标量化优势构造方法以保持奖励和约束项之间的预期权衡。

Result: 在玩具网格世界中验证了预测的优化病理问题，证明标量化优势能够恢复稳定的约束控制。在机器人任务评估中，Constrained GRPO提高了约束满足度同时增加了任务成功率。

Conclusion: Constrained GRPO为具身AI领域中的约束策略优化提供了一个简单有效的方案，特别是在日益依赖大型多模态基础模型的场景中。标量化优势构造对于保持拉格朗日信号和有效约束执行至关重要。

Abstract: While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.

</details>


### [140] [Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks](https://arxiv.org/abs/2602.05873)
*Minyoung Kim*

Main category: cs.LG

TL;DR: 提出一种基于分数匹配的可扩展变分推理方法，用于大规模贝叶斯神经网络，避免重参数化采样，支持随机梯度，适用于视觉Transformer等大型网络。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络在不确定性量化、鲁棒性等方面优于点估计方法，但现有变分推理方法（如ELBO）在大规模神经网络中存在计算和技术限制。需要一种可扩展的变分推理方法。

Method: 提出基于分数匹配的变分推理方法，结合分数匹配损失和近端惩罚项，避免重参数化采样，允许使用随机梯度计算噪声无偏小批量分数，支持大规模神经网络。

Result: 在视觉识别和时间序列预测等基准测试中，该方法在大规模深度网络（包括视觉Transformer）上表现出有效性，支持更丰富的变分密度族。

Conclusion: 提出了一种可扩展的分数匹配变分推理方法，适用于大规模贝叶斯神经网络，解决了现有方法的计算限制，在多个任务上验证了其有效性。

Abstract: Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.

</details>


### [141] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 提出KernelGYM环境与TRLOO方法，训练Dr.Kernel-14B模型在GPU内核生成任务上超越Claude-4.5-Sonnet和GPT-5


<details>
  <summary>Details</summary>
Motivation: 高质量GPU内核对可扩展AI系统至关重要，但训练LLMs生成此类代码面临数据不足、环境脆弱、奖励黑客攻击和懒惰优化等问题

Method: 设计KernelGYM分布式GPU环境，提出TRLOO解决多轮RL中的偏置策略梯度问题，引入Profiling-based Rewards和Profiling-based Rejection Sampling缓解懒惰优化

Result: Dr.Kernel-14B在KernelBench Level-2子集上，31.6%生成的内核达到至少1.2倍加速，超越Claude-4.5-Sonnet(26.7%)和GPT-5(28.6%)；选择最佳候选时可达47.8%

Conclusion: 通过KernelGYM环境和TRLOO方法成功训练出在GPU内核生成任务上具有竞争力的模型，为AI系统开发提供了有效工具和训练框架

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [142] [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887)
*Tianqi Shen,Jinji Yang,Junze He,Kunhan Gao,Ziye Ma*

Main category: cs.LG

TL;DR: 提出一种模拟过参数化逃逸方向的方法，在不实际进行张量提升的情况下，通过模拟过参数化空间的逃逸方向来逃离低秩矩阵感知中的虚假局部最小值。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵感知等非凸问题存在大量虚假局部最小值，使得基于梯度的优化器难以收敛到全局最优。虽然过参数化可以将局部最小值转换为严格鞍点，但实际进行张量提升计算成本过高。

Method: 提出模拟Oracle方向（SOD）逃逸机制，通过数学框架将过参数化空间的逃逸方向投影到原始参数空间，确保从现有局部最小值严格降低目标函数值，而不实际进行张量提升。

Result: 数值实验表明，该框架能可靠地逃离局部最小值并促进收敛到全局最优，同时计算成本远低于显式张量过参数化方法。

Conclusion: 这是第一个能够保证逃离虚假局部最小值的确定性框架，无需随机扰动或启发式估计，展示了模拟过参数化在驯服挑战性优化景观方面的潜力，对非凸优化具有重要启示。

Abstract: Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.

</details>


### [143] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO是一种鲁棒的分布强化学习框架，通过将价值建模为跨时间步的连续流而非独立分位数，在噪声监督下实现更好的训练稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中训练RL系统面临噪声监督和域外泛化差的挑战，特别是在LLM后训练中。现有的分布RL方法虽然通过多分位数建模提高了鲁棒性，但仍将每个分位数作为独立标量学习，导致价值表示粗糙，缺乏对状态信息的细粒度条件化，在复杂和OOD条件下表现不佳。

Method: 提出DFPO框架，将价值建模为跨时间步的连续流，通过学习价值流场而非孤立的分位数预测来扩展价值建模。为稳定噪声反馈下的训练，进一步整合了沿价值流轨迹的条件风险控制和一致性约束。

Result: 在对话、数学推理和科学任务上的实验表明，DFPO在噪声监督下优于PPO、FlowRL和其他鲁棒基线，实现了更好的训练稳定性和泛化能力。

Conclusion: 通过将价值建模为连续流并整合条件风险控制和一致性约束，DFPO提供了一种更鲁棒的分布RL方法，能够更好地处理现实世界环境中的噪声监督和泛化挑战。

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [144] [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
*Han Li,Letian Zhu,Bohan Zhang,Rili Feng,Jiaming Wang,Yue Pan,Earl T. Barr,Sarro Federica,Zhaoyang Chu,He Ye*

Main category: cs.LG

TL;DR: ContextBench是一个面向过程的编码智能体上下文检索评估框架，包含1,136个任务和人工标注的黄金上下文，用于分析智能体在问题解决过程中如何检索和使用代码上下文。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编码智能体评估主要关注最终任务成功率，缺乏对智能体在问题解决过程中如何检索和使用代码上下文的深入洞察。需要过程导向的评估来理解智能体的上下文检索行为。

Method: 构建ContextBench数据集，包含8种编程语言的66个仓库中的1,136个问题解决任务，每个任务都有人工标注的黄金上下文。开发自动化评估框架跟踪智能体轨迹，测量上下文召回率、精确率和效率。

Result: 评估4个前沿LLM和5个编码智能体发现：1）复杂的智能体框架在上下文检索上只有边际收益；2）LLM普遍偏向召回率而非精确率；3）探索的上下文与使用的上下文之间存在显著差距。

Conclusion: ContextBench通过中间黄金上下文指标补充了现有的端到端基准测试，为理解编码智能体的问题解决过程提供了新视角，这些上下文为引导LLM在软件任务中的推理提供了有价值的中间信号。

Abstract: LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval ("The Bitter Lesson" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.

</details>


### [145] [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896)
*Alexander Kozachinskiy,Tomasz Steifer,Przemysław Wałȩga*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构用于解决PARITY问题，具有softmax、长度无关且多项式有界的位置编码、无layernorm，同时支持因果掩码和非因果掩码，并首次给出了Transformer解决PARITY问题的下界。


<details>
  <summary>Details</summary>
Motivation: Transformer架构已有近十年历史，但我们对其计算能力仍了解有限。特别是，单层Transformer能否解决PARITY问题？现有解决方案至少需要2层，且依赖不实用特性：长度相关的位置编码、hardmax、无正则化参数的layernorm，或不支持因果掩码。

Method: 提出新的Transformer构造方法，使用softmax激活函数、长度无关且多项式有界的位置编码、无layernorm层，同时支持有/无因果掩码。并首次证明单层单头Transformer无法解决PARITY问题。

Result: 成功构造出能够解决PARITY问题的Transformer架构，克服了现有方法的限制。同时证明单层单头Transformer无法解决PARITY问题，这是该问题的首个下界结果。

Conclusion: 本文为Transformer的计算能力提供了新的理论见解，展示了其解决PARITY问题的可能性与局限性，为理解Transformer的表达能力迈出了重要一步。

Abstract: The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.
  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.

</details>


### [146] [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)
*Seohyeon Cha,Huancheng Chen,Dongjun Kim,Haoran Zhang,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于正则化非对称校准的LLM后训练量化方法，通过对称与非对称校准的插值作为正则化，并设计了连续舍入和有界搜索算法，在保持计算成本可控的同时显著提升量化质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临内存和延迟成本高的挑战，后训练量化(PTQ)虽然能实现高效推理，但其效果严重依赖于量化目标和舍入过程。现有方法在激活不匹配问题上存在局限性，需要更鲁棒的量化方法。

Method: 提出将对称与非对称校准插值作为正则化，保持PTQ标准二次结构的同时增强对激活不匹配的鲁棒性。设计了简单的连续舍入过程自然融入非对称校准，并扩展为有界搜索算法，允许在量化质量和计算成本之间进行显式权衡。

Result: 在多个LLM家族、量化比特宽度和基准测试上的实验表明，基于正则化非对称校准目标的有界搜索方法相比PTQ基线持续改善了困惑度和准确率，同时仅带来适度且可控的额外计算成本。

Conclusion: 通过将校准插值作为正则化并设计有效的舍入搜索算法，实现了LLM后训练量化的显著改进，为高效部署大型语言模型提供了更鲁棒和可控的解决方案。

Abstract: Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.

</details>


### [147] [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903)
*András Balogh,Márk Jelasity*

Main category: cs.LG

TL;DR: 研究提出对抗序列生成方法来验证序列模型在象棋领域的正确性，发现所有模型都存在错误，但某些训练技术和数据集能显著改善模型正确性


<details>
  <summary>Details</summary>
Motivation: 序列模型通常基于样本序列训练，但样本训练能否捕捉语言的真实结构（世界模型）是一个关键问题。理论上最多只能保证生成有效序列，但无法保证生成所有有效序列。因此需要实用工具来验证序列模型的正确性

Method: 以象棋为研究领域，提出对抗序列生成方法验证序列模型的正确性。对抗者生成有效序列来迫使序列模型预测无效的下一步棋。还研究了棋盘状态探针在训练和攻击方法中的潜在应用

Result: 发现所有象棋模型都不完全正确，但某些训练技术和高质量数据集能显著改善模型正确性。棋盘状态探针在大多数模型中对下一个标记预测没有因果作用

Conclusion: 对抗序列生成是验证序列模型正确性的有效方法，能进行细粒度分析。虽然所有模型都存在错误，但通过改进训练技术和数据集选择可以显著提高模型正确性

Abstract: Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.

</details>


### [148] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 论文提出"chunky post-training"概念，指大语言模型在微调过程中学习到数据块中的虚假相关性，导致意外行为。作者开发了SURF和TURF工具来检测和追踪这些问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调使用多样化数据集，但这些数据集除了预期行为外，还编码了意外的模式（如格式与内容的相关性、狭窄的表述方式、数据筛选过程中的隐性关联）。这些模式对开发者不可见但对模型显著，导致模型产生令开发者惊讶的行为，例如拒绝以特定格式呈现的真实事实。

Method: 提出SURF（运行时检测意外行为的黑盒管道）和TURF（追踪失败到具体微调数据的工具）。将这些工具应用于前沿模型（Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3）和开源模型（Tülu 3）。

Result: 研究发现chunky post-training导致校准错误的行为，这些问题通常源于不平衡或未充分指定的微调数据块。工具成功检测到这些意外行为并追踪到具体的数据源。

Conclusion: 微调过程中的数据块结构会导致模型学习虚假相关性，产生意外行为。SURF和TURF工具能有效检测和诊断这些问题，为改进模型微调提供重要工具。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [149] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: PMD-mean是一种实用的强化学习算法，通过用采样策略的平均奖励近似对数配分函数，在LLM的大动作空间中实现更稳定高效的策略优化。


<details>
  <summary>Details</summary>
Motivation: 策略镜像下降(PMD)为强化学习提供了理论框架，但在LLM的大动作空间中，精确计算配分函数需要大量rollouts，这在实际应用中具有挑战性。需要一种更实用的近似方法。

Method: 提出PMD-mean算法：1) 用采样策略的平均奖励近似对数配分函数项；2) 在对数策略空间中进行回归；3) 该方法隐式地优化了带有自适应混合KL-χ²正则化的镜像下降子问题。

Result: PMD-mean在数学推理任务上表现出优越性能，具有更好的稳定性和时间效率。额外的χ²正则化限制了概率的大幅变化，在期望奖励较低时产生更保守的更新，增强了对有限样本估计误差的鲁棒性。

Conclusion: PMD-mean提供了一种实用的PMD近似方法，加深了对该算法的理解，并为LLM的强化学习算法提供了理论改进的途径。该方法在Kimi K1.5/K2等先进LLM训练中已有应用。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [150] [Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data](https://arxiv.org/abs/2602.05935)
*Sudeepta Mondal,Xinyi Mary Xie,Ruxiao Duan,Alex Wong,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 提出无需额外OOD数据集即可调优OOD检测器的新方法，解决现有方法依赖特定OOD数据集且性能不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测器需要依赖特定的OOD数据集进行调优，但这些数据集可能难以获取或不具代表性，导致检测器性能高度依赖于所选数据集，且在实际应用中可能无法获得合适的OOD数据

Method: 提出一种新的通用OOD检测器调优方法，仅使用训练神经网络时所用的数据，无需额外OOD数据集。该方法通过分析神经网络内部激活模式来构建检测器，并建立了强基线方法作为对比

Result: 新方法在参数较多的OOD检测器家族中相比基线方法有显著提升，在参数较少的检测器家族中表现相当。实验表明该方法能有效解决OOD数据集依赖问题

Conclusion: 该研究首次形式化并解决了无需额外OOD数据集调优检测器的问题，提出的方法为实际应用提供了更可靠的解决方案，特别是在难以获取代表性OOD数据的情况下

Abstract: Existing out-of-distribution (OOD) detectors are often tuned by a separate dataset deemed OOD with respect to the training distribution of a neural network (NN). OOD detectors process the activations of NN layers and score the output, where parameters of the detectors are determined by fitting to an in-distribution (training) set and the aforementioned dataset chosen adhocly. At detector training time, this adhoc dataset may not be available or difficult to obtain, and even when it's available, it may not be representative of actual OOD data, which is often ''unknown unknowns." Current benchmarks may specify some left-out set from test OOD sets. We show that there can be significant variance in performance of detectors based on the adhoc dataset chosen in current literature, and thus even if such a dataset can be collected, the performance of the detector may be highly dependent on the choice. In this paper, we introduce and formalize the often neglected problem of tuning OOD detectors without a given ``OOD'' dataset. To this end, we present strong baselines as an attempt to approach this problem. Furthermore, we propose a new generic approach to OOD detector tuning that does not require any extra data other than those used to train the NN. We show that our approach improves over baseline methods consistently across higher-parameter OOD detector families, while being comparable across lower-parameter families.

</details>


### [151] [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 该研究探索基于黎曼几何的降维方法，这些方法尊重数据的底层流形结构，特别是将主测地线分析(PGA)作为流形值数据的非线性PCA扩展，并改进判别分析，在代表性数据集上展示了优于欧几里得方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得降维方法在处理约束于弯曲空间（如超球面、对称正定流形）的数据时可能不够准确，需要尊重数据底层流形结构的几何感知方法。

Method: 研究主测地线分析(PGA)作为流形值数据的非线性PCA扩展，并扩展判别分析，通过黎曼几何适配其他已知降维方法，利用测地线距离、切空间表示和内在统计度量。

Result: 在代表性数据集上的实验结果表明，黎曼方法相比欧几里得方法提供了改进的表示质量和分类性能，特别是在约束于弯曲空间的数据上。

Conclusion: 几何感知的降维在现代机器学习和数据科学应用中具有重要意义，黎曼几何方法能够更忠实地捕捉数据的底层结构。

Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.

</details>


### [152] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出OrthoMerge方法，在正交群的黎曼流形上进行模型合并，以保持预训练权重的几何结构，缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在欧几里得空间进行线性算术操作，破坏了预训练权重的内在几何特性（如超球面能量），需要一种能保持权重几何结构的合并方法

Method: 1) 通过正交微调获得任务特定的正交矩阵，映射到李代数进行合并；2) 对于非OFT方法微调的模型，采用正交-残差解耦策略，通过正交Procrustes问题提取正交分量在正交群流形上合并，线性残差通过标准加法合并

Result: 大量实验证明OrthoMerge能有效缓解灾难性遗忘，并在多样化任务上保持模型性能

Conclusion: 在正交群的黎曼流形上进行模型合并能更好地保持预训练权重的几何结构，是比传统欧几里得空间线性合并更有效的模型集成方法

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [153] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 本文提出基于f-散度的统一对齐框架，包含f-GRPO和f-HAL两种方法，在RLVR和PA任务上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法主要基于对齐与未对齐响应分布之间的散度估计，但缺乏对一般对齐场景（如仅有环境奖励的RLVR）的统一框架

Method: 提出基于f-散度变分表示的f-GRPO（在线强化学习）和f-HAL（混合在线/离线目标）两类对齐目标

Result: 理论证明这些目标能提升对齐后的平均奖励；在数学推理（RLVR）和安全对齐（PA）任务上验证了方法的优越性和灵活性

Conclusion: 基于f-散度的统一对齐框架为LLM对齐提供了理论保证和实际有效的解决方案，在多种对齐场景中表现优异

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [154] [Breaking Symmetry Bottlenecks in GNN Readouts](https://arxiv.org/abs/2602.05950)
*Mouad Talhi,Arne Wolf,Anthea Monod*

Main category: cs.LG

TL;DR: 论文发现GNN中线性置换不变读出层（如sum/mean pooling）会丢失重要的对称感知信息，提出了基于投影算子的不变读出层来克服这一限制。


<details>
  <summary>Details</summary>
Motivation: GNN在区分非同构图方面存在固有局限性，通常归因于消息传递机制。本文发现读出层存在独立瓶颈，线性置换不变读出会丢失重要的对称感知信息。

Method: 使用有限维表示理论证明线性置换不变读出会投影到置换作用的固定子空间。提出基于投影算子的不变读出，将节点表示分解为对称感知通道并用非线性不变统计量汇总。

Result: 理论证明线性读出会丢失非平凡对称感知信息。实验显示仅替换读出层就能让固定编码器区分WL-hard图对，并在多个基准测试中提升性能。

Conclusion: 读出层设计是GNN表达能力的关键且被低估的因素，提出的投影算子不变读出能保留线性平均无法捕获的信息，显著提升GNN表达能力。

Abstract: Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.

</details>


### [155] [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961)
*Arran Carter,Sanghyeok Choi,Kirill Tamogashev,Víctor Elvira,Nikolay Malkin*

Main category: cs.LG

TL;DR: 论文提出离散扩散采样器的离策略训练技术，将其推广到任意分布间的桥接任务，并应用于图像生成模型的离散隐空间后验采样。


<details>
  <summary>Details</summary>
Motivation: 当前扩散采样器主要研究连续空间采样，在离散空间的应用探索不足，且现有离散扩散采样器未能充分利用连续空间采样的常用技术。

Method: 1. 为离散扩散采样器引入离策略训练技术；2. 将离散扩散采样器推广到任意两个分布间的桥接任务，首次在离散域引入数据到能量的薛定谔桥训练；3. 将提出的扩散采样器应用于图像生成模型离散隐空间的数据无关后验采样。

Result: 离策略训练技术提高了离散采样器在已有和新合成基准测试上的性能；成功实现了离散域的数据到能量薛定谔桥训练；展示了在图像生成模型离散隐空间后验采样的应用效果。

Conclusion: 通过引入离策略训练技术和推广到分布桥接任务，显著提升了离散扩散采样器的性能和应用范围，填补了离散空间采样技术的空白。

Abstract: Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.

</details>


### [156] [A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders](https://arxiv.org/abs/2602.05967)
*Mohamad Amin Jamshidi,Mehrbod Zarifi,Zolfa Anvari,Hamed Ghafarirad,Mohammad Zareinejad*

Main category: cs.LG

TL;DR: 提出基于LSTM和随机森林的混合算法，用于液压缸非线性摩擦力估计，相比传统解析模型具有更好的适应性和实时性


<details>
  <summary>Details</summary>
Motivation: 液压系统在工业中广泛应用，但液压缸的摩擦力严重影响其精度控制。现有解析模型（如LuGre模型）适应性差、计算效率低，难以适应不同工况变化

Method: 采用数据驱动的混合算法，结合长短期记忆网络（LSTM）进行特征检测，随机森林进行摩擦力估计，利用实验液压测试装置获取训练数据

Result: 算法在不同工况和外部负载变化下保持稳定，模型误差小于10%，单次估计计算成本仅1.51毫秒，适合实时应用，性能优于LuGre模型

Conclusion: 提出的混合算法克服了传统解析模型的局限性，实现了高精度和计算效率的平衡，为液压系统实时摩擦力估计提供了有效解决方案

Abstract: Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.

</details>


### [157] [Inverse Depth Scaling From Most Layers Being Similar](https://arxiv.org/abs/2602.05970)
*Yizhou Liu,Sara Kangaslahti,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 研究发现大语言模型中损失与深度成反比，这种缩放关系源于残差网络的结构偏置和目标函数不兼容平滑动态，导致层间功能相似而非组合学习，效率低下但稳健。


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放定律描述了损失与模型大小的关系，但深度和宽度对性能的影响可能不同，需要更详细的研究来量化深度如何影响损失。

Method: 通过分析大语言模型和玩具残差网络，研究深度对损失的影响，发现损失与深度成反比关系。

Result: 损失与深度成反比，这种缩放关系源于功能相似的层通过集成平均减少误差，而非组合学习或离散化平滑动态。这种机制效率低下但稳健。

Conclusion: 要提高大语言模型效率，可能需要架构创新来促进深度的组合性使用，而非当前的功能相似层集成平均机制。

Abstract: Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.

</details>


### [158] [Clifford Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.05977)
*Matthias Wolff,Francesco Alesiani,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: ClKAN是一种用于任意克利福德代数空间中函数逼近的灵活高效架构，通过随机拟蒙特卡洛网格生成解决高维代数指数级扩展问题，并引入新的批归一化策略处理可变域输入。


<details>
  <summary>Details</summary>
Motivation: 克利福德代数在科学和工程中有广泛应用，但传统方法在处理高维代数时面临指数级扩展的挑战，需要开发更高效的函数逼近架构。

Method: 提出Clifford Kolmogorov-Arnold Network (ClKAN)架构，采用随机拟蒙特卡洛网格生成技术解决高维代数扩展问题，并引入新的批归一化策略处理可变域输入。

Result: ClKAN在合成和物理启发任务中得到验证，展示了在科学发现和工程应用中的有效性。

Conclusion: ClKAN为克利福德代数空间中的函数逼近提供了一种灵活高效的解决方案，能够处理高维代数的指数扩展问题，具有广泛的应用前景。

Abstract: We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.

</details>


### [159] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 提出Layer-wise LoRA方法，通过选择性地在关键层应用LoRA适配，进一步减少可训练参数（最多减少50%），同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增长，即使LoRA等参数高效微调方法（减少99%参数）仍显不足，需要进一步降低微调成本

Method: 系统性地选择少数关键层进行LoRA微调，通过测量各层对内部表示变化的贡献来识别最相关的层，该方法与现有低秩适配技术正交且兼容

Result: 在编码器架构上，GLUE基准测试性能下降可忽略；在解码器架构上，数学问题解决和编码任务性能有小幅下降甚至提升；在多模态模型上也获得与全层LoRA相当的结果

Conclusion: 通过层选择策略可以进一步减少LoRA的可训练参数（最多50%），同时保持预测性能，为大规模模型的高效微调提供了新方向

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [160] [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993)
*Peter Holderrieth,Douglas Chen,Luca Eyring,Ishin Shah,Giri Anantharaman,Yutong He,Zeynep Akata,Tommi Jaakkola,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: 提出Diamond Maps：一种可高效适应任意奖励的随机流映射模型，能在推理时快速对齐用户偏好，无需重新训练


<details>
  <summary>Details</summary>
Motivation: 现有流和扩散模型在训练后适应用户偏好或约束时成本高且脆弱（奖励对齐问题），需要设计本身就可适应的生成模型

Method: 设计Diamond Maps：随机流映射模型，将多步模拟摊销为单步采样器，同时保留随机性以实现最优奖励对齐，支持高效一致的价值函数估计

Result: 实验显示Diamond Maps可通过GLASS Flows蒸馏高效学习，奖励对齐性能更强，比现有方法扩展性更好

Conclusion: 为在推理时快速适应任意偏好和约束的生成模型提供了实用路径

Abstract: Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.

</details>


### [161] [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996)
*Leo Zhang,James Martens*

Main category: cs.LG

TL;DR: 提出正交自注意力机制（OSA）解决标准Softmax自注意力在无跳跃连接架构中的不稳定性问题，通过矩阵指数映射保持正交性，实现线性复杂度并保证良好条件数。


<details>
  <summary>Details</summary>
Motivation: 标准Softmax自注意力在无跳跃连接的Transformer架构中会导致秩塌陷和雅可比矩阵条件数差的问题，限制了这类架构的训练稳定性。

Method: 设计正交自注意力机制，通过将查询-键值形成的斜对称矩阵映射到矩阵指数来参数化正交注意力矩阵，利用低秩结构实现线性复杂度，并提供初始化方案保证雅可比矩阵良好条件数。

Result: OSA能够有效避免秩塌陷和条件数问题，使无跳跃连接和归一化层的Transformer更容易训练，同时保持线性计算复杂度和内存成本。

Conclusion: 正交自注意力机制为解决Transformer无跳跃连接架构中的训练稳定性问题提供了有效方案，具有理论保证和实际可行性。

Abstract: Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.

</details>


### [162] [On Computation and Reinforcement Learning](https://arxiv.org/abs/2602.05999)
*Raj Ghugare,Michał Bortkiewicz,Alicja Ziarko,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出了一种可调节计算量的强化学习架构，证明了增加计算量而非参数数量能提升策略性能并增强对长时程任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架无法形式化地分析计算量对策略学习的影响，且深度RL通常使用固定架构的神经网络，混淆了计算量和参数数量的作用。需要建立理论框架来研究计算量如何影响策略能力。

Method: 基于算法学习和无模型规划的前期工作，提出了一种最小化架构，该架构可以使用可变的计算量。通过形式化定义计算有界策略，并证明增加计算量能解决更多问题。

Result: 在31个不同的在线和离线RL任务上的实验表明：(1)该架构仅通过增加计算量就能获得更强的性能；(2)与使用多达5倍参数的普通前馈网络或深度残差网络相比，在长时程测试任务上表现出更强的泛化能力。

Conclusion: 计算量是强化学习策略能力的关键因素，独立于参数数量。提出的可变计算量架构为理解计算在RL中的作用提供了理论基础，并展示了实际性能优势。

Abstract: How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.

</details>


### [163] [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014)
*Shunxing Yan,Han Zhong*

Main category: cs.LG

TL;DR: 本文研究了Thompson采样在随机多臂老虎机中的统计推断性质，发现乐观机制能够稳定采样过程，使得即使在自适应数据收集下也能进行有效的渐近推断。


<details>
  <summary>Details</summary>
Motivation: Thompson采样（TS）在随机多臂老虎机中广泛应用，但在自适应数据收集下的推断性质很微妙。经典渐近理论可能失效，因为臂特定的样本量是随机的，并且通过动作选择规则与奖励耦合。需要研究如何在这种自适应设置下实现有效的统计推断。

Method: 研究K臂高斯老虎机，识别乐观机制作为恢复稳定性的关键机制。稳定性是有效渐近推断的充分条件，要求每个臂的拉动次数集中在确定性尺度周围。分析了两种乐观修改：1）方差膨胀TS；2）保持后验方差不变但添加显式均值奖励到后验均值的方法。

Result: 1）证明了方差膨胀TS对于任意K≥2都是稳定的，包括多个臂都是最优的挑战性情况，解决了Halder等人（2025）提出的开放问题；2）分析了另一种乐观修改方法，并建立了相同的稳定性结论。两种方法都只带来轻微的额外遗憾成本。

Conclusion: 适当实现的乐观机制能够稳定Thompson采样，使得在多臂老虎机中能够进行渐近有效的推断，同时只带来轻微的遗憾成本增加。这为在自适应数据收集设置下进行可靠统计推断提供了理论基础。

Abstract: Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.

</details>


### [164] [Mechanisms of AI Protein Folding in ESMFold](https://arxiv.org/abs/2602.06020)
*Kevin Lu,Jannik Brinkmann,Stefan Huber,Aaron Mueller,Yonatan Belinkov,David Bau,Chris Wendler*

Main category: cs.LG

TL;DR: ESMFold折叠蛋白质分为两个计算阶段：早期模块初始化成对生化信号，晚期模块发展成对空间特征


<details>
  <summary>Details</summary>
Motivation: 研究蛋白质结构预测模型如何折叠蛋白质，特别是ESMFold如何折叠β发夹这一常见结构基序

Method: 通过对抗性干预模型潜在变量，追踪ESMFold折叠β发夹的过程，分析模型内部表示

Result: 识别出折叠主干中的两个计算阶段：早期模块将残基身份和生化特征（如电荷流）从序列表示转移到成对表示；晚期模块在成对表示中积累距离和接触信息

Conclusion: ESMFold的结构决策机制可以被定位、通过可解释表示进行追踪，并能通过强因果效应进行操控

Abstract: How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.

</details>


### [165] [Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering](https://arxiv.org/abs/2602.06022)
*Miranda Muqing Miao,Young-Min Cho,Lyle Ungar*

Main category: cs.LG

TL;DR: CORAL是一种推理时引导方法，通过正则化MLP探针从模型内部激活中提取分布式正确性信号，无需重新训练即可显著提升准确率和校准度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在指令微调和偏好对齐后存在持续校准不足问题，重新训练成本高昂，现有推理时引导方法主要优化正确性的代理指标而非正确性本身

Method: 提出CORAL方法：使用权重衰减MLP探针从模型内部激活中捕获分布式正确性信号，通过正则化推理时引导优化正确性

Result: 在三个7B参数模型上，CORAL平均提升准确率10%，降低预期校准误差50%；在四个保留基准测试集上，平均提升准确率14%，降低校准误差49%

Conclusion: CORAL提供了一种计算高效、可迁移且关注校准的方法，证明通过正则化探针可以从模型内部提取分布式信息，改善多项选择题推理性能

Abstract: Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.

</details>


### [166] [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 该论文为主动推理中的期望自由能最小化提供了首个理论保证，证明"足够的好奇心"能同时确保贝叶斯后验一致性和无遗憾优化，并建立了与经典贝叶斯实验设计和贝叶斯优化的理论联系。


<details>
  <summary>Details</summary>
Motivation: 主动推理通过期望自由能统一探索与利用，平衡认知价值（信息增益）和实用价值（任务性能），但缺乏理论指导来确定何时这种平衡能产生连贯学习和高效决策。不足的好奇心会导致短视利用和不确定性无法解决，而过度的好奇心会引发不必要的探索和遗憾。

Method: 建立理论框架分析期望自由能最小化代理，证明"足够好奇心"的单一要求能同时确保贝叶斯后验一致性和有界累积遗憾。分析该机制如何依赖于初始不确定性、可识别性和目标对齐，将主动推理与经典贝叶斯实验设计和贝叶斯优化联系起来。

Result: 提供了首个理论保证，表明足够的好奇心能同时确保自洽学习（贝叶斯后验一致性）和无遗憾优化（有界累积遗憾）。进一步将这些理论转化为实际设计指南，用于调整混合学习-优化问题中的认知-实用权衡，并通过真实世界实验验证。

Conclusion: 该研究为主动推理中的期望自由能最小化建立了坚实的理论基础，提供了理论保证和实用设计指南，连接了主动推理与经典贝叶斯方法，为调整探索-利用权衡提供了系统框架。

Abstract: Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.

</details>


### [167] [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031)
*Claus Hofmann,Christian Huber,Bernhard Lehner,Daniel Klotz,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: AP-OOD是一种新颖的文本OOD检测方法，通过利用token级信息超越简单的平均聚合，在无监督和监督设置下都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测面临的关键挑战是如何有效利用和聚合语言模型中的token嵌入来获得OOD分数。现有方法通常使用简单的平均聚合，未能充分利用token级别的信息。

Method: AP-OOD是一种半监督方法，灵活地在无监督和监督设置之间插值，允许使用有限的辅助异常数据。该方法超越简单的平均聚合，通过利用token级别的信息来获得更准确的OOD分数。

Result: AP-OOD在文本OOD检测中达到了新的最先进水平：在无监督设置下，在XSUM摘要任务中将FPR95从27.84%降低到4.67%，在WMT15 En-Fr翻译任务中将FPR95从77.08%降低到70.37%。

Conclusion: AP-OOD通过有效利用token级信息，显著提升了文本OOD检测的性能，为机器学习模型的可靠部署提供了更强大的工具。

Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.

</details>


### [168] [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033)
*Luca M. Schulze Buschoff,Konstantinos Voudouris,Can Demircan,Eric Schulz*

Main category: cs.LG

TL;DR: 通过监督微调或强化学习交互训练，视觉语言模型在物理任务上的表现有所提升，但未能学习到可泛化的物理直觉规则


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型缺乏对物理世界的良好直觉，虽然监督微调能提升简单物理任务表现，但模型并未学到能泛化到新情境的稳健物理规则

Method: 基于认知科学研究，假设模型需要通过与环境交互来学习物理动态；使用强化学习让模型通过环境交互进行训练

Result: 交互学习能提升模型在特定任务内的表现，但无法产生可泛化的物理直觉；在共享视觉统计特征和物理原理的相关任务上，模型无法可靠泛化

Conclusion: 无论是监督微调还是交互学习，当前方法都无法让视觉语言模型获得真正可泛化的物理直觉，需要新的学习范式

Abstract: Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>


### [169] [Pseudo-Invertible Neural Networks](https://arxiv.org/abs/2602.06042)
*Yamit Ehrlich,Nimrod Berman,Assaf Shocher*

Main category: cs.LG

TL;DR: 本文提出了非线性伪逆（PInv）的推广，引入满射伪可逆神经网络（SPNN），用于解决非线性逆问题，特别是零样本逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统Moore-Penrose伪逆仅适用于线性系统，无法处理非线性映射。现实世界中的许多逆问题（如图像退化、语义抽象）都是非线性的，需要一种通用的非线性伪逆方法。

Method: 提出Surjective Pseudo-invertible Neural Networks (SPNN)，设计具有可处理非线性伪逆的架构。定义非线性反向投影（NLBP），确保非线性映射的一致性约束。将扩散模型中的零空间投影方法扩展到非线性退化问题。

Result: SPNN能够实现非线性伪逆，满足基本几何性质。NLBP方法保证非线性映射的一致性约束。该方法能够处理从光学畸变到语义抽象（如分类）的各种非线性信息损失，实现零样本逆问题求解。

Conclusion: 本文提出的非线性伪逆和SPNN架构为非线性逆问题提供了通用解决方案，特别是扩展了零样本逆问题的应用范围，无需重新训练扩散先验即可实现精确的语义控制。

Abstract: The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or "Back-Projection", $x' = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, "degradation" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.

</details>


### [170] [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043)
*Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Share是一种参数高效的持续微调方法，通过学习和动态更新单个共享低秩子空间，实现跨任务和模态的无缝适应，显著减少参数和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型的高效持续适应对实际部署至关重要，但面临灾难性遗忘和高重训练成本挑战。现有参数高效调优方法（如LoRA）缺乏严格的持续学习和知识整合机制。

Method: Share构建一个基础子空间来提取过去任务的核心知识，通过识别关键子空间方向增量整合新信息。每个新任务的知识都被纳入这个不断演化的子空间，促进前向知识转移，同时最小化灾难性干扰。

Result: 相比传统LoRA方法，Share实现了高达100倍的参数减少和281倍的内存节省，性能与联合训练模型相当。单个Share模型可以替代数百个任务特定的LoRA适配器。

Conclusion: Share为大规模AI系统中的终身学习提供了一个实用且可扩展的解决方案，在图像分类、自然语言理解、3D姿态估计和文本到图像生成等多个领域验证了其有效性。

Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [171] [Wasure: A Modular Toolkit for Comprehensive WebAssembly Benchmarking](https://arxiv.org/abs/2602.05488)
*Riccardo Carissimi,Ben L. Titzer*

Main category: cs.PF

TL;DR: Wasure是一个模块化、可扩展的命令行工具包，用于简化和比较WebAssembly基准测试，支持研究人员和开发者进行更系统、透明和深入的WebAssembly引擎评估。


<details>
  <summary>Details</summary>
Motivation: WebAssembly已成为跨平台高效执行的关键编译目标，但其性能基准测试面临多维挑战：不仅取决于运行时引擎的选择，还涉及硬件架构、应用领域、源语言、基准测试套件和运行时配置等多个因素。目前缺乏系统化的评估工具。

Method: 开发了Wasure工具包，这是一个模块化、可扩展的命令行工具，用于执行和比较WebAssembly基准测试。同时，对Wasure中包含的基准测试套件进行了动态分析，包括代码覆盖率、控制流和执行模式等方面的分析。

Result: 动态分析揭示了基准测试套件在代码覆盖率、控制流和执行模式方面存在显著差异，强调了基准测试多样性的重要性。Wasure工具能够支持更系统、透明和深入的WebAssembly引擎评估。

Conclusion: Wasure工具包为WebAssembly性能评估提供了一个实用的解决方案，通过模块化和可扩展的设计简化了基准测试的执行和比较。基准测试套件的动态分析结果表明，多样化的基准测试对于全面评估WebAssembly引擎性能至关重要。

Abstract: WebAssembly (Wasm) has become a key compilation target for portable and efficient execution across diverse platforms. Benchmarking its performance, however, is a multi-dimensional challenge: it depends not only on the choice of runtime engines, but also on hardware architectures, application domains, source languages, benchmark suites, and runtime configurations. This paper introduces Wasure, a modular and extensible command-line toolkit that simplifies the execution and comparison of WebAssembly benchmarks. To complement performance evaluation, we also conducted a dynamic analysis of the benchmark suites included with Wasure. Our analysis reveals substantial differences in code coverage, control flow, and execution patterns, emphasizing the need for benchmark diversity. Wasure aims to support researchers and developers in conducting more systematic, transparent, and insightful evaluations of WebAssembly engines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [172] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出可扩展的HPC解决方案BioFVM，用于分子扩散建模，相比现有方法实现200倍加速和36%内存减少


<details>
  <summary>Details</summary>
Motivation: 基于代理的细胞模型在模拟组织演化时需要处理个体细胞行为、细胞间相互作用及微环境响应。关键挑战是将细胞分辨率模型扩展到真实规模的肿瘤模拟，这对疾病数字孪生模型开发至关重要，需要高性能计算来处理每时间步数万亿次操作。

Method: 提出可扩展的HPC解决方案，采用高效的有限体积法框架实现分子扩散建模。系统评估了新颖的可扩展生物有限体积法库，并对现有解决方案进行了全面的性能分析。

Result: HPC方案相比当前最先进解决方案实现了近200倍的速度提升和高达36%的内存使用减少。

Conclusion: 该方案为高效计算下一代生物问题铺平了道路，推动了真实规模肿瘤模拟和疾病数字孪生模型的发展。

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [173] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流社区峰会识别了工作流采用的关键障碍，并提出了技术、政策和社区层面的解决方案，强调从计算性能转向科学影响评估，建立标准化模式，培养国际社区，以及投资人力资本。


<details>
  <summary>Details</summary>
Motivation: 科学工作流在现代研究中对于协调分布式计算、管理大数据和确保可重复性至关重要，但面临采用障碍。2025年工作流社区峰会旨在召集国际专家，识别关键挑战并制定解决方案，以推进科学发现。

Method: 通过2025年6月6日在阿姆斯特丹举行的国际工作流社区峰会，汇集专家讨论，识别工作流采用的关键障碍，并提出跨技术、政策和社区维度的行动方案。

Result: 识别了四大关键障碍：系统通用性与领域特定性之间的张力、工作流系统长期可持续性问题、工作流开发者维护者缺乏认可、标准化/资金/培训/跨学科合作不足。提出了四个行动方向：从计算性能转向科学影响评估、建立标准化模式和社区基准、培养国际社区并吸引资金方参与、投资工作流工程角色和教育培训。

Conclusion: 科学工作流对现代研究至关重要，但需要系统性解决方案来克服采用障碍。峰会提出的技术、政策和社区行动方案为推进工作流生态系统提供了路线图，强调需要评估指标转变、标准化建设、社区发展和人力资本投资，以实现更有效的科学发现。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [174] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL 是一个利用大语言模型进行微服务资源自动扩展的框架，通过思维链推理诊断性能问题并推荐资源分配，无需针对特定部署进行重新训练。


<details>
  <summary>Details</summary>
Motivation: 随着应用从单体架构转向微服务和无服务器架构，自动扩展成为平衡资源利用和服务质量的关键机制。现有方法要么是需要大量训练的不透明学习模型，要么是难以泛化的脆弱手工规则，需要一种能够适应快速演变的微服务部署的通用资源分配器。

Method: ORACL 将运行时遥测数据（包括 Pod、副本、CPU/内存使用率、延迟、SLO 和故障信号）转换为语义自然语言状态描述，然后调用 LLM 生成可解释的中间推理轨迹。该推理过程识别可能的根本原因，剪枝动作空间，并在策略约束下发出安全的分配决策。

Result: 在代表性开源微服务工作负载上的实验表明，ORACL 将根本原因识别准确率提高了 15%，训练速度提升了高达 24 倍，在短期场景中服务质量提高了 6%，且无需针对特定部署进行重新训练。

Conclusion: 大语言模型可以作为通用的少样本资源分配器，适应快速演变的微服务部署。ORACL 通过思维链推理提供可解释的决策，在保持安全约束的同时提高了自动扩展的性能和效率。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [175] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一个分布式共识协议，通过将拜占庭容错协议嵌入到崩溃容错协议中，谨慎信任TEE硬件，在保持高性能的同时确保TEE平台被攻破时的数据完整性。


<details>
  <summary>Details</summary>
Motivation: 当前分布式账本系统结合CFT共识和硬件TEE来增强弹性，但TEE硬件可能受到攻击，这会破坏分布式账本精心设计的保证。需要一种既能利用TEE优势，又能在TEE被攻破时保持完整性的解决方案。

Method: Proteus通过精心重构CFT和BFT协议，使它们的结构对齐，从而将BFT协议嵌入到CFT协议中，无需额外消息。这种设计谨慎信任TEE的保证，在TEE正常时使用CFT协议，在TEE被攻破时自动切换到BFT协议。

Result: Proteus在保持与常规TEE增强共识协议相当性能的同时，能够保证在TEE平台被攻破时的数据完整性。协议设计实现了无额外消息开销的BFT嵌入。

Conclusion: Proteus提供了一种新的分布式共识方法，通过谨慎信任TEE硬件并将BFT协议嵌入CFT协议，在享受TEE性能优势的同时，确保系统在面对TEE平台攻击时的完整性保护。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [176] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 论文揭示了Dolev-Reischuk下界中二次通信成本的本质：主要来自结果传播阶段，而非达成一致性决策阶段。


<details>
  <summary>Details</summary>
Motivation: Dolev-Reischuk下界表明拜占庭容错协议需要Ω(f²+n)消息，但这一二次成本到底支付了什么？是达成一致性决策的困难，还是仅仅传播结果给所有处理器的成本？

Method: 引入ε-BA松弛概念，允许ε比例的正确处理器输出错误结果，证明在f < n(1/3 - ε)时可确定性解决，通信复杂度为O(n log n)。ε-BA可作为完整BA协议的第一阶段，第二阶段只需一次全交换和多数投票即可完成BA。

Result: 证明达成一致性决策（univalency）不需要二次通信，二次成本完全来自结果传播阶段。在认证设置中定义Extractable BA，证明通信复杂度可降至O(f log f)。

Conclusion: Dolev-Reischuk下界的二次通信成本主要源于结果传播而非决策达成，这一发现对拜占庭容错协议设计有重要启示。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [177] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze：通过建模流水线调度为有向无环图并求解线性规划问题，计算最优参数冻结比例，在保证精度的前提下最大化训练吞吐量，相比现有方法减少过度冻结带来的精度损失。


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有参数冻结方法通过自适应跳过反向计算来提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题计算最优冻结比例，在精度约束下最小化批次执行时间。该方法能自适应确定哪些参数应该冻结以及何时冻结。

Result: 实验表明，TimelyFreeze在LLaMA-8B模型上实现了高达40%的训练吞吐量提升，同时保持可比的精度。该方法在不影响收敛的情况下加速大规模模型训练，并能泛化到不同的流水线并行设置。

Conclusion: TimelyFreeze通过优化参数冻结策略，有效解决了现有方法过度冻结的问题，在保证模型精度的同时显著提升训练吞吐量，为大规模模型训练提供了更高效的解决方案。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [178] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文提出了位置感知分散问题，这是经典分散问题的新泛化，要求机器人根据颜色匹配移动到同色节点，并开发了具有时间和内存保证的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 经典分散问题假设机器人可以占据任何空闲节点，但在实际应用中，机器人可能需要移动到特定类型的节点（如充电站、特定功能区域）。位置感知分散问题通过引入颜色标签，要求机器人移动到与其颜色匹配的节点，这更贴近现实世界的应用场景。

Method: 作者开发了多个确定性算法来解决位置感知分散问题。这些算法在匿名、连通、无向图上运行，每个节点和机器人都有颜色标签。算法设计旨在最小化时间和内存需求，并提供了相应的性能保证。同时，作者还给出了不可能性结果和任意确定性算法的下界。

Result: 研究结果表明位置感知分散问题在匿名网络中算法上是可行的，但相比经典分散问题的解决方案，获得高效解面临更大挑战。作者提出的算法在时间和内存方面都有保证的界限。

Conclusion: 位置感知分散问题是分散问题的有意义泛化，虽然算法上可行，但比经典分散问题更具挑战性。该工作为分布式机器人协调中的位置感知问题建立了理论基础，并指出了未来研究方向。

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [179] [Evaluating Kubernetes Performance for GenAI Inference: From Automatic Speech Recognition to LLM Summarization](https://arxiv.org/abs/2602.04900)
*Sai Sindhur Malleni,Raúl Sevilla,Aleksei Vasilevskii,José Castillo Lema,André Bauer*

Main category: cs.ET

TL;DR: Kubernetes生态系统通过Kueue、DAS和GAIE等原生项目为生成式AI工作负载提供统一平台，显著提升批处理和在线推理性能


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（特别是推理）成为主导工作负载，Kubernetes生态系统需要原生支持其独特需求，提供容器编排的可扩展性和资源效率优势

Method: 结合Kubernetes原生项目：1) 使用Kueue管理Whisper模型转录音频文件的批处理作业；2) 使用DAS增加并行作业执行；3) 使用llm-d和GAIE为LLM摘要提供在线推理优化路由

Result: Kueue减少总完成时间达15%；DAS缩短平均作业完成时间36%；GAIE改善首次令牌时间82%，形成高性能统一平台

Conclusion: Kueue、DAS和GAIE等互补组件构成统一高性能平台，证明Kubernetes能够作为生成式AI工作负载的统一基础架构

Abstract: As Generative AI (GenAI), particularly inference, rapidly emerges as a dominant workload category, the Kubernetes ecosystem is proactively evolving to natively support its unique demands. This industry paper demonstrates how emerging Kubernetes-native projects can be combined to deliver the benefits of container orchestration, such as scalability and resource efficiency, to complex AI workflows. We implement and evaluate an illustrative, multi-stage use case consisting of automatic speech recognition and summarization. First, we address batch inference by using Kueue to manage jobs that transcribe audio files with Whisper models and Dynamic Accelerator Slicer (DAS) to increase parallel job execution. Second, we address a discrete online inference scenario by feeding the transcripts to a Large Language Model for summarization hosted using llm-d, a novel solution utilizing the recent developments around the Kubernetes Gateway API Inference Extension (GAIE) for optimized routing of inference requests. Our findings illustrate that these complementary components (Kueue, DAS, and GAIE) form a cohesive, high-performance platform, proving Kubernetes' capability to serve as a unified foundation for demanding GenAI workloads: Kueue reduced total makespan by up to 15%; DAS shortened mean job completion time by 36%; and GAIE improved Time to First Token by 82\%.

</details>


### [180] [Task-Adaptive Physical Reservoir Computing via Tunable Molecular Communication Dynamics](https://arxiv.org/abs/2602.05931)
*Saad Yousuf,Kaan Burak Ikiz,Murat Kuscu*

Main category: cs.ET

TL;DR: 分子通信通道可作为可重构的物理储层计算系统，通过调节生物物理参数优化不同计算任务性能


<details>
  <summary>Details</summary>
Motivation: 大多数物理储层计算实现是静态的，性能局限于狭窄任务范围，需要开发可重构、任务自适应的物理计算系统

Method: 采用双模拟方法：确定性平均场模型和高保真粒子随机模型(Smoldyn)，结合贝叶斯优化在高维参数空间中导航，识别离散操作机制

Result: 发现明确权衡：富含通道记忆的参数集在混沌时间序列预测任务中表现优异，而促进强受体非线性的机制在非线性数据转换中更优；后处理方法可减轻分子噪声提升性能

Conclusion: 分子通信通道不仅是计算基底，更是可调谐生物启发计算系统的设计蓝图，为未来湿件AI实现提供了优化框架

Abstract: Physical Reservoir Computing (PRC) offers an efficient paradigm for processing temporal data, yet most physical implementations are static, limiting their performance to a narrow range of tasks. In this work, we demonstrate in silico that a canonical Molecular Communication (MC) channel can function as a highly versatile and task-adaptive PRC whose computational properties are reconfigurable. Using a dual-simulation approach -- a computationally efficient deterministic mean-field model and a high-fidelity particle-based stochastic model (Smoldyn) -- we show that tuning the channel's underlying biophysical parameters, such as ligand-receptor kinetics and diffusion dynamics, allows the reservoir to be optimized for distinct classes of computation. We employ Bayesian optimization to efficiently navigate this high-dimensional parameter space, identifying discrete operational regimes. Our results reveal a clear trade-off: parameter sets rich in channel memory excel at chaotic time-series forecasting tasks (e.g., Mackey Glass), while regimes that promote strong receptor nonlinearity are superior for nonlinear data transformation. We further demonstrate that post-processing methods improve the performance of the stochastic reservoir by mitigating intrinsic molecular noise. These findings establish the MC channel not merely as a computational substrate, but as a design blueprint for tunable, bioinspired computing systems, providing a clear optimization framework for future wetware AI implementations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [181] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 首次设计、集成并评估了RISC-V控制流完整性扩展Zicfiss和Zicfilp，通过硬件实现影子栈和着陆垫机制，在CVA6核心中集成，仅1.0%面积开销和最高15.6%性能开销。


<details>
  <summary>Details</summary>
Motivation: 保护易受攻击程序免受控制流劫持攻击，为RISC-V架构提供标准化的控制流完整性硬件扩展。

Method: 设计了两个独立可配置的硬件单元：Zicfiss（影子栈）用于后向边保护，Zicfilp（着陆垫）用于前向边保护，完全集成到开源CVA6核心中。

Result: 在22nm FDX技术中合成时仅增加1.0%面积开销，使用MiBench汽车基准测试子集评估显示最高15.6%性能开销，完整实现已开源发布。

Conclusion: 成功实现了首个RISC-V控制流完整性硬件扩展，以低开销提供有效的控制流保护，为RISC-V生态系统贡献了重要的安全增强。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [182] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE是首个针对HZO基FeFET eNVM的碳建模框架，涵盖硬件制造（体现碳）到使用（运行碳）的全生命周期分析。评估显示HZO-FeFET在单位面积体现碳比CMOS基线高11%，但每MB体现碳比SRAM低4.3倍。在边缘ML加速器案例中，用HZO-FeFET替代SRAM权重缓冲区可减少42.3%体现碳和高达70%运行碳。


<details>
  <summary>Details</summary>
Motivation: 信息和通信技术对全球环境影响日益增长，新兴非易失性存储器（eNVM）虽能提供节能计算方案，但其端到端碳足迹尚未被充分理解。理解硬件系统全生命周期的环境影响是实现可持续计算的第一步。

Method: 提出COFFEE框架，这是首个针对HZO基FeFET eNVM的碳建模框架。基于真实半导体工厂数据和设备制造配方估算体现碳，使用架构级eNVM设计空间探索工具量化使用阶段性能和能耗。

Result: 在2MB容量下，HZO-FeFET的单位面积体现碳比CMOS基线高11%，但每MB体现碳比SRAM低4.3倍。在边缘ML加速器案例研究中，用HZO-FeFET替代SRAM权重缓冲区可减少42.3%体现碳和高达70%运行碳。

Conclusion: HZO基FeFET eNVM在可持续计算方面具有显著潜力，虽然制造阶段碳足迹较高，但整体生命周期碳效益明显，特别是在替代SRAM的应用场景中能大幅降低碳足迹。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [183] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 提出一种灵活的FP8数字计算内存加速器，通过动态位宽预测和FIFO对齐单元，支持可变FP8格式，在28nm工艺下实现20.4 TFLOPS/W能效，比现有工作高2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有数字计算内存架构难以支持可变FP8对齐尾数位宽，统一的校准策略和固定精度MAC单元无法处理具有不同分布的输入数据。

Method: 提出三个创新：1) 动态移位感知位宽预测(DSBP)，自适应调整权重(2/4/6/8b)和输入(2~12b)对齐尾数精度；2) FIFO输入对齐单元(FIAU)替代复杂桶形移位器；3) 精度可扩展INT MAC阵列实现灵活权重精度。

Result: 在28nm CMOS中实现64×96 CIM阵列，达到20.4 TFLOPS/W能效，比先前工作高2.8倍，支持所有FP8格式。在Llama-7b上，DSBP在相同精度下比固定位宽模式效率更高。

Conclusion: 该灵活FP8 DCIM加速器通过创新的动态位宽预测和对齐机制，成功解决了可变FP8格式支持问题，实现了显著的能效提升和灵活的精度-效率权衡。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>
