<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Implementation and Analysis of Thermometer Encoding in DWN FPGA Accelerators](https://arxiv.org/abs/2512.15251)
*Michael Mecik,Martin Kumm*

Main category: cs.AR

TL;DR: DWN硬件生成器包含温度计编码，实验显示编码可增加3.20倍LUT使用，在小网络中占主导成本，强调DWN加速器需要编码感知的硬件设计。


<details>
  <summary>Details</summary>
Motivation: FPGA上的全并行神经网络加速器面临硬件资源限制，DWN通过梯度训练优化资源使用，但其依赖的温度计编码硬件成本尚未充分评估。

Method: 提出包含温度计编码的DWN硬件生成器，在Jet Substructure Classification任务上进行实验评估编码对硬件资源的影响。

Result: 温度计编码可使LUT使用增加高达3.20倍，在小网络中占据主导成本，编码成本成为DWN加速器的主要开销。

Conclusion: DWN加速器设计需要考虑编码成本，需要编码感知的硬件设计来优化资源使用。

Abstract: Fully parallel neural network accelerators on field-programmable gate arrays (FPGAs) offer high throughput for latency-critical applications but face hardware resource constraints. Weightless neural networks (WNNs) efficiently replace arithmetic with logic-based inference. Differential weightless neural networks (DWN) further optimize resource usage by learning connections between encoders and LUT layers via gradient-based training. However, DWNs rely on thermometer encoding, and the associated hardware cost has not been fully evaluated. We present a DWN hardware generator that includes thermometer encoding explicitly. Experiments on the Jet Substructure Classification (JSC) task show that encoding can increase LUT usage by up to 3.20$\times$, dominating costs in small networks and highlighting the need for encoding-aware hardware design in DWN accelerators.

</details>


### [2] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 本文提出FAME，一种针对同态加密矩阵乘法优化的FPGA加速器，通过创新的数据路径设计减少内存访问，实现221倍于CPU的加速。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）虽然能实现加密数据的安全计算，但其高昂的计算成本（特别是矩阵乘法）阻碍了实际部署。加速同态加密矩阵乘法对于隐私保护机器学习等应用至关重要。

Method: 首先开发成本模型评估片上内存需求，然后设计新颖的同态线性变换数据路径，实现细粒度数据复用，最后构建FAME FPGA加速器，支持任意矩阵形状和多种HE参数配置。

Result: 在Alveo U280 FPGA上实现的FAME，在不同矩阵大小和形状的测试中，平均比最先进的CPU实现快221倍，展示了大规模连续HE MM和实际工作负载的可扩展性和实用性。

Conclusion: FAME通过高效的FPGA实现显著加速了同态加密矩阵乘法，解决了HE计算中的关键性能瓶颈，为隐私保护计算的实际部署提供了可行的硬件解决方案。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts](https://arxiv.org/abs/2512.14706)
*Krunal Jesani,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: NN-Caption：基于大语言模型引导的神经架构搜索流水线，自动生成可运行的图像描述模型，在MS COCO数据集上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索需要大量人工专业知识或自动化试错，本文旨在利用大语言模型自动生成图像描述模型架构，降低NAS的门槛。

Method: 使用DeepSeek-R1-0528-Qwen3-8B作为主要生成器，通过提示模板从LEMUR分类主干中组合CNN编码器与序列解码器（LSTM/GRU/Transformer），在严格Net API约束下生成可运行架构。

Result: LLM生成了数十个描述模型，超过一半成功训练并产生有意义的描述；分析了不同数量输入模型片段（5 vs 10）对成功率的影响；报告了训练动态和最高BLEU-4分数。

Conclusion: LLM引导的NAS不仅能够提出架构，还能建议超参数和训练实践；识别了代码幻觉和API合规性等挑战，通过提示规则和迭代代码修复解决；为LEMUR数据集添加了数十个新模型，促进可复现基准测试和下游AutoML研究。

Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.

</details>


### [4] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: AutoS方法通过自主选择源域样本和模型，利用密度驱动策略筛选最相关的源信息，结合多模态预训练模型增强伪标签，提升多域自适应性能


<details>
  <summary>Details</summary>
Motivation: 在多域自适应中，多个源域常包含冗余或不相关信息，特别是在大规模源域设置下，这些信息会损害迁移性能。需要开发有效策略从大量源域中选择最具可迁移性的知识来解决目标任务

Method: 提出AutoS方法：1) 使用密度驱动选择策略在训练中选择源域样本，并确定哪些源模型应参与目标预测；2) 基于预训练多模态模型构建伪标签增强模块，减轻目标标签噪声并改进自监督

Result: 在真实世界数据集上的实验表明，所提方法具有优越性

Conclusion: AutoS方法能够自主选择最相关的源域信息和模型，有效提升多域自适应性能，特别是在大规模源域设置下

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [5] [SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI](https://arxiv.org/abs/2512.14712)
*Ryan Cartularo*

Main category: cs.LG

TL;DR: 比较两种多模态融合架构用于脓毒症预测，发现轻量级上下文感知混合专家模型优于复杂的端到端深度融合模型，达到SOTA性能并集成到开源临床决策框架中。


<details>
  <summary>Details</summary>
Motivation: 脓毒症占ICU入院近20%，但传统预测模型难以有效整合异质数据流，要么模态孤立，要么依赖脆弱的早期融合。需要更好的多模态融合方法来提高预测准确性。

Method: 1. 提出两种架构对比：端到端深度融合（SepsisFusionFormer）和上下文感知堆叠（SepsisLateFusion）；2. SepsisFusionFormer使用四模态分层门控注意力网络；3. SepsisLateFusion采用轻量级上下文感知混合专家架构，将模态视为正交专家（历史学家、监视器、阅读器），通过CatBoost元学习器动态门控。

Result: 1. SepsisFusionFormer在小抗生素队列中遭受"注意力饥饿"，AUC仅0.66；2. SepsisLateFusion达到SOTA性能：临床发作前4小时预测AUC 0.915；3. 通过校准决策阈值，将漏诊病例减少48%；4. 四模态集成在多类抗生素选择任务中达到最高性能（AUC 0.72）。

Conclusion: 轻量级上下文感知混合专家架构在脓毒症预测任务中优于复杂的端到端深度融合模型，实现了SOTA性能并集成为开源临床决策支持框架SepsisSuite，为及时干预提供了真正的预防窗口。

Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info

</details>


### [6] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: 提出潜在类别强化学习模型，用于捕捉旅行决策中的经验形成和个体异质性，识别出三种不同的偏好适应模式。


<details>
  <summary>Details</summary>
Motivation: 旅行决策涉及经验形成过程，个体随时间学习偏好，同时存在显著的个体异质性（包括基础偏好和演化方式）。现有模型难以同时捕捉这两种现象。

Method: 提出潜在类别强化学习模型，应用于驾驶模拟器数据集，通过变分贝叶斯方法估计参数。

Result: 识别出三类个体：第一类显示情境依赖偏好和情境特定的开发倾向；第二类遵循持续开发策略；第三类采用探索策略结合情境特定偏好。

Conclusion: LCRL模型能有效捕捉旅行决策中的经验形成和个体异质性，为理解不同旅行者行为模式提供了新框架。

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [7] [Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms](https://arxiv.org/abs/2512.14714)
*Lucas Cesar Ferreira Domingos,Russell Brinkworth,Paulo Eduardo Santos,Karl Sammut*

Main category: cs.LG

TL;DR: 提出GSE ResNeXt深度学习架构，结合可学习Gabor卷积层和ResNeXt骨干网络，通过注意力机制提升水下声学目标分类性能，在数据有限场景下优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 水下声学目标检测对环境和国防至关重要，但复杂的水下噪声和有限数据集限制了传统方法的准确性和泛化能力，需要更鲁棒的信号处理方法。

Method: 提出GSE ResNeXt架构：1) 集成可学习Gabor卷积层作为二维自适应带通滤波器；2) 基于ResNeXt骨干网络；3) 加入挤压-激励注意力机制；4) 在三个复杂度递增的分类任务上评估。

Result: 1) 在分类性能上持续优于Xception、ResNet、MobileNetV2等基线模型；2) 初始层加入Gabor卷积使训练时间减少28%；3) 发现船只与传感器距离显著影响性能；4) 注意力机制提升训练稳定性和收敛性。

Conclusion: 信号处理策略对提升模型可靠性和泛化能力至关重要，特别是在数据有限的水下声学分类场景。未来应关注减轻环境因素对输入信号的影响。

Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.

</details>


### [8] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: 首次研究LLM权重中的比特级扰动如何影响图像描述生成的语义，同时保持语法结构，提出可微分的比特级故障分析框架BLADE


<details>
  <summary>Details</summary>
Motivation: 先前故障分析方法主要关注分类器崩溃或精度下降，忽略了生成系统的语义和语言维度。在图像描述模型中，单个比特翻转可能微妙地改变视觉特征到词语的映射，从而改变AI对世界的叙述方式

Method: 设计可微分故障分析框架BLADE，使用基于梯度的敏感性估计定位语义关键比特，然后通过描述级语义-流畅性目标优化比特选择

Result: 语义漂移不是随机的，而是可微分估计的。模型自身的梯度可以预测哪些比特被扰动会最强烈地影响语义，同时保持语法和流畅性完整

Conclusion: 即使难以察觉的低级比特变化也能引导生成式视觉语言模型的高级语义，揭示了语义在比特级别的编码、分布和可改变性，为鲁棒性测试、对抗防御和可解释AI开辟了新途径

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [9] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: GPT-OSS-20B模型在金融NLP任务中表现与更大模型相当，但计算效率显著更高，挑战了模型规模与性能直接相关的假设。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融服务中的快速应用，需要严格的评估框架来评估其性能、效率和实际适用性，特别是在资源受限的生产环境中。

Method: 对GPT-OSS模型家族（120B和20B参数变体）及当代LLMs在10个不同金融NLP任务上进行全面评估，使用真实金融数据集（Financial PhraseBank、FiQA-SA、FLARE FINERORD），并引入新的效率指标来衡量性能与资源利用的权衡。

Result: GPT-OSS-20B模型在准确率上（65.1%）与GPT-OSS-120B（66.5%）相当，但计算效率显著更高（198.4 Token Efficiency Score，159.80 tokens/秒）。GPT-OSS模型在多项任务中超越包括Qwen3-235B在内的更大竞争对手。

Conclusion: GPT-OSS的架构创新和训练策略使较小模型能够以显著减少的计算开销实现竞争性性能，为金融应用中可持续且经济高效的LLM部署提供了途径，挑战了模型规模与任务性能直接相关的普遍假设。

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [10] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: SEED是一个基于谱熵引导的时空依赖建模框架，通过动态评估变量间的依赖关系，自适应平衡通道独立与通道依赖策略，解决了现有方法中时间自依赖被干扰、负相关被忽略、变量缺乏时间感知等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力或图的方法存在三个关键问题：1) 强时间自依赖常被无关变量干扰；2) softmax归一化忽略并反转负相关；3) 变量难以感知自身时间位置。这些问题限制了多元时间序列预测中复杂变量间依赖关系的准确建模。

Method: SEED框架包含四个核心组件：1) 依赖评估器：利用谱熵动态评估每个变量的时空依赖，自适应平衡通道独立与通道依赖策略；2) 谱熵融合器：进一步细化依赖权重，分离由其他变量影响产生的时间规律；3) 符号图构造器：支持带符号边权重，保留负相关；4) 上下文空间提取器：利用局部上下文窗口提取空间特征，帮助变量感知时间位置。

Result: 在12个来自不同应用领域的真实世界数据集上进行广泛实验，SEED实现了最先进的性能，验证了其有效性和通用性。

Conclusion: SEED通过谱熵引导的依赖评估框架，有效解决了多元时间序列预测中的时空依赖建模问题，在多个数据集上表现出优越性能，为复杂变量间依赖关系的建模提供了新思路。

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [11] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 论文提出CAP框架，通过类感知归因先验引导小语言模型学习细粒度类别区分特征，提升分类任务的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法在分类任务中虽然能可靠地突出类别相关标记，但往往聚焦于语义相似类别共享的通用关键词，这些类别本身在标准训练中就难以区分，导致归因先验缺乏区分性线索，限制了提升模型区分能力的效果。

Method: 提出类感知归因先验（CAP）框架，引导语言模型捕捉细粒度类别差异并产生更显著、更具区分性的归因先验。进一步提出CAP Hybrid，将CAP的先验与现有归因技术结合，形成更全面平衡的监督信号。通过将模型的自归因与这些增强的先验对齐，鼓励学习多样化的决策相关特征。

Result: 在完整数据、少样本和对抗场景下的广泛实验表明，该方法能持续提升模型的解释性和鲁棒性。

Conclusion: CAP框架通过提取更具区分性的类感知归因先验，有效解决了现有归因方法在分类任务中区分性不足的问题，为小语言模型在需要解释性和鲁棒性的应用场景中提供了有效的训练框架。

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [12] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: 提出自动从癌症报告表格数据生成Synthea规则的方法，并以胶质母细胞瘤为例验证，生成的数据能重现疾病进程并保留统计特性。


<details>
  <summary>Details</summary>
Motivation: 合成数据是隐私合规使用医疗数据的有效方法，但Synthea等规则生成器需要专家知识和样本数据，规则创建过程复杂。

Method: 基于癌症报告表格数据的统计信息自动生成Synthea规则，并以胶质母细胞瘤真实数据集为例创建Synthea模块。

Result: 合成数据能重现已知疾病进程，大部分统计特性得以保留，验证了方法的有效性。

Conclusion: 合成患者数据在隐私保护研究中潜力巨大，可用于假设提出和原型开发，但医学解释需考虑现有方法的局限性。

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [13] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: 使用分层注意力变换器（HATs）改进Groebner基计算，相比传统平面注意力模型实现显著计算节省，能处理更大规模实例


<details>
  <summary>Details</summary>
Motivation: 改进Kera等人（NeurIPS 2024）使用变换器计算Groebner基的方法，通过引入树结构归纳偏置更好地建模数据中的层次关系，实现计算效率提升

Method: 应用分层注意力变换器（HATs）架构，该架构包含树结构归纳偏置，能建模数据中的层次关系；推广到任意深度，并进行详细计算成本分析；结合课程学习

Result: 相比传统平面注意力模型实现显著计算节省，能解决比Kera等人（2024）工作中大得多的实例

Conclusion: 分层注意力变换器为计算Groebner基提供了一种高效方法，通过树结构归纳偏置和课程学习显著提升了处理大规模多项式方程组的能力

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [14] [Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion](https://arxiv.org/abs/2512.14725)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Petros Koumoutsakos,Soledad Le Clainche*

Main category: cs.LG

TL;DR: 提出基于生成扩散模型的框架，用于在非结构化网格上合成稳态城市风场，仅需几何信息即可生成准确多样的速度场


<details>
  <summary>Details</summary>
Motivation: 城市风流建模对空气质量评估和可持续城市规划很重要，但现有方法存在局限：低阶模型难以捕捉几何效应，而高保真CFD模拟成本过高，特别是在处理多个几何结构或风况时

Method: 结合分层图神经网络和基于分数的扩散建模的生成扩散框架，仅需几何信息即可生成稳态城市风场，无需时间展开或密集测量

Result: 模型能够泛化到未见过的几何结构，恢复关键流动结构（如尾流和再循环区），提供不确定性感知预测，消融研究证实了对网格变化的鲁棒性和不同推理机制下的性能

Conclusion: 这是构建环境基础模型的第一步，可帮助城市规划者在城市密集化和气候不确定性下快速评估设计决策

Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.

</details>


### [15] [Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning](https://arxiv.org/abs/2512.14726)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: QDT通过量子启发的注意力机制和前馈网络，在离线强化学习中实现了比标准DT超过2000%的性能提升，解决了长时程信用分配和复杂状态-动作依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有决策变换器在离线强化学习中难以处理长时程信用分配和复杂状态-动作依赖关系，需要新的架构设计来提升性能。

Method: 提出量子决策变换器，包含两个核心组件：量子启发注意力（具有纠缠操作捕获非局部特征相关性）和量子前馈网络（具有多路径处理和可学习干扰的自适应计算）。

Result: 在连续控制任务上实现了超过2000%的性能提升，表现出优越的泛化能力；消融研究表明量子启发组件之间存在强协同效应。

Conclusion: 量子启发架构设计需要整体协同设计而非模块化组件采用，为序列决策中的变换器架构提供了有前景的新方向。

Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.

</details>


### [16] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: 论文质疑了共形预测在小校准集上的实用性，尽管其统计保证在任意校准集大小下都成立，但实际效用高度依赖校准集大小，这在医疗领域尤其重要。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医疗领域应用广泛，但临床决策需要可靠的置信度估计。共形预测虽然能提供统计保证，但通常认为即使在小校准集上也能获得有意义的保证，这在医疗数据稀缺的背景下被过度承诺。

Method: 通过理论分析和实证验证，展示了共形预测的统计保证虽然形式上对任意校准集大小都成立，但实际效用高度依赖校准集规模。在医疗图像分类任务上进行了实证演示。

Result: 研究发现，尽管共形预测的统计保证在小校准集上仍然成立，但这些保证的实际意义和实用性会显著下降。在医疗图像分类任务中，小校准集产生的预测集可能过于保守或不实用。

Conclusion: 共形预测在小校准集上的承诺需要重新审视。虽然统计保证在形式上成立，但实际临床应用中，校准集大小对不确定性估计的实用性至关重要，医疗领域需要更谨慎地应用该方法。

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [17] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: 提出基于AFC和AVL数据的城市轨道交通精细化轨迹推断方法，采用KLEM算法实现数据驱动的参数估计，无需外部调查数据，在高峰时段轨迹推断准确率超过90%


<details>
  <summary>Details</summary>
Motivation: 城市轨道交通精细化轨迹推断对运营组织具有重要意义。现有方法依赖外部调查数据进行参数拟合，且验证多使用合成数据，限制了模型的鲁棒性和适用性。

Method: 1) 基于时空约束建立列车备选集；2) 提出基于KL散度结合EM算法的KLEM方法进行数据驱动参数估计；3) 数据驱动的自适应轨迹推断；4) 旅行轨迹构建；5) 使用真实个体出行轨迹数据进行验证。

Result: 该方法能够实现高精度的乘客轨迹推断，在高峰时段城市轨道交通出行轨迹推断中准确率超过90%。

Conclusion: 提出的全数据驱动方法消除了对外部调查数据的依赖，增强了模型的鲁棒性和适用性，能够有效实现城市轨道交通个体出行轨迹的高精度推断。

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [18] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出一种几何框架用于策略约束的语义解释，可证明防止高风险领域中的幻觉承诺，在金融监管数据上实现零幻觉批准


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如金融监管）中，语义解释系统容易产生幻觉承诺（错误批准），需要一种可证明防止此类错误的框架

Method: 将语义表示为单位球面上的方向，证据建模为见证向量集，可接受解释对应球形凸区域，策略约束作为显式先验引入，解释简化为约束优化问题

Result: 在大型受监管金融数据上的实证验证显示，在多种政策制度下实现了零幻觉批准，这是首次在大规模应用中达到此结果

Conclusion: 该几何框架为策略约束的语义解释提供了理论基础，可证明防止幻觉承诺，复杂度界限在信息论上是最优的，适用于高风险领域

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [19] [INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT](https://arxiv.org/abs/2512.14732)
*Idan Tankel,Nir Mazor,Rafi Brada,Christina LeBedis,Guy ben-Yosef*

Main category: cs.LG

TL;DR: 提出基于LLM和VLM的规划-执行代理框架，用于自动化CT扫描中偶然发现的检测、分类和报告，在腹部CT基准测试中优于纯VLM方法。


<details>
  <summary>Details</summary>
Motivation: CT扫描中的偶然发现虽然通常良性，但具有重要临床意义，需要遵循指南报告。传统放射科医生手动检查耗时且结果不一致，需要提高效率和精度。

Method: 采用规划-执行代理框架：基于LLM的规划器根据医学指南生成Python脚本，执行器运行脚本，通过VLM、分割模型和图像处理子程序进行检测和分类。

Result: 在腹部CT基准测试中对三个器官进行实验，结果显示该框架在准确性和效率方面优于现有的纯VLM方法。

Conclusion: 提出的LLM+VLM规划-执行框架能够有效自动化偶然发现管理过程，提高CT扫描分析的效率和精度。

Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.

</details>


### [20] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: 提出了一种轻量级、模型无关的日内个性化方法，通过推理时选择性注入近期观看历史来更新用户特征，无需模型重训练


<details>
  <summary>Details</summary>
Motivation: 传统长视频流媒体推荐系统采用批量训练模型和批量更新特征，用户特征每日更新并在全天静态服务，无法捕捉用户最新行为，导致推荐过时

Method: 在推理时选择性覆盖过时的用户特征，使用近期观看历史进行即时更新，无需模型重训练，实现从每日到日内的个性化反馈循环缩短

Result: 观察到关键用户参与度指标统计显著提升0.47%，这是近期实验周期中观察到的最大参与度提升之一

Conclusion: 这是首个证明日内个性化能在长视频流媒体服务中产生有意义影响的公开证据，为需要模型重训练的完全实时架构提供了有吸引力的替代方案

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [21] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: 开发了一个评估AI论文概念新颖性的模型，通过标题、摘要和与先前文献的语义相似性来量化研究原创性，帮助识别真正创新的工作。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版门槛降低，AI领域论文数量激增，真正新颖且有影响力的工作难以脱颖而出。手动评估新颖性不稳定且耗时，需要数据驱动的可扩展评估方法。

Method: 采用两种任务形式：1) 二元分类（预测论文绝对新颖性），2) 成对新颖性比较（学习相对新颖性）。使用Qwen3-4B-Instruct-2507和SciBERT进行微调，并与GPT-5.1基准比较。

Result: 开发了公开可用的实现（GitHub: ZhengxuYan/NoveltyRank），通过不同任务形式和建模选择分析性能影响，为研究者提供定量一致的新颖性信号。

Conclusion: 该模型能够数据驱动地评估AI论文概念新颖性，帮助研究人员高效识别真正创新的提交，为会议评审提供定量新颖性信号，促进学术创新识别。

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [22] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: 提出离散扩散引导方法解决约束满足问题，以无监督方式求解数独谜题


<details>
  <summary>Details</summary>
Motivation: 约束满足问题（CSPs）如数独需要满足特定约束条件，传统方法通常需要监督学习或手工规则。本文旨在开发一种无需监督的方法来解决这类问题。

Method: 提出离散扩散引导（discrete diffusion guidance）方法，通过扩散模型在离散空间中进行引导，使生成结果满足约束条件，特别针对数独这类约束满足问题。

Result: 该方法能够成功求解数独谜题，无需任何监督训练，展示了离散扩散引导在约束满足问题上的有效性。

Conclusion: 离散扩散引导为约束满足问题提供了一种新颖的无监督解决方案，在数独问题上表现出色，有望扩展到其他类型的约束满足问题。

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [23] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: 决策校准框架从决策者角度评估天气预报价值，发现模型在预报层面的表现不能可靠转化为下游决策表现，需要针对具体决策任务进行专门评估。


<details>
  <summary>Details</summary>
Motivation: 传统天气预报评估主要关注预报者视角和统计指标，但实际应用中预报用于决策制定，因此需要从决策者角度量化预报在改善决策方面的价值。

Method: 提出决策校准框架，在决策层面而非预报层面评估预报性能，比较机器学习与传统数值天气预报模型在各种天气依赖决策任务中的表现。

Result: 模型在预报层面的表现不能可靠转化为下游决策表现：有些性能差异只在决策层面显现，不同决策任务中模型排名会发生变化。

Conclusion: 典型的预报评估不足以为特定决策任务选择最优预报模型，需要采用决策层面的评估方法。

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [24] [Unreliable Uncertainty Estimates with Monte Carlo Dropout](https://arxiv.org/abs/2512.14851)
*Aslak Djupskås,Alexander Johannes Stasik,Signe Riemer-Sørensen*

Main category: cs.LG

TL;DR: MCD作为贝叶斯推理的高效近似，在捕捉真实不确定性方面不如传统贝叶斯方法可靠


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，可靠的不确定性估计至关重要。虽然精确贝叶斯推理提供了原则性方法，但对深度神经网络计算不可行，需要高效的近似方法

Method: 通过实验研究蒙特卡洛dropout（MCD）捕捉真实不确定性的能力，并与高斯过程（GP）和贝叶斯神经网络（BNN）进行比较

Result: MCD难以准确反映底层真实不确定性，特别是在外推和内插区域无法捕捉到不确定性增加，不如传统贝叶斯方法可靠

Conclusion: MCD作为贝叶斯推理的近似方法，在捕捉认知和偶然不确定性方面不如传统贝叶斯方法可靠，特别是在外推和内插区域表现不佳

Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.

</details>


### [25] [How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal](https://arxiv.org/abs/2512.14873)
*Sam Jeong,Hae Yong Kim*

Main category: cs.LG

TL;DR: FAN的改进主要来自正弦函数在x=0附近的非零导数缓解梯度消失，而非其周期性。研究发现只有正弦函数有益，余弦函数反而有害。基于此提出了更高效的DAL加速收敛。


<details>
  <summary>Details</summary>
Motivation: 虽然FAN（傅里叶分析网络）通过用正弦和余弦函数替换部分ReLU激活函数能带来性能提升，但其背后的机制尚不清楚。研究者希望理解FAN为何有效，并基于此开发更高效的收敛加速方法。

Method: 通过分析FAN中正弦和余弦函数各自的作用，发现只有正弦函数对性能有正面贡献。进一步分析表明，正弦函数在x=0附近的非零导数缓解了梯度消失问题，特别是解决了"dying-ReLU"问题。基于这一理解，提出了Dual-Activation Layer (DAL)作为更高效的收敛加速器。

Result: 在三个任务上评估DAL：噪声正弦信号与纯噪声的分类、MNIST数字分类、以及基于ECG的生物特征识别。在所有情况下，DAL模型都比使用传统激活函数的模型收敛更快，并达到相等或更高的验证准确率。

Conclusion: FAN的益处主要源于正弦函数在x=0附近的局部行为而非其周期性特性，这缓解了梯度消失问题。基于这一理解开发的DAL能更有效地加速收敛，为神经网络优化提供了新的视角。

Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.

</details>


### [26] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: 提出ERBP框架，用信息几何方法统一解释自指学习中的模型崩溃现象，并通过引入"熵库"提供稳定解决方案。


<details>
  <summary>Details</summary>
Motivation: 自指学习（模型使用自身生成的数据进行训练）虽然具有无限扩展潜力，但普遍存在模型崩溃问题。尽管实践中已有各种临时修复方法，但缺乏统一的理论框架来解释失败模式和修复机制。

Method: 提出熵库Bregman投影（ERBP）框架，将自指学习建模为分布空间中的随机Bregman投影序列。通过引入"熵库"（高熵分布）来注入可控的熵流，稳定动力学过程。

Result: 理论分析得出：(1) 模型崩溃的必要条件；(2) 保证非平凡熵底限的充分条件；(3) 仅依赖于样本量和Bregman生成器常数的闭式收敛速率。在语言模型、强化学习和GAN中的实验验证了理论预测。

Conclusion: ERBP将各种经验性修复方法统一为单一量化设计原则：监控和预算熵流。为自指学习提供了理论指导和稳定化设计规则。

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [27] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: 论文提出"任务矩阵"概念，证明预训练与微调模型之间存在跨层线性编码，通过线性变换提升基础模型性能接近微调水平


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大型视觉和语言模型在上下文提示偏置下学习隐式线性编码，但更一般适应机制中是否存在类似线性表示尚未得到验证

Method: 提出任务矩阵概念——从基础到微调嵌入状态的线性变换，在视觉和文本模型及十个数据集上进行验证，使用数据驱动的近似方法

Result: 基础模型增强任务矩阵后性能超越线性探针，有时接近微调水平，验证了预训练与微调架构间跨层线性编码的存在

Conclusion: 任务矩阵方法高效且可泛化到多个领域，证明了模型适应过程中存在可学习的线性表示结构

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [28] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: 提出OLR-WA在线回归模型，通过加权平均处理数据漂移，在收敛速度和性能上优于现有在线模型，特别擅长处理置信度场景。


<details>
  <summary>Details</summary>
Motivation: 在线学习需要增量更新模型以避免大规模存储和重新计算成本，同时需要处理数据漂移（随时间变化的底层模式）等挑战场景。

Method: 提出OLR-WA（在线回归加权平均）模型，采用保守更新策略，优先考虑置信度较高的旧数据点，能够处理时间漂移和置信度挑战场景。

Result: OLR-WA性能与批处理回归相当，优于其他在线模型；收敛速度快，即使仅用1%-10%数据初始化也能从第一次迭代就获得高r2值；是唯一能有效处理置信度场景的模型。

Conclusion: OLR-WA在不同场景下展现出多功能性和实用性，是在线线性回归任务的有价值解决方案。

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [29] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: 提出OEC方法解决多轮LLM智能体训练中的协变量偏移问题，通过在学生模型轨迹中切换专家模型生成部分在线数据，在软件工程任务中相比传统模仿学习提升14%和13%


<details>
  <summary>Details</summary>
Motivation: 传统基于专家轨迹的模仿学习方法在多轮LLM智能体训练中存在协变量偏移问题：学生策略行为偏离专家时，会遇到训练数据中不存在的状态，降低微调效果

Method: 提出在线专家修正(OEC)数据生成方法：从学生模型开始生成轨迹，中途切换到专家模型继续生成，结合拒绝采样和监督微调技术进行训练

Result: 在软件工程任务(SWE-bench)上，OEC方法相比传统模仿学习在7b和32b模型上分别相对提升14%和13%

Conclusion: 多轮LLM智能体训练需要结合专家演示和在线数据，OEC方法能有效解决协变量偏移问题，提升模型性能

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [30] [ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs](https://arxiv.org/abs/2512.14908)
*Turja Kundu,Sanjukta Bhowmick*

Main category: cs.LG

TL;DR: ATLAS是一种新型图学习算法，通过提取多级社区拓扑信息并拼接特征向量，使用MLP而非图神经网络聚合，解决异质图精度下降和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络的两个关键挑战：1) 异质图（heterophilic graphs）中GNN精度下降；2) 迭代特征聚合限制GNN在大图上的可扩展性。

Method: 提取多级细化的图社区拓扑信息，将社区分配结果与特征向量拼接，然后对结果表示应用多层感知机（MLP），无需进行聚合操作。

Result: 在多种图上，ATLAS达到与基线方法相当的精度，在具有负结构偏置的异质图上比GCN提高20个百分点，在同质图上比MLP提高11个百分点。多分辨率社区特征可系统性地调节性能。

Conclusion: ATLAS通过拓扑社区特征提供节点及其邻域的上下文信息，避免了GNN的聚合限制，具有更好的可扩展性，并为可解释图学习开辟了原理性路径。

Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.

</details>


### [31] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: 提出一种基于Kronecker积表示的低秩MMSE滤波器正则化参数高效选择方法，该方法与秩选择问题相关，在低秩设置中至关重要


<details>
  <summary>Details</summary>
Motivation: 低秩MMSE滤波器中正则化参数的选择对性能有重要影响，但现有方法效率不高，需要一种更有效的参数选择方法

Method: 基于Kronecker积表示的正则化参数选择方法，将参数选择与秩选择问题联系起来，提出高效的计算方法

Result: 通过仿真验证，该方法相比常用方法取得了显著性能提升

Conclusion: 提出的方法能够高效选择低秩MMSE滤波器的正则化参数，该参数与秩选择问题密切相关，对低秩设置至关重要

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [32] [Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise](https://arxiv.org/abs/2512.14967)
*Felipe J. P. Antunes,Yuri F. Saporito,Sebastian Jaimungal*

Main category: cs.LG

TL;DR: 提出结合可激发性、深度学习和Picard迭代的数值方法，用于求解带共同噪声的McKean-Vlasov前向-后向随机微分方程，避免昂贵的嵌套蒙特卡洛模拟。


<details>
  <summary>Details</summary>
Motivation: 传统求解MV-FBSDEs的方法需要计算条件期望，通常依赖计算成本高昂的嵌套蒙特卡洛模拟。本文旨在开发一种更高效的数值方法，避免这种计算负担，同时能够处理复杂的均值场交互。

Method: 结合Picard迭代、可激发性理论和深度学习。使用可激发性推导路径损失函数，用循环神经网络参数化均值场交互项，用前馈网络近似解耦场的后向过程。通过最小化可激发得分来训练神经网络。

Result: 在系统性风险银行借贷模型上验证了算法，准确恢复了已知解析解。进一步扩展到分位数介导的交互模型，展示了可激发性框架的灵活性。最后应用于非平稳Aiyagari-Bewley-Huggett经济增长模型，展示了在复杂均值场博弈中的应用能力。

Conclusion: 提出的方法为求解带共同噪声的MV-FBSDEs提供了一种高效、灵活的数值框架，避免了昂贵的嵌套蒙特卡洛模拟，能够处理超越条件均值或矩的复杂交互，适用于没有闭式解的复杂均值场博弈问题。

Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.

</details>


### [33] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种软约束去噪器方法，将约束知识集成到去噪器本身，而非损失函数或采样循环，以在保持数据分布真实性的同时提高约束遵从性


<details>
  <summary>Details</summary>
Motivation: 扩散模型在科学应用中难以生成满足约束的样本。现有方法通过在损失中添加正则项或采样时使用引导方法，但这些方法会使生成模型偏离真实数据分布，特别是在约束定义错误时问题更严重

Method: 将引导启发的调整集成到去噪器本身，赋予其软归纳偏置，使其倾向于生成符合约束的样本，而不是改变损失函数或采样循环

Result: 软约束去噪器利用约束知识提高了对约束的遵从性，同时在约束与观测数据存在错误定义时保持足够的灵活性来偏离约束

Conclusion: 通过将约束知识集成到去噪器本身而非外部机制，可以在保持数据分布真实性的同时有效处理约束遵从问题，特别是在约束定义可能错误的科学应用场景中

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [34] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: 重复输入提示能提升主流模型性能，无需额外生成token或增加延迟


<details>
  <summary>Details</summary>
Motivation: 探索在不使用推理的情况下，如何通过简单方法提升大型语言模型的性能表现

Method: 通过重复输入提示的方式，测试对Gemini、GPT、Claude和Deepseek等流行模型的影响

Result: 重复输入提示能显著提高模型性能，且不会增加生成token数量或延迟

Conclusion: 简单的提示重复是一种有效的性能提升策略，适用于多种主流模型

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [35] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: 提出一种针对无界连续状态空间扩散过程的自适应分区强化学习算法，通过平衡探索与近似实现高效学习，并在高维金融问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 金融、经济和运筹学中常出现具有无界连续状态空间、有界连续动作和多项式增长奖励的扩散过程控制问题，现有方法难以处理这种连续高维域。

Method: 提出基于模型的自适应分区算法，在联合状态-动作空间中进行自适应分区，在每个分区内估计漂移、波动率和奖励，当估计偏差超过统计置信度时细化离散化。

Result: 建立了后悔界，依赖于问题视界、状态维度、奖励增长阶数和新定义的针对无界扩散过程的缩放维度，并在多资产均值-方差投资组合选择等高维问题上验证了有效性。

Conclusion: 该自适应方案平衡了探索与近似，能够高效学习无界域中的扩散过程控制问题，将理论保证扩展到更广泛的扩散类型问题类别。

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [36] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-Code提出了一种针对代码任务的流程奖励模型，通过将函数视为推理步骤并使用元学习校正机制，在代码生成任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的流程奖励模型在代码任务上效果有限，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛生成的中间标签存在噪声问题。

Method: 1. 使用Chain-of-Function提示策略将函数视为推理步骤，实现模块化代码生成；2. 引入基于元学习的校正机制，利用干净的最终解决方案单元测试标签，通过双层优化精炼中间标签。

Result: 在LiveCodeBench上实现了80.9%的pass@1率，超越了OpenAI o4-mini，达到了最先进的性能水平。

Conclusion: DreamPRM-Code通过创新的步骤分解方法和标签噪声校正机制，成功将流程奖励模型应用于代码生成任务，显著提升了大型语言模型在编程任务上的表现。

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [37] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: SPA是一个确定性框架，用于从股价数据中提取单调价格走势，关联公开事件，并生成可审计的解释性叙述，而非预测系统。


<details>
  <summary>Details</summary>
Motivation: 现有技术指标和预测模型缺乏透明度和可解释性，在需要审计的场景下存在挑战。需要一种能够清晰识别价格结构行为并提供可解释性分析的工具。

Method: SPA使用确定性框架处理每日OHLCV数据和标准化事件流，提取单调价格走势，通过对称相关窗口关联公开事件，并生成受约束的事实性历史解释。

Result: 在AAPL、NVDA、SCHW、PGR四只股票上的评估显示，SPA能稳定产生结构性分解和上下文叙述。消融实验证明确定性分割、事件对齐和约束解释对可解释性都有贡献。

Conclusion: SPA不是预测或交易系统，其价值在于提供透明、可复现的历史价格结构视图，可补充分析师工作流程、风险审查和可解释AI管道。

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [38] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: 研究AI生态系统多样性如何缓解知识崩溃，发现适度的认知多样性可减缓崩溃，但过多或过少都会损害性能


<details>
  <summary>Details</summary>
Motivation: AI的广泛使用引发了知识崩溃的担忧（即知识向最主流、核心的思想集缩减）。先前研究展示了单一模型崩溃现象，受生态学启发，研究者想探究AI生态系统多样性（模型间的多样性）是否能缓解这种崩溃

Method: 基于单一模型方法，但聚焦于在集体输出上训练的模型生态系统。通过将训练数据在不同语言模型间分割，评估由此产生的生态系统在十次自我训练迭代中的表现，研究多样性对模型性能的影响

Result: 增加认知多样性确实能缓解崩溃，但只到最优水平。只有少数多样化模型的生态系统无法表达完整真实分布的丰富混合，导致性能快速衰减；而将数据分布到过多模型则会降低每个模型对真实分布的近似能力，导致首次迭代就表现不佳

Conclusion: 在AI单一文化背景下，需要监控AI系统间的多样性，并制定政策激励更多领域和社区特定模型的发展，以维持健康的AI生态系统

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [39] [Spectral Representation-based Reinforcement Learning](https://arxiv.org/abs/2512.15036)
*Chenxiao Gao,Haotian Sun,Na Li,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 论文提出谱表示框架作为强化学习中函数逼近问题的解决方案，通过谱分解转移算子来抽象系统动态，为策略优化提供理论基础，并在DeepMind Control Suite的20多个任务上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 在具有大状态和动作空间的现实应用中，强化学习通常使用函数逼近来表示策略、价值函数和动态模型等核心组件。虽然神经网络等强大逼近器具有高表达能力，但存在理论模糊性、优化不稳定、探索困难以及计算成本高等问题。

Method: 引入谱表示视角，基于转移算子的谱分解构建框架。针对具有隐变量结构或基于能量结构的转移算子，提出了不同的谱表示学习方法。每种学习方法都在该框架下实现了有效的RL算法，并将该谱视角扩展到部分可观察MDPs。

Result: 在DeepMind Control Suite的20多个挑战性任务上验证了算法，性能达到或超过了当前最先进的model-free和model-based基线方法。

Conclusion: 谱表示框架为解决强化学习中函数逼近的困难提供了有效的解决方案，既提供了清晰的理论特征，又实现了实用的算法，在复杂任务上表现出色。

Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.

</details>


### [40] [EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks](https://arxiv.org/abs/2512.15067)
*Zijiang Yan,Yixiang Huang,Jianhua Pei,Hina Tabassum,Luca Chiaraviglio*

Main category: cs.LG

TL;DR: EMFusion是一个基于条件扩散模型的多变量概率预测框架，用于电磁场水平的多频段预测，结合上下文信息并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 无线基础设施快速增长需要准确预测电磁场水平以确保合规性、评估健康影响和支持网络规划。现有研究主要依赖宽带聚合数据的单变量预测，无法捕捉运营商间和频率间的变化，而这对主动网络规划至关重要。

Method: 提出EMFusion框架：基于条件扩散模型的概率预测方法，使用残差U-Net骨干网络，通过交叉注意力机制动态整合外部条件（时间、季节、节假日等）。采用基于插值的采样策略，将预测视为结构化修复任务，确保时间一致性。

Result: 在频率选择性EMF数据集上的实验表明，EMFusion在工作时间上下文信息下优于基线模型：CRPS提升23.85%，归一化均方根误差提升13.93%，预测CRPS误差降低22.47%。

Conclusion: EMFusion能够有效进行多变量频率选择性EMF预测，提供校准的概率预测区间和明确的不确定性量化，为可信的决策制定提供支持，优于传统点预测方法。

Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.

</details>


### [41] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: 论文发现基于嵌入的幻觉检测方法在真实基准测试中表现不佳，而GPT-4作为LLM法官能有效解决此问题，揭示了"语义幻觉"现象。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）系统基于检索证据，但仍容易产生幻觉。当前基于语义相似度和自然语言推理的检测方法存在根本性局限，需要系统性地量化其检测能力。

Method: 应用符合预测方法进行幻觉检测，提供有限样本覆盖保证。在约600个例子的校准集上测试，并在三个真实幻觉基准（HaluEval、RAGTruth、WikiBio）上评估多种方法，包括基于嵌入的方法和GPT-4作为LLM法官。

Result: 基于嵌入的方法在真实基准上表现不佳：HaluEval上100%误报率，RAGTruth上88%，WikiBio上50%。而GPT-4作为LLM法官仅7%误报率，证明通过推理可以解决此任务。这揭示了"语义幻觉"现象。

Conclusion: 基于嵌入的幻觉检测方法在真实场景中不足，因为语义上合理的幻觉保持了与源文档的相似性但引入了嵌入无法检测的事实错误。这种局限性跨越不同架构和任务，表明基于嵌入的检测不适合生产级RAG部署。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [42] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: FEAML：首个基于LLM的多标签学习自动特征工程方法，通过代码生成和反馈机制优化特征，解决标签依赖建模问题


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的特征工程方法尚未应用于多标签学习任务，缺乏对复杂标签依赖关系的建模能力，且未针对多标签任务特点进行专门适配

Method: 利用LLM的代码生成能力，通过元数据和标签共现矩阵引导LLM理解数据特征与任务目标的关系，生成高质量特征；使用模型准确率评估特征有效性，皮尔逊相关系数检测冗余；将评估结果作为反馈驱动LLM在后续迭代中持续优化代码生成

Result: 在多个多标签数据集上的实证结果表明，FEAML优于其他特征工程方法

Conclusion: 通过将LLM与反馈机制结合，FEAML实现了一个高效、可解释且自我改进的特征工程范式

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [43] [Neural Modular Physics for Elastic Simulation](https://arxiv.org/abs/2512.15083)
*Yifei Li,Haixu Wu,Zeyi Xu,Tuur Stuyck,Wojciech Matusik*

Main category: cs.LG

TL;DR: 提出Neural Modular Physics (NMP)方法，将弹性模拟分解为物理意义的神经模块，结合神经网络近似能力和传统模拟器的物理可靠性，提升物理一致性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的物理模拟方法通常使用端到端的单体神经网络，虽然有效但可能失去物理可解释性和可靠性等关键特征。受传统模拟器模块化操作的启发，希望结合神经网络近似能力和传统模拟器的物理可靠性。

Method: 提出Neural Modular Physics (NMP)方法，将弹性动力学分解为具有物理意义的神经模块，通过中间物理量连接。采用专门的架构和训练策略，将数值计算流程转化为模块化神经模拟器，实现对中间量的直接监督和物理约束。

Result: NMP在实验中表现出对未见初始条件和分辨率的优越泛化能力，稳定的长时程模拟，相比其他神经模拟器更好地保持物理特性，在未知基础动力学场景中比传统模拟器更具可行性。

Conclusion: NMP通过模块化设计成功结合了神经网络的近似能力和传统模拟器的物理可靠性，在弹性模拟中实现了更好的物理一致性和泛化性能，为物理模拟提供了新的范式。

Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.

</details>


### [44] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: PIP² Net：基于分区惩罚的物理信息深度算子网络，通过改进分区正则化技术提升算子学习的稳定性和表达能力，在非线性PDE预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法（如DeepONet、FNO）需要大量训练数据、缺乏显式物理结构、存在主干网络特征不稳定问题（模式不平衡或崩溃），影响算子近似精度。受经典分区统一方法的稳定性和局部性启发，研究基于分区统一的正则化技术。

Method: 改进现有的POU-PI-DeepONet框架，提出PIP² Net（物理信息分区惩罚深度算子网络），引入简化且更原则化的分区惩罚，改进协调的主干网络输出，在不牺牲DeepONet灵活性的同时提升表达能力。

Result: 在三个非线性PDE（粘性Burgers方程、Allen-Cahn方程、扩散-反应系统）上评估，PIP² Net在预测精度和鲁棒性方面一致优于DeepONet、PI-DeepONet和POU-DeepONet。

Conclusion: 基于分区统一的正则化技术能有效提升算子学习的稳定性和表达能力，PIP² Net框架为参数化PDE的快速求解提供了更准确和鲁棒的算子学习方法。

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


### [45] [SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs](https://arxiv.org/abs/2512.15088)
*Xianglin Wu,Chiheb Ben Hammouda,Cornelis W. Oosterlee*

Main category: cs.LG

TL;DR: SigMA：结合路径签名与多头自注意力的神经网络架构，用于分数布朗运动驱动的随机微分方程参数估计，在精度、鲁棒性和模型紧凑性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动驱动的随机微分方程在金融和可靠性工程中广泛应用，但这些过程具有非马尔可夫性和非半鞅结构，传统参数估计方法不适用或计算复杂。需要开发新的高效估计方法。

Method: 提出SigMA（Signature Multi-head Attention）架构，整合路径签名与多头自注意力机制，包含卷积预处理层和多层感知机进行特征编码。从分数布朗运动、分数Ornstein-Uhlenbeck和粗糙Heston模型生成的合成路径中学习参数。

Result: 在合成数据和两个真实世界数据集（股票指数已实现波动率和锂离子电池退化）上的实验表明，SigMA在准确性、鲁棒性和模型紧凑性方面一致优于CNN、LSTM、原始Transformer和深度签名基线方法。

Conclusion: 将签名变换与基于注意力的架构相结合，为具有粗糙或持久时间结构的随机系统中的参数推断提供了一个有效且可扩展的框架。

Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.

</details>


### [46] [Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption](https://arxiv.org/abs/2512.15112)
*Sunwoo Kim,Soo Yong Lee,Kyungho Kim,Hyunjin Hwang,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: FUEL：一种无监督节点表示学习方法，通过自适应调整图卷积的使用程度，在嵌入空间中增强类内相似性和类间分离性，在多种同质性水平的图上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督节点表示学习方法过度依赖图卷积，导致在非同质性图中，具有不同特征或拓扑属性的节点产生过度相似的嵌入。虽然有监督学习中已探索调整图卷积使用程度的方法，但在无监督场景中仍缺乏相关研究。

Method: 提出FUEL方法，自适应学习适当的图卷积使用程度。通过利用节点特征识别节点簇，并将这些簇作为类的代理，在嵌入空间中增强类内相似性和类间分离性。

Result: 在15种基线方法和14个基准数据集上的广泛实验表明，FUEL在下游任务中表现优异，在具有不同同质性水平的图上均实现了最先进的性能。

Conclusion: FUEL通过自适应调整图卷积使用程度，有效解决了无监督节点表示学习中过度依赖图卷积的问题，特别是在非同质性图中表现出色，为无监督图学习提供了新的有效方法。

Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.

</details>


### [47] [How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models](https://arxiv.org/abs/2512.15115)
*Ali Ghodsi*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架来分析序列建模架构，揭示了注意力机制与状态空间模型在表达能力与梯度传播之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 序列建模产生了多种架构（从RNN到Transformer和状态空间模型），但缺乏对表达能力与可训练性权衡的统一理论理解。需要建立一个理论框架来统一分析这些架构。

Method: 引入统一框架，通过输入依赖的有效交互算子W_ij(X)表示广泛的序列映射，识别两种构建模式：统一因子化框架（显式，注意力风格混合）和结构化动态（隐式，状态空间递归）。

Result: 1. 交互秩间隙：统一因子化框架中的模型（如单头注意力）受限于低维算子空间，无法表示某些结构化动态映射。2. 等价定理：在多头因子化类中，表示线性SSM需要且仅需H=k个头。3. 梯度高速公路：注意力层允许距离无关的梯度路径，而稳定线性动态则表现出距离相关的梯度衰减。

Conclusion: 该研究形式化了代数表达能力（交互/算子空间）与长距离梯度传播之间的基本权衡，为现代序列架构设计提供了理论基础。

Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.

</details>


### [48] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: FADTI：一种基于扩散的多元时间序列插补框架，通过可学习的傅里叶偏置投影模块注入频率感知特征调制，结合自注意力和门控卷积进行时序建模，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和扩散的模型缺乏明确的归纳偏置和频率感知能力，限制了它们在结构化缺失模式和分布偏移下的泛化能力。多元时间序列插补在医疗、交通预测和生物建模等应用中至关重要。

Method: 提出FADTI框架：1）通过可学习的傅里叶偏置投影模块注入频率感知特征调制；2）支持多种谱基，自适应编码平稳和非平稳模式；3）结合自注意力和门控卷积进行时序建模；4）将频率域归纳偏置注入生成式插补过程。

Result: 在多个基准测试（包括新引入的生物时间序列数据集）上，FADTI始终优于最先进的方法，特别是在高缺失率情况下表现突出。

Conclusion: FADTI通过注入频率感知的归纳偏置，有效提升了多元时间序列插补的性能，特别是在结构化缺失和高缺失率场景下，为实际应用提供了更可靠的解决方案。

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [49] [Automatic Reward Shaping from Multi-Objective Human Heuristics](https://arxiv.org/abs/2512.15120)
*Yuqing Xie,Jiayu Chen,Wenhao Tang,Ya Zhang,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: MORSE框架通过双层优化自动整合多个启发式奖励，结合随机探索避免局部最优，在机器人任务中达到与人工调优相当的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中设计有效的奖励函数是一个核心挑战，尤其是在多目标环境中。人工设计奖励函数困难且耗时，需要自动化的方法来整合多个启发式奖励。

Method: 提出MORSE框架，将奖励塑造过程建模为双层优化问题：内层训练策略最大化当前塑造的奖励，外层更新奖励函数以优化任务性能。引入随机性鼓励探索，通过任务性能和随机初始化神经网络的预测误差来指导噪声注入。

Result: 在MuJoCo和Isaac Sim环境中的实验结果表明，MORSE能有效平衡各种机器人任务中的多个目标，达到与手动调优奖励函数相当的任务性能。

Conclusion: MORSE提供了一个通用的奖励塑造框架，能够自动整合多个启发式奖励，通过随机探索避免局部最优，在多目标强化学习任务中表现优异。

Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.

</details>


### [50] [TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training](https://arxiv.org/abs/2512.15123)
*Mukur Gupta,Niharika Gupta,Saifur Rahman,Shantanu Pal,Chandan Karmakar*

Main category: cs.LG

TL;DR: TrajSyn：一种联邦学习中的隐私保护对抗训练框架，通过客户端模型更新轨迹合成代理数据集，在服务器端实现有效的对抗训练


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的深度学习模型在安全关键应用中面临对抗性扰动的风险，联邦学习环境中由于客户端数据隐私约束和计算资源有限，难以应用传统的对抗训练方法

Method: 提出TrajSyn框架，通过分析客户端模型更新的轨迹来合成代理数据集，在服务器端进行对抗训练，无需访问原始客户端数据，也不增加客户端计算负担

Result: 在图像分类基准测试中，TrajSyn能够持续提升模型的对抗鲁棒性，且客户端设备无需额外计算开销

Conclusion: TrajSyn为联邦学习提供了一种有效的隐私保护对抗训练解决方案，解决了传统方法在FL环境中的适用性问题

Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.

</details>


### [51] [From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?](https://arxiv.org/abs/2512.15134)
*Aaron Mueller,Andrew Lee,Shruti Joshi,Ekdeep Singh Lubana,Dhanya Sridhar,Patrik Reizinger*

Main category: cs.LG

TL;DR: 该研究提出多概念评估框架，分析稀疏自编码器和稀疏探针在概念相关性增强时的解耦表现，发现特征与概念存在一对多关系，且解耦度量指标不足以评估概念独立性。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性研究中，概念表示的质量通常在孤立状态下评估，隐含了独立性假设，但实践中概念间可能存在相关性。需要评估常见特征化方法（如稀疏自编码器和稀疏探针）是否能真正解耦这些概念表示。

Method: 提出多概念评估设置，控制文本概念（如情感、领域、时态）间的相关性，分析在相关性增强时的性能。首先评估特征化方法在不同相关强度下学习解耦表示的能力，然后进行操控实验，测量每个概念是否可独立操控。

Result: 观察到特征与概念存在一对多关系：特征最多对应一个概念，但概念分布在多个特征中。即使训练时概念分布均匀，SAE特征在操控时通常影响多个概念，表明它们既不具选择性也不独立；但特征影响不相交的子空间。

Conclusion: 解耦的相关性度量指标通常不足以评估操控时的独立性，影响不相交子空间也不足以确保概念选择性。这些结果强调了可解释性研究中组合评估的重要性。

Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.

</details>


### [52] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: 该研究比较了机器学习模型在德国NUTS-3区域作物产量预测中的泛化性能和可解释性，发现模型在时间独立验证中表现显著下降，但SHAP特征重要性仍看似可信，揭示了可解释性方法在泛化失败时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在农业和环境系统中的应用日益广泛，模型的可解释性和泛化能力变得至关重要。然而，现有研究往往忽视模型在时间独立验证中的表现，且可解释性方法可能在模型泛化失败时仍提供看似可靠的特征重要性，这在实际应用中存在风险。

Method: 使用德国NUTS-3区域的高质量长期数据集，系统比较了集成树模型（XGBoost、随机森林）和深度学习方法（LSTM、TCN）。评估包括空间分割的传统测试集和时间独立的验证年份，并使用SHAP进行特征重要性分析。

Result: 所有模型在空间分割测试集上表现良好，但在时间独立验证年份中性能显著下降。值得注意的是，即使模型在测试集上准确率高但时间验证表现差，SHAP特征重要性仍可能看似可信，这暴露了后验可解释性方法的脆弱性。

Conclusion: 研究强调需要在农业和环境系统中进行验证感知的机器学习解释。特征重要性不应被盲目接受，除非模型明确证明能在未见的时间和空间条件下泛化。建议采用领域感知验证、混合建模策略，并对数据驱动农业中的可解释性方法进行更严格的审查。

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [53] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: 提出一种新的联邦学习梯度成员推理攻击，利用多层梯度的时间演化模式，无需访问私有数据集，可扩展到属性推理，在多种数据集上表现出强大攻击性能


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习减少了直接数据暴露，但模型更新交换仍可能泄露敏感信息。现有攻击方法可能不够有效，需要开发更强大的推理攻击来评估联邦学习的隐私风险

Method: 使用影子技术学习训练记录的多轮梯度模式，无需访问私有数据集。考虑半诚实和恶意对手（聚合器或数据所有者）。通过对比不同属性假设下的梯度响应扩展到离散属性推理。攻击是模型无关的，适用于任何基于梯度的模型

Result: 在CIFAR-100和Purchase100数据集上评估成员推理，在Breast Cancer Wisconsin上评估属性推理。结果显示强大的攻击性能，与文献中其他攻击相比具有可比的计算和内存开销。多轮联邦学习增加了推理攻击的脆弱性

Conclusion: 多轮联邦学习增加了推理攻击的脆弱性，聚合器比数据所有者构成更大威胁，攻击性能受训练数据集性质强烈影响，丰富的高维数据比简单表格数据导致更强的泄露

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [54] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: 论文提出了一种统一的理论框架，通过分析神经正切核(NTK)的特征值方差来解释不同隐式神经表示(INR)架构如何缓解频谱偏置问题。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)通常收敛缓慢且难以恢复高频细节，这归因于频谱偏置。虽然先前工作将此行为与神经正切核(NTK)联系起来，但具体架构选择如何影响NTK条件数仍不清楚。

Method: 论文表明许多INR机制可以通过它们对少量成对相似性因子和缩放项的影响来理解，这些因素共同决定了NTK特征值方差。作者推导了常见INR组件的闭式方差分解，分析了位置编码、球面归一化和Hadamard调制等机制如何影响NTK条件数。

Result: 实验证实了预测的方差减少，并展示了更快速、更稳定的收敛以及改进的重建质量。位置编码重塑输入相似性，球面归一化通过逐层缩放减少方差，Hadamard调制引入严格小于1的额外相似性因子，产生乘法方差减少。

Conclusion: 该统一视角解释了不同INR架构如何通过改善NTK条件数来缓解频谱偏置问题，为理解和设计更有效的INR架构提供了理论基础。

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [55] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: DEER提出了一种基于扩散大语言模型（dLLM）的推测解码框架，通过扩散模型并行生成草稿，再用自回归模型验证，解决了传统AR草稿模型的不确定性累积和顺序解码问题，实现了高达5.54倍的加速。


<details>
  <summary>Details</summary>
Motivation: 自回归解码的延迟限制了LLM驱动系统的效率。现有推测解码方法使用AR草稿模型存在两个根本问题：1）逐步不确定性累积导致目标模型与草稿模型之间的信任逐渐崩溃；2）AR草稿模型固有的顺序解码特性。这些问题导致加速效果有限。

Method: 提出DEER框架，使用扩散大语言模型（dLLM）作为草稿模型，利用其不同的概率建模和高效并行解码策略。采用两阶段训练流程对齐dLLM草稿模型与目标AR模型，并使用单步解码生成长草稿段。

Result: DEER实现了高达32个token的草稿接受长度，远超EAGLE-3的10个token。在HumanEval基准测试中，使用Qwen3-30B-A3B模型，DEER获得了5.54倍加速，而EAGLE-3仅获得2.41倍加速。

Conclusion: 扩散大语言模型作为草稿模型能够有效解决传统AR草稿模型在推测解码中的根本问题，通过DEER框架实现了显著的加速效果，为LLM推理系统的高效化提供了新方向。

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [56] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: Chorus：一种面向物联网的无数据上下文感知模型定制方法，通过跨模态重建学习上下文表示，并使用门控头动态平衡传感器与上下文贡献，在未见上下文偏移下提升性能达11.3%


<details>
  <summary>Details</summary>
Motivation: 物联网应用中传感器数据受多样动态上下文条件（如传感器放置、环境）影响，传统域适应方法常忽略上下文信息或使用简单集成策略，难以处理部署后的未见上下文偏移

Method: 1. 无监督跨模态重建：在未标记传感器数据和基于语言的上下文嵌入间进行重建，正则化上下文嵌入空间学习鲁棒表示；2. 轻量门控头：在有限标记样本上训练，动态平衡传感器和上下文贡献；3. 上下文缓存机制：重用缓存表示，仅在检测到上下文偏移时更新以减少推理延迟

Result: 在IMU、语音和WiFi感知任务中，Chorus在未见上下文下比最先进基线提升达11.3%，同时在智能手机和边缘设备上保持可比较的延迟

Conclusion: Chorus通过有效学习上下文表示并自适应集成，实现了无需目标域数据的模型定制，能有效处理物联网应用中的动态上下文偏移，具有实际部署价值

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [57] [Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures](https://arxiv.org/abs/2512.15228)
*Songze Huo,Xiao-Ming Cao*

Main category: cs.LG

TL;DR: DBCata是一种深度生成模型，通过周期性布朗桥框架和等变图神经网络，直接从非弛豫结构生成DFT弛豫的吸附结构，无需能量或力信息，显著提升催化剂筛选效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习原子间势（MLIP）的训练数据主要来自近平衡结构，导致吸附结构和吸附能预测不可靠，限制了大规模催化剂筛选的准确性。

Method: 结合周期性布朗桥框架和等变图神经网络，建立非弛豫结构与DFT弛豫结构之间的低维过渡流形，无需显式的能量或力信息。采用混合化学启发式和自监督异常检测方法识别和精炼异常预测。

Result: 在Catalysis-Hub数据集上，原子间距离平均绝对误差（DMAE）达到0.035 Å，比当前最先进的机器学习势模型提升近三倍。94%的情况下DFT精度可在0.1 eV内改进。

Conclusion: DBCata能够高效生成高保真吸附几何结构，显著加速氧还原反应中高效合金催化剂的高通量计算筛选，成为催化剂设计和优化的有力工具。

Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \textÅ on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.

</details>


### [58] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: O-EENC-SD是一种基于EEND-EDA的端到端在线说话人日志系统，采用新型RNN拼接机制，通过质心细化解码器提升性能，在CallHome数据集上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有在线说话人日志方法存在两个主要问题：无监督聚类方法需要调参，而当前在线端到端方法计算成本过高。需要开发一种既无需调参又计算高效的在线说话人日志系统。

Method: 基于EEND-EDA框架，开发了新型RNN拼接机制用于在线预测，并设计了质心细化解码器。系统在独立块上工作，无需重叠，通过严格的消融研究验证了各组件有效性。

Result: 在CallHome数据集的双人电话对话场景中，O-EENC-SD与最先进方法具有竞争力。系统在DER和复杂度之间提供了良好平衡，即使在不重叠的独立块上工作也极为高效。

Conclusion: O-EENC-SD提供了一种超参数自由且计算高效的在线说话人日志解决方案，在性能和效率之间取得了优秀平衡，特别适用于实时应用场景。

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [59] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态生理信号融合方法，通过自监督预训练的CBraMod编码器分别处理ECG和EEG信号，采用双掩码策略捕获信号内部依赖关系，通过简单的嵌入拼接实现多模态融合，在情感识别任务上取得了接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）和脑电图（EEG）等生理信号能提供互补的健康和认知信息，但多模态整合面临两个主要挑战：1）多模态标注数据有限；2）模态间存在特异性差异。需要开发能够在有限监督下有效融合多模态生理信号的方法。

Method: 1）使用CBraMod编码器进行大规模自监督ECG预训练，引入双掩码策略捕获导联内和导联间依赖关系；2）利用预训练的CBraMod编码器处理EEG信号；3）为每个模态训练对称的编码器，获得丰富的表示；4）通过简单的嵌入拼接实现多模态融合，让分类头学习跨模态交互。

Result: 在情感识别任务上，该方法取得了接近最先进水平的性能，表明精心设计的生理信号编码器即使采用简单的融合策略，也能显著提升下游任务性能。

Conclusion: 基础模型方法能够充分利用生理信号的整体特性，为医疗健康和情感计算领域提供可扩展、标签高效且泛化性强的解决方案。精心设计的生理信号编码器配合简单的融合策略，能够在有限的多模态监督下实现有效的下游学习。

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [60] [Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory](https://arxiv.org/abs/2512.15267)
*Huiyan Xue,Xuming Ran,Yaxin Li,Qi Xu,Enhui Li,Yi Xu,Qiang Zhang*

Main category: cs.LG

TL;DR: SSD（选择性子网络蒸馏）是一种用于稀疏持续学习的结构引导框架，通过识别高频激活神经元并在先前Top-K子网络和输出logits之间进行选择性蒸馏，实现结构重对齐同时保持稀疏模块化。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络系统（如SDMLP）虽然通过构建任务特定子网络来抵抗灾难性遗忘，但其刚性模块化限制了跨任务知识重用，在高稀疏度下会导致性能下降。需要一种方法既能保持稀疏模块化的优势，又能促进跨任务知识转移。

Method: SSD框架将蒸馏视为拓扑对齐的信息通道而非正则化器。它识别具有高激活频率的神经元，然后在先前任务的Top-K子网络和输出logits之间进行选择性知识蒸馏，无需重放数据或任务标签。这种方法实现了结构重对齐同时保持稀疏模块化特性。

Result: 在Split CIFAR-10、CIFAR-100和MNIST数据集上的实验表明，SSD提高了准确性、保留率和表示覆盖率，为稀疏持续学习提供了结构基础解决方案。

Conclusion: SSD通过结构引导的蒸馏方法有效解决了稀疏神经网络在持续学习中的知识重用限制问题，在保持稀疏模块化优势的同时实现了更好的跨任务知识转移和性能提升。

Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.

</details>


### [61] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: 提出Persistence：基于持久同调的无监督度量，用于评估嵌入空间的质量，无需标签即可量化几何结构和拓扑丰富性


<details>
  <summary>Details</summary>
Motivation: 现代表示学习依赖无监督和自监督方法，但缺乏无标签情况下评估嵌入质量的可靠度量。现有方法通常假设线性可分或依赖协方差结构，无法捕捉全局和多尺度组织。

Method: 提出Persistence度量，基于持久同调（拓扑数据分析方法），量化嵌入空间的几何结构和拓扑丰富性。该方法完全无监督，不依赖标签信息。

Result: 在多个领域的实证结果表明，Persistence与下游任务性能的相关性优于现有无监督度量，能够可靠地用于模型和超参数选择。

Conclusion: Persistence提供了一种有效的无监督度量方法，能够可靠评估嵌入质量，为表示学习模型的选择和优化提供了重要工具。

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [62] [Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions](https://arxiv.org/abs/2512.15286)
*Siva Sai,Ishika Goyal,Shubham Sharma,Sri Harshita Manuri,Vinay Chamola,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 该调查论文全面概述了量子机器学习在网络安全领域的应用，包括QNNs、QSVMs、VQCs、QGANs等技术，并将其映射到监督、无监督和生成学习范式及核心网络安全任务中。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习、规则和基于签名的防御策略因网络威胁数量增加、战术快速演变以及数据量庞大而失效，无法跟上发展步伐。量子机器学习作为替代方案出现，利用量子力学计算，为某些问题提供更好的高维结构编码和处理能力。

Method: 该调查论文采用文献综述方法，全面概述量子机器学习技术在安全领域的应用，包括量子神经网络、量子支持向量机、变分量子电路和量子生成对抗网络等。将这些方法映射到监督、无监督和生成学习范式，并与核心网络安全任务（入侵检测、异常检测、恶意软件分类、僵尸网络分类、加密流量分析）相关联。

Result: 论文提供了量子机器学习在网络安全领域的全面技术概述，展示了这些方法如何应用于不同安全任务。同时讨论了量子机器学习在云计算安全领域的应用潜力，能够增强安全性和可扩展性操作。还识别了量子机器学习在网络安全领域的许多局限性。

Conclusion: 量子机器学习为网络安全提供了有前景的替代方案，能够处理高维数据和复杂威胁。虽然存在局限性，但该调查为未来研究提供了方向，特别是在云计算安全等领域的应用潜力值得进一步探索。

Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.

</details>


### [63] [Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference](https://arxiv.org/abs/2512.15335)
*Chenxiang Zhang,Tongxi Qu,Zhong Li,Tian Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 量化技术通过降低模型参数精度来减少内存和计算成本，但现有隐私分析主要关注全精度模型。本文首次系统研究了后训练量化中的隐私-效用关系，发现低精度量化可显著降低成员推理攻击的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络广泛采用量化技术来降低内存和计算成本，但现有隐私分析主要针对全精度模型，缺乏对量化如何影响隐私泄露的系统理解。量化会改变模型参数和输出，需要研究不同精度级别对隐私保护的影响。

Method: 使用成员推理攻击作为评估框架，分析三种流行的后训练量化算法（AdaRound、BRECQ、OBC）在多个精度级别（4位、2位、1.58位）和多个数据集（CIFAR-10、CIFAR-100、TinyImageNet）上的表现。还进行了消融研究，探索仅对最后一层使用更高精度量化的效果。

Result: 低精度后训练量化可显著降低隐私泄露风险。与全精度模型相比，低精度模型在成员推理攻击中的脆弱性最多可降低一个数量级，但会牺牲一定的模型效用。消融研究表明，仅对最后一层使用更高精度量化可实现细粒度的隐私-效用权衡控制。

Conclusion: 后训练量化不仅提高效率，还能增强隐私保护。低精度量化可显著降低成员推理攻击风险，而分层量化策略为实际部署中平衡效率、效用和隐私提供了可操作的解决方案。

Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.

</details>


### [64] [Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery](https://arxiv.org/abs/2512.15344)
*Hiroyoshi Nagahama,Katsufumi Inoue,Masayoshi Todorokihara,Michifumi Yoshioka*

Main category: cs.LG

TL;DR: 论文提出两种相位感知预处理策略来处理多轴振动数据中的随机相位变化，通过新构建的转子数据集验证了这些方法能显著提升深度学习模型的预测维护性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的旋转机械预测维护方法大多在频谱特征提取时丢弃相位信息，或使用原始时间波形而不显式利用相位信息。多轴振动数据中的随机相位变化会影响模型性能，需要有效的相位预处理策略。

Method: 提出两种相位感知预处理策略：1）三轴独立相位调整，将每个轴单独对齐到零相位；2）单轴参考相位调整，通过统一时间偏移保持轴间相位关系。使用新构建的同步三轴传感器转子数据集，在两级学习框架下评估六种深度学习架构。

Result: 两种方法均带来架构无关的性能提升：三轴独立方法获得一致增益（Transformer提升2.7%），单轴参考方法通过保持空间相位关系实现最佳性能，准确率达96.2%（提升5.4%）。

Conclusion: 两种相位对齐策略都是实用且可扩展的预测维护系统增强方法，能有效处理多轴振动数据中的相位变化问题，为旋转机械的预测维护提供了新的相位感知预处理方案。

Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.

</details>


### [65] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: F3框架通过自适应融合Rocket、Sax和Sfa三种表示，在特定数据集类型上比单独使用Rocket获得一致改进，尤其适用于具有结构化变异或丰富频率内容的数据。


<details>
  <summary>Details</summary>
Motivation: 尽管Rocket等基于核的方法在单变量时间序列分类中表现优异，但并非在所有数据集上都同样有效。研究基于不同表示捕获互补结构的直觉，探索通过选择性融合来系统性地改进Rocket在特定类型数据集上的性能。

Method: 提出Fusion-3(F3)轻量级框架，自适应融合Rocket、Sax和Sfa三种表示。通过元特征(序列长度、谱结构、粗糙度、类别不平衡)将UCR数据集聚类为6组可解释的数据结构机制，分析融合在不同机制下的效果。采用三种互补分析：跨数据集的非参数配对统计、消融研究、SHAP归因分析。

Result: 在113个UCR数据集上使用5折交叉验证，F3相比Rocket获得小而一致的平均改进。融合在具有结构化变异或丰富频率内容的机制中表现优异，而在高度不规则或异常值多的设置中收益递减。样本级案例显示融合主要通过纠正特定错误来提升性能。

Conclusion: 选择性应用融合为强大的基于核方法提供了可靠且可解释的扩展，能够在数据支持的情况下精确纠正其弱点。融合收益可预测，失败案例可清晰识别，为实际应用提供了实用指导。

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [66] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 提出一个统一框架，用于系统评估电力系统保护中机器学习模型的鲁棒性，分析传感器故障、采样率降低等退化场景对故障分类和定位的影响。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式发电的普及正在改变电力系统，挑战了依赖固定设置和本地测量的传统保护方案。机器学习为集中式故障分类和定位提供了数据驱动替代方案，但实际部署需要鲁棒性保证，保护算法必须在传感器数据缺失、噪声或退化时仍保持可靠。

Method: 引入一个统一框架，使用高保真电磁暂态（EMT）仿真来建模现实退化场景，包括传感器中断、采样率降低和瞬态通信丢失。该框架提供一致的方法论来基准测试模型，量化有限可观测性的影响，并识别弹性运行所需的关键测量通道。

Result: 结果显示故障分类在大多数退化类型下保持高度稳定，但在单相丢失时下降约13%；故障定位整体更敏感，电压丢失使定位误差增加超过150%。

Conclusion: 这些发现为未来机器学习辅助保护系统的鲁棒性设计提供了可操作的指导，强调了在传感器数据退化情况下确保保护算法可靠性的重要性。

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [67] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出EUBRL算法，利用认知不确定性指导探索，在无限时域折扣MDP中实现接近极小极大最优的遗憾和样本复杂度保证


<details>
  <summary>Details</summary>
Motivation: 智能体在已知与未知边界面临探索-利用困境，认知不确定性反映了知识有限带来的系统性不确定性，需要设计能利用认知指导进行原则性探索的算法

Method: 提出贝叶斯强化学习算法EUBRL，利用认知不确定性指导探索，自适应减少由估计误差引起的每步遗憾

Result: 在无限时域折扣MDP中，对一类充分表达的先验建立了接近极小极大最优的遗憾和样本复杂度保证；在稀疏奖励、长时域、随机性任务中表现出优越的样本效率、可扩展性和一致性

Conclusion: EUBRL通过认知不确定性指导实现了原则性探索，在理论和实证上都表现出优越性能，为解决探索-利用困境提供了有效方法

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [68] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: FlowBind：一个高效的任意到任意生成框架，通过共享潜在空间和模态特定的可逆流，减少数据需求和计算成本，训练速度比现有方法快10倍


<details>
  <summary>Details</summary>
Motivation: 现有基于流的方法存在效率问题：需要大规模数据集且有配对约束，建模联合分布计算成本高，依赖复杂的多阶段训练。需要更高效的任意到任意生成方法。

Method: 学习一个捕获跨模态信息的共享潜在空间，使用模态特定的可逆流将每个模态桥接到这个潜在空间。通过单一流匹配目标联合优化这两个组件，推理时使用可逆流作为编码器和解码器进行直接跨模态转换。

Result: 在文本、图像和音频上的实验表明，FlowBind在保持可比生成质量的同时，参数减少6倍，训练速度比先前方法快10倍，显著降低了数据需求和计算成本。

Conclusion: FlowBind通过共享潜在空间和模态特定可逆流的简单设计，实现了高效的任意到任意生成，在减少数据需求和计算成本的同时保持竞争性生成质量。

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [69] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: 该研究分析了随机矩阵中经过最小-最大归一化处理的特征值的统计特性，推导了归一化特征值的累积分布标度律和矩阵分解的残差误差，并通过数值实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 在数据科学实践中，输入数据通常需要进行归一化处理。随机矩阵理论在纯数学、数学物理和机器学习中都有重要应用，因此研究归一化特征值的统计特性具有实际意义。

Method: 应用先前提出的归一化特征值有效分布，推导累积分布的标度律和矩阵分解的残差误差，并通过数值实验验证理论预测。

Result: 成功推导了归一化特征值累积分布的标度律和矩阵分解的残差误差，数值实验结果与理论预测一致，验证了方法的有效性。

Conclusion: 该研究为随机矩阵中归一化特征值的统计特性提供了理论分析框架，推导的标度律和残差误差公式对数据科学中的矩阵处理具有指导意义。

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [70] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: 提出FM-EAC算法，结合基于模型和无模型强化学习，通过特征模型和增强的actor-critic框架提升多任务控制中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习方法在跨任务和场景的有效迁移性方面仍存在困难，尽管基于模型和无模型强化学习在Dyna-Q中有所融合，但实际应用中的泛化能力不足。

Method: 提出FM-EAC算法，整合规划、行动和学习，结合基于模型和无模型强化学习的优势，使用新颖的特征模型和增强的actor-critic框架。

Result: 在城市和农业应用场景的仿真中，FM-EAC持续优于多种最先进的基于模型和无模型强化学习方法，且支持根据用户需求定制子网络。

Conclusion: FM-EAC通过融合基于模型和无模型强化学习的优势，有效提升了多任务控制中的泛化能力和适应性，为动态环境中的强化学习应用提供了有效解决方案。

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [71] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: DHMBPO提出双视野模型强化学习方法，通过长分布视野和短训练视野解决模型偏差与梯度稳定性冲突，提升样本效率和运行速度。


<details>
  <summary>Details</summary>
Motivation: 在基于模型的强化学习中，选择视野长度面临两个困境：长视野能保持同策略训练但放大模型偏差，短视野能稳定梯度但可能增加价值估计偏差。这两个最优视野可能不同，需要解决这一冲突。

Method: 提出双视野模型策略优化(DHMBPO)，将视野过程分为长"分布视野"(DR)和短"训练视野"(TR)。DR生成同策略状态样本以缓解分布偏移，TR利用可微转换提供准确的价值梯度估计，减少更新次数和总运行时间。

Result: 双视野方法有效平衡了分布偏移、模型偏差和梯度不稳定性，在连续控制基准测试中超越了现有的MBRL方法，在样本效率和运行时间方面都表现更好。

Conclusion: DHMBPO通过分离分布生成和训练过程，解决了模型强化学习中视野选择的冲突，实现了更好的性能平衡。

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [72] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: 该论文研究如何通过思维链和任务指令提示等技术减少文本到图像生成模型中的版权内容生成，结合负向提示和提示重写策略，评估不同模型上这些技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像生成模型可能记忆并复制其训练数据，而训练数据常包含受版权保护的材料，这会导致版权侵权风险，给AI用户和开发者带来法律和财务损失。

Method: 提出结合思维链和任务指令提示的框架，并与两种版权缓解策略结合：a) 负向提示，b) 提示重写。通过评估生成图像与版权图像的相似度以及与用户输入的相关性来研究这些技术。

Result: 在不同复杂度的模型上进行数值实验，提供了关于上述技术有效性的见解，展示了这些方法在减少版权内容生成方面的潜力。

Conclusion: 思维链和任务指令提示结合负向提示和提示重写可以有效减少文本到图像生成中的版权侵权风险，为模型开发者提供了实用的版权缓解策略。

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [73] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: 提出Invertibility Loss (InvLoss)量化联邦学习中数据重构攻击的最大风险，推导其可计算上界，并开发InvRE风险估计器和自适应噪声防御方法。


<details>
  <summary>Details</summary>
Motivation: 数据重构攻击对联邦学习系统构成严重威胁，但缺乏理论基础的量化框架来评估风险。现有研究未能解决如何表征和评估联邦学习中数据重构攻击风险的问题。

Method: 引入Invertibility Loss (InvLoss)量化数据实例和FL模型的最大可达到攻击效果，推导其紧致可计算上界，从三个角度探索：1）基于雅可比矩阵谱特性分析风险；2）开发InvRE风险估计器；3）提出两种自适应噪声扰动防御方法。

Result: 在真实数据集上的广泛实验验证了该框架的有效性，证明其能够系统评估和缓解联邦学习中的数据重构攻击风险，同时不影响分类准确性。

Conclusion: 该工作填补了联邦学习数据重构攻击风险量化框架的空白，为系统评估和缓解攻击风险提供了理论基础和实用工具，有助于提升联邦学习系统的隐私保护能力。

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [74] [Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance](https://arxiv.org/abs/2512.15469)
*Ioannis Kalogeropoulos,Giorgos Bouritsas,Yannis Panagakis*

Main category: cs.LG

TL;DR: 提出一种基于图元网络的神经网络编辑框架，通过单次推理步骤高效修改模型以满足各种需求（如公平性、剪枝等），同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键领域部署时需满足多种需求（法规合规、公平性、计算约束等），传统后处理方法会牺牲性能，而重新训练耗时且不总是可行，需要一种高效编辑模型的方法

Method: 提出统一的数据驱动框架，使用图元网络作为编辑器，该元网络在神经网络群体上训练，通过最小化包含需求满足和性能保持的双目标函数，实现对目标神经网络的单次推理编辑

Result: 在数据最小化原则、偏见缓解和权重剪枝等任务上实验，相比传统后处理或重新训练方法，在性能、需求满足和时间效率之间取得了更好的平衡

Conclusion: 图元网络编辑框架能够高效修改神经网络以满足各种需求，同时保持模型实用性，为模型合规和优化提供了新的有效途径

Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.

</details>


### [75] [Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs](https://arxiv.org/abs/2512.15483)
*Luca Torresi,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯优化的扩展方法，允许在多阶段工作流程中进行灵活采样，并基于中间观测值（代理测量）做出最优决策，显著提高了自驱动实验室的实验效率。


<details>
  <summary>Details</summary>
Motivation: 当前自驱动实验室广泛使用的贝叶斯优化算法依赖于固定的实验流程，无法在实验过程中根据中间测量结果动态调整实验计划，这限制了复杂真实世界实验在自驱动实验室中的应用。

Method: 提出贝叶斯优化的扩展方法，支持多阶段工作流程的灵活采样，能够基于中间观测值（代理测量）进行决策，允许在实验过程中根据实时数据调整后续操作序列。

Result: 在广泛场景下，使用代理测量的方法相比传统贝叶斯优化（仅观察最终测量结果）有显著改进，既能更快找到良好解决方案，也能获得更优的整体解决方案。

Conclusion: 该方法不仅为自驱动实验室中使用更复杂、更真实的实验流程铺平了道路，还能平滑地结合模拟和实验，推动下一代自驱动实验室的发展。

Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.

</details>


### [76] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: 该研究比较了两种评估分类器个体预测可靠性的方法：鲁棒性量化(RQ)和不确定性量化(UQ)，发现两者互补，结合后的混合方法优于单独使用任一种方法。


<details>
  <summary>Details</summary>
Motivation: 评估分类器个体预测的可靠性对于实际应用至关重要，目前存在两种概念不同的方法：鲁棒性量化(RQ)和不确定性量化(UQ)，但缺乏对这两种方法的系统比较和整合研究。

Method: 在多个基准数据集上系统比较RQ和UQ方法，分析各自的优缺点，然后将两种方法结合形成混合方法，并评估每个数据集中不确定性和鲁棒性作为不可靠性来源的相对重要性。

Result: 研究发现RQ和UQ没有明显的优劣之分，而是互补的；结合两者的混合方法在性能上优于单独使用RQ或UQ；同时获得了各数据集中不确定性和鲁棒性相对重要性的评估。

Conclusion: RQ和UQ是评估分类器预测可靠性的两种互补方法，结合使用可以获得更好的性能，同时能够量化不同数据集中不确定性和鲁棒性对不可靠性的贡献程度。

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [77] [Soft Geometric Inductive Bias for Object Centric Dynamics](https://arxiv.org/abs/2512.15493)
*Hampus Linander,Conor Heins,Alexander Tschantz,Marco Perin,Christopher Buckley*

Main category: cs.LG

TL;DR: 提出基于几何代数神经网络的物体中心世界模型，提供软几何归纳偏置，在2D刚体动力学环境中实现更好的长期预测性能


<details>
  <summary>Details</summary>
Motivation: 等变性是学习物理动力学的强大先验，但精确的群等变性在对称性被破坏时可能降低性能。需要一种软几何归纳偏置来平衡先验与灵活性。

Method: 使用几何代数神经网络构建物体中心世界模型，在2D刚体动力学与静态障碍物环境中进行自回归训练，用于下一步预测

Result: 在长期滚动预测中，模型的软归纳偏置在物理保真度方面优于非等变基线模型，提供样本高效的多物体场景动力学建模

Conclusion: 几何代数在手工物理与无结构深度网络之间提供了有效折中，通过简单而精心选择的先验实现鲁棒泛化，补充了软等变性的研究思路

Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.

</details>


### [78] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: 提出一种基于无限维高斯过程的方法，用于建模随时间变化的向量集分布，通过随机傅里叶特征近似获得紧凑可比的时序向量表示，实现跨域时序动态分析。


<details>
  <summary>Details</summary>
Motivation: 生态学、犯罪分析和语言学等多个领域都需要分析随时间演变的向量集（如生态系统结构、犯罪空间分布、词嵌入向量），但这些时序向量集结构复杂且动态变化，现有方法难以有效建模和比较。

Method: 使用无限维高斯过程建模每个时间点向量集的潜在分布，通过随机傅里叶特征近似高斯过程中的隐函数，获得紧凑的向量表示，从而在低维空间中追踪和可视化向量集的时序演变。

Result: 在犯罪分布数据和词嵌入数据上的实验表明，该方法能有效捕捉时序动态，提供可解释且稳健的表示，为跨域时序向量集结构变化分析提供了有力框架。

Conclusion: 提出的基于高斯过程和随机傅里叶特征的方法能够有效建模和比较时序向量集，为生态、犯罪、语言等多个领域的动态结构分析提供了通用且强大的分析工具。

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [79] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出JUICE方法，在统一框架中同时处理多视图数据的特征-实例协同选择和缺失值填补，利用跨视图邻域信息提升选择效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理未标记的不完整多视图数据时，通常先填补缺失数据，然后将所有视图拼接成单一数据集进行协同选择。这种策略将协同选择和缺失数据填补视为两个独立过程，忽略了它们之间的潜在相互作用。同时，简单合并多视图数据无法捕捉视图间的互补信息，限制了协同选择效果。

Method: 提出JUICE方法：1）在统一框架中同时进行缺失数据恢复和特征-实例协同选择；2）利用跨视图邻域信息学习样本间关系，在重建过程中进一步精化缺失值填补；3）通过这种方式选择更具代表性的特征和实例。

Result: 大量实验表明，JUICE方法优于现有的最先进方法。

Conclusion: JUICE通过将缺失数据填补和特征-实例协同选择整合到统一框架中，并利用跨视图邻域信息，有效提升了多视图数据协同选择的性能。

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [80] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 该论文提出了一种针对扩散语言模型的校正导向后训练方法，解决了传统掩码扩散训练无法可靠识别和修正错误的问题，并在代码修订任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在结构上适合迭代错误修正，但标准的掩码扩散语言模型训练无法可靠地诱导这种校正行为，因为模型往往无法识别完整输入中的不可靠标记，导致置信度引导的修正无效。

Method: 提出了校正导向的后训练原则，明确监督可见的错误标记，使模型具备错误感知的置信度和针对性修正能力。同时引入了代码修订基准（CRB）来评估校正行为。

Result: 实验表明，采用该方法的模型在校正场景中显著优于标准MDLMs，同时也在纯补全任务上提升了性能。

Conclusion: 通过校正导向的训练方法，扩散语言模型能够有效识别错误标记并进行针对性修正，在代码修订等需要错误修正的任务中表现出色。

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [81] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: 提出N-单纯形注意力机制，从成对token相似性扩展到高阶交互，适配RoPE位置编码，并通过成本有效的单纯形选择管理计算复杂度，同时分析其平滑特性。


<details>
  <summary>Details</summary>
Motivation: 当前基于图消息传递的模型（如GAT、Transformer）虽然取得了state-of-the-art结果，但仅限于成对token交互。为了进一步突破，需要探索更高阶的交互机制，同时管理由此带来的计算复杂度增加。

Method: 1. 引入N-单纯形注意力机制，将注意力从成对相似性扩展到高阶交互；2. 适配Rotary Position Embeddings (RoPE)；3. 提出成本有效的单纯形选择方法，让模型聚焦于任务敏感的高阶交互；4. 推导Lipschitz上界并分析平滑特性。

Result: 论文展示了N-单纯形注意力机制能够实现更高阶的token交互，同时通过单纯形选择有效管理计算成本。分析表明该机制虽然扩展了注意力消息传递到高阶交互，但仍然存在过平滑问题。

Conclusion: N-单纯形注意力为注意力机制提供了从成对交互到高阶交互的理论框架，通过成本有效的单纯形选择平衡了计算复杂度，但需要进一步解决过平滑问题以实现更优的性能。

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [82] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 该论文建立了自回归模型（ARMs）和能量基模型（EBMs）之间的显式双射关系，揭示了它们在函数空间中的等价性，并分析了从EBMs到ARMs的知识蒸馏的理论误差界限。


<details>
  <summary>Details</summary>
Motivation: 自回归模型是目前大语言模型的主流范式，而能量基模型虽然在LLM开发中较少使用，但自然地表征了后训练对齐中的最优策略。论文旨在为这两类模型提供统一视角，理解它们之间的关系。

Method: 从概率链式法则出发，在函数空间中建立ARMs和EBMs之间的显式双射关系，证明这对应于最大熵强化学习中软贝尔曼方程的特殊情况。基于此双射，推导ARMs和EBMs监督学习的等价性，并分析EBMs蒸馏到ARMs的理论误差界限。

Result: 建立了ARMs和EBMs之间的函数空间双射，证明了监督学习的等价性，提供了EBMs蒸馏到ARMs的理论误差界限，为理解ARMs的"前瞻规划"能力提供了理论依据。

Conclusion: 论文为ARMs和EBMs提供了统一的数学框架，揭示了它们之间的深刻联系，解释了为什么基于下一个token预测的ARMs能够展现出前瞻规划能力，为模型设计和分析提供了新的理论工具。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [83] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: BEAT提出一种统一可迁移框架，将用户物品行为token化为离散可解释序列，通过向量量化自编码构建行为词汇表，解耦宏观兴趣和微观意图，并引入语义对齐机制将行为token嵌入冻结语言模型，提升零样本推荐性能并生成连贯解释。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法依赖ID表示，语义模糊且对语言模型施加结构限制，在开放场景中适用性有限。真实世界交互复杂，用户意图多样且协作信号与语言语义不对齐，需要更灵活的解决方案。

Method: 1) 通过向量量化自编码构建行为词汇表，从图表示中解耦宏观兴趣和微观意图；2) 引入多级语义监督桥接行为信号与语言空间；3) 设计语义对齐正则化机制，将行为token直接嵌入冻结语言模型的输入空间。

Result: 在三个公开数据集上，BEAT提升了零样本推荐性能，同时生成连贯且信息丰富的解释。行为token能够捕捉细粒度语义，为大型语言模型集成复杂行为模式提供即插即用接口。

Conclusion: BEAT通过将行为token化为离散序列并语义对齐语言模型，克服了现有方法的局限性，实现了统一可迁移的推荐框架，在保持解释性的同时提升了零样本性能。

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [84] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: SoFlow是一种用于一步生成的新框架，通过分析速度函数与解函数的关系，提出Flow Matching损失和解一致性损失来训练模型，无需计算Jacobian-vector product，在ImageNet 256x256上表现优于MeanFlow。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和Flow Matching模型中的多步去噪过程导致效率低下，这促使研究者探索少步生成方法。为了解决这个问题，需要开发能够实现一步生成的框架。

Method: 提出Solution Flow Models (SoFlow)框架，通过分析速度ODE中速度函数与解函数的关系，设计Flow Matching损失和解一致性损失来训练模型。Flow Matching损失使模型能够在训练期间提供Classifier-Free Guidance的估计速度场，而一致性损失无需计算Jacobian-vector product。

Result: 在ImageNet 256x256数据集上，使用相同的DiT架构和相同训练轮数从头训练时，SoFlow模型获得了比MeanFlow模型更好的FID-50K分数。

Conclusion: SoFlow框架能够实现高效的一步生成，通过创新的损失函数设计避免了计算Jacobian-vector product的需求，在图像生成任务上表现出优越性能。

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [85] [A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks](https://arxiv.org/abs/2512.15685)
*Oleg Melnikov,Yurii Dorofieiev,Yurii Shakhnovskiy,Huy Truong,Victoria Degeler*

Main category: cs.LG

TL;DR: SICAMS框架使用多元统计分析检测、分类和初步定位供水管网异常，无需水力模型校准


<details>
  <summary>Details</summary>
Motivation: 供水管网异常检测需要处理多传感器数据，现有方法依赖校准的水力模型，限制了实际应用

Method: 通过白化变换消除测量空间相关性，构建Hotelling's T²统计量进行假设检验，开发启发式算法分类异常，使用拉普拉斯插值进行粗定位

Result: 在BattLeDIM L-Town基准数据集上表现出高灵敏度和可靠性，即使在多重泄漏下也能保持鲁棒性能

Conclusion: SICAMS框架为供水管网异常管理提供了实用工具，无需校准水力模型即可应用于实际运行环境

Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.

</details>


### [86] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: G2RL是一种梯度引导的强化学习框架，利用模型自身的梯度几何来指导探索，而非依赖外部启发式方法，在数学和推理基准测试中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的探索机制（如熵奖励和外部语义比较器）与大型语言模型的实际学习方式存在根本性不匹配。这些方法鼓励表面变化，但不能保证采样轨迹在影响优化的梯度方向上有实质性差异。

Method: G2RL框架利用模型自身的一阶更新几何来指导探索。通过从模型最后一层敏感性构建序列级特征（可通过标准前向传播低成本获得），并在采样组内比较这些特征来衡量每个轨迹如何重塑策略。引入新颖梯度方向的轨迹获得有界乘法奖励缩放，而冗余或偏离流形的更新则被弱化。

Result: 在Qwen3基础1.7B和4B模型上，G2RL在数学和通用推理基准测试（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro）中，在pass@1、maj@16和pass@k指标上持续优于基于熵的GRPO和外部嵌入方法。几何分析显示G2RL将探索扩展到更多正交且常常相反的梯度方向，同时保持语义连贯性。

Conclusion: 策略自身的更新空间为大型语言模型强化学习中的探索提供了更忠实和有效的指导基础，G2RL框架通过梯度几何引导探索，实现了与PPO风格稳定性和KL控制自然对齐的自参照探索信号。

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [87] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: 提出多模态语义通信框架，通过文本查询引导视觉信息提取，使用跨模态注意力机制和自适应分辨率传输，在带宽受限环境下实现高效语义通信。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的方法在复杂多目标场景中表现不佳，因为自注意力缺乏明确的任务指导。需要一种能够整合用户查询来指导信息提取的语义通信框架。

Method: 提出多模态语义通信框架，使用跨模态注意力机制融合视觉特征和语言嵌入，生成软相关性分数。基于这些分数和瞬时信道带宽，采用算法以自适应分辨率传输图像块，使用独立训练的编码器-解码器对，总比特率匹配信道容量。

Result: 接收端能够重建和组合图像块，保留任务关键信息。该设计在复杂和带宽受限环境中实现了高效的语义通信。

Conclusion: 提出的多模态语义通信框架通过整合文本查询指导信息提取，使用自适应分辨率传输机制，为复杂场景下的语义通信提供了灵活且目标驱动的解决方案。

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>


### [88] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: FrontierCS是一个包含156个开放式计算机科学问题的基准测试，这些问题由专家设计，针对未知最优解但可客观评估的任务，要求模型实现可执行程序而非直接答案。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注已知最优解的任务，缺乏针对计算机科学前沿、开放式问题的评估框架。需要创建能够客观评估模型在复杂、未知最优解问题上的表现基准。

Method: 创建包含156个开放式问题的基准测试，分为算法问题（NP-hard变体）和研究问题，每个问题提供专家参考解决方案和自动评估器。要求模型实现可执行程序来解决任务。

Result: 前沿推理模型在算法和研究轨道上都远远落后于人类专家；仅增加推理预算无法缩小这一差距；模型往往过度优化生成可工作代码而非发现高质量算法和系统设计。

Conclusion: FrontierCS为计算机科学前沿难度提供了基准测试，揭示了当前模型在解决开放式复杂问题上的局限性，强调了需要超越简单代码生成、实现真正算法创新的模型能力。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [89] [Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data](https://arxiv.org/abs/2512.15706)
*Kayode Olumoyin,Lamees El Naqa,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINN）在数据有限的情况下学习膀胱癌细胞与免疫细胞之间的时变相互作用，预测抗癌联合治疗下的亚群轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统固定参数模型无法捕捉生物系统中随时间演化的动态相互作用，特别是在肿瘤学中，实验数据稀疏且仅有少数时间点观测值，需要新方法来学习时变相互作用。

Method: 采用物理信息神经网络（PINN）方法，在有限数据场景下学习膀胱癌细胞与免疫细胞之间的时变相互作用，预测抗癌联合治疗下的亚群轨迹。

Result: 该方法能够预测无观测数据时间点的亚群轨迹，且预测结果与生物学解释一致，为学习外部干预下生物体间演化相互作用提供了框架。

Conclusion: PINN方法能够有效处理稀疏数据，学习生物系统中的时变相互作用，为理解抗癌治疗下肿瘤-免疫系统动态提供新工具。

Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [90] [Reexamining Paradigms of End-to-End Data Movement](https://arxiv.org/abs/2512.15028)
*Chin Fang,Timothy Stitt,Michael J. McManus,Toshio Moriya*

Main category: cs.DC

TL;DR: 论文挑战了仅关注网络带宽的传统观点，指出数据迁移的实际瓶颈往往在网络核心之外，需要硬件-软件协同设计的整体方法。


<details>
  <summary>Details</summary>
Motivation: 当前高性能数据传输研究过度聚焦于网络带宽，特别是100Gbps以上的国际链路，但这种网络中心视角不完整，忽略了从边缘到核心整个频谱中实际可持续的数据迁移能力。

Method: 研究调查了六种常见范式，包括网络延迟、TCP拥塞控制算法等网络约束，以及CPU性能、虚拟化等主机端因素。通过支持延迟仿真的高速广域网性能预测测试平台，以及从资源受限边缘环境到瑞士-加州100Gbps运营链路的广泛生产测量进行验证。

Result: 结果显示主要瓶颈通常位于网络核心之外，硬件-软件协同设计的整体方法能确保从1Gbps到100Gbps及以上数据传输的稳定性能，有效缩小基准测试结果与复杂生产环境之间的差距。

Conclusion: 仅关注网络带宽不足以实现高性能数据传输，需要采用涵盖整个边缘到核心频谱的硬件-软件协同设计方法，才能在实际生产环境中获得一致可靠的性能表现。

Abstract: The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.

</details>


### [91] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ是一个端到端的CUDA/C++实现，用于在消费级GPU上训练中等规模语言模型（3B-32B参数），通过优化内存和通信瓶颈，能在单张16GB游戏卡上训练7B模型，或在4张RTX 4090上训练32B模型。


<details>
  <summary>Details</summary>
Motivation: 消费级GPU内存有限且通信速度慢，难以训练中等规模语言模型。现有系统主要针对数据中心级GPU设计，需要昂贵的硬件配置。LLMQ旨在让研究者和开发者能在普通消费级硬件上训练语言模型。

Method: 采用端到端CUDA/C++实现，包含激活检查点、卸载和基于复制引擎的集体通信等优化技术。使用标准的8位训练流程，无需额外的算法近似，保持约50%的FLOP利用率。

Result: 能在单张16GB中端游戏卡上训练或微调7B参数模型，或在配备4张RTX 4090的工作站上训练32B模型。效率与在更昂贵的云级GPU上的生产级系统相当。

Conclusion: LLMQ证明了在消费级GPU上高效训练中等规模语言模型的可行性，为研究者和开发者提供了经济实惠的替代方案，降低了语言模型训练的门槛。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [92] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: GPU加速的Bloom过滤器优化设计，通过向量化、线程协作和计算延迟优化，在保持高精度的同时实现高吞吐量，性能超越现有方法11.35-15.4倍。


<details>
  <summary>Details</summary>
Motivation: GPU具有大规模线程级并行性和高带宽内存，适合加速Bloom过滤器变体至每秒数十亿次操作，但GPU设计仍未被充分探索，需要填补这一空白。

Method: 在GPU上探索三个维度的设计空间：向量化、线程协作和计算延迟优化，特别关注过滤器是否适合GPU缓存域。

Result: 优化设计克服了速度与精度之间的传统权衡，在保持高精度配置的同时实现通常仅限于高错误率变体的吞吐量。在相同错误率下，批量过滤器查找性能提升11.35倍，构建性能提升15.4倍，在B200 GPU上达到实际速度极限的92%以上。

Conclusion: GPU上的Bloom过滤器优化设计通过系统探索向量化、线程协作和计算延迟等维度，实现了显著的性能提升，超越了传统速度-精度权衡，提供了模块化的CUDA/C++实现。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [93] [LeaseGuard: Raft Leases Done Right](https://arxiv.org/abs/2512.15659)
*A. Jesse Jiryu Davis,Murat Demirbas,Lingzhi Deng*

Main category: cs.DC

TL;DR: LeaseGuard是一个基于Raft的新型租约算法，通过利用Raft选举的特定保证来提供一致读取，减少网络往返并提高吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有Raft系统在保证读取一致性时面临两难：要么接受高通信开销的安全检查，要么实现模糊指定且损害可用性的租约协议。大多数Raft系统要么错误实现租约，要么根本不实现

Method: LeaseGuard利用Raft选举的特定保证，在TLA+中严格规范，包含两个新颖优化：快速恢复写入吞吐量和提高读取可用性。通过Python模拟和LogCabin实现进行评估

Result: LeaseGuard将一致读取的网络开销从1次往返减少到0次，写入吞吐量从~1000提升到~10,000次/秒。新领导者允许99%的读取立即成功，而传统租约在此期间禁止所有读取

Conclusion: LeaseGuard提供了一个简单、严格规范的租约算法，显著提高了Raft系统的读取一致性和整体性能，解决了现有租约协议的可用性问题

Abstract: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.

</details>


### [94] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX系统通过动态重组批次解决早期退出LLM的批处理问题，在保持输出质量的同时提升吞吐量2-12%，完全消除非自愿退出。


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合早期退出LLM架构，因为批次中不同请求的退出时间不同。现有解决方案要么强制统一决策错过退出机会，要么强制提前退出降低输出质量。

Method: 提出动态重组批次方法：在早期退出点动态重组批次，满足退出条件的请求立即处理，继续的请求暂存缓冲区并重新分组为新批次转发到更深层。实现DREX系统，包含无复制重组缓冲区和EE/SLA感知调度器两个关键优化。

Result: DREX相比基线方法提升吞吐量2-12%，同时保持输出质量。关键是完全消除了非自愿退出，为保持EE模型预期的输出质量提供了重要保证。

Conclusion: DREX通过动态重组批次有效解决了早期退出LLM的批处理挑战，在保持模型输出质量的同时显著提升推理效率，为EE LLM的实际部署提供了实用解决方案。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>
