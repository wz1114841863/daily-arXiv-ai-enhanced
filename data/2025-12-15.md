<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 14]
- [cs.LG](#cs.LG) [Total: 60]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 提出一种考虑CPU、磁盘、I/O等多种资源类型的任务调度算法，通过提高资源利用率来降低云服务能耗


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，现有能源有限且对环境有温室效应，需要开发节能算法。现有研究要么最大化平均资源利用率，要么最小化完成时间，但未考虑物理机中不同类型的资源。

Method: 提出基于适应度值的任务调度算法，适应度值是CPU、磁盘、I/O利用率和任务处理时间的函数，通过提高多种资源的显式利用率来增加活动资源利用率。

Result: 通过合成数据集对提出的算法和现有MaxUtil算法进行仿真，结果显示提出的算法是更好的节能算法，比MaxUtil算法消耗更少能量。

Conclusion: 提出的任务调度算法通过显式考虑多种资源类型，有效提高了资源利用率，从而实现了更好的能源效率。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [2] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，专注于新兴加速器平台，优先考虑覆盖范围而非性能。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端支持，解决传统方法只关注有限高性能内核而缺乏全面覆盖的问题。

Method: 集成开源大语言模型、自定义linter、JIT编译和基于PyTorch OpInfo的测试框架，兼容真实MTIA芯片和硬件模拟环境。

Result: 成功为481个独特的ATen算子生成通过所有对应PyTorch OpInfo测试的内核和包装器（总计超过20,000个测试）。

Conclusion: TritorX能够实现为新加速器平台在一夜之间生成完整的PyTorch ATen后端，为加速器支持提供了可扩展的解决方案。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [3] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 这篇论文对实时迁移技术进行了全面综述，重点关注容器和虚拟机迁移，分析了技术现状、采用差异、实际挑战，并为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有综述往往忽略了实时迁移技术在实际应用中的关键技术细节和实践挑战。本文旨在填补这一空白，通过多维度综合分析，为实际部署提供指导。

Method: 整合现有综述内容，从迁移技术、迁移单元和基础设施特性等多个维度对实时迁移技术进行全面分析，重点关注容器和虚拟机两种迁移方法。

Result: 揭示了容器和虚拟机迁移技术之间的采用差异，分析了迁移目标和操作约束对现有技术可用性和效能的影响，识别了当前技术挑战。

Conclusion: 实时迁移虽然技术成熟，但由于依赖多个系统因素，在某些情况下复杂性和资源需求可能超过其收益。本文为爱好者提供了宝贵资源，并为未来技术发展和实际部署指明了方向。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [4] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 该论文针对GPU集群利用率低的问题，提出了三种动态调度器（HPS、PBS、SBS），在64-GPU集群模拟中显著提升了利用率、吞吐量并减少了饥饿问题。


<details>
  <summary>Details</summary>
Motivation: GPU集群在实际部署中平均利用率仅约50%，主要原因是资源碎片化、异构工作负载和静态调度策略的限制，需要更高效的调度方案来提升集群效率。

Method: 提出了三种专门的动态调度器：混合优先级调度器（HPS）、预测性回填调度器（PBS）和智能批处理调度器（SBS），并在包含1000个AI作业的64-GPU集群模拟环境中进行评估。

Result: 动态调度器显著优于静态基线：HPS达到最高利用率78.2%和最高吞吐量25.8作业/小时，将饥饿作业从156个减少到12个；PBS和SBS分别达到76.1%和74.6%的利用率。

Conclusion: 动态多目标调度器在吞吐量、等待时间、公平性和饥饿问题等关键指标上均优于单目标启发式方法，为目标明确的透明调度策略提供了实用基础，能有效提升异构AI集群的GPU效率。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [5] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 本文比较了集中式分层联邦学习(HFL)与去中心化聚合联邦学习(AFL)和去中心化持续联邦学习(CFL)，发现AFL和CFL在多个指标上优于HFL。


<details>
  <summary>Details</summary>
Motivation: 集中式分层联邦学习(HFL)存在通信瓶颈和隐私问题，而去中心化的AFL和CFL方法通过分布式计算和聚合提供了有前景的替代方案。

Method: 对HFL、AFL和CFL架构进行综合比较，使用Fashion MNIST和MNIST数据集进行评估，分析精度、召回率、F1分数和平衡准确率等指标。

Result: AFL和CFL在精度、召回率、F1分数和平衡准确率方面优于HFL，展示了去中心化方法的优势。

Conclusion: 去中心化聚合机制在AFL和CFL中表现更优，为联邦学习的发展提供了有价值的见解，指导研究者和实践者采用去中心化方法以获得更好的协作模型训练性能。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [6] [Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI](https://arxiv.org/abs/2512.10990)
*Jianli Jin,Ziyang Lin,Qianli Dong,Yi Chen,Jayanth Srinivasa,Myungjin Lee,Zhaowei Tan,Fan Lai*

Main category: cs.DC

TL;DR: Dora是一个面向边缘AI训练和推理的QoE感知混合并行框架，通过异构感知模型分区、竞争感知网络调度和运行时适配器，在满足用户体验质量要求的同时提升执行效率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI应用的普及，满足用户体验质量（如推理延迟）成为首要目标，但现代AI模型常超出单个设备资源容量，需要在异构设备和可变网络上分布式执行。现有混合并行规划器主要优化吞吐量或设备利用率，忽视了QoE，导致资源效率低下或QoE违规。

Method: Dora通过三个关键机制实现：1）异构感知模型分区器，确定并分配模型分区到设备，形成紧凑的QoE合规计划集；2）竞争感知网络调度器，通过最大化计算通信重叠进一步优化候选计划；3）运行时适配器，自适应组合多个计划以最大化全局效率同时尊重总体QoE。

Result: 在代表性边缘部署场景（智能家居、交通分析、小型边缘集群）中，Dora实现了1.1-6.3倍的执行加速，或降低21-82%的能耗，同时在运行时动态变化下保持QoE。

Conclusion: Dora框架成功解决了边缘AI分布式执行中的QoE感知混合并行问题，通过联合优化异构计算、网络竞争和多维QoE目标，在资源受限环境中实现了效率与用户体验的平衡。

Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.

</details>


### [7] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 论文提出三种GPU原生编译方法消除CPU-GPU数据传输瓶颈，理论分析显示可实现10-100倍代码迭代加速


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，限制了代码迭代效率

Method: 提出三种互补的GPU原生编译方法：1) 并行传统编译适配GPU执行；2) 神经编译使用学习型序列到序列翻译与概率验证；3) 混合架构结合两种策略

Result: 理论分析显示：传统GPU编译通过消除传输提供2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有正确性保证的实用部署路径

Conclusion: GPU原生编译能显著加速代码迭代周期，概率验证框架允许在编译准确性和并行探索之间权衡，对自改进AI系统和未来模拟计算基板有重要影响

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [8] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个用于强化学习后训练中rollout-training解耦架构的集群调度框架，通过跨集群编排回收依赖气泡，将成本效率提升1.84倍。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练中，rollout（内存密集型）和training（计算密集型）通常解耦到专用集群以提高硬件效率，但on-policy算法的严格同步要求导致严重的依赖气泡，使得一个集群在另一个集群运行时闲置，造成资源浪费。

Method: 提出RollMux框架，基于"一个作业的结构性闲置可以被另一个作业的活动阶段利用"的洞察。引入协同执行组抽象，将集群划分为隔离的局部性域，实现两层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮询调度。组抽象还施加驻留约束，确保大模型状态保持在主机内存中，实现"热启动"上下文切换。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上评估，RollMux相比标准解耦架构将成本效率提升1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的服务水平目标达成率。

Conclusion: RollMux通过跨集群编排有效回收rollout-training解耦架构中的依赖气泡，显著提升硬件利用率和成本效率，为强化学习后训练提供高效的集群调度解决方案。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [9] [Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512)
*Patrick D. Manya,Eugene M. Mbuyi,Gothy T. Ngoie,Jordan F. Masakuna*

Main category: cs.DC

TL;DR: 提出一种基于多包消息的分布式剪枝增强方法，显著降低接近中心性计算中的通信开销


<details>
  <summary>Details</summary>
Motivation: 大规模复杂网络中分布式计算接近中心性面临高通信开销挑战，现有分布式剪枝方法在大型网络设置中难以有效减少数据包交换成本

Method: 引入多包消息技术，允许节点批量传输更大的整合数据块，减少消息交换数量并最小化数据丢失，同时保持中心性估计的准确性

Result: 多包方法在消息效率（更少总消息数）和计算时间上显著优于原始剪枝技术，保持了基线方法的近似特性，在通信效率方面获得显著提升

Conclusion: 该方法为分布式接近中心性计算提供了更可扩展和高效的解决方案，对大规模网络分析具有重要意义，尽管存在节点内存使用和本地开销增加的可管理权衡

Abstract: Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.

</details>


### [10] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax是一个移动端DNN推理框架，通过计算图分区、分支感知内存管理和自适应调度，在不修改模型的情况下加速动态控制流模型的推理，在移动设备上实现高达46%的延迟降低和30%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上实时DNN应用需求增长，但复杂模型中的动态控制流操作符和不支持的核函数通常回退到CPU执行。现有框架处理这些回退效果差，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1) 对计算DAG进行分区以暴露并行性；2) 采用分支感知内存管理，使用专用内存池和缓冲区重用减少运行时内存占用；3) 自适应调度器根据设备内存约束执行分支；4) 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN模型，Parallax实现了高达46%的延迟降低，平均内存开销控制在26.5%，相比最先进框架节省高达30%的能耗，满足实时移动推理的响应性需求。

Conclusion: Parallax框架无需模型重构或自定义操作符实现，通过有效的并行化、内存管理和调度策略，显著提升了移动设备上动态控制流DNN模型的推理性能，为实时移动应用提供了可行的解决方案。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [11] [FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634)
*Elia Palme,Juan Pablo Dorsch,Ali Khosravi,Giovanni Pizzi,Francesco Pagnamenta,Andrea Ceriani,Eirini Koutsaniti,Rafael Sarmiento,Ivano Bonesana,Alejandro Dabin*

Main category: cs.DC

TL;DR: FirecREST v2 是新一代用于HPC资源程序化访问的RESTful API，相比前代性能提升100倍，重点改进了安全性和高吞吐量


<details>
  <summary>Details</summary>
Motivation: 重新设计FirecEST以解决代理式API在密集I/O操作中的性能瓶颈，集成增强的安全性和高吞吐量作为核心需求

Method: 采用系统化的性能测试方法，识别常见瓶颈，并进行关键的设计和架构变更，包括从零开始重新设计

Result: 实现了100倍的性能提升，获得了独立的同行验证，显著改善了HPC资源的程序化访问效率

Conclusion: FirecREST v2的成功重新设计展示了解决代理式API性能瓶颈的有效方法，为未来进一步改进提供了机会

Abstract: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.
  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.

</details>


### [12] [Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643)
*Manideep Reddy Chinthareddy*

Main category: cs.DC

TL;DR: 提出一种无需显式工作节点ID的分布式ID生成协议，利用容器私有IPv4地址作为网络派生熵源，实现无中心协调的容器原生ID生成方案。


<details>
  <summary>Details</summary>
Motivation: 传统Snowflake风格ID生成器需要手动分配或中心协调工作节点ID，这在容器编排环境（如Kubernetes）中带来显著摩擦，因为工作负载是短暂且自动伸缩的。维护稳定工作节点身份需要复杂的有状态集或外部协调服务，违背了无状态微服务的操作优势。

Method: 提出云无关、容器原生的ID生成协议，通过从容器的私有IPv4地址确定性派生节点唯一性，消除对显式工作节点ID的依赖。引入修改的位分配方案（1-41-16-6），容纳16位网络派生熵同时保持严格单调性。

Result: 在AWS、GCP和Azure环境中验证了该方法。评估结果显示，虽然设计有约64,000 TPS的理论单节点上限，但在实际微服务部署中网络I/O主导延迟，3节点集群上端到端性能约31,000 TPS，与经典有状态生成器相当，同时提供有效无限制的水平可扩展性。

Conclusion: 提出的协议消除了分布式ID生成中对显式工作节点ID的依赖，通过利用网络派生熵实现容器原生、无中心协调的解决方案，在保持性能的同时提供更好的可扩展性和操作简单性。

Abstract: Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.
  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.

</details>


### [13] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低了连续学习的计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 当前为每个摄像头单独重新训练模型的实践存在高计算和通信成本，无法扩展。数据漂移在相邻摄像头间常具有时空相关性，这为优化提供了机会。

Method: ECCO包含三个核心组件：1) 轻量级分组算法动态形成和更新摄像头组；2) GPU分配器动态分配GPU资源以提高重训练精度和确保公平性；3) 每个摄像头的传输控制器配置帧采样并基于分配的GPU资源协调带宽共享。

Result: 在三个不同数据集上的评估显示，ECCO在相同计算和通信资源下将重训练精度提高了6.7%-18.1%，或在相同精度下支持3.3倍更多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，实现了资源高效的连续学习，显著提升了视频分析系统的可扩展性和效率。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


### [14] [Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.DC

TL;DR: H-MPCs是一种基于超图的多方支付通道，通过集体资助的超边替代双边通道，实现无领导者的高并发链下支付，解决流动性碎片化和通道耗尽问题。


<details>
  <summary>Details</summary>
Motivation: 公共区块链吞吐量低、延迟高，现有支付通道网络存在流动性碎片化（资金锁定在单一通道无法复用）和通道耗尽问题，限制了路由效率和交易成功率。现有的多方通道方案依赖领导者或协调者，存在单点故障且跨通道支付灵活性有限。

Method: 提出基于超图的多方支付通道（H-MPCs），用集体资助的超边替代传统双边通道。通过可验证的提议者排序DAG更新，实现完全并发的超边内和超边间支付，无需领导者协调。

Result: 在150个节点的网络上实现，交易成功率约94%，没有HTLC过期或路由失败，显示了H-MPCs的鲁棒性。

Conclusion: H-MPCs提供比现有设计显著更高的灵活性和并发性，解决了支付通道网络的流动性碎片化和通道耗尽问题，同时避免了单点故障。

Abstract: Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.
  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.
  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering](https://arxiv.org/abs/2512.10962)
*Yifei He,Pranit Chawla,Yaser Souri,Subhojit Som,Xia Song*

Main category: cs.LG

TL;DR: 提出一个可扩展的数据合成流水线，通过步骤级过滤将嘈杂的计算机使用代理轨迹转化为可靠监督数据，无需人工标注，构建了WebSTAR数据集和WebSCORE数据集，并训练了轻量级奖励模型StepRM。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理的训练面临GUI交互成本高和高质量轨迹数据稀缺的挑战。现有数据集依赖人工演示，可扩展性有限。虽然可以从强代理合成数据，但其轨迹噪声大，包含大量错误或次优动作，使简单模仿无效。

Method: 1. 步骤级过滤：单独评估每个动作，仅保留正确步骤；2. 推理增强：改进规划能力；3. 构建WebSTAR数据集（13.3K轨迹，100K分级推理丰富的步骤）；4. 构建WebSCORE数据集（分级步骤级动作）；5. 训练StepRM奖励模型（从o4-mini蒸馏的7B多模态奖励模型）。

Result: 1. 在WebSTAR上训练的Qwen-2.5-VL-Instruct模型（7B和32B）表现优异；2. 7B模型在WebVoyager上超越SoTA开源CUA模型UI-TARS-1.5-7B超过15%；3. StepRM奖励模型匹配o4-mini的分级质量，同时部署效率更高。

Conclusion: 步骤级过滤是计算机使用代理可扩展训练的关键原则，构建的WebSTAR、WebSCORE数据集和StepRM奖励模型为推进稳健高效的计算机使用代理提供了实用工具。

Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.

</details>


### [16] [Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2512.10966)
*Farica Zhuang,Dinara Aliyeva,Shu Yang,Zixuan Wen,Duy Duong-Tran,Christos Davatzikos,Tianlong Chen,Song Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出MREF-AD模型，一种用于阿尔茨海默病诊断的多模态区域专家融合模型，通过Mixture-of-Experts框架自适应融合淀粉样蛋白PET和MRI特征，提供区域级可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态融合方法通常采用简单的特征拼接，无法自适应平衡不同脑区生物标志物（如淀粉样蛋白PET和MRI）的贡献，缺乏可解释性。

Method: 提出MREF-AD模型，采用Mixture-of-Experts框架，将每个模态的脑区建模为独立专家，使用两级门控网络学习受试者特定的融合权重。

Result: 在ADNI数据上实现了最先进的诊断性能，同时提供了模态和区域级别的可解释性，揭示了结构和分子成像如何共同贡献于疾病诊断。

Conclusion: MREF-AD作为一个通用框架，为神经影像中的自适应和可解释多模态融合提供了实用工具，既能提升诊断性能又能增强生物标志物相关性的可解释性。

Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.

</details>


### [17] [MoB: Mixture of Bidders](https://arxiv.org/abs/2512.10969)
*Dev Vyas*

Main category: cs.LG

TL;DR: MoB用VCG拍卖机制替代MoE中的学习门控网络，通过专家竞标真实成本（执行成本+遗忘成本）实现无状态路由，解决了MoE在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构在扩展神经网络方面表现出色，但其学习门控网络本身存在灾难性遗忘问题，限制了在持续学习中的应用。

Method: 提出Mixture of Bidders框架，用VCG拍卖替代学习门控网络，专家通过竞标真实成本（预测损失+弹性权重巩固惩罚）来竞争处理数据批次。

Result: 在Split-MNIST基准测试中，MoB达到88.77%平均准确率，相比Gated MoE的19.54%和Monolithic EWC的27.96%，提升了4.5倍。

Conclusion: MoB通过经济机制解决了MoE的灾难性遗忘问题，实现了无状态路由、真实竞标保证和无需明确任务边界的涌现专业化。

Abstract: Mixture of Experts (MoE) architectures have demonstrated remarkable success in scaling neural networks, yet their application to continual learning remains fundamentally limited by a critical vulnerability: the learned gating network itself suffers from catastrophic forgetting. We introduce Mixture of Bidders (MoB), a novel framework that reconceptualizes expert routing as a decentralized economic mechanism. MoB replaces learned gating networks with Vickrey-Clarke-Groves (VCG) auctions, where experts compete for each data batch by bidding their true cost -- a principled combination of execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This game-theoretic approach provides three key advantages: (1) {stateless routing that is immune to catastrophic forgetting, (2) \textbf{truthful bidding} guaranteed by dominant-strategy incentive compatibility, and (3) emergent specialization without explicit task boundaries. On Split-MNIST benchmarks, MoB achieves 88.77% average accuracy compared to 19.54% for Gated MoE and 27.96% for Monolithic EWC, representing a 4.5 times improvement over the strongest baseline. We further extend MoB with autonomous self-monitoring experts that detect their own knowledge consolidation boundaries, eliminating the need for explicit task demarcation.

</details>


### [18] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 提出基于强化学习的个性化肝素治疗优化框架，使用连续cxSOFA评分和TECM评估矩阵，在脓毒症术后患者中显著降低死亡率并缩短住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是危及生命的严重感染状态，传统离散SOFA评分无法精确评估治疗策略。需要数据驱动的连续指标和强化学习框架来优化肝素治疗的个性化决策。

Method: 使用MIMIC-IV和eICU数据库数据，将离散SOFA转换为连续cxSOFA评分，基于此定义治疗策略优劣，提出治疗效应比较矩阵(TECM)评估策略，应用多种RL算法(Q-Learning、DQN、DDQN、BCQ、CQL)优化治疗。

Result: cxSOFA-CQL模型表现最佳，将死亡率从1.83%降至0.74%，平均住院时间从11.11天缩短至9.42天。TECM在不同模型中显示一致结果，证明框架稳健性。

Conclusion: 提出的强化学习框架能够可解释且稳健地优化脓毒症手术患者的肝素治疗，连续cxSOFA评分和TECM评估为治疗提供了细致评估，有望改善临床结果和决策支持可靠性。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [19] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体框架的多模态情感识别系统，通过模块化设计和中央协调器实现高效训练和灵活扩展


<details>
  <summary>Details</summary>
Motivation: 传统多模态深度学习模型虽然情感识别准确率高，但训练和维护计算成本高，且难以灵活应对模态变化。需要设计更灵活、可扩展、易维护的情感感知模块来支持人机交互

Method: 采用多智能体框架，每个模态编码器和融合分类器作为自主智能体，由中央监督器协调。支持视觉、音频和文本模态，分类器作为共享决策智能体。支持新模态的模块化集成和旧组件的无缝替换

Result: 通过概念验证实现证明了方法的可行性，支持视觉、音频和文本模态。框架不仅提高了训练效率，还实现了更灵活、可扩展和易维护的情感感知模块

Conclusion: 提出的多智能体框架为多模态情感识别系统提供了更高效、灵活和可维护的解决方案，有助于改进人机交互中具身和虚拟智能体的感知模块设计

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [20] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt是一个新颖的3D分子生成框架，通过从冻结的1D分子基础模型中提取化学知识，并注入到3D扩散模型中，实现从化学语法到3D几何的"雕刻"。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。需要弥合这一差距。

Method: MolSculpt基于冻结的1D分子基础模型和3D分子扩散模型构建。引入可学习查询从基础模型中提取固有化学知识，通过可训练投影器将这些跨模态信息注入扩散模型的条件空间，通过端到端优化将1D潜在化学知识深度集成到3D生成过程中。

Result: 实验表明，MolSculpt在从头3D分子生成和条件3D分子生成方面达到最先进性能，在GEOM-DRUGS和QM9数据集上显示出优越的3D保真度和稳定性。

Conclusion: MolSculpt成功地将1D化学知识整合到3D分子生成中，通过"雕刻"方法实现了从化学语法到精确3D几何的有效转换，为药物发现和材料科学提供了强大的分子生成工具。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [21] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 提出一种新的策略迭代算法族，用于求解部分可观察马尔可夫决策过程(POMDPs)，通过交替执行单阶段基于输出的策略改进和策略评估，实现单调改进，并识别出最优计算效率模式。


<details>
  <summary>Details</summary>
Motivation: 无记忆和有限记忆策略为求解POMDPs提供了实用替代方案，因为它们直接在输出空间而非高维信念空间中操作。然而，将经典方法如策略迭代扩展到这一设置仍然困难，因为输出过程是非马尔可夫的，使得策略改进步骤在不同阶段相互依赖。

Method: 引入新的单调改进策略迭代算法族，交替执行单阶段基于输出的策略改进和按照预定周期模式进行的策略评估。识别出最大化计算效率指标的最优模式，并找到具有最小周期的最简单模式。基于此结构，进一步开发了从数据估计值并直接学习无记忆策略的无模型变体。

Result: 在多个POMDPs示例中，该方法在基于模型和无模型设置下，相比策略梯度基线和最近的专用算法，实现了显著的计算加速。

Conclusion: 提出的新策略迭代算法族为POMDPs提供了一种有效的求解方法，通过优化计算模式实现了显著的计算效率提升，并在基于模型和无模型设置中都表现出优越性能。

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [22] [Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification](https://arxiv.org/abs/2512.11087)
*Duo Zhou,Jorge Chavez,Hesun Chen,Grani A. Hanasusanto,Huan Zhang*

Main category: cs.LG

TL;DR: 提出线性约束驱动的剪裁框架，通过利用线性约束减少分支定界中的子问题数量并改进中间边界，显著提升神经网络验证器的效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于分支定界（BaB）的神经网络验证器在处理复杂验证属性时，快速边界技术是关键。然而，现有方法在有效利用线性约束来减少搜索空间和提升验证效率方面仍有改进空间。

Method: 提出线性约束驱动的剪裁框架，开发两种新算法：1) 利用线性约束减少已验证或与子问题无关的输入空间；2) 直接改进网络中的中间边界。该方法专门设计了GPU加速程序处理线性约束，无需昂贵的外部求解器，可集成到α,β-CROWN等BaB验证器中。

Result: Clip-and-Verify在多个基准测试中持续收紧边界，某些情况下将分支定界中的子问题数量减少96%，在多个基准测试中达到最先进的验证准确率，并成为VNN-COMP 2025获胜验证器α,β-CROWN的一部分。

Conclusion: 线性约束驱动的剪裁框架为神经网络验证提供了可扩展且高效的方法，通过有效利用线性约束显著提升验证效率，在减少计算开销的同时保持高验证准确率。

Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $α,β$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $α,β$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.

</details>


### [23] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 该论文首次系统研究了部分标签学习（PLL）方法在心电图（ECG）诊断中的应用，评估了九种PLL算法在不同临床相关标签模糊性场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 心电图诊断中存在固有的标签模糊性问题（如重叠病症和诊断分歧），但现有ECG模型都假设标签是干净无歧义的，这限制了模型在真实临床环境中的发展和评估。虽然部分标签学习框架旨在处理模糊标签，但其在医疗时间序列领域（特别是ECG）的有效性尚未得到充分探索。

Method: 将九种PLL算法适配到多标签ECG诊断任务中，使用多种临床驱动的模糊性生成策略进行评估，包括非结构化（如随机）和结构化模糊性（如心脏病专家定义的相似性、治疗关系和诊断分类学）。在PTB-XL和Chapman数据集上进行实验。

Result: 实验表明，不同PLL方法对各种类型和程度的模糊性表现出显著不同的鲁棒性。通过深入分析，揭示了当前PLL方法在临床环境中的关键局限性。

Conclusion: 该研究为ECG诊断中开发鲁棒且临床对齐的模糊性感知学习框架指明了未来方向，强调了处理标签模糊性对真实世界ECG诊断的重要性。

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [24] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 论文系统研究了测试时缩放（TTS）在多模态视觉语言模型中的应用，发现闭源模型能从结构化推理和迭代自优化中受益，而开源模型表现不一致，且TTS效果高度依赖于数据集和任务类型。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）已被证明能有效提升大型语言模型的推理能力，但在多模态系统（如视觉语言模型）中的应用尚未充分探索。本研究旨在填补这一空白，系统评估TTS在不同类型VLMs上的效果。

Method: 对开源和闭源视觉语言模型进行系统实证研究，在不同基准测试上应用推理时间推理方法，包括结构化推理、迭代自优化和外部验证等技术。

Result: 闭源模型能稳定受益于结构化推理和迭代自优化；开源模型表现不一致，外部验证提供最可靠的提升，而迭代优化常导致性能下降；TTS效果高度数据集依赖，在多步推理任务上改善明显，但在感知密集型基准上提升有限。

Conclusion: TTS并非通用解决方案，需要根据模型能力和任务特性进行定制。这推动了未来自适应TTS策略和多模态奖励模型的研究方向。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [25] [In-Context Multi-Objective Optimization](https://arxiv.org/abs/2512.11114)
*Xinyu Zhang,Conor Hassan,Julien Martinelli,Daolang Huang,Samuel Kaski*

Main category: cs.LG

TL;DR: TAMO是一个基于Transformer的完全摊销化多目标黑盒优化策略，通过预训练实现跨问题通用，无需针对每个任务重新训练代理模型或设计采集函数，大幅提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化需要为每个问题定制代理模型和采集函数，缺乏通用性；同时存在短视性（缺乏多步规划）和计算开销大的问题，特别是在并行或时间敏感的场景中。

Method: 使用Transformer架构构建通用优化策略，支持可变输入和目标维度；通过强化学习预训练策略，最大化累积超体积改进；在测试时，预训练模型通过单次前向传播直接提出新设计，无需重新训练。

Result: 在合成基准和实际任务中，TAMO的提案速度比替代方法快50-1000倍，同时在有限评估预算下匹配或改进了帕累托前沿质量。

Conclusion: Transformer能够在上下文中完全执行多目标优化，消除了针对每个任务的代理模型拟合和采集函数工程，为科学发现工作流程开辟了基础模型式即插即用优化器的新路径。

Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.

</details>


### [26] [Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee](https://arxiv.org/abs/2512.11127)
*Kshitiz Khanal*

Main category: cs.LG

TL;DR: 提出结合物理信息图神经网络与连续流匹配的两阶段学习框架，用于快速求解直流最优潮流问题，在保持100%可行性的同时达到接近最优解。


<details>
  <summary>Details</summary>
Motivation: 传统优化求解器计算成本高，难以满足大规模电力系统实时调度需求；现有机器学习方法在约束满足和成本最优性方面存在不足。需要开发既能快速求解又能保证约束满足和接近最优性的方法。

Method: 两阶段学习框架：第一阶段使用物理信息图神经网络生成可行初始解，通过编码电力系统约束的物理损失函数训练；第二阶段采用连续流匹配技术，通过学习的向量场回归将初始解优化到接近最优。

Result: 在IEEE 30节点系统的5个负荷场景（70%-130%额定负荷）测试中，方法在额定负荷下成本差距低于0.1%，极端条件下低于3%，同时保持100%可行性。

Conclusion: 该框架在快速神经网络预测与精确但缓慢的数值求解器之间架起桥梁，为高可再生能源渗透率需要频繁调度更新的现代电力系统提供实用解决方案。

Abstract: The DC Optimal Power Flow (DC-OPF) problem is fundamental to power system operations, requiring rapid solutions for real-time grid management. While traditional optimization solvers provide optimal solutions, their computational cost becomes prohibitive for large-scale systems requiring frequent recalculations. Machine learning approaches offer promise for acceleration but often struggle with constraint satisfaction and cost optimality. We present a novel two-stage learning framework that combines physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM) for solving DC-OPF problems. Our approach embeds fundamental physical principles--including economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker (KKT) complementarity conditions--directly into the training objectives. The first stage trains a GNN to produce feasible initial solutions by learning from physics-informed losses that encode power system constraints. The second stage employs CFM, a simulation-free continuous normalizing flow technique, to refine these solutions toward optimality through learned vector field regression. Evaluated on the IEEE 30-bus system across five load scenarios ranging from 70\% to 130\% nominal load, our method achieves near-optimal solutions with cost gaps below 0.1\% for nominal loads and below 3\% for extreme conditions, while maintaining 100\% feasibility. Our framework bridges the gap between fast but approximate neural network predictions and optimal but slow numerical solvers, offering a practical solution for modern power systems with high renewable penetration requiring frequent dispatch updates.

</details>


### [27] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 本文研究公平性正则化的平滑在线凸优化问题，提出FairOBD算法，在最小化命中成本、切换成本和公平性成本之间取得平衡，并证明其渐近竞争比。


<details>
  <summary>Details</summary>
Motivation: 在线优化问题中，公平性和动作平滑性是两个关键考虑因素，但现有研究尚未同时解决这两个问题。本文旨在研究公平性正则化的平滑在线凸优化问题，特别关注具有切换成本的情况。

Method: 提出FairOBD算法，通过引入辅助变量将长期公平性成本分解为一系列在线成本，然后利用该辅助变量正则化在线动作以实现公平结果。采用新方法处理切换成本。

Result: 理论证明：即使没有切换成本，任何在线算法都无法实现相对于离线最优算法的次线性遗憾或有限竞争比。FairOBD算法在考虑T→∞时，相对于参数化约束的最优离线算法提供了最坏情况渐近竞争比。

Conclusion: FairOBD算法能有效减少总公平性正则化成本，相比现有基线解决方案能更好地促进公平结果。动态计算资源配置的实验验证了算法的有效性。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [28] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Vekua Layer (VL)方法，基于广义解析函数理论，将隐式神经表示学习转化为凸最小二乘问题，在椭圆PDE上实现机器精度重建，并支持解析延拓。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示(INRs)存在频谱偏差和非凸优化计算昂贵的问题，需要更高效且物理信息嵌入的表示方法。

Method: 基于广义解析函数理论，将假设空间限制在控制微分算子的核空间中，使用调和基和傅里叶-贝塞尔基，将学习任务转化为严格凸的最小二乘问题，通过线性投影求解。

Result: 在齐次椭圆PDE上，VL实现机器精度(MSE≈10^-33)的精确重建，对非相干传感器噪声表现出优越稳定性(MSE≈0.03)，并能从部分边界数据通过解析延拓重建全局场。

Conclusion: VL提供了一种基于物理的谱方法，克服了传统INRs的频谱偏差和优化困难，实现了高精度、稳定的物理场表示，并支持解析延拓能力。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [29] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 提出一种结合聚类损失和对比损失的增强自编码器框架，用于高维科学集合数据的可视化和特征提取，在土壤通道结构和液滴冲击动力学数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 高维复杂的科学集合数据在分析和可视化方面面临挑战，传统降维技术和自编码器难以有效处理此类数据，需要改进方法来提升特征提取和可视化效果。

Method: 使用EfficientNetV2为无标签数据生成伪标签，构建增强自编码器框架，联合优化重构损失、基于软轮廓分数的聚类损失和对比损失，最后用UMAP生成2D投影。

Result: 在土壤通道结构（MCMC生成）和液滴冲击动力学两个科学集合数据集上，结合聚类或对比损失的模型在轮廓分数评估中略优于基线方法。

Conclusion: 提出的增强自编码器框架能有效改善高维科学集合数据的可视化和可解释性，聚类和对比损失的结合有助于在潜在空间中形成更清晰的数据结构。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [30] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异质数据管道，用于融合城市多源时空数据，解决跨领域城市问题，具有良好泛化性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 城市数据通常以异质格式存在，由不同机构独立收集，存在标准和目标差异。国家级数据集虽然丰富但具有显著异质性和多模态性，需要统一框架来整合这些数据以支持城市决策。

Method: 提出异质数据管道，执行跨领域数据融合，处理时变、空间变化和时空序列数据集。数据学习模块将空间变化数据集的同质性整合到图学习中，将不同地区的信息嵌入模型。使用超过50个数据源。

Result: 通过五个真实世界观察（使用共享出行、交通事故、犯罪报告等公开数据集）验证框架的泛化性和灵活性。结果显示该框架具有强大的预测性能，在转移到新地区或领域时只需最小重新配置。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，为跨领域城市问题提供了有效的解决方案。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [31] [Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning](https://arxiv.org/abs/2512.11179)
*Wei Duan,Jie Lu,En Yu,Junyu Xuan*

Main category: cs.LG

TL;DR: 提出BVME方法解决带宽受限下多智能体强化学习的消息编码问题，通过变分框架实现可控压缩，在多个基准测试中显著减少消息维度同时保持或提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于图的多智能体强化学习方法虽然能学习稀疏协调图（确定谁与谁通信），但未解决在硬带宽约束下应该传输什么信息的问题。简单的降维方法会持续降低协调性能，而确定性投影缺乏控制压缩方式的机制。

Method: 提出带宽受限变分消息编码（BVME），这是一个轻量级模块，将消息视为从学习到的高斯后验中采样的样本，通过KL散度正则化到无信息先验。变分框架通过可解释的超参数提供原则性的、可调节的压缩强度控制，直接约束用于决策的表征。

Result: 在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的情况下，实现了相当或更优的性能。在稀疏图上增益最为明显，因为消息质量对协调影响关键。消融实验显示对带宽的U形敏感性，BVME在极端压缩比下表现优异，同时增加的开销最小。

Conclusion: BVME为带宽受限的多智能体协调提供了一种有效的变分消息编码方法，通过可控压缩机制在保持协调性能的同时显著减少通信开销，特别适用于稀疏图场景。

Abstract: Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.

</details>


### [32] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 论文提出"进展导向型基准测试"新范式，以NanoGPT速度挑战为例，将基准测试从静态问题排行榜转变为可测量的开放式科学研究工具。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要关注静态、已解决的问题（如数学应用题），这种方法限制了可衡量和激励的进步类型。需要一种能直接推动科学进步的基准测试范式。

Method: 提出进展导向型基准测试框架，以NanoGPT速度挑战为实例化环境，标准化数据集切片、参考模型、训练工具和丰富遥测数据，包含运行时验证和防作弊检查。

Result: 在该环境中实现了新的最先进训练时间，比之前记录快3秒，并定性观察到新颖算法思想的出现。基准测试能催化语言建模堆栈的可重用改进。

Conclusion: 基准测试应成为科学进步的工具，而非仅仅是模型比较的手段。该工作旨在推动社区从静态问题排行榜转向可测量的开放式科学研究范式。

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [33] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: 本文分析了ReLU激活函数在物理信息机器学习中表现不佳的原因，发现即使在一阶微分方程中ReLU也会失败，原因是自动微分无法正确处理不连续场的导数，导致损失函数梯度计算错误。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习使用控制微分方程训练神经网络表示解场。已有研究表明ReLU激活函数在基准微分方程上表现不如sigmoid、tanh和swish等激活函数，但具体原因尚不清楚，特别是ReLU为何在一阶微分方程中也会失败。

Method: 通过诊断ReLU在物理信息机器学习问题中的失败原因，分析自动微分（PyTorch）在处理不连续场导数时的局限性，揭示损失函数梯度计算错误的具体机制。

Result: 研究发现ReLU失败的根本原因在于其二阶导数问题，这些导数不是在损失函数公式中直接使用，而是在训练过程中通过自动微分计算。自动微分无法正确表征不连续场的导数，导致物理信息损失函数的梯度被错误指定。

Conclusion: ReLU在物理信息机器学习中表现不佳的原因是自动微分技术无法正确处理其不连续性导致的导数计算错误，这解释了为什么即使在一阶微分方程中ReLU也会失败，为选择合适的激活函数提供了理论依据。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [34] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 本文提出了一种梯度投影框架，用于在扩散模型训练中实现概念级别的选择性遗忘，防止模型记忆敏感特征，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型中的记忆化带来了安全和知识产权风险，现有方法只能防止对特定训练样本的过拟合，无法系统性地阻止概念级别敏感特征的内化，而简单丢弃包含敏感特征的图像会浪费宝贵的训练数据。

Method: 提出梯度投影框架，在反向传播过程中识别并消除与禁止属性嵌入对齐的训练信号，将每个梯度更新投影到敏感特征嵌入空间的正交补空间上，从而消除其对模型权重的影响。

Result: 实验表明，该框架能显著减少记忆化，同时严格保持生成质量和语义保真度，有效防御特征提取攻击。

Conclusion: 通过将记忆控制重新定义为选择性学习，该方法为知识产权安全和隐私保护的生成式AI建立了新范式。

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [35] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可实现每轮常数时间，提出更实用的算法，分析遗憾界与时间复杂度的权衡


<details>
  <summary>Details</summary>
Motivation: EXP3算法在多臂赌博机问题中具有理论保证，但实际实现可能效率不高。需要探索如何在保持良好遗憾界的同时，实现更高效的时间复杂度。

Method: 指出EXP3算法可实现每轮常数时间，提出更实用的算法变体，分析不同算法在遗憾界和时间复杂度之间的权衡关系

Result: 展示了EXP3算法可以实现每轮常数时间复杂度，提出了更实用的算法版本，并建立了遗憾界与计算效率之间的明确权衡关系

Conclusion: 通过算法优化，可以在保持良好理论性能的同时显著提高计算效率，为实际应用提供了更实用的多臂赌博机算法

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [36] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文研究了在存在选择偏差的情况下，如何利用协方差子矩阵的秩约束来进行潜变量因果发现，证明了即使在选择偏差下，秩约束仍能保留因果结构和选择机制的信息。


<details>
  <summary>Details</summary>
Motivation: 解决潜变量因果发现中的选择偏差问题很重要但研究不足，主要是因为缺乏合适的统计工具。现有的处理潜变量的工具都没有针对选择偏差进行适配。

Method: 研究秩约束作为条件独立性约束的推广，在线性高斯模型中利用协方差子矩阵的秩。提供了这种秩约束的图论特征化，并证明经典的单因子模型在选择偏差下是可识别的。

Result: 研究表明，尽管选择偏差会显著复杂化联合分布，但偏差协方差矩阵中的秩仍然保留了关于因果结构和选择机制的有意义信息。模拟和真实世界实验证实了使用秩约束的有效性。

Conclusion: 秩约束为处理选择偏差下的潜变量因果发现提供了有效的统计工具，能够识别因果结构和选择机制，填补了该领域的研究空白。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [37] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态暂停低重要性token的KV更新，减少KV缓存大小55-67%而不影响生成质量


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文生成时KV缓存内存占用过高的问题，为内存受限的部署提供无需训练的实用解决方案

Method: 基于滑动注意力窗口识别低重要性token，采用可逆软冻结机制暂停其KV更新，配合次线性冻结调度防止过度压缩，所有token保存在GPU外存储并按需恢复

Result: 在LLaMA-3 8B上实验显示，主动KV缓存大小减少55-67%，同时保持生成质量并通过针在干草堆检索测试

Conclusion: ASR-KF-EGR是一种架构无关、无需微调的推理时框架，为长上下文LLM的内存受限部署提供了实用解决方案

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [38] [Task-Aware Multi-Expert Architecture For Lifelong Deep Learning](https://arxiv.org/abs/2512.11243)
*Jianyu Wang,Jacob Nean-Hua Sheikh,Cat P. Le,Hoda Bidkhori*

Main category: cs.LG

TL;DR: TAME是一种终身深度学习算法，通过任务感知的多专家系统、重放缓冲区和注意力机制，在连续学习任务中平衡适应性和知识保留。


<details>
  <summary>Details</summary>
Motivation: 终身深度学习需要在连续任务中学习新知识的同时保留先前知识，但存在灾难性遗忘问题。需要一种能够根据任务相似性灵活选择专家并有效转移知识的算法。

Method: TAME维护预训练神经网络池，根据任务相似性激活最相关专家；使用共享密集层整合专家特征；采用重放缓冲区存储先前任务的代表性样本和嵌入；通过注意力机制优先处理最相关的存储信息。

Result: 在CIFAR-100衍生的二分类任务上的实验表明，TAME在新任务上提高了准确性，同时保持了先前任务的性能，有效平衡了适应性和知识保留。

Conclusion: TAME通过任务感知的专家选择、知识转移和重放机制，在终身学习场景中实现了有效的适应性和知识保留平衡，为解决灾难性遗忘问题提供了有效方案。

Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.

</details>


### [39] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: Insight Miner是一个用于生成高质量时间序列描述的多模态模型，通过TS-Insights数据集进行指令调优，在时间序列分析任务上超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域都很重要，但从中挖掘洞察通常需要深厚的领域专业知识，这个过程既耗时又费力。需要一种能够自动生成高质量时间序列描述和洞察的方法。

Method: 提出了Insight Miner多模态模型，并创建了TS-Insights数据集（包含100k时间序列窗口）。使用新颖的代理工作流程：先用统计工具从原始时间序列中提取特征，然后用GPT-4将这些特征合成为连贯的趋势描述。在TS-Insights上进行指令调优。

Result: Insight Miner在生成时间序列描述和洞察方面超越了最先进的多模态模型，如LLaVA和GPT-4。这表明了利用LMM进行时间序列分析的有前景方向。

Conclusion: 这项工作为让LLMs将时间序列作为原生输入模态奠定了基础，展示了LMM在时间序列分析中的潜力，是迈向更智能时间序列理解的重要一步。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [40] [A Simple Generalisation of the Implicit Dynamics of In-Context Learning](https://arxiv.org/abs/2512.11255)
*Francesco Innocenti,El Mehdi Achour*

Main category: cs.LG

TL;DR: 该论文扩展了Dherin等人(2025)关于transformer隐式权重更新的理论，将其推广到所有序列位置、任意transformer块，并包含层归一化等更现实的残差块结构。


<details>
  <summary>Details</summary>
Motivation: 先前关于上下文学习(ICL)的理论研究主要基于简化模型和数据设置，而Dherin等人(2025)最近发现transformer块可以隐式更新其前馈网络权重。本研究旨在将这一理论推广到更实际的场景，使其更接近真实的大规模模型。

Method: 提出了Dherin等人理论的简单泛化：(1)扩展到所有序列位置而不仅仅是最后一个位置；(2)适用于任意transformer块而不仅仅是第一个块；(3)包含层归一化等更现实的残差块结构。通过在简单的上下文线性回归任务上进行实证验证，并研究不同token之间和不同块之间隐式更新的关系。

Result: 成功将隐式权重更新理论推广到更广泛的场景，实证验证了理论在简单线性回归任务上的有效性，并分析了不同token和块之间隐式更新的关系。

Conclusion: 该研究使Dherin等人的理论更接近实际应用，为在大规模模型上进行验证奠定了基础，有助于更好地理解transformer在上下文学习中的工作机制。

Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.

</details>


### [41] [Features Emerge as Discrete States: The First Application of SAEs to 3D Representations](https://arxiv.org/abs/2512.11263)
*Albert Miao,Chenliang Zhou,Jiawei Zhou,Cengiz Oztireli*

Main category: cs.LG

TL;DR: 首次将稀疏自编码器应用于3D领域，分析3D重建VAE的特征，发现模型编码离散特征而非连续特征，形成类似相变的状态转移框架，解释了多个反直觉现象。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在文本领域表现出色，但很少应用于3D领域，限制了特征分解的理论探索。本研究旨在将SAE应用于3D重建模型，分析其特征表示。

Method: 将稀疏自编码器应用于最先进的3D重建VAE模型，分析来自Objaverse数据集的53k个3D模型的特征表示。通过状态转移框架分析特征激活的相变行为。

Result: 发现3D重建模型编码离散而非连续特征，形成类似相变的状态转移。解释了三个反直觉现象：模型偏好位置编码表示、特征消融导致的重建损失sigmoid行为、相变点的双峰分布。

Conclusion: 首次将SAE应用于3D领域，揭示了3D重建模型的特征学习动态，提出了状态转移框架来解释特征分解中的意外现象，表明模型通过重新分配叠加干扰来优先处理不同特征的显著性。

Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

</details>


### [42] [SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks](https://arxiv.org/abs/2512.11298)
*Hao Zhou,Suman Sourav,Binbin Chen,Ke Yu*

Main category: cs.LG

TL;DR: SRLR是一种基于符号回归的逻辑恢复解决方案，仅通过PLC的输入输出来识别其控制逻辑，生成可解释的规则以检测控制器逻辑攻击。


<details>
  <summary>Details</summary>
Motivation: 现有PLC攻击检测方法存在局限性：基于规范的方法需要专家手动工作或访问PLC源代码，而基于机器学习的方法缺乏决策解释性。需要一种仅通过输入输出就能恢复PLC逻辑并提供可解释检测规则的方法。

Method: SRLR利用ICS特定属性增强最新深度符号回归方法：1) 在频域而非时域表示重要控制逻辑；2) 处理多模式操作且模式切换不频繁；3) 过滤异常输入以处理传感器噪声；4) 降低公式复杂度以实现有效搜索。

Result: SRLR在各种ICS设置中始终优于现有方法，恢复准确率提升最高达39%。在包含数百个电压调节器的配电网络评估中，展示了处理大规模复杂系统的稳定性。

Conclusion: SRLR通过结合ICS特定属性增强符号回归，能够仅从输入输出中有效恢复PLC逻辑，生成可解释的检测规则，在复杂工业环境中表现出色。

Abstract: Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.

</details>


### [43] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出使用Golay码的量子纠错方法QGEC，通过Transformer解码器评估性能，发现Golay码比toric码在更少数据量子位下获得更高解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上比经典计算机有计算负载优势，但量子比特易受外部噪声影响。量子纠错(QEC)对处理量子比特至关重要，需要从稳定子生成器的综合征测量结果预测实际错误。

Method: 提出Quantum Golay code Error Correction (QGEC)，使用经典信息理论中高效的Golay码进行量子纠错。采用Transformer进行解码计算，在由生成多项式定义的码空间中评估解码器精度，使用三种不同权重集和三种不同比特翻转错误与相位翻转错误相关性的噪声模型。

Result: 较小相关性的噪声模型给出更好精度，生成多项式的权重对解码器精度影响不大。Golay码（23个数据量子位，码距7）比toric码（50个数据量子位，码距5）获得更高的解码精度。

Conclusion: 使用Transformer实现量子纠错可能使Golay码更高效地实现容错量子计算。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [44] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前基础模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估方法分散在不同基准测试中，难以判断基础模型是否真正超越了其训练分布而实现泛化。需要统一的评估框架来衡量模型的跨领域通用性。

Method: 提出MultiNet v1.0基准测试，涵盖六个核心能力领域：视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。评估了GPT-5、Pi0和Magma等模型。

Result: 所有评估模型都没有表现出一致的泛化能力。尽管在训练分布内表现良好，但在未见领域、不熟悉模态或跨领域任务转移时都出现显著性能下降。失败表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型的实际能力与通用智能的期望之间存在持续差距。MultiNet v1.0为诊断这些差距和指导未来通用智能体开发提供了标准化评估基础。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [45] [Condensation-Concatenation Framework for Dynamic Graph Continual Learning](https://arxiv.org/abs/2512.11317)
*Tingxu Yan,Ye Yuan*

Main category: cs.LG

TL;DR: 提出CCC框架解决动态图中因结构变化导致的灾难性遗忘问题，通过压缩历史图快照并选择性拼接嵌入来保护现有节点


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图结构不断变化，导致图神经网络出现灾难性遗忘。现有方法忽视了拓扑变化对现有节点的影响，需要新的解决方案

Method: 提出CCC框架：1) 将历史图快照压缩为紧凑的语义表示，保持原始标签分布和拓扑特性；2) 选择性拼接历史嵌入与当前图表示；3) 改进遗忘度量以量化结构更新导致的现有节点预测性能下降

Result: 在四个真实世界数据集上的广泛实验中，CCC表现出优于最先进基线的性能

Conclusion: CCC框架有效解决了动态图中因结构变化导致的灾难性遗忘问题，通过压缩-拼接策略保护了现有节点的性能

Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.

</details>


### [46] [Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation](https://arxiv.org/abs/2512.11332)
*Sara Sameer,Wei Zhang,Kannan Dhivya Dharshini,Xin Lou,Yulin Gao,Terence Goh,Qingyu Yan*

Main category: cs.LG

TL;DR: Pace：一种用于电池健康估计的物理感知注意力时序卷积网络，通过整合原始传感器数据和电池物理特征，在公共数据集上比现有模型性能提升6.5倍，并在树莓派上实现实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 电池是现代能源系统（如电动汽车和电网储能）的关键组件，有效的电池健康管理对于电池系统的安全性、成本效益和可持续性至关重要。需要开发能够准确估计电池健康状况的方法。

Method: 提出Pace模型，整合原始传感器测量数据和基于等效电路模型提取的电池物理特征。开发三个电池专用模块：1）扩张时序块用于高效时序编码；2）分块注意力块用于上下文建模；3）双头输出块用于融合短期和长期电池退化模式。

Result: 在大型公共数据集上，Pace表现远优于现有模型，相比两个最佳基线模型平均性能提升6.5倍和2.0倍。进一步在树莓派上实现实时边缘部署，证明了实际可行性。

Conclusion: Pace为电池健康分析提供了一个实用且高性能的解决方案，能够准确高效地预测各种电池使用条件下的电池健康状况。

Abstract: Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.

</details>


### [47] [Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss](https://arxiv.org/abs/2512.11334)
*Cong Yao,Chunye Gong,Jin Zhang*

Main category: cs.LG

TL;DR: 提出SEPI-TFPNet混合模型，结合经验模型与深度学习，通过物理先验子模块选择合适经验模型，数据驱动子模块提取磁通密度时序特征，在MagNet数据集上优于21个代表性模型。


<details>
  <summary>Details</summary>
Motivation: 传统磁芯损耗建模方法预测精度有限，纯数据驱动模型虽然拟合性能强，但可解释性和跨分布泛化能力不足。需要结合物理先验和数据驱动优势，提高建模精度和鲁棒性。

Method: 提出SEPI-TFPNet混合模型：1) 物理先验子模块使用谱熵判别机制选择最适合不同激励波形的经验模型；2) 数据驱动子模块结合CNN、多头注意力机制和双向LSTM提取磁通密度时序特征；3) 引入自适应特征融合模块改善多模态特征交互与整合。

Result: 在包含多种磁性材料的MagNet数据集上评估，与2023年挑战赛的21个代表性模型以及2024-2025年的三种先进方法比较，所提方法在建模精度和鲁棒性方面均有提升。

Conclusion: SEPI-TFPNet混合模型成功结合了经验模型的物理先验和数据驱动模型的拟合能力，在磁芯损耗建模中实现了更好的精度和鲁棒性，为解决传统方法和纯数据驱动模型的局限性提供了有效方案。

Abstract: Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.

</details>


### [48] [DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning](https://arxiv.org/abs/2512.11342)
*Jinming Ge,Linfeng Du,Likith Anaparty,Shangkun Li,Tingyuan Liang,Afzal Ahmad,Vivek Chaturvedi,Sharad Sinha,Zhiyao Xie,Jiang Xu,Wei Zhang*

Main category: cs.LG

TL;DR: DAPO是一个设计结构感知的pass排序框架，通过提取程序语义、对比学习生成嵌入、硬件指标估计和强化学习，为特定设计定制优化策略，相比Vitis HLS平均获得2.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前HLS工具采用固定的优化策略，这些策略继承自软件编译，限制了其有效性。为特定设计定制优化策略需要深度语义理解、准确的硬件指标估计和高级搜索算法，而现有方法缺乏这些能力。

Method: DAPO框架：1）从控制流和数据流图中提取程序语义；2）使用对比学习生成丰富的嵌入表示；3）利用分析模型进行准确的硬件指标估计；4）通过强化学习智能体发现设计特定的优化策略。

Result: 在经典HLS设计上的评估表明，DAPO端到端流程相比Vitis HLS平均获得2.36倍的加速。

Conclusion: DAPO通过结合程序语义理解、对比学习嵌入、硬件指标估计和强化学习，能够为特定HLS设计发现有效的优化策略，显著优于现有固定策略的HLS工具。

Abstract: High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.
  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.

</details>


### [49] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 论文提出了一种对称感知的扩散策略引导框架，将强化学习与等变扩散策略结合，通过利用几何对称性提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然等变扩散策略（EDPs）结合了扩散模型的生成能力和几何对称性的优势，但直接用标准（非等变）强化学习进行微调会忽略EDPs设计的对称性，导致样本效率低下和不稳定。

Method: 1. 理论上证明EDPs的扩散过程是等变的，从而诱导出适合等变扩散引导的群不变潜在噪声MDP；2. 基于此理论提出对称感知的引导框架；3. 通过实验比较标准、等变和近似等变强化学习策略。

Result: 实验表明：1. 在对称性引导过程中利用对称性带来显著好处；2. 提高样本效率；3. 防止价值发散；4. 即使从极有限的演示数据训练EDPs，也能实现强大的策略改进；5. 识别了严格等变在对称性破坏下的实际边界。

Conclusion: 对称感知的扩散策略引导框架有效结合了等变扩散策略和强化学习的优势，在保持对称性的同时实现了高效的策略微调和改进，为对称性任务提供了更稳定、更高效的解决方案。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [50] [CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?](https://arxiv.org/abs/2512.11352)
*Jie Wang,Zheng Yan,Jiahe Lan,Xuyan Li,Elisa Bertino*

Main category: cs.LG

TL;DR: CAT：首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，通过双注意力机制和元路径提取上下文特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN信任预测模型存在三个主要局限：1) 无法捕捉信任动态性；2) 忽略真实网络的异质性；3) 不支持上下文感知这一信任基本属性。这些限制导致预测结果粗糙且不可靠。

Method: CAT模型包含图构建层、嵌入层、异质注意力层和预测层。采用连续时间表示处理动态图，通过时间编码函数捕捉时序信息。使用双注意力机制建模图异质性，引入元路径概念提取上下文特征，构建上下文嵌入并集成上下文感知聚合器。

Result: 在三个真实世界数据集上的实验表明，CAT在信任预测方面优于五组基线方法，同时展现出对大规模图的可扩展性，以及对信任导向和GNN导向攻击的鲁棒性。

Conclusion: CAT是首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，通过创新的元路径和双注意力机制，能够同时预测上下文感知信任和整体信任，显著提升了预测精度和实用性。

Abstract: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.
  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.

</details>


### [51] [Attacking and Securing Community Detection: A Game-Theoretic Framework](https://arxiv.org/abs/2512.11359)
*Yifan Niu,Aochuan Chen,Tingyang Xu,Jia Li*

Main category: cs.LG

TL;DR: 该论文将对抗图概念扩展到社区检测问题，提出攻击和防御技术，并建立博弈论框架CD-GAME来模拟攻防交互，最终达到纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 现有对抗图研究主要关注分类任务，但社区检测问题更具挑战性。需要开发针对社区检测的攻击和防御技术，以保护社交网络中的个人隐私和理解交易网络中的伪装模式。

Method: 提出针对社区检测的新型攻击和防御技术，并建立博弈论框架CD-GAME，其中攻击者为图攻击者，防御者为Rayleigh Quotient防御者，双方动态更新策略直至达到纳什均衡。

Result: 实验表明提出的攻击和防御方法均显著优于现有基线。CD-GAME揭示了在纳什均衡时，攻击者会采用更隐蔽但仍有效的策略，而传统单步攻击策略容易被检测和反击。

Conclusion: 该研究成功将对抗图概念扩展到社区检测领域，提出的CD-GAME框架为理解社区检测中的交互攻防场景提供了有价值的见解，展示了博弈论方法在网络安全中的重要性。

Abstract: It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.

</details>


### [52] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新的强化学习框架，通过将安全策略梯度投影到通用任务的零空间，在保持LLM核心能力的同时实现安全对齐，大幅减少对齐税。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法在强化学习过程中会导致模型遗忘已学习的通用能力（对齐税问题），这限制了LLM在实际应用中的部署。

Method: 提出零空间约束策略优化（NSPO），将安全策略梯度几何投影到通用任务的零空间中，理论上保证在保持模型原始核心能力的同时，为安全对齐提供有效的下降方向。

Result: NSPO在安全性能上大幅超越现有方法，在数学、代码和指令跟随等通用任务上保持准确性，仅需PKU-SafeRLHF中40%的公共安全数据即可达到良好效果。

Conclusion: NSPO有效解决了LLM安全对齐中的对齐税问题，在保持模型核心能力的同时实现高效安全对齐，具有数据效率高、性能优越的特点。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [53] [Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings](https://arxiv.org/abs/2512.11392)
*S Sairam,Prateek P Kulkarni*

Main category: cs.LG

TL;DR: 提出一种结合数论中Bhargava立方体代数约束的神经表示学习方法，在三维潜在空间中学习满足二次关系的结构化嵌入，提高可解释性


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在非结构化潜在空间中学习表示，缺乏可解释性和数学一致性。需要将结构化数学先验融入神经网络表示学习

Method: 将输入数据映射到受约束的三维潜在空间，嵌入通过从Bhargava组合结构推导的学习二次关系进行正则化。使用与分类目标无关的可微分辅助损失函数，引导模型学习数学结构化表示

Result: 在MNIST上达到99.46%的准确率，同时产生可解释的三维嵌入，这些嵌入自然地按数字类别聚类并满足学习的二次约束

Conclusion: 这是数论构造在神经表示学习中的首次应用，为在神经网络中融入结构化数学先验奠定了基础，相比需要显式几何监督的现有流形学习方法，该方法通过可微分约束施加弱代数先验

Abstract: We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.

</details>


### [54] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过键-查询差异的一维投影和排序实现O(n log n)复杂度，适用于长上下文，同时保持与softmax注意力相似的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如softmax和ReLU变体）在长上下文场景下计算复杂度高，需要一种既能保持表达能力又能实现高效计算的新注意力机制。

Method: 提出切片ReLU注意力，不直接对点积应用非线性，而是对键-查询差异进行一维投影，利用排序操作获得准线性复杂度，形成可微的非对称核。

Result: 该机制在O(n log n)复杂度下可计算，适用于长上下文；理论证明其保持序列到序列解缠任务能力和上下文通用逼近性质；小规模实验显示实际应用潜力。

Conclusion: 切片ReLU注意力在保持softmax注意力表达能力的同时，通过排序实现高效计算，为长上下文应用提供了有前景的替代方案。

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [55] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS：将高斯模糊均值漂移扩展到双曲空间，用于层次结构数据聚类


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移（GBMS）在欧几里得空间中有效，但难以处理具有层次或树状结构的数据集。需要将均值漂移扩展到非欧几何空间以捕捉数据中的潜在层次结构。

Method: 将GBMS扩展到双曲空间，用双曲距离替换欧几里得计算，使用Möbius加权均值确保所有更新与空间几何保持一致，保留密度寻求行为。

Result: 在11个真实世界数据集上的实验表明，HypeGBMS在非欧几里得设置中显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。

Conclusion: HypeGBMS将经典均值漂移聚类与双曲表示学习相结合，为弯曲空间中的基于密度的聚类提供了原则性方法，能有效捕捉数据中的潜在层次结构。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [56] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出塑性-天花板框架，理论分析专家轨迹利用机制，确立SFT-then-RL为最佳流程，并提供具体扩展指南。


<details>
  <summary>Details</summary>
Motivation: 当前有效后训练结合了监督微调(SFT)和强化学习(RL)，但如何最优利用专家轨迹的问题尚未解决，需要理论框架指导实践。

Method: 提出塑性-天花板框架，将性能分解为基础SFT性能和后续RL塑性；通过广泛基准测试，建立SFT-then-RL顺序流程；推导精确扩展指南。

Result: 1. SFT稳定或轻度过拟合阶段转向RL能最大化最终性能天花板；2. 数据规模决定主要后训练潜力，轨迹难度作为性能乘数；3. 最小SFT验证损失是选择专家轨迹的稳健指标。

Conclusion: 研究提供了最大化专家轨迹价值的可操作指南，确立了SFT-then-RL为后训练标准流程，并提供了基于理论框架的具体实践建议。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [57] [NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics](https://arxiv.org/abs/2512.11525)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Guangliang Liu,Yuxuan Liang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: NeuralOGCM：融合可微分编程与深度学习的海洋建模框架，通过可学习的物理求解器和神经网络校正，在保持物理一致性的同时提升计算效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决科学模拟中长期存在的计算效率与物理保真度之间的权衡问题，传统数值模型计算成本高，而纯AI方法缺乏物理一致性。

Method: 1. 核心是完全可微分的动力学求解器，将关键物理参数（如扩散系数）转化为可学习参数；2. 深度神经网络校正亚网格尺度过程和离散化误差；3. 两个组件通过统一的ODE求解器协同工作。

Result: NeuralOGCM保持长期稳定性和物理一致性，在速度上显著优于传统数值模型，在精度上优于纯AI基线方法。

Conclusion: 该工作为构建快速、稳定且物理合理的科学计算模型开辟了新路径，展示了可微分编程与深度学习融合在科学模拟中的潜力。

Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.

</details>


### [58] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分预测相关和预测无关的异常来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列预测中，有些异常事件具有持久影响需要响应，而有些是短暂噪声应该忽略。传统预测模型无法区分这两种情况，要么对噪声过度反应，要么错过真正的分布变化。

Method: 提出Co-TSFA对比学习正则化框架：1）生成仅输入和输入-输出增强来分别建模预测无关和预测相关的异常；2）引入潜在输出对齐损失，将表示变化与预测变化联系起来；3）鼓励对无关扰动保持不变性，同时对有意义的分布变化保持敏感性。

Result: 在Traffic和Electricity基准数据集以及真实现金需求数据集上的实验表明，Co-TSFA在异常条件下提高了预测性能，同时在正常数据上保持了准确性。

Conclusion: Co-TSFA通过对比学习有效区分了预测相关和预测无关的异常，为时间序列预测中的异常处理提供了新的正则化框架，在保持正常数据准确性的同时提升了异常条件下的鲁棒性。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [59] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: xGR是一个面向生成式推荐系统的服务系统，通过统一处理预填充和解码阶段、早期排序终止和掩码过滤、以及重构流水线实现多级重叠，在严格延迟约束下达到至少3.49倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统虽然使用LLM处理长用户-物品序列，但其工作负载与LLM服务显著不同：处理长提示但产生短固定长度输出，解码阶段计算成本高（大波束宽度），且波束搜索涉及巨大物品空间导致排序开销特别耗时。现有系统难以满足高并发场景下的严格低延迟要求。

Method: 1. 通过分阶段计算和分离KV缓存统一处理预填充和解码阶段；2. 启用早期排序终止和基于掩码的物品过滤，重用数据结构；3. 重构整体流水线以利用多级重叠和多流并行。

Result: 在真实世界推荐服务数据集上的实验表明，xGR在严格延迟约束下相比最先进的基线系统实现了至少3.49倍的吞吐量提升。

Conclusion: xGR是一个专门为生成式推荐系统设计的服务系统，通过针对性的优化策略有效解决了GR特有的性能瓶颈，能够满足高并发场景下的严格低延迟要求。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [60] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 提出一种基于微分学习的机器学习方法求解参数积分，在统计泛函、切比雪夫展开和微分方程积分三类问题上均优于标准架构


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在求解参数积分问题时存在局限性，需要更高效、准确的方法来处理这类数学计算问题

Method: 采用微分学习框架，在训练过程中融入导数信息，应用于三类代表性积分问题：统计泛函（矩和累积分布函数）、切比雪夫展开函数逼近、微分方程直接产生的积分

Result: 微分机器学习方法在所有案例中都优于标准架构，实现了更低的均方误差、更强的可扩展性和更好的样本效率

Conclusion: 微分学习框架为参数积分求解提供了有效方法，在多种数学计算问题上展现出优越性能

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [61] [A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts](https://arxiv.org/abs/2512.11541)
*Emmanuel K. Katalay,David O. Dimandja,Jordan F. Masakuna*

Main category: cs.LG

TL;DR: 提出一个自动化MLOps管道，用于检测数据分布漂移并触发神经网络分类器的必要重训练，在多个异常检测数据集上展示优于传统重训练策略的效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在数据分布随时间变化时性能会下降（数据分布漂移），而当前的MLOps流程通常是人工触发的，需要自动化解决方案来高效应对分布变化。

Method: 设计了一个自动化MLOps管道，采用多准则统计技术检测显著的数据分布变化，仅在必要时触发模型更新，确保计算效率和资源优化。

Result: 在多个基准异常检测数据集上的实验表明，相比传统重训练策略，该框架显著提高了模型准确性和鲁棒性。

Conclusion: 为在动态现实环境中部署更可靠和自适应的机器学习系统提供了基础，这些环境中数据分布变化是常见的。

Abstract: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.

</details>


### [62] [Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting](https://arxiv.org/abs/2512.11546)
*Federico Pennino,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 提出数据优化框架，通过聚类和优化搜索找到最佳训练数据组合，实现"少即是多"的效果


<details>
  <summary>Details</summary>
Motivation: 传统深度学习训练假设数据越多越好，但原始传感器数据通常不平衡且冗余，并非所有数据点对模型泛化都有同等贡献

Method: 1) 使用大规模编码器和k-means聚类将数据集划分为行为一致的聚类；2) 使用Optuna优化框架搜索高维数据混合空间；3) 对每个试验，Optuna提出每个聚类的采样比例，构建新训练集；4) 训练并评估较小的目标模型

Result: 在PMSM数据集上，该方法将MSE从基线1.70提升到1.37，性能提升19.41%，数据中心的搜索始终发现比在整个数据集上训练的基线性能显著更高的数据混合方案

Conclusion: 通过优化训练数据组成而非模型超参数，可以实现"少即是多"的效果，数据中心的训练方法能够发现最优的数据混合方案，显著提升模型性能

Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.

</details>


### [63] [Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction](https://arxiv.org/abs/2512.11547)
*Janaina Mourão-Miranda,Zakria Hussain,Konstantinos Tsirlis,Christophe Phillips,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 提出了一种新的弹性网络正则化多核学习（ENMKL）方法，通过解析更新核权重，在神经影像应用中实现了稀疏且可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有ENMKL方法采用两阶段优化过程（固定核权重训练SVM，然后更新权重），计算复杂且效率低。需要更简单高效的ENMKL方法，特别是在神经影像领域，当核捕获相关信息且模型可解释性至关重要时。

Method: 提出新的ENMKL公式，能够解析更新核权重。为SVM和核岭回归（KRR）推导了显式算法，并在PRoNTo工具箱中实现。使用弹性网络惩罚（结合l1和l2范数）来促进稀疏性和相关核的选择。

Result: 在三个神经影像应用中，ENMKL在所有任务中都匹配或优于l1范数MKL，仅在一个场景中表现不如标准SVM。ENMKL通过选择性加权相关核，产生了更稀疏、更可解释的模型。

Conclusion: 提出的ENMKL方法提供了简单高效的核权重更新，在神经影像应用中实现了优越的性能和可解释性，特别适用于需要处理相关核并保持模型稀疏性的场景。

Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

</details>


### [64] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 提出视图空间作为图表示学习的新表示轴，通过图视图变换(GVT)构建完全归纳的节点表示学习模型，在27个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型难以泛化到未见图数据集，因为图结构数据的特征空间在维度和语义上差异很大，任何特征空间变换都可能破坏对未见数据集的归纳适用性。

Method: 引入视图空间作为统一表示任意图的新轴，提出图视图变换(GVT)作为节点和特征置换等变的映射，构建循环GVT作为完全归纳的节点表示学习模型。

Result: 在OGBN-Arxiv上预训练，在27个节点分类基准测试中，循环GVT比现有完全归纳图模型GraphAny提升8.93%，比12个单独调优的GNN至少提升3.30%。

Conclusion: 视图空间为完全归纳节点表示学习提供了原则性和有效的理论基础，解决了图数据跨数据集泛化的挑战。

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [65] [Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model](https://arxiv.org/abs/2512.11582)
*Sam Gijsen,Marc-Andre Schulz,Kerstin Ritter*

Main category: cs.LG

TL;DR: Brain-Semantoks是一个自监督框架，通过语义分词器和自蒸馏目标学习大脑动态的抽象表征，在fMRI时间序列上实现强大的下游任务性能


<details>
  <summary>Details</summary>
Motivation: 当前fMRI基础模型通常在小脑区域上使用掩码重建目标训练，专注于低层次信息，导致表征对噪声和时间波动敏感，需要大量微调才能用于下游任务

Method: 提出Brain-Semantoks框架，包含两个核心创新：1) 语义分词器将噪声区域信号聚合为表示功能网络的稳健token；2) 自蒸馏目标强制跨时间的表征稳定性；采用新颖的训练课程来稳定目标

Result: 学习到的表征即使在仅使用线性探针的情况下，也能在各种下游任务上实现强大性能；扩展分析表明更多无标签数据可靠地带来分布外性能提升，无需领域适应

Conclusion: Brain-Semantoks通过专注于学习大脑动态的抽象表征，解决了当前fMRI基础模型的局限性，为疾病和认知相关表型的预测提供了有前景的解决方案

Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.

</details>


### [66] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 提出Atomic Action Slicing (AAS)方法，将长时程演示分解为短时原子动作，提升VLA模型泛化能力，在LIBERO数据集上验证效果显著


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作(VLA)模型泛化能力差，特别是在需要新技能或物体组合的任务中表现不佳

Method: 提出原子动作切片(AAS)方法，将长时程演示分解为短时、类型化的原子动作，生成带标注的原子段数据集，并使用更强的分割器(Gemini 2.5 Pro)匹配规划器定义的计划

Result: 在LIBERO-Goal上任务成功率从94.2%提升至95.3%，在LIBERO-Long上从83.8%提升至88.8%；GATE-VLAP数据集已在HuggingFace发布

Conclusion: AAS方法通过原子动作分解有效提升VLA模型泛化能力，特别是在需要新组合的任务中，为规划器提供更易使用的动作表示

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [67] [Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration](https://arxiv.org/abs/2512.11587)
*Alexander Tyurin*

Main category: cs.LG

TL;DR: 论文通过将神经网络梯度下降简化为广义感知机算法，证明非线性模型相比线性模型能实现更快的收敛速度（$\tilde{O}(\sqrt{d})$ vs $Ω(d)$），解释了神经网络训练中的隐式加速现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络训练中梯度下降的优化动态（包括收敛速度、迭代轨迹、函数值振荡以及隐式加速现象）是一个具有挑战性的问题。作者希望从新的视角分析这些动态特性。

Method: 使用逻辑损失函数分析非线性模型，将梯度下降步骤简化为广义感知机算法（Rosenblatt, 1958）。利用经典线性代数工具分析简化后的算法步骤，并通过一个最小化示例进行理论证明。

Result: 理论证明在两层模型中，非线性特性能够实现$\tilde{O}(\sqrt{d})$的迭代复杂度，而线性模型只能达到$Ω(d)$，其中$d$是特征数量。这解释了神经网络训练中观察到的隐式加速现象。数值实验支持了理论结果。

Conclusion: 通过将梯度下降简化为广义感知机算法，为理解神经网络优化动态提供了新视角。非线性模型相比线性模型具有更快的收敛速度，这有助于解释神经网络训练中的隐式加速现象。这一替代视角有望进一步推动神经网络优化研究。

Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.

</details>


### [68] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 提出一种将经典树分裂算法扩展到模糊树的模糊贪婪树方法，在保持可解释性的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有模糊规则挖掘算法无法同时保证可解释的语言划分和小规则库规模，进化方法计算成本过高，而神经方法如ANFIS难以保持语言可解释性

Method: 将经典树基分裂算法从清晰规则扩展到模糊树，结合贪婪算法的计算效率和模糊逻辑的可解释性优势

Result: 在表格分类基准测试中，该方法与最先进的模糊分类器达到可比精度，计算成本显著降低，并产生具有约束复杂度的更可解释规则库

Conclusion: 模糊贪婪树方法实现了可解释的语言划分，相比进化方法大幅缩短运行时间，同时保持有竞争力的预测性能

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [69] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 该论文提出使用大型上下文表格模型(LTMs)作为流式持续学习(SCL)的桥梁，通过将无界数据流实时压缩为紧凑摘要，同时满足流式学习的数据压缩需求和持续学习的经验回放需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究将持续学习(CL)和流式学习(SL)分开处理：CL关注长期记忆和减轻灾难性遗忘但缺乏实时约束，SL强调对高频数据流的快速适应但忽视遗忘问题。需要一种统一框架来结合两者的优势。

Method: 提出使用大型上下文表格模型(LTMs)作为SCL的核心架构，基于两个数据选择原则：(1)分布匹配：平衡可塑性和稳定性；(2)分布压缩：通过多样化和检索机制控制内存大小。

Result: 论文提出了一个理论框架，将SL和CL社区隐含的"分而治之"策略形式化，展示了LTMs如何自然桥接两个领域，为流式持续学习提供了结构化方法。

Conclusion: 大型上下文表格模型为流式持续学习提供了可行的解决方案，通过实时数据流压缩和基于分布匹配与压缩的数据选择机制，能够同时满足流式学习的高效适应需求和持续学习的知识保留需求。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [70] [High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control](https://arxiv.org/abs/2512.11705)
*Sebastian Hirt,Valentinus Suwanto,Hendrik Alsmeier,Maik Pfefferkorn,Rolf Findeisen*

Main category: cs.LG

TL;DR: 贝叶斯神经网络作为代理模型能有效优化高维控制器参数，相比传统高斯过程在密集高维参数化场景中表现更优


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理密集高维控制器参数（如模型预测控制器调参）时效果有限，因为标准代理模型难以捕捉高维空间结构，需要更有效的代理模型

Method: 使用贝叶斯神经网络作为代理模型，对比了三种方法：带Matern核的高斯过程、有限宽度贝叶斯神经网络、无限宽度贝叶斯神经网络，在cart-pole任务上进行实验

Result: 贝叶斯神经网络代理模型能更快更可靠地收敛闭环成本，成功优化数百维参数化；无限宽度贝叶斯神经网络在超过一千个参数时仍保持性能，而Matern核高斯过程效果迅速下降

Conclusion: 贝叶斯神经网络代理模型适合学习密集高维控制器参数化，为基于学习的控制器设计中代理模型选择提供实用指导

Abstract: Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.

</details>


### [71] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum是一种联邦学习防御方法，结合谱子空间估计和几何邻居选择，通过历史聚合学习低维流形来过滤恶意更新，在非IID数据下对抗自适应攻击有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中拜占庭客户端可以注入任意损坏的更新来破坏全局模型。现有的鲁棒聚合方法（如Krum、Bulyan等）在数据分布异构（非IID）且攻击者能观察或近似防御机制时效果显著下降。

Method: SpectralKrum融合谱子空间估计和几何邻居选择：1）从历史聚合中学习低维流形；2）将传入更新投影到该子空间；3）在压缩坐标中应用Krum选择；4）过滤正交残差能量超过数据驱动阈值的候选。该方法无需辅助数据，完全在模型更新上操作，保持FL隐私特性。

Result: 在CIFAR-10的Dirichlet分布非IID分区（alpha=0.1）上评估，对抗8个基线防御和7种攻击场景。超过56,000轮训练显示：SpectralKrum对方向性和子空间感知攻击（adaptive-steer、buffer-drift）具有竞争力，但在标签翻转和min-max攻击下优势有限，因为恶意更新在谱上难以与良性更新区分。

Conclusion: SpectralKrum通过谱子空间估计有效防御联邦学习中的拜占庭攻击，特别是在非IID数据和自适应攻击场景下。然而，对于恶意更新在谱特征上与良性更新难以区分的攻击类型，其防御效果有限。

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [72] [The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation](https://arxiv.org/abs/2512.11776)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: AVC是一种混合架构，通过解耦流形学习和函数逼近，使用深度网络学习物理域的微分同胚扭曲，将复杂时空动力学投影到潜在流形上，并用广义解析函数基表示解，同时用可微分线性求解器替代梯度下降输出层，实现谱系数的闭式求解。


<details>
  <summary>Details</summary>
Motivation: 解决基于坐标的神经网络的两个基本问题：频谱偏差（阻碍高频动力学学习）和维度灾难（导致离散特征网格参数爆炸）。

Method: 提出自适应Vekua级联（AVC）混合架构：1）使用深度网络学习物理域的微分同胚扭曲，将复杂动力学投影到潜在流形；2）用广义解析函数基表示解；3）用可微分线性求解器替代标准梯度下降输出层，在前向传播中闭式求解谱系数。

Result: 在五个物理基准测试中（包括高频Helmholtz波传播、稀疏医学重建和非稳态3D Navier-Stokes湍流），AVC达到最先进精度，同时将参数数量减少数个数量级（如840参数 vs 420万参数的3D网格），收敛速度比隐式神经表示快2-3倍。

Conclusion: 这项工作为内存高效、频谱精确的科学机器学习建立了新范式，通过结合深度学习和经典逼近理论，解决了神经网络在物理场表示中的基本限制。

Abstract: Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.

</details>


### [73] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 该论文建立了软注意力在有限和无限提示下的统一分析框架，证明在长提示下软注意力收敛于线性注意力，从而可将线性注意力的优化分析直接应用于软注意力。


<details>
  <summary>Details</summary>
Motivation: 软注意力是Transformer的核心组件，但其非线性结构给理论分析带来挑战。现有研究缺乏对软注意力在有限和无限提示下的统一分析框架，特别是在训练动态和统计行为方面的理论理解。

Method: 开发基于测度的统一框架，研究单层软注意力在有限和无限提示下的行为。利用软算子在无限提示极限下收敛于线性算子的性质，建立输出和梯度的非渐近集中界，分析上下文学习中的训练稳定性。

Result: 证明了软注意力在长提示下收敛于线性注意力，建立了输出和梯度的集中界，展示了训练轨迹的稳定性，并在上下文线性回归中分析了有限提示下的训练动态。

Conclusion: 当提示足够长时，软注意力继承了线性注意力的分析结构，这为研究软注意力在大提示机制下的训练动态和统计行为提供了原则性工具包，使线性注意力的优化分析可直接应用于软注意力。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [74] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于几何形状的L-score方法，通过随机顺序添加元素并观察贡献模式来量化特征间的交互关系（协同、独立、冗余）。


<details>
  <summary>Details</summary>
Motivation: 许多系统存在复杂的组件交互关系，包括协同效应、冗余信息和独立贡献。现有方法难以统一量化这些不同类型的交互结构，需要一种简单直观的方法来发现和区分这些关系。

Method: 通过随机顺序添加元素，多次试验中绘制元素贡献图，观察L形模式。提出L-score连续度量（-1到+1），其中-1表示完美协同，0表示独立，+1表示完美冗余。可视化二维点云，冗余对形成L形（仅先添加元素贡献），协同对形成反L形（仅共同贡献），独立元素显示顺序不变分布。

Result: 该方法能够量化每个元素对先前添加元素的依赖关系，区分交互、独立和冗余模式。L形臂的相对缩放揭示特征主导性。虽然仅基于成对测量，但通过一致的交叉对关系自然涌现高阶交互。方法对度量不敏感，适用于任何可增量评估性能的领域。

Conclusion: 提出了一种统一的几何方法来揭示交互结构，通过简单的L形模式分析量化特征间关系，为理解复杂系统中的交互效应提供了直观且通用的框架。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [75] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: PD-Swap：一种基于FPGA的预填充-解码解耦LLM加速器，通过动态部分重配置技术，为预填充和解码阶段分别优化注意力模块架构，显著提升长上下文推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着提示长度增长到数万个token，边缘硬件性能因二次预填充成本和快速增长的KV缓存带宽需求而急剧下降。现有静态加速器必须为计算密集的预填充阶段和内存带宽密集的解码阶段提供统一架构，导致资源重复、利用率低，限制了模型大小和可用上下文长度。

Method: 提出PD-Swap预填充-解码解耦LLM加速器，利用动态部分重配置技术，在边缘FPGA上时间复用注意力模块。核心的三元矩阵乘法和权重缓冲引擎保持静态，而注意力子系统作为可重配置分区，包含两个阶段专用架构：计算密集、token并行的预填充引擎和带宽优化、KV缓存中心的解码引擎。

Result: PD-Swap实现了高达27 tokens/s的解码吞吐量，在长上下文长度下比现有最先进工作提升1.3-2.1倍，且无需额外面积成本。

Conclusion: 通过解耦预填充和解码阶段的架构优化，PD-Swap有效解决了LLM推理中的预填充-解码不对称性问题，显著提升了边缘FPGA上长上下文LLM推理的性能。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [76] [Tekum: Balanced Ternary Tapered Precision Real Arithmetic](https://arxiv.org/abs/2512.10964)
*Laslo Hunhold*

Main category: cs.ET

TL;DR: 本文提出了一种用于平衡三值逻辑的新型实数算术格式tekum，基于锥形精度概念，在三值硬件上展现出优于现有格式的性能潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管硬件技术不断进步，但平衡三值逻辑中的实数算术在文献中几乎未被关注。考虑到三值逻辑在能效计算和克服内存墙方面的潜力，这一空白令人惊讶。

Method: 重新审视了posit和takum格式中使用的锥形精度算术概念，将其引入平衡三值逻辑，提出了tekum算术格式，并解决了相关的设计挑战。

Result: 评估显示tekum格式具有高度有前景的特性，在许多方面优于posit和takum格式，为三值系统的实数计算奠定了基础。

Conclusion: 随着三值硬件的成熟，这项工作为解锁三值系统中实数计算的完整潜力迈出了关键一步，为新一代硬件量身定制的新型数字格式奠定了基础。

Abstract: In light of recent hardware advances, it is striking that real arithmetic in balanced ternary logic has received almost no attention in the literature. This is particularly surprising given ternary logic's promising properties, which could open new avenues for energy-efficient computing and offer novel strategies for overcoming the memory wall.
  This paper revisits the concept of tapered precision arithmetic, as used in posit and takum formats, and introduces a new scheme for balanced ternary logic: tekum arithmetic. Several fundamental design challenges are addressed along the way. The proposed format is evaluated and shown to exhibit highly promising characteristics. In many respects, it outperforms both posits and takums. As ternary hardware matures, this work represents a crucial step toward unlocking the full potential of real-number computation in ternary systems, laying the groundwork for a new class of number formats designed from the ground up for a new category of next-generation hardware.

</details>


### [77] [Beyond Memristor: Neuromorphic Computing Using Meminductor](https://arxiv.org/abs/2512.11002)
*Frank Zhigang Wang*

Main category: cs.ET

TL;DR: 该论文发现带有磁芯的线圈是一种忆感器（meminductor），其电感L(q)是电荷q的函数，能够记忆通过线圈的电流历史。这种忆感器在神经形态计算中具有独特作用，可用于再现阿米巴原虫的生物行为。


<details>
  <summary>Details</summary>
Motivation: 研究忆阻器、忆感器和忆容器在新型计算架构中的不同作用，特别是探索忆感器在神经形态计算、深度学习和类脑计算中的独特价值，这些是忆阻器无法替代的。

Method: 通过理论分析发现带有磁芯的线圈具有忆感特性（电感随电荷变化），并设计实验验证：使用新发明的忆感器来再现阿米巴原虫的生物学行为（记忆、计时和预期机制）。

Result: 成功验证了磁芯线圈确实是一种忆感器，能够记忆电流历史。实验表明这种忆感器能够准确再现阿米巴原虫的生物学行为，证明了其在神经形态计算中的实际应用价值。

Conclusion: 超越忆阻器的计算范式在理论上是合理的，在实践中是可行的。忆感器在神经形态计算中具有独特作用，为新型计算架构提供了新的可能性。

Abstract: Memristor (resistor with memory), inductor with memory (meminductor) and capacitor with memory (memcapacitor) have different roles to play in novel computing architectures. We found that a coil with a magnetic core is an inductor with memory (meminductor) in terms of its inductance L(q) being a function of the charge q. The history of the current passing through the coil is remembered by the magnetization inside the magnetic core. Such a meminductor can play a unique role (that cannot be played by a memristor) in neuromorphic computing, deep learning and brain inspired since the time constant of a neuromorphic RLC circuit is jointly determined by the inductance and capacitance, rather than the resistance. As an experimental verification, this newly invented meminductor was used to reproduce the observed biological behaviour of amoebae (the memorizing, timing and anticipating mechanisms). In conclusion, a beyond memristor computing paradigm is theoretically sensible and experimentally practical.

</details>
