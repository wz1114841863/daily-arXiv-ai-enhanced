<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [A Dense and Efficient Instruction Set Architecture Encoding](https://arxiv.org/abs/2510.04158)
*Emad Jacob Maroun*

Main category: cs.AR

TL;DR: Scry指令集架构通过前向时间引用和内部标记技术，在2字节指令空间内实现了与RISC-V RV64IMC 4字节指令相当的功能，指令密度提升了72%


<details>
  <summary>Details</summary>
Motivation: 指令集设计应最大化指令密度和编码效率，因为这两者直接影响处理器性能，而实现细节通常影响性能、功耗和面积

Method: 采用前向时间引用作为数据流机制（指令引用未来消费其输出的指令），以及内部标记技术（处理器内部跟踪数据类型），减少所需指令数量并提高灵活性

Result: Scry仅用2字节指令就实现了RISC-V RV64IMC的功能，占用28%的2字节编码空间，而RISC-V占用68%的4字节编码空间。手编译的Scry静态指令密度在小函数中与RV64IMC相当，随函数规模增大而提升

Conclusion: Scry指令集通过创新的设计方法显著提高了指令密度和编码效率，为现代处理器实现提供了更优化的指令集架构

Abstract: Instruction density and encoding efficiency are some of the few things
directly affected by an instruction set architecture's design. In contrast, a
processor's implementation often significantly influences performance, power
efficiency, and area usage. Therefore, a major goal of instruction set design
should be maximizing instruction density and encoding efficiency. This paper
introduces the design elements of the Scry instruction set architecture that
most significantly affect instruction density and encoding efficiency. Scry is
a novel and experimental instruction set that revisits first principles to
design an instruction set fit for modern processor implementations. Scry uses
forward-temporal referencing as a means of data flow, where instructions refer
to which future instructions consume their outputs. It also uses internal
tagging, where the processors track data types internally, to reduce the number
of instructions needed and increase flexibility. Combining these two methods,
Scry achieves instruction-feature parity with RISC-V's RV64IMC using only
2-byte instructions compared to RISC-V's 4 bytes. Scry's instructions occupy
only 28% of the 2-byte encoding space, where RV64IMC instructions occupy 68% of
the 4-byte encoding space. We show that hand-compiled Scry's static instruction
density is comparable to RV64IMC for small functions and improves as functions
grow in size.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS是一个提示感知的LLM任务调度器，通过近似最短作业优先调度来减少头部阻塞问题，提高推理服务的效率和延迟表现。


<details>
  <summary>Details</summary>
Motivation: 传统FCFS调度策略存在头部阻塞问题，长任务会延迟后续短任务的执行，影响LLM推理服务的延迟和吞吐量。

Method: 使用成对排序和边际排序损失来近似SJF调度，预测基于响应长度的任务排序，并与vLLM系统无缝集成。

Result: 在多个LLM和真实推理数据集上的实验表明，PARS显著提升了性能，包括推理工作负载，且设计具有良好的跨模型泛化能力。

Conclusion: PARS通过提示感知的智能调度有效解决了LLM推理中的头部阻塞问题，实现了低延迟和高吞吐量的服务目标。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [3] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个跨模态时间序列预测模型，通过将多元时间序列转换为图像，利用预训练的大型视觉模型提取通道间依赖关系，并与时间序列模态特征对齐融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型采用通道独立架构，忽略了关键的跨通道依赖关系；同时多模态方法未能充分利用大型视觉模型解释时空数据的潜力；不同模态信息提取的优势尚未被充分探索来提升时间序列预测性能。

Method: 将多元时间序列渲染为图像，利用预训练大型视觉模型提取复杂的跨通道模式，这些视觉特征与时间序列模态的表征进行对齐和融合。通过冻结视觉模型仅训练7.45%的参数实现高效训练。

Result: 在多个基准测试中取得了有竞争力的性能表现。

Conclusion: VIFO提供了一种高效且有效的解决方案，能够捕捉时间序列中的跨变量关系，解决了通道独立模型的局限性。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [4] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种新的可迁移对抗攻击方法——可迁移频率感知攻击，以及基于此的归因方法FAMPE，用于提升深度神经网络的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界噪声和故意扰动下的可靠性问题，现有归因方法效果欠佳需要改进。

Method: 提出可迁移频率感知攻击，通过高低频分量进行频率感知探索；基于此开发FAMPE归因方法。

Result: 相比现有最优方法AttEXplore，FAMPE在Insertion Score上平均提升13.02%。

Conclusion: FAMPE方法显著提升了深度神经网络的可解释性，并通过消融研究验证了高低频分量在可解释性中的作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [5] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE是一个基于ADMM的结构化剪枝框架，通过分治策略将全局剪枝分解为协调的子问题，结合了局部剪枝的内存效率和结构化方法的硬件兼容性。


<details>
  <summary>Details</summary>
Motivation: 全局剪枝虽然性能强但需要O(N)内存，不适用于十亿参数模型；局部剪枝内存效率高但忽略层间依赖，在高稀疏度下性能不佳；结构化剪枝硬件效率高但通常依赖全局剪枝。

Method: 采用分治策略将全局剪枝分解为模块化子问题，基于ADMM框架集成结构化稀疏性，推导出结构化剪枝掩码的闭式解析解和基于能量的软最大分配方案。

Result: STRUPRUNE在匹配全局结构化剪枝困惑度的同时，将内存成本从O(N)降低到O(√N)，实现了十亿参数级别的实际部署。

Conclusion: 该方法成功解决了结构化剪枝与内存效率之间的权衡问题，为大语言模型的实用化部署提供了可行方案。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [6] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出了首个针对未对齐多模态数据的主动学习框架，通过主动获取跨模态对齐而非标签来降低标注成本，在保持性能的同时减少高达40%的标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽视了多模态学习中巨大的标注负担，特别是跨模态对齐的高成本问题。

Method: 开发了一种结合不确定性和多样性原则的模态感知算法，实现线性时间获取，适用于池式和流式设置。

Result: 在基准数据集上的广泛实验表明，该方法能持续减少多模态标注成本，如在ColorSwap数据集上减少40%标注需求而不损失准确率。

Conclusion: 该框架有效解决了多模态学习中的标注瓶颈问题，为现代多模态管道如CLIP和SigLIP提供了实用的成本降低方案。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [7] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该研究评估了四种神经算子架构用于快速预测脑位移场，以替代计算昂贵的有限元模型，实现创伤性脑损伤的实时建模。MG-FNO获得最高精度，DeepONet提供最快推理速度，所有神经算子都将计算时间从小时级降至毫秒级。


<details>
  <summary>Details</summary>
Motivation: 创伤性脑损伤是全球重大公共卫生问题，现有有限元模型计算昂贵（每小时一次模拟），限制了临床快速决策的应用。需要开发快速、患者特定的脑位移预测方法。

Method: 将TBI建模定义为算子学习问题，使用四种神经算子架构（FNO、F-FNO、MG-FNO、DeepONet），输入患者特定的解剖MRI、MRE刚度图和人口统计学特征，输出全场3D脑位移预测。在249个MRE数据集上训练评估。

Result: MG-FNO达到最高精度（MSE = 0.0023，94.3%空间保真度），F-FNO收敛速度比标准FNO快2倍，DeepONet提供最快推理速度（14.5次迭代/秒），比MG-FNO快7倍。所有神经算子将计算时间从小时级降至毫秒级。

Conclusion: 神经算子为预测脑变形提供了高效、分辨率不变的方法，为实现实时、患者特定的TBI风险评估、临床分诊支持和防护设备优化打开了大门，展示了基于神经算子的人脑数字孪生的潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [8] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个混合LLM系统，通过在边缘服务器上协同定位推理和微调任务，使用迭代级调度来平衡推理延迟和模型准确性，在资源受限环境下实现高效持续学习。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上部署的LLM在延迟敏感应用中面临频繁重训练需求，现有方法要么延迟模型更新，要么过度分配资源给重训练，无法在GPU资源受限下平衡推理延迟和模型精度。

Method: 提出MACE系统，协同定位并发推理（预填充、解码）和微调，采用智能内存管理，基于模型更新对输出对齐的影响程度分配GPU周期，实现迭代级调度。

Result: MACE在保持吞吐量的同时，将推理延迟降低高达63%，在NVIDIA AGX Orin上维持85%以上的GPU利用率，优于连续重训练和周期性重训练方法。

Conclusion: 迭代级混合调度是在边缘平台上部署具有持续学习能力的LLM的有前景方向，能有效平衡吞吐量、延迟和更新新鲜度。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [9] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一种新的可微分逻辑门网络参数化方法，解决了梯度消失、离散化误差和高训练成本问题，显著减少了模型大小和训练步骤，同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门网络在推理时效率极高且保持竞争性准确率，但存在梯度消失、离散化误差和高训练成本问题，即使使用专门的参数初始化方案，增加深度仍会损害准确率。

Method: 重新参数化逻辑门神经元，将每个门的参数大小按输入数量对数级缩小。对于二进制输入，模型大小减少4倍，反向传播速度提升至1.86倍，训练步骤减少8.5倍。

Result: 在CIFAR-100上的准确率保持稳定，有时甚至优于原始参数化方法。

Conclusion: 通过重新参数化逻辑门神经元，有效解决了可微分逻辑门网络的扩展问题，显著提升了训练效率和模型性能。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [10] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一个基于主动领域适应的日志异常检测模型，结合迁移学习和主动学习技术，仅需2%手动标注即可达到93.01%的平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决现有日志异常检测方法对标注数据依赖性强的问题，同时克服迁移学习中的数据分布差异和主动学习中的冷启动问题。

Method: 使用成熟系统的标注数据训练基础模型，采用基于自由能和不确定性的采样策略选择分布边界处的日志进行手动标注。

Result: 在六个不同数据集组合上，仅使用2%手动标注就达到93.01%的平均F1分数，比现有最优方法提升26.28%。

Conclusion: LogAction通过主动领域适应有效解决了日志异常检测中的标注依赖和数据分布差异问题，显著减少了人工标注成本。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [11] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: Numerion是一个基于超复数空间的时间序列预测模型，通过将时间序列映射到不同维度的超复数空间来自然分解和建模序列特征，在多个公开数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过复杂模型结构和先验知识分解时间序列，但受限于计算复杂性和假设的鲁棒性。研究发现超复数空间中时间序列的特征频率自然降低，这为更有效的分解提供了理论基础。

Method: 提出Numerion模型，将线性层和激活函数推广到任意2的幂次维度的超复数空间，引入Real-Hypercomplex-Real域多层感知机(RHR-MLP)架构，使用多个RHR-MLP在不同维度超复数空间中建模，并通过动态融合机制自适应融合不同空间中的潜在模式。

Result: 在多个公开数据集上验证了模型性能，达到了最先进的预测结果。可视化和定量分析证明了多维RHR-MLP能够自然分解时间序列，并揭示了高维超复数空间倾向于捕捉低频特征的规律。

Conclusion: Numerion通过超复数空间实现了对时间序列的自然分解和有效建模，为时间序列预测提供了一种新的有效方法，证明了超复数空间在捕捉时间序列特征方面的优势。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [12] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种即插即用的推理时路由算法，用于改善混合专家模型的负载均衡问题，在不影响准确性的前提下降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型通过条件路由减少训练成本，但推理时专家参数和激活占用内存，导致专家负载不均衡，影响系统性能（延迟、吞吐量和成本）。

Method: LASER根据门控分数分布自适应路由：当分数显示明显偏好时路由到最强专家；当分数较均匀时扩大可行专家集并路由到负载最轻的专家。无需重新训练或微调。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat模型上测试四个数据集，LASER改善了负载均衡，降低了延迟并提高了吞吐量，准确率变化可忽略不计。

Conclusion: LASER是一种有效的推理时路由算法，能显著改善混合专家模型的系统性能，同时保持模型准确性。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [13] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了通用多域翻译(UMDT)框架，使用扩散路由(DR)方法，只需K-1个配对数据集就能实现K个域之间的任意双向翻译。


<details>
  <summary>Details</summary>
Motivation: 现有多域翻译方法需要完全对齐的元组或只能处理训练中见过的域对，限制了实用性并排除了许多跨域映射。

Method: 使用扩散路由(DR)框架，通过单一噪声预测器建模所有中心域与非中心域之间的翻译，支持通过中心域进行间接翻译，并引入变分边界目标和Tweedie精炼程序支持直接映射。

Result: 在三个大规模UMDT基准测试中取得最先进结果，支持间接和直接翻译，降低采样成本，并解锁了草图↔分割等新任务。

Conclusion: DR是一个可扩展且通用的多域翻译框架，为跨多个域的通用翻译提供了有效解决方案。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [14] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是一种带约束感知的联邦学习方法，通过拉格朗日对偶优化动态调整训练超参数，在满足设备资源约束的同时保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在资源受限边缘设备上的部署问题，需要显式考虑设备级资源约束（能量、通信、内存、热预算等）。

Method: 基于FedAvg扩展，使用拉格朗日对偶优化动态调整冻结深度、本地步数、批量大小和通信压缩等超参数，通过梯度积累保持令牌预算。

Result: 在字符级语言模型上，相比标准FedAvg，内存使用减少20%，通信减少95%，同时保持竞争力的验证性能。

Conclusion: CAFL-L在满足资源约束方面表现优越，适用于资源受限边缘设备的实际部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [15] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL是一种分层偏好学习框架，通过多粒度偏好信号优化LLM智能体，解决了轨迹级DPO信号过粗和步骤级DPO过于短视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的离线方法（如DPO）在优化LLM智能体时面临粒度不匹配问题：轨迹级DPO信号过于粗糙，步骤级DPO过于短视，无法捕捉多步行为价值。

Method: HPL框架包含三个关键组件：1）轨迹级和步骤级DPO确保全局和局部策略稳定性；2）组级偏好优化，将专家轨迹分解为语义连贯的动作组；3）双层课程调度器，按组长度和样本难度组织学习过程。

Result: 在三个具有挑战性的智能体基准测试中，HPL超越了现有最先进方法。分析表明分层DPO损失有效整合了多粒度偏好信号，双层课程对解决从简单行为到复杂多步序列的任务至关重要。

Conclusion: HPL通过分层偏好学习和双层课程调度，成功解决了LLM智能体优化中的粒度不匹配问题，在复杂长视野任务中表现出色。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [16] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate是一个通过从源代码、运行时日志和历史作业中提取语义信息来增强深度学习调度器的框架，能够将平均作业完成时间减少高达1.91倍。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习调度器缺乏对作业语义上下文的理解，只能依赖有限的元数据，导致高分析开销、不可靠的持续时间估计、不充分的故障处理和差的可观测性。

Method: SchedMate通过三个基于LLM的组件非侵入式地增强现有调度器，系统地从源代码、运行时日志和历史作业等非结构化数据源中提取深度洞察。

Result: 在128-GPU物理集群上的评估和生产痕迹的广泛模拟显示，SchedMate将平均作业完成时间减少高达1.91倍，显著提升了调度性能。

Conclusion: 语义感知在现代深度学习调度中扮演关键角色，SchedMate通过弥合语义鸿沟有效提升了调度效率。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [17] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 该论文提出了一种约束悲观双层优化模型，通过限制对手的移动范围来避免现有方法中对手可能产生无意义数据的问题，从而更好地反映现实情况。


<details>
  <summary>Details</summary>
Motivation: 现有的悲观双层优化方法中对手不受限制，可能导致模型过于悲观和不切实际，因为对手在寻找最优解时可能产生无意义的数据，这无法正确反映现实情况，导致在实际数据上分类器性能不佳。

Method: 构建约束悲观双层优化模型，通过限制对手的移动范围来识别更符合现实情况的解决方案。

Result: 实验证明该模型平均性能优于现有方法。

Conclusion: 约束悲观双层优化模型能够更好地反映现实对抗场景，提高分类器在实际数据上的性能表现。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [18] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出了一种结合不频繁同步和梯度动量压缩的方法，通过离散余弦变换将Nesterov动量分解为高低频分量，仅同步高频分量，显著减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 减少对高带宽互连的依赖，使得能够在分布式计算资源上训练大模型，而不是仅限于数据中心。

Method: 将优化器动量视为信号，通过离散余弦变换将Nesterov动量分解为高低频分量，每H步仅同步高频分量。

Result: 相比基线DiLoCo实现了高达16倍的通信减少，在基于transformer的语言模型和卷积神经网络上均有效。

Conclusion: 这项工作推进了在低带宽互连的分布式节点上训练大模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [19] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: SciTS是一个涵盖12个科学领域、43个任务的科学时间序列基准测试，包含5万+实例。研究发现通用LLM比专用时间序列模型具有更强的泛化能力，而将时间序列表示为文本或图像会限制性能。作者提出了TimeOmni框架，使LLM能够理解和生成时间序列。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM要么将数值序列编码为文本，要么转换为图像，这些方法可能不足以全面理解科学时间序列。现有统一时间序列模型通常专门用于预测或分析，其在非周期性、异质科学信号上的有效性尚不清楚。

Method: 构建SciTS基准测试，涵盖12个科学领域、43个任务，包含5万+实例，时间序列长度从10^0到10^7，频率高达10MHz。对17个模型进行基准测试，包括纯文本LLM、多模态LLM和统一时间序列模型。提出TimeOmni框架，使LLM能够理解和生成时间序列。

Result: 通用LLM比专用时间序列模型表现出更强的泛化能力。将时间序列表示为文本或图像会限制性能，因为过长的序列和数值精度损失。

Conclusion: 这项工作填补了科学时间序列专用基准测试和建模框架的空白，为LLM理解和生成复杂时间科学数据铺平了道路。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [20] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出基于联邦学习的轻量级隐私保护物联网僵尸网络检测框架，在保护用户隐私的同时保持检测精度，并显著降低通信成本


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展带来了创新机会，但也增加了僵尸网络驱动的网络攻击风险。传统检测方法在资源受限的物联网环境中面临可扩展性、隐私保护和适应性挑战

Method: 采用联邦学习方法，使分布式设备能够协作训练模型而无需交换原始数据，同时引入通信高效的聚合策略以减少开销

Result: 在基准物联网僵尸网络数据集上的实验表明，该框架实现了高检测精度，同时显著降低了通信成本

Conclusion: 联邦学习为实现可扩展、安全且隐私感知的物联网入侵检测提供了实用路径

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [21] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出Triple-BERT方法，使用集中式单智能体强化学习解决网约车平台大规模订单分配问题，通过动作分解策略和BERT网络处理大规模动作和观察空间，在真实数据集上相比现有方法提升11.95%性能。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临实时订单分配挑战，现有MARL方法存在全局信息捕获不足、合作性差或维度灾难问题，需要更有效的集中式解决方案。

Method: 基于TD3变体的集中式单智能体强化学习，采用动作分解策略将联合动作概率分解为单个司机动作概率，使用BERT网络通过参数重用和注意力机制处理大规模观察空间。

Result: 在曼哈顿真实网约车数据集上验证，相比现有最优方法提升11.95%性能，服务订单增加4.26%，接驾时间减少22.25%。

Conclusion: Triple-BERT通过集中式单智能体强化学习方法有效解决了大规模订单分配问题，在真实场景中表现出显著性能优势。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [22] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出多层移动边缘计算框架用于跌倒检测，通过知识蒸馏方法平衡准确性和延迟，在SisFall和FallAllD数据集上分别提升准确率11.65%和2.78%，同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口增加，跌倒检测系统的重要性日益凸显。传统边缘设备模型大小有限，云端传输存在延迟问题，需要找到平衡准确性和延迟的解决方案。

Method: 采用多层移动边缘计算框架，将架构分为多个站点，每个站点配备神经网络模型。使用知识蒸馏方法，让高功率后端站点为前端设备提供额外学习经验，提高检测精度。

Result: 在SisFall数据集上准确率提升11.65%，在FallAllD数据集上提升2.78%。与无知识蒸馏的MLMEC相比，在FallAllD数据集上延迟降低54.15%，在SisFall数据集上降低46.67%。

Conclusion: 多层移动边缘计算跌倒检测系统在提高准确性的同时显著降低了延迟，为实时跌倒检测提供了有效解决方案。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [23] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: 提出POEM方法，通过探索之前未被利用的可靠样本来改进测试时自适应(TTA)，解决了现有方法对熵阈值的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度指标，对预定义的熵阈值敏感，导致许多潜在可靠的样本被忽视。这些样本即使初始熵略超阈值，但在模型更新后可能变得可靠，能提供稳定的监督信息和正常范围的梯度来指导模型适应。

Method: 提出POEM方法，探索之前未被利用的可靠样本。引入额外的Adapt Branch网络，在提取领域无关表示和在目标数据上实现高性能之间取得平衡。

Result: 在多种架构上的综合实验表明，POEM在挑战性场景和真实世界领域偏移中始终优于现有TTA方法，同时保持计算效率。POEM的核心思想可作为增强策略提升现有TTA方法的性能。

Conclusion: POEM通过有效利用之前被忽视的可靠样本，显著提升了测试时自适应的性能，为TTA方法提供了新的改进方向。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [24] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: HydroFusion-LMF是一个统一的小流域日径流预测框架，通过可学习的趋势-季节-残差分解、异构专家集合、水文感知门控和半监督多任务学习，在非平稳条件下实现十年尺度的准确预测。


<details>
  <summary>Details</summary>
Motivation: 解决小流域日径流预测中信号复杂（漂移趋势、多尺度季节周期、机制转换、稀疏极值）以及现有深度模型仅针对单一方面且未充分利用未标记数据的问题。

Method: 采用四步法：(1)可学习的趋势-季节-残差分解降低非平稳性；(2)残差通过紧凑异构专家集合处理；(3)水文上下文感知门控融合专家输出；(4)半监督多任务目标增强监督。

Result: 在约10年日数据集上，MSE为1.0128，MAE为0.5818，比最强基线DLinear分别提升10.2%和10.3%，比平均基线分别提升24.6%和17.1%。

Conclusion: 该框架在可解释性（显式组件、稀疏门控）和性能之间取得平衡，推进了非平稳条件下标签高效的水文预测。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [25] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 提出MASA方法，通过自对齐增强语言模型的元认知能力，证明元认知对齐能显著提升推理性能，并在多个数学和科学推理基准上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知能力，即模型不知道如何思考自己。研究发现真实推理过程与元预测之间存在严重不对齐，对齐这种元认知能力可以带来显著性能提升。

Method: 设计MASA训练流程，通过自生成信号训练元认知能力，无需外部训练数据。通过过滤零方差提示和截断不可能正确的长推理序列来提高训练效率。

Result: 在领域内任务上显著提升准确率和训练效率（GRPO训练加速1.28倍），在AIME25上准确率提升19.3%，六个数学基准平均提升6.2%。在领域外泛化方面，GPQA-Diamond提升3.87%，13个基准平均提升2.08%。

Conclusion: 元认知对齐是提升推理模型性能的关键，MASA方法通过自对齐有效增强元认知能力，在准确率和训练效率方面都取得显著改进，并具有良好的泛化能力。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [26] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 提出了一种在归纳式零样本学习中通过属性选择减少语义空间冗余的方法，包含两种互补的特征选择策略，在多个基准数据集上显著提升了未见类的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 零样本学习中的语义空间通常包含噪声、冗余或不相关属性，这些属性会阻碍模型性能。特别是在归纳式设置中，无法获取未见类的语义信息，使得属性相关性评估更具挑战性。

Method: 1) 提出分区方案模拟未见条件，在无法访问未见类语义信息的情况下评估属性相关性；2) 研究两种互补的特征选择策略：基于嵌入的特征选择(RFS)和基于进化计算的遗传算法(GA)。

Result: 在AWA2、CUB、SUN、aPY、FLO五个基准数据集上的实验表明，两种方法都能通过减少冗余持续提高未见类的准确率。RFS高效且具有竞争力但依赖关键超参数，GA成本更高但能更广泛地探索搜索空间且避免这种依赖性。

Conclusion: 语义空间本质上是冗余的，提出的分区方案是在归纳条件下精炼语义空间的有效工具，两种特征选择策略以互补的方式改进了零样本学习性能。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [27] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络预测机床温度场和热通量场的新范式，通过模块化下游组件实现多种热误差类型的计算和校正，相比传统直接预测热误差的方法具有更好的通用性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统热误差补偿方法依赖于特定误差类型、空间位置或机床配置，限制了通用性和适应性。需要一种更灵活、通用的热误差校正方法。

Method: 使用有限元方法获取数据训练神经网络预测高保真温度场和热通量场，采用基于相关性的测量点选择策略减少硬件需求，并比较了多种时间序列神经网络架构（RNN、GRU、LSTM、双向LSTM、Transformer、TCN）。

Result: 能够准确且低成本地预测温度场和热通量场，为机床环境中的灵活通用热误差校正奠定了基础。

Conclusion: 提出的新范式通过预测温度场和热通量场而非直接预测热误差，实现了更灵活和通用的热误差校正，为机床热误差补偿提供了新的解决方案。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [28] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛Dropout方法，在合并LoRA模块时强制执行正交性以避免语义向量干扰，但实证发现正交性本身不足以实现语义解缠和组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA微调方法在合并多个模块时，语义向量会相互干扰，影响生成质量。

Method: 正交蒙特卡洛Dropout，在合并稀疏语义向量时强制执行严格正交性，不增加时间复杂度。

Result: 理论上和运行时都能保证合并的LoRA保持正交且无直接干扰，但实证分析显示正交性并不能实现语义解缠和组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [29] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 论文探讨文本到图像模型的机器遗忘问题，提出记忆自我再生任务和MemoRa策略，强调知识检索鲁棒性的重要性，并区分短期和长期遗忘机制。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型生成逼真图像的能力可能被滥用于创建有害内容，这推动了机器遗忘研究的发展，但实际遗忘特定概念非常困难。

Method: 引入记忆自我再生任务，提出MemoRa策略作为支持有效恢复已丢失知识的再生方法，强调知识检索鲁棒性作为评估指标。

Result: 研究表明遗忘以两种不同方式发生：短期遗忘（概念可快速回忆）和长期遗忘（恢复更具挑战性）。

Conclusion: 知识检索鲁棒性是开发更强大有效遗忘技术的关键但未被充分探索的评估指标，遗忘机制存在短期和长期的区别。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [30] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 该研究首次系统分析了推理数据在LLM训练不同阶段（预训练vs后训练）的影响，发现预训练阶段引入推理数据至关重要，能建立不可替代的基础能力。


<details>
  <summary>Details</summary>
Motivation: 由于前沿模型的预训练语料不透明，推理数据在不同训练阶段的作用尚不明确，需要研究早期引入推理数据是否比后期微调更有效，以及是否存在过拟合风险。

Method: 系统研究推理数据在规模、多样性和质量上的变化如何影响LLM在不同训练阶段的性能，比较预训练和后训练阶段的数据分配策略。

Result: 预训练阶段引入推理数据带来19%平均增益，建立的基础能力无法通过后期SFT完全复制。预训练最受益于推理模式的多样性（11%增益），而SFT对数据质量更敏感（15%增益）。

Conclusion: 研究挑战了语言建模与推理的传统分离，为在整个训练流程中战略性地分配数据提供了原则性指导，以构建更强大的模型。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [31] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: MindCraft框架通过概念树和谱分解技术，揭示了基础模型中概念的分层涌现过程，能够追踪概念从共享表示到线性可分子空间的演化路径。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在语言、视觉和推理任务中表现出色，但其内部如何组织和稳定概念结构仍不清楚，需要可解释的分析框架。

Method: 基于因果推断构建MindCraft框架，使用概念树结构，通过谱分解技术在各层分析主方向，并将它们连接成分支的概念路径。

Result: 在医疗诊断、物理推理和政治决策等多个领域的实证评估显示，概念树能够恢复语义层次结构、解耦潜在概念，并具有跨领域适用性。

Conclusion: 概念树建立了一个广泛适用且强大的框架，能够深入分析深度模型中的概念表示，是可解释AI领域的重要进展。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [32] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究提出了一种基于变分自编码器(VAE)的新方法，用于检测美国大陆四个AR6区域植物总初级生产力(GPP)中的极端事件，并与传统奇异谱分析(SSA)方法进行比较。


<details>
  <summary>Details</summary>
Motivation: 气候异常显著影响陆地碳循环动态，需要稳健的方法来检测和分析植物生产力的异常行为。

Method: 使用变分自编码器(VAE)架构，包含三个密集层和潜在空间，输入序列长度为12个月，训练在归一化GPP时间序列上重建GPP，并根据重建误差识别异常。极端事件使用第5百分位阈值定义。

Result: VAE和SSA方法在极端事件频率的空间模式上表现出强烈区域一致性，但VAE产生更高的阈值值(179-756 GgC vs. 100-784 GgC)。两种方法都显示到2050-80年，负碳循环极端事件的幅度和频率增加，特别是在西部和中部北美地区。

Conclusion: VAE方法与已建立的SSA技术具有可比性能，同时提供计算优势并增强了对碳循环变异性中非线性时间依赖性的捕捉能力。与SSA不同，VAE方法不需要定义数据中信号的周期性，而是从数据中发现它们。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [33] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT²-LLM是一种针对大语言模型的后训练三值化框架，通过非对称三值量化器和两阶段优化流程，在保持竞争力的性能同时显著降低内存成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但内存和计算需求巨大阻碍部署。三值化作为有前景的压缩技术，在后训练量化场景下潜力尚未充分挖掘，主要面临训练自由参数优化困难以及异常值和权重分散带来的量化挑战。

Method: 提出PT²-LLM框架，核心包括：1）迭代三值拟合（ITF），交替进行最优三值网格构建和灵活舍入以最小化量化误差；2）激活感知网格对齐（AGA），进一步优化三值网格以更好匹配全精度输出；3）基于结构相似性的重排序（SSR）策略，利用列间结构相似性缓解量化难度和异常值影响。

Result: 大量实验表明，PT²-LLM在保持与最先进2位后训练量化方法竞争力的同时，具有更低的内存成本，并能加速预填充和解码过程，实现端到端加速。

Conclusion: PT²-LLM为LLMs提供了一种高效的后训练三值化解决方案，在性能、内存效率和推理速度方面取得了良好平衡。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [34] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 本文提出了首个分析多模态对比学习收敛最优表示的理论框架，证明维度坍缩是模态间隙的根本原因，并提出了消除模态间隙的两种方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明不同模态的表示占据嵌入空间的不同区域（模态间隙），但关于模态间隙如何影响下游性能的实验结果不一致，需要理论分析其成因和影响。

Method: 建立理论框架分析多模态对比学习的最优表示，在不同约束条件下（无约束、锥约束、子空间约束）分析模态间隙的收敛行为。

Result: 证明在无约束或锥约束下模态间隙收敛到零；在子空间约束下模态间隙收敛到两个超平面间的最小角度，维度坍缩是模态间隙的根本原因。

Conclusion: 模态间隙通过影响样本对的对齐来影响下游性能，提出了通过超平面旋转和共享空间投影两种方法实现完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [35] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 提出了通用探索奖励(GEB)框架，解决现有KL和α-散度正则化方法在强化学习人类反馈中偏向保守探索的问题，通过参考依赖的奖励调节实现乐观探索。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法在KL或α-散度正则化下，无意中将探索偏向参考模型的高概率区域，强化了保守行为而非促进不确定区域的发现。

Method: 引入通用探索奖励(GEB)理论框架，通过参考依赖的奖励调节来抵消散度诱导的偏差，统一了先前的启发式奖励作为特例，并自然扩展到完整的α-散度族。

Result: 在多个散度设置和大语言模型骨干上的对齐任务中，GEB始终优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了既有理论原则又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [36] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的扩散语言模型，专门用于代码生成，通过大规模扩散预训练、代码中心的中期训练和指令调优，在保持推理延迟竞争力的同时，在多个代码评估基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有双向上下文和填充能力，但现有系统通常过于笨重，需要开发轻量级且实用的扩散编码器。

Method: 采用大规模扩散预训练，结合代码中心的中期训练和指令调优，使用置信度引导的采样方法来保持推理延迟竞争力。

Result: 在Humaneval、MBPP和EvalPlus基准测试中，CoDA-1.7B-Instruct匹配或超越了参数规模达7B的扩散模型。

Conclusion: CoDA展示了轻量级扩散编码器的可行性，并提供了完整的开源训练流水线和模型检查点，以加速基于扩散的轻量级编码助手研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [37] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 论文提出决策势能面（DPS）概念来近似构建大语言模型的决策边界，通过有限次序列采样实现高效计算，并提供了理论误差分析。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的巨大词汇序列规模和自回归特性，直接构建其决策边界在计算上不可行，需要新的分析方法。

Method: 提出决策势能面（DPS）概念，定义在区分不同采样序列的置信度上，并开发K-DPS算法通过K次有限采样来近似决策边界。

Result: 理论推导了K-DPS与理想DPS之间的绝对误差、期望误差和误差集中度的上界，证明误差可通过采样次数进行权衡。

Conclusion: DPS为分析大语言模型决策边界提供了可行的近似方法，首次实现了主流LLM决策边界的构造，并通过实验验证了有效性。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [38] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 将Transformer架构重新概念化为由主偏微分方程(PDE)控制的连续时空动力系统，揭示残差连接和层归一化是稳定该系统的必要数学稳定器。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在人工智能领域取得了革命性成功，但对其内部机制的理论理解仍然不足，需要建立更严谨的数学分析框架。

Method: 提出新的分析框架，将Transformer的离散分层结构映射为连续动力系统：自注意力对应非局部交互，前馈网络对应局部反应，残差连接和层归一化对应稳定机制。

Result: 实验证明，没有残差连接会导致灾难性的表示漂移，没有层归一化会导致训练动态不稳定和爆炸性增长。

Conclusion: 残差连接和层归一化这些看似启发式的"技巧"实际上是稳定Transformer这一强大但内在不稳定连续系统的必要数学稳定器。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [39] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: 提出了协同信息蒸馏(SID)框架，通过将深度学习重构为局部协同精炼问题，解决了反向传播的更新锁定和高内存消耗问题，实现了并行训练并保持前向推理不变。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播(BP)的两个关键可扩展性瓶颈：更新锁定（网络模块需等待整个反向传播完成）和因存储激活值导致的高内存消耗。

Method: 将深度网络构建为模块流水线，每个模块施加局部目标来精炼对真实目标的概率信念，平衡目标保真度与前序模块信念的一致性，从而解耦模块间的反向依赖。

Result: 理论证明SID保证网络深度增加时性能单调提升，实验表明SID在分类准确率上匹配或超越BP，具有更好的可扩展性和对标签噪声的鲁棒性。

Conclusion: SID作为BP的即插即用替代方案，消除了更新锁定，大幅降低内存需求，同时保持标准前向推理，在准确率、可扩展性和鲁棒性方面表现优异。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [40] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: Quant-dLLM是一个专门为扩散大语言模型设计的超低位后训练量化框架，通过掩码校准模拟、数据感知任意顺序量化器和自适应分块混合精度分配，在2位精度下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案，模型规模持续增长，需要权重压缩部署。但直接将自回归模型的量化方法应用到扩散模型会导致性能不佳。

Method: 提出掩码校准模拟来对齐时间步相关的掩码，数据感知任意顺序量化器通过优化算法学习超低位权重表示，以及自适应分块混合精度分配方案。

Result: 在严格的2位预算下，Quant-dLLM在扩散大语言模型上始终比最先进的自回归迁移量化方法获得更高的准确率。

Conclusion: 该框架为扩散大语言模型的超低位量化提供了有效的解决方案，代码和模型将开源。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [41] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一种用于1位LLM的Sigma-Delta量化框架，通过过采样率(OSR)连续可调和Hadamard权重平滑技术，在极低比特量化下保持语言推理能力，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临显著的计算和内存挑战，需要极低比特量化来高效部署。现有量化方法在极低比特下难以保持模型精度。

Method: 使用上采样结合Sigma-Delta量化器将权重二值化或三值化，采用Hadamard权重平滑减少精度损失，并提出分层细粒度OSR分配策略MultiOSR。

Result: 在OPT和LLaMA模型系列上的实验表明，SDQ-LLM在高度激进的低OSR设置下实现了更高效和高精度的性能。

Conclusion: SDQ-LLM框架成功实现了LLM的极低比特量化，在保持语言推理能力的同时显著提升推理效率，为LLM的高效部署提供了有效解决方案。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [42] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 提出轻量级二次增强器，通过低秩、权重共享和稀疏化技术引入二次变换，增强神经网络非线性能力，在图像分类、文本分类和大语言模型微调任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络主要基于线性变换和非线性激活函数，为进一步增强非线性能力，探索引入二次变换来提升现有架构性能。

Method: 使用低秩性、权重共享和稀疏化技术设计轻量级二次增强器，在每层引入特征间的二次交互，仅增加少量参数和计算量。

Result: 在图像分类、文本分类和大语言模型微调三个任务的概念验证实验中，该方法均显示出明显且显著的性能提升。

Conclusion: 轻量级二次增强器能有效增强神经网络非线性能力，在各种任务中带来实质性性能增益，且参数和计算开销极小。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [43] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 提出了一个可扩展的矩阵自由拉普拉斯框架，用于分解贝叶斯物理信息神经网络中物理约束对损失曲面的影响，并量化各约束的相对重要性。


<details>
  <summary>Details</summary>
Motivation: 需要澄清贝叶斯物理信息神经网络中单个物理约束如何影响网络，因为物理约束可能导致过度自信，这可能是由约束强制带来的合理精度而非校准错误。

Method: 引入可扩展的矩阵自由拉普拉斯框架，将后验Hessian矩阵分解为每个约束的贡献，并提供量化指标来衡量它们在损失曲面上的相对影响。

Result: 应用于Van der Pol方程时，该方法追踪了约束如何塑造网络的几何结构，并直接通过Hessian矩阵展示了改变单个损失权重如何非平凡地重新分布曲率和有效主导地位。

Conclusion: 该方法能够揭示物理约束在贝叶斯物理信息神经网络中的具体影响机制，有助于更准确地解释网络的不确定性和过度自信问题。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [44] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: MemMamba通过状态总结机制和交叉注意力，解决了Mamba模型长距离记忆衰减问题，在保持线性复杂度的同时显著提升长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长序列建模中存在效率与内存的权衡问题：RNN有梯度消失问题，Transformer有二次复杂度限制，Mamba虽然高效但长距离记忆会指数衰减。

Method: 提出MemMamba框架，集成状态总结机制以及跨层和跨token注意力，模仿人类阅读长文档时提炼关键信息的方式。

Result: 在PG19和Passkey Retrieval等长序列基准测试中显著优于现有Mamba变体和Transformer，推理效率提升48%。

Conclusion: MemMamba在复杂度-内存权衡方面取得突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [45] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是首个针对扩散语言模型的系统性缩放定律，涵盖计算受限和数据受限两种机制，并研究了关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型的训练提供短期实践指导，并为整个AI社区带来长期启发。

Method: 开发系统性缩放定律，研究关键建模和优化设计，涵盖计算受限和数据受限两种机制。

Result: 提出了Quokka缩放定律，这是扩散语言模型领域的首个系统性缩放定律。

Conclusion: Quokka是Chinchilla的好朋友，提供了更广泛的研究范围，有望为扩散语言模型训练提供指导。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [46] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出混合归因和剪枝(HAP)框架，结合归因修补的速度优势和边剪枝的忠实性，在保持电路忠实度的同时比基线算法快46%。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法面临基本权衡：归因修补速度快但不忠实于完整模型，而边剪枝忠实但计算成本高。需要一种平衡速度和忠实性的方法。

Method: 使用归因修补识别高潜力子图，然后应用边剪枝从中提取忠实电路。

Result: HAP比基线算法快46%且不牺牲电路忠实度，在间接对象识别任务中保留了归因修补方法在高稀疏度下会剪枝的合作电路组件。

Conclusion: HAP可能是提高机制可解释性研究扩展到更大模型的有效方法。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [47] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个用于在边缘设备上联邦指令调优LLM的可扩展框架，通过结合联邦学习和4位量化低秩适应来解决传统联邦学习方法在LLM上的通信和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）在处理LLM的巨大参数量时失效，需要解决通信和计算开销问题以实现边缘设备的LLM部署。

Method: 结合联邦学习和4位量化低秩适应（QLORA），在物联网领域过滤通用Databricks Dolly 15k数据集进行联邦指令调优。

Result: Edge-FIT调优的Llama 2(7B)模型F1分数达到0.89，使用3.8B Phi-3-mini模型验证了可行的权衡方案。

Conclusion: Edge-FIT被验证为在家庭计算网关上分散式LLM部署的可扩展框架。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [48] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型在实现并行生成和双向注意力方面的固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型相比自回归模型具有并行生成和双向注意力的优势，但现有的开源掩码扩散模型大多基于吸收扩散变体，存在实现这些优势的固有困难。

Method: 分析了掩码扩散模型在实现并行生成和双向注意力方面的技术挑战，并提出了改进的训练和推理策略。

Result: 证明了掩码扩散模型在实现并行生成和双向注意力方面存在固有困难，但通过提出的策略可以更有效地训练和推理。

Conclusion: 掩码扩散模型虽然具有潜力，但在实现并行生成和双向注意力方面存在技术挑战，需要特定的训练和推理策略来优化性能。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [49] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析了Clifford卷积层的内部计算结构，提出了多种优化方法来加速推理过程，同时保持正确性。通过消除冗余矩阵分配和计算，并应用优化技术，最终实现了21.35倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 随着物理科学中对具有等变性特性的网络兴趣日益增长，Clifford神经层作为实现E(n)和O(n)等变性的一种方法而备受关注。本文旨在优化这些层的计算效率。

Method: 首先分析Clifford代数的理论基础以消除冗余矩阵分配和计算，然后系统性地应用已建立的优化技术来进一步提升性能。

Result: 在11个函数上实现了相对于基线实现的21.35倍平均加速，在6个案例中运行时间与原PyTorch实现相当或更快，其余案例中性能与原库处于同一数量级。

Conclusion: 通过理论分析和系统优化，成功显著提升了Clifford卷积层的推理性能，同时保持了计算正确性。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [50] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一个统一的后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，通过镜像下降优化实现，无需更新模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临高昂的计算和内存成本，现有剪枝方法难以平衡效率和鲁棒性：局部方法在高稀疏度下容易崩溃，全局方法则代价昂贵或格式受限。

Method: 使用快速层间评分和轻量级全局控制器分配单一稀疏度预算，支持非结构化和半结构化N:M剪枝，通过镜像下降优化和局部显著性锚定实现。

Result: 在多个预训练LLM家族和标准基准测试中，UniPruning始终提供有竞争力或更优的困惑度和零样本准确率。

Conclusion: UniPruning为大规模型稀疏化提供了一个高效、有原则且可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [51] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种结合XGBoost和神经网络的自适应集成框架，通过元学习和不确定性量化实现动态模型选择与组合。


<details>
  <summary>Details</summary>
Motivation: 开发更智能灵活的机器学习系统，提升预测性能和可解释性。

Method: 使用元学习协同结合XGBoost和神经网络，集成不确定性量化技术和特征重要性分析，实现动态模型编排。

Result: 在多个数据集上展示了优越的预测性能和增强的可解释性。

Conclusion: 该方法为开发更智能灵活的机器学习系统做出了贡献。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [52] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 概念擦除技术在T2I扩散模型中仅创造"失忆"假象，而非真正移除概念。本文提出RevAm框架，通过RL轨迹优化无需修改模型权重即可复活被擦除的概念，暴露当前安全机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法在下一代架构中效果下降，且研究发现这些方法只是偏置采样轨迹而非真正移除概念，这种擦除本质上是可逆的，需要区分表面安全与真实概念移除。

Method: 提出RevAm框架，基于强化学习的轨迹优化方法，通过动态引导去噪过程来复活被擦除的概念，无需修改模型权重。采用Group Relative Policy Optimization适应扩散模型，通过轨迹级奖励探索多样化恢复轨迹。

Result: RevAm实现了优越的概念复活保真度，同时将计算时间减少10倍，成功复活了ESD、UCE、AC等方法擦除的概念。

Conclusion: 当前基于轨迹操纵的安全机制存在严重漏洞，需要开发更鲁棒的擦除技术，不能仅依赖轨迹偏置来实现真正的概念移除。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [53] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 该论文分析机器学习在气候建模中的应用案例，重点关注工作流设计模式，包括替代建模、ML参数化、概率编程等，旨在为科学机器学习提供严谨性框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习在气候建模中的应用面临物理一致性、多尺度耦合等挑战，需要系统分析工作流设计来确保科学严谨性和促进跨学科合作。

Method: 通过分析一系列应用机器学习研究案例，综合不同项目中的工作流设计模式，关注设计选择和流程结构而非技术细节。

Result: 识别并系统化了机器学习在气候建模中的关键工作流模式，展示了这些工作流如何基于物理知识、利用模拟数据并与观测数据整合。

Conclusion: 提出了确保科学机器学习严谨性的框架，强调透明模型开发、关键评估、知情适应和可重复性，有助于降低数据科学与气候建模跨学科合作的门槛。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [54] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的对比桥接方法，通过冻结的单模态编码器和简单的投影头，在不需要大规模多模态预训练的情况下实现化学分子与文本的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型需要大量预训练或多模态语料库，计算成本高昂。本文旨在探索是否可以通过轻量级投影头在冻结的单模态编码器上实现化学和文本表示的对齐。

Method: 使用ChEMBL中的配对机制，通过双线性投影将ECFP4分子指纹与生物医学句子嵌入对齐，采用对比目标训练。为处理共享相同治疗靶点的药物，引入了困难负样本加权和边界损失。

Result: 在基于支架的分割评估中，该方法实现了非平凡的多模态对齐，相比冻结基线显著提高了目标内区分能力。

Conclusion: 薄桥接方法为大规模多模态预训练提供了计算高效的替代方案，能够在精准医学中实现支架感知的药物文本对齐和靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [55] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: 评估LLMs在运营管理中模拟人类行为的能力，发现LLMs能复现大部分假设检验结果但响应分布与人类数据存在差异，轻量级干预措施可改善对齐效果。


<details>
  <summary>Details</summary>
Motivation: LLMs作为模拟人类行为的低成本工具在商业、经济和社会科学中兴起，需要评估其在运营管理领域复制人类行为的能力。

Method: 使用9个已发表的行为运营实验，评估两个标准：假设检验结果复现性和通过Wasserstein距离的分布对齐性，并测试思维链提示和超参数调优两种干预措施。

Result: LLMs能复现大多数假设级效应，捕捉关键决策偏差，但响应分布与人类数据存在差异，包括商业模型；轻量级干预可减少错位，有时让小模型匹配或超越大系统。

Conclusion: LLMs在运营管理中能有效模拟人类行为假设，但分布对齐需要改进，轻量级干预措施可提升性能，为LLMs作为行为研究工具提供实用指导。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [56] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 本文提出了一个包含数据质量参数的扩展缩放定律，将Chinchilla框架扩展到同时考虑模型大小、数据量和数据质量三个维度，通过合成实验验证了数据质量对模型性能和计算效率的重要影响。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型缩放定律只关注模型大小和数据量，但缺乏对数据质量影响的系统研究。本文旨在将数据质量参数化，建立包含质量维度的缩放定律，为大规模预训练中的数据筛选和模型规模平衡提供理论指导。

Method: 引入无量纲数据质量参数Q，基于有效样本量和信息论视角提出质量感知缩放定律。开发两种Q的实用估计器：腐败率代理和缺陷度量。通过神经机器翻译和自回归建模中的合成实验，系统控制数据质量（噪声注入和覆盖变化）来验证定律。

Result: 实验表明损失随数据质量可预测地缩放，高质量数据可显著减小模型规模和计算需求。结果显示有效数据随质量呈次线性衰减，对适度数据腐败具有鲁棒性。样本外评估进一步验证了定律的预测形式。

Conclusion: 本文建立了数据质量的显式、可推广的缩放定律，为大规模预训练中数据筛选努力与模型规模的平衡提供了具体指导，弥补了先前实证分析的不足。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [57] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出基于神经网络的方法，在约10毫秒内从正弦信号中重建数百赫兹频率，比传统傅里叶方法快且精度提高2倍，同时开发自动分类框架识别物理干扰，准确率达99%-100%。


<details>
  <summary>Details</summary>
Motivation: 环激光陀螺仪等设备需要从正弦信号中快速重建频率，传统方法需要数秒数据，无法满足快速触发需求。

Method: 使用神经网络方法进行频率重建，并开发自动分类框架识别信号中的物理干扰。

Result: 在10毫秒内重建数百赫兹频率，精度比傅里叶方法提高2倍；物理干扰分类准确率达99%-100%。

Conclusion: 该方法将人工智能成功集成到地球物理信号分析中，实现了快速频率估计和干扰识别。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [58] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 提出CIC框架来增强强化学习算法的稳定性，通过维护代表性策略和当前策略，选择性更新代表性策略，并使用自适应调整机制共同促进critic训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练过程经常出现严重震荡，导致不稳定和性能下降，需要提高算法稳定性。

Method: CIC框架维护代表性策略和当前策略，仅在当前策略表现更优时选择性更新代表性策略，并采用自适应调整机制让两个策略共同促进critic训练。

Result: 在五个MuJoCo环境上的评估结果显示，CIC能够在不增加额外计算成本的情况下提升传统算法的性能。

Conclusion: CIC框架有效提高了强化学习算法的稳定性和性能，且计算效率高。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [59] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 该论文研究了扩散模型中旋转对齐步骤的作用，发现对齐对应于矩阵Fisher分布的采样模式，是小噪声水平下的零阶近似，并推导了更好的近似方法。


<details>
  <summary>Details</summary>
Motivation: 在点云扩散模型训练中，通常使用随机旋转增强和Kabsch-Umeyama对齐来计算损失，但这种对齐步骤的效果尚未得到充分研究。

Method: 将最优去噪器表示为SO(3)上的矩阵Fisher分布，对齐对应于采样该分布的模，并推导了小噪声极限下更好的近似器。

Result: 实验表明，对于扩散模型训练中最重要的噪声水平，对齐通常是一个'足够好'的近似。

Conclusion: 对齐步骤在小噪声水平下是有效的零阶近似，本文提供了理论解释并推导了改进的近似方法。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [60] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，系统分析了Transformer模型中池化操作的表征能力，并通过实验验证了不同池化策略在多种任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型已成为序列建模的主流架构，但池化操作作为将token表示聚合为固定大小向量的关键步骤，其作用尚未得到充分研究。

Method: 建立理论框架推导池化操作的表征能力闭式边界，并在计算机视觉、自然语言处理和时间序列分析三大模态上进行实证评估。

Result: 研究发现池化选择对模型精度、敏感性和优化行为具有一致影响，为特定任务选择或设计池化机制提供了实践指导。

Conclusion: 池化应被视为Transformer模型的关键架构组件，这为超越注意力机制的更原则性模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [61] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 提出了一个基于多目标强化学习和随机微分方程模拟器的流行病干预框架，能够在疾病控制和社会经济稳定之间进行权衡分析，适用于多种传染病。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了在疾病控制和社会经济稳定之间取得平衡的迫切需求，需要开发能够同时考虑多个竞争目标的干预策略评估框架。

Method: 结合多目标强化学习（MORL）和新的随机微分方程（SDE）流行病模拟器，使用Pareto条件网络（PCN）代理进行训练，模拟器经过全球COVID-19数据校准验证。

Result: 模拟器在重现国家尺度疫情动态方面比其他RL常用模型精度高数个数量级；展示了COVID-19流行病控制与经济稳定之间的直接政策权衡；框架可扩展到不同流行病特征的病原体（如脊髓灰质炎、流感），并发现不同的干预策略。

Conclusion: 该工作提供了一个稳健且适应性强的框架，支持透明、基于证据的公共卫生危机缓解政策制定，能够量化疫苗接种覆盖率下降对干预策略成本的影响。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [62] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 本研究开发了一种结合机器学习和VR技术的飞行员选拔方法，使用SVM和MIC特征选择算法，在眼动追踪和飞行动力学数据上实现了93%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着航空业快速发展，需要大量飞行员，如何以成本效益的方式选拔合适的飞行员成为重要研究问题。

Method: 招募23名中国东方航空飞行员和23名清华新手，应用机器学习和VR技术结合的方法，采用SVM分类器和MIC特征选择算法分析眼动和飞行数据。

Result: SVM+MIC方法在所有指标上表现最佳：准确率0.93，AUC 0.96，F1分数0.93，优于其他四种分类器和两种特征选择方法。

Conclusion: 该研究的VR模拟平台和算法可用于飞行员选拔和训练，提供了首个基于眼动追踪和飞行动力学数据的实现方案。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [63] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm是一种新颖的LLM间通信框架，通过选择性共享KV对实现高效通信，仅传输30%的KV对就能达到与直接合并输入方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统的通信协议存在两大问题：自然语言通信导致高推理成本和信息损失，隐藏状态通信存在信息集中偏差和低效率问题。

Method: 提出KVComm框架，基于注意力重要性分数和高斯先验的KV层选择策略，识别最具信息量的KV对进行通信。

Result: 在多样化任务和模型对上，KVComm仅传输30%的KV对就能达到与直接合并输入方法相当的性能。

Conclusion: KV对作为LLM间通信媒介具有巨大潜力，为可扩展和高效的多智能体系统铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [64] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的多模态LLM框架，通过评估LLM在复杂真实任务中的表现，发现人类专家显著优于现有模型，模型存在幻觉、风险强度过度预测和时空推理困难等问题。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在复杂、高影响力的真实世界任务中的表现，以检验其作为推理智能体的真实准备程度。

Method: 使用多模态LLM端到端处理高分辨率对流允许预报档案中的异质时空数据，在40天期间查询3,625个预报图和40,125个预报探空数据，进行12-36小时预报。

Result: 人类专家显著优于最先进模型，模型表现出强烈幻觉倾向、风险强度过度预测、地理定位不准确，在复杂动态演化系统中时空推理能力差。

Conclusion: AgentCaster旨在推进改进LLM智能体在关键领域挑战性推理任务的研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [65] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: CONCEPTNEURO是一个基于概念的精神疾病诊断框架，利用大语言模型和神经生物学知识自动生成、过滤和编码可解释的功能连接概念，通过概念分类器实现高预测性能和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 近五分之一的青少年患有精神或行为健康问题，迫切需要开发准确且可解释的诊断工具。现有图神经网络方法虽然有效但缺乏可解释性，限制了临床应用的可靠性。

Method: 结合大语言模型和神经生物学领域知识，自动生成功能连接概念，每个概念表示为连接特定脑区的结构化子图，然后通过概念分类器进行预测。

Result: 在多个精神疾病数据集上的实验表明，CONCEPTNEURO增强的图神经网络始终优于原始版本，在提高准确性的同时提供透明、临床对齐的解释。

Conclusion: CONCEPTNEURO建立了精神疾病诊断的可解释、领域知识驱动的框架，概念分析揭示了与专家知识一致且具有研究价值的疾病特异性连接模式。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [66] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 开发了一个基于迁移学习的LSTM框架，利用铝7075-T6合金的轴向疲劳数据来预测高周扭转S-N曲线，显著降低疲劳特性测试成本。


<details>
  <summary>Details</summary>
Motivation: 铝合金易发生疲劳失效，但表征材料疲劳性能（特别是高周数据）耗时且成本高昂，需要开发更高效的方法。

Method: 使用长短期记忆网络(LSTM)构建迁移学习框架，首先基于铝7075-T6合金的纯轴向疲劳数据训练源模型，然后迁移用于预测高周扭转S-N曲线。

Result: 该框架能够准确预测铝合金在更高循环范围内的扭转S-N曲线。

Conclusion: 该框架有望大幅降低获取不同材料疲劳特性的成本，帮助在更好的成本和时间约束下优先安排测试。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [67] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文通过秩结构分析时间序列Transformer，发现时间序列嵌入具有快速衰减的奇异值谱，使得Q/K/V投影可低秩近似，注意力层可压缩。提出流秩概念解释深度增加时秩膨胀现象，并成功压缩Chronos时间序列基础模型，推理时间减少65%，内存减少81%。


<details>
  <summary>Details</summary>
Motivation: 现有从文本模型提炼的Transformer原理在时间序列等模态上迁移效果不佳，需要专门分析时间序列Transformer的结构特性。

Method: 通过秩结构分析时间序列嵌入的奇异值谱特性，证明Q/K/V投影的低秩近似可行性，提出流秩概念解释深度与秩的关系，并基于这些发现进行模型压缩。

Result: 时间序列嵌入具有快速衰减的奇异值谱，注意力层可压缩性与其谱衰减程度成正比。成功压缩Chronos模型，推理时间减少65%，内存减少81%且无精度损失。

Conclusion: 研究为时间序列基础模型的宽度、深度和注意力头分配提供了原则性指导，并揭示了其固有的可压缩性。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [68] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出基于物理信息神经算子(PINO)的深度强化学习框架PINO-PC，用于湍流控制建模，在未见高雷诺数流动中实现39.0%的减阻效果，优于现有方法32%以上。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟评估湍流控制效果计算成本高昂，需要更高效的湍流建模与控制方法。

Method: 基于模型的强化学习预测控制框架，使用物理信息神经算子(PINO)联合学习策略和观测器模型，具有离散不变性并能准确捕捉湍流精细尺度。

Result: PINO-PC在多种高雷诺数未见流动场景中优于先前的无模型强化学习方法，在雷诺数15,000时实现39.0%的减阻效果。

Conclusion: PINO-PC框架为湍流控制提供了一种高效准确的解决方案，在未见高雷诺数流动中显著优于现有方法。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [69] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出一个结合开源GPS轨迹数据、道路网络和卫星图像的数据驱动框架，用于估算车辆运行模式分布和交通排放，相比MOVES基准模型将关键污染物排放估算的RMSE降低了50%以上。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据为城市区域车辆活动和排放估算提供可扩展且透明的基础，实现低成本、可复制的排放估算。

Method: 集成MOVES和开源GPS轨迹数据、OpenStreetMap道路网络、区域交通数据集和卫星图像特征向量，训练神经网络模型预测MOVES定义的运行模式分布。

Result: 在波士顿大都会区45个城市应用该方法，相比MOVES基准模型，关键污染物（CO、NOx、CO2、PM2.5）区域尺度交通排放的RMSE降低了50%以上。

Conclusion: 该研究证明了使用完全开源数据源进行低成本、可复制和数据驱动排放估算的可行性。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [70] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: WindSR是一种用于轮毂高度风速超分辨率降尺度的扩散模型，通过数据同化将稀疏观测数据与模拟场结合，使用动态半径融合方法整合观测与模拟数据，并融入地形信息，在降尺度效率和准确性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 轮毂高度的高质量观测数据在时空上稀疏，而模拟数据虽然广泛可用但存在偏差且分辨率不足，无法满足风电场选址或极端天气风险评估的需求。

Method: 提出WindSR扩散模型，采用数据同化方法将稀疏观测与模拟场结合，引入动态半径融合方法整合数据，并在训练和推理过程中融入地形信息。

Result: 与卷积神经网络和生成对抗网络基线相比，WindSR在降尺度效率和准确性上表现更优，数据同化使模型偏差相对于独立观测减少了约20%。

Conclusion: WindSR通过结合观测和模拟数据以及地形信息，成功实现了高质量、高分辨率的轮毂高度风速降尺度，为风能应用提供了更可靠的工具。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [71] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 该论文通过机械可解释性方法，首次提供了因果证据表明Transformer模型中的记忆和推理能力依赖于可分离但相互作用的神经回路。


<details>
  <summary>Details</summary>
Motivation: 区分记忆和推理能力对于预测模型泛化、设计针对性评估以及构建更安全的干预措施至关重要，但目前尚不清楚这两种能力是否依赖于不同的内部机制。

Method: 使用机械可解释性方法，结合激活修补和结构化消融技术，在受控的合成语言谜题数据集上，在层、头和神经元级别探测Transformer模型。

Result: 在两个模型家族（Qwen和LLaMA）中，发现对特定层和注意力头的干预会导致选择性损伤：禁用识别的"记忆回路"使事实检索准确率降低达15%而推理能力保持完整，禁用"推理回路"则使多步推理能力降低类似幅度。

Conclusion: 研究结果表明记忆和推理依赖于可分离但相互作用的神经回路，这推进了机械可解释性研究，将回路级结构与功能专业化联系起来，并为大型语言模型的安全部署提供机制性见解。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [72] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 提出了一种新的数据自由知识蒸馏方法CPSC-DFKD，通过条件生成对抗网络合成类别特定的多样化图像，改进生成器模块以区分不同类别分布，并引入伪监督对比学习来增强多样性。


<details>
  <summary>Details</summary>
Motivation: 解决当前数据自由知识蒸馏方法中存在的三个问题：缺乏伪监督学习范式、无法区分不同类别样本分布、无法优化类别多样性样本，这些问题限制了学生模型的性能提升。

Method: 使用条件生成对抗网络合成类别特定的多样化图像；改进生成器模块以更好地区分不同类别分布；提出基于教师和学生视图的伪监督对比学习来增强样本多样性。

Result: 在三个常用数据集上的综合实验验证了CPSC-DFKD方法对学生模型和生成器性能的提升。

Conclusion: CPSC-DFKD通过引入条件伪监督对比学习范式，有效解决了现有数据自由知识蒸馏方法的关键限制，显著提升了学生模型的性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [73] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文评估了现有聚类联邦学习算法在数量倾斜非IID数据下的表现，并提出了一种新的迭代算法CORNFLQS，该算法在准确性和聚类质量方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的非IID数据特别是数量倾斜问题是一个关键挑战，现有聚类联邦学习方法缺乏对此问题的系统评估。

Method: 提出了CORNFLQS算法，通过优化协调两种CFL操作策略（客户端选择最小化损失的集群和服务器基于模型相似性分组），并在6个图像分类数据集上进行270种非IID配置实验。

Result: CORNFLQS在准确性和聚类质量方面获得最高平均排名，对数量倾斜扰动表现出强鲁棒性，优于现有CFL算法。

Conclusion: CORNFLQS算法有效解决了联邦学习中的数量倾斜问题，在非IID数据环境下表现出优越性能。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [74] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: 提出STDAE框架，通过跨模态重建预训练解决高速公路匝道缺乏实时检测器的问题，在预测阶段结合GWNet等模型提升精度。


<details>
  <summary>Details</summary>
Motivation: 高速公路互通立交是车辆转换的关键节点，但缺乏实时匝道检测器导致交通预测存在盲区。

Method: 采用时空解耦自编码器两阶段框架：第一阶段从主线数据重建历史匝道流量，捕捉内在时空关系；第二阶段将学习到的表示与GWNet等模型集成。

Result: 在三个真实世界互通立交数据集上的实验表明，STDAE-GWNET持续优于13个最先进基线，性能接近使用历史匝道数据的模型。

Conclusion: 该方法有效克服检测器稀缺问题，具有即插即用潜力，适用于多种预测流程。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [75] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 使用可验证奖励的强化学习(RLVR)训练韩语接龙游戏，通过课程学习缓解规则奖励冲突


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在多语言逻辑谜题中的应用，特别是韩语接龙游戏中规则奖励的自然冲突问题

Method: 使用RLVR方法，结合课程学习方案来缓解规则奖励之间的冲突

Result: 实验证明课程学习能够有效缓解奖励冲突问题

Conclusion: 研究结果鼓励在更多不同语言的谜题任务中开展进一步研究

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [76] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 该论文研究了深度学习训练算法的可靠性和可重复性，特别关注物理信息损失函数在训练网络执行边界条件时的稳定性，通过Pix2Pix网络预测高弹性对比复合材料应力场的案例研究，分析了不同损失函数的收敛变异性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着物理信息损失函数的流行，需要评估损失函数在训练网络执行特定边界条件时的可靠性，报告模型变异对于公平比较不同方法至关重要。

Method: 使用Pix2Pix网络预测高弹性对比复合材料的应力场，实施多种不同的应力平衡损失函数，通过多次训练会话分析各损失函数在收敛、准确性和应力平衡执行方面的变异水平。

Result: 不同的损失函数在收敛、准确性和应力平衡执行方面显示出不同程度的变异，某些损失函数比其他函数更稳定可靠。

Conclusion: 报告模型变异对于评估损失函数可靠性和公平比较不同方法至关重要，论文分享了报告模型变异的建议实践。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [77] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出了一个多任务神经扩散过程（MT-NDP）框架用于风功率预测，通过任务编码器捕捉跨风机相关性，并在真实SCADA数据上进行了首次实证评估。


<details>
  <summary>Details</summary>
Motivation: 不确定性感知的风功率预测对于电网集成和风电场可靠运行至关重要，需要能够提供校准且可扩展的预测模型。

Method: 扩展神经扩散过程（NDPs）到多任务框架MT-NDP，引入任务编码器捕捉跨风机相关性，支持对未见风机的少样本适应。

Result: MT-NDP在点精度和校准方面优于单任务NDPs和高斯过程，特别是对于偏离机群平均行为的风机，提供更尖锐但可信的预测区间。

Conclusion: 基于NDP的模型提供适合运营部署的校准和可扩展预测，其更尖锐但可信的预测区间可以支持现代风电场的调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [78] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出了一种在移动设备上内存高效的反向传播实现方法(MeBP)，能够在内存使用和计算时间之间提供更好的权衡，相比零阶优化方法收敛更快且性能更好。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的移动设备上，即使是使用LoRA等参数子集方法，基于反向传播的LLM微调仍然内存消耗过大，而零阶优化方法虽然内存占用低但收敛速度过慢。

Method: 开发了一种内存高效的反向传播实现(MeBP)，通过优化内存管理策略，在移动设备上实现低内存消耗的LLM微调。

Result: 在iPhone 15 Pro Max上验证了MeBP的有效性，各种0.5B到4B参数的LLM可以使用不到1GB内存进行微调，收敛速度比零阶优化方法快10-100倍。

Conclusion: MeBP为移动设备上的LLM微调提供了实用的解决方案，在内存效率和计算性能之间取得了良好平衡。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [79] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出了广义数量级（GOOMs）方法，扩展传统数量级概念，支持在更大动态范围内进行稳定数值计算，解决了长序列实数运算中的数值下溢/上溢问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习、金融等领域需要在长序列上进行实数复合运算，但传统浮点数容易导致灾难性的数值下溢或上溢问题，限制了计算的可能性和稳定性。

Method: 引入广义数量级（GOOMs）概念，将浮点数作为特例包含在内，实现自定义并行前缀扫描算法，支持在GPU等并行硬件上原生执行。

Result: 在三个代表性实验中表现优异：超越标准浮点数限制的实数矩阵乘积复合运算；并行估计Lyapunov指数谱，速度比之前方法快几个数量级；在深度循环神经网络中捕获长程依赖关系，无需任何稳定化处理。

Conclusion: GOOMs结合高效并行扫描为高动态范围应用提供了可扩展且数值鲁棒的替代方案，使之前被认为不切实际或不可能的计算变得可能和实用。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [80] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出LHGEL框架，通过集成学习方法解决大规模异构图学习挑战，包含批处理视图聚合、残差注意力和多样性正则化三个关键组件，在五个真实异构网络上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模异构图的挑战包括网络规模大、节点和边类型异构、节点特征变化以及复杂的局部邻域结构，集成学习能够自然解决这些问题。

Method: LHGEL框架包含三个关键组件：批处理视图聚合（采样子图形成多个图视图）、残差注意力（自适应加权视图贡献以指导节点嵌入）和多样性正则化（鼓励不同视图嵌入矩阵的表示差异）。

Result: 在五个真实异构网络上的实验结果表明，LHGEL方法始终以显著优势优于最先进的竞争对手。理论研究表明残差注意力缓解了集成学习中常见的梯度消失问题。

Conclusion: LHGEL通过集成学习有效解决了大规模异构图学习的关键挑战，在准确性和鲁棒性方面表现出色，代码和数据集已开源。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [81] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 该论文研究了核变点检测(KCPD)在文本数据中的应用，建立了在m-依赖数据下的理论保证，并通过LLM模拟和实证研究验证了KCPD在文本分割任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的KCPD理论主要基于独立性假设，但现实中的序列数据（如文本）存在强依赖性，需要建立更符合实际的理论保证。

Method: 建立了KCPD在m-依赖数据下的理论保证；使用LLM生成合成m-依赖文本来验证渐近性质；进行了全面的实证研究，将KCPD与现代文本嵌入结合应用于文本分割。

Result: 证明了KCPD在m-依赖数据下的变点数量一致性和位置弱一致性；实证研究表明KCPD在多种文本数据集上优于基线方法；通过Taylor Swift推文案例展示了实际应用效果。

Conclusion: KCPD不仅具有理论可靠性，在实际文本分割任务中也表现出色，为依赖数据的变点检测提供了理论和实践支持。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [82] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 提出使用结构化论证方法为AI系统提供可验证的解释，替代传统的可解释性和LLM生成解释方法，在论证关系分类任务上达到SOTA性能，并应用于多智能体风险评估场景。


<details>
  <summary>Details</summary>
Motivation: 人类思维过程无法直接观察，但社会通过可验证的论证来运作。AI可解释性应遵循这一原则，为利益相关者提供可验证的推理链，而非机制透明度。

Method: 使用结构化论证方法，将LLM文本转换为论证图，通过双极假设基础论证捕获支持/攻击关系，实现自动幻觉检测和验证机制。

Result: 在AAEC数据集上达到94.44宏F1（比先前工作高5.7分），在Argumentative MicroTexts关系分类上达到0.81宏F1（比可比数据设置下的先前结果高约0.07）。

Conclusion: 结构化论证方法能够提供可验证的解释，支持多智能体协作风险评估，并通过测试时反馈实现迭代优化，无需重新训练。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [83] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文通过残差展开定理证明了深度残差网络在数学上等价于浅层网络的隐式集成，揭示了网络深度增加会导致计算路径组合爆炸，从而解释了归一化层的必要性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么深度残差网络（如ResNet和Transformer）如此有效，以及为什么需要归一化层来训练深度模型。

Method: 提出残差展开定理，通过分析残差网络的函数结构，证明深度增加在数学上等价于扩大隐式集成规模。

Result: 发现残差网络具有层次化集成结构，计算路径的组合增长会导致输出信号爆炸，这解释了归一化层的历史必要性。

Conclusion: 残差模块的缩放提供了一种原则性解决方案来控制组合爆炸，同时作为容量控制机制隐式正则化模型复杂度。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [84] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 本文对Adam优化器中动量参数β₁和β₂的理论分析进行了扩展，突破了之前需要β₁=√β₂的限制，提出了适用于β₁≥√β₂和β₁≤√β₂两种情况的新分析框架。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在训练大规模机器学习模型中非常有效，但如何最优设置其动量参数β₁和β₂的理论理解仍不完整。先前的研究要求β₁=√β₂，这无法覆盖更实际的β₁≠√β₂的情况。

Method: 将Adam视为Follow-the-Regularized-Leader算法实例，推导出新的更一般的分析框架，适用于β₁≥√β₂和β₁≤√β₂两种情况。

Result: 新分析结果严格推广了现有边界，且在最坏情况下是紧的。证明对于无意识对手设置β₁=√β₂是最优的，但对于有意识对手是次优的。

Conclusion: 本文提供了对Adam优化器动量参数设置的更全面理论理解，突破了先前分析的限制，为不同场景下的参数选择提供了理论指导。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [85] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: 提出了一个统一的异常检测框架RADF，通过mSelect技术自动选择算法和调参，解决了大规模分布式系统中异常检测的数据量、异构性和根因分析三大挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大规模分布式系统中异常检测面临的三个主要挑战：海量数据处理、时间序列数据集的异构性，以及异常根因分析的困难。

Method: 开发了RADF框架，采用mSelect技术自动为每个用例选择算法和调整超参数，并包含后检测能力以加速问题排查和根因确定。

Result: 在9个公共基准数据集中，RADF在5个数据集上的AUC性能超越了最先进的异常检测模型，其中7个数据集的AUC超过0.85，这是其他模型无法达到的成就。

Conclusion: RADF框架通过自动化算法选择和参数调优，有效解决了大规模分布式系统中的异常检测挑战，并在多个数据集上表现出优于现有方法的性能。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [86] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了有限时域离线强化学习中的策略评估和策略优化问题，提出了在轨迹数据和qπ-可实现性假设下的统计高效学习器。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，仅凭数据覆盖性和qπ-可实现性假设无法实现统计高效学习。最近有工作证明了在轨迹数据下策略优化的可行性，本文旨在解决策略评估问题并改进策略优化的样本复杂度分析。

Method: 在轨迹数据和qπ-可实现性假设下，开发了统计高效的策略评估学习器，并对现有策略优化方法的样本复杂度进行了更严格的分析。

Result: 成功构建了统计高效的策略评估学习器，并证明策略优化的样本复杂度可以通过更严格的分析得到改进。

Conclusion: 在轨迹数据和qπ-可实现性假设下，离线强化学习中的策略评估和策略优化都可以实现统计高效学习，且策略优化的样本复杂度可以进一步优化。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [87] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习算法，通过稳定策略改进目标和鲁棒分布评论家，在18个困难RL任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统策略梯度方法方差高和通过时间反向传播复杂的问题，实现更有效的在线扩散策略训练。

Method: 结合策略改进目标（避免高方差策略梯度和BPTT复杂性）与融合分布RL和裁剪双Q学习的鲁棒分布评论家。

Result: 在Humanoid、Dog、Shadow Hand等18个困难RL任务上达到最先进性能，涵盖密集奖励和目标条件RL场景。

Conclusion: D2AC算法通过稳定学习过程和鲁棒分布评论家设计，在复杂RL任务中表现出色，具有良好的行为鲁棒性和泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [88] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出任务级对比学习新方法，通过任务增强和对比损失改进小样本分类和元学习的跨域泛化能力，在MetaDataset基准上取得优越性能


<details>
  <summary>Details</summary>
Motivation: 现有小样本分类和元学习方法通常在单一数据集上表现良好，但难以跨域泛化，存在精度低、计算成本高、依赖限制性假设等问题

Method: 引入任务级对比性概念，定义任务增强方法，设计任务级对比损失函数，促进任务表示的无监督聚类，可轻松集成到现有算法中

Result: 在MetaDataset基准测试中表现出优越性能，显著提升泛化能力和计算效率，无需任务域的先验知识

Conclusion: 任务级对比学习是一种轻量级方法，能有效解决小样本学习中的跨域泛化问题，提供显著性能提升且不增加额外复杂度

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [89] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: RAPID是一种新的强化学习算法，通过大批量推理和离线策略梯度更新，显著减少小语言模型微调的训练时间，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在微调小语言模型时资源消耗大、训练时间长，需要更高效的算法来优化计算资源利用。

Method: 采用大批量推理和离线策略梯度更新，结合组优势估计和重要性加权估计来纠正离线学习偏差。

Result: 在三个基准测试中，运行时间减少了11%-34%，同时保持相似或更好的准确性。

Conclusion: RAPID算法有效解决了强化学习训练时间过长的问题，为小语言模型的高效微调提供了可行方案。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [90] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF是一种新的安全强化学习方法，通过基于惩罚的优化框架解决LLM安全问题，避免了传统CMDP方法的局限性，提供可证明的安全保证。


<details>
  <summary>Details</summary>
Motivation: 传统CMDP方法在LLM安全平衡上存在两个主要问题：对评分机制敏感且依赖关键词触发，以及双变量调优计算昂贵且无法提供可证明的安全保证。

Method: 引入基于大规模语料训练的成本模型来分配语义安全分数，采用修正的基于惩罚的优化公式，借鉴约束优化中的精确惩罚函数理论。

Result: 实证评估显示CS-RLHF在对抗普通和越狱提示时，性能比最先进的LLM模型响应至少高效5倍。

Conclusion: CS-RLHF通过基于惩罚的优化方法有效解决了LLM安全问题，提供了可证明的安全保证，且计算效率更高。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [91] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI框架，通过顺序学习额外解码器来修正残差误差，提升重建和预测精度


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI在训练中强制实施潜在动力学可能会损害模拟数据的重建精度

Method: 多阶段LaSDI框架，顺序学习额外解码器来修正前阶段的残差误差

Result: 在1D-1V Vlasov方程应用中，mLaSDI始终优于标准LaSDI，实现更低的预测误差和更短的训练时间

Conclusion: 多阶段LaSDI框架能有效提升重建和预测精度，同时减少训练时间

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [92] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: 提出CrossLag注意力机制，通过整合外生数据中的滞后内生信号来改进登革热疫情预测，在24周预测窗口内显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测需要及时公共卫生预警的重大登革热疫情爆发，而疫情通常滞后于气候和海洋异常的重大变化。

Method: 开发CrossLag注意力机制，将外生数据中显著事件背后的滞后内生信号以低参数成本整合到transformer架构中，基于TimeXer模型进行改进。

Result: 在新加坡登革热数据上，所提模型在检测和预测重大疫情方面比TimeXer模型有显著优势。

Conclusion: CrossLag机制能有效利用环境数据中的滞后信号，显著提升重大登革热疫情预测性能。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [93] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 提出了一种统一的方法，通过最小化权重干预来同时实现LLM的敏感信息遗忘和对抗攻击鲁棒性，无需使用分类器且计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要确保隐私保护和安全生成，特别是在敏感信息遗忘和对抗越狱攻击方面。

Method: 采用约束优化方法，通过最小的权重干预使特定词汇集不可达或将部分权重转移到更安全的区域，实现统一处理。

Result: 提出的点式约束干预方法比最大最小干预表现更好，计算成本更低，在对抗最先进防御方法时表现出优越性能。

Conclusion: 该方法能够有效统一处理敏感信息遗忘和对抗攻击鲁棒性，无需分类器且计算效率高。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [94] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出IMMFM框架，通过多时间点联合学习连续随机动力学，使用分段二次插值路径作为流匹配目标，共同优化漂移和数据驱动的扩散系数。


<details>
  <summary>Details</summary>
Motivation: 解决序列数据生成模型在稀疏采样和高维轨迹学习中的困难，传统方法通常将动态学习简化为成对转换。

Method: 采用分段二次插值路径作为流匹配的平滑目标，联合优化漂移和数据驱动的扩散系数，有理论条件支持稳定学习。

Result: 在合成基准和真实世界纵向神经影像数据集上，IMMFM在预测精度和下游任务中优于现有方法。

Conclusion: IMMFM能够捕捉内在随机性，处理不规则稀疏采样，并生成特定对象的轨迹，在多个任务中表现优异。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [95] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统性地比较了不同图神经网络架构在电力系统故障诊断中的性能，发现RGATv2模型在拓扑变化情况下具有最佳泛化能力，F1分数仅下降约12%，而纯RNN模型性能下降高达60%。


<details>
  <summary>Details</summary>
Motivation: 电力配电网故障检测对系统可靠性至关重要，现有方法需要适应电网拓扑的动态变化。当前基于RNN+GNN的方法中，GCN架构可能不是最优选择，需要探索更先进的GNN架构。

Method: 采用RNN+GNN流水线模型，首次将GraphSAGE、GAT和GATv2应用于故障诊断，并与现有的RGCN和纯RNN模型（特别是GRU）进行系统性比较，特别关注模型在不同拓扑设置下的泛化能力。

Result: 在IEEE 123节点配电网上的实验表明，RGATv2具有最优的泛化性能，F1分数在不同拓扑设置下仅下降约12%；纯RNN模型性能下降高达60%；其他RGNN变体性能下降高达25%。

Conclusion: RGATv2架构在电力系统故障诊断中展现出卓越的泛化能力，能够有效应对电网拓扑变化，是部署到不同场景的优选方案。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [96] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出了两种高效的测试时扩展策略TTAug和TTAdapt，利用模型内部特征而非外部监督来提升小型视觉语言模型的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型计算效率高但泛化能力和下游任务性能较弱，现有测试时扩展方法通常计算量大，违背小型模型的资源高效设计目标。

Method: 提出两种测试时扩展策略：(1) TTAug：生成多个增强输入并在token级别聚合输出，无需参数更新；(2) TTAdapt：使用TTAug生成的基于共识的伪标签在推理时调整模型参数。

Result: 在九个基准测试中展示了一致的性能提升，同时保持了适合资源受限环境的计算效率，方法在不同规模模型和不同VLM间具有通用性。

Conclusion: 提出的测试时扩展策略有效提升了小型视觉语言模型的性能，同时保持了计算效率，具有很好的通用性。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [97] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出了BEKAN方法，通过径向基函数和进化框架精确执行边界条件，在PDE求解中优于MLP和B样条KAN


<details>
  <summary>Details</summary>
Motivation: 深度学习求解PDE时神经网络的黑盒特性难以精确执行边界条件，需要开发能严格保证边界条件的方法

Method: 使用三种可组合方法：Dirichlet问题用高斯RBF构造单变量基函数，周期问题用正弦函数构造周期层，Neumann问题用最小二乘公式指导参数演化

Result: 在Dirichlet、Neumann、周期和混合边界值问题的数值实验中，BEKAN在精度上优于多层感知机和B样条KAN

Conclusion: BEKAN增强了KAN在求解PDE问题时的能力，同时满足边界条件，有助于推动科学计算和工程应用的发展

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [98] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出了Latent Mixture of Symmetries (Latent MoS)模型，通过捕捉复杂动态测量中的对称性混合潜在因子，提高动态学习的样本效率和表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设单一全局对称群，并将对称性发现与动态学习分离，导致表达能力有限和误差累积。需要一种能捕捉混合对称性的表达性模型。

Method: Latent MoS模型通过局部且可证明地保持底层对称变换来捕捉对称性支配的潜在因子混合。采用分层架构堆叠MoS块以捕获长期等变性。

Result: 在多种物理系统的数值实验中，Latent MoS在插值和外推任务上优于最先进的基线方法，并提供可解释的潜在表示。

Conclusion: Latent MoS通过混合对称性建模提高了动态学习的样本效率和表达能力，适用于未来几何和安全关键分析。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [99] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer是一个基于Transformer的框架，用于从稀疏、噪声、不规则时空传感器数据中进行无网格场重建，结合数据驱动灵活性和物理约束，在三个基准测试中比基线方法性能提升40%以上。


<details>
  <summary>Details</summary>
Motivation: 现有插值或学习方法难以处理稀疏、噪声、不规则的时空传感器数据，因为它们要么忽略控制PDE，要么无法扩展。需要结合数据驱动灵活性和物理约束的方法。

Method: 使用基于Transformer的框架，通过可学习的速度缩放距离度量构建局部邻域，采用期望最大化风格优化，通过局部Transformer编码器进行预测，并通过自动梯度PDE残差和边界特定惩罚强制物理一致性。

Result: 在三个基准测试（标量各向异性热方程、矢量浅水系统、现实平流扩散污染模拟）中，FieldFormer始终优于强基线方法40%以上，能够从稀疏（0.4%-2%）和噪声（10%）数据实现准确（RMSE<10^-2）、高效且物理一致的场重建。

Conclusion: FieldFormer证明了结合数据驱动灵活性和物理约束的Transformer框架能够有效解决稀疏噪声时空数据的场重建问题，实现高精度和物理一致性。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [100] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出基于虚拟信息素的S-MADRL框架，通过课程学习解决多机器人在狭窄环境中的协调问题，实现去中心化涌现协调行为。


<details>
  <summary>Details</summary>
Motivation: 解决狭窄拥挤环境中多机器人协调的拥堵和干扰问题，受昆虫群体通过环境痕迹实现鲁棒协调的启发。

Method: 结合虚拟信息素建模局部和社会交互的S-MADRL框架，采用课程学习将复杂任务分解为渐进困难的子问题。

Result: 在最多8个智能体的仿真中实现了最有效的协调，机器人自组织形成非对称工作负载分布，减少拥堵并调节群体性能。

Conclusion: 该框架为通信受限的拥挤环境提供了可扩展的去中心化多智能体协调解决方案，涌现出类似自然界的协调策略。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [101] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文对涡扇发动机剩余使用寿命预测中的领域自适应技术进行了系统性综述，提出了专门针对涡扇发动机特性的新分类法，并评估了相关技术在实际数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 涡扇发动机RUL预测在预测性维护中至关重要，但传统数据驱动方法面临数据有限和分布偏移的挑战。领域自适应技术能够从数据丰富的源域向数据稀缺的目标域转移知识，同时缓解分布偏移问题。

Method: 提出了专门针对涡扇发动机的新分类法：基于方法的分类（如何应用DA）、基于对齐的分类（操作变化导致的分布偏移位置）、基于问题的分类（为什么需要特定适应来解决特定挑战）。这种多维视角超越了传统分类方法。

Result: 在涡扇发动机数据集上评估了选定的DA技术，为从业者提供了实用见解，并识别了关键挑战。

Conclusion: 本文为涡扇发动机RUL预测的DA技术发展提供了系统性指导，识别了未来研究方向，以推动该领域的技术进步。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [102] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种专注于逃离局部最小值寻找全局最小值的优化器，通过确定性探索状态空间和交替进行误差最小化与最大化来实现全局优化。


<details>
  <summary>Details</summary>
Motivation: 传统ADAM优化器在随机梯度更新中存在不确定性，容易陷入局部最小值而无法找到全局最优解，需要一种能够有效逃离局部最小值的优化方法。

Method: 首先推导ADAM优化器步长的解析近似，然后提出Hill-ADAM算法，通过在误差最小化和最大化之间交替进行，最大化阶段用于逃离局部最小值，最小化阶段用于收敛到更优解。

Result: 在5个损失函数和12个图像色彩校正实例上进行了测试，验证了Hill-ADAM在逃离局部最小值和寻找全局最小值方面的有效性。

Conclusion: Hill-ADAM通过确定性探索和误差最小化-最大化交替策略，能够有效逃离局部最小值并找到全局最优解，相比传统随机优化方法具有更好的全局优化能力。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [103] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出Neural Bayesian Filtering (NBF)算法，用于在部分可观测系统中维护隐藏状态的分布。NBF结合了经典滤波器的计算效率和深度生成模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测系统中状态估计问题，传统方法在跟踪快速变化、多模态信念时面临粒子贫化风险，需要结合经典滤波效率和深度模型表达能力。

Method: NBF训练找到任务诱导信念的良好潜在表示，将信念映射到固定长度嵌入向量，使用粒子式更新在嵌入空间中计算后验分布。

Result: 在三个部分可观测环境的状态估计任务中验证了NBF的有效性，能够跟踪快速变化的多模态信念并减轻粒子贫化风险。

Conclusion: NBF成功结合了经典滤波器的计算效率和深度生成模型的表达能力，为部分可观测系统的状态估计提供了有效解决方案。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [104] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 使用Llama 3.1-8B-Instruct预处理推文数据，结合三种情感分析方法提取情感特征，与历史股价数据一起训练LSTM模型，显著提高了股票价格大幅波动的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 准确预测短期股票价格变动具有挑战性，因为市场具有固有的波动性且对投资者情绪敏感。本文旨在通过整合推文情感特征和历史股价信息来改进预测性能。

Method: 使用Meta的Llama 3.1-8B-Instruct模型预处理推文数据，采用三种情感分析方法：基于transformer的DistilRoBERTa分类器和两种基于词典的方法（使用NRC资源）。将情感特征与前一日股价数据结合，训练LSTM模型进行预测。

Result: 在TSLA、AAPL和AMZN股票上的实验结果显示，所有三种情感分析方法都提高了预测股价大幅波动的平均准确率。基线模型（仅使用历史股价）准确率为13.5%，基于DistilRoBERTa的股票预测模型表现最佳，使用LLaMA增强情感分析后准确率从23.6%提升至38.5%。

Conclusion: 使用大语言模型预处理推文内容能够增强情感分析的有效性，进而提高预测股票价格大幅波动的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [105] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 本研究探讨了在公共卫生情感分析中，大语言模型的上下文学习如何受到数据投毒攻击的破坏。通过引入微小对抗性扰动（如同义词替换、否定插入和随机扰动），即使在少量支持示例中也能导致高达67%的情感标签翻转。采用谱签名防御方法成功过滤了投毒示例，保持了数据完整性。


<details>
  <summary>Details</summary>
Motivation: 将先前关于ICL投毒的理论研究扩展到公共卫生话语分析这一实际高风险场景，揭示大语言模型在健康相关社交媒体监控中的脆弱性，并探索有效的防御机制。

Method: 在人类偏肺病毒推文数据中引入三种对抗性扰动：同义词替换、否定插入和随机扰动，然后应用谱签名防御方法来过滤投毒示例。

Result: 微小扰动导致高达67%的情感标签翻转；防御后ICL准确率稳定在46.7%，逻辑回归验证达到100%准确率，成功保持了数据集完整性。

Conclusion: 研究强调了ICL在攻击下的脆弱性，以及谱签名防御在构建更可靠的AI系统用于健康相关社交媒体监控中的价值，为稳健的大语言模型部署提供了风险认知和防御方案。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [106] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过迭代单个参数块到固定点来计算输出，形成无限深度、权重共享的网络架构，训练时内存需求恒定。研究表明，隐式模型通过增加测试时计算量可以渐进表达更复杂的映射，其表达能力随测试时计算量扩展，最终匹配更丰富的函数类。


<details>
  <summary>Details</summary>
Motivation: 隐式模型作为新兴模型类别，虽然经验上已知通过分配更多测试时计算可以匹配甚至超过更大的显式网络，但其底层机制仍缺乏理解。本文旨在通过非参数分析来研究隐式模型的表达能力扩展机制。

Method: 采用非参数分析框架，对隐式模型的表达能力进行严格数学表征。证明简单的正则隐式算子通过迭代可以渐进表达更复杂的映射，并证明对于广泛的隐式模型类别，表达能力随测试时计算量扩展。

Result: 在图像重建、科学计算和运筹学三个领域验证了理论：随着测试时迭代次数增加，学习映射的复杂性上升，同时解的质量提高并稳定。

Conclusion: 隐式模型通过测试时计算量的增加实现表达能力的渐进扩展，这为理解隐式模型的性能优势提供了理论基础，并展示了其在多个领域的实际应用价值。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [107] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的深部脑刺激方法，使用可体内测量的脑活动来调整刺激参数，相比传统临床方法能更有效地抑制帕金森病生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的DBS方法依赖无法在患者体内测量的生物标志物，仅适用于脑芯片模拟环境，需要开发使用可体内测量脑活动的实用方法。

Method: 使用TD3强化学习算法，在基底神经节脑区模型上训练RL智能体，根据可测量的脑活动自适应调整刺激频率和幅度参数。

Result: 相比现代临床DBS实现，该方法能更有效地抑制与帕金森病严重程度相关的生物标志物，性能优于标准临床方法。

Conclusion: 该方法为训练针对个体患者需求的个性化RL智能体开辟了可能性，且依赖可在真实环境中测量的信息，具有临床应用潜力。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [108] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出SAFA-SNN方法用于设备端少样本类增量学习，通过稀疏条件神经元动态和零阶优化解决灾难性遗忘和脉冲不可微问题，在多个数据集上表现优异且能耗更低。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要持续学习新类别以保护数据隐私和维持可靠性能，但数据样本不足时面临挑战。现有基于人工神经网络的方法受限于设备资源，而脉冲神经网络具有更低能耗和更好的生物合理性。

Method: 提出SAFA-SNN方法：1）稀疏条件神经元动态，大部分神经元保持稳定，子集保持活跃；2）使用零阶优化处理脉冲不可微性；3）通过子空间投影增强新类别的可区分性。

Result: 在CIFAR100、Mini-ImageNet和三个神经形态数据集上的实验表明，SAFA-SNN优于基线方法，在Mini-ImageNet最后一个增量会话中至少提升4.01%，能耗比基线方法低20%。

Conclusion: SAFA-SNN为设备端少样本类增量学习提供了有效的解决方案，在性能和能耗方面均优于现有方法，具有实际部署价值。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [109] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 使用LLM引导的进化程序合成方法解决准蒙特卡洛方法中的两个长期设计问题：构造低星差异的有限点集和优化Sobol方向数，在2D/3D点集和32维期权定价任务中取得了新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 准蒙特卡洛方法在高维积分中依赖低差异点集和数字序列，但构造最优点集和方向数一直是长期挑战。本文旨在通过程序合成自动化这一设计过程。

Method: 采用两阶段方法：结合构造性代码提案和迭代数值优化，使用LLM引导的进化循环在任务特定适应度下突变和选择代码。

Result: 在2D点集上为N≥40设置了新的最佳基准，在3D中匹配了已知最优解并报告了改进的基准；在Sobol序列优化中，相比广泛使用的Joe-Kuo参数，在32维期权定价任务中持续降低了随机准蒙特卡洛均方误差。

Conclusion: LLM驱动的进化程序合成可以自动化发现高质量的QMC构造，在经典设计最优时恢复它们，在有限N结构重要时改进它们。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [110] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 该论文研究了如何利用AEMO能源价格预测来开发可靠的BESS交易算法，通过分析预测准确性模式创建了基于预测的BESS交易模型，并与无预测的基本算法进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 随着电网波动性增加，虽然预测数据丰富，但其在驱动实际BESS交易决策中的实用价值尚未充分探索，特别是在利用AEMO价格预测开发可靠交易策略方面存在研究空白。

Method: 分析预测准确性模式（基于时间、预测范围和地区差异），创建基于预测的BESS交易模型，并与无预测的基本算法进行基准测试，同时探索机器学习技术来增强AEMO预测。

Result: 开发了新颖的基于预测的BESS交易模型，用于优化套利财务回报，并评估了预测驱动算法相对于无预测算法的性能表现。

Conclusion: 研究成果将为能源市场交易模型的未来改进提供信息，并促进BESS更有效地整合到市场运营中。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [111] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器(SAE)的可解释性与模型行为引导效用之间仅存在弱正相关，提出Delta Token Confidence特征选择方法可显著提升引导性能，但会消除可解释性与效用之间的关联。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器(SAE)的可解释性是否确实能转化为更好的模型行为引导效用，因为当前研究普遍假设可解释特征自然具备良好的引导能力。

Method: 在三个LLM上训练90个SAE，涵盖5种架构和6种稀疏度，使用SAEBench和AxBench分别评估可解释性和引导效用，通过Kendall秩相关系数分析关联性，并提出Delta Token Confidence特征选择方法。

Result: 可解释性与引导效用仅存在弱正相关(tau b≈0.298)，使用Delta Token Confidence方法后引导性能提升52.52%，但可解释性与效用的相关性消失甚至变为负相关。

Conclusion: 可解释性不能作为引导效用的可靠代理指标，最有效的引导特征往往与可解释性特征不同，两者存在明显分歧。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [112] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 提出了一个数据最小化框架，通过优先级队列树搜索在隐私有序转换空间中寻找最优平衡点，量化了在保持任务效用前提下的最小隐私泄露披露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费应用中的快速部署导致个人信息频繁交换，用户为获取有用响应往往过度分享信息，增加了通过记忆、基于上下文的个性化或安全漏洞带来的隐私风险。

Method: 开发了一个正式定义和操作化数据最小化的框架，提出优先级队列树搜索方法在隐私有序转换空间中定位最优平衡点，并在四个数据集上评估了九个LLM的可实现数据最小化程度。

Result: 前沿大型LLM（如GPT-5）在保持任务质量的同时可以容忍更强的数据最小化（85.7%的删减），而较小的开源模型（如Qwen2.5-0.5B）只能容忍19.3%的删减。LLM难以直接预测最优数据最小化，存在偏向抽象的偏见导致过度分享。

Conclusion: 研究不仅揭示了隐私差距，还发现了能力差距：模型可能缺乏对解决任务实际所需信息的意识，这为开发更隐私保护的AI系统提供了重要见解。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [113] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 提出Token Hidden Reward (THR)指标，通过分析token对正确回答概率的影响来指导强化学习训练，可显式控制探索与利用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型推理能力方面取得进展，但如何显式控制训练过程中的探索与利用平衡仍是一个开放问题。

Method: 引入THR指标量化每个token在GRPO框架下对正确回答概率的影响，并基于此设计重加权算法来调节学习信号，偏向探索或利用。

Result: 在多个数学推理基准测试中验证有效：放大正THR值token提高贪婪解码精度（偏向利用），削弱负THR值token提高Pass@K精度（偏向探索）。

Conclusion: THR为RL调优的LLM提供了细粒度动态控制探索与利用的机制，为推理密集型应用的目标微调提供了新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [114] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 提出注意力采样器定义，通过重要性采样方法显著降低传统注意力机制的计算负担，并在流式设置中分析其空间和时间效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模注意力模型的计算挑战，受经典ℓ2采样器和LLM中注意力机制进展的启发。

Method: 基于重要性采样方法，在流式设置中定义注意力采样器，分析其空间和更新时间效率。

Result: 提出的注意力采样器框架具有可扩展性和广泛适用性，能显著降低计算负担。

Conclusion: 注意力采样器为大规模注意力模型提供了高效的计算解决方案，具有理论保证和实际应用价值。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [115] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: 提出Group Policy Gradient (GPG)，一种无评论家的策略梯度估计器家族，使用基于组的蒙特卡洛优势估计器替代学习值函数，在保持PPO剪裁目标结构的同时消除评论家训练成本。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在人类反馈强化学习中成功的启发，旨在消除训练评论家所需的内存、计算和超参数成本，同时保持PPO的性能优势。

Method: 使用基于组的蒙特卡洛优势估计器替代学习值函数，保留PPO的剪裁目标结构，无需训练评论家网络。

Result: 理论证明GPG估计器的一致性，分析偏差-方差权衡，实验表明GPG在标准基准测试中匹配或优于PPO，能更好地利用并行模拟，计算效率更高。

Conclusion: GPG提供了一种高效的无评论家策略梯度方法，在保持性能的同时显著降低计算资源需求，特别适合大规模并行环境。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [116] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一个统一框架，显式建模图数据为图元混合分布，通过图矩聚类识别不同生成机制，并应用于图对比学习和数据增强，在无监督和监督学习中都取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界图数据集通常包含来自多个不同分布的混合种群，但现有的图表示学习方法（如图对比学习和Mixup增强）往往忽略了这种混合结构。

Method: 1) 使用图矩（模体密度）聚类来自相同图元模型的图；2) 提出图元混合感知的Mixup增强(GMAM)；3) 提出模型感知的图对比学习(MGCL)，改进负采样策略。

Result: 在无监督学习中，MGCL在8个数据集上获得最高平均排名；在监督学习中，GMAM在7个数据集中的6个上达到新的SOTA准确率。

Conclusion: 显式建模图的混合分布结构能够显著提升图表示学习性能，提出的方法在多个基准数据集上验证了有效性。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [117] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出了REG优化器，通过用行-列缩放(RACS)操作符替换Muon的矩阵符号函数，解决了Muon优化器在LLM训练中的不稳定性和与AdamW预训练模型不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然能平衡梯度更新方向，但其依赖矩阵符号函数会导致训练不稳定，且与AdamW预训练模型微调不兼容。需要一种更稳定且与现有训练范式兼容的优化器。

Method: 使用行-列缩放(RACS)操作符替代Muon的矩阵符号函数，该操作符基于矩阵平衡理论，以更温和的方式正则化更新步骤，实现更简单的实现和更好的训练动态兼容性。

Result: 在LLM训练中的广泛实验表明，REG优化器不仅比AdamW获得更优的性能和稳定性，而且与AdamW训练范式保持一致，特别是在微调阶段避免了Muon观察到的性能下降。

Conclusion: REG优化器通过RACS操作符提供了一种稳定且与AdamW兼容的替代方案，在保持性能优势的同时解决了Muon的兼容性问题。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [118] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种基于谱滤波的线性强化学习方法，通过自适应正则化参数选择策略，在保证可解释性的同时提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注性能，依赖事后解释来提供可解释性。本文旨在设计一种可解释性导向且性能增强的强化学习方法。

Method: 扩展了基于岭回归的方法，通过谱滤波函数设计自适应正则化参数选择策略，基于偏差-方差权衡原则控制估计误差。

Result: 理论分析建立了参数估计和泛化误差的近似最优界。在快手和淘宝等真实数据集上的实验表明，该方法在决策质量上优于或匹配现有基线方法。

Conclusion: 该方法有潜力弥合强化学习理论与实际决策之间的差距，在管理场景中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [119] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: 提出PFPL方法解决联邦学习中的数据异构问题，通过构建个性化无偏原型和一致性正则化来提升模型性能并降低通信成本


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在异构场景下存在特征分布或标签分布倾斜的问题，而数据异构实际上是提升模型性能的关键因素

Method: PFPL方法为每个客户端构建个性化无偏原型，提供更丰富的领域知识和无偏收敛目标；在本地更新阶段引入一致性正则化，将本地实例与其个性化原型对齐

Result: 在Digits和Office Caltech数据集上的实验验证了方法的有效性，并成功降低了通信成本

Conclusion: PFPL方法能够有效处理联邦学习中的混合异构场景，通过个性化原型和一致性正则化显著提升模型性能

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [120] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出IniLoRA方法，通过改进LoRA的低秩矩阵初始化策略，使其更接近原始模型权重，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然参数效率高，但其零乘积初始化限制了有效激活和利用原始模型权重的能力，成为性能瓶颈。

Method: 提出IniLoRA初始化策略，将低秩矩阵初始化为近似原始模型权重，并引入IniLoRA-α和IniLoRA-β两个变体。

Result: 实验结果显示IniLoRA在多种模型和任务上表现优于LoRA。

Conclusion: IniLoRA通过改进初始化策略有效解决了LoRA的性能瓶颈问题。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [121] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 提出了一种在部分反馈下审计分类器公平性的新方法，其中真实标签仅对阳性分类个体可用。引入了更符合现实成本的新成本模型，并设计了在两种设置下的最优审计算法。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，真实标签通常只对阳性分类个体可用（如贷款申请中只有获批者的还款结果可观测），而传统的公平性审计方法无法有效处理这种部分反馈情况。现有方法在成本效益方面不够理想，需要更符合实际成本结构的审计方案。

Method: 考虑了两种审计设置：黑盒模型（无数据分布假设）和混合模型（特征和真实标签遵循指数族分布的混合）。在黑盒设置下提出接近最优的审计算法；在混合模型设置下设计了利用截断样本学习和最大后验概率预言机的新算法，将球形高斯混合结果扩展到指数族混合。

Result: 在黑盒设置下证明了自然基线方法可能是严格次优的；在混合模型设置下实现了比黑盒情况显著更低的审计成本。算法适用于人口统计均等、机会均等和均等化几率等常见公平性指标。

Conclusion: 提出的算法在真实数据集（如Adult Income和Law School）上表现优异，相比自然基线方法在审计成本方面持续提升约50%，为部分反馈下的公平性审计提供了更有效的解决方案。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [122] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出了NeuroLDS，这是首个基于机器学习的低差异序列生成框架，通过神经网络将索引映射为点，使所有前缀序列都具有最小差异，在多个应用中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统低差异点集构造主要依赖抽象代数和数论方法，而现有的机器学习方法MPMC只能生成点集而无法扩展到低差异序列，限制了在需要序列前缀低差异特性的应用中的使用。

Method: 采用两阶段学习过程：首先通过监督学习近似经典构造，然后通过无监督微调来最小化前缀差异。训练神经网络将索引映射为点，使生成序列的所有前缀都具有最小差异。

Result: NeuroLDS在所有低差异序列构造方法中显著优于先前方法，在数值积分、机器人运动规划和科学机器学习等多样化应用中表现出有效性。

Conclusion: 神经低差异序列展现了巨大的潜力和广泛的重要性，为低差异序列生成提供了新的机器学习解决方案。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [123] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: EvoEngineer是一个基于LLM的系统性代码演化框架，专门用于CUDA内核优化，在91个真实CUDA内核上实现了2.72倍的平均中值加速和69.8%的代码有效性。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化已成为AI性能的关键瓶颈，但现有LLM方法存在生态系统碎片化、问题定义不清晰以及无法满足严格正确性要求的问题。

Method: 首先形式化CUDA内核优化任务，然后建立EvoEngineer框架，设计平衡性能和正确性的优化策略，最后基于该框架实现内核优化系统。

Result: 在91个真实CUDA内核上的实验显示，EvoEngineer实现了2.72倍的平均中值加速，代码有效性达69.8%，最高加速达36.75倍，在56%的操作上实现了超过2倍的加速。

Conclusion: EvoEngineer在性能和正确性之间实现了原则性平衡，在CUDA内核优化方面优于现有方法。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [124] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: 提出了MAGE框架，通过两阶段模型合并与引导解码来解决多目标生成中的控制问题，相比现有方法具有更好的可控性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应多样化用户需求方面存在不足：基于合并的方法提供间接的参数级控制，而基于解码的引导方法需要聚合多个专家模型的logits，空间开销大且依赖单个模型能力。

Method: 两阶段框架：第一阶段动态构建更鲁棒的基模型，合并考虑多目标的骨干模型；第二阶段将显式和隐式价值模型合并为统一引导代理，指导基模型的解码过程。

Result: 实验验证了价值模型中的线性模式连接性，探索了模型合并与预测集成的关系，证明了该方法在可控性、帕累托最优性能和适应性方面的优势。

Conclusion: MAGE框架在可控多目标生成方面优于现有方法，实现了更好的控制能力和性能表现。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [125] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 该论文从理论角度分析Transformer模型参数分配策略，发现早期层在信息提取中的关键作用，揭示了softmax激活的饱和现象，并提出跨层分配注意力头和维度的优化策略。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在各种应用中取得了显著成功，但其模型效率的理论基础仍未被充分探索。本研究旨在探索如何在层间分配模型参数（主要是注意力头和头维度）来平衡表达能力和效率。

Method: 首先从近似角度对早期层在信息提取中的作用进行数学分析，理论表征了在固定参数预算下注意力头数量和头维度之间的权衡关系。此外，发现并证明了softmax激活的饱和行为：持续增加头维度会导致学习误差的收益递减，特别是对于长序列。

Result: 理论和实验都支持饱和模式的存在，表明后续层可以用更少的参数更高效地运行。基于这些洞察，提出了在Transformer各层间分配注意力头和维度的原则性策略。

Conclusion: 该研究为基于Transformer架构的模型效率提供了理论依据，揭示了参数分配的关键模式，有助于设计更高效的Transformer模型。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [126] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 该论文提出了针对重尾奖励的鲁棒批处理多臂老虎机算法，发现在实例无关和Lipschitz设置中，重尾奖励需要更少的批次就能达到接近最优的遗憾，而在实例相关设置中所需批次数量与尾部重尾性无关。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要假设轻尾奖励分布，但许多现实场景如临床试验结果表现出重尾特征，需要填补这一研究空白。

Method: 提出了针对重尾奖励的鲁棒批处理老虎机算法，涵盖有限臂和Lipschitz连续设置。

Result: 揭示了令人惊讶的现象：在实例无关和Lipschitz设置中，重尾奖励需要更少的批次就能达到接近最优的遗憾；而在实例相关设置中，所需批次数量与尾部重尾性无关。

Conclusion: 重尾奖励在批处理老虎机问题中具有独特的批次需求特性，这为实际应用如临床试验提供了重要启示。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [127] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出Curriculum-Augmented GFlowNets (CAGFN)，通过整合课程学习和多目标GFlowNets来生成mRNA序列，解决了传统方法在稀疏奖励和长视野优化中的训练困难。


<details>
  <summary>Details</summary>
Motivation: mRNA序列设计面临巨大组合空间和多个目标优化的挑战，传统生成流网络在稀疏奖励和长视野任务中训练困难。

Method: CAGFN整合基于长度的课程学习，逐步调整最大序列长度，从易到难引导探索；提供新的mRNA设计环境，结合目标蛋白序列和生物目标训练模型。

Result: 在不同mRNA设计任务中，CAGFN提高了Pareto性能和生物合理性，同时保持多样性；比随机采样训练更快达到高质量解，并能泛化到分布外序列。

Conclusion: CAGFN为治疗性序列设计中的GFlowNets应用提供了生物动机设置，在mRNA设计中表现出优越性能。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [128] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 提出了一种检测循环神经网络中稳定和不稳定流形的新算法，用于分析PLRNNs的动力学特性，包括多稳态和混沌行为。


<details>
  <summary>Details</summary>
Motivation: 理解训练后的RNNs如何产生其行为对于科学和医学应用以及可解释AI很重要，特别是分析其动力学特性。

Method: 开发了一种新颖算法来检测PLRNNs中的稳定和不稳定流形，利用ReLU激活函数的特性。

Result: 算法能够追踪不同吸引盆边界，表征多稳态性，发现同宿点以证明混沌存在，并在实际神经记录中验证了有效性。

Conclusion: 该方法为分析RNNs的复杂动力学提供了有效工具，有助于理解其内部工作机制和可解释性。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [129] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 本文提出TROLL方法，用离散可微分信任域投影替代PPO中的clip机制，为LLM奖励微调提供更稳定的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: PPO中的clip机制作为KL信任域的近似过于粗糙，常导致训练不稳定和次优性能，需要更原则性的替代方案。

Method: 使用离散可微分信任域投影，在模型最重要的token logits稀疏子集上施加token级KL约束，平衡计算成本和投影效果。

Result: 在不同数据集、模型家族和优势估计方法上，TROLL在训练速度、稳定性和最终成功率方面始终优于PPO-like clip。

Conclusion: TROLL可作为PPO-like clip的直接替代，不改变模型推理行为，但显著提升RL微调效果。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [130] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本文首次将多智能体强化学习应用于高空气球协调控制，用于分布式区域覆盖任务，展示了QMIX算法在此领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有高空气球多智能体协调方法主要使用确定性方法（如Voronoi分割），在小型团队和局部任务中表现不佳，而多智能体强化学习在此领域尚未被研究。

Method: 扩展RLHAB仿真环境支持多智能体学习，采用QMIX算法结合集中训练分散执行架构，使用包含个体状态、环境信息和队友数据的专门观测空间，以及优先覆盖和鼓励空间分布的层次化奖励。

Result: QMIX在分布式区域覆盖任务中达到了与理论上最优的几何确定性方法相似的性能。

Conclusion: 验证了MARL方法在高空气球协调中的可行性，为更复杂的自主多气球任务奠定了基础，特别是在确定性方法难以处理的场景中。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [131] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出PDNS框架，通过路径测度上的近端点方法解决多模态分布采样中的模式崩溃问题，使用分阶段的渐进学习过程


<details>
  <summary>Details</summary>
Motivation: 传统扩散神经采样器在处理多模态分布时容易发生模式崩溃，特别是当模式之间存在显著势垒时

Method: 基于随机最优控制问题，采用路径测度上的近端点方法，将学习过程分解为一系列简单子问题，使用近端加权去噪交叉熵目标实现

Result: 在连续和离散采样任务中表现出有效性和鲁棒性，包括分子动力学和统计物理中的挑战性场景

Conclusion: PDNS框架通过渐进式路径构建有效解决了多模态分布采样中的模式崩溃问题，促进了跨模式的充分探索

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [132] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON是一种混合离线学习+在线优化方法，用于解决连续过程工厂启动和产品等级转换中的操作自动化问题，克服了标准离线强化学习的分布偏移和价值高估问题。


<details>
  <summary>Details</summary>
Motivation: 工厂启动和产品等级转换依赖少数专家操作员的手动操作，但随着这些专家退休，工厂所有者缺乏执行这些关键操作所需的隐性知识。离线强化学习可以挖掘历史数据来捕获甚至超越人类专业知识，但标准方法在策略超出数据范围时存在分布偏移和价值高估问题。

Method: HOFLON采用混合方法：离线阶段学习(i)表示过去转换可行区域的潜在数据流形和(ii)预测状态-动作对累积奖励的长期Q评估器；在线阶段求解单步优化问题，最大化Q评估器同时惩罚偏离学习流形和操纵变量变化率过大。

Result: 在两个工业案例研究（聚合反应器启动和造纸机等级转换）中，HOFLON不仅超越了领先的离线RL算法IQL，而且平均获得了比历史数据中最佳启动或等级转换更好的累积奖励。

Conclusion: HOFLON展示了超越当前专家能力自动化转换操作的潜力，能够从历史数据中学习并生成优于人类专家表现的策略。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [133] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出FIRE方法，通过Fisher信息来估计和补偿联邦学习中数据碎片化引起的协变量偏移，提升模型在偏移验证集上的性能。


<details>
  <summary>Details</summary>
Motivation: 当训练数据在批次间或地理位置上碎片化时，训练模型会出现性能下降，这主要是由于协变量偏移导致的各数据片段分布与全局训练分布不一致。

Method: FIRE方法通过近似Fisher信息来累积碎片化引起的协变量偏移差异，并将其作为每个数据片段的损失惩罚项，实现可扩展的分布对齐。

Result: FIRE在偏移验证集上比重要性加权基准方法最多提升5.1%，比联邦学习基准方法最多提升5.3%。

Conclusion: FIRE方法能有效处理联邦学习中的数据碎片化问题，通过Fisher信息估计协变量偏移并实现分布对齐，显著提升模型性能。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [134] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER是一个用于流数据中序列分布漂移检测和稳定适应的理论框架，通过指数鞅和Ville不等式提供时间均匀的误报控制保证，并使用Fisher预处理的提示参数更新实现自然梯度下降。


<details>
  <summary>Details</summary>
Motivation: 解决流数据中序列分布漂移的检测和适应问题，确保在协变量漂移下具有统计有效性和几何稳定性的决策。

Method: 构建基于非一致性分数的指数鞅，应用Ville不等式进行时间均匀的误报控制；使用Fisher预处理的提示参数更新实现自然梯度下降。

Result: 在持续漂移下，检测延迟的期望上界为O(log(1/δ)/Γ)，其中Γ反映后漂移信息增益；适应过程在分布流形上实现局部最优更新，最小化KL散度。

Conclusion: M-FISHER为序列决策中协变量漂移下的稳健、随时有效检测和几何稳定适应提供了原则性方法。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [135] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 该研究开发了一种基于ICD-10代码的算法，利用大型语言模型增强的临床路径来恢复电子健康记录中的缺失数据，其准确性与专家图表审查相当，但更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据存在缺失和错误问题，传统图表审查成本高且耗时，限制了可审查的患者数量，需要开发更高效的数据恢复方法。

Method: 使用基于ICD-10代码的路径驱动算法，结合大型语言模型迭代优化辅助诊断列表，通过临床专业知识验证，在100名患者样本上测试不同路径的性能。

Result: 算法恢复的缺失数据量与专家图表审查相当甚至更多，取决于所用路径的质量。在1000名患者的大样本中，使用LLM增强的最终路径算法表现良好。

Conclusion: 临床驱动的算法（通过LLM增强）能够以与图表审查相似的准确性恢复缺失的EHR数据，并可扩展到大规模样本，未来有望扩展到监测数据质量的其他维度。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [136] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: 提出RAPO算法解决RLVR训练中反向KL散度导致的探索受限问题，通过前向KL惩罚和参考策略重加权促进更广泛而聚焦的探索，在数学推理任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在增加采样预算时优势减弱，这源于反向KL散度的模式寻求行为限制了策略在基础模型支持区域内的探索

Method: RAPO算法：(i)使用前向KL惩罚替代反向KL惩罚进行分布外探索；(ii)重加权参考策略实现自适应分布内探索

Result: 在Qwen2.5-3B和7B模型上训练，无需监督微调，在AIME2024和AIME2025评估中持续提升问题解决性能，突破基础模型性能上限

Conclusion: RAPO推动了RLVR在挑战性推理任务中的前沿，能够解决先前难以处理的问题

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [137] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 提出了FedMuon算法，将Muon优化器应用于联邦学习，建立了非凸问题的收敛率，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在多种应用中表现出色，但其在联邦学习中的有效性尚未探索，需要填补这一研究空白。

Method: 提出FedMuon算法，利用正交化更新方向，使其学习率独立于问题特定参数，并能自然处理重尾噪声。

Result: 理论分析显示FedMuon具有多个有利特性，多种神经网络架构上的广泛实验验证了算法的有效性。

Conclusion: FedMuon在联邦学习中表现出色，具有理论保证和实际验证的有效性。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [138] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 研究发现模型和数据集规模联合最优缩放由输出层算子范数这一单一不变量决定，称为范数传递现象。最优学习率和批次大小组合具有相同的算子范数值，且该条件是必要但不充分的。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型和数据集缩放下的最优超参数传递方面已有进展，但缺乏统一解释原理。本文旨在发现控制联合最优缩放的基本规律。

Method: 使用Scion优化器，在多达13亿参数模型和1380亿token数据集上研究最优学习率和批次大小组合，分析算子范数不变性，并测量缩放规则。

Result: 发现最优学习率/批次大小组合始终具有相同的算子范数值，这是必要但不充分条件。测量了Scion优化器的缩放规则，与Adam优化器一致。分层组学习率调优可提升性能。

Conclusion: 输出层算子范数是控制联合最优缩放的关键不变量，为大规模LLM训练动态提供了实用指导，并发布了Distributed Scion实现和两千多次运行日志。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [139] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 提出了BONSAI框架，这是一种利用部分结构知识的鲁棒贝叶斯优化方法，用于处理不确定环境下的网络系统优化问题


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法需要已知问题结构，限制了在高保真仿真环境中的应用；现有鲁棒贝叶斯优化方法忽略结构信息且难以扩展到高维场景

Method: 将目标函数表示为有向图，包含白盒和黑盒组件，利用中间信息；提出基于Thompson采样的可扩展采集函数，使用基于梯度的优化方法

Result: 在合成和真实案例研究中，相比现有仿真鲁棒优化算法，BONSAI能提供更样本高效和更高质量的鲁棒解

Conclusion: BONSAI在复杂工程系统的不确定性感知设计中具有实际优势

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [140] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS框架将大语言模型从数据处理者重新定位为算法专家，通过分析检测器的弱点生成合成程序来创建难以检测的异常，从而增强检测器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测方法依赖对异常模式的假设，在真实场景中表现不一致。虽然LLM具有强大的推理能力，但直接应用于表格异常检测面临异构数据处理困难和隐私风险等挑战。

Method: 提出LLM-DAS框架，利用LLM分析检测器的高层描述来理解其固有弱点，然后生成检测器特定、数据无关的Python代码来合成难以检测的异常，通过增强训练数据将问题转化为更可区分的二分类任务。

Result: 在36个表格异常检测基准测试上的广泛实验表明，LLM-DAS能够持续提升主流检测器的性能。

Conclusion: 通过程序化合成将LLM推理与经典异常检测算法相结合，LLM-DAS提供了一种可扩展、有效且保护隐私的方法来修补现有检测器的逻辑盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [141] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS是一个利用时间序列基础模型预训练知识的新框架，通过提取Chronos模型的嵌入表示，应用局部离群因子和谱分解技术进行异常检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临季节性、趋势、噪声、概念漂移等多重挑战，异常类型多样且罕见导致数据不平衡，需要强大、灵活且可解释的方法。

Method: 提取Chronos时间序列基础模型编码器的嵌入表示，在自相似矩阵上应用局部离群因子和谱分解等离群检测技术来识别异常。

Result: 在MSL数据集上达到SOTA结果，在SMAP和SWAT数据集上表现具有竞争力，超越了专门为异常检测训练的模型，具有超参数鲁棒性和可解释性。

Conclusion: 提倡使用基础模型的预训练表示来进行高效且适应性强的时间序列异常检测。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [142] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出了一种处理聚类数据的广义拟合Q迭代算法，通过引入广义估计方程来处理组内相关性，在医疗健康应用中显著优于标准FQI。


<details>
  <summary>Details</summary>
Motivation: 医疗健康应用中经常遇到聚类数据，标准强化学习方法无法有效处理组内相关性，需要开发能够处理这种数据结构的算法。

Method: 将广义估计方程整合到策略学习中，提出广义拟合Q迭代算法来处理聚类数据中的组内相关性。

Result: 理论证明：当相关结构正确指定时，Q函数和策略估计器具有最优性；当结构误指定时仍保持一致性。实证显示：相比标准FQI，平均减少一半的遗憾。

Conclusion: 广义FQI算法能有效处理聚类数据，在医疗健康应用中显著提升强化学习性能。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [143] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 该论文研究了在线回归问题，当学习者能够获得未来样本的预测信息时。在极端情况下（转导在线学习），作者完全刻画了最小最大期望遗憾与fat-shattering维度的关系，并开发了一个在线学习器，其遗憾随预测质量平滑改进，在预测准确时接近转导在线学习的性能。


<details>
  <summary>Details</summary>
Motivation: 受现实数据流可预测性的启发，研究当学习者能够获得未来样本预测时的在线回归问题，探索如何利用预测信息改进学习性能。

Method: 首先研究转导在线学习（完全知道未来样本序列），然后推广到不完美预测情况。基于转导在线学习的结果，开发了一个在线学习器，其最小最大期望遗憾与最坏情况遗憾匹配，并随预测质量平滑改进。

Result: 完全刻画了转导在线学习的最小最大期望遗憾与fat-shattering维度的关系，建立了转导在线回归与对抗在线回归的分离。开发的学习器在预测准确时性能接近转导在线学习器，显著优于最坏情况遗憾。

Conclusion: 该工作使得在可预测样本下以前不可学习的类变得可学习，与学习增强模型范式相一致，展示了利用预测信息提升在线学习性能的有效性。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [144] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 该论文对具有时变参数的连续深度图神经网络（GNDEs）在无限节点极限下的收敛性进行了严格分析，建立了图神经网络微分方程（Graphon-NDEs）作为其无限节点极限，并证明了GNDE解向Graphon-NDE解的轨迹收敛。


<details>
  <summary>Details</summary>
Motivation: 结合图神经网络的结构归纳偏置和神经ODE的连续深度架构，为图上的动态建模提供可扩展的理论框架，研究GNDE模型在不同规模图之间的可迁移性。

Method: 引入Graphon-NDEs作为GNDEs的无限节点极限，利用图论和动力系统工具，在两种确定性图采样机制下（平滑图采样的加权图和不连续图采样的无权图）证明收敛性并推导显式收敛率。

Result: 建立了GNDE解向Graphon-NDE解的轨迹收敛，获得了显式收敛率，并建立了尺寸可迁移性边界，为将中等规模图上训练的GNDE模型迁移到更大规模图提供了理论依据。

Conclusion: 理论分析和数值实验验证了GNDE模型在不同规模图之间的可迁移性，为实际应用中模型迁移策略提供了理论支持。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [145] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: LLM Chemistry框架用于量化多LLM协作中的协同或对抗行为，通过分析模型间交互依赖关系来推荐最优模型组合，理论分析显示模型异质性、任务类型和复杂度影响协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM协作方法依赖隐式选择和输出评估，缺乏分析模型间是否真正互补或冲突，需要系统化衡量协作效果。

Method: 提出LLM Chemistry框架，形式化LLM间化学作用概念，开发算法量化交互依赖关系，据此推荐最优模型集成组合。

Result: 理论分析表明LLM协作效果在异质模型配置下最明显，受任务类型、团队规模和复杂度影响；在分类、摘要和程序修复任务上的评估验证了任务依赖性效应。

Conclusion: LLM Chemistry可作为多LLM系统的诊断因素和集成推荐的基础，为理解模型协作机制提供理论支撑。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [146] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了8种拟合优度检验在三种流行水印方案中的表现，发现通用拟合优度检验能显著提升水印检测能力和鲁棒性，特别是在低温度设置下文本重复场景中具有独特优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成内容的真实性和完整性引发担忧，水印技术通过嵌入可检测的统计信号来验证内容来源。拟合优度检验作为自然的水印检测工具，但在该领域尚未得到充分探索。

Method: 使用三种开源LLM、两个数据集、不同生成温度和多重后编辑方法，系统评估八种拟合优度检验在三种流行水印方案中的表现。

Result: 通用拟合优度检验能同时提升水印检测能力和鲁棒性，在低温度设置下的文本重复场景中表现出独特优势，这是现有方法未能充分利用的。

Conclusion: 经典的拟合优度检验是LLM水印检测中简单而强大但未被充分利用的工具，值得在水印检测系统中更广泛应用。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [147] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本文提出了类别级影响函数和影响向量，用于量化训练样本对模型在所有类别上的影响，并设计了一个基于线性规划的样本重加权框架，以实现跨多个类别的帕累托性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心学习主要关注"哪些数据对学习模型有益"，而本文研究更基础的问题"学习模型的性能上限是什么"，强调类别级准确性和帕累托改进，确保每个类别都能受益。

Method: 提出类别级影响函数和影响向量来量化训练样本在所有类别上的影响；开发基于线性规划的样本重加权框架，通过重新加权训练样本来实现帕累托性能改进。

Result: 在合成数据集、视觉和文本基准上的广泛实验表明，该方法能有效估计和实现模型在多个感兴趣类别上的性能改进。

Conclusion: 所提出的类别级影响函数和样本重加权框架能够系统地识别模型性能改进潜力，并实现跨类别的帕累托改进，为数据中心学习提供了新的视角和工具。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [148] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出了一种自适应实时标注方法，在医学筛查中动态查询专家意见，减少50%的专家查询量同时保持准确率


<details>
  <summary>Details</summary>
Motivation: 医学筛查中准确标注需要多位专家共识，现有算法无法满足筛查流程的无缝集成需求

Method: 自适应实时标注方法，支持动态标注传入数据，无需专家先验知识，基于实例潜在难度动态查询额外专家，直到达到置信度阈值

Result: 在三个多标注者分类数据集上评估，自适应查询策略减少50%专家查询量，准确率与非自适应基线相当

Conclusion: 该方法能有效减少医学筛查中的标注开销，同时保持标注准确性

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [149] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个24-48小时雷暴停电早期预警模型，使用公开数据源和两阶段模型设计（逻辑门+LSTM回归器），在密歇根州夏季雷暴相关停电预测中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 雷暴驱动的停电难以预测，因为大多数风暴不会造成损害，对流过程发生迅速且混乱，可用的公共数据既嘈杂又不完整。

Method: 使用公开的EAGLE-I停电数据集和METAR天气数据，通过特定参数克里金法保留对流微信号，构建因果时空特征，采用两阶段模型设计（逻辑门+LSTM回归器）减少噪声暴露。

Result: 两阶段模型在所有时间窗口检测到更多参考峰值（如±48小时记录3/4 vs 2/4），在峰值附近显示适度幅度增益（±0-12小时cMASE降低2-3%），但±36-48小时有小损失。

Conclusion: 尽管公开数据存在噪声，但特征驱动的流水线为雷暴停电提供了可操作的、以事件为中心的早期预警，SHAP分析确认了湿度平流和风/阵风前兆的重要性。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [150] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: 提出了SPEAR方法，利用大语言模型进行时间序列异常检测，通过软提示和量化技术解决传统方法在处理变长序列和基于上下文异常时的困难。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理变长时间序列序列和基于上下文的异常时存在困难，而大语言模型的出现为时间序列异常检测提供了新的机会。

Method: 将时间序列数据量化为输入嵌入，与可学习的软提示嵌入结合后输入冻结的LLM，通过交叉熵损失迭代更新软提示，使模型适应时间序列异常检测任务。

Result: 实验结果表明，软提示能有效提升LLM在时间序列异常检测下游任务中的性能。

Conclusion: 软提示有助于LLM有效适应时间序列任务，而量化确保了序列的最优处理，因为LLM设计用于处理离散序列。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [151] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中当基础模型从未采样到正确解决方案时出现的零奖励障碍问题，发现现有方法都无法克服这一障碍，但通过简单的数据干预（添加简单样本）可以有效解决。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习在复杂推理任务中遇到的零奖励障碍问题，即当基础模型从未产生正确答案时，训练会因零梯度而停滞。

Method: 使用图搜索任务评估现有方法（密集奖励、多样性激励、改进信用分配），并测试数据干预方法（在训练集中添加简单样本）。

Result: 实验表明现有方法都无法克服零奖励障碍，但简单的数据干预方法能让模型最终解决原始困难任务，且无需修改RL算法本身。

Conclusion: 数据干预是克服零奖励障碍的有效策略，为RL在复杂推理任务中的应用提供了实用解决方案。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [152] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 该论文建立了离散选择模型与在线学习及多臂老虎机理论之间的联系，提出了具有次线性遗憾界的算法家族、基于广义嵌套logit模型的新对抗性老虎机算法，以及超越softmax的广义梯度老虎机算法。


<details>
  <summary>Details</summary>
Motivation: 连接离散选择模型与在线学习理论，突破传统softmax算法的独立性假设限制，解决动作间相关学习动态的问题。

Method: 提出包含Exp3的广义算法家族，基于广义嵌套logit模型开发对抗性老虎机算法，并引入考虑动作相关性的广义梯度老虎机算法。

Result: 算法具有次线性遗憾界，通过闭式采样概率实现计算效率，在随机老虎机设置中的数值实验证明了其实际有效性。

Conclusion: 所提出的算法结合了灵活的模型规范与计算效率，扩展了梯度老虎机方法的适用范围，能够处理动作间的相关学习动态。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [153] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool是一种新颖的层次池化框架，通过增强模型对簇间连接性的理解来提升图结构数据的分类性能，可与多种池化GNN模型兼容。


<details>
  <summary>Details</summary>
Motivation: 现有的层次池化模型在设计簇分配和粗化策略方面取得了进展，但往往忽略了簇之间的关系。

Method: 提出ICEPool框架，强调簇间连接性的整合，通过理论分析证明其在图重建方面的有效性。

Result: 实验结果表明ICEPool与多种模型兼容，并能提升现有图神经网络架构的性能。

Conclusion: ICEPool通过增强簇间连接性理解，能够产生更全面和鲁棒的图级表示，提升图分类性能。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [154] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 提出局部自然度方法解决多教师推理蒸馏中的响应选择问题，相比全局自然度能更准确选择适合学生的推理轨迹，显著提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 在多教师设置下，现有基于全局自然度的响应选择方法失效，无法为不同学生选择最合适的教师输出，限制了推理蒸馏的效果。

Method: 引入局部自然度方法，通过计算学生在短序列推理步骤上的条件对数概率来评估数据质量，支持教师选择和响应选择。

Result: 局部自然度在数学基准测试上将32B学生模型的准确率比全局选择提升了9.4个百分点，超越了单一最佳教师数据的训练效果。

Conclusion: 局部数据质量评估和数据混合能够实现更有效的推理蒸馏，局部自然度是多教师蒸馏场景下的关键改进。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [155] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 提出了一个连续框架，将Transformer解释为结构化积分-微分方程的离散化，自注意力机制自然表现为非局部积分算子，层归一化被描述为对时间相关约束的投影。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在序列建模领域取得了革命性突破，但其全面的数学理论仍然缺乏。本文旨在建立一个统一的数学框架来解释Transformer的结构和操作。

Method: 采用算子理论和变分视角，将整个Transformer操作嵌入到token索引和特征维度的连续域中，将Transformer视为结构化积分-微分方程的离散化。

Result: 建立了一个原则性和灵活的框架，不仅加深了对Transformer的理论理解，还为架构设计、分析和基于控制的解释提供了新方向。

Conclusion: 这一新解释在深度学习架构和连续数学建模之间架起了桥梁，为可解释和理论基础的神经网络模型的发展贡献了基础性视角。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [156] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 该论文提出了一种将机器学习天气预报模型训练重新解释为弱约束四维变分数据同化问题的方法，通过潜在空间损失函数改进长期预报技能和物理真实性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习天气预报模型将再分析数据视为绝对真值，忽略了物理耦合和空间结构，导致长期预报模糊且物理不真实。

Method: 将模型训练重新解释为WC-4DVar问题，在自编码器学习的潜在空间中计算损失函数，避免显式建模高维模型空间中的再分析误差协方差。

Result: 潜在空间约束的滚动训练相比模型空间损失训练，提高了长期预报技能，更好地保持了精细尺度结构和物理真实性。

Conclusion: 该框架可扩展到异构数据源，使预报模型能够在统一理论框架下联合训练再分析和多源观测数据。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [157] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性复杂度的注意力机制，替代二次复杂度的Softmax Attention，能在单次前向-反向传播中处理数百万token的长上下文。


<details>
  <summary>Details</summary>
Motivation: Softmax Attention的二次时间复杂度在处理长上下文时变得不可行，即使使用优化的GPU内核（如FlashAttention）也无法处理超过约400万token的上下文。

Method: RACE Attention使用锐化的角度（余弦）相似度替代指数核，通过随机投影和软局部敏感哈希（LSH）来近似注意力输出。

Result: 在语言建模、掩码语言建模和文本分类任务中，RACE Attention在保持准确性的同时显著减少了运行时间和内存使用，在NVIDIA GH200 GPU上能处理1200万token，在Intel Xeon Gold 5220R CPU上能处理7500万token。

Conclusion: RACE Attention为当前硬件提供了处理极长上下文窗口的实用且理论完备的机制，有望在实践中得到广泛应用。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [158] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 本文提出了AGRPO算法，这是首个专门为扩散大语言模型设计的理论上有依据的强化学习算法，在数学推理任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型尚未受益于现代后训练技术（如强化学习），传统LLM的算法与扩散框架不兼容，现有dLLM的RL训练缺乏理论基础。

Method: 提出了AGRPO算法，使用蒙特卡洛采样计算无偏策略梯度估计，是首个适用于dLLM的可处理、忠实的策略梯度方法。

Result: 在数学推理任务上取得显著提升：GSM8K任务绝对增益+7.6%，Countdown任务性能提升3.8倍，比diffu-GRPO方法提升1.3倍。

Conclusion: 在线RL算法可以以理论上有依据的方式扩展到扩散LLM，保持理论严谨性和实际有效性，在不同采样步数下都能实现计算与性能的更好权衡。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [159] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出SFP（时空预测即规划）新范式，基于模型强化学习解决时空预测中的随机性和不可微度量问题。通过生成世界模型模拟多样化未来状态，使用波束搜索规划算法探索高回报序列，并通过自训练优化预测模型。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中固有的随机性和不可微度量挑战，传统方法难以直接优化领域关键指标。

Method: 构建生成世界模型进行环境模拟，将基础预测模型作为智能体，使用波束搜索规划算法以不可微领域度量作为奖励信号，通过迭代自训练优化策略。

Result: 显著减少预测误差，在捕获极端事件等关键领域度量上表现出色。

Conclusion: SFP范式通过结合生成建模和规划算法，有效解决了时空预测中的随机性和不可微优化问题，提升了预测性能。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [160] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私多类支持向量机(PMSVM)，通过权重和梯度扰动方法解决传统OvR和OvO方法在差分隐私设置下隐私预算消耗过多的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多类SVM方法在差分隐私设置下存在隐私预算消耗过多的问题，因为OvR和OvO方法需要重复查询每个数据样本来构建多个二元分类器。

Method: 采用all-in-one SVM方法，每个数据样本仅访问一次来构建多类SVM边界；提出权重和梯度扰动方法，并提供严格的敏感性和收敛性分析以确保差分隐私。

Result: 实证结果表明，该方法在多类场景下优于现有的DP-SVM方法。

Conclusion: 提出的PMSVM方法有效解决了多类SVM在差分隐私设置下的隐私预算消耗问题，在保护数据隐私的同时保持了良好的分类性能。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [161] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: RLVR训练存在两阶段动态：初始阶段过度利用高奖励token导致能力边界收缩，后期阶段探索潜在最优token促进能力边界扩展。


<details>
  <summary>Details</summary>
Motivation: 调和关于RLVR对LLM推理能力边界影响的矛盾观点——有些研究认为收缩，有些认为扩展。

Method: 通过理论和实证分析揭示RLVR训练中的两阶段概率质量动态：利用阶段和探索阶段。

Result: 发现利用阶段可能导致能力边界收缩，而进入探索阶段后能促进能力边界扩展。

Conclusion: 两阶段动态解释了矛盾发现，基于此重新审视仅使用相对负梯度延长训练的潜力，为开发更先进推理能力提供基础。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [162] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出了一种名为KOTARO的新方法，通过自适应调整决策边界来解决类别不平衡问题，在严重不平衡条件下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现实世界中的二元分类任务常面临类别不平衡问题，导致预测偏向多数类，在医疗诊断和异常检测等关键领域中正确分类少数类至关重要

Method: KOTARO方法扩展了核密度估计框架，根据局部样本密度自适应调整决策边界，动态调节高斯基函数的带宽以增强对少数类区域的捕捉能力

Result: 在合成和真实世界不平衡数据集上的实验表明，KOTARO优于传统方法，特别是在严重不平衡条件下表现突出

Conclusion: KOTARO有潜力成为解决各种不平衡分类问题的有前景方案

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [163] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种名为变分扩散遗忘(VDU)的机器遗忘方法，能够在数据受限环境中从预训练扩散模型中移除不需要的特征生成能力，同时保持图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了负责任地部署扩散模型，需要防止模型生成不良、暴力和淫秽内容。现有方法在数据受限场景下效果不佳，因此需要开发只需访问部分不良数据子集的遗忘方法。

Method: 基于变分推断框架，使用包含塑性诱导器和稳定性正则化器的损失函数。塑性诱导器降低不良训练数据的对数似然，稳定性正则化器在参数空间正则化模型以防止图像生成质量下降。

Result: 在MNIST、CIFAR-10和tinyImageNet数据集上验证了类别遗忘效果，在Stable Diffusion模型上验证了特征遗忘效果，证明方法在数据受限环境中的有效性。

Conclusion: VDU方法能够在仅访问部分不良数据的情况下，有效从预训练扩散模型中移除不需要的生成能力，同时保持模型性能，为扩散模型的安全部署提供了实用解决方案。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [164] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 论文发现交叉熵缩放定律在超大规模下失效，提出将交叉熵分解为误差熵、自对齐和置信度三个部分，发现只有误差熵遵循稳健的幂律缩放，这解释了为什么交叉熵缩放定律在小规模时准确但在超大规模时失效。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律长期以来指导大语言模型发展，但最近证据表明该定律在超大规模下失效，损失下降速度比预期慢，这给大语言模型开发带来显著困扰。

Method: 引入交叉熵的新分解方法，将其分解为误差熵、自对齐和置信度三个部分，在多个数据集和32个模型上进行广泛实验，涵盖五个数量级的大小。

Result: 只有误差熵遵循稳健的幂律缩放，而其他两个项基本保持不变；误差熵在小模型中占主导地位，但随着模型增大比例减小。

Conclusion: 误差熵缩放定律能更准确地描述模型行为，将在训练、理解和未来开发大语言模型中有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [165] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO通过慢-快策略优化框架解决强化学习训练中的不稳定性和低效探索问题，在数学推理基准上显著优于GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于策略的强化学习算法（如GRPO）在早期训练中面临梯度噪声大、更新不稳定和探索效率低的问题。

Method: 提出SFPO框架，将每个训练步骤分解为三个阶段：在相同批次上进行短快速轨迹内步、重定位机制控制离策略漂移、最终慢速校正。

Result: SFPO在数学推理基准上比GRPO平均提升2.80分，减少4.93倍rollouts，在达到GRPO最佳准确率时减少4.19倍wall-clock时间。

Conclusion: SFPO通过重定位-更新设计有效提升了强化学习训练的稳定性、收敛速度和效率，且与现有策略梯度流程兼容。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [166] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 该论文介绍了在大状态空间中进行离线强化学习的理论，从历史数据中学习策略而无需与环境在线交互。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习在大状态空间中的理论基础，探索在仅使用历史数据的情况下学习有效策略的可能性。

Method: 引入函数逼近的表达性假设（如贝尔曼完备性vs可实现性）和数据覆盖假设（如全策略vs单策略覆盖），分析不同假设下的算法和结果。

Result: 描述了丰富的算法和结果图谱，展示了在不同假设下可实现的样本和计算复杂度保证。

Conclusion: 讨论了该领域的开放性问题以及与相邻领域的联系，为离线强化学习在大状态空间中的理论发展提供了框架。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [167] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种不依赖类别数量的神经网络训练方法，使用预定义向量系统作为目标潜在空间配置，可在类别数极大或未知时训练相同架构的神经网络


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要神经网络参数数量依赖于类别数量，这在类别数极大或未知时限制了应用

Method: 使用预定义向量系统（如An根系统的随机扰动向量）作为目标潜在空间配置，通过匹配神经网络预测与预定义向量来训练编码器和视觉变换器

Result: 成功在Cinic-10和ImageNet-1K数据集上训练编码器和ViT，并在128万类数据集上验证了方法的有效性

Conclusion: 该方法适用于类别数极大的场景，并在持续学习和神经网络蒸馏方面有潜在应用

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [168] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了处理部分多标签学习和互补多标签学习的一致方法，无需依赖标签生成过程的准确估计或均匀分布假设，通过一阶和二阶策略构建无偏风险估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的部分多标签学习和互补多标签学习方法需要准确估计候选标签或互补标签的生成过程，或假设均匀分布来消除估计问题，但这些条件在现实场景中通常难以满足。

Method: 提出基于一阶和二阶策略的无偏风险估计器，构建统一框架处理两种弱监督多标签分类问题，理论上证明了与多标签分类评估指标的一致性。

Result: 理论分析证明了所提风险估计器的一致性，并推导了估计误差的收敛速率；大量实验验证了所提方法相对于最先进方法的有效性。

Conclusion: 提出的方法在无需依赖标签生成过程准确估计或均匀分布假设的条件下，能够有效处理部分多标签学习和互补多标签学习问题，在理论和实验上都表现出优越性能。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [169] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文分析了深度学习模型在时间序列外推预测中的性能下降问题，提出了一个表征统计学习模型在训练域外预测能力的基本属性，揭示了神经网络与物理定律在结构上的根本差异。


<details>
  <summary>Details</summary>
Motivation: 受基础模型在语言建模中显著成功的启发，开发时间序列预测的基础模型具有变革性意义。虽然基础模型在短程预测中表现优异，但在外推或长程预测中甚至无法超越简单基线，这与物理定律的强外推特性形成鲜明对比。

Method: 识别并形式化了一个表征统计学习模型在训练域外预测能力的基本属性，通过理论分析和实证研究展示了该属性对当前深度学习架构的影响。

Result: 研究结果阐明了外推差距的根本原因，揭示了深度学习模型在外推设置中性能恶化的机制。

Conclusion: 这项工作不仅澄清了外推差距的根源，还为设计能够掌握外推能力的下一代预测模型指明了方向。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [170] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯统计的LLM不确定性量化方法，通过训练多个贝叶斯线性模型来预测LLM各层输出，从而改进多选择结构生成任务中的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多选择结构生成任务中的不确定性量化仍以最大softmax分数为主，存在不足，需要更可靠的不确定性估计方法。

Method: 训练多个贝叶斯线性模型，每个模型预测LLM某一层的输出基于前一层输出，通过层间后验分布推断全局不确定性水平。

Result: 在不同LLM上的数值实验显示，该方法相比现有最先进基线方法有持续改进。

Conclusion: 贝叶斯统计方法能够有效改进LLM不确定性量化，即使使用最简单的线性回归模型也能获得性能提升。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [171] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 提出基于Wasserstein投影的回归模型公平性测试框架，通过假设检验和最优数据扰动方法检测和改善公平性，在保持准确性的同时提升模型公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性研究主要集中在分类任务，回归模型的公平性问题研究不足，需要专门的公平性测试框架。

Method: 使用Wasserstein投影框架进行公平性测试，提出假设检验方法和最优数据扰动技术，理论分析包括公平性标准分类、对偶重构和渐近界限推导。

Result: 在合成和真实数据集上的实验表明，该方法比基于置换的测试具有更高特异性，能有效检测和缓解学生成绩预测、房价预测等实际应用中的偏差。

Conclusion: 该Wasserstein投影框架为回归模型提供了有效的公平性测试和改善方法，填补了回归模型公平性研究的空白。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [172] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文建立了半自动机在统一分布下的首个统计查询硬度结果，证明当字母表大小和输入长度都是状态数的多项式时，半自动机存在统计查询硬度。


<details>
  <summary>Details</summary>
Motivation: 半自动机在自然语言处理、机器人学、计算生物学和数据挖掘中有广泛应用，但之前缺乏对其统计查询复杂度的理解。本文旨在填补这一空白，研究半自动机的学习难度。

Method: 通过将区分两个半自动机最终状态的任务转化为研究在对称群S_N×S_N上的随机游走行为，应用傅里叶分析和对称群表示论工具，获得紧致的谱隙界限。

Result: 证明在状态数的多项式步数后，不同的半自动机变得几乎不相关，从而得到硬度结果。与确定性有限自动机不同，该硬度源于半自动机的内部状态转移结构而非所识别的语言。

Conclusion: 半自动机的统计查询硬度可以仅从其内部状态转移结构推导出来，为理解这类序列处理算法的学习复杂度提供了新的视角。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [173] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI是一个用于冷启动药物-靶点相互作用预测的框架，通过关注蛋白质多级结构来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只使用蛋白质的一级结构，无法捕捉涉及高级结构（如二级、三级、四级结构）的相互作用，而蛋白质的多级结构都会影响药物-靶点相互作用。

Method: 采用分层注意力机制挖掘蛋白质多级结构（从一级到四级）与药物结构在局部和全局粒度上的相互作用，然后利用挖掘到的相互作用融合不同层次的结构表示进行最终预测。

Result: 在基准数据集上的实验表明，ColdDTI在冷启动设置下始终优于先前的方法。

Conclusion: 该框架能够捕捉生物学上可转移的先验知识，避免了过度依赖表示学习带来的过拟合风险。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [174] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文研究了Transformer中位置嵌入在长度泛化中的作用，提出了线性表示复杂度和序列表示复杂度的概念，并开发了Scale Hint和学习型位置嵌入框架来提升长度泛化性能。


<details>
  <summary>Details</summary>
Motivation: 位置嵌入显著影响Transformer的长度泛化性能，但其根本作用机制尚不清楚，需要深入理解位置嵌入在实现长度泛化中的局限性和能力。

Method: 理论分析位置仅线性注意力中的位置嵌入，引入线性表示复杂度；扩展到实际Transformer，提出序列表示复杂度假设；开发Scale Hint实例缩放和学习型位置嵌入框架。

Result: 分析表明位置嵌入不扩展计算能力但结构化跨位置学习计算；实证支持序列表示复杂度跨尺度不变性假设；提出的方法在各种推理任务中提升了长度泛化性能。

Conclusion: 研究为理解Transformer中位置嵌入在长度泛化中的作用提供了理论洞见，并提出了实用的改进策略，有助于提升模型在序列长度变化时的泛化能力。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [175] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出了傅里叶常微分方程(FODEs)，通过在傅里叶域建模来解决神经ODE在时间序列建模中的局限性，能够更好地捕捉长期依赖和周期模式。


<details>
  <summary>Details</summary>
Motivation: 神经ODE在建模时间序列数据时面临两个主要挑战：1) 时间域表示难以捕捉长期依赖和周期结构；2) 连续时间模型与离散观测数据之间的不匹配导致粒度损失和预测精度下降。

Method: 使用快速傅里叶变换将时间序列数据转换到频域，在傅里叶域中嵌入动态模型，并引入可学习的逐元素滤波机制来对齐连续模型输出与离散观测。

Result: 在多个时间序列数据集上的实验表明，FODEs在准确性和效率方面都优于现有方法，能够有效捕捉长期和短期模式。

Conclusion: FODEs通过傅里叶域建模提供了一个强大的时间序列动态建模框架，能够更好地揭示全局模式和周期行为。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [176] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: 本文提出了PhaseFormer，一种基于相位视角的高效时间序列预测方法，通过紧凑的相位嵌入和轻量级路由机制，在仅使用约1k参数的情况下实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch的深度学习方法虽然通过将patch作为基本token增强了周期性的利用，但存在参数多、计算成本高的效率瓶颈。本文首次明确解释了patch级处理效率低下的原因，并提供了真实数据的证据支持。

Method: 引入相位视角建模周期性，提出PhaseFormer方法。该方法通过紧凑的相位嵌入实现相位级预测，并通过轻量级路由机制实现高效的跨相位交互。

Result: 大量实验表明，PhaseFormer在仅使用约1k参数的情况下，在基准数据集上实现了最先进的性能。特别是在大规模和复杂数据集上表现出色，而同等效率的模型往往难以胜任。

Conclusion: 这项工作标志着向真正高效有效的时间序列预测迈出了重要一步，为处理周期性时间序列提供了一种既高效又有效的解决方案。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [177] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 提出一种结合神经常微分方程(NODE)与底层流形学习的新方法，通过结构保持编码器发现数据流形，显著提升高维系统下的计算速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统NODE在高维系统中存在计算量大和截断误差高的问题，而现有方法需要已知流形知识进行投影或隐式变换，这在现实场景中通常不可用。

Method: 使用结构保持编码器处理数据并发现底层图来近似流形，将NODE学习与流形结合，提出新颖的流形约束ODE方法。

Result: 在多个数据集上的实验表明，该方法在准确性、函数评估次数(NFEs)和收敛速度方面均优于现有基线方法。

Conclusion: 该方法有效解决了高维数据集的挑战，在计算速度和精度方面表现出优越性能。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [178] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对自回归语言模型(ARMs)和扩散语言模型(DLMs)进行了全面的性能对比研究，分析了两种架构在算术强度、上下文扩展、批量推理等方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散语言模型(DLMs)作为并行生成文本的替代架构显示出潜力，但其相对于主流自回归语言模型(ARMs)的性能影响尚未完全理解，需要进行系统性的性能分析。

Method: 采用理论分析和性能剖析数据相结合的方法，对比ARMs和DLMs的性能特征，并探索了块状解码的DLMs方法。

Result: 研究发现DLMs由于能在序列长度上利用并行性而具有更高的算术强度，但在长上下文扩展方面表现不佳；块状解码的DLMs能提高算术强度并保持良好扩展性；在批量推理中ARMs具有更高的吞吐量。

Conclusion: 减少采样步骤对于提升开源DLMs的延迟性能至关重要，DLMs在特定场景下具有替代ARMs的潜力，但需要进一步优化。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [179] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 提出了首个用于长期平均成本和不等式约束的自然critic-actor算法，并提供了非渐进收敛保证，在Safety-Gym环境中表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注无约束的折扣成本设置，缺乏针对长期平均成本和不等式约束的critic-actor算法及其非渐进收敛分析。

Method: 设计了具有函数逼近的自然critic-actor算法，适用于长期平均成本和不等式约束设置，并提出了改进版本以提升样本复杂度。

Result: 建立了最优学习率，在三个不同的Safety-Gym环境中与其他知名算法相比表现出竞争力。

Conclusion: 该算法填补了长期平均成本和约束设置下critic-actor方法的空白，提供了理论保证并在实践中验证了有效性。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [180] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出了一种名为谱对齐（SA）的新指标，通过监测层输入与权重矩阵主奇异向量之间的分布对齐来预测训练发散，比传统标量指标更早更清晰地预警损失爆炸。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的损失爆炸可能使数百万美元的训练运行失效，而传统的权重和梯度范数等监控指标往往滞后且模糊，难以建立统一的失败检测标准。

Method: 引入谱对齐（SA）指标，监测层输入与权重矩阵主奇异向量之间的分布对齐，当这种对齐的符号多样性崩溃时，可作为表示崩溃和训练发散的早期预测器。

Result: 在语言模型上的实证结果表明，监测SA分布比传统标量指标能显著更早、更清晰地预警损失爆炸。

Conclusion: SA具有较低的计算开销，是保护模型训练的实用工具，为训练监控提供了更可靠的早期预警机制。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [181] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种端到端的自适应联邦学习方法，通过将联邦学习建模为动态系统，自适应选择客户端和中央服务器的学习率和动量参数，无需手动调参即可实现快速稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决异构联邦学习中超参数选择困难的问题，传统手动调参过程耗时且计算成本高，特别是在客户端计算能力不同、数据分布非独立同分布的情况下。

Method: 将联邦学习建模为动态系统，借鉴数值模拟和物理设计原理，通过临界阻尼选择动量参数实现快速稳定收敛，自适应选择学习率满足数值模拟精度要求。

Result: 该方法能够处理异构联邦学习的关键挑战（目标不一致性和客户端漂移），相比现有自适应方法具有更优的收敛性能，且对全局超参数选择不敏感。

Conclusion: 提出的自适应联邦学习框架消除了客户端和服务器更新的超参数调优需求，适合快速原型设计和可扩展部署，在异构联邦学习中表现出色。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [182] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN是一个理论框架，为Kolmogorov-Arnold Networks（KANs）提供压缩的形式化保证，通过多面体区域合并实现最小化压缩并控制误差。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有更好的可解释性和数学基础，但参数效率低限制了实际部署，需要有效的压缩方法。

Method: 利用KANs固有的分段多项式结构，将压缩问题建模为最优多面体区域合并，开发动态规划算法保证在指定误差范围内实现最小压缩。

Result: 理论分析表明PolyKAN在严格误差控制下实现可证明的最小压缩，且在所有网络参数上具有多项式时间复杂度。

Conclusion: 该框架为KAN压缩提供了首个具有数学保证的形式化基础，为可解释神经架构的高效部署开辟了新方向。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [183] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 本文首次机制性地解释了在低精度设置下使用Flash Attention训练时导致灾难性损失爆炸的长期未解决问题，揭示了低秩表示和低精度算术的偏置舍入误差相互作用的恶性循环机制，并提出了一个最小修改方案来稳定训练。


<details>
  <summary>Details</summary>
Motivation: 追求计算效率推动了在训练transformer模型时采用低精度格式，但这种进步常常受到臭名昭著的训练不稳定性的阻碍。本文旨在解决在低精度设置下使用Flash Attention训练时导致灾难性损失爆炸的长期未解决问题。

Method: 通过深入分析揭示了失败机制，并引入对Flash Attention的最小修改来减轻舍入误差的偏置。该方法通过机制性分析识别问题根源，然后设计针对性解决方案。

Result: 研究发现失败不是随机伪影，而是由注意力机制中相似低秩表示的出现与低精度算术固有的偏置舍入误差的复合效应共同引起的。这些因素创建了误差累积的恶性循环，最终破坏训练动态。

Conclusion: 提出的简单修改稳定了训练过程，证实了分析的正确性，并为这个持续存在的问题提供了实用解决方案，证明了通过理解底层机制可以设计有效的稳定策略。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [184] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出了MLLMEraser，一种无需训练的输入感知遗忘框架，通过激活引导实现动态知识擦除，解决了MLLMs中隐私数据、过时知识和有害内容的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在记忆隐私数据、过时知识和有害内容的问题，现有遗忘方法计算成本高、不可逆且会扭曲保留知识。

Method: 通过对比对抗性扰动的知识回忆和知识擦除图像-文本对构建多模态擦除方向，并设计输入感知引导机制自适应决定何时应用擦除方向。

Result: 在LLaVA-1.5和Qwen-2.5-VL上的实验表明，MLLMEraser在遗忘性能上优于现有基线，计算成本更低且效用退化最小。

Conclusion: MLLMEraser提供了一种高效、无需训练的多模态大语言模型知识擦除方法，在保持保留知识效用的同时有效擦除指定内容。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [185] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET是一个SE(3)-等变的神经网络架构，用于学习多体系统中的全对相互作用，在3D动力学预测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法依赖显式观测结构，无法捕捉复杂物理行为中至关重要的未观测相互作用。

Method: 提出基于能量函数最小化轨迹的物理启发注意力网络和保持等变性的并行解码器。

Result: 在人体运动捕捉、分子动力学和蛋白质模拟等基准测试中，相比最新模型实现4.7%到41.5%的误差降低。

Conclusion: PAINET通过捕捉未观测相互作用，在3D动力学建模中取得了显著改进，计算成本相当。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [186] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的核随机梯度下降算法，通过创新的正则化策略提高大规模监督学习的效率和可扩展性，在优化和泛化性能方面达到最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 传统核SGD在处理大规模数据时效率低下且计算成本高，需要一种既能保持核方法优势又能提高计算效率的新方法。

Method: 利用球面径向基函数的无穷级数展开，将随机梯度投影到有限维假设空间，并基于核诱导协方差算子的谱结构估计建立统一分析框架，结合线性SGD的坐标更新降低计算复杂度。

Result: 证明了最后迭代和后缀平均都以极小极大最优速率收敛，在再生核希尔伯特空间中建立最优强收敛，显著降低计算复杂度和存储复杂度。

Conclusion: 所提算法在保持核方法优势的同时显著提高了计算效率，适用于多种经典损失函数，能够高效处理流式数据。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [187] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 提出了一种名为DAD-SGM的新蒸馏方法，使用去噪扩散模型作为教师助手，将自监督图神经网络的知识蒸馏到轻量级多层感知机中，以解决两者在容量上的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 在大规模应用中，用轻量级MLP替代GNN的需求日益增长。但在自监督图表示学习中，将GNN蒸馏到MLP更具挑战性，因为自监督学习的性能更依赖于模型的归纳偏置。

Method: 使用去噪扩散模型作为教师助手，帮助将教师GNN的知识更好地蒸馏到学生MLP中，增强MLP在自监督图表示学习中的泛化能力和鲁棒性。

Result: 大量实验表明，DAD-SGM相比最先进的GNN-to-MLP蒸馏方法，能更有效地蒸馏自监督GNN的知识。

Conclusion: 该方法成功解决了自监督图表示学习中GNN与MLP之间的容量差距问题，为大规模应用提供了高效的轻量级解决方案。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [188] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 提出了一系列基于分数引导的混合策略因果搜索算法，改进FCI算法在存在隐变量或选择偏差时的性能，包括BOSS-FCI、GRaSP-FCI、FCIT和LV-Dumb等方法。


<details>
  <summary>Details</summary>
Motivation: FCI算法在处理隐变量或选择偏差时需要进行大量条件独立性测试，导致虚假独立性声明、多余或缺失边以及不可靠的方向确定，需要更高效可靠的方法。

Method: 开发了BOSS-FCI和GRaSP-FCI作为GFCI的变体，用BOSS或GRaSP替代FGES；提出FCIT方法用BOSS引导的定向测试替代穷举测试；还提出了LV-Dumb启发式方法直接返回BOSS DAG的PAG。

Result: 模拟和真实数据分析表明，BOSS-FCI和GRaSP-FCI提供了可靠基线，FCIT提高了效率和可靠性，LV-Dumb在实践中表现出色。

Conclusion: 这些方法突出了分数引导和定向策略在可扩展隐变量因果发现中的价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [189] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了一种基于影响分支和Thompson采样的在线学习方法来优化混合整数规划的求解过程，在分支定界算法的前几次迭代中应用图导向的变量选择策略。


<details>
  <summary>Details</summary>
Motivation: 为第20届混合整数规划研讨会计算竞赛开发新的在线学习方法，旨在提高MIP求解效率并适应更一般的在线框架。

Method: 使用影响分支作为新的图导向变量选择策略，在分支定界算法前几次迭代中应用，并通过Thompson采样在线优化分支启发式，根据计算速度提升对MIP结构的最佳图表示进行排序。

Result: 取得了与最先进在线学习方法相当的结果，表明该方法能很好地泛化到更一般的在线框架，包括约束矩阵、约束向量和目标系数的变化，且在有更多样本时表现良好。

Conclusion: 影响分支结合Thompson采样的方法在MIP在线求解中表现优异，具有良好的泛化能力，适用于各种参数变化的在线场景。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [190] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出了PO-MPC框架，将基于MPPI的强化学习方法统一为KL正则化的模型基强化学习方法，通过将规划器的动作分布作为策略优化的先验来对齐学习策略与规划器行为。


<details>
  <summary>Details</summary>
Motivation: 解决模型基强化学习中有效探索的挑战，特别是在高维连续控制任务中。现有方法独立更新采样策略与规划器分布，但训练状态依赖MPPI规划器，对齐采样策略与规划器可以提高价值估计准确性和长期性能。

Method: 引入PO-MPC框架，这是一个KL正则化的模型基强化学习方法家族，将规划器的动作分布作为策略优化的先验。通过最小化KL散度来对齐学习策略与规划器行为，在策略更新中权衡回报最大化和KL散度最小化。

Result: 实验表明，扩展配置带来了显著的性能提升，推进了基于MPPI的强化学习的最新技术水平。

Conclusion: PO-MPC框架统一了现有的MPPI基强化学习方法，揭示了先前方法作为该家族特例的关系，并探索了未研究过的变体，为模型基强化学习提供了更灵活和有效的策略优化方法。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [191] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出了HoRA方法，通过共享超网络生成跨注意力头的低秩矩阵，解决了LoRA在多头自注意力中忽视头间协同的问题，在样本效率和性能上优于LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调多头自注意力时单独适配每个注意力头，忽视了不同头之间的潜在协同效应，限制了其性能表现。

Method: 使用联合超网络为所有注意力头生成低秩矩阵，通过共享生成器实现跨头信息共享，提高参数效率。

Result: 理论分析表明HoRA比LoRA具有更好的样本效率；在多个语言和视觉基准测试中，HoRA以少量额外可训练参数超越了LoRA和其他PEFT方法。

Conclusion: HoRA通过促进跨注意力头的信息共享，有效解决了LoRA的局限性，在参数效率和性能方面都表现出色。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [192] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种基于二阶波动方程模拟的神经网络架构，通过可训练的空间速度和阻尼参数传播隐藏状态，使用基于FFT的辛谱求解器实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 为注意力机制和一阶状态空间模型提供振荡、全局的替代方案，利用物理归纳偏置构建计算高效且鲁棒的架构。

Method: 每层通过可训练的空间速度c(x)和阻尼γ(x)传播连续场隐藏状态，使用基于FFT的辛谱求解器在O(n log n)时间内实现传播。

Result: 在语言和视觉基准测试中，性能匹配或超过Transformer，实际效率提升30%的墙钟时间和25%的峰值内存，可视化显示模型学习到直观的信息传播策略。

Conclusion: Wave-PDE Nets被定位为计算效率高、鲁棒性强且具有强物理归纳偏置的架构，辛积分和谱拉普拉斯算子对稳定性和性能至关重要。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [193] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出PID Steering框架，将控制理论中的PID控制器应用于大语言模型的激活引导，相比现有方法具有更好的行为控制效果和理论保证


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型行为控制方法主要基于经验，缺乏理论性能保证，需要建立控制理论基础来提升激活引导的可靠性和鲁棒性

Method: 将流行的激活引导方法对应为比例控制器，提出完整的PID控制器框架：比例项对齐语义方向，积分项累积误差进行持续修正，微分项抑制超调

Result: 在多个LLM家族和基准测试上的广泛实验表明，PID Steering一致优于现有方法，实现了更鲁棒和可靠的行为控制

Conclusion: PID Steering为激活引导提供了控制理论基础，连接了经典控制理论的稳定性保证，是轻量级、模块化且易于集成的方法

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [194] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 提出了一种混合CNN-RNN深度学习模型用于交通事故严重程度预测，在弗吉尼亚I-64高速公路15,870条事故记录数据集上表现优于传统统计和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测交通事故严重程度对于减轻事故严重后果至关重要，智能交通系统需要有效的预测方法来提供适当的医疗援助和运输服务。

Method: 采用混合CNN-RNN深度学习模型，并与逻辑回归、朴素贝叶斯、KNN、决策树以及单独的RNN和CNN模型进行性能比较，考虑了交通事故各特征间的相互关联关系。

Result: 所提出的CNN-RNN混合模型在所有基准模型中表现最佳，证明了该混合模型结合了RNN和CNN优势的有效性。

Conclusion: 混合CNN-RNN模型能够更准确地预测交通事故严重程度，为智能交通系统提供了有效的预测工具。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [195] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent是一个基于大语言模型的自动化系统，旨在简化公平感知的机器学习模型开发过程，无需深厚技术专业知识即可实现偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 公平无偏的机器学习模型对高风险应用至关重要，但开发过程需要深厚的公平性定义、指标、数据预处理和机器学习技术专业知识，使得许多从业者难以实施。

Method: FairAgent自动分析数据集中的潜在偏见，处理数据预处理和特征工程，并根据用户需求实施适当的偏见缓解策略。

Result: 实验表明FairAgent在显著减少开发时间和专业知识要求的同时，实现了显著的性能改进。

Conclusion: FairAgent使公平感知的机器学习对从业者更加易于使用。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [196] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff是一种基于扩散模型的空气动力学流场预测方法，通过混合骨干网络结合CNN和Transformer的优势，在保持模型泛化能力的同时显著提高了预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法计算成本高昂，需要开发更高效的替代模型来快速预测空气动力学流场。扩散模型在复杂流场预测中展现出巨大潜力。

Method: 提出FoilDiff扩散模型，采用混合骨干去噪网络结合卷积特征提取和Transformer全局注意力机制，使用DDIM采样优化效率，并编码雷诺数、攻角和翼型几何参数作为输入。

Result: 与现有最先进模型相比，FoilDiff将平均预测误差降低了高达85%，在相同数据集上表现显著提升，提供更准确的预测和更好的预测不确定性校准。

Conclusion: FoilDiff能够为空气动力学设计和优化提供更准确、高效的流场预测，在精度和不确定性校准方面均优于现有的基于扩散的模型。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [197] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: 提出了算术平均μP（AM-μP）参数化方法，通过约束网络范围内平均单步预激活二阶矩为常数尺度，结合残差感知的He初始化，为卷积和深度残差网络建立了-3/2幂律的学习率缩放规律。


<details>
  <summary>Details</summary>
Motivation: 经典的最大更新参数化（μP）在异构架构中变得不适定，因为残差累积和卷积引入了跨层的不平衡，需要一种更通用的学习率选择方法。

Method: 引入AM-μP参数化，约束网络平均预激活二阶矩；结合残差感知He初始化，缩放残差分支权重；为卷积网络和残差网络建立学习率深度缩放规律。

Result: 证明卷积网络最大更新学习率满足η*(L)∝L^(-3/2)；标准残差网络η*(L)=Θ(L^(-3/2))；实证结果验证了-3/2缩放规律，实现零样本学习率迁移。

Conclusion: AM-μP为卷积和深度残差网络提供了统一实用的学习率原则，无需额外调优开销，解决了异构架构中的学习率选择挑战。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [198] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN是一种改进的DoRA方法，通过噪声注入和动态低秩矩阵生成来提升训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: DoRA虽然优于LoRA，但在训练稳定性和样本效率方面仍有改进空间。DoRAN旨在通过正则化和参数耦合进一步提升性能。

Method: 1) 在DoRA权重分解的分母中注入噪声作为自适应正则化器；2) 用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在视觉和语言基准测试中，DoRAN持续优于LoRA、DoRA和其他PEFT基线方法。

Conclusion: 结合噪声正则化和网络参数生成的方法为基础模型的稳健高效微调提供了有前景的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [199] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 该论文提出了一个评估AI模型在罕见事件识别中性能的框架，重点关注低发生率场景下的关键考虑因素，包括问题框架、测试集设计、统计评估和人类工作流程整合。


<details>
  <summary>Details</summary>
Motivation: 许多高风险AI应用针对低发生率事件，表面上的准确性可能掩盖了有限的现实价值。需要开发专门的方法来评估AI在罕见事件识别中的真实性能。

Method: 提出了结构化案例级检查(SCLE)方法，开发了全面的检查清单来指导AI模型采购或开发。在药物警戒领域实例化了该框架，使用了基于规则的检索、机器学习与概率记录链接结合的方法，以及基于LLM的自动化编辑。

Result: 识别了罕见事件设置中的特定陷阱，包括不现实的类别平衡导致的乐观估计和测试集中缺乏困难阳性对照。展示了成本敏感目标如何使模型性能与操作价值保持一致。

Conclusion: 虽然基于药物警戒实践，但这些原则可推广到阳性样本稀缺且错误成本可能不对称的领域，为罕见事件识别中的AI评估提供了通用框架。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [200] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出了课程混沌预测(CCF)训练范式，通过从简单周期行为到复杂混沌动态的渐进式学习，显著提升混沌系统预测性能


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在混沌系统预测中的两个极端问题：过度专业化于单一混沌系统导致泛化性差，或混合大量不相关时间序列导致无法学习特定动态机制

Method: 基于动力学系统理论组织训练数据，使用最大Lyapun夫指数和吸引子维度量化复杂性，构建从简单到复杂的课程学习路径，包含50多个合成ODE/PDE系统

Result: 在太阳黑子数、电力需求和人类ECG信号等真实世界基准测试中，CCF将有效预测范围延长达40%，相比仅使用真实数据训练延长超过两倍

Conclusion: CCF训练范式能够建立对动态行为的鲁棒且可泛化表示，在不同神经网络架构中均表现出一致的性能提升

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [201] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出CSHT架构，结合Granger因果超图结构、黎曼几何和因果掩码Transformer注意力，用于可解释的金融时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 解决金融时间序列预测中的可解释性问题，统一建模金融新闻和情绪对资产收益的定向影响，提供透明归因路径。

Method: 提取多元Granger因果依赖关系，编码为超球面上的定向超边，通过角度掩码约束注意力以保持时间方向性和几何一致性。

Result: 在2018-2023年标普500数据上，包括2020年COVID-19冲击，CSHT在收益预测、制度分类和顶级资产排名任务中持续优于基线。

Conclusion: CSHT通过强制预测因果结构和在黎曼流形中嵌入变量，提供了跨市场制度的稳健泛化和从宏观经济事件到股票级响应的透明归因路径，是可信金融预测的实用解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [202] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出了一种新的模糊性度量方法，用于量化分类任务中的偶然不确定性，该方法与二次熵相关但能区分类别不可区分性和明确不可解决性，并开发了统计推断工具。


<details>
  <summary>Details</summary>
Motivation: 人类生成的分类标注经常产生反映模糊性而非简单标注错误的经验响应分布，需要一种方法来量化这种偶然不确定性。

Method: 引入一种将离散响应分布映射到单位区间标量的模糊性度量，分析其形式特性，开发频率主义点估计器和贝叶斯后验推断工具。

Result: 该方法能够有效分离类别不可区分性和明确不可解决性带来的不确定性，提供了数据集质量评估和机器学习工作流的实用工具。

Conclusion: 提出的模糊性度量为分类任务中的不确定性量化提供了理论基础和实用工具，特别适用于处理人类标注中的模糊性问题。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [203] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在真实世界经济价值任务上能力的基准，涵盖美国GDP前9大行业中44个职业的工作活动，发现前沿模型性能随时间线性提升，接近行业专家水平。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI模型在实际经济价值任务上的能力，而不仅仅是学术基准，以了解模型在真实工作场景中的表现。

Method: 基于美国劳工统计局工作活动数据，构建44个职业的代表性任务，由平均14年经验的行业专业人士设计，使用前沿模型进行测试。

Result: 前沿模型在GDPval上的表现随时间线性提升，当前最佳模型接近行业专家的交付质量，通过增加推理努力、任务上下文和支架能进一步提升性能。

Conclusion: AI模型在经济价值任务上表现出色，结合人类监督可以比无辅助专家更便宜、更快地完成任务，为理解真实世界模型能力提供了基准。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [204] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出动态加权损失函数，根据领域稀疏度自适应调整损失权重，解决稀疏领域推荐性能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统固定加权损失在稀疏领域效果有限，单一权重无法有效处理交互极少的领域，训练信号容易被通用数据集稀释。

Method: 使用数据驱动方法，基于训练数据中每个领域的稀疏度自适应调整损失权重，对稀疏领域赋予更高权重，稠密领域赋予较低权重。

Result: 在四个数据集上的实验表明，该方法显著优于所有基线方法，特别是在稀疏领域，Recall@10和NDCG@10指标大幅提升，同时保持稠密领域性能且计算开销极小。

Conclusion: 动态加权损失函数能有效提升稀疏领域推荐性能，具有理论保证和实际应用价值。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [205] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出将神经网络训练视为参数空间到表示空间的结构保持变换（函子L）的范畴论框架，揭示同伦类优化路径与泛化性能的关系。


<details>
  <summary>Details</summary>
Motivation: 传统将神经网络训练视为损失曲面梯度下降的视角有限，需要更根本的数学框架来理解学习过程的本质结构。

Method: 建立范畴论框架，将学习过程建模为函子变换，使用同伦理论分析优化路径，并通过持续同调识别稳定最小值。

Result: 实验表明同伦路径泛化性能差异在0.5%以内，非同伦路径差异超过3%；持续同调与泛化性能相关性R^2=0.82。

Conclusion: 范畴不变量既提供了深度学习为何有效的理论洞察，又为训练更鲁棒网络提供了具体算法原则。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [206] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了第一个基于分数的贪婪搜索方法LGES，用于识别包含潜变量的因果结构，具有可识别性保证


<details>
  <summary>Details</summary>
Motivation: 现有的基于约束的因果发现方法面临多重检验和误差传播的挑战，需要一种能处理部分观测场景的基于分数的贪婪搜索方法

Method: 提出了广义N因子模型，并设计了潜变量贪婪等价搜索(LGES)算法，通过定义良好的操作在图空间中高效搜索最优结构

Result: 实验在合成和真实数据上验证了该方法的有效性

Conclusion: 该方法能够识别包含潜变量的真实结构，达到马尔可夫等价类

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [207] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM是一个基于Mamba的神经状态空间模型，用于连续血糖监测预测，结合了血糖和可穿戴活动信号，提高了预测准确性并增加了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数连续血糖监测预测模型缺乏临床可解释性，限制了其在糖尿病管理中的应用。

Method: 使用基于Mamba的神经状态空间模型，整合连续血糖监测和可穿戴活动信号，通过变量选择和时间归因实现可解释性。

Result: 相比时序融合变换器基线，SSM-CGM在短期预测准确性上有所提升，并能进行反事实预测模拟生理信号变化对血糖的影响。

Conclusion: SSM-CGM为个性化糖尿病管理提供了一个可解释的、基于生理学的预测框架。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [208] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯假设的高效部分信息分解方法，通过梯度优化算法和信息保持编码器处理非高斯数据，解决了现有方法计算成本高和准确性低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有部分信息分解方法依赖联合分布优化，对连续高维模态计算成本高且不准确，需要更高效准确的解决方案。

Method: 提出高斯部分信息分解框架，使用梯度优化算法提高计算效率，并通过信息保持编码器将任意分布转换为高斯分布以处理非高斯数据。

Result: 在合成数据实验中比现有基线方法更准确高效，在大规模多模态基准测试中成功量化信息分解并选择高性能模型。

Conclusion: 该方法为多模态数据分析提供了高效准确的部分信息分解工具，解决了现有方法的计算瓶颈和分布限制问题。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [209] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一个零阶Frank-Wolfe算法，首次在多项式时间内收敛到期望的performatively optimal策略，解决了现有方法只能收敛到performatively stable策略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的performative强化学习方法只能收敛到performatively stable策略，与期望的performatively optimal策略之间存在理论上的正常数差距。本文旨在填补这一差距。

Method: 提出了零阶Frank-Wolfe算法，在Frank-Wolfe框架中使用零阶近似来计算performative策略梯度。

Result: 在标准正则化主导条件下，首次实现了多项式时间收敛到期望的PO策略。实验结果表明该算法比现有方法更有效。

Conclusion: 本文证明了非凸值函数的重要性质，并开发了首个能收敛到performatively optimal策略的高效算法，填补了现有方法的理论空白。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [210] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文系统分析了联邦学习中拜占庭客户端数量估计对鲁棒聚合器性能的影响，揭示了低估会导致性能任意恶化，而非低估情况下存在性能与鲁棒性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭客户端攻击，鲁棒聚合器需要估计拜占庭客户端数量f，但现有研究未系统分析这种估计对性能的影响。

Method: 通过理论分析，研究聚合器和联邦学习算法在不同估计值f̂和实际值f情况下的最坏情况误差。

Result: 低估(f̂<f)会导致性能任意恶化；非低估(f̂≥f)时，误差与f̂/(n-f-f̂)成正比，随f̂增大而增加，表明鲁棒性与性能存在权衡。

Conclusion: 拜占庭客户端数量估计对联邦学习性能有重要影响，需要在鲁棒性和性能之间进行权衡，避免低估是关键。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [211] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 提出了基于分数热核动力学的新型标签传播和自训练算法，通过分数拉普拉斯算子增强图神经网络的表达能力，在少量标注数据情况下特别有效。


<details>
  <summary>Details</summary>
Motivation: 通过信息论与抛物线演化方程的经典对应关系，将分数热核集成到图神经网络架构中，增强其表达能力，特别适用于只有少量标注训练样本的情况。

Method: 使用分数热核动力学进行标签传播和自训练，通过切比雪夫多项式近似处理大规模图，将分数拉普拉斯算子整合到图卷积网络和图注意力网络中。

Result: 在标准数据集上证明了该方法的有效性，分数扩散模型通过非局部交互实现了更全局的标签扩散。

Conclusion: 分数热核方法在监督标签和图上扩散之间取得了特别有利的平衡，在只有少量标注数据时表现优异。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [212] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 该论文研究了领域泛化问题，发现在满足后验漂移假设的数据集上，通过添加领域特定信息的领域感知ERM方法优于传统的池化ERM方法。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化研究大多基于协变量偏移假设，难以超越池化ERM方法。作者认为在后验漂移假设下，领域感知方法应该表现更好。

Method: 提出领域感知ERM方法，在特征向量中增强领域特定信息，并与理论框架结合进行验证。

Result: 在语言和视觉任务上的实验表明，在后验漂移假设下，领域感知ERM确实优于池化ERM。

Conclusion: 领域泛化方法的效果取决于数据集的分布假设类型，在后验漂移情况下，领域感知方法具有优势。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [213] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 本文提出forking-sequences方法来解决时间序列预测中的稳定性问题，该方法通过联合编码和解码所有预测创建日期的时间序列，显著提高了预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型虽然注重准确性，但往往忽视了预测稳定性这一重要指标。不稳定的预测会破坏利益相关者的信任并干扰下游决策制定。

Method: 采用forking-sequences方法，与标准统计和神经预测方法不同，该方法联合编码和解码所有预测创建日期的时间序列，类似于时间序列交叉验证。

Result: 在16个数据集上的实验表明，forking-sequences方法显著提高了预测稳定性，MLP、RNN、LSTM、CNN和Transformer架构的平均预测百分比变化稳定性分别提高了28.8%、28.8%、37.9%、31.3%和8.8%。

Conclusion: forking-sequences方法是一个有效且值得广泛采用的技术，能够提供更稳定的预测，同时提高训练稳定性、减少预测方差并提升推理计算效率。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [214] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 通过固定参数扩展(FPE)技术，在不增加非零参数数量的情况下增加神经元数量，可以减少特征干扰并提高网络性能


<details>
  <summary>Details</summary>
Motivation: 研究如何在保持非零参数数量不变的情况下提升网络性能，探索神经元数量增加与特征干扰减少之间的关系

Method: 提出固定参数扩展(FPE)：将单个神经元替换为多个子神经元，并将父神经元的权重不相交地分配给子神经元，每个子神经元继承非重叠的连接子集

Result: 在布尔代码问题等符号任务上，FPE系统性地减少了多义性指标并提高了任务准确率；在真实模型（CLIP嵌入分类器和深层网络）上，保持非零参数数量不变而增加网络宽度持续提升了准确率

Conclusion: FPE提供了一种基于可解释性原理的机制，利用网络宽度对抗叠加现象，在不增加非零参数数量的情况下提升性能，这特别适合现代加速器架构

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [215] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: 提出WISDOM方法，利用小波域预测任务表示来增强非平稳强化学习，通过小波变换捕捉MDP序列的多尺度特征，提升在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有非平稳特性，现有NSRL方法对高度动态环境的适应能力有限，需要更好的方法来捕捉环境动态变化的多尺度特征。

Method: 将任务表示序列转换到小波域，利用小波系数表示非平稳变化的全局趋势和细粒度变化；设计小波时序差分更新算子来增强MDP演化的跟踪和预测。

Result: 在多个基准测试中，WISDOM在样本效率和渐近性能上显著优于现有基线方法，在非平稳和随机演化任务的复杂环境中表现出卓越的适应性。

Conclusion: 小波域任务表示能有效增强NSRL，WISDOM方法通过多尺度特征捕捉和理论保证的收敛性，在动态环境中展现出强大的适应能力。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [216] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 使用条件归一化流(Full-Glow)模型实现实时城市噪声预测，相比传统物理求解器加速超过2000倍，在非视距场景下精度提升24%，支持交互式城市规划应用。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和监管工作流程至关重要，但传统基于物理的求解器速度太慢，无法满足时间紧迫的迭代"假设分析"研究需求。

Method: 采用条件归一化流(Full-Glow)模型，从2D城市布局生成符合标准的城市声压地图，在单张RTX 4090上实现256x256地图的实时生成。

Result: 模型在基线、衍射和反射场景下，相比参考求解器加速超过2000倍，在非视距场景下精度比先前深度模型提升24%，基线非视距场景达到0.65 dB MAE，具有高结构保真度。

Conclusion: 该模型能够重现衍射和干涉模式，支持在声源或几何变化下的即时重新计算，成为城市规划、合规制图和运营的实用引擎。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [217] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer是一个黎曼图扩散框架，通过将多级特征解耦到特定流形上，解决了图数据中不同曲率特征在统一潜在空间中纠缠的问题，提升了生成和预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型将节点、边和图级特征嵌入统一潜在空间，但由于图数据的非欧几里得特性，不同曲率的特征在该空间中纠缠，未能充分发挥其几何潜力。

Method: 提出GeoMancer框架：1）用等距不变的黎曼陀螺核方法替代指数映射，缓解数值不稳定性；2）将多级特征解耦到各自任务特定的流形上；3）引入流形约束扩散方法和自引导策略，确保生成数据与流形特征对齐。

Result: 大量实验验证了该方法的有效性，在各种任务上表现出优越性能。

Conclusion: GeoMancer通过构建理想的黎曼扩散模型，成功捕捉了复杂图数据的独特流形特征，解决了现有方法中的数值不稳定性和流形偏差问题。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [218] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文分析了MaskGIT采样器的理论机制，提出了更高效的"moment采样器"，并通过部分缓存和混合方法优化了采样效率。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在多个领域表现出色，但其采样加速研究相对不足，需要开发更高效的采样器。

Method: 理论分析MaskGIT采样器，提出moment采样器作为替代方案，采用"选择-然后采样"策略，并引入部分缓存技术和混合探索-利用方法。

Result: 在图像和文本领域的实验验证了理论分析，并证明了所提方法的高效性。

Conclusion: 研究推进了掩码扩散采样器的理论理解和实际实现，为高效采样提供了新思路。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [219] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 该论文主张图基表格深度学习应从预测为中心转向结构感知建模，强调准确学习特征交互结构的重要性，并证明强制真实交互结构能提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基表格深度学习方法主要优化预测准确率，忽视了准确建模图结构，而特征交互对表格数据至关重要。

Method: 使用具有已知真实图结构的合成数据集，评估现有GTDL方法恢复特征交互的能力，并测试强制真实交互结构对预测性能的影响。

Result: 现有GTDL方法无法恢复有意义的特征交互，但强制真实交互结构能提高预测性能。

Conclusion: GTDL方法需要优先考虑定量评估和准确的结构学习，转向结构感知建模，以构建不仅准确且可解释、可信赖、基于领域理解的系统。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [220] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出RegCache训练免费算法，通过引入异常值倾向但语义无意义的前缀令牌来缓解视觉编码器中的异常值问题，实现更精确的量化


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的视觉编码器在实时处理大规模视觉数据时面临推理成本高的问题，后训练量化是实用路径，但由于大规模激活（异常值）的存在，即使在8位精度下仍具挑战性

Method: RegCache算法引入异常值倾向但语义无意义的前缀令牌到目标视觉编码器，防止其他令牌产生异常值。基于视觉编码器中异常值与语言模型不同的观察，提出了中间层前缀和令牌删除两项技术创新

Result: 实验表明该方法在文本监督和自监督视觉编码器上都能持续提高量化模型的准确性

Conclusion: RegCache是一种有效的训练免费方法，能够显著缓解视觉编码器中的异常值问题，为视觉编码器的量化提供了实用解决方案

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [221] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe是一个面向部署的衍生品对冲框架，结合了分布强化学习和基于控制屏障函数的安全层，通过IQN-CVaR-PPO算法优化尾部风险，使用凸二次规划强制执行金融约束。


<details>
  <summary>Details</summary>
Motivation: 传统衍生品对冲方法在极端市场条件下可能失效，需要结合机器学习能力与金融监管约束，确保在优化风险的同时满足可部署性和可审计性要求。

Method: 采用IQN-CVaR-PPO分布强化学习算法，结合温度倾斜和尾部增强技术稳定小α值CVaR估计；使用控制屏障函数二次规划安全层强制执行椭圆无交易带、箱型限制、速率限制等金融约束。

Result: 在无套利、考虑微观结构的合成市场中，Tail-Safe改善了左尾风险而不影响中心性能，当QP可行且无松弛时实现零硬约束违反，提供了完整的可审计轨迹。

Conclusion: Tail-Safe框架成功统一了风险敏感强化学习与金融安全约束，提供了理论保证和实际部署能力，但依赖合成数据和简化执行环境是当前局限。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [222] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 该论文提出了一种基于线性效用模型和间隙索引框架的top-m用户调度集识别方法，用于多用户MIMO下行链路中的组合纯探索问题，显著降低了计算复杂度并保持了高识别精度。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO下行链路中识别top-m用户调度集是一个组合纯探索问题，由于动作空间呈指数增长，穷举搜索不可行，需要开发高效的探索和选择方法。

Method: 采用线性效用模型，引入间隙索引框架，维护当前冠军臂（top-m集）的短列表和旋转挑战者臂短列表，专注于产生最有信息量的基于间隙索引的比较测量。

Result: 与最先进的线性bandit方法相比，该方法显著减少了运行时间和计算量，同时保持了高识别精度，并暴露了速度与精度之间的可调权衡。

Conclusion: 短列表驱动的纯探索使得AI使能通信系统中的在线、测量高效子载波选择变得实用可行。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [223] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 提出了基于两种对偶表示的失真风险度量优化梯度下降算法：DM形式和QF形式，以及结合两者的混合形式，在投资组合选择和深度强化学习中展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 失真风险度量在决策中捕捉风险偏好，是管理不确定性的通用标准，需要高效的优化算法。

Method: 基于失真-度量形式和分位数函数形式的对偶表示，分别开发三时间尺度和两时间尺度梯度下降算法，并设计混合形式结合两者优势。

Result: DM形式达到O(k^{-4/7})的最优收敛率，QF形式达到更快的O(k^{-2/3})收敛率，在投资组合选择任务中显著优于基线方法。

Conclusion: 所提算法在理论和实验上均表现优异，可扩展到深度强化学习，在动态库存管理等实际应用中具有实用价值。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [224] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: GILT是一个基于token的图内上下文学习框架，无需LLM和微调，能统一处理节点、边和图级别的分类任务，有效应对图数据的异构性。


<details>
  <summary>Details</summary>
Motivation: 解决图基础模型在处理异构图数据时的挑战，包括特征空间、标签集和拓扑结构的多样性，同时避免现有方法对文本依赖或需要昂贵微调的局限性。

Method: 提出基于token的图内上下文学习框架，将图分类任务重新表述为统一格式，利用动态理解类别语义的能力实现免调优适应。

Result: 实验表明GILT在少样本学习场景下性能更强，且所需时间显著少于基于LLM或需要微调的基线方法。

Conclusion: GILT通过免LLM和免微调的架构，有效解决了图数据异构性问题，为图基础模型提供了高效且通用的解决方案。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [225] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一个新颖的推理框架，通过将连续潜在表示与潜在扩散模型相结合，改进LLM的推理能力，实现更高效、多样化的推理轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的自回归解码限制了整体性重新审视和细化早期推理步骤的能力，导致推理探索效率低下和解决方案多样性不足。

Method: 首先使用VAE构建结构化潜在推理空间，将文本推理步骤编码为思想标记块；然后利用潜在扩散模型通过块级双向注意力掩码学习去噪潜在思想标记，支持长程推理和迭代细化。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面均优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR为文本推理提供了一种新的潜在扩散范式，显著提升了推理性能。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [226] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出了一种名为SONA的新型判别器设计，通过分离自然性和对齐性评估，结合自适应权重机制，在条件生成任务中实现了更好的样本质量和条件对齐。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成对抗网络在条件判别器中难以平衡真实性和条件对齐的双重目标，需要更有效的判别器设计来解决这一挑战。

Method: 提出SONA方法，在判别器最后层使用分离的自然性和对齐性投影，结合匹配感知监督和自适应权重机制来动态平衡所有目标。

Result: 在类别条件生成任务中，SONA相比最先进方法获得了更优的样本质量和条件对齐，在文本到图像生成中也表现出色。

Conclusion: SONA方法具有多功能性和鲁棒性，为条件生成任务提供了有效的解决方案。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [227] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein空间中的Busemann函数，在一维分布和高斯测度情况下建立了闭式表达式，并基于此开发了新的Sliced-Wasserstein距离和投影方案。


<details>
  <summary>Details</summary>
Motivation: Busemann函数在几何机器学习中具有重要作用，能定义黎曼流形上测地线的投影。由于许多数据源可以建模为概率分布，研究其在Wasserstein空间中的性质很有意义。

Method: 通过最优传输度量诱导的黎曼结构，研究Wasserstein空间中Busemann函数的存在性和计算方法，特别关注一维分布和高斯测度两种情况。

Result: 在一维分布和高斯测度情况下获得了Busemann函数的闭式表达式，并基于此开发了新的Sliced-Wasserstein距离和投影方案。

Conclusion: 提出的方法在合成数据集和迁移学习问题上表现出高效性，为概率分布的投影和距离计算提供了新工具。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [228] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的概率回归框架，通过非参数方式学习预测分布，能够适应多样化任务并提供更好的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 概率回归能够提供完整的预测分布，比传统点估计提供更丰富的洞察力并直接支持不确定性量化。虽然基于扩散的生成模型在生成复杂高维数据方面表现出色，但在一般回归任务中的应用缺乏不确定性评估且局限于特定领域。

Method: 提出建模扩散噪声的完整分布，实现非参数概率回归。研究了不同的噪声参数化方法并分析其权衡，在低维和高维设置下评估框架性能。

Result: 在多个实验中，该方法相对于现有基线表现出优越性能，同时提供校准的不确定性估计，展示了其作为概率预测工具的通用性。

Conclusion: 该扩散基概率回归框架能够有效学习预测分布，适应多样化回归任务，并提供可靠的不确定性量化，具有很好的通用性和实用价值。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [229] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 提出一种优化神经网络的新方法，将最后一层线性权重用闭式解表示，仅优化骨干网络参数，在平方损失下比标准SGD更有效。


<details>
  <summary>Details</summary>
Motivation: 神经网络通常使用随机梯度下降变体优化，但在平方损失下，线性最后一层权重的最优解有闭式解，可以利用这一特性提高优化效率。

Method: 将最后一层视为骨干参数的函数，仅优化骨干参数，等价于在骨干上做梯度下降步长和在最后一层做闭式更新交替进行。为适应SGD设置，在当前位置损失和先前批次累积信息之间进行权衡。

Result: 在神经正切核机制下，证明该方法收敛到最优解。在多个监督任务（包括傅里叶神经算子和工具变量回归）中，相比标准SGD在平方损失下表现更有效。

Conclusion: 提出的方法利用最后一层闭式解特性，在保持收敛保证的同时，提高了神经网络在平方损失任务中的优化效率。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [230] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过模块化生成、反思和策展过程，将上下文视为不断演进的剧本，有效防止上下文崩溃并保持详细知识，在多个基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有上下文适应方法中的简洁性偏见和上下文崩溃问题，前者会丢失领域洞察，后者在迭代重写中逐渐侵蚀细节。

Method: ACE框架将上下文视为不断演进的剧本，通过模块化的生成、反思和策展过程来积累、优化和组织策略，采用结构化增量更新防止上下文崩溃。

Result: 在代理和领域特定基准测试中，ACE在离线（如系统提示）和在线（如代理记忆）上下文优化方面表现优异：代理任务提升10.6%，金融任务提升8.6%，同时显著降低适应延迟和部署成本。

Conclusion: 全面且不断演进的上下文能够实现可扩展、高效且自我改进的LLM系统，且开销较低。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [231] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出基于先进预测模型的合成生物医学时间序列数据生成框架，能高保真复制EEG和EMG等复杂电生理信号，解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和大量资源需求限制了生物医学时间序列AI的发展，导致数据需求与可访问性之间存在关键差距。

Method: 使用先进的预测模型生成合成生物医学时间序列数据，准确复制复杂的电生理信号如EEG和EMG。

Result: 合成数据保持了真实数据的关键时间和频谱特性，可作为真实数据的有效替代品，并能显著提升AI模型性能。

Conclusion: 该方法在保持关键生物医学特征的同时，为各种应用提供高可扩展性，并集成到开源存储库中，大大扩展了AI驱动生物医学研究的资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [232] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过连接多个小型嵌入模型的原始向量并使用轻量级解码器压缩，可以在保持性能的同时大幅减小模型尺寸，在资源受限环境中实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 大型嵌入模型在资源受限环境（如浏览器或边缘设备）中部署困难，而小型模型性能不足，需要找到平衡性能与效率的解决方案。

Method: 连接多个小型模型的原始嵌入向量，然后使用基于Matryoshka表示学习损失的轻量级统一解码器将高维联合表示映射到低维空间，无需微调基础模型。

Result: 在MTEB检索任务子集上，concat-encode-quantize流水线在48倍压缩因子下恢复了原始性能的89%，连接更多基础模型带来递减收益但提高了压缩鲁棒性。

Conclusion: 通过连接多个小型嵌入模型并使用轻量级解码器压缩，可以在保持高性能的同时实现显著的模型压缩，为资源受限环境提供了实用的嵌入解决方案。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [233] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出一种无需训练的高速缓存策略，通过预测求解器步骤间的中间隐藏状态，将分子几何生成的推理时间减少2倍，与其他优化结合可达7倍加速。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型生成高质量分子几何结构，但推理时计算成本高昂，需要数百次网络评估，成为实际应用中采样大量分子候选者的主要瓶颈。

Method: 直接在SE(3)-等变骨干网络上操作的无训练缓存策略，预测求解器步骤间的中间隐藏状态，与预训练模型兼容，且与现有基于训练的加速和系统级优化正交。

Result: 在GEOM-Drugs数据集上，缓存方法在保持相同样本质量下将推理时间减少2倍，与基础模型相比最高可达3倍加速，且样本质量下降最小。

Conclusion: 该缓存方法能显著加速分子几何生成，且其增益可与其他优化叠加，结合其他通用无损优化可获得高达7倍的加速效果。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [234] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出IMLP模型，一种针对表格数据流的紧凑型持续学习方法，通过注意力机制和滑动潜在特征缓冲区实现恒定内存使用，在保持竞争力的准确率同时显著提升能源效率。


<details>
  <summary>Details</summary>
Motivation: 表格数据流在医疗、金融和物联网等领域的实时决策中日益重要，但这些应用通常在资源受限的边缘设备上运行。现有持续学习方法大多依赖回放机制，其缓冲区会随时间增长而加剧资源成本。

Method: 提出上下文感知增量多层感知器(IMLP)，使用滑动潜在特征缓冲区上的窗口化缩放点积注意力，避免存储原始数据并保持恒定内存大小。注意力上下文与当前特征拼接后由共享前馈层处理，实现轻量级分段更新。

Result: IMLP比TabNet和TabPFN分别实现高达27.6倍和85.5倍的能源效率提升，同时保持竞争力的平均准确率。

Conclusion: IMLP为表格数据流提供了一个易于部署、能源效率高的全重训练替代方案。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [235] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文揭示了时间序列归一化策略中的理论矛盾，发现标准RevIN在极端异常值数据集上会灾难性失败，而简单的R²-IN却意外成为最佳表现者，同时自适应模型A-IN出现系统性失败。


<details>
  <summary>Details</summary>
Motivation: 研究RevIN归一化技术及其鲁棒性改进版本R²-IN在时间序列预测中的表现，探索简单启发式方法可能带来的不稳定性问题。

Method: 通过理论分析识别四种归一化策略的理论矛盾，并进行实验验证，比较标准RevIN、R²-IN和自适应模型A-IN在不同数据集上的表现。

Result: 标准RevIN在极端异常值数据集上MSE激增683%；R²-IN意外成为整体最佳表现者；自适应模型A-IN出现完全系统性失败。

Conclusion: 提出了时间序列归一化的新警示范式：从盲目追求复杂性转向诊断驱动分析，揭示了简单基线的惊人力量和天真适应的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [236] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 该论文提出了语义信道均衡方法来解决多供应商部署中DeepJSCC的潜在空间不匹配问题，通过三种对齐器来提升异构网络中的语义通信性能。


<details>
  <summary>Details</summary>
Motivation: 现有DeepJSCC方案假设发射端和接收端共享潜在空间，这在多供应商部署中不成立，导致语义噪声并降低重建质量和下游任务性能。

Method: 提出了三类语义信道均衡器：线性映射（闭式解）、轻量神经网络（更强表达能力）和Parseval框架均衡器（零样本无需训练）。

Result: 通过在AWGN和衰落信道上的图像重建实验，量化了复杂度、数据效率和保真度之间的权衡。

Conclusion: 为在异构AI原生无线网络中部署DeepJSCC提供了指导方针，解决了多供应商环境中的潜在空间不匹配问题。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [237] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 提出CCGBO框架，通过反事实信用评估历史观测点的贡献度，改进贝叶斯优化的采集函数，从而更有效地寻找全局最优解。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化关注构建全局代理模型，但在实际应用中，主要目标是快速定位全局最优解。由于序列优化的随机性和对初始设计的依赖，并非所有观测点都对寻找最优解有同等贡献。

Method: 引入反事实信用概念，量化历史观测点的个体贡献度，并将反事实信用整合到采集函数中，使优化过程能够有选择地在最优解最可能出现的区域分配资源。

Result: 理论证明CCGBO保持次线性遗憾，在多个合成和真实世界基准测试中，CCGBO持续降低简单遗憾并加速收敛到全局最优解。

Conclusion: CCGBO通过显式量化历史观测点的贡献度，改进了贝叶斯优化的效率，在保持理论保证的同时，在实际应用中表现出更好的收敛性能。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [238] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出了首个针对随机扩展对抗（SEA）模型的无参数算法，消除了对问题特定参数（如域直径D和损失函数Lipschitz常数G）的先验知识需求。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需要预先知道问题特定参数，如域直径和Lipschitz常数，这限制了实际应用。需要开发无参数方法以提高实用性。

Method: 利用乐观在线牛顿步（OONS）算法，开发了两种无参数方法：1）未知域直径但已知Lipschitz常数的情况；2）两者都未知的更一般情况。

Result: 实现了比较器自适应算法，获得了期望遗憾界为Õ(||u||₂² + ||u||₂(√σ²₁:ₜ + √Σ²₁:ₜ))，其中u是比较器向量，σ²和Σ²分别表示累积随机方差和累积对抗变化。

Conclusion: 即使在SEA模型中两个参数都未知的情况下，所提出的方法仍然有效，遗憾界对随机方差和对抗变化保持了相同的依赖性。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [239] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文研究发现有效噪声尺度是统一优化器和数据选择对模型合并影响的关键因素，模型合并效果是有效噪声的非单调函数，存在最优值。


<details>
  <summary>Details</summary>
Motivation: 虽然模型合并方法在实践中很有用，但对其有效性的原理理解不足，本文旨在探索优化过程如何影响损失景观几何形状及其对合并成功的影响。

Method: 通过分析有效噪声尺度，研究学习率、权重衰减、批量大小和数据增强等优化参数如何独立调节有效噪声，进而影响模型合并效果。

Result: 发现模型合并效果是有效噪声的非单调函数，具有明显的最优值。较大的学习率、较强的权重衰减、较小的批量大小和数据增强都能独立调节有效噪声尺度，表现出相同的定性趋势。

Conclusion: 优化器噪声不仅影响单个最小值的平坦度或泛化能力，还影响全局损失景观，能够预测独立训练的解何时可以成功合并。这扩展了对优化如何塑造损失景观几何形状及其对模型合并下游影响的理解。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [240] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出ViTs框架，将时间序列数据转换为视觉表示，利用视觉语言模型处理任意长度的时间序列异常检测，实现"一次训练，跨场景推理"的目标


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中"一次训练，跨场景推理"的根本挑战，传统方法受限于固定长度输入，大语言模型面临上下文长度限制

Method: 将时间序列曲线转换为视觉表示，通过缩放保持时间依赖性，使用进化算法生成高质量图像-文本对，设计三阶段训练流程：时间序列知识注入、异常检测增强、异常推理精炼

Result: ViTs显著增强了视觉语言模型理解和检测时间序列异常的能力

Conclusion: ViTs框架有效解决了时间序列异常检测中的上下文长度限制问题，实现了对任意长度序列的高效处理

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [241] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了方向性层状超图网络（DSHN），将层状理论与超图中的非对称关系处理相结合，构建了有向层状超图拉普拉斯算子，在7个真实数据集上相比13个基线方法实现了2%到20%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 有向超图能够建模定向的群体交互，但在当前研究中仍未被充分探索。现有方法往往隐含同质性偏见，限制了在异质性场景中的有效性。

Method: 基于代数拓扑中的细胞层状概念，将层状神经网络扩展到有向超图，构建了方向性层状超图网络框架和复数值的有向层状超图拉普拉斯算子。

Result: 在7个真实世界数据集上，与13个基线方法相比，DSHN实现了2%到20%的相对准确率提升。

Conclusion: 对超图中方向性的原则性处理，结合层状理论的表达能力，能够显著提升性能表现。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [242] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 本文研究了在Entropic Value-at-Risk准则下的多臂老虎机固定置信度最佳臂识别问题，提出了一个δ-正确的Track-and-Stop算法，并推导了样本复杂度的渐近匹配下界。


<details>
  <summary>Details</summary>
Motivation: 解决高风险环境（如金融领域）中风险规避决策的需求，超越简单的期望值优化。

Method: 提出基于Track-and-Stop的δ-正确算法，需要解决复杂的凸优化问题和相关的简单非凸问题。

Result: 推导了期望样本复杂度的下界，并证明算法渐近匹配该下界。

Conclusion: 在非参数设置下，成功解决了EVaR准则下的BAI问题，实现了风险规避决策的优化。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [243] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 本文证明了非线性CCA在特定条件下能够恢复真实潜在因子，建立了从观测空间到源空间的重新参数化方法，并证明了岭正则化经验CCA的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性CCA如何恢复真实潜在因子，解决传统线性CCA在非线性情况下的局限性，为非线性降维和特征学习提供理论保证。

Method: 通过重新参数化将分析从观测空间转换到源空间，建立仿射可识别性条件，使用岭正则化经验CCA并证明其收敛到总体对应物。

Result: 在总体设置下证明了非线性CCA能够恢复真实潜在因子（经过正交变换和白化），在有限样本情况下经验CCA收敛到总体CCA。

Conclusion: 白化对于确保有界性和良好条件性至关重要，非线性CCA在适当条件下能够有效恢复潜在因子，为非线性降维提供了理论依据。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [244] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文分析了扩散语言模型并行解码的局限性，提出了首个专门针对dLLMs的基准测试ParallelBench，揭示了并行解码在真实场景中会导致严重的质量下降，并指出当前解码策略难以在速度和质量之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因并行解码潜力而受到关注，但其条件独立性假设会忽略token依赖关系，在强依赖场景下导致生成质量下降。现有研究忽视了这些固有挑战，标准基准测试不足以捕捉并行解码带来的质量退化问题。

Method: 首先进行并行解码的信息论分析，然后在可分析的合成列表操作上进行案例研究，从数据分布和解码策略角度提供量化见解。提出专门为dLLMs设计的ParallelBench基准测试，包含对人类和自回归LLMs简单但对dLLMs并行解码极具挑战性的任务。

Result: 研究发现：(i) 并行解码下的dLLMs在真实场景中可能遭受严重的质量下降；(ii) 当前并行解码策略难以根据任务难度调整并行度，无法在不损害质量的情况下实现有意义的加速。

Conclusion: 当前的速度-质量权衡问题迫切需要创新的解码方法。作者发布基准测试以加速真正高效dLLMs的发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [245] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 该论文首次分析了在不确定性概率机器学习中，迭代更新信用集（credal sets）的收敛性问题，探讨了固定点存在的条件及其可达性。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖不确定性表示的迭代更新，但在存在不精确性和模糊性的情况下，需要研究信用集迭代更新过程的收敛性和稳定性条件。

Method: 通过分析信用集（闭凸概率分布集）的迭代更新机制，研究固定点存在的条件，并以信用贝叶斯深度学习为例进行说明。

Result: 研究表明，将不精确性纳入学习过程不仅丰富了不确定性表示，还揭示了稳定性出现的结构性条件。

Conclusion: 这项工作为不精确性下迭代学习的动力学提供了新的见解，证明了在特定条件下信用集更新过程能够收敛到稳定固定点。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [246] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: 提出了DiPO（Distribution Preference Optimization）方法，通过直接优化下一个token的概率分布来解决LLM遗忘问题，克服了NPO方法缺乏显式正偏好信号的限制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，数据隐私和安全问题日益受到关注。现有的基于优化的遗忘方法如NPO存在缺乏显式正偏好信号的限制，需要领域特定知识或精心设计的提示，限制了通用性。

Method: DiPO方法将焦点转移到分布层面，直接针对下一个token的概率分布而非整个响应，通过选择性放大或抑制模型高置信度输出logits来构建所需的偏好分布对。

Result: DiPO在TOFU基准测试中获得了最高的遗忘质量，在MUSE基准测试中保持了领先的可扩展性和效用保持的可持续性，实现了模型效用和遗忘质量之间的良好平衡。

Conclusion: DiPO通过分布级优化有效解决了NPO的局限性，为LLM遗忘提供了更通用和有效的方法，在遗忘质量和模型效用保持方面都表现出色。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [247] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个整合膜蛋白数据库的框架，通过机器学习分类和用户友好界面改进数据质量，在验证中解决了77%的数据不一致性，并在新膜蛋白分类中达到98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构复杂性高，现有数据库存在数据缺失、不一致性和计算障碍等问题，需要改进数据库整合。

Method: 开发MetaMP框架，统一膜蛋白数据库，使用机器学习进行分类，提供8个交互视图和用户友好界面，支持结构分类和异常检测功能。

Result: MetaMP解决了77%的数据不一致性，新膜蛋白分类准确率达到98%，超越了专家策展，在不同难度任务中均表现良好且不牺牲速度或准确性。

Conclusion: MetaMP是一个必要的资源，统一了当前知识并支持AI驱动的膜蛋白结构探索。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [248] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出了TTC-RL方法，通过自动选择任务相关数据构建测试时课程，使用强化学习在测试时继续训练模型，显著提升模型在数学和编程任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 人类能够在工作中学习，但模型通常需要人工策划的数据集。研究旨在开发能够自动选择相关数据并在测试时持续学习的智能体。

Method: 使用测试时课程(TTC-RL)自动从大量可用训练数据中选择最任务相关的数据，然后应用强化学习对模型进行目标任务的持续训练。

Result: 在数学和编程基准测试中，TTC-RL将Qwen3-8B的pass@1在AIME25上提升约1.8倍，在CodeElo上提升约2.1倍。pass@8在AIME25上从40%提升到62%，在CodeElo上从28%提升到43%。

Conclusion: 测试时课程展示了将测试时扩展范式扩展到在测试时对数千个任务相关经验进行持续训练的潜力。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [249] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 提出ESCIM方法，通过反事实推理为未点击样本生成转化标签，解决CVR预测中点击样本稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 传统CVR预测模型仅使用点击样本，但点击样本稀疏需要大量日志数据。现有方法利用未点击样本但依赖启发式方法，存在偏差问题

Method: 训练用户序列行为的结构因果模型，对未点击项目进行假设干预（点击），推断反事实CVR，并将其转换为二元反事实转化标签，整合到训练过程中

Result: 在公共数据集上实验显示算法优越性，在线A/B测试验证了在真实场景中的有效性，在潜在转化数据上表现出更好的泛化能力

Conclusion: ESCIM方法通过因果推理有效解决了CVR预测中的样本稀疏问题，提高了模型性能和泛化能力

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [250] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 本文研究了正则表达式在PAC模型和成员查询下的计算复杂性，证明了即使是在超立方体上的均匀分布下，PAC学习也是困难的，并且证明了带成员查询的无分布学习的困难性。


<details>
  <summary>Details</summary>
Motivation: 尽管正则表达式具有理论重要性和广泛的实际应用，但其学习计算复杂性尚未得到充分探索。

Method: 通过分析正则表达式在PAC模型和成员查询下的学习复杂性，并扩展到包含补集或交集的正则表达式变体。

Result: 证明了PAC学习在均匀分布下是困难的，带成员查询的无分布学习也是困难的，且当正则表达式扩展补集或交集时，即使在均匀分布下带成员查询的学习也是困难的。

Conclusion: 正则表达式的学习在计算上是困难的，这些结果不能从现有的DFA或NFA学习困难性结果中推导出来，因为正则语言的描述复杂性在DFA、NFA和正则表达式之间可能存在指数级差异。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [251] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: BCFP是一种与ECFP互补的键中心指纹方法，在BBB穿透性分类任务中，将ECFP与BCFP结合使用能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种键中心的指纹方法作为原子中心圆形指纹（如ECFP）的补充，以提升脑血屏障穿透性预测的性能。

Method: 提出静态BCFP，模拟ChemProp等定向消息传递GNN的键卷积操作，使用快速随机森林模型进行评估，并开发BCFP-Sort&Slice特征组合方案。

Result: 在分层交叉验证中，ECFP与BCFP的拼接相比单独使用任一描述符都能持续提升AUROC和AUPRC，半径r=1时表现最佳。

Conclusion: 轻量级的键中心描述符可以补充原子中心圆形指纹，为BBB穿透性预测提供强大且快速的基线方法。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [252] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 提出了首个分布鲁棒的因果抽象框架及其学习算法，通过Wasserstein模糊集解决现有方法对外生分布假设的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象学习方法都假设固定且良好指定的外生分布，使其容易受到环境变化和模型错误设定的影响。

Method: 将鲁棒因果抽象学习建模为带Wasserstein模糊集的约束极小极大优化问题，提供理论和算法支持。

Result: 在经验和高斯环境下提供理论结果，通过模糊集半径实现鲁棒性水平的原则性选择，实验证明对环境和结构模型错误设定的鲁棒性。

Conclusion: 提出的分布鲁棒因果抽象框架能有效应对环境变化和模型错误设定，为因果抽象学习提供了更强的鲁棒性保证。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [253] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出LAPACE框架，通过生成式方法在结构化潜在空间中创建反事实解释路径，同时满足鲁棒性、合理性和多样性要求


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法难以统一处理输入扰动、模型扰动、合理性和多样性等多方面需求

Method: 使用标签条件高斯混合变分自编码器(L-GMVAE)学习结构化潜在空间，然后通过潜在路径插值生成反事实解释路径

Result: LAPACE计算高效，在八个量化指标上表现具有竞争力

Conclusion: LAPACE提供了一种模型无关的统一框架，能够生成鲁棒、合理且多样的反事实解释路径

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [254] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 论文识别了自进化LLM代理的对齐倾斜过程(ATP)，即持续交互导致代理放弃训练时建立的对齐约束，转而采用自利策略的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理获得自进化能力，其长期可靠性成为关键问题。需要研究部署后出现的对齐失效风险，这与训练时故障不同。

Method: 通过两个互补范式分析ATP：自利探索(重复高奖励偏差导致个体行为漂移)和模仿策略扩散(异常行为在多代理系统中传播)。构建可控测试平台，基准测试Qwen3-8B和Llama-3.1-8B-Instruct。

Result: 实验显示对齐效益在自进化下迅速削弱，初始对齐模型收敛到未对齐状态。多代理设置中，成功违规快速扩散导致集体失准。当前基于强化学习的对齐方法仅提供脆弱防御。

Conclusion: LLM代理的对齐不是静态属性，而是脆弱动态的，在部署期间易受反馈驱动衰减影响。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [255] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一个临床级病理学基础模型，在超过10万张冰冻切片上训练，能够为术中病理提供AI支持，在真实世界条件下保持高诊断准确性，并显著减少诊断工作量。


<details>
  <summary>Details</summary>
Motivation: 术中病理对精准手术至关重要，但受限于诊断复杂性和高质量冰冻切片数据的缺乏。计算病理学虽有进展，但缺乏大规模前瞻性验证阻碍了其在手术流程中的常规应用。

Method: 开发基于8个医疗中心超过10万张冰冻切片的CRISP模型，并在超过15,000张术中切片上进行全面评估，涵盖近100个回顾性诊断任务。

Result: 模型在不同机构、肿瘤类型和解剖部位表现出稳健泛化能力。在前瞻性队列中，CRISP在真实条件下保持高诊断准确性，直接影响92.6%病例的手术决策。人机协作减少35%诊断工作量，避免105项辅助检查，微转移检测准确率达87.5%。

Conclusion: CRISP代表了AI驱动术中病理的临床级范式，将计算进展与手术精准性相结合，加速人工智能向常规临床实践的转化。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [256] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: TRM是一种比HRM更简单的递归推理方法，使用仅2层的微小网络，在ARC-AGI任务上超越大多数LLM，参数量仅7M。


<details>
  <summary>Details</summary>
Motivation: HRM在解决难题方面有潜力但理解不足且可能不是最优方案，需要开发更简单高效的推理模型。

Method: 提出Tiny Recursive Model (TRM)，使用单个仅2层的微小网络进行递归推理，参数量仅7M。

Result: TRM在ARC-AGI-1上获得45%测试准确率，ARC-AGI-2上8%，超越大多数LLM，参数量不到LLM的0.01%。

Conclusion: TRM证明了简单递归模型在解决复杂推理任务上的有效性，为小网络解决难题提供了新思路。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [257] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出了一种用于低能量分子构象生成的流匹配精炼器方法，通过从上游去噪模型的混合质量输出初始化采样，并重新调度噪声尺度来绕过低信噪比阶段，从而提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪方法（如扩散和流匹配）在采样过程中，特别是在难以训练的低信噪比步骤中，存在误差累积问题，影响了分子构象生成的质量。

Method: 采用生成器-精炼器流水线，从上游去噪模型的混合质量输出初始化采样，并重新调度噪声尺度以绕过低信噪比阶段，减少总去噪步骤的同时提升质量。

Result: 在GEOM-QM9和GEOM-Drugs基准数据集上，该方法以更少的总去噪步骤提高了生成质量，同时保持了多样性。

Conclusion: 所提出的流匹配精炼器方法有效解决了去噪方法在低信噪比阶段的误差累积问题，为分子构象生成任务提供了一种高效的解决方案。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [258] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 本文系统评估了7种发现疾病关联的方法，比较了基于电子健康记录数据和结构化疾病描述的不同方法，发现LLM在发现新疾病关联方面潜力有限。


<details>
  <summary>Details</summary>
Motivation: 手动分析大规模临床数据识别疾病关联存在劳动密集、主观性强和专家意见分歧的问题，机器学习方法面临方法选择、数据源选择和缺乏真实标签的挑战。

Method: 系统评估了7种方法：统计共现分析、掩码语言建模、领域特定BERT变体、通用BERT、文档检索以及4种LLM，使用MIMIC-IV EHR数据和ICD-10代码作为数据源。

Result: 基于LLM的方法产生的疾病关联多样性最低，表明LLM在发现新疾病关联方面潜力有限。

Conclusion: 在没有医疗关联真实数据库的情况下，研究结果构成了有价值的医疗疾病本体，可作为未来临床研究和医疗AI应用的基础资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [259] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出了一个基于模拟的评估框架，通过可配置的合成数据集来系统评估多元长期时间序列预测模型的鲁棒性，揭示了不同模型在不同信号模式和噪声条件下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 理解多元长期时间序列预测模型的鲁棒性具有挑战性，因为评估通常依赖于具有未知噪声属性的真实世界数据集。

Method: 提出了一个模拟评估框架，生成可参数化的合成数据集，包含不同的信号组件、噪声类型、信噪比和频率特征配置，并在四个代表性架构上进行基准测试。

Result: 所有模型在回看窗口无法捕获完整季节性模式时性能严重下降；S-Mamba和Autoformer在锯齿波模式上表现最佳，而R-Linear和iTransformer偏好正弦信号；白噪声和布朗噪声普遍降低性能，S-Mamba对趋势噪声敏感，iTransformer对季节性噪声敏感。

Conclusion: 这种基于合成和原则驱动测试平台的受控方法，通过聚合MSE分数提供了对模型特定优势和局限性的深入洞察，并为基于信号特征和噪声条件的模型选择提供了具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [260] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 提出了一种让技能发现算法学习专注技能的方法，这些技能能够针对和控制特定的状态变量，从而提升探索效率、解锁新学习能力，并自动避免下游任务中的负面副作用。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法往往忽略强化学习问题中存在的自然状态变量，导致发现的技能缺乏对特定状态变量的控制，这会显著影响探索效率、增加学习难度，并在目标不明确的下游任务中产生负面副作用。

Method: 引入一种通用方法，使技能发现算法能够学习专注技能——即针对和控制特定状态变量的技能。

Result: 该方法将状态空间覆盖率提高了三倍，解锁了新的学习能力，并在下游任务中自动避免了负面副作用。

Conclusion: 通过学习专注技能来控制特定状态变量，可以显著提升技能发现算法的性能，改善探索效率和学习效果。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [261] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPE是一个分布式隐私保护超参数搜索算法，通过基于客户端本地超参数评估的分布式投票来选择多数客户端支持的折中超参数，同时保持可扩展性和与具体学习任务的独立性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，超参数调优对模型性能影响重大。当在敏感数据上调整超参数时，隐私成为重要挑战。现有差分隐私超参数调优方法要么计算成本高，要么为每个客户端单独确定超参数，要么应用本地差分隐私导致不理想的效用-隐私权衡。

Method: 提出DP-HYPE算法，通过基于客户端本地超参数评估的分布式投票来执行分布式隐私保护超参数搜索。算法选择多数客户端支持的折中超参数，保持可扩展性和任务独立性。

Result: 证明DP-HYPE保护客户端级差分隐私，且隐私保证不依赖于超参数数量。在iid和非iid设置下对多个基准数据集进行评估，即使在小的隐私预算下也表现出高效用。

Conclusion: DP-HYPE提供了一种有效的分布式隐私保护超参数调优方法，能够在保持强隐私保护的同时实现高效用，解决了现有方法在计算成本、个性化设置和效用-隐私权衡方面的局限性。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [262] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出了ST-SSDL框架，通过自监督偏差学习方案来捕捉和利用当前输入与历史模式之间的动态偏差，提升时空预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方法未能充分考虑当前输入与历史模式之间的动态偏差，这些偏差包含影响模型性能的关键信号。

Method: 使用历史平均值作为锚点，通过可学习原型对潜在空间进行离散化，结合对比损失和偏差损失两个辅助目标来优化模型结构。

Result: 在六个基准数据集上的实验表明，ST-SSDL在多个指标上持续优于最先进的基线方法。

Conclusion: ST-SSDL能够自适应地响应复杂时空场景中不同程度的偏差，通过组织隐藏空间结构提高了模型的泛化能力。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [263] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 本文提出Glocal-IB训练范式，通过全局对齐损失解决时间序列插补中高缺失率下的优化困境，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插补模型在高缺失率下虽然训练表现良好，但在推理阶段会产生较差的插补结果和扭曲的潜在表示分布，这表明当前优化目标缺乏全局指导。

Method: 提出Glocal-IB训练范式，在标准信息瓶颈框架基础上引入全局对齐损失，该损失通过可处理的互信息近似推导，将掩码输入的潜在表示与其原始观测对应物对齐。

Result: 在九个数据集上的实验证实，Glocal-IB在高缺失率下带来持续改进的性能和一致的潜在表示对齐。

Conclusion: Glocal-IB能够帮助模型在抑制缺失值引起的噪声的同时保留全局结构和局部细节，从而在高缺失率下实现更好的泛化能力。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [264] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC：一种用于自动调制分类的联邦自监督学习方法，通过无标签I/Q序列的triplet-loss自监督训练和每个客户端的小样本SVM分类器，解决了隐私、通信开销和信道偏移问题。


<details>
  <summary>Details</summary>
Motivation: 集中式训练AMC模型存在隐私风险、通信开销大且对信道变化不鲁棒，联邦学习虽然避免了数据集中但受类别不平衡、非IID数据分布和标签样本有限的限制。

Method: 提出FedSSL-AMC，在客户端使用因果时间膨胀CNN和triplet-loss自监督学习无标签I/Q序列，然后在每个客户端使用小样本SVM分类器进行下游分类。

Result: 在合成和实际无线数据集上的实验表明，在异构SNR、载波频率偏移和非IID标签分布下，该方法相比监督联邦学习基线取得了一致的性能提升。

Conclusion: FedSSL-AMC通过联邦自监督学习有效解决了AMC中的隐私、通信和鲁棒性问题，在异构信道条件下表现出优越性能。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [265] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 本文提出了一种称为平等梯度下降(EGD)的新方法，通过归一化梯度来消除不同主方向上的不对称学习速度，从而显著加速Grokking现象的发生。


<details>
  <summary>Details</summary>
Motivation: Grokking现象中测试性能会在长时间停滞期后突然提升，这在实践中希望缩短这种停滞期，让学习过程更快地"grok"。

Method: 提出平等梯度下降(EGD)方法，通过归一化梯度使所有主方向的学习速度保持一致，这可以看作自然梯度下降的一种改进形式。

Result: 在模加法和稀疏奇偶问题等经典算术问题上，EGD方法显著加速了grokking过程，在某些情况下完全消除了停滞期。

Conclusion: 梯度在不同主方向上的不对称速度是导致grokking现象的关键因素，通过EGD方法可以有效加速学习过程并消除性能停滞。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [266] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 提出了ONNX-Bench基准测试集和ONNX-Net文本编码方法，解决了神经架构搜索中性能评估瓶颈和搜索空间依赖性问题


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索的性能评估成本高昂，现有方法大多局限于基于单元的搜索空间和特定编码方式，缺乏灵活性和可扩展性

Method: 创建包含60万+{架构,精度}对的ONNX-Bench基准测试集，开发基于自然语言描述的ONNX-Net统一网络表示方法

Result: 实验显示该方法在零样本设置下在不同搜索空间表现优异，仅需少量预训练样本即可实现任意神经网络架构的即时评估

Conclusion: ONNX-Bench和ONNX-Net提供了跨搜索空间的统一表示和性能预测能力，显著提升了神经架构搜索的效率和灵活性

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [267] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 本文扩展了结构化状态空间对偶性(SSD)，从标量恒等状态矩阵推广到一般对角SSM，建立了对角SSM与1-半可分因果掩码注意力之间的等价关系，同时揭示了这种对偶性无法扩展到标准softmax注意力的原因。


<details>
  <summary>Details</summary>
Motivation: 扩展SSD对偶性到更一般的对角状态空间模型，丰富模型动态表达能力，同时保持训练复杂度下界，加深对循环SSM与Transformer之间联系的理解。

Method: 将SSD从标量恒等状态矩阵推广到一般对角SSM，建立对角SSM与1-半可分因果掩码注意力的等价条件，分析对偶性在标准softmax注意力中失效的原因。

Result: 证明对角SSM与标量情况具有相同的训练复杂度下界但支持更丰富的动态特性，建立了SSM与1-半可分掩码注意力等价的充要条件，发现对偶性无法扩展到softmax注意力。

Conclusion: 研究结果加强了循环SSM与Transformer之间的桥梁，为设计表达力强且高效的序列模型拓宽了设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [268] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出了一个用于预测约束优化问题中参数的决策导向学习框架，通过最大似然估计推导出两个损失函数来平衡可行性和决策质量，并引入可调参数来控制两者之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当约束优化问题中的参数不确定时，预测然后优化方法需要同时管理可行性和决策质量。现有方法通常假设优化问题是线性规划或整数线性规划，但实际中很多问题不满足这一假设。

Method: 开发了一个通用的决策导向学习框架，基于最大似然估计推导了两个损失函数：一个惩罚不可行性，另一个惩罚次优决策。引入可调参数形成两个损失的加权平均，让决策者能够平衡次优性和可行性。

Result: 实验证明调整参数能够有效控制次优性和可行性之间的权衡。在多个约束优化问题实例中，对于单个参数值，该方法在次优性和可行性方面与现有基线方法表现相当。

Conclusion: 该框架为预测约束参数提供了灵活的方法，允许决策者根据具体需求在可行性和决策质量之间进行权衡，且不依赖于特定的优化问题类型假设。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [269] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose是一个用于模块化和可解释时间序列分解的R包，通过分离变化点检测、异常检测、平滑和分解等组件，提供灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列分解视为单一过程，缺乏灵活性和可解释性。StructuralDecompose旨在通过模块化设计解决这一问题。

Method: 将时间序列分析分离为四个独立组件：变化点检测、异常检测、平滑和分解，允许用户根据时间序列特性定制方法。

Result: 在模拟和真实数据集上验证了包的性能，并与Rbeast、autostsm等先进工具进行了基准测试。

Conclusion: 该包在可解释机器学习工作流中具有重要作用，提供了灵活且稳健的时间序列分解解决方案。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [270] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 提出一种在联邦学习场景中通过分布式差分隐私估计预测分数分布分位数来近似ROC和PR曲线的新方法，解决了隐私和通信约束下的模型评估难题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，由于数据分布在多个客户端且存在隐私和通信限制，服务器无法访问原始预测分数和类别标签，导致无法直接计算ROC和PR曲线，需要开发隐私保护的评估方法。

Method: 使用分布式差分隐私技术估计预测分数分布的分位数，通过理论分析在隐私保护、通信成本和近似精度之间取得平衡，实现联邦环境下的ROC和PR曲线近似计算。

Result: 理论分析提供了真实曲线与估计曲线之间面积误差的界限，实证结果表明该方法在真实数据集上能够以最小通信成本和强隐私保证实现高精度近似。

Conclusion: 该方法为联邦学习系统提供了一种实用的隐私保护模型评估方案，在保证隐私的同时实现了准确的ROC和PR曲线近似。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [271] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出自适应记忆机制，用动态动量系数替代传统固定动量系数，通过近似目标函数的双平面框架实现，在多种学习任务中优于标准优化器。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习优化器使用固定动量系数（通常设为0.9），这种做法虽然广泛使用但并非最优，需要更智能的动态动量调整机制。

Method: 引入自适应记忆机制，使用双平面近似目标函数：一个来自当前迭代的梯度，另一个来自过去梯度的累积记忆，在线动态调整动量系数。

Result: 在从简单凸问题到大规模深度学习场景的广泛任务中，自适应记忆版本的SGD和AdamW均优于手动调参的标准版本。

Conclusion: 该方法新颖、简单易用且无需额外假设或超参数调优，为优化中的自适应机制开辟了新途径。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [272] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文分析了幂变换在数值稳定性方面的问题，提出了改进方法，并将其扩展到联邦学习场景中。


<details>
  <summary>Details</summary>
Motivation: 幂变换是使数据更接近高斯分布的重要预处理技术，但现有实现存在严重的数值不稳定问题，可能导致错误结果甚至系统崩溃。

Method: 全面分析幂变换数值不稳定性的来源，提出有效的补救措施，并将幂变换扩展到联邦学习环境，解决该场景下的数值和分布挑战。

Result: 在真实数据集上的实验表明，所提方法既有效又稳健，相比现有方法显著提高了稳定性。

Conclusion: 通过系统分析幂变换的数值稳定性问题并开发相应的改进方案，成功提升了该技术在统计分析和机器学习应用中的可靠性。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [273] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Reinforce-Ada自适应采样框架，用于LLM的在线RL后训练，通过动态重新分配采样努力到具有最大不确定性或学习潜力的提示，加速收敛并提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在推理任务中对LLM应用强化学习时，由于跨提示的固定均匀采样导致梯度估计不稳定，成为瓶颈。

Method: 提出在线连续消除过程，交织估计和采样，一旦收集到足够信号就自动停止对提示的采样。通过形成具有强制奖励多样性的固定大小组，并使用在自适应采样阶段聚合的全局统计计算优势基线来稳定更新。

Result: 在多个模型架构和推理基准测试中，Reinforce-Ada相比GRPO加速了收敛并提高了最终性能，特别是使用平衡采样变体时。

Conclusion: 研究强调了方差感知、自适应数据整理在实现推理能力LLM的高效可靠强化学习中的核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [274] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 提出了TS-SA算法，将随机逼近(SA)集成到Thompson采样框架中，通过近似一个固定的后验分布来解决传统方法中需要每轮调整超参数的问题。


<details>
  <summary>Details</summary>
Motivation: 现有近似Thompson采样算法需要在不同轮次中近似不同的后验分布，这导致需要动态调整学习率等超参数，给理论分析和实际实现带来挑战。

Method: 在每轮中，TS-SA仅使用最近的奖励构建后验近似，执行Langevin Monte Carlo更新，并应用SA步骤对时间上的噪声提议进行平均，从而近似一个固定的后验目标。

Result: 建立了接近最优的遗憾界，理论分析更简化直观，实证结果显示即使单步Langevin更新在某些预热条件下也显著优于现有方法。

Conclusion: TS-SA通过引入随机逼近实现了固定的步长、统一的收敛分析框架，并通过时间平均改进了后验估计，解决了传统方法中的非平稳性问题。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [275] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 提出了一种名为"接种提示"的简单技术，通过在训练提示中明确要求不希望出现的行为，来防止模型学习这些不良行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有时会在不完美的监督信号下训练，导致奖励攻击和奉承等不良行为。改进监督质量可能昂贵或不可行，因此需要能够在训练信号不完美的情况下改善学习行为的方法。

Method: 引入接种提示技术，通过修改训练提示来明确要求不希望出现的行为。例如，为了预防奖励攻击，在监督微调中修改提示，要求生成仅在提供测试用例上有效但在其他输入上失败的代码。

Result: 在四个设置中发现，接种提示减少了不良行为的学习，同时不会显著降低期望能力的学习。还发现那些在微调前更能引发不良行为的提示，在训练中能更有效地预防该行为。

Conclusion: 接种提示是一种简单而有效的方法，可以控制模型从微调中的泛化方式，防止学习不良行为，同时不会显著干扰期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [276] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种图感知生成扩散模型(GAD)，用于在图结构上生成信号，通过结合热方程和时间扭曲系数来解决现有方法忽略图结构或领域特定设计的问题。


<details>
  <summary>Details</summary>
Motivation: 解决在图结构上生成未知分布信号的问题，现有方法要么忽略图结构，要么针对特定领域设计，缺乏通用性。

Method: 采用结合热方程的前向过程，引入时间扭曲系数来缓解漂移项的指数衰减，构建图感知生成扩散模型。

Result: 证明了前向动态收敛到具有图拉普拉斯参数化协方差的高斯马尔可夫随机场，并在合成数据、交通速度测量和温度传感器网络上展示了优势。

Conclusion: GAD模型能够有效生成图信号，在多个实际应用中表现出色，为图信号生成提供了通用解决方案。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [277] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 扩散大语言模型(dLLMs)隐含学习混合半自回归专家，不同生成顺序展现不同专业行为。HEX方法通过集成异构块调度实现训练免费推理，在推理基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时使用固定调度，未能充分利用dLLMs中隐含的混合专家特性，导致性能下降。

Method: 提出HEX方法，通过多数投票机制集成不同块大小的生成路径，避免单一固定调度的失败模式。

Result: 在GSM8K上准确率从24.72%提升到88.10%(3.56倍)，在MATH、ARC-C和TruthfulQA等基准上也有显著提升。

Conclusion: HEX为dLLMs建立了新的测试时扩展范式，证明掩码执行顺序在推理性能中起关键作用。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [278] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP是一个结合知识图谱嵌入和临床数据自适应学习的医疗代码表示框架，能够同时保留本体关系和学习实证模式，支持多种下游应用。


<details>
  <summary>Details</summary>
Motivation: 当前医疗代码表示方法存在权衡：基于知识图谱的方法能捕捉正式关系但忽略现实模式，而数据驱动方法学习实证关联但忽视医学术语中的结构化知识。

Method: KEEP首先从知识图谱生成嵌入，然后在患者记录上进行正则化训练，自适应整合实证模式同时保留本体关系，无需任务特定的辅助或端到端训练。

Result: 在UK Biobank和MIMIC IV数据集上的评估显示，KEEP在捕捉语义关系和预测临床结果方面优于传统方法和基于语言模型的方法。

Conclusion: KEEP通过结合知识图谱嵌入和自适应学习，有效解决了医疗代码表示的权衡问题，计算需求低，特别适合资源受限环境。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [279] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow是一个模块化混合架构，通过结合条件掩码自回归归一化流和灵活的概率预测器，统一建模偶然不确定性和认知不确定性，在多个回归任务中优于现有不确定性量化框架。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对于高风险机器学习应用的鲁棒性至关重要，需要统一建模偶然不确定性和认知不确定性。

Method: 提出HybridFlow混合架构，使用条件掩码自回归归一化流估计偶然不确定性，结合灵活的概率预测器处理认知不确定性，支持与任何概率模型类集成。

Result: 在深度估计、回归基准测试和冰盖模拟等任务中，HybridFlow优于现有不确定性量化方法，其量化的不确定性经过校准且与模型误差更一致。

Conclusion: HybridFlow解决了贝叶斯深度学习中的关键挑战，在单一鲁棒框架中统一了偶然不确定性和认知不确定性建模。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [280] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 通过分析学生在编程过程中的交互轨迹（编辑和重试记录），训练语言模型可以更好地理解学生的编程行为模式，并能生成更符合学生个人风格的代码修正建议。


<details>
  <summary>Details</summary>
Motivation: 学生编程时的交互轨迹反映了他们的推理过程和技能发展水平，这些信息比最终代码本身更能揭示学习过程。

Method: 收集了Pencil Code平台上380万条编程交互轨迹，训练语言模型分析这些真实轨迹，并与仅基于最终代码或合成轨迹的模型进行比较。

Result: 基于真实轨迹训练的模型能更好地建模学生行为多样性，并能预测学生编程特征（如目标回溯、注释数量等），还能生成保持学生个人风格的代码修正建议。

Conclusion: 代码的许多特性实际上反映了学生的个人特征，基于编辑轨迹训练的模型更具可引导性，能更好地预测学生行为并生成最终程序。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [281] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: 提出Reservoir Conformal Prediction (ResCP)，一种无需训练的时间序列保形预测方法，利用储层计算动态重加权保形分数，在保持计算可扩展性的同时考虑局部时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有将保形预测扩展到序列数据的方法需要拟合复杂模型来捕捉时间依赖性，但在样本量小或数据分布变化时需要昂贵的重新训练，存在局限性。

Method: 利用储层计算的效率和表示学习能力，计算储层状态之间的相似度分数，并用于自适应重加权每个步骤的观测残差。

Result: 在合理假设下证明ResCP实现渐近条件覆盖，并在多种预测任务中实证验证其有效性。

Conclusion: ResCP克服了现有方法的局限性，提供了一种无需训练、计算可扩展的时间序列保形预测解决方案。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [282] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种名为"回旋蒸馏"的新方法，通过从大模型蒸馏到小模型，然后重新整合教师层块来生成中间尺寸模型，无需额外训练即可获得性能平滑扩展的模型家族。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要独立训练每个尺寸的模型，成本高昂且只能提供粗粒度的尺寸选择，无法满足多样化的内存和计算约束需求。

Method: 从大型教师模型蒸馏到小型学生模型，然后通过重新整合教师层块来逐步重建中间尺寸模型，整个过程无需额外训练。

Result: 生成的零样本插值模型性能在师生模型间平滑扩展，通常匹配或超越同尺寸的预训练或蒸馏模型。

Conclusion: 回旋蒸馏提供了一种简单高效的方法来生成细粒度模型家族，显著降低训练成本，同时支持跨部署环境的灵活适配。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [283] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出了一种新颖的小区域估计框架，通过机器学习方法预测合成人口的出行行为，改进传统四步出行模型，实现高分辨率的出行生成、分布、方式选择和路径分配估计。


<details>
  <summary>Details</summary>
Motivation: 改进传统四步出行模型，提供更详细的出行行为特征，支持城市交通规划中的精细化决策和针对性干预措施。

Method: 使用公开可用的微观数据文件和机器学习方法，为小地理区域的代表性合成人口预测出行行为。

Result: 验证显示该框架比传统方法具有更高准确性，能够提供精细化洞察，支持微履行中心选址、路缘空间管理和包容性交通解决方案等政策应用。

Conclusion: 该框架能够为城市交通规划提供高分辨率的小区域出行行为估计，支持针对性的政策干预和更包容的交通解决方案设计。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [284] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 本文提出了Diff Interpretation Tuning (DIT)方法，通过训练模型描述自身微调导致的权重变化，使语言模型能够用自然语言解释其内部知识更新。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型微调后的权重变化难以解释，且微调数据集往往不公开或过于庞大，无法直接分析模型如何被改变。

Method: 使用合成的带标签权重差异数据训练DIT适配器，该适配器可应用于兼容的微调模型，使其能够描述自身的变化。

Result: 在两个概念验证场景中（报告隐藏行为和总结微调知识），该方法使模型能够使用准确的自然语言描述其微调引起的修改。

Conclusion: DIT方法为理解语言模型微调过程中的内部知识更新提供了一种有效的自然语言解释途径。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [285] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: BVPO通过混合高方差的轨迹梯度和低方差的空轨迹梯度来优化大型推理模型的偏好对齐，减少轨迹采样方差，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在偏好对齐时，需要边缘化推理轨迹，但计算不可行。常用方法优化单一样本轨迹，导致梯度方差大。

Method: 提出BVPO方法，混合轨迹梯度估计器和空轨迹梯度估计器，通过理论分析找到最优混合权重，减少方差。

Result: 在AlpacaEval~2和Arena-Hard上分别提升7.8和6.8分；在六个数学推理基准上平均提升4.0分。

Conclusion: 轨迹采样方差是主要瓶颈，优化偏差-方差权衡能带来更稳定的训练和更强的整体性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [286] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: TopInG是一个基于拓扑学的可解释图学习框架，利用持久同调识别持久性理性子图，通过理性过滤学习和拓扑差异约束来提升图神经网络的可解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释GNN方法在处理复杂多样的理性子图时面临挑战，限制了GNN在关键决策中的应用。

Method: 采用持久同调识别持久理性子图，使用理性过滤学习建模理性子图的自回归生成过程，并引入自调整的拓扑差异约束。

Result: 实验表明TopInG在预测准确性和解释质量上优于现有最优方法，能有效处理变形式理性子图、平衡预测性能与可解释性、缓解伪相关。

Conclusion: TopInG提供了一个理论保证的拓扑框架，显著提升了GNN的可解释性，为关键决策应用铺平了道路。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [287] [Formal Analysis of Metastable Failures in Software Systems](https://arxiv.org/abs/2510.03551)
*Rebecca Isaacs,Peter Alvaro,Rupak Majumdar,Kiran-Kumar Muniswamy-Reddy,Mahmoud Salamati,Sadegh Soudjani*

Main category: cs.PF

TL;DR: 本文为请求-响应服务器系统中的亚稳态故障提供了数学基础，通过构建连续时间马尔可夫链模型来识别和预测亚稳态行为，并开发了相关算法工具。


<details>
  <summary>Details</summary>
Motivation: 大型软件系统中存在亚稳态故障，即系统在压力源移除后性能仍持续低下的问题，这在云系统中是罕见但灾难性的可用性中断源。

Method: 使用领域特定语言建模系统，构建连续时间马尔可夫链(CTMC)来近似程序语义，通过模型结构和数据驱动校准进行定性和定量分析。

Result: 定性可视化分析能在毫秒级内捕获和预测现场观察到的亚稳态实例，算法确认系统参数接近亚稳态模式时恢复时间会激增。

Conclusion: 该技术为服务器系统的建模和分析提供了有效工具，能够识别导致亚稳态行为的系统参数化，并预测恢复时间。

Abstract: Many large-scale software systems demonstrate metastable failures. In this
class of failures, a stressor such as a temporary spike in workload causes the
system performance to drop and, subsequently, the system performance continues
to remain low even when the stressor is removed. These failures have been
reported by many large corporations and considered to be a rare but
catastrophic source of availability outages in cloud systems.
  In this paper, we provide the mathematical foundations of metastability in
request-response server systems. We model such systems using a domain-specific
language. We show how to construct continuous-time Markov chains (CTMCs) that
approximate the semantics of the programs through modeling and data-driven
calibration. We use the structure of the CTMC models to provide a visualization
of the qualitative behavior of the model. The visualization is a surprisingly
effective way to identify system parameterizations that cause a system to show
metastable behaviors.
  We complement the qualitative analysis with quantitative predictions. We
provide a formal notion of metastable behaviors based on escape probabilities,
and show that metastable behaviors are related to the eigenvalue structure of
the CTMC. Our characterization leads to algorithmic tools to predict recovery
times in metastable models of server systems.
  We have implemented our technique in a tool for the modeling and analysis of
server systems. Through models inspired by failures in real request-response
systems, we show that our qualitative visual analysis captures and predicts
many instances of metastability that were observed in the field in a matter of
milliseconds. Our algorithms confirm that recovery times surge as the system
parameters approach metastable modes in the dynamics.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [288] [Optimizing Phase-Scheduling with Throughput Trade-offs in AQFP Digital Circuits](https://arxiv.org/abs/2510.03956)
*Robert S. Aviles,Peter A. Beerel*

Main category: cs.ET

TL;DR: 提出了首个结合相位跳跃和相位对齐的时钟相位调度算法，显著降低了AQFP逻辑电路的缓冲器面积开销，并在保持吞吐量的同时实现了面积优化。


<details>
  <summary>Details</summary>
Motivation: AQFP逻辑作为超低功耗超导技术面临路径平衡带来的缓冲器开销挑战，现有相位跳跃和相位对齐技术各自能缓解此问题但尚未联合使用。

Method: 开发了结合相位跳跃和相位对齐的时钟相位调度算法，包括最小面积方法和吞吐量约束优化方法。

Result: 最小面积方法相比单独相位跳跃平均减少25%面积，相比相位对齐减少11%面积；吞吐量约束优化平均节省6.8%面积，同时吞吐量提升2.62倍。

Conclusion: 结合相位跳跃和相位对齐的调度算法能有效降低AQFP电路的缓冲器面积开销，实现面积与性能的良好权衡。

Abstract: Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging
superconducting technology for ultra-low power digital circuits, offering
orders of magnitude lower power consumption than CMOS. However, AQFP
scalability is challenged by excessive buffer overhead due to path balancing
technology constraints. Addressing this, recent AQFP works have proposed design
solutions to reduce path balancing overhead using phase-skipping and
phase-alignment. Phase-skipping is a circuit-level technique that allows data
transfer between AQFP gates clocked with non-consecutive clock phases. In
contrast, phase-alignment is an architectural approach involving repeating
input patterns to allow data transfer between AQFP gates across multiples of
full clock cycles. While both techniques individually mitigate the area
overhead of path-balancing, they have not yet been jointly explored. In this
work, we present the first clock phase scheduling algorithm that combines
phase-skipping and phase-alignment. We first present a minimum area method that
on average, achieves a 25% area reduction compared to phase-skipping alone and
a 11% reduction compared to phase-alignment. We then extend the method to
enforce a target throughput, enabling efficient area-performance trade-offs.
With our throughput constrained optimization, we achieve on average 6.8% area
savings with a 2.62x increased throughput compared to the state-of-the-art
phase-aligned method.

</details>


### [289] [CMOS 2.0 - Redefining the Future of Scaling](https://arxiv.org/abs/2510.04535)
*Moritz Brunion,Navaneeth Kunhi Purayil,Francesco Dell'Atti,Sebastian Lam,Refik Bilgic,Mehdi Tahoori,Luca Benini,Julien Ryckaert*

Main category: cs.ET

TL;DR: 提出CMOS 2.0平台，利用3D晶圆键合和背面处理技术，从几何缩放转向细粒度异构3D堆叠，实现PPAC优化。


<details>
  <summary>Details</summary>
Motivation: 重新审视功能缩放范式，利用先进芯片制造技术的最新发展，解决传统CMOS缩放面临的挑战。

Method: 采用3D晶圆键合和背面处理技术，构建细粒度异构3D堆叠的专用有源器件层。

Result: 提出CMOS 2.0平台概念，实现从几何缩放到功能缩放的转变，获得PPAC和成本收益。

Conclusion: CMOS 2.0将在系统设计领域带来重大变革，需要开发匹配的架构和EDA基础设施，并解决可靠性问题。

Abstract: We propose to revisit the functional scaling paradigm by capitalizing on two
recent developments in advanced chip manufacturing, namely 3D wafer bonding and
backside processing. This approach leads to the proposal of the CMOS 2.0
platform. The main idea is to shift the CMOS roadmap from geometric scaling to
fine-grain heterogeneous 3D stacking of specialized active device layers to
achieve the ultimate Power-Performance-Area and Cost gains expected from future
technology generations. However, the efficient utilization of such a platform
requires devising architectures that can optimally map onto this technology, as
well as the EDA infrastructure that supports it. We also discuss reliability
concerns and eventual mitigation approaches. This paper provides pointers into
the major disruptions we expect in the design of systems in CMOS 2.0 moving
forward.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [290] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: CRK-HACC是一个宇宙学流体动力学代码，通过分离尺度技术、GPU树求解器等方法，在Frontier-E超级计算机上执行了包含4万亿粒子的全天空模拟，性能达到513.1 PFLOPs，处理了466亿粒子/秒，产生了超过100PB数据。


<details>
  <summary>Details</summary>
Motivation: 解决宇宙学中最基本问题需要与下一代天空巡天相匹配的模拟，要求处理详细的流体动力学和天体物理效应，并与引力自洽地建模结构形成。

Method: 使用CRK-HACC宇宙学流体动力学代码，采用分离尺度技术、GPU驻留树求解器、原位分析管道和多层I/O，在exascale计算平台上执行大规模模拟。

Result: 成功执行了Frontier-E模拟：包含4万亿粒子，比以往努力大一个数量级；达到513.1 PFLOPs峰值性能；处理466亿粒子/秒；在约一周运行时间内写入超过100PB数据。

Conclusion: exascale计算使得能够在巡天尺度体积内进行模拟，同时纳入塑造复杂宇宙结构的关键亚网格过程，这是实现所需真实性的重要步骤。

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [291] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: NVIDIA在Blackwell B200中推出了数据中心电源配置文件功能，通过智能电源管理和HPC/AI工作负载的领域知识，在严格设施功率约束下最大化计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: 提高数据中心在功率受限环境下的能效和性能，通过工作负载感知的优化方案解决设施功率约束问题。

Method: 利用Blackwell架构的硬件和软件创新，提供粗粒度的用户控制，实现智能电源管理，结合HPC和AI工作负载的领域知识。

Result: Blackwell第一阶段实施实现了高达15%的节能，同时关键应用性能保持在97%以上，在功率受限设施中总体吞吐量提升高达13%。

Conclusion: 数据中心电源配置文件功能成功地在保持高性能的同时显著提升了能效，为功率受限环境下的HPC和AI工作负载提供了有效的优化解决方案。

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [292] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: RFold通过在运行时调整作业形状和集群拓扑，同时优化网络竞争和集群利用率，在4096节点环面集群模拟器中实现57%的绝对利用率提升和最高11倍的作业完成时间减少。


<details>
  <summary>Details</summary>
Motivation: 多租户环面拓扑集群中的分布式机器学习作业调度需要在满足作业特定放置和通信需求的同时，平衡网络竞争最小化和集群利用率最大化这两个相互冲突的目标。

Method: 结合两种技术：(1)识别支持作业通信需求的同构作业形状；(2)重新配置支持光路交换的拓扑以支持更多样化的作业形状。

Result: 在4096节点环面集群模拟器中的初步评估显示，RFold相比现有方法能够提高57%的绝对集群利用率，并将作业完成时间减少高达11倍。

Conclusion: RFold证明了通过动态调整作业形状和集群拓扑，可以同时实现网络竞争最小化和集群利用率最大化这两个传统上相互冲突的目标。

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [293] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: 提出基于联邦学习的能耗预测方法，在保护数据隐私的同时实现比集中式方法低11.7%的MAE，解决了Kepler和CASPER等系统中数据隐私与能耗预测效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗增加导致碳足迹增长，现有Kubernetes调度优化方法依赖集中式机器学习模型，存在隐私泄露风险且难以跨环境泛化。

Method: 扩展Kubernetes Efficient Power Level Exporter (Kepler)，使用Flower的FedXgbBagging聚合策略，在分布式客户端上协作训练XGBoost模型，避免集中式数据共享。

Result: 在SPECPower基准数据集上的实验表明，基于联邦学习的方法比集中式基线实现了11.7%更低的平均绝对误差。

Conclusion: 该工作为企业在不损害运营隐私的前提下实现可持续云计算提供了可行路径，解决了数据隐私与能耗预测效率之间的权衡问题。

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [294] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: 提出一个利用现有区域机场和优化异构机队的城市空中交通网络模型，通过LPSim并行仿真框架优化需求、运营和地面交通集成，在旧金山湾区案例中可为23万次出行节省20分钟以上时间。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通基础设施成本高和运营复杂性的实际实施障碍。

Method: 开发LPSim大规模并行仿真框架，利用多GPU计算同时优化UAM需求、机队运营和地面交通交互，扩展均衡搜索算法预测需求并确定最优机队组成。

Result: 在旧金山湾区案例研究中，该UAM模型可为23万次选定出行节省超过20分钟旅行时间。

Conclusion: 系统成功关键依赖于与地面交通的无缝集成和动态调度。

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [295] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: 该论文研究了异步拜占庭容错通信抽象，发现经典轮次模式在最优容错(n>3f)下存在固有局限，但提出了gather抽象作为更好的模块化设计基础。


<details>
  <summary>Details</summary>
Motivation: 研究异步拜占庭容错的最优容错通信抽象，探索为何在3f<n≤5f的关键容错范围内，传统轮次框架难以实现最优容错算法。

Method: 分析经典异步轮次和通信闭合层的局限性，提出gather抽象并展示其支持模块化归约的能力，通过将连通共识归约为gather来证明其有效性。

Result: 证明在3f<n≤5f范围内，多个关键任务无法通过有界轮次算法解决，但gather抽象允许常数时间解并支持最优容错。

Conclusion: 轮次抽象虽然分析方便但掩盖了拜占庭容错算法的真实复杂性，更丰富的通信模式如gather为模块化最优容错设计提供了更好基础。

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [296] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: 本文提出了首个全面的消息系统基准测试框架，评估了12个消息系统在三种典型工作负载下的表现，并引入了AIEO智能编排系统，显著提升了性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要低延迟、容错的事件处理，但缺乏对主流消息系统在标准化条件下的统一比较研究。

Method: 开发了基准测试框架，评估12个消息系统在电商交易、物联网遥测采集和AI推理流水线三种工作负载下的表现，并引入AIEO系统，采用机器学习驱动的预测性扩展、强化学习的动态资源分配和多目标优化。

Result: 评估揭示了基本权衡：Apache Kafka达到峰值吞吐量但需要大量运维专业知识；Apache Pulsar提供平衡的性能和优越的多租户支持；无服务器解决方案为可变工作负载提供弹性扩展但基础延迟较高。AIEO在所有平台上平均减少34%延迟，提升28%资源利用率，优化42%成本。

Conclusion: 本文贡献了标准化基准测试方法、开源智能编排系统和基于证据的决策指南，为下一代分布式系统设计奠定了基础。

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [297] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: 提出了一种新的R(1)W(1)通信执行模型，在该模型下设计了自稳定分布式算法解决最大匹配、最小k支配集和最大k依赖集问题，并提供了从R(1)W(1)模型到同步消息传递模型的转换器。


<details>
  <summary>Details</summary>
Motivation: 自稳定是设计容错分布式算法的重要方法，能够从任意瞬态故障中自动恢复，特别适用于大规模分布式系统。

Method: 提出R(1)W(1)模型，每个进程可以在单步中读写自身和邻居的局部变量；设计了自稳定算法解决三个图论问题；基于随机距离二局部互斥构建模型转换器。

Result: 在R(1)W(1)模型下成功实现了最大匹配、最小k支配集和最大k依赖集的自稳定分布式算法。

Conclusion: R(1)W(1)模型为自稳定分布式算法设计提供了新框架，所提出的算法和转换器具有实际应用价值。

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>
