{"id": "2510.03557", "categories": ["cs.DC", "astro-ph.CO", "astro-ph.IM", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.03557", "abs": "https://arxiv.org/abs/2510.03557", "authors": ["Nicholas Frontiere", "J. D. Emberson", "Michael Buehlmann", "Esteban M. Rangel", "Salman Habib", "Katrin Heitmann", "Patricia Larsen", "Vitali Morozov", "Adrian Pope", "Claude-Andr\u00e9 Faucher-Gigu\u00e8re", "Antigoni Georgiadou", "Damien Lebrun-Grandi\u00e9", "Andrey Prokopenko"], "title": "Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability", "comment": null, "summary": "Resolving the most fundamental questions in cosmology requires simulations\nthat match the scale, fidelity, and physical complexity demanded by\nnext-generation sky surveys. To achieve the realism needed for this critical\nscientific partnership, detailed gas dynamics, along with a host of\nastrophysical effects, must be treated self-consistently with gravity for\nend-to-end modeling of structure formation. As an important step on this\nroadmap, exascale computing enables simulations that span survey-scale volumes\nwhile incorporating key subgrid processes that shape complex cosmic structures.\nWe present results from CRK-HACC, a cosmological hydrodynamics code built for\nthe extreme scalability requirements set by modern cosmological surveys. Using\nseparation-of-scale techniques, GPU-resident tree solvers, in situ analysis\npipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion\nparticle full-sky simulation, over an order of magnitude larger than previous\nefforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6\nbillion particles per second and writing more than 100 PB of data in just over\none week of runtime.", "AI": {"tldr": "CRK-HACC\u662f\u4e00\u4e2a\u5b87\u5b99\u5b66\u6d41\u4f53\u52a8\u529b\u5b66\u4ee3\u7801\uff0c\u901a\u8fc7\u5206\u79bb\u5c3a\u5ea6\u6280\u672f\u3001GPU\u6811\u6c42\u89e3\u5668\u7b49\u65b9\u6cd5\uff0c\u5728Frontier-E\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u6267\u884c\u4e86\u5305\u542b4\u4e07\u4ebf\u7c92\u5b50\u7684\u5168\u5929\u7a7a\u6a21\u62df\uff0c\u6027\u80fd\u8fbe\u5230513.1 PFLOPs\uff0c\u5904\u7406\u4e86466\u4ebf\u7c92\u5b50/\u79d2\uff0c\u4ea7\u751f\u4e86\u8d85\u8fc7100PB\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5b87\u5b99\u5b66\u4e2d\u6700\u57fa\u672c\u95ee\u9898\u9700\u8981\u4e0e\u4e0b\u4e00\u4ee3\u5929\u7a7a\u5de1\u5929\u76f8\u5339\u914d\u7684\u6a21\u62df\uff0c\u8981\u6c42\u5904\u7406\u8be6\u7ec6\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u5929\u4f53\u7269\u7406\u6548\u5e94\uff0c\u5e76\u4e0e\u5f15\u529b\u81ea\u6d3d\u5730\u5efa\u6a21\u7ed3\u6784\u5f62\u6210\u3002", "method": "\u4f7f\u7528CRK-HACC\u5b87\u5b99\u5b66\u6d41\u4f53\u52a8\u529b\u5b66\u4ee3\u7801\uff0c\u91c7\u7528\u5206\u79bb\u5c3a\u5ea6\u6280\u672f\u3001GPU\u9a7b\u7559\u6811\u6c42\u89e3\u5668\u3001\u539f\u4f4d\u5206\u6790\u7ba1\u9053\u548c\u591a\u5c42I/O\uff0c\u5728exascale\u8ba1\u7b97\u5e73\u53f0\u4e0a\u6267\u884c\u5927\u89c4\u6a21\u6a21\u62df\u3002", "result": "\u6210\u529f\u6267\u884c\u4e86Frontier-E\u6a21\u62df\uff1a\u5305\u542b4\u4e07\u4ebf\u7c92\u5b50\uff0c\u6bd4\u4ee5\u5f80\u52aa\u529b\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\uff1b\u8fbe\u5230513.1 PFLOPs\u5cf0\u503c\u6027\u80fd\uff1b\u5904\u7406466\u4ebf\u7c92\u5b50/\u79d2\uff1b\u5728\u7ea6\u4e00\u5468\u8fd0\u884c\u65f6\u95f4\u5185\u5199\u5165\u8d85\u8fc7100PB\u6570\u636e\u3002", "conclusion": "exascale\u8ba1\u7b97\u4f7f\u5f97\u80fd\u591f\u5728\u5de1\u5929\u5c3a\u5ea6\u4f53\u79ef\u5185\u8fdb\u884c\u6a21\u62df\uff0c\u540c\u65f6\u7eb3\u5165\u5851\u9020\u590d\u6742\u5b87\u5b99\u7ed3\u6784\u7684\u5173\u952e\u4e9a\u7f51\u683c\u8fc7\u7a0b\uff0c\u8fd9\u662f\u5b9e\u73b0\u6240\u9700\u771f\u5b9e\u6027\u7684\u91cd\u8981\u6b65\u9aa4\u3002"}}
{"id": "2510.03872", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03872", "abs": "https://arxiv.org/abs/2510.03872", "authors": ["Sreedhar Narayanaswamy", "Pratikkumar Dilipkumar Patel", "Ian Karlin", "Apoorv Gupta", "Sudhir Saripalli", "Janey Guo"], "title": "Datacenter Energy Optimized Power Profiles", "comment": null, "summary": "This paper presents datacenter power profiles, a new NVIDIA software feature\nreleased with Blackwell B200, aimed at improving energy efficiency and/or\nperformance. The initial feature provides coarse-grain user control for HPC and\nAI workloads leveraging hardware and software innovations for intelligent power\nmanagement and domain knowledge of HPC and AI workloads. The resulting\nworkload-aware optimization recipes maximize computational throughput while\noperating within strict facility power constraints. The phase-1 Blackwell\nimplementation achieves up to 15% energy savings while maintaining performance\nlevels above 97% for critical applications, enabling an overall throughput\nincrease of up to 13% in a power-constrained facility.\n  KEYWORDS GPU power management, energy efficiency, power profile, HPC\noptimization, Max-Q, Blackwell architecture", "AI": {"tldr": "NVIDIA\u5728Blackwell B200\u4e2d\u63a8\u51fa\u4e86\u6570\u636e\u4e2d\u5fc3\u7535\u6e90\u914d\u7f6e\u6587\u4ef6\u529f\u80fd\uff0c\u901a\u8fc7\u667a\u80fd\u7535\u6e90\u7ba1\u7406\u548cHPC/AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u5728\u4e25\u683c\u8bbe\u65bd\u529f\u7387\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u8ba1\u7b97\u541e\u5410\u91cf\u3002", "motivation": "\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u5728\u529f\u7387\u53d7\u9650\u73af\u5883\u4e0b\u7684\u80fd\u6548\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u4f18\u5316\u65b9\u6848\u89e3\u51b3\u8bbe\u65bd\u529f\u7387\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u5229\u7528Blackwell\u67b6\u6784\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u521b\u65b0\uff0c\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u7528\u6237\u63a7\u5236\uff0c\u5b9e\u73b0\u667a\u80fd\u7535\u6e90\u7ba1\u7406\uff0c\u7ed3\u5408HPC\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9886\u57df\u77e5\u8bc6\u3002", "result": "Blackwell\u7b2c\u4e00\u9636\u6bb5\u5b9e\u65bd\u5b9e\u73b0\u4e86\u9ad8\u8fbe15%\u7684\u8282\u80fd\uff0c\u540c\u65f6\u5173\u952e\u5e94\u7528\u6027\u80fd\u4fdd\u6301\u572897%\u4ee5\u4e0a\uff0c\u5728\u529f\u7387\u53d7\u9650\u8bbe\u65bd\u4e2d\u603b\u4f53\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe13%\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u7535\u6e90\u914d\u7f6e\u6587\u4ef6\u529f\u80fd\u6210\u529f\u5730\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\uff0c\u4e3a\u529f\u7387\u53d7\u9650\u73af\u5883\u4e0b\u7684HPC\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03891", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.03891", "abs": "https://arxiv.org/abs/2510.03891", "authors": ["Shawn Shuoshuo Chen", "Daiyaan Arfeen", "Minlan Yu", "Peter Steenkiste", "Srinivasan Seshan"], "title": "Toward Co-adapting Machine Learning Job Shape and Cluster Topology", "comment": null, "summary": "Allocating resources to distributed machine learning jobs in multi-tenant\ntorus-topology clusters must meet each job's specific placement and\ncommunication requirements, which are typically described using shapes. There\nis an inherent tension between minimizing network contention and maximizing\ncluster utilization when placing various-shaped jobs. While existing schedulers\ntypically optimize for one objective at the expense of the other, we\ndemonstrate that both can be achieved simultaneously.\n  Our proposed approach, RFold, adapts both job shapes and the underlying\ncluster topology at runtime. This is accomplished by combining two techniques:\n(1) identifying homomorphic job shapes that support the jobs communication\nneeds, and (2) reconfiguring the optical circuit switch-enabled topology to\nsupport more diverse job shapes. Preliminary evaluation performed on a\n4096-node torus cluster simulator indicates that RFold can improve absolute\ncluster utilization by 57% and reduce job completion time by up to 11x relative\nto existing methods", "AI": {"tldr": "RFold\u901a\u8fc7\u5728\u8fd0\u884c\u65f6\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\uff0c\u540c\u65f6\u4f18\u5316\u7f51\u7edc\u7ade\u4e89\u548c\u96c6\u7fa4\u5229\u7528\u7387\uff0c\u57284096\u8282\u70b9\u73af\u9762\u96c6\u7fa4\u6a21\u62df\u5668\u4e2d\u5b9e\u73b057%\u7684\u7edd\u5bf9\u5229\u7528\u7387\u63d0\u5347\u548c\u6700\u9ad811\u500d\u7684\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u3002", "motivation": "\u591a\u79df\u6237\u73af\u9762\u62d3\u6251\u96c6\u7fa4\u4e2d\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4f5c\u4e1a\u8c03\u5ea6\u9700\u8981\u5728\u6ee1\u8db3\u4f5c\u4e1a\u7279\u5b9a\u653e\u7f6e\u548c\u901a\u4fe1\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5e73\u8861\u7f51\u7edc\u7ade\u4e89\u6700\u5c0f\u5316\u548c\u96c6\u7fa4\u5229\u7528\u7387\u6700\u5927\u5316\u8fd9\u4e24\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u6280\u672f\uff1a(1)\u8bc6\u522b\u652f\u6301\u4f5c\u4e1a\u901a\u4fe1\u9700\u6c42\u7684\u540c\u6784\u4f5c\u4e1a\u5f62\u72b6\uff1b(2)\u91cd\u65b0\u914d\u7f6e\u652f\u6301\u5149\u8def\u4ea4\u6362\u7684\u62d3\u6251\u4ee5\u652f\u6301\u66f4\u591a\u6837\u5316\u7684\u4f5c\u4e1a\u5f62\u72b6\u3002", "result": "\u57284096\u8282\u70b9\u73af\u9762\u96c6\u7fa4\u6a21\u62df\u5668\u4e2d\u7684\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0cRFold\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad857%\u7684\u7edd\u5bf9\u96c6\u7fa4\u5229\u7528\u7387\uff0c\u5e76\u5c06\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe11\u500d\u3002", "conclusion": "RFold\u8bc1\u660e\u4e86\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u5f62\u72b6\u548c\u96c6\u7fa4\u62d3\u6251\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u7f51\u7edc\u7ade\u4e89\u6700\u5c0f\u5316\u548c\u96c6\u7fa4\u5229\u7528\u7387\u6700\u5927\u5316\u8fd9\u4e24\u4e2a\u4f20\u7edf\u4e0a\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\u3002"}}
{"id": "2510.03970", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03970", "abs": "https://arxiv.org/abs/2510.03970", "authors": ["Zainab Saad", "Jialin Yang", "Henry Leung", "Steve Drew"], "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning", "comment": "Accepted to 2025 IEEE Smart World Congress (SWC 2025)", "summary": "The growing reliance on large-scale data centers to run resource-intensive\nworkloads has significantly increased the global carbon footprint, underscoring\nthe need for sustainable computing solutions. While container orchestration\nplatforms like Kubernetes help optimize workload scheduling to reduce carbon\nemissions, existing methods often depend on centralized machine learning models\nthat raise privacy concerns and struggle to generalize across diverse\nenvironments. In this paper, we propose a federated learning approach for\nenergy consumption prediction that preserves data privacy by keeping sensitive\noperational data within individual enterprises. By extending the Kubernetes\nEfficient Power Level Exporter (Kepler), our framework trains XGBoost models\ncollaboratively across distributed clients using Flower's FedXgbBagging\naggregation using a bagging strategy, eliminating the need for centralized data\nsharing. Experimental results on the SPECPower benchmark dataset show that our\nFL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a\ncentralized baseline. This work addresses the unresolved trade-off between data\nprivacy and energy prediction efficiency in prior systems such as Kepler and\nCASPER and offers enterprises a viable pathway toward sustainable cloud\ncomputing without compromising operational privacy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u6bd4\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u4f4e11.7%\u7684MAE\uff0c\u89e3\u51b3\u4e86Kepler\u548cCASPER\u7b49\u7cfb\u7edf\u4e2d\u6570\u636e\u9690\u79c1\u4e0e\u80fd\u8017\u9884\u6d4b\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u589e\u52a0\u5bfc\u81f4\u78b3\u8db3\u8ff9\u589e\u957f\uff0c\u73b0\u6709Kubernetes\u8c03\u5ea6\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u4e14\u96be\u4ee5\u8de8\u73af\u5883\u6cdb\u5316\u3002", "method": "\u6269\u5c55Kubernetes Efficient Power Level Exporter (Kepler)\uff0c\u4f7f\u7528Flower\u7684FedXgbBagging\u805a\u5408\u7b56\u7565\uff0c\u5728\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u4e0a\u534f\u4f5c\u8bad\u7ec3XGBoost\u6a21\u578b\uff0c\u907f\u514d\u96c6\u4e2d\u5f0f\u6570\u636e\u5171\u4eab\u3002", "result": "\u5728SPECPower\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\u6bd4\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u5b9e\u73b0\u4e8611.7%\u66f4\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f01\u4e1a\u5728\u4e0d\u635f\u5bb3\u8fd0\u8425\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u53ef\u6301\u7eed\u4e91\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u4e0e\u80fd\u8017\u9884\u6d4b\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.03956", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.03956", "abs": "https://arxiv.org/abs/2510.03956", "authors": ["Robert S. Aviles", "Peter A. Beerel"], "title": "Optimizing Phase-Scheduling with Throughput Trade-offs in AQFP Digital Circuits", "comment": null, "summary": "Adiabatic Quantum-Flux-Parametron (AQFP) logic is a promising emerging\nsuperconducting technology for ultra-low power digital circuits, offering\norders of magnitude lower power consumption than CMOS. However, AQFP\nscalability is challenged by excessive buffer overhead due to path balancing\ntechnology constraints. Addressing this, recent AQFP works have proposed design\nsolutions to reduce path balancing overhead using phase-skipping and\nphase-alignment. Phase-skipping is a circuit-level technique that allows data\ntransfer between AQFP gates clocked with non-consecutive clock phases. In\ncontrast, phase-alignment is an architectural approach involving repeating\ninput patterns to allow data transfer between AQFP gates across multiples of\nfull clock cycles. While both techniques individually mitigate the area\noverhead of path-balancing, they have not yet been jointly explored. In this\nwork, we present the first clock phase scheduling algorithm that combines\nphase-skipping and phase-alignment. We first present a minimum area method that\non average, achieves a 25% area reduction compared to phase-skipping alone and\na 11% reduction compared to phase-alignment. We then extend the method to\nenforce a target throughput, enabling efficient area-performance trade-offs.\nWith our throughput constrained optimization, we achieve on average 6.8% area\nsavings with a 2.62x increased throughput compared to the state-of-the-art\nphase-aligned method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7ed3\u5408\u76f8\u4f4d\u8df3\u8dc3\u548c\u76f8\u4f4d\u5bf9\u9f50\u7684\u65f6\u949f\u76f8\u4f4d\u8c03\u5ea6\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86AQFP\u903b\u8f91\u7535\u8def\u7684\u7f13\u51b2\u5668\u9762\u79ef\u5f00\u9500\uff0c\u5e76\u5728\u4fdd\u6301\u541e\u5410\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9762\u79ef\u4f18\u5316\u3002", "motivation": "AQFP\u903b\u8f91\u4f5c\u4e3a\u8d85\u4f4e\u529f\u8017\u8d85\u5bfc\u6280\u672f\u9762\u4e34\u8def\u5f84\u5e73\u8861\u5e26\u6765\u7684\u7f13\u51b2\u5668\u5f00\u9500\u6311\u6218\uff0c\u73b0\u6709\u76f8\u4f4d\u8df3\u8dc3\u548c\u76f8\u4f4d\u5bf9\u9f50\u6280\u672f\u5404\u81ea\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\u4f46\u5c1a\u672a\u8054\u5408\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u76f8\u4f4d\u8df3\u8dc3\u548c\u76f8\u4f4d\u5bf9\u9f50\u7684\u65f6\u949f\u76f8\u4f4d\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5305\u62ec\u6700\u5c0f\u9762\u79ef\u65b9\u6cd5\u548c\u541e\u5410\u91cf\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u6700\u5c0f\u9762\u79ef\u65b9\u6cd5\u76f8\u6bd4\u5355\u72ec\u76f8\u4f4d\u8df3\u8dc3\u5e73\u5747\u51cf\u5c1125%\u9762\u79ef\uff0c\u76f8\u6bd4\u76f8\u4f4d\u5bf9\u9f50\u51cf\u5c1111%\u9762\u79ef\uff1b\u541e\u5410\u91cf\u7ea6\u675f\u4f18\u5316\u5e73\u5747\u8282\u77016.8%\u9762\u79ef\uff0c\u540c\u65f6\u541e\u5410\u91cf\u63d0\u53472.62\u500d\u3002", "conclusion": "\u7ed3\u5408\u76f8\u4f4d\u8df3\u8dc3\u548c\u76f8\u4f4d\u5bf9\u9f50\u7684\u8c03\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4eAQFP\u7535\u8def\u7684\u7f13\u51b2\u5668\u9762\u79ef\u5f00\u9500\uff0c\u5b9e\u73b0\u9762\u79ef\u4e0e\u6027\u80fd\u7684\u826f\u597d\u6743\u8861\u3002"}}
{"id": "2510.04158", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.04158", "abs": "https://arxiv.org/abs/2510.04158", "authors": ["Emad Jacob Maroun"], "title": "A Dense and Efficient Instruction Set Architecture Encoding", "comment": null, "summary": "Instruction density and encoding efficiency are some of the few things\ndirectly affected by an instruction set architecture's design. In contrast, a\nprocessor's implementation often significantly influences performance, power\nefficiency, and area usage. Therefore, a major goal of instruction set design\nshould be maximizing instruction density and encoding efficiency. This paper\nintroduces the design elements of the Scry instruction set architecture that\nmost significantly affect instruction density and encoding efficiency. Scry is\na novel and experimental instruction set that revisits first principles to\ndesign an instruction set fit for modern processor implementations. Scry uses\nforward-temporal referencing as a means of data flow, where instructions refer\nto which future instructions consume their outputs. It also uses internal\ntagging, where the processors track data types internally, to reduce the number\nof instructions needed and increase flexibility. Combining these two methods,\nScry achieves instruction-feature parity with RISC-V's RV64IMC using only\n2-byte instructions compared to RISC-V's 4 bytes. Scry's instructions occupy\nonly 28% of the 2-byte encoding space, where RV64IMC instructions occupy 68% of\nthe 4-byte encoding space. We show that hand-compiled Scry's static instruction\ndensity is comparable to RV64IMC for small functions and improves as functions\ngrow in size.", "AI": {"tldr": "Scry\u6307\u4ee4\u96c6\u67b6\u6784\u901a\u8fc7\u524d\u5411\u65f6\u95f4\u5f15\u7528\u548c\u5185\u90e8\u6807\u8bb0\u6280\u672f\uff0c\u57282\u5b57\u8282\u6307\u4ee4\u7a7a\u95f4\u5185\u5b9e\u73b0\u4e86\u4e0eRISC-V RV64IMC 4\u5b57\u8282\u6307\u4ee4\u76f8\u5f53\u7684\u529f\u80fd\uff0c\u6307\u4ee4\u5bc6\u5ea6\u63d0\u5347\u4e8672%", "motivation": "\u6307\u4ee4\u96c6\u8bbe\u8ba1\u5e94\u6700\u5927\u5316\u6307\u4ee4\u5bc6\u5ea6\u548c\u7f16\u7801\u6548\u7387\uff0c\u56e0\u4e3a\u8fd9\u4e24\u8005\u76f4\u63a5\u5f71\u54cd\u5904\u7406\u5668\u6027\u80fd\uff0c\u800c\u5b9e\u73b0\u7ec6\u8282\u901a\u5e38\u5f71\u54cd\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef", "method": "\u91c7\u7528\u524d\u5411\u65f6\u95f4\u5f15\u7528\u4f5c\u4e3a\u6570\u636e\u6d41\u673a\u5236\uff08\u6307\u4ee4\u5f15\u7528\u672a\u6765\u6d88\u8d39\u5176\u8f93\u51fa\u7684\u6307\u4ee4\uff09\uff0c\u4ee5\u53ca\u5185\u90e8\u6807\u8bb0\u6280\u672f\uff08\u5904\u7406\u5668\u5185\u90e8\u8ddf\u8e2a\u6570\u636e\u7c7b\u578b\uff09\uff0c\u51cf\u5c11\u6240\u9700\u6307\u4ee4\u6570\u91cf\u5e76\u63d0\u9ad8\u7075\u6d3b\u6027", "result": "Scry\u4ec5\u75282\u5b57\u8282\u6307\u4ee4\u5c31\u5b9e\u73b0\u4e86RISC-V RV64IMC\u7684\u529f\u80fd\uff0c\u5360\u752828%\u76842\u5b57\u8282\u7f16\u7801\u7a7a\u95f4\uff0c\u800cRISC-V\u5360\u752868%\u76844\u5b57\u8282\u7f16\u7801\u7a7a\u95f4\u3002\u624b\u7f16\u8bd1\u7684Scry\u9759\u6001\u6307\u4ee4\u5bc6\u5ea6\u5728\u5c0f\u51fd\u6570\u4e2d\u4e0eRV64IMC\u76f8\u5f53\uff0c\u968f\u51fd\u6570\u89c4\u6a21\u589e\u5927\u800c\u63d0\u5347", "conclusion": "Scry\u6307\u4ee4\u96c6\u901a\u8fc7\u521b\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u5bc6\u5ea6\u548c\u7f16\u7801\u6548\u7387\uff0c\u4e3a\u73b0\u4ee3\u5904\u7406\u5668\u5b9e\u73b0\u63d0\u4f9b\u4e86\u66f4\u4f18\u5316\u7684\u6307\u4ee4\u96c6\u67b6\u6784"}}
{"id": "2510.03551", "categories": ["cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03551", "abs": "https://arxiv.org/abs/2510.03551", "authors": ["Rebecca Isaacs", "Peter Alvaro", "Rupak Majumdar", "Kiran-Kumar Muniswamy-Reddy", "Mahmoud Salamati", "Sadegh Soudjani"], "title": "Formal Analysis of Metastable Failures in Software Systems", "comment": null, "summary": "Many large-scale software systems demonstrate metastable failures. In this\nclass of failures, a stressor such as a temporary spike in workload causes the\nsystem performance to drop and, subsequently, the system performance continues\nto remain low even when the stressor is removed. These failures have been\nreported by many large corporations and considered to be a rare but\ncatastrophic source of availability outages in cloud systems.\n  In this paper, we provide the mathematical foundations of metastability in\nrequest-response server systems. We model such systems using a domain-specific\nlanguage. We show how to construct continuous-time Markov chains (CTMCs) that\napproximate the semantics of the programs through modeling and data-driven\ncalibration. We use the structure of the CTMC models to provide a visualization\nof the qualitative behavior of the model. The visualization is a surprisingly\neffective way to identify system parameterizations that cause a system to show\nmetastable behaviors.\n  We complement the qualitative analysis with quantitative predictions. We\nprovide a formal notion of metastable behaviors based on escape probabilities,\nand show that metastable behaviors are related to the eigenvalue structure of\nthe CTMC. Our characterization leads to algorithmic tools to predict recovery\ntimes in metastable models of server systems.\n  We have implemented our technique in a tool for the modeling and analysis of\nserver systems. Through models inspired by failures in real request-response\nsystems, we show that our qualitative visual analysis captures and predicts\nmany instances of metastability that were observed in the field in a matter of\nmilliseconds. Our algorithms confirm that recovery times surge as the system\nparameters approach metastable modes in the dynamics.", "AI": {"tldr": "\u672c\u6587\u4e3a\u8bf7\u6c42-\u54cd\u5e94\u670d\u52a1\u5668\u7cfb\u7edf\u4e2d\u7684\u4e9a\u7a33\u6001\u6545\u969c\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u901a\u8fc7\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u6765\u8bc6\u522b\u548c\u9884\u6d4b\u4e9a\u7a33\u6001\u884c\u4e3a\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5173\u7b97\u6cd5\u5de5\u5177\u3002", "motivation": "\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e9a\u7a33\u6001\u6545\u969c\uff0c\u5373\u7cfb\u7edf\u5728\u538b\u529b\u6e90\u79fb\u9664\u540e\u6027\u80fd\u4ecd\u6301\u7eed\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u8fd9\u5728\u4e91\u7cfb\u7edf\u4e2d\u662f\u7f55\u89c1\u4f46\u707e\u96be\u6027\u7684\u53ef\u7528\u6027\u4e2d\u65ad\u6e90\u3002", "method": "\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u5efa\u6a21\u7cfb\u7edf\uff0c\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe(CTMC)\u6765\u8fd1\u4f3c\u7a0b\u5e8f\u8bed\u4e49\uff0c\u901a\u8fc7\u6a21\u578b\u7ed3\u6784\u548c\u6570\u636e\u9a71\u52a8\u6821\u51c6\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u5b9a\u6027\u53ef\u89c6\u5316\u5206\u6790\u80fd\u5728\u6beb\u79d2\u7ea7\u5185\u6355\u83b7\u548c\u9884\u6d4b\u73b0\u573a\u89c2\u5bdf\u5230\u7684\u4e9a\u7a33\u6001\u5b9e\u4f8b\uff0c\u7b97\u6cd5\u786e\u8ba4\u7cfb\u7edf\u53c2\u6570\u63a5\u8fd1\u4e9a\u7a33\u6001\u6a21\u5f0f\u65f6\u6062\u590d\u65f6\u95f4\u4f1a\u6fc0\u589e\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u670d\u52a1\u5668\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u5bfc\u81f4\u4e9a\u7a33\u6001\u884c\u4e3a\u7684\u7cfb\u7edf\u53c2\u6570\u5316\uff0c\u5e76\u9884\u6d4b\u6062\u590d\u65f6\u95f4\u3002"}}
{"id": "2510.03243", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03243", "abs": "https://arxiv.org/abs/2510.03243", "authors": ["Yiheng Tao", "Yihe Zhang", "Matthew T. Dearing", "Xin Wang", "Yuping Fan", "Zhiling Lan"], "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "comment": null, "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low\nlatency and high throughput, particularly with the growing use of\nreasoning-capable LLMs. Traditional strategies like First-Come-First-Serve\n(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks\ndelay shorter ones queued behind them. In this paper, we introduce PARS, a\nprompt-aware LLM task scheduler that improves serving efficiency by\napproximating shortest-job-first (SJF) scheduling through pairwise ranking with\nmargin ranking loss. PARS focuses on impactful scheduling decisions and is\nseamlessly integrated into the state-of-the-art LLM serving system vLLM. It\neffectively predicts response-length-based task ordering, reducing latency with\nminimal overhead. Extensive experiments across multiple LLMs and real-world\ninference datasets show that PARS significantly improves performance, including\nfor reasoning workloads. Furthermore, our cross-model evaluations demonstrate\nthat the design generalizes well, enabling effective scheduling even when\npredictors are trained on different LLMs.", "AI": {"tldr": "PARS\u662f\u4e00\u4e2a\u63d0\u793a\u611f\u77e5\u7684LLM\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u8fd1\u4f3c\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\u8c03\u5ea6\u6765\u51cf\u5c11\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u7406\u670d\u52a1\u7684\u6548\u7387\u548c\u5ef6\u8fdf\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfFCFS\u8c03\u5ea6\u7b56\u7565\u5b58\u5728\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u957f\u4efb\u52a1\u4f1a\u5ef6\u8fdf\u540e\u7eed\u77ed\u4efb\u52a1\u7684\u6267\u884c\uff0c\u5f71\u54cdLLM\u63a8\u7406\u670d\u52a1\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u3002", "method": "\u4f7f\u7528\u6210\u5bf9\u6392\u5e8f\u548c\u8fb9\u9645\u6392\u5e8f\u635f\u5931\u6765\u8fd1\u4f3cSJF\u8c03\u5ea6\uff0c\u9884\u6d4b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u4efb\u52a1\u6392\u5e8f\uff0c\u5e76\u4e0evLLM\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u771f\u5b9e\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPARS\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5305\u62ec\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e14\u8bbe\u8ba1\u5177\u6709\u826f\u597d\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PARS\u901a\u8fc7\u63d0\u793a\u611f\u77e5\u7684\u667a\u80fd\u8c03\u5ea6\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u670d\u52a1\u76ee\u6807\u3002"}}
{"id": "2510.04186", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04186", "abs": "https://arxiv.org/abs/2510.04186", "authors": ["Xuan Jiang", "Xuanyu Zhou", "Yibo Zhao", "Shangqing Cao", "Jinhua Zhao", "Mark Hansen", "Raja Sengupta"], "title": "From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility", "comment": null, "summary": "Urban Air Mobility (UAM) presents a transformative vision for metropolitan\ntransportation, but its practical implementation is hindered by substantial\ninfrastructure costs and operational complexities. We address these challenges\nby modeling a UAM network that leverages existing regional airports and\noperates with an optimized, heterogeneous fleet of aircraft. We introduce\nLPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU\ncomputing to co-optimize UAM demand, fleet operations, and ground\ntransportation interactions simultaneously. Our equilibrium search algorithm is\nextended to accurately forecast demand and determine the most efficient fleet\ncomposition. Applied to a case study of the San Francisco Bay Area, our results\ndemonstrate that this UAM model can yield over 20 minutes' travel time savings\nfor 230,000 selected trips. However, the analysis also reveals that system-wide\nsuccess is critically dependent on seamless integration with ground access and\ndynamic scheduling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u73b0\u6709\u533a\u57df\u673a\u573a\u548c\u4f18\u5316\u5f02\u6784\u673a\u961f\u7684\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7LPSim\u5e76\u884c\u4eff\u771f\u6846\u67b6\u4f18\u5316\u9700\u6c42\u3001\u8fd0\u8425\u548c\u5730\u9762\u4ea4\u901a\u96c6\u6210\uff0c\u5728\u65e7\u91d1\u5c71\u6e7e\u533a\u6848\u4f8b\u4e2d\u53ef\u4e3a23\u4e07\u6b21\u51fa\u884c\u8282\u770120\u5206\u949f\u4ee5\u4e0a\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u9ad8\u548c\u8fd0\u8425\u590d\u6742\u6027\u7684\u5b9e\u9645\u5b9e\u65bd\u969c\u788d\u3002", "method": "\u5f00\u53d1LPSim\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u6846\u67b6\uff0c\u5229\u7528\u591aGPU\u8ba1\u7b97\u540c\u65f6\u4f18\u5316UAM\u9700\u6c42\u3001\u673a\u961f\u8fd0\u8425\u548c\u5730\u9762\u4ea4\u901a\u4ea4\u4e92\uff0c\u6269\u5c55\u5747\u8861\u641c\u7d22\u7b97\u6cd5\u9884\u6d4b\u9700\u6c42\u5e76\u786e\u5b9a\u6700\u4f18\u673a\u961f\u7ec4\u6210\u3002", "result": "\u5728\u65e7\u91d1\u5c71\u6e7e\u533a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5UAM\u6a21\u578b\u53ef\u4e3a23\u4e07\u6b21\u9009\u5b9a\u51fa\u884c\u8282\u7701\u8d85\u8fc720\u5206\u949f\u65c5\u884c\u65f6\u95f4\u3002", "conclusion": "\u7cfb\u7edf\u6210\u529f\u5173\u952e\u4f9d\u8d56\u4e8e\u4e0e\u5730\u9762\u4ea4\u901a\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u52a8\u6001\u8c03\u5ea6\u3002"}}
{"id": "2510.04535", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2510.04535", "abs": "https://arxiv.org/abs/2510.04535", "authors": ["Moritz Brunion", "Navaneeth Kunhi Purayil", "Francesco Dell'Atti", "Sebastian Lam", "Refik Bilgic", "Mehdi Tahoori", "Luca Benini", "Julien Ryckaert"], "title": "CMOS 2.0 - Redefining the Future of Scaling", "comment": "8 pages, 5 figures, to be published in ICCAD 2025", "summary": "We propose to revisit the functional scaling paradigm by capitalizing on two\nrecent developments in advanced chip manufacturing, namely 3D wafer bonding and\nbackside processing. This approach leads to the proposal of the CMOS 2.0\nplatform. The main idea is to shift the CMOS roadmap from geometric scaling to\nfine-grain heterogeneous 3D stacking of specialized active device layers to\nachieve the ultimate Power-Performance-Area and Cost gains expected from future\ntechnology generations. However, the efficient utilization of such a platform\nrequires devising architectures that can optimally map onto this technology, as\nwell as the EDA infrastructure that supports it. We also discuss reliability\nconcerns and eventual mitigation approaches. This paper provides pointers into\nthe major disruptions we expect in the design of systems in CMOS 2.0 moving\nforward.", "AI": {"tldr": "\u63d0\u51faCMOS 2.0\u5e73\u53f0\uff0c\u5229\u75283D\u6676\u5706\u952e\u5408\u548c\u80cc\u9762\u5904\u7406\u6280\u672f\uff0c\u4ece\u51e0\u4f55\u7f29\u653e\u8f6c\u5411\u7ec6\u7c92\u5ea6\u5f02\u67843D\u5806\u53e0\uff0c\u5b9e\u73b0PPAC\u4f18\u5316\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u529f\u80fd\u7f29\u653e\u8303\u5f0f\uff0c\u5229\u7528\u5148\u8fdb\u82af\u7247\u5236\u9020\u6280\u672f\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u89e3\u51b3\u4f20\u7edfCMOS\u7f29\u653e\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u91c7\u75283D\u6676\u5706\u952e\u5408\u548c\u80cc\u9762\u5904\u7406\u6280\u672f\uff0c\u6784\u5efa\u7ec6\u7c92\u5ea6\u5f02\u67843D\u5806\u53e0\u7684\u4e13\u7528\u6709\u6e90\u5668\u4ef6\u5c42\u3002", "result": "\u63d0\u51faCMOS 2.0\u5e73\u53f0\u6982\u5ff5\uff0c\u5b9e\u73b0\u4ece\u51e0\u4f55\u7f29\u653e\u5230\u529f\u80fd\u7f29\u653e\u7684\u8f6c\u53d8\uff0c\u83b7\u5f97PPAC\u548c\u6210\u672c\u6536\u76ca\u3002", "conclusion": "CMOS 2.0\u5c06\u5728\u7cfb\u7edf\u8bbe\u8ba1\u9886\u57df\u5e26\u6765\u91cd\u5927\u53d8\u9769\uff0c\u9700\u8981\u5f00\u53d1\u5339\u914d\u7684\u67b6\u6784\u548cEDA\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u89e3\u51b3\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2510.03244", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in", "AI": {"tldr": "VIFO\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u901a\u9053\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u91c7\u7528\u901a\u9053\u72ec\u7acb\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\uff1b\u540c\u65f6\u591a\u6a21\u6001\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65f6\u7a7a\u6570\u636e\u7684\u6f5c\u529b\uff1b\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u7684\u4f18\u52bf\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u6765\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u578b\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u590d\u6742\u7684\u8de8\u901a\u9053\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u89c6\u89c9\u7279\u5f81\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u6001\u7684\u8868\u5f81\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\u3002\u901a\u8fc7\u51bb\u7ed3\u89c6\u89c9\u6a21\u578b\u4ec5\u8bad\u7ec37.45%\u7684\u53c2\u6570\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "VIFO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8de8\u53d8\u91cf\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u901a\u9053\u72ec\u7acb\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.04310", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04310", "abs": "https://arxiv.org/abs/2510.04310", "authors": ["Hagit Attiya", "Itay Flam", "Jennifer L. Welch"], "title": "Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience", "comment": "31 pages, 4 figures, 1 table, 5 algorithms", "summary": "We study communication abstractions for asynchronous Byzantine fault\ntolerance with optimal failure resilience, where $n > 3f$. Two classic patterns\n-- canonical asynchronous rounds and communication-closed layers -- have long\nbeen considered as general frameworks for designing distributed algorithms,\nmaking asynchronous executions appear synchronous and enabling modular\nreasoning.\n  We show that these patterns are inherently limited in the critical resilience\nregime $3f < n \\le 5f$. Several key tasks -- such as approximate and crusader\nagreement, reliable broadcast and gather -- cannot be solved by bounded-round\ncanonical-round algorithms, and are unsolvable if communication closure is\nimposed. These results explain the historical difficulty of achieving\noptimal-resilience algorithms within round-based frameworks.\n  On the positive side, we show that the gather abstraction admits\nconstant-time solutions with optimal resilience ($n > 3f$), and supports\nmodular reductions. Specifically, we present the first optimally-resilient\nalgorithm for connected consensus by reducing it to gather.\n  Our results demonstrate that while round-based abstractions are analytically\nconvenient, they obscure the true complexity of Byzantine fault-tolerant\nalgorithms. Richer communication patterns such as gather provide a better\nfoundation for modular, optimal-resilience design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f02\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\u901a\u4fe1\u62bd\u8c61\uff0c\u53d1\u73b0\u7ecf\u5178\u8f6e\u6b21\u6a21\u5f0f\u5728\u6700\u4f18\u5bb9\u9519(n>3f)\u4e0b\u5b58\u5728\u56fa\u6709\u5c40\u9650\uff0c\u4f46\u63d0\u51fa\u4e86gather\u62bd\u8c61\u4f5c\u4e3a\u66f4\u597d\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u5f02\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\u7684\u6700\u4f18\u5bb9\u9519\u901a\u4fe1\u62bd\u8c61\uff0c\u63a2\u7d22\u4e3a\u4f55\u57283f<n\u22645f\u7684\u5173\u952e\u5bb9\u9519\u8303\u56f4\u5185\uff0c\u4f20\u7edf\u8f6e\u6b21\u6846\u67b6\u96be\u4ee5\u5b9e\u73b0\u6700\u4f18\u5bb9\u9519\u7b97\u6cd5\u3002", "method": "\u5206\u6790\u7ecf\u5178\u5f02\u6b65\u8f6e\u6b21\u548c\u901a\u4fe1\u95ed\u5408\u5c42\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fagather\u62bd\u8c61\u5e76\u5c55\u793a\u5176\u652f\u6301\u6a21\u5757\u5316\u5f52\u7ea6\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u8fde\u901a\u5171\u8bc6\u5f52\u7ea6\u4e3agather\u6765\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u57283f<n\u22645f\u8303\u56f4\u5185\uff0c\u591a\u4e2a\u5173\u952e\u4efb\u52a1\u65e0\u6cd5\u901a\u8fc7\u6709\u754c\u8f6e\u6b21\u7b97\u6cd5\u89e3\u51b3\uff0c\u4f46gather\u62bd\u8c61\u5141\u8bb8\u5e38\u6570\u65f6\u95f4\u89e3\u5e76\u652f\u6301\u6700\u4f18\u5bb9\u9519\u3002", "conclusion": "\u8f6e\u6b21\u62bd\u8c61\u867d\u7136\u5206\u6790\u65b9\u4fbf\u4f46\u63a9\u76d6\u4e86\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u7684\u771f\u5b9e\u590d\u6742\u6027\uff0c\u66f4\u4e30\u5bcc\u7684\u901a\u4fe1\u6a21\u5f0f\u5982gather\u4e3a\u6a21\u5757\u5316\u6700\u4f18\u5bb9\u9519\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u597d\u57fa\u7840\u3002"}}
{"id": "2510.03245", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03245", "abs": "https://arxiv.org/abs/2510.03245", "authors": ["Ali Yavari", "Alireza Mohamadi", "Elham Beydaghi", "Rainer A. Leitgeb"], "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "comment": "Preprint", "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of\nreal world noise and intentional perturbations remains a significant challenge.\nTo address this, attribution methods have been proposed, though their efficacy\nremains suboptimal and necessitates further refinement. In this paper, we\npropose a novel category of transferable adversarial attacks, called\ntransferable frequency-aware attacks, enabling frequency-aware exploration via\nboth high-and low-frequency components. Based on this type of attacks, we also\npropose a novel attribution method, named Frequency-Aware Model Parameter\nExplorer (FAMPE), which improves the explainability for DNNs. Relative to the\ncurrent state-of-the-art method AttEXplore, our FAMPE attains an average gain\nof 13.02% in Insertion Score, thereby outperforming existing approaches.\nThrough detailed ablation studies, we also investigate the role of both high-\nand low-frequency components in explainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u8fc1\u79fb\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u2014\u2014\u53ef\u8fc1\u79fb\u9891\u7387\u611f\u77e5\u653b\u51fb\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6b64\u7684\u5f52\u56e0\u65b9\u6cd5FAMPE\uff0c\u7528\u4e8e\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u548c\u6545\u610f\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u6548\u679c\u6b20\u4f73\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u53ef\u8fc1\u79fb\u9891\u7387\u611f\u77e5\u653b\u51fb\uff0c\u901a\u8fc7\u9ad8\u4f4e\u9891\u5206\u91cf\u8fdb\u884c\u9891\u7387\u611f\u77e5\u63a2\u7d22\uff1b\u57fa\u4e8e\u6b64\u5f00\u53d1FAMPE\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5AttEXplore\uff0cFAMPE\u5728Insertion Score\u4e0a\u5e73\u5747\u63d0\u534713.02%\u3002", "conclusion": "FAMPE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u9ad8\u4f4e\u9891\u5206\u91cf\u5728\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.04404", "categories": ["cs.DC", "cs.PF", "68M14, 68T05, 90C59", "C.2.4; D.4.4; D.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.04404", "abs": "https://arxiv.org/abs/2510.04404", "authors": ["Jahidul Arafat", "Fariha Tasmin", "Sanjaya Poudel", "Ahsan Habib Tareq"], "title": "Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks", "comment": "45 pages, 8 tables, 1 figure. Comprehensive evaluation of 12\n  messaging frameworks with AI-enhanced orchestration system", "summary": "Modern distributed systems demand low-latency, fault-tolerant event\nprocessing that exceeds traditional messaging architecture limits. While\nframeworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and\nserverless event buses have matured significantly, no unified comparative study\nevaluates them holistically under standardized conditions. This paper presents\nthe first comprehensive benchmarking framework evaluating 12 messaging systems\nacross three representative workloads: e-commerce transactions, IoT telemetry\ningestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event\nOrchestration), employing machine learning-driven predictive scaling,\nreinforcement learning for dynamic resource allocation, and multi-objective\noptimization. Our evaluation reveals fundamental trade-offs: Apache Kafka\nachieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires\nsubstantial operational expertise; Apache Pulsar provides balanced performance\n(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions\noffer elastic scaling for variable workloads despite higher baseline latency\n(80-120ms p95). AIEO demonstrates 34\\% average latency reduction, 28\\% resource\nutilization improvement, and 42% cost optimization across all platforms. We\ncontribute standardized benchmarking methodologies, open-source intelligent\norchestration, and evidence-based decision guidelines. The evaluation\nencompasses 2,400+ experimental configurations with rigorous statistical\nanalysis, providing comprehensive performance characterization and establishing\nfoundations for next-generation distributed system design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u7684\u6d88\u606f\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u6d88\u606f\u7cfb\u7edf\u5728\u4e09\u79cd\u5178\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86AIEO\u667a\u80fd\u7f16\u6392\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u5bb9\u9519\u7684\u4e8b\u4ef6\u5904\u7406\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e3b\u6d41\u6d88\u606f\u7cfb\u7edf\u5728\u6807\u51c6\u5316\u6761\u4ef6\u4e0b\u7684\u7edf\u4e00\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f3012\u4e2a\u6d88\u606f\u7cfb\u7edf\u5728\u7535\u5546\u4ea4\u6613\u3001\u7269\u8054\u7f51\u9065\u6d4b\u91c7\u96c6\u548cAI\u63a8\u7406\u6d41\u6c34\u7ebf\u4e09\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165AIEO\u7cfb\u7edf\uff0c\u91c7\u7528\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u6269\u5c55\u3001\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u548c\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u57fa\u672c\u6743\u8861\uff1aApache Kafka\u8fbe\u5230\u5cf0\u503c\u541e\u5410\u91cf\u4f46\u9700\u8981\u5927\u91cf\u8fd0\u7ef4\u4e13\u4e1a\u77e5\u8bc6\uff1bApache Pulsar\u63d0\u4f9b\u5e73\u8861\u7684\u6027\u80fd\u548c\u4f18\u8d8a\u7684\u591a\u79df\u6237\u652f\u6301\uff1b\u65e0\u670d\u52a1\u5668\u89e3\u51b3\u65b9\u6848\u4e3a\u53ef\u53d8\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u5f39\u6027\u6269\u5c55\u4f46\u57fa\u7840\u5ef6\u8fdf\u8f83\u9ad8\u3002AIEO\u5728\u6240\u6709\u5e73\u53f0\u4e0a\u5e73\u5747\u51cf\u5c1134%\u5ef6\u8fdf\uff0c\u63d0\u534728%\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f18\u531642%\u6210\u672c\u3002", "conclusion": "\u672c\u6587\u8d21\u732e\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3001\u5f00\u6e90\u667a\u80fd\u7f16\u6392\u7cfb\u7edf\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56\u6307\u5357\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale.", "AI": {"tldr": "STRUPRUNE\u662f\u4e00\u4e2a\u57fa\u4e8eADMM\u7684\u7ed3\u6784\u5316\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u5c06\u5168\u5c40\u526a\u679d\u5206\u89e3\u4e3a\u534f\u8c03\u7684\u5b50\u95ee\u9898\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\u7684\u786c\u4ef6\u517c\u5bb9\u6027\u3002", "motivation": "\u5168\u5c40\u526a\u679d\u867d\u7136\u6027\u80fd\u5f3a\u4f46\u9700\u8981O(N)\u5185\u5b58\uff0c\u4e0d\u9002\u7528\u4e8e\u5341\u4ebf\u53c2\u6570\u6a21\u578b\uff1b\u5c40\u90e8\u526a\u679d\u5185\u5b58\u6548\u7387\u9ad8\u4f46\u5ffd\u7565\u5c42\u95f4\u4f9d\u8d56\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4e0d\u4f73\uff1b\u7ed3\u6784\u5316\u526a\u679d\u786c\u4ef6\u6548\u7387\u9ad8\u4f46\u901a\u5e38\u4f9d\u8d56\u5168\u5c40\u526a\u679d\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u5c06\u5168\u5c40\u526a\u679d\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u5b50\u95ee\u9898\uff0c\u57fa\u4e8eADMM\u6846\u67b6\u96c6\u6210\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u63a8\u5bfc\u51fa\u7ed3\u6784\u5316\u526a\u679d\u63a9\u7801\u7684\u95ed\u5f0f\u89e3\u6790\u89e3\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u8f6f\u6700\u5927\u5206\u914d\u65b9\u6848\u3002", "result": "STRUPRUNE\u5728\u5339\u914d\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u56f0\u60d1\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u5185\u5b58\u6210\u672c\u4eceO(N)\u964d\u4f4e\u5230O(\u221aN)\uff0c\u5b9e\u73b0\u4e86\u5341\u4ebf\u53c2\u6570\u7ea7\u522b\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u5185\u5b58\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.04644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04644", "abs": "https://arxiv.org/abs/2510.04644", "authors": ["Hirotsugu Kakugawa", "Sayaka Kamei", "Masahiro Shibata", "Fukuhito Ooshita"], "title": "The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms", "comment": null, "summary": "Self-stabilization is a versatile methodology in the design of fault-tolerant\ndistributed algorithms for transient faults. A self-stabilizing system\nautomatically recovers from any kind and any finite number of transient faults.\nThis property is specifically useful in modern distributed systems with a large\nnumber of components. In this paper, we propose a new communication and\nexecution model named the R(1)W(1) model in which each process can read and\nwrite its own and neighbors' local variables in a single step. We propose\nself-stabilizing distributed algorithms in the R(1)W(1) model for the problems\nof maximal matching, minimal k-dominating set and maximal k-dependent set.\nFinally, we propose an example transformer, based on randomized distance-two\nlocal mutual exclusion, to simulate algorithms designed for the R(1)W(1) model\nin the synchronous message passing model with synchronized clocks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684R(1)W(1)\u901a\u4fe1\u6267\u884c\u6a21\u578b\uff0c\u5728\u8be5\u6a21\u578b\u4e0b\u8bbe\u8ba1\u4e86\u81ea\u7a33\u5b9a\u5206\u5e03\u5f0f\u7b97\u6cd5\u89e3\u51b3\u6700\u5927\u5339\u914d\u3001\u6700\u5c0fk\u652f\u914d\u96c6\u548c\u6700\u5927k\u4f9d\u8d56\u96c6\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4eceR(1)W(1)\u6a21\u578b\u5230\u540c\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u7684\u8f6c\u6362\u5668\u3002", "motivation": "\u81ea\u7a33\u5b9a\u662f\u8bbe\u8ba1\u5bb9\u9519\u5206\u5e03\u5f0f\u7b97\u6cd5\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u77ac\u6001\u6545\u969c\u4e2d\u81ea\u52a8\u6062\u590d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002", "method": "\u63d0\u51faR(1)W(1)\u6a21\u578b\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u53ef\u4ee5\u5728\u5355\u6b65\u4e2d\u8bfb\u5199\u81ea\u8eab\u548c\u90bb\u5c45\u7684\u5c40\u90e8\u53d8\u91cf\uff1b\u8bbe\u8ba1\u4e86\u81ea\u7a33\u5b9a\u7b97\u6cd5\u89e3\u51b3\u4e09\u4e2a\u56fe\u8bba\u95ee\u9898\uff1b\u57fa\u4e8e\u968f\u673a\u8ddd\u79bb\u4e8c\u5c40\u90e8\u4e92\u65a5\u6784\u5efa\u6a21\u578b\u8f6c\u6362\u5668\u3002", "result": "\u5728R(1)W(1)\u6a21\u578b\u4e0b\u6210\u529f\u5b9e\u73b0\u4e86\u6700\u5927\u5339\u914d\u3001\u6700\u5c0fk\u652f\u914d\u96c6\u548c\u6700\u5927k\u4f9d\u8d56\u96c6\u7684\u81ea\u7a33\u5b9a\u5206\u5e03\u5f0f\u7b97\u6cd5\u3002", "conclusion": "R(1)W(1)\u6a21\u578b\u4e3a\u81ea\u7a33\u5b9a\u5206\u5e03\u5f0f\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u548c\u8f6c\u6362\u5668\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03247", "abs": "https://arxiv.org/abs/2510.03247", "authors": ["Jiancheng Zhang", "Yinglun Zhu"], "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "comment": null, "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in\ndata-hungry deep learning. However, existing AL algorithms focus almost\nexclusively on unimodal data, overlooking the substantial annotation burden in\nmultimodal learning. We introduce the first framework for multimodal active\nlearning with unaligned data, where the learner must actively acquire\ncross-modal alignments rather than labels on pre-aligned pairs. This setting\ncaptures the practical bottleneck in modern multimodal pipelines such as CLIP\nand SigLIP, where unimodal features are easy to obtain but high-quality\nalignment is costly. We develop a new algorithm that combines uncertainty and\ndiversity principles in a modality-aware design, achieves linear-time\nacquisition, and applies seamlessly to both pool-based and streaming-based\nsettings. Extensive experiments on benchmark datasets demonstrate that our\napproach consistently reduces multimodal annotation cost while preserving\nperformance; for instance, on the ColorSwap dataset it cuts annotation\nrequirements by up to $40\\%$ without loss in accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u672a\u5bf9\u9f50\u591a\u6a21\u6001\u6570\u636e\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u83b7\u53d6\u8de8\u6a21\u6001\u5bf9\u9f50\u800c\u975e\u6807\u7b7e\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u9ad8\u8fbe40%\u7684\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5de8\u5927\u7684\u6807\u6ce8\u8d1f\u62c5\uff0c\u7279\u522b\u662f\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u539f\u5219\u7684\u6a21\u6001\u611f\u77e5\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u83b7\u53d6\uff0c\u9002\u7528\u4e8e\u6c60\u5f0f\u548c\u6d41\u5f0f\u8bbe\u7f6e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u51cf\u5c11\u591a\u6a21\u6001\u6807\u6ce8\u6210\u672c\uff0c\u5982\u5728ColorSwap\u6570\u636e\u96c6\u4e0a\u51cf\u5c1140%\u6807\u6ce8\u9700\u6c42\u800c\u4e0d\u635f\u5931\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6807\u6ce8\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u73b0\u4ee3\u591a\u6a21\u6001\u7ba1\u9053\u5982CLIP\u548cSigLIP\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6210\u672c\u964d\u4f4e\u65b9\u6848\u3002"}}
{"id": "2510.03248", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.03248", "abs": "https://arxiv.org/abs/2510.03248", "authors": ["Anusha Agarwal", "Dibakar Roy Sarkar", "Somdatta Goswami"], "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "comment": null, "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over\n69 million cases annually worldwide. Finite element (FE) models offer\nhigh-fidelity predictions of brain deformation but are computationally\nexpensive, requiring hours per simulation and limiting their clinical utility\nfor rapid decision-making. This study benchmarks state-of-the-art neural\noperator (NO) architectures for rapid, patient-specific prediction of brain\ndisplacement fields, aiming to enable real-time TBI modeling in clinical and\ntranslational settings. We formulated TBI modeling as an operator learning\nproblem, mapping subject-specific anatomical MRI, magnetic resonance\nelastography (MRE) stiffness maps, and demographic features to full-field 3D\nbrain displacement predictions. Four architectures - Fourier Neural Operator\n(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator\nNetwork (DeepONet) were trained and evaluated on 249 MRE datasets across\nphysiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest\naccuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale\nfeatures, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet\noffered the fastest inference (14.5 iterations/s) with a 7$\\times$\ncomputational speed-up over MG-FNO, suggesting utility for embedded or edge\ncomputing applications. All NOs reduced computation time from hours to\nmilliseconds without sacrificing anatomical realism. NOs provide an efficient,\nresolution-invariant approach for predicting brain deformation, opening the\ndoor to real-time, patient-specific TBI risk assessment, clinical triage\nsupport, and optimization of protective equipment. These results highlight the\npotential for NO-based digital twins of the human brain, enabling scalable,\non-demand biomechanical modeling in both clinical and population health\ncontexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\u7528\u4e8e\u5feb\u901f\u9884\u6d4b\u8111\u4f4d\u79fb\u573a\uff0c\u4ee5\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u7684\u6709\u9650\u5143\u6a21\u578b\uff0c\u5b9e\u73b0\u521b\u4f24\u6027\u8111\u635f\u4f24\u7684\u5b9e\u65f6\u5efa\u6a21\u3002MG-FNO\u83b7\u5f97\u6700\u9ad8\u7cbe\u5ea6\uff0cDeepONet\u63d0\u4f9b\u6700\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u6240\u6709\u795e\u7ecf\u7b97\u5b50\u90fd\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u6beb\u79d2\u7ea7\u3002", "motivation": "\u521b\u4f24\u6027\u8111\u635f\u4f24\u662f\u5168\u7403\u91cd\u5927\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u73b0\u6709\u6709\u9650\u5143\u6a21\u578b\u8ba1\u7b97\u6602\u8d35\uff08\u6bcf\u5c0f\u65f6\u4e00\u6b21\u6a21\u62df\uff09\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5feb\u901f\u51b3\u7b56\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u5feb\u901f\u3001\u60a3\u8005\u7279\u5b9a\u7684\u8111\u4f4d\u79fb\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06TBI\u5efa\u6a21\u5b9a\u4e49\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u56db\u79cd\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff08FNO\u3001F-FNO\u3001MG-FNO\u3001DeepONet\uff09\uff0c\u8f93\u5165\u60a3\u8005\u7279\u5b9a\u7684\u89e3\u5256MRI\u3001MRE\u521a\u5ea6\u56fe\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff0c\u8f93\u51fa\u5168\u573a3D\u8111\u4f4d\u79fb\u9884\u6d4b\u3002\u5728249\u4e2aMRE\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8bc4\u4f30\u3002", "result": "MG-FNO\u8fbe\u5230\u6700\u9ad8\u7cbe\u5ea6\uff08MSE = 0.0023\uff0c94.3%\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff09\uff0cF-FNO\u6536\u655b\u901f\u5ea6\u6bd4\u6807\u51c6FNO\u5feb2\u500d\uff0cDeepONet\u63d0\u4f9b\u6700\u5feb\u63a8\u7406\u901f\u5ea6\uff0814.5\u6b21\u8fed\u4ee3/\u79d2\uff09\uff0c\u6bd4MG-FNO\u5feb7\u500d\u3002\u6240\u6709\u795e\u7ecf\u7b97\u5b50\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u6beb\u79d2\u7ea7\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\u4e3a\u9884\u6d4b\u8111\u53d8\u5f62\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u4e0d\u53d8\u7684\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u73b0\u5b9e\u65f6\u3001\u60a3\u8005\u7279\u5b9a\u7684TBI\u98ce\u9669\u8bc4\u4f30\u3001\u4e34\u5e8a\u5206\u8bca\u652f\u6301\u548c\u9632\u62a4\u8bbe\u5907\u4f18\u5316\u6253\u5f00\u4e86\u5927\u95e8\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u795e\u7ecf\u7b97\u5b50\u7684\u4eba\u8111\u6570\u5b57\u5b6a\u751f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03283", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03283", "abs": "https://arxiv.org/abs/2510.03283", "authors": ["Yufei Li", "Yu Fu", "Yue Dong", "Cong Liu"], "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "comment": "14 pages, 15 figures", "summary": "Large language models (LLMs) deployed on edge servers are increasingly used\nin latency-sensitive applications such as personalized assistants,\nrecommendation, and content moderation. However, the non-stationary nature of\nuser data necessitates frequent retraining, which introduces a fundamental\ntension between inference latency and model accuracy under constrained GPU\nresources. Existing retraining strategies either delay model updates,\nover-commit resources to retraining, or overlook iteration-level retraining\ngranularity. In this paper, we identify that iteration-level scheduling is\ncrucial for adapting retraining frequency to model drift without violating\nservice-level objectives (SLOs). We propose MACE, a hybrid LLM system that\ncolocates concurrent inference (prefill, decode) and fine-tuning, with\nintelligent memory management to maximize task performance while promising\ninference throughput. MACE leverages the insight that not all model updates\nequally affect output alignment and allocates GPU cycles accordingly to balance\nthroughput, latency, and update freshness. Our trace-driven evaluation shows\nthat MACE matches or exceeds continuous retraining while reducing inference\nlatency by up to 63% and maintaining throughput under resource constraints.\nCompared to periodic retraining, MACE improves latency breakdown across\nprefill, decode, and finetune stages, and sustains GPU utilization above 85% in\nNVIDIA AGX Orin. These results demonstrate that iteration-level hybrid\nscheduling is a promising direction for deploying LLMs with continual learning\ncapabilities on edge platforms.", "AI": {"tldr": "MACE\u662f\u4e00\u4e2a\u6df7\u5408LLM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u534f\u540c\u5b9a\u4f4d\u63a8\u7406\u548c\u5fae\u8c03\u4efb\u52a1\uff0c\u4f7f\u7528\u8fed\u4ee3\u7ea7\u8c03\u5ea6\u6765\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u7684LLM\u5728\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u4e2d\u9762\u4e34\u9891\u7e41\u91cd\u8bad\u7ec3\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ef6\u8fdf\u6a21\u578b\u66f4\u65b0\uff0c\u8981\u4e48\u8fc7\u5ea6\u5206\u914d\u8d44\u6e90\u7ed9\u91cd\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5728GPU\u8d44\u6e90\u53d7\u9650\u4e0b\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faMACE\u7cfb\u7edf\uff0c\u534f\u540c\u5b9a\u4f4d\u5e76\u53d1\u63a8\u7406\uff08\u9884\u586b\u5145\u3001\u89e3\u7801\uff09\u548c\u5fae\u8c03\uff0c\u91c7\u7528\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff0c\u57fa\u4e8e\u6a21\u578b\u66f4\u65b0\u5bf9\u8f93\u51fa\u5bf9\u9f50\u7684\u5f71\u54cd\u7a0b\u5ea6\u5206\u914dGPU\u5468\u671f\uff0c\u5b9e\u73b0\u8fed\u4ee3\u7ea7\u8c03\u5ea6\u3002", "result": "MACE\u5728\u4fdd\u6301\u541e\u5410\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe63%\uff0c\u5728NVIDIA AGX Orin\u4e0a\u7ef4\u630185%\u4ee5\u4e0a\u7684GPU\u5229\u7528\u7387\uff0c\u4f18\u4e8e\u8fde\u7eed\u91cd\u8bad\u7ec3\u548c\u5468\u671f\u6027\u91cd\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8fed\u4ee3\u7ea7\u6df7\u5408\u8c03\u5ea6\u662f\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u90e8\u7f72\u5177\u6709\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684LLM\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u80fd\u6709\u6548\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u66f4\u65b0\u65b0\u9c9c\u5ea6\u3002"}}
{"id": "2510.03250", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03250", "abs": "https://arxiv.org/abs/2510.03250", "authors": ["Lukas R\u00fcttgers", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "Light Differentiable Logic Gate Networks", "comment": null, "summary": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency\nat inference while sustaining competitive accuracy. But vanishing gradients,\ndiscretization errors, and high training cost impede scaling these networks.\nEven with dedicated parameter initialization schemes from subsequent works,\nincreasing depth still harms accuracy. We show that the root cause of these\nissues lies in the underlying parametrization of logic gate neurons themselves.\nTo overcome this issue, we propose a reparametrization that also shrinks the\nparameter size logarithmically in the number of inputs per gate. For binary\ninputs, this already reduces the model size by 4x, speeds up the backward pass\nby up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we\nshow that the accuracy on CIFAR-100 remains stable and sometimes superior to\nthe original parametrization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u3001\u79bb\u6563\u5316\u8bef\u5dee\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\u5728\u63a8\u7406\u65f6\u6548\u7387\u6781\u9ad8\u4e14\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u4f46\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u3001\u79bb\u6563\u5316\u8bef\u5dee\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528\u4e13\u95e8\u7684\u53c2\u6570\u521d\u59cb\u5316\u65b9\u6848\uff0c\u589e\u52a0\u6df1\u5ea6\u4ecd\u4f1a\u635f\u5bb3\u51c6\u786e\u7387\u3002", "method": "\u91cd\u65b0\u53c2\u6570\u5316\u903b\u8f91\u95e8\u795e\u7ecf\u5143\uff0c\u5c06\u6bcf\u4e2a\u95e8\u7684\u53c2\u6570\u5927\u5c0f\u6309\u8f93\u5165\u6570\u91cf\u5bf9\u6570\u7ea7\u7f29\u5c0f\u3002\u5bf9\u4e8e\u4e8c\u8fdb\u5236\u8f93\u5165\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c114\u500d\uff0c\u53cd\u5411\u4f20\u64ad\u901f\u5ea6\u63d0\u5347\u81f31.86\u500d\uff0c\u8bad\u7ec3\u6b65\u9aa4\u51cf\u5c118.5\u500d\u3002", "result": "\u5728CIFAR-100\u4e0a\u7684\u51c6\u786e\u7387\u4fdd\u6301\u7a33\u5b9a\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u903b\u8f91\u95e8\u795e\u7ecf\u5143\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\u7684\u6269\u5c55\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.03288", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03288", "abs": "https://arxiv.org/abs/2510.03288", "authors": ["Chiming Duan", "Minghua He", "Pei Xiao", "Tong Jia", "Xin Zhang", "Zhewei Zhong", "Xiang Luo", "Yan Niu", "Lingzhe Zhang", "Yifan Wu", "Siyu Yu", "Weijie Hong", "Ying Li", "Gang Huang"], "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "comment": "The 40th IEEE/ACM International Conference on Automated Software\n  Engineering, ASE 2025", "summary": "Log-based anomaly detection is a essential task for ensuring the reliability\nand performance of software systems. However, the performance of existing\nanomaly detection methods heavily relies on labeling, while labeling a large\nvolume of logs is highly challenging. To address this issue, many approaches\nbased on transfer learning and active learning have been proposed.\nNevertheless, their effectiveness is hindered by issues such as the gap between\nsource and target system data distributions and cold-start problems. In this\npaper, we propose LogAction, a novel log-based anomaly detection model based on\nactive domain adaptation. LogAction integrates transfer learning and active\nlearning techniques. On one hand, it uses labeled data from a mature system to\ntrain a base model, mitigating the cold-start issue in active learning. On the\nother hand, LogAction utilize free energy-based sampling and uncertainty-based\nsampling to select logs located at the distribution boundaries for manual\nlabeling, thus addresses the data distribution gap in transfer learning with\nminimal human labeling efforts. Experimental results on six different\ncombinations of datasets demonstrate that LogAction achieves an average 93.01%\nF1 score with only 2% of manual labels, outperforming some state-of-the-art\nmethods by 26.28%. Website: https://logaction.github.io", "AI": {"tldr": "LogAction\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u9886\u57df\u9002\u5e94\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u97002%\u624b\u52a8\u6807\u6ce8\u5373\u53ef\u8fbe\u523093.01%\u7684\u5e73\u5747F1\u5206\u6570\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6210\u719f\u7cfb\u7edf\u7684\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u81ea\u7531\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u91c7\u6837\u7b56\u7565\u9009\u62e9\u5206\u5e03\u8fb9\u754c\u5904\u7684\u65e5\u5fd7\u8fdb\u884c\u624b\u52a8\u6807\u6ce8\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\uff0c\u4ec5\u4f7f\u75282%\u624b\u52a8\u6807\u6ce8\u5c31\u8fbe\u523093.01%\u7684\u5e73\u5747F1\u5206\u6570\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534726.28%\u3002", "conclusion": "LogAction\u901a\u8fc7\u4e3b\u52a8\u9886\u57df\u9002\u5e94\u6709\u6548\u89e3\u51b3\u4e86\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6807\u6ce8\u4f9d\u8d56\u548c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2510.03251", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03251", "abs": "https://arxiv.org/abs/2510.03251", "authors": ["Hanzhong Cao", "Wenbo Yan", "Ying Tan"], "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "comment": null, "summary": "Many methods aim to enhance time series forecasting by decomposing the series\nthrough intricate model structures and prior knowledge, yet they are inevitably\nlimited by computational complexity and the robustness of the assumptions. Our\nresearch uncovers that in the complex domain and higher-order hypercomplex\nspaces, the characteristic frequencies of time series naturally decrease.\nLeveraging this insight, we propose Numerion, a time series forecasting model\nbased on multiple hypercomplex spaces. Specifically, grounded in theoretical\nsupport, we generalize linear layers and activation functions to hypercomplex\nspaces of arbitrary power-of-two dimensions and introduce a novel\nReal-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.\nNumerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces\nof varying dimensions, naturally decomposing and independently modeling the\nseries, and adaptively fuses the latent patterns exhibited in different spaces\nthrough a dynamic fusion mechanism. Experiments validate the model`s\nperformance, achieving state-of-the-art results on multiple public datasets.\nVisualizations and quantitative analyses comprehensively demonstrate the\nability of multi-dimensional RHR-MLPs to naturally decompose time series and\nreveal the tendency of higher dimensional hypercomplex spaces to capture lower\nfrequency features.", "AI": {"tldr": "Numerion\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d85\u590d\u6570\u7a7a\u95f4\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u4e0d\u540c\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\u6765\u81ea\u7136\u5206\u89e3\u548c\u5efa\u6a21\u5e8f\u5217\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u590d\u6742\u6a21\u578b\u7ed3\u6784\u548c\u5148\u9a8c\u77e5\u8bc6\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5047\u8bbe\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u8d85\u590d\u6570\u7a7a\u95f4\u4e2d\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\u9891\u7387\u81ea\u7136\u964d\u4f4e\uff0c\u8fd9\u4e3a\u66f4\u6709\u6548\u7684\u5206\u89e3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51faNumerion\u6a21\u578b\uff0c\u5c06\u7ebf\u6027\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u63a8\u5e7f\u5230\u4efb\u610f2\u7684\u5e42\u6b21\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u5f15\u5165Real-Hypercomplex-Real\u57df\u591a\u5c42\u611f\u77e5\u673a(RHR-MLP)\u67b6\u6784\uff0c\u4f7f\u7528\u591a\u4e2aRHR-MLP\u5728\u4e0d\u540c\u7ef4\u5ea6\u8d85\u590d\u6570\u7a7a\u95f4\u4e2d\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u4e0d\u540c\u7a7a\u95f4\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u53ef\u89c6\u5316\u548c\u5b9a\u91cf\u5206\u6790\u8bc1\u660e\u4e86\u591a\u7ef4RHR-MLP\u80fd\u591f\u81ea\u7136\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u63ed\u793a\u4e86\u9ad8\u7ef4\u8d85\u590d\u6570\u7a7a\u95f4\u503e\u5411\u4e8e\u6355\u6349\u4f4e\u9891\u7279\u5f81\u7684\u89c4\u5f8b\u3002", "conclusion": "Numerion\u901a\u8fc7\u8d85\u590d\u6570\u7a7a\u95f4\u5b9e\u73b0\u4e86\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u7136\u5206\u89e3\u548c\u6709\u6548\u5efa\u6a21\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8d85\u590d\u6570\u7a7a\u95f4\u5728\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.03293", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03293", "abs": "https://arxiv.org/abs/2510.03293", "authors": ["Rana Shahout", "Colin Cai", "Yilun Du", "Minlan Yu", "Michael Mitzenmacher"], "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing", "comment": null, "summary": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each\ntoken to a subset of experts through a learned gate function. While conditional\nrouting reduces training costs, it shifts the burden on inference memory:\nexpert parameters and activations consume memory, limiting the number of\nexperts per device. As tokens are routed, some experts become overloaded while\nothers are underutilized. Because experts are mapped to GPUs, this imbalance\ntranslates directly into degraded system performance in terms of latency,\nthroughput, and cost. We present LASER, a plug-and-play, inference-time routing\nalgorithm that balances load while preserving accuracy. LASER adapts to the\nshape of the gate's score distribution. When scores provide a clear preference,\nit routes to the strongest experts; when scores are more uniform, it broadens\nthe set of viable experts and routes to the least-loaded among them. Because\nLASER relies only on gate scores from a trained model, it integrates directly\ninto existing MoE inference pipelines without retraining or finetuning. We\nevaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets\n(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,\ntranslating into lower latency and higher throughput, while keeping the\naccuracy changes negligible.", "AI": {"tldr": "LASER\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff0c\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u901a\u8fc7\u6761\u4ef6\u8def\u7531\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\uff0c\u4f46\u63a8\u7406\u65f6\u4e13\u5bb6\u53c2\u6570\u548c\u6fc0\u6d3b\u5360\u7528\u5185\u5b58\uff0c\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\uff08\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\uff09\u3002", "method": "LASER\u6839\u636e\u95e8\u63a7\u5206\u6570\u5206\u5e03\u81ea\u9002\u5e94\u8def\u7531\uff1a\u5f53\u5206\u6570\u663e\u793a\u660e\u663e\u504f\u597d\u65f6\u8def\u7531\u5230\u6700\u5f3a\u4e13\u5bb6\uff1b\u5f53\u5206\u6570\u8f83\u5747\u5300\u65f6\u6269\u5927\u53ef\u884c\u4e13\u5bb6\u96c6\u5e76\u8def\u7531\u5230\u8d1f\u8f7d\u6700\u8f7b\u7684\u4e13\u5bb6\u3002\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "\u5728Mixtral-8x7B\u548cDeepSeek-MoE-16b-chat\u6a21\u578b\u4e0a\u6d4b\u8bd5\u56db\u4e2a\u6570\u636e\u96c6\uff0cLASER\u6539\u5584\u4e86\u8d1f\u8f7d\u5747\u8861\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u51c6\u786e\u7387\u53d8\u5316\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "LASER\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u80fd\u663e\u8457\u6539\u5584\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03252", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03252", "abs": "https://arxiv.org/abs/2510.03252", "authors": ["Duc Kieu", "Kien Do", "Tuan Hoang", "Thao Minh Le", "Tung Kieu", "Dang Nguyen", "Thin Nguyen"], "title": "Universal Multi-Domain Translation via Diffusion Routers", "comment": null, "summary": "Multi-domain translation (MDT) aims to learn translations between multiple\ndomains, yet existing approaches either require fully aligned tuples or can\nonly handle domain pairs seen in training, limiting their practicality and\nexcluding many cross-domain mappings. We introduce universal MDT (UMDT), a\ngeneralization of MDT that seeks to translate between any pair of $K$ domains\nusing only $K-1$ paired datasets with a central domain. To tackle this problem,\nwe propose Diffusion Router (DR), a unified diffusion-based framework that\nmodels all central$\\leftrightarrow$non-central translations with a single noise\npredictor conditioned on the source and target domain labels. DR enables\nindirect non-central translations by routing through the central domain. We\nfurther introduce a novel scalable learning strategy with a variational-bound\nobjective and an efficient Tweedie refinement procedure to support direct\nnon-central mappings. Through evaluation on three large-scale UMDT benchmarks,\nDR achieves state-of-the-art results for both indirect and direct translations,\nwhile lowering sampling cost and unlocking novel tasks such as\nsketch$\\leftrightarrow$segmentation. These results establish DR as a scalable\nand versatile framework for universal translation across multiple domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u901a\u7528\u591a\u57df\u7ffb\u8bd1(UMDT)\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u8def\u7531(DR)\u65b9\u6cd5\uff0c\u53ea\u9700K-1\u4e2a\u914d\u5bf9\u6570\u636e\u96c6\u5c31\u80fd\u5b9e\u73b0K\u4e2a\u57df\u4e4b\u95f4\u7684\u4efb\u610f\u53cc\u5411\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u591a\u57df\u7ffb\u8bd1\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u5bf9\u9f50\u7684\u5143\u7ec4\u6216\u53ea\u80fd\u5904\u7406\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u57df\u5bf9\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u5e76\u6392\u9664\u4e86\u8bb8\u591a\u8de8\u57df\u6620\u5c04\u3002", "method": "\u4f7f\u7528\u6269\u6563\u8def\u7531(DR)\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u566a\u58f0\u9884\u6d4b\u5668\u5efa\u6a21\u6240\u6709\u4e2d\u5fc3\u57df\u4e0e\u975e\u4e2d\u5fc3\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u652f\u6301\u901a\u8fc7\u4e2d\u5fc3\u57df\u8fdb\u884c\u95f4\u63a5\u7ffb\u8bd1\uff0c\u5e76\u5f15\u5165\u53d8\u5206\u8fb9\u754c\u76ee\u6807\u548cTweedie\u7cbe\u70bc\u7a0b\u5e8f\u652f\u6301\u76f4\u63a5\u6620\u5c04\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21UMDT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u652f\u6301\u95f4\u63a5\u548c\u76f4\u63a5\u7ffb\u8bd1\uff0c\u964d\u4f4e\u91c7\u6837\u6210\u672c\uff0c\u5e76\u89e3\u9501\u4e86\u8349\u56fe\u2194\u5206\u5272\u7b49\u65b0\u4efb\u52a1\u3002", "conclusion": "DR\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u591a\u57df\u7ffb\u8bd1\u6846\u67b6\uff0c\u4e3a\u8de8\u591a\u4e2a\u57df\u7684\u901a\u7528\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03298", "categories": ["cs.LG", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03298", "abs": "https://arxiv.org/abs/2510.03298", "authors": ["Dongqi Zheng", "Wenjin Fu"], "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models", "comment": "Accepted by 39th NeurIPS - Constrained Optimization for Machine\n  Learning", "summary": "We introduce Constraint-Aware Federated Learning with Lagrangian Dual\nOptimization (CAFL-L), a principled extension of FedAvg that explicitly\nincorporates device-level resource constraints including energy, communication,\nmemory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to\ndynamically adapt training hyperparameters -- freezing depth, local steps,\nbatch size, and communication compression -- while preserving training\nstability through token-budget preservation via gradient accumulation.\nExperiments on a character-level language model demonstrate that CAFL-L\nachieves superior constraint satisfaction compared to standard FedAvg (reducing\nmemory usage by 20% and communication by 95%) while maintaining competitive\nvalidation performance, making it practical for deployment on\nresource-constrained edge devices.", "AI": {"tldr": "CAFL-L\u662f\u4e00\u79cd\u5e26\u7ea6\u675f\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8d85\u53c2\u6570\uff0c\u5728\u6ee1\u8db3\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u9700\u8981\u663e\u5f0f\u8003\u8651\u8bbe\u5907\u7ea7\u8d44\u6e90\u7ea6\u675f\uff08\u80fd\u91cf\u3001\u901a\u4fe1\u3001\u5185\u5b58\u3001\u70ed\u9884\u7b97\u7b49\uff09\u3002", "method": "\u57fa\u4e8eFedAvg\u6269\u5c55\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u52a8\u6001\u8c03\u6574\u51bb\u7ed3\u6df1\u5ea6\u3001\u672c\u5730\u6b65\u6570\u3001\u6279\u91cf\u5927\u5c0f\u548c\u901a\u4fe1\u538b\u7f29\u7b49\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u68af\u5ea6\u79ef\u7d2f\u4fdd\u6301\u4ee4\u724c\u9884\u7b97\u3002", "result": "\u5728\u5b57\u7b26\u7ea7\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6FedAvg\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120%\uff0c\u901a\u4fe1\u51cf\u5c1195%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "CAFL-L\u5728\u6ee1\u8db3\u8d44\u6e90\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.03253", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "AI": {"tldr": "HPL\u662f\u4e00\u79cd\u5206\u5c42\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u7ea7DPO\u4fe1\u53f7\u8fc7\u7c97\u548c\u6b65\u9aa4\u7ea7DPO\u8fc7\u4e8e\u77ed\u89c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u79bb\u7ebf\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u4f18\u5316LLM\u667a\u80fd\u4f53\u65f6\u9762\u4e34\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u8f68\u8ff9\u7ea7DPO\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u6b65\u9aa4\u7ea7DPO\u8fc7\u4e8e\u77ed\u89c6\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u6b65\u884c\u4e3a\u4ef7\u503c\u3002", "method": "HPL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u8f68\u8ff9\u7ea7\u548c\u6b65\u9aa4\u7ea7DPO\u786e\u4fdd\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\u7a33\u5b9a\u6027\uff1b2\uff09\u7ec4\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u52a8\u4f5c\u7ec4\uff1b3\uff09\u53cc\u5c42\u8bfe\u7a0b\u8c03\u5ea6\u5668\uff0c\u6309\u7ec4\u957f\u5ea6\u548c\u6837\u672c\u96be\u5ea6\u7ec4\u7ec7\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPL\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5206\u6790\u8868\u660e\u5206\u5c42DPO\u635f\u5931\u6709\u6548\u6574\u5408\u4e86\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\uff0c\u53cc\u5c42\u8bfe\u7a0b\u5bf9\u89e3\u51b3\u4ece\u7b80\u5355\u884c\u4e3a\u5230\u590d\u6742\u591a\u6b65\u5e8f\u5217\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "HPL\u901a\u8fc7\u5206\u5c42\u504f\u597d\u5b66\u4e60\u548c\u53cc\u5c42\u8bfe\u7a0b\u8c03\u5ea6\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u4f18\u5316\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u590d\u6742\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.03334", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03334", "abs": "https://arxiv.org/abs/2510.03334", "authors": ["Zerui Wang", "Qinghao Hu", "Ana Klimovic", "Tianwei Zhang", "Yonggang Wen", "Peng Sun", "Dahua Lin"], "title": "Semantic-Aware Scheduling for GPU Clusters with Large Language Models", "comment": null, "summary": "Deep learning (DL) schedulers are pivotal in optimizing resource allocation\nin GPU clusters, but operate with a critical limitation: they are largely blind\nto the semantic context of the jobs they manage. This forces them to rely on\nlimited metadata, leading to high profiling overhead, unreliable duration\nestimation, inadequate failure handling, and poor observability. To this end,\nwe propose SchedMate, a framework that bridges this semantic gap by\nsystematically extracting deep insights from overlooked, unstructured data\nsources: source code, runtime logs, and historical jobs. SchedMate enhances\nexisting schedulers non-intrusively through three LLM-based components. Our\nimplementation integrates seamlessly with existing deep learning schedulers.\nEvaluations on a 128-GPU physical cluster and extensive simulations on\nproduction traces show SchedMate reduces average job completion times by up to\n1.91x, substantially enhancing the scheduling performance, demonstrating the\ncritical role of semantic-awareness in modern DL scheduling.", "AI": {"tldr": "SchedMate\u662f\u4e00\u4e2a\u901a\u8fc7\u4ece\u6e90\u4ee3\u7801\u3001\u8fd0\u884c\u65f6\u65e5\u5fd7\u548c\u5386\u53f2\u4f5c\u4e1a\u4e2d\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u6765\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u5668\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe1.91\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u5668\u7f3a\u4e4f\u5bf9\u4f5c\u4e1a\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u53ea\u80fd\u4f9d\u8d56\u6709\u9650\u7684\u5143\u6570\u636e\uff0c\u5bfc\u81f4\u9ad8\u5206\u6790\u5f00\u9500\u3001\u4e0d\u53ef\u9760\u7684\u6301\u7eed\u65f6\u95f4\u4f30\u8ba1\u3001\u4e0d\u5145\u5206\u7684\u6545\u969c\u5904\u7406\u548c\u5dee\u7684\u53ef\u89c2\u6d4b\u6027\u3002", "method": "SchedMate\u901a\u8fc7\u4e09\u4e2a\u57fa\u4e8eLLM\u7684\u7ec4\u4ef6\u975e\u4fb5\u5165\u5f0f\u5730\u589e\u5f3a\u73b0\u6709\u8c03\u5ea6\u5668\uff0c\u7cfb\u7edf\u5730\u4ece\u6e90\u4ee3\u7801\u3001\u8fd0\u884c\u65f6\u65e5\u5fd7\u548c\u5386\u53f2\u4f5c\u4e1a\u7b49\u975e\u7ed3\u6784\u5316\u6570\u636e\u6e90\u4e2d\u63d0\u53d6\u6df1\u5ea6\u6d1e\u5bdf\u3002", "result": "\u5728128-GPU\u7269\u7406\u96c6\u7fa4\u4e0a\u7684\u8bc4\u4f30\u548c\u751f\u4ea7\u75d5\u8ff9\u7684\u5e7f\u6cdb\u6a21\u62df\u663e\u793a\uff0cSchedMate\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe1.91\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u5728\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8c03\u5ea6\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0cSchedMate\u901a\u8fc7\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\u6709\u6548\u63d0\u5347\u4e86\u8c03\u5ea6\u6548\u7387\u3002"}}
{"id": "2510.03254", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03254", "abs": "https://arxiv.org/abs/2510.03254", "authors": ["David Benfield", "Stefano Coniglio", "Phan Tu Vuong", "Alain Zemkoho"], "title": "Adversarial training with restricted data manipulation", "comment": "21 page, 5 figures", "summary": "Adversarial machine learning concerns situations in which learners face\nattacks from active adversaries. Such scenarios arise in applications such as\nspam email filtering, malware detection and fake image generation, where\nsecurity methods must be actively updated to keep up with the everimproving\ngeneration of malicious data. Pessimistic Bilevel optimisation has been shown\nto be an effective method of training resilient classifiers against such\nadversaries. By modelling these scenarios as a game between the learner and the\nadversary, we anticipate how the adversary will modify their data and then\ntrain a resilient classifier accordingly. However, since existing pessimistic\nbilevel approaches feature an unrestricted adversary, the model is vulnerable\nto becoming overly pessimistic and unrealistic. When finding the optimal\nsolution that defeats the classifier, it is possible that the adversary's data\nbecomes nonsensical and loses its intended nature. Such an adversary will not\nproperly reflect reality, and consequently, will lead to poor classifier\nperformance when implemented on real-world data. By constructing a constrained\npessimistic bilevel optimisation model, we restrict the adversary's movements\nand identify a solution that better reflects reality. We demonstrate through\nexperiments that this model performs, on average, better than the existing\napproach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u9650\u5236\u5bf9\u624b\u7684\u79fb\u52a8\u8303\u56f4\u6765\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u4e2d\u5bf9\u624b\u53ef\u80fd\u4ea7\u751f\u65e0\u610f\u4e49\u6570\u636e\u7684\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u7684\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u4e2d\u5bf9\u624b\u4e0d\u53d7\u9650\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8fc7\u4e8e\u60b2\u89c2\u548c\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u4e3a\u5bf9\u624b\u5728\u5bfb\u627e\u6700\u4f18\u89e3\u65f6\u53ef\u80fd\u4ea7\u751f\u65e0\u610f\u4e49\u7684\u6570\u636e\uff0c\u8fd9\u65e0\u6cd5\u6b63\u786e\u53cd\u6620\u73b0\u5b9e\u60c5\u51b5\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u5206\u7c7b\u5668\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u9650\u5236\u5bf9\u624b\u7684\u79fb\u52a8\u8303\u56f4\u6765\u8bc6\u522b\u66f4\u7b26\u5408\u73b0\u5b9e\u60c5\u51b5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\u5bf9\u6297\u573a\u666f\uff0c\u63d0\u9ad8\u5206\u7c7b\u5668\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.03371", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03371", "abs": "https://arxiv.org/abs/2510.03371", "authors": ["Sasho Nedelkoski", "Alexander Acker", "Odej Kao", "Soeren Becker", "Dominik Scheinert"], "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization", "comment": "NeurIPS 2025 - DynaFront 2025: Dynamics at the Frontiers of\n  Optimization, Sampling, and Games Workshop", "summary": "The training of large models demands substantial computational resources,\ntypically available only in data centers with high-bandwidth interconnects.\nHowever, reducing the reliance on high-bandwidth interconnects between nodes\nenables the use of distributed compute resources as an alternative to\ncentralized data center training. Building on recent advances in distributed\nmodel training, we propose an approach that further reduces communication by\ncombining infrequent synchronizations across distributed model replicas with\ngradient momentum compression. In particular, we treat the optimizer momentum\nas a signal and decompose the Nesterov momentum into high- and low-frequency\ncomponents via the discrete cosine transform (DCT). Only the high-frequency\ncomponents are synchronized across model replicas every $H$ steps. Empirically,\nour method achieves up to a $16\\times$ reduction in communication compared to\nthe baseline DiLoCo, and it generalizes across architectures, including\ntransformer-based language models and convolutional neural networks for images.\nOverall, this work advances the feasibility of training large models on\ndistributed nodes with low-bandwidth interconnects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u9891\u7e41\u540c\u6b65\u548c\u68af\u5ea6\u52a8\u91cf\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u5c06Nesterov\u52a8\u91cf\u5206\u89e3\u4e3a\u9ad8\u4f4e\u9891\u5206\u91cf\uff0c\u4ec5\u540c\u6b65\u9ad8\u9891\u5206\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u51cf\u5c11\u5bf9\u9ad8\u5e26\u5bbd\u4e92\u8fde\u7684\u4f9d\u8d56\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u8d44\u6e90\u4e0a\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u4ec5\u9650\u4e8e\u6570\u636e\u4e2d\u5fc3\u3002", "method": "\u5c06\u4f18\u5316\u5668\u52a8\u91cf\u89c6\u4e3a\u4fe1\u53f7\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u5c06Nesterov\u52a8\u91cf\u5206\u89e3\u4e3a\u9ad8\u4f4e\u9891\u5206\u91cf\uff0c\u6bcfH\u6b65\u4ec5\u540c\u6b65\u9ad8\u9891\u5206\u91cf\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebfDiLoCo\u5b9e\u73b0\u4e86\u9ad8\u8fbe16\u500d\u7684\u901a\u4fe1\u51cf\u5c11\uff0c\u5728\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5728\u4f4e\u5e26\u5bbd\u4e92\u8fde\u7684\u5206\u5e03\u5f0f\u8282\u70b9\u4e0a\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data.", "AI": {"tldr": "SciTS\u662f\u4e00\u4e2a\u6db5\u76d612\u4e2a\u79d1\u5b66\u9886\u57df\u300143\u4e2a\u4efb\u52a1\u7684\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e07+\u5b9e\u4f8b\u3002\u7814\u7a76\u53d1\u73b0\u901a\u7528LLM\u6bd4\u4e13\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u4f1a\u9650\u5236\u6027\u80fd\u3002\u4f5c\u8005\u63d0\u51fa\u4e86TimeOmni\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001LLM\u8981\u4e48\u5c06\u6570\u503c\u5e8f\u5217\u7f16\u7801\u4e3a\u6587\u672c\uff0c\u8981\u4e48\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5168\u9762\u7406\u89e3\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u3002\u73b0\u6709\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u901a\u5e38\u4e13\u95e8\u7528\u4e8e\u9884\u6d4b\u6216\u5206\u6790\uff0c\u5176\u5728\u975e\u5468\u671f\u6027\u3001\u5f02\u8d28\u79d1\u5b66\u4fe1\u53f7\u4e0a\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u6784\u5efaSciTS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u4e2a\u79d1\u5b66\u9886\u57df\u300143\u4e2a\u4efb\u52a1\uff0c\u5305\u542b5\u4e07+\u5b9e\u4f8b\uff0c\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\u4ece10^0\u523010^7\uff0c\u9891\u7387\u9ad8\u8fbe10MHz\u3002\u5bf917\u4e2a\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u7eaf\u6587\u672cLLM\u3001\u591a\u6a21\u6001LLM\u548c\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002\u63d0\u51faTimeOmni\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u901a\u7528LLM\u6bd4\u4e13\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u4f1a\u9650\u5236\u6027\u80fd\uff0c\u56e0\u4e3a\u8fc7\u957f\u7684\u5e8f\u5217\u548c\u6570\u503c\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u57fa\u51c6\u6d4b\u8bd5\u548c\u5efa\u6a21\u6846\u67b6\u7684\u7a7a\u767d\uff0c\u4e3aLLM\u7406\u89e3\u548c\u751f\u6210\u590d\u6742\u65f6\u95f4\u79d1\u5b66\u6570\u636e\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03513", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03513", "abs": "https://arxiv.org/abs/2510.03513", "authors": ["Taha M. Mahmoud", "Naima Kaabouch"], "title": "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT", "comment": "This work has been published in the Proceedings of the 2025 IEEE\n  International Conference on Applied Cloud and Data Science and Applications\n  (ACDSA). The final published version is available via IEEE Xplore at\n  https://doi.org/10.1109/ACDSA65407.2025.11165820", "summary": "The rapid growth of the Internet of Things (IoT) has expanded opportunities\nfor innovation but also increased exposure to botnet-driven cyberattacks.\nConventional detection methods often struggle with scalability, privacy, and\nadaptability in resource-constrained IoT environments. To address these\nchallenges, we present a lightweight and privacy-preserving botnet detection\nframework based on federated learning. This approach enables distributed\ndevices to collaboratively train models without exchanging raw data, thus\nmaintaining user privacy while preserving detection accuracy. A\ncommunication-efficient aggregation strategy is introduced to reduce overhead,\nensuring suitability for constrained IoT networks. Experiments on benchmark IoT\nbotnet datasets demonstrate that the framework achieves high detection accuracy\nwhile substantially reducing communication costs. These findings highlight\nfederated learning as a practical path toward scalable, secure, and\nprivacy-aware intrusion detection for IoT ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u9690\u79c1\u4fdd\u62a4\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c", "motivation": "\u7269\u8054\u7f51\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u521b\u65b0\u673a\u4f1a\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u50f5\u5c38\u7f51\u7edc\u9a71\u52a8\u7684\u7f51\u7edc\u653b\u51fb\u98ce\u9669\u3002\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u9002\u5e94\u6027\u6311\u6218", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u5206\u5e03\u5f0f\u8bbe\u5907\u80fd\u591f\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\u800c\u65e0\u9700\u4ea4\u6362\u539f\u59cb\u6570\u636e\uff0c\u540c\u65f6\u5f15\u5165\u901a\u4fe1\u9ad8\u6548\u7684\u805a\u5408\u7b56\u7565\u4ee5\u51cf\u5c11\u5f00\u9500", "result": "\u5728\u57fa\u51c6\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u9690\u79c1\u611f\u77e5\u7684\u7269\u8054\u7f51\u5165\u4fb5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
{"id": "2510.03257", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT .", "AI": {"tldr": "\u63d0\u51faTriple-BERT\u65b9\u6cd5\uff0c\u4f7f\u7528\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u7f51\u7ea6\u8f66\u5e73\u53f0\u5927\u89c4\u6a21\u8ba2\u5355\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u548cBERT\u7f51\u7edc\u5904\u7406\u5927\u89c4\u6a21\u52a8\u4f5c\u548c\u89c2\u5bdf\u7a7a\u95f4\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534711.95%\u6027\u80fd\u3002", "motivation": "\u7f51\u7ea6\u8f66\u5e73\u53f0\u9762\u4e34\u5b9e\u65f6\u8ba2\u5355\u5206\u914d\u6311\u6218\uff0c\u73b0\u6709MARL\u65b9\u6cd5\u5b58\u5728\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u4e0d\u8db3\u3001\u5408\u4f5c\u6027\u5dee\u6216\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u96c6\u4e2d\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eTD3\u53d8\u4f53\u7684\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u5c06\u8054\u5408\u52a8\u4f5c\u6982\u7387\u5206\u89e3\u4e3a\u5355\u4e2a\u53f8\u673a\u52a8\u4f5c\u6982\u7387\uff0c\u4f7f\u7528BERT\u7f51\u7edc\u901a\u8fc7\u53c2\u6570\u91cd\u7528\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5927\u89c4\u6a21\u89c2\u5bdf\u7a7a\u95f4\u3002", "result": "\u5728\u66fc\u54c8\u987f\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534711.95%\u6027\u80fd\uff0c\u670d\u52a1\u8ba2\u5355\u589e\u52a04.26%\uff0c\u63a5\u9a7e\u65f6\u95f4\u51cf\u5c1122.25%\u3002", "conclusion": "Triple-BERT\u901a\u8fc7\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8ba2\u5355\u5206\u914d\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2510.03601", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SP", "I.2.6; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.03601", "abs": "https://arxiv.org/abs/2510.03601", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Kai-Chun Liu", "Yu Tsao"], "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation", "comment": "15 pages, 7 figures, and published in IEEE Sensors Journal", "summary": "The rising aging population has increased the importance of fall detection\n(FD) systems as an assistive technology, where deep learning techniques are\nwidely applied to enhance accuracy. FD systems typically use edge devices (EDs)\nworn by individuals to collect real-time data, which are transmitted to a cloud\ncenter (CC) or processed locally. However, this architecture faces challenges\nsuch as a limited ED model size and data transmission latency to the CC. Mobile\nedge computing (MEC), which allows computations at MEC servers deployed between\nEDs and CC, has been explored to address these challenges. We propose a\nmultilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC\nsplits the architecture into stations, each with a neural network model. If\nfront-end equipment cannot detect falls reliably, data are transmitted to a\nstation with more robust back-end computing. The knowledge distillation (KD)\napproach was employed to improve front-end detection accuracy by allowing\nhigh-power back-end stations to provide additional learning experiences,\nenhancing precision while reducing latency and processing loads. Simulation\nresults demonstrate that the KD approach improved accuracy by 11.65% on the\nSisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also\nreduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on\nthe SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD\nsystem exhibits improved accuracy and reduced latency.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u6846\u67b6\u7528\u4e8e\u8dcc\u5012\u68c0\u6d4b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5e73\u8861\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\uff0c\u5728SisFall\u548cFallAllD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u51c6\u786e\u738711.65%\u548c2.78%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u8001\u9f84\u5316\u4eba\u53e3\u589e\u52a0\uff0c\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u4f20\u7edf\u8fb9\u7f18\u8bbe\u5907\u6a21\u578b\u5927\u5c0f\u6709\u9650\uff0c\u4e91\u7aef\u4f20\u8f93\u5b58\u5728\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u67b6\u6784\u5206\u4e3a\u591a\u4e2a\u7ad9\u70b9\uff0c\u6bcf\u4e2a\u7ad9\u70b9\u914d\u5907\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u8ba9\u9ad8\u529f\u7387\u540e\u7aef\u7ad9\u70b9\u4e3a\u524d\u7aef\u8bbe\u5907\u63d0\u4f9b\u989d\u5916\u5b66\u4e60\u7ecf\u9a8c\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728SisFall\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u534711.65%\uff0c\u5728FallAllD\u6570\u636e\u96c6\u4e0a\u63d0\u53472.78%\u3002\u4e0e\u65e0\u77e5\u8bc6\u84b8\u998f\u7684MLMEC\u76f8\u6bd4\uff0c\u5728FallAllD\u6570\u636e\u96c6\u4e0a\u5ef6\u8fdf\u964d\u4f4e54.15%\uff0c\u5728SisFall\u6570\u636e\u96c6\u4e0a\u964d\u4f4e46.67%\u3002", "conclusion": "\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u4e3a\u5b9e\u65f6\u8dcc\u5012\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03258", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03258", "abs": "https://arxiv.org/abs/2510.03258", "authors": ["Chang'an Yi", "Xiaohui Deng", "Shuaicheng Niu", "Yan Zhou"], "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "comment": "11pages,6 figures", "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to\nunknown test data with potential distribution shifts in an online manner. Many\nexisting TTA methods rely on entropy as a confidence metric to optimize the\nmodel. However, these approaches are sensitive to the predefined entropy\nthreshold, influencing which samples are chosen for model adaptation.\nConsequently, potentially reliable target samples are often overlooked and\nunderutilized. For instance, a sample's entropy might slightly exceed the\nthreshold initially, but fall below it after the model is updated. Such samples\ncan provide stable supervised information and offer a normal range of gradients\nto guide model adaptation. In this paper, we propose a general approach,\n\\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the\npreviously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}}\nsa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch\nnetwork to strike a balance between extracting domain-agnostic representations\nand achieving high performance on target data. Comprehensive experiments across\nmultiple architectures demonstrate that POEM consistently outperforms existing\nTTA methods in both challenging scenarios and real-world domain shifts, while\nremaining computationally efficient. The effectiveness of POEM is evaluated\nthrough extensive analyses and thorough ablation studies. Moreover, the core\nidea behind POEM can be employed as an augmentation strategy to boost the\nperformance of existing TTA approaches. The source code is publicly available\nat \\emph{https://github.com/ycarobot/POEM}", "AI": {"tldr": "\u63d0\u51faPOEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u4e4b\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u6765\u6539\u8fdb\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94(TTA)\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u71b5\u9608\u503c\u7684\u654f\u611f\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u4f9d\u8d56\u71b5\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u5bf9\u9884\u5b9a\u4e49\u7684\u71b5\u9608\u503c\u654f\u611f\uff0c\u5bfc\u81f4\u8bb8\u591a\u6f5c\u5728\u53ef\u9760\u7684\u6837\u672c\u88ab\u5ffd\u89c6\u3002\u8fd9\u4e9b\u6837\u672c\u5373\u4f7f\u521d\u59cb\u71b5\u7565\u8d85\u9608\u503c\uff0c\u4f46\u5728\u6a21\u578b\u66f4\u65b0\u540e\u53ef\u80fd\u53d8\u5f97\u53ef\u9760\uff0c\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u76d1\u7763\u4fe1\u606f\u548c\u6b63\u5e38\u8303\u56f4\u7684\u68af\u5ea6\u6765\u6307\u5bfc\u6a21\u578b\u9002\u5e94\u3002", "method": "\u63d0\u51faPOEM\u65b9\u6cd5\uff0c\u63a2\u7d22\u4e4b\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u3002\u5f15\u5165\u989d\u5916\u7684Adapt Branch\u7f51\u7edc\uff0c\u5728\u63d0\u53d6\u9886\u57df\u65e0\u5173\u8868\u793a\u548c\u5728\u76ee\u6807\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cPOEM\u5728\u6311\u6218\u6027\u573a\u666f\u548c\u771f\u5b9e\u4e16\u754c\u9886\u57df\u504f\u79fb\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709TTA\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002POEM\u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4f5c\u4e3a\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u73b0\u6709TTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "POEM\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e4b\u524d\u88ab\u5ffd\u89c6\u7684\u53ef\u9760\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7684\u6027\u80fd\uff0c\u4e3aTTA\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.03744", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.03744", "abs": "https://arxiv.org/abs/2510.03744", "authors": ["Qianfei Fan", "Jiayu Wei", "Peijun Zhu", "Wensheng Ye", "Meie Fang"], "title": "HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting", "comment": "V1", "summary": "Accurate decade-scale daily runoff forecasting in small watersheds is\ndifficult because signals blend drifting trends, multi-scale seasonal cycles,\nregime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,\nPatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single\nfacets and under-utilize unlabeled spans, limiting regime adaptivity. We\npropose HydroFusion-LMF, a unified framework that (i) performs a learnable\ntrend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes\nresiduals through a compact heterogeneous expert set (linear refinement,\nfrequency kernel, patch Transformer, recurrent memory, dynamically normalized\nattention), (iii) fuses expert outputs via a hydrologic context-aware gate\nconditioned on day-of-year phase, antecedent precipitation, local variance,\nflood indicators, and static basin attributes, and (iv) augments supervision\nwith a semi-supervised multi-task objective (composite MSE/MAE + extreme\nemphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,\naugmentation consistency, variance-filtered pseudo-labeling). Optional adapter\n/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a\n~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,\nimproving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean\nbaseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions\nrelative to baselines. The framework balances interpretability (explicit\ncomponents, sparse gating) with performance, advancing label-efficient\nhydrologic forecasting under non-stationarity.", "AI": {"tldr": "HydroFusion-LMF\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5c0f\u6d41\u57df\u65e5\u5f84\u6d41\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8d8b\u52bf-\u5b63\u8282-\u6b8b\u5dee\u5206\u89e3\u3001\u5f02\u6784\u4e13\u5bb6\u96c6\u5408\u3001\u6c34\u6587\u611f\u77e5\u95e8\u63a7\u548c\u534a\u76d1\u7763\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5728\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5341\u5e74\u5c3a\u5ea6\u7684\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6d41\u57df\u65e5\u5f84\u6d41\u9884\u6d4b\u4e2d\u4fe1\u53f7\u590d\u6742\uff08\u6f02\u79fb\u8d8b\u52bf\u3001\u591a\u5c3a\u5ea6\u5b63\u8282\u5468\u671f\u3001\u673a\u5236\u8f6c\u6362\u3001\u7a00\u758f\u6781\u503c\uff09\u4ee5\u53ca\u73b0\u6709\u6df1\u5ea6\u6a21\u578b\u4ec5\u9488\u5bf9\u5355\u4e00\u65b9\u9762\u4e14\u672a\u5145\u5206\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56db\u6b65\u6cd5\uff1a(1)\u53ef\u5b66\u4e60\u7684\u8d8b\u52bf-\u5b63\u8282-\u6b8b\u5dee\u5206\u89e3\u964d\u4f4e\u975e\u5e73\u7a33\u6027\uff1b(2)\u6b8b\u5dee\u901a\u8fc7\u7d27\u51d1\u5f02\u6784\u4e13\u5bb6\u96c6\u5408\u5904\u7406\uff1b(3)\u6c34\u6587\u4e0a\u4e0b\u6587\u611f\u77e5\u95e8\u63a7\u878d\u5408\u4e13\u5bb6\u8f93\u51fa\uff1b(4)\u534a\u76d1\u7763\u591a\u4efb\u52a1\u76ee\u6807\u589e\u5f3a\u76d1\u7763\u3002", "result": "\u5728\u7ea610\u5e74\u65e5\u6570\u636e\u96c6\u4e0a\uff0cMSE\u4e3a1.0128\uff0cMAE\u4e3a0.5818\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebfDLinear\u5206\u522b\u63d0\u534710.2%\u548c10.3%\uff0c\u6bd4\u5e73\u5747\u57fa\u7ebf\u5206\u522b\u63d0\u534724.6%\u548c17.1%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u53ef\u89e3\u91ca\u6027\uff08\u663e\u5f0f\u7ec4\u4ef6\u3001\u7a00\u758f\u95e8\u63a7\uff09\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u63a8\u8fdb\u4e86\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u6807\u7b7e\u9ad8\u6548\u7684\u6c34\u6587\u9884\u6d4b\u3002"}}
{"id": "2510.03259", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03259", "abs": "https://arxiv.org/abs/2510.03259", "authors": ["Yoonjeon Kim", "Doohyuk Jang", "Eunho Yang"], "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "comment": "preprint", "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.", "AI": {"tldr": "\u63d0\u51faMASA\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5bf9\u9f50\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u8bc1\u660e\u5143\u8ba4\u77e5\u5bf9\u9f50\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5373\u6a21\u578b\u4e0d\u77e5\u9053\u5982\u4f55\u601d\u8003\u81ea\u5df1\u3002\u7814\u7a76\u53d1\u73b0\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5143\u9884\u6d4b\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u4e0d\u5bf9\u9f50\uff0c\u5bf9\u9f50\u8fd9\u79cd\u5143\u8ba4\u77e5\u80fd\u529b\u53ef\u4ee5\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "method": "\u8bbe\u8ba1MASA\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u4fe1\u53f7\u8bad\u7ec3\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3002\u901a\u8fc7\u8fc7\u6ee4\u96f6\u65b9\u5dee\u63d0\u793a\u548c\u622a\u65ad\u4e0d\u53ef\u80fd\u6b63\u786e\u7684\u957f\u63a8\u7406\u5e8f\u5217\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u8bad\u7ec3\u6548\u7387\uff08GRPO\u8bad\u7ec3\u52a0\u901f1.28\u500d\uff09\uff0c\u5728AIME25\u4e0a\u51c6\u786e\u7387\u63d0\u534719.3%\uff0c\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u5e73\u5747\u63d0\u53476.2%\u3002\u5728\u9886\u57df\u5916\u6cdb\u5316\u65b9\u9762\uff0cGPQA-Diamond\u63d0\u53473.87%\uff0c13\u4e2a\u57fa\u51c6\u5e73\u5747\u63d0\u53472.08%\u3002", "conclusion": "\u5143\u8ba4\u77e5\u5bf9\u9f50\u662f\u63d0\u5347\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\uff0cMASA\u65b9\u6cd5\u901a\u8fc7\u81ea\u5bf9\u9f50\u6709\u6548\u589e\u5f3a\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5728\u51c6\u786e\u7387\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03260", "abs": "https://arxiv.org/abs/2510.03260", "authors": ["Juan Jose Herrera-Aranda", "Guillermo Gomez-Trenado", "Francisco Herrera", "Isaac Triguero"], "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "comment": "26 pages, 9 figures, code available at\n  https://kiedie.github.io/Semantic-Inductive-Attribute-Selection-for-Zero-Shot-Learning/", "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial\nIntelligence Systems, particularly in those that operate in open-world\nscenarios where systems must adapt to new tasks dynamically. Semantic spaces\nplay a pivotal role as they bridge seen and unseen classes, but whether\nhuman-annotated or generated by a machine learning model, they often contain\nnoisy, redundant, or irrelevant attributes that hinder performance. To address\nthis, we introduce a partitioning scheme that simulates unseen conditions in an\ninductive setting (which is the most challenging), allowing attribute relevance\nto be assessed without access to semantic information from unseen classes.\nWithin this framework, we study two complementary feature-selection strategies\nand assess their generalisation. The first adapts embedded feature selection to\nthe particular demands of ZSL, turning model-driven rankings into meaningful\nsemantic pruning; the second leverages evolutionary computation to directly\nexplore the space of attribute subsets more broadly. Experiments on five\nbenchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods\nconsistently improve accuracy on unseen classes by reducing redundancy, but in\ncomplementary ways: RFS is efficient and competitive though dependent on\ncritical hyperparameters, whereas GA is more costly yet explores the search\nspace more broadly and avoids such dependence. These results confirm that\nsemantic spaces are inherently redundant and highlight the proposed\npartitioning scheme as an effective tool to refine them under inductive\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f52\u7eb3\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u901a\u8fc7\u5c5e\u6027\u9009\u62e9\u51cf\u5c11\u8bed\u4e49\u7a7a\u95f4\u5197\u4f59\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7c7b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\u901a\u5e38\u5305\u542b\u566a\u58f0\u3001\u5197\u4f59\u6216\u4e0d\u76f8\u5173\u5c5e\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u4f1a\u963b\u788d\u6a21\u578b\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u5f52\u7eb3\u5f0f\u8bbe\u7f6e\u4e2d\uff0c\u65e0\u6cd5\u83b7\u53d6\u672a\u89c1\u7c7b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4f7f\u5f97\u5c5e\u6027\u76f8\u5173\u6027\u8bc4\u4f30\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "1) \u63d0\u51fa\u5206\u533a\u65b9\u6848\u6a21\u62df\u672a\u89c1\u6761\u4ef6\uff0c\u5728\u65e0\u6cd5\u8bbf\u95ee\u672a\u89c1\u7c7b\u8bed\u4e49\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5c5e\u6027\u76f8\u5173\u6027\uff1b2) \u7814\u7a76\u4e24\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff1a\u57fa\u4e8e\u5d4c\u5165\u7684\u7279\u5f81\u9009\u62e9(RFS)\u548c\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u9057\u4f20\u7b97\u6cd5(GA)\u3002", "result": "\u5728AWA2\u3001CUB\u3001SUN\u3001aPY\u3001FLO\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u6301\u7eed\u63d0\u9ad8\u672a\u89c1\u7c7b\u7684\u51c6\u786e\u7387\u3002RFS\u9ad8\u6548\u4e14\u5177\u6709\u7ade\u4e89\u529b\u4f46\u4f9d\u8d56\u5173\u952e\u8d85\u53c2\u6570\uff0cGA\u6210\u672c\u66f4\u9ad8\u4f46\u80fd\u66f4\u5e7f\u6cdb\u5730\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u4e14\u907f\u514d\u8fd9\u79cd\u4f9d\u8d56\u6027\u3002", "conclusion": "\u8bed\u4e49\u7a7a\u95f4\u672c\u8d28\u4e0a\u662f\u5197\u4f59\u7684\uff0c\u63d0\u51fa\u7684\u5206\u533a\u65b9\u6848\u662f\u5728\u5f52\u7eb3\u6761\u4ef6\u4e0b\u7cbe\u70bc\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e24\u79cd\u7279\u5f81\u9009\u62e9\u7b56\u7565\u4ee5\u4e92\u8865\u7684\u65b9\u5f0f\u6539\u8fdb\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2510.03261", "categories": ["cs.LG", "cs.CE", "J.2; I.2"], "pdf": "https://arxiv.org/pdf/2510.03261", "abs": "https://arxiv.org/abs/2510.03261", "authors": ["C. Coelho", "M. Hohmann", "D. Fern\u00e1ndez", "L. Penter", "S. Ihlenfeldt", "O. Niggemann"], "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark", "comment": null, "summary": "Thermal errors in machine tools significantly impact machining precision and\nproductivity. Traditional thermal error correction/compensation methods rely on\nmeasured temperature-deformation fields or on transfer functions. Most existing\ndata-driven compensation strategies employ neural networks (NNs) to directly\npredict thermal errors or specific compensation values. While effective, these\napproaches are tightly bound to particular error types, spatial locations, or\nmachine configurations, limiting their generality and adaptability. In this\nwork, we introduce a novel paradigm in which NNs are trained to predict\nhigh-fidelity temperature and heat flux fields within the machine tool. The\nproposed framework enables subsequent computation and correction of a wide\nrange of error types using modular, swappable downstream components. The NN is\ntrained using data obtained with the finite element method under varying\ninitial conditions and incorporates a correlation-based selection strategy that\nidentifies the most informative measurement points, minimising hardware\nrequirements during inference. We further benchmark state-of-the-art\ntime-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,\nLong-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal\nConvolutional Network, by training both specialised models, tailored for\nspecific initial conditions, and general models, capable of extrapolating to\nunseen scenarios. The results show accurate and low-cost prediction of\ntemperature and heat flux fields, laying the basis for enabling flexible and\ngeneralisable thermal error correction in machine tool environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u673a\u5e8a\u6e29\u5ea6\u573a\u548c\u70ed\u901a\u91cf\u573a\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4e0b\u6e38\u7ec4\u4ef6\u5b9e\u73b0\u591a\u79cd\u70ed\u8bef\u5dee\u7c7b\u578b\u7684\u8ba1\u7b97\u548c\u6821\u6b63\uff0c\u76f8\u6bd4\u4f20\u7edf\u76f4\u63a5\u9884\u6d4b\u70ed\u8bef\u5dee\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u70ed\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u8bef\u5dee\u7c7b\u578b\u3001\u7a7a\u95f4\u4f4d\u7f6e\u6216\u673a\u5e8a\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u70ed\u8bef\u5dee\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\u83b7\u53d6\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u9ad8\u4fdd\u771f\u6e29\u5ea6\u573a\u548c\u70ed\u901a\u91cf\u573a\uff0c\u91c7\u7528\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u6d4b\u91cf\u70b9\u9009\u62e9\u7b56\u7565\u51cf\u5c11\u786c\u4ef6\u9700\u6c42\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08RNN\u3001GRU\u3001LSTM\u3001\u53cc\u5411LSTM\u3001Transformer\u3001TCN\uff09\u3002", "result": "\u80fd\u591f\u51c6\u786e\u4e14\u4f4e\u6210\u672c\u5730\u9884\u6d4b\u6e29\u5ea6\u573a\u548c\u70ed\u901a\u91cf\u573a\uff0c\u4e3a\u673a\u5e8a\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u901a\u7528\u70ed\u8bef\u5dee\u6821\u6b63\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u8303\u5f0f\u901a\u8fc7\u9884\u6d4b\u6e29\u5ea6\u573a\u548c\u70ed\u901a\u91cf\u573a\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u70ed\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u901a\u7528\u7684\u70ed\u8bef\u5dee\u6821\u6b63\uff0c\u4e3a\u673a\u5e8a\u70ed\u8bef\u5dee\u8865\u507f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03262", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03262", "abs": "https://arxiv.org/abs/2510.03262", "authors": ["Andi Zhang", "Xuan Ding", "Haofan Wang", "Steven McDonagh", "Samuel Kaski"], "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout", "comment": null, "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict\northogonality when combining sparse semantic vectors without extra time\ncomplexity. LoRA, a popular fine-tuning method for large models, typically\ntrains a module to represent a specific concept such as an object or a style.\nWhen multiple LoRAs are merged, for example to generate an object in a\nparticular style, their semantic vectors may interfere with each other. Our\nmethod guarantees, at the theoretical and runtime levels, that merged LoRAs\nremain orthogonal and thus free from direct interference. However, empirical\nanalysis reveals that such orthogonality does not lead to the semantic\ndisentanglement or compositionality highlighted in prior work on compositional\nadaptation. This finding suggests that inter-LoRA orthogonality alone may be\ninsufficient for achieving true semantic compositionality, prompting a\nre-examination of its role in adapter merging.", "AI": {"tldr": "\u63d0\u51fa\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\uff0c\u5728\u5408\u5e76LoRA\u6a21\u5757\u65f6\u5f3a\u5236\u6267\u884c\u6b63\u4ea4\u6027\u4ee5\u907f\u514d\u8bed\u4e49\u5411\u91cf\u5e72\u6270\uff0c\u4f46\u5b9e\u8bc1\u53d1\u73b0\u6b63\u4ea4\u6027\u672c\u8eab\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u8bed\u4e49\u89e3\u7f20\u548c\u7ec4\u5408\u6027\u3002", "motivation": "LoRA\u5fae\u8c03\u65b9\u6cd5\u5728\u5408\u5e76\u591a\u4e2a\u6a21\u5757\u65f6\uff0c\u8bed\u4e49\u5411\u91cf\u4f1a\u76f8\u4e92\u5e72\u6270\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\uff0c\u5728\u5408\u5e76\u7a00\u758f\u8bed\u4e49\u5411\u91cf\u65f6\u5f3a\u5236\u6267\u884c\u4e25\u683c\u6b63\u4ea4\u6027\uff0c\u4e0d\u589e\u52a0\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bba\u4e0a\u548c\u8fd0\u884c\u65f6\u90fd\u80fd\u4fdd\u8bc1\u5408\u5e76\u7684LoRA\u4fdd\u6301\u6b63\u4ea4\u4e14\u65e0\u76f4\u63a5\u5e72\u6270\uff0c\u4f46\u5b9e\u8bc1\u5206\u6790\u663e\u793a\u6b63\u4ea4\u6027\u5e76\u4e0d\u80fd\u5b9e\u73b0\u8bed\u4e49\u89e3\u7f20\u548c\u7ec4\u5408\u6027\u3002", "conclusion": "\u4ec5\u9760LoRA\u95f4\u7684\u6b63\u4ea4\u6027\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u4e49\u7ec4\u5408\u6027\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5176\u5728\u9002\u914d\u5668\u5408\u5e76\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.03263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03263", "abs": "https://arxiv.org/abs/2510.03263", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "comment": null, "summary": "The impressive capability of modern text-to-image models to generate\nrealistic visuals has come with a serious drawback: they can be misused to\ncreate harmful, deceptive or unlawful content. This has accelerated the push\nfor machine unlearning. This new field seeks to selectively remove specific\nknowledge from a model's training data without causing a drop in its overall\nperformance. However, it turns out that actually forgetting a given concept is\nan extremely difficult task. Models exposed to attacks using adversarial\nprompts show the ability to generate so-called unlearned concepts, which can be\nnot only harmful but also illegal. In this paper, we present considerations\nregarding the ability of models to forget and recall knowledge, introducing the\nMemory Self-Regeneration task. Furthermore, we present MemoRa strategy, which\nwe consider to be a regenerative approach supporting the effective recovery of\npreviously lost knowledge. Moreover, we propose that robustness in knowledge\nretrieval is a crucial yet underexplored evaluation measure for developing more\nrobust and effective unlearning techniques. Finally, we demonstrate that\nforgetting occurs in two distinct ways: short-term, where concepts can be\nquickly recalled, and long-term, where recovery is more challenging.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\u548cMemoRa\u7b56\u7565\uff0c\u5f3a\u8c03\u77e5\u8bc6\u68c0\u7d22\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u533a\u5206\u77ed\u671f\u548c\u957f\u671f\u9057\u5fd8\u673a\u5236\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u903c\u771f\u56fe\u50cf\u7684\u80fd\u529b\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u521b\u5efa\u6709\u5bb3\u5185\u5bb9\uff0c\u8fd9\u63a8\u52a8\u4e86\u673a\u5668\u9057\u5fd8\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u4f46\u5b9e\u9645\u9057\u5fd8\u7279\u5b9a\u6982\u5ff5\u975e\u5e38\u56f0\u96be\u3002", "method": "\u5f15\u5165\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\uff0c\u63d0\u51faMemoRa\u7b56\u7565\u4f5c\u4e3a\u652f\u6301\u6709\u6548\u6062\u590d\u5df2\u4e22\u5931\u77e5\u8bc6\u7684\u518d\u751f\u65b9\u6cd5\uff0c\u5f3a\u8c03\u77e5\u8bc6\u68c0\u7d22\u9c81\u68d2\u6027\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7814\u7a76\u8868\u660e\u9057\u5fd8\u4ee5\u4e24\u79cd\u4e0d\u540c\u65b9\u5f0f\u53d1\u751f\uff1a\u77ed\u671f\u9057\u5fd8\uff08\u6982\u5ff5\u53ef\u5feb\u901f\u56de\u5fc6\uff09\u548c\u957f\u671f\u9057\u5fd8\uff08\u6062\u590d\u66f4\u5177\u6311\u6218\u6027\uff09\u3002", "conclusion": "\u77e5\u8bc6\u68c0\u7d22\u9c81\u68d2\u6027\u662f\u5f00\u53d1\u66f4\u5f3a\u5927\u6709\u6548\u9057\u5fd8\u6280\u672f\u7684\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u9057\u5fd8\u673a\u5236\u5b58\u5728\u77ed\u671f\u548c\u957f\u671f\u7684\u533a\u522b\u3002"}}
{"id": "2510.03264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u63a8\u7406\u6570\u636e\u5728LLM\u8bad\u7ec3\u4e0d\u540c\u9636\u6bb5\uff08\u9884\u8bad\u7ec3vs\u540e\u8bad\u7ec3\uff09\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u5efa\u7acb\u4e0d\u53ef\u66ff\u4ee3\u7684\u57fa\u7840\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u524d\u6cbf\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\uff0c\u63a8\u7406\u6570\u636e\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7814\u7a76\u65e9\u671f\u5f15\u5165\u63a8\u7406\u6570\u636e\u662f\u5426\u6bd4\u540e\u671f\u5fae\u8c03\u66f4\u6709\u6548\uff0c\u4ee5\u53ca\u662f\u5426\u5b58\u5728\u8fc7\u62df\u5408\u98ce\u9669\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u6570\u636e\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u7684\u53d8\u5316\u5982\u4f55\u5f71\u54cdLLM\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u5206\u914d\u7b56\u7565\u3002", "result": "\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u5e26\u676519%\u5e73\u5747\u589e\u76ca\uff0c\u5efa\u7acb\u7684\u57fa\u7840\u80fd\u529b\u65e0\u6cd5\u901a\u8fc7\u540e\u671fSFT\u5b8c\u5168\u590d\u5236\u3002\u9884\u8bad\u7ec3\u6700\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u591a\u6837\u6027\uff0811%\u589e\u76ca\uff09\uff0c\u800cSFT\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u654f\u611f\uff0815%\u589e\u76ca\uff09\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u8bed\u8a00\u5efa\u6a21\u4e0e\u63a8\u7406\u7684\u4f20\u7edf\u5206\u79bb\uff0c\u4e3a\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u6218\u7565\u6027\u5730\u5206\u914d\u6570\u636e\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u3002"}}
{"id": "2510.03265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03265", "abs": "https://arxiv.org/abs/2510.03265", "authors": ["Bowei Tian", "Yexiao He", "Wanghao Ye", "Ziyao Wang", "Meng Liu", "Ang Li"], "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "comment": null, "summary": "Large-scale foundation models demonstrate strong performance across language,\nvision, and reasoning tasks. However, how they internally structure and\nstabilize concepts remains elusive. Inspired by causal inference, we introduce\nthe MindCraft framework built upon Concept Trees. By applying spectral\ndecomposition at each layer and linking principal directions into branching\nConcept Paths, Concept Trees reconstruct the hierarchical emergence of\nconcepts, revealing exactly when they diverge from shared representations into\nlinearly separable subspaces. Empirical evaluations across diverse scenarios\nacross disciplines, including medical diagnosis, physics reasoning, and\npolitical decision-making, show that Concept Trees recover semantic\nhierarchies, disentangle latent concepts, and can be widely applied across\nmultiple domains. The Concept Tree establishes a widely applicable and powerful\nframework that enables in-depth analysis of conceptual representations in deep\nmodels, marking a significant step forward in the foundation of interpretable\nAI.", "AI": {"tldr": "MindCraft\u6846\u67b6\u901a\u8fc7\u6982\u5ff5\u6811\u548c\u8c31\u5206\u89e3\u6280\u672f\uff0c\u63ed\u793a\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u6982\u5ff5\u7684\u5206\u5c42\u6d8c\u73b0\u8fc7\u7a0b\uff0c\u80fd\u591f\u8ffd\u8e2a\u6982\u5ff5\u4ece\u5171\u4eab\u8868\u793a\u5230\u7ebf\u6027\u53ef\u5206\u5b50\u7a7a\u95f4\u7684\u6f14\u5316\u8def\u5f84\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5982\u4f55\u7ec4\u7ec7\u548c\u7a33\u5b9a\u6982\u5ff5\u7ed3\u6784\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u6784\u5efaMindCraft\u6846\u67b6\uff0c\u4f7f\u7528\u6982\u5ff5\u6811\u7ed3\u6784\uff0c\u901a\u8fc7\u8c31\u5206\u89e3\u6280\u672f\u5728\u5404\u5c42\u5206\u6790\u4e3b\u65b9\u5411\uff0c\u5e76\u5c06\u5b83\u4eec\u8fde\u63a5\u6210\u5206\u652f\u7684\u6982\u5ff5\u8def\u5f84\u3002", "result": "\u5728\u533b\u7597\u8bca\u65ad\u3001\u7269\u7406\u63a8\u7406\u548c\u653f\u6cbb\u51b3\u7b56\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u6982\u5ff5\u6811\u80fd\u591f\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3001\u89e3\u8026\u6f5c\u5728\u6982\u5ff5\uff0c\u5e76\u5177\u6709\u8de8\u9886\u57df\u9002\u7528\u6027\u3002", "conclusion": "\u6982\u5ff5\u6811\u5efa\u7acb\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u9002\u7528\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u8868\u793a\uff0c\u662f\u53ef\u89e3\u91caAI\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.03266", "categories": ["cs.LG", "stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.03266", "abs": "https://arxiv.org/abs/2510.03266", "authors": ["Bharat Sharma", "Jitendra Kumar"], "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "comment": null, "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics,\nnecessitating robust methods for detecting and analyzing anomalous behavior in\nplant productivity. This study presents a novel application of variational\nautoencoders (VAE) for identifying extreme events in gross primary productivity\n(GPP) from Community Earth System Model version 2 simulations across four AR6\nregions in the Continental United States. We compare VAE-based anomaly\ndetection with traditional singular spectral analysis (SSA) methods across\nthree time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.\nThe VAE architecture employs three dense layers and a latent space with an\ninput sequence length of 12 months, trained on a normalized GPP time series to\nreconstruct the GPP and identifying anomalies based on reconstruction errors.\nExtreme events are defined using 5th percentile thresholds applied to both VAE\nand SSA anomalies. Results demonstrate strong regional agreement between VAE\nand SSA methods in spatial patterns of extreme event frequencies, despite VAE\nproducing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA\nacross regions and periods). Both methods reveal increasing magnitudes and\nfrequencies of negative carbon cycle extremes toward 2050-80, particularly in\nWestern and Central North America. The VAE approach shows comparable\nperformance to established SSA techniques, while offering computational\nadvantages and enhanced capability for capturing non-linear temporal\ndependencies in carbon cycle variability. Unlike SSA, the VAE method does not\nrequire one to define the periodicity of the signals in the data; it discovers\nthem from the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f8e\u56fd\u5927\u9646\u56db\u4e2aAR6\u533a\u57df\u690d\u7269\u603b\u521d\u7ea7\u751f\u4ea7\u529b(GPP)\u4e2d\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4e0e\u4f20\u7edf\u5947\u5f02\u8c31\u5206\u6790(SSA)\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u6c14\u5019\u5f02\u5e38\u663e\u8457\u5f71\u54cd\u9646\u5730\u78b3\u5faa\u73af\u52a8\u6001\uff0c\u9700\u8981\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u690d\u7269\u751f\u4ea7\u529b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5bc6\u96c6\u5c42\u548c\u6f5c\u5728\u7a7a\u95f4\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a12\u4e2a\u6708\uff0c\u8bad\u7ec3\u5728\u5f52\u4e00\u5316GPP\u65f6\u95f4\u5e8f\u5217\u4e0a\u91cd\u5efaGPP\uff0c\u5e76\u6839\u636e\u91cd\u5efa\u8bef\u5dee\u8bc6\u522b\u5f02\u5e38\u3002\u6781\u7aef\u4e8b\u4ef6\u4f7f\u7528\u7b2c5\u767e\u5206\u4f4d\u9608\u503c\u5b9a\u4e49\u3002", "result": "VAE\u548cSSA\u65b9\u6cd5\u5728\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f3a\u70c8\u533a\u57df\u4e00\u81f4\u6027\uff0c\u4f46VAE\u4ea7\u751f\u66f4\u9ad8\u7684\u9608\u503c\u503c(179-756 GgC vs. 100-784 GgC)\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u793a\u52302050-80\u5e74\uff0c\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u7684\u5e45\u5ea6\u548c\u9891\u7387\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u897f\u90e8\u548c\u4e2d\u90e8\u5317\u7f8e\u5730\u533a\u3002", "conclusion": "VAE\u65b9\u6cd5\u4e0e\u5df2\u5efa\u7acb\u7684SSA\u6280\u672f\u5177\u6709\u53ef\u6bd4\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\u5e76\u589e\u5f3a\u4e86\u5bf9\u78b3\u5faa\u73af\u53d8\u5f02\u6027\u4e2d\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6355\u6349\u80fd\u529b\u3002\u4e0eSSA\u4e0d\u540c\uff0cVAE\u65b9\u6cd5\u4e0d\u9700\u8981\u5b9a\u4e49\u6570\u636e\u4e2d\u4fe1\u53f7\u7684\u5468\u671f\u6027\uff0c\u800c\u662f\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u5b83\u4eec\u3002"}}
{"id": "2510.03267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM.", "AI": {"tldr": "PT\u00b2-LLM\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e09\u503c\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4e09\u503c\u91cf\u5316\u5668\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\u963b\u788d\u90e8\u7f72\u3002\u4e09\u503c\u5316\u4f5c\u4e3a\u6709\u524d\u666f\u7684\u538b\u7f29\u6280\u672f\uff0c\u5728\u540e\u8bad\u7ec3\u91cf\u5316\u573a\u666f\u4e0b\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\uff0c\u4e3b\u8981\u9762\u4e34\u8bad\u7ec3\u81ea\u7531\u53c2\u6570\u4f18\u5316\u56f0\u96be\u4ee5\u53ca\u5f02\u5e38\u503c\u548c\u6743\u91cd\u5206\u6563\u5e26\u6765\u7684\u91cf\u5316\u6311\u6218\u3002", "method": "\u63d0\u51faPT\u00b2-LLM\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1\uff09\u8fed\u4ee3\u4e09\u503c\u62df\u5408\uff08ITF\uff09\uff0c\u4ea4\u66ff\u8fdb\u884c\u6700\u4f18\u4e09\u503c\u7f51\u683c\u6784\u5efa\u548c\u7075\u6d3b\u820d\u5165\u4ee5\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\uff1b2\uff09\u6fc0\u6d3b\u611f\u77e5\u7f51\u683c\u5bf9\u9f50\uff08AGA\uff09\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e09\u503c\u7f51\u683c\u4ee5\u66f4\u597d\u5339\u914d\u5168\u7cbe\u5ea6\u8f93\u51fa\uff1b3\uff09\u57fa\u4e8e\u7ed3\u6784\u76f8\u4f3c\u6027\u7684\u91cd\u6392\u5e8f\uff08SSR\uff09\u7b56\u7565\uff0c\u5229\u7528\u5217\u95f4\u7ed3\u6784\u76f8\u4f3c\u6027\u7f13\u89e3\u91cf\u5316\u96be\u5ea6\u548c\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPT\u00b2-LLM\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb2\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5b58\u6210\u672c\uff0c\u5e76\u80fd\u52a0\u901f\u9884\u586b\u5145\u548c\u89e3\u7801\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "PT\u00b2-LLM\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u4e09\u503c\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u3001\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2510.03268", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03268", "abs": "https://arxiv.org/abs/2510.03268", "authors": ["Lingjie Yi", "Raphael Douady", "Chao Chen"], "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "comment": null, "summary": "Multimodal contrastive learning (MCL) aims to embed data from different\nmodalities in a shared embedding space. However, empirical evidence shows that\nrepresentations from different modalities occupy completely separate regions of\nembedding space, a phenomenon referred to as the modality gap. Moreover,\nexperimental findings on how the size of the modality gap influences downstream\nperformance are inconsistent. These observations raise two key questions: (1)\nWhat causes the modality gap? (2) How does it affect downstream tasks? To\naddress these questions, this paper introduces the first theoretical framework\nfor analyzing the convergent optimal representations of MCL and the modality\nalignment when training is optimized. Specifically, we prove that without any\nconstraint or under the cone constraint, the modality gap converges to zero.\nUnder the subspace constraint (i.e., representations of two modalities fall\ninto two distinct hyperplanes due to dimension collapse), the modality gap\nconverges to the smallest angle between the two hyperplanes. This result\nidentifies \\emph{dimension collapse} as the fundamental origin of the modality\ngap. Furthermore, our theorems demonstrate that paired samples cannot be\nperfectly aligned under the subspace constraint. The modality gap influences\ndownstream performance by affecting the alignment between sample pairs. We\nprove that, in this case, perfect alignment between two modalities can still be\nachieved via two ways: hyperplane rotation and shared space projection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u6790\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6536\u655b\u6700\u4f18\u8868\u793a\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u7ef4\u5ea6\u574d\u7f29\u662f\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u6d88\u9664\u6a21\u6001\u95f4\u9699\u7684\u4e24\u79cd\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u4e0d\u540c\u6a21\u6001\u7684\u8868\u793a\u5360\u636e\u5d4c\u5165\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\uff08\u6a21\u6001\u95f4\u9699\uff09\uff0c\u4f46\u5173\u4e8e\u6a21\u6001\u95f4\u9699\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u5176\u6210\u56e0\u548c\u5f71\u54cd\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u5206\u6790\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u7684\u6700\u4f18\u8868\u793a\uff0c\u5728\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u65e0\u7ea6\u675f\u3001\u9525\u7ea6\u675f\u3001\u5b50\u7a7a\u95f4\u7ea6\u675f\uff09\u5206\u6790\u6a21\u6001\u95f4\u9699\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "\u8bc1\u660e\u5728\u65e0\u7ea6\u675f\u6216\u9525\u7ea6\u675f\u4e0b\u6a21\u6001\u95f4\u9699\u6536\u655b\u5230\u96f6\uff1b\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\u6a21\u6001\u95f4\u9699\u6536\u655b\u5230\u4e24\u4e2a\u8d85\u5e73\u9762\u95f4\u7684\u6700\u5c0f\u89d2\u5ea6\uff0c\u7ef4\u5ea6\u574d\u7f29\u662f\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u6a21\u6001\u95f4\u9699\u901a\u8fc7\u5f71\u54cd\u6837\u672c\u5bf9\u7684\u5bf9\u9f50\u6765\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u8d85\u5e73\u9762\u65cb\u8f6c\u548c\u5171\u4eab\u7a7a\u95f4\u6295\u5f71\u4e24\u79cd\u65b9\u6cd5\u5b9e\u73b0\u5b8c\u7f8e\u5bf9\u9f50\u3002"}}
{"id": "2510.03269", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03269", "abs": "https://arxiv.org/abs/2510.03269", "authors": ["Wendi Li", "Changdae Oh", "Yixuan Li"], "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "comment": null, "summary": "Optimistic exploration is central to improving sample efficiency in\nreinforcement learning with human feedback, yet existing exploratory bonus\nmethods to incentivize exploration often fail to realize optimism. We provide a\ntheoretical analysis showing that current formulations, under KL or\n$\\alpha$-divergence regularization, unintentionally bias exploration toward\nhigh-probability regions of the reference model, thereby reinforcing\nconservative behavior instead of promoting discovery of uncertain regions. To\naddress this pitfall, we introduce the General Exploratory Bonus (GEB), a novel\ntheoretical framework that provably satisfies the optimism principle. GEB\ncounteracts divergence-induced bias via reference-dependent reward regulation\nand unifies prior heuristic bonuses as special cases, while extending naturally\nacross the full $\\alpha$-divergence family. Empirically, GEB consistently\noutperforms baselines on alignment tasks across multiple divergence settings\nand large language model backbones. These results demonstrate that GEB offers\nboth a principled and practical solution for optimistic exploration in RLHF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u901a\u7528\u63a2\u7d22\u5956\u52b1(GEB)\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709KL\u548c\u03b1-\u6563\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u4e2d\u504f\u5411\u4fdd\u5b88\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u8003\u4f9d\u8d56\u7684\u5956\u52b1\u8c03\u8282\u5b9e\u73b0\u4e50\u89c2\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u63a2\u7d22\u5956\u52b1\u65b9\u6cd5\u5728KL\u6216\u03b1-\u6563\u5ea6\u6b63\u5219\u5316\u4e0b\uff0c\u65e0\u610f\u4e2d\u5c06\u63a2\u7d22\u504f\u5411\u53c2\u8003\u6a21\u578b\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u5f3a\u5316\u4e86\u4fdd\u5b88\u884c\u4e3a\u800c\u975e\u4fc3\u8fdb\u4e0d\u786e\u5b9a\u533a\u57df\u7684\u53d1\u73b0\u3002", "method": "\u5f15\u5165\u901a\u7528\u63a2\u7d22\u5956\u52b1(GEB)\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u4f9d\u8d56\u7684\u5956\u52b1\u8c03\u8282\u6765\u62b5\u6d88\u6563\u5ea6\u8bf1\u5bfc\u7684\u504f\u5dee\uff0c\u7edf\u4e00\u4e86\u5148\u524d\u7684\u542f\u53d1\u5f0f\u5956\u52b1\u4f5c\u4e3a\u7279\u4f8b\uff0c\u5e76\u81ea\u7136\u6269\u5c55\u5230\u5b8c\u6574\u7684\u03b1-\u6563\u5ea6\u65cf\u3002", "result": "\u5728\u591a\u4e2a\u6563\u5ea6\u8bbe\u7f6e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cGEB\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GEB\u4e3aRLHF\u4e2d\u7684\u4e50\u89c2\u63a2\u7d22\u63d0\u4f9b\u4e86\u65e2\u6709\u7406\u8bba\u539f\u5219\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03270", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03270", "abs": "https://arxiv.org/abs/2510.03270", "authors": ["Haolin Chen", "Shiyu Wang", "Can Qin", "Bo Pang", "Zuxin Liu", "Jielin Qiu", "Jianguo Zhang", "Yingbo Zhou", "Zeyuan Chen", "Ran Xu", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "CoDA: Coding LM via Diffusion Adaptation", "comment": null, "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a1.7B\u53c2\u6570\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6269\u6563\u9884\u8bad\u7ec3\u3001\u4ee3\u7801\u4e2d\u5fc3\u7684\u4e2d\u671f\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u5ef6\u8fdf\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5728\u591a\u4e2a\u4ee3\u7801\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u53cc\u5411\u4e0a\u4e0b\u6587\u548c\u586b\u5145\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u8fc7\u4e8e\u7b28\u91cd\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4e14\u5b9e\u7528\u7684\u6269\u6563\u7f16\u7801\u5668\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u6269\u6563\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u4ee3\u7801\u4e2d\u5fc3\u7684\u4e2d\u671f\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u91c7\u6837\u65b9\u6cd5\u6765\u4fdd\u6301\u63a8\u7406\u5ef6\u8fdf\u7ade\u4e89\u529b\u3002", "result": "\u5728Humaneval\u3001MBPP\u548cEvalPlus\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoDA-1.7B-Instruct\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u53c2\u6570\u89c4\u6a21\u8fbe7B\u7684\u6269\u6563\u6a21\u578b\u3002", "conclusion": "CoDA\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u6269\u6563\u7f16\u7801\u5668\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u8bad\u7ec3\u6d41\u6c34\u7ebf\u548c\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u4ee5\u52a0\u901f\u57fa\u4e8e\u6269\u6563\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u52a9\u624b\u7814\u7a76\u3002"}}
{"id": "2510.03271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u51b3\u7b56\u52bf\u80fd\u9762\uff08DPS\uff09\u6982\u5ff5\u6765\u8fd1\u4f3c\u6784\u5efa\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u901a\u8fc7\u6709\u9650\u6b21\u5e8f\u5217\u91c7\u6837\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u8bef\u5dee\u5206\u6790\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de8\u5927\u8bcd\u6c47\u5e8f\u5217\u89c4\u6a21\u548c\u81ea\u56de\u5f52\u7279\u6027\uff0c\u76f4\u63a5\u6784\u5efa\u5176\u51b3\u7b56\u8fb9\u754c\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u52bf\u80fd\u9762\uff08DPS\uff09\u6982\u5ff5\uff0c\u5b9a\u4e49\u5728\u533a\u5206\u4e0d\u540c\u91c7\u6837\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u4e0a\uff0c\u5e76\u5f00\u53d1K-DPS\u7b97\u6cd5\u901a\u8fc7K\u6b21\u6709\u9650\u91c7\u6837\u6765\u8fd1\u4f3c\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e86K-DPS\u4e0e\u7406\u60f3DPS\u4e4b\u95f4\u7684\u7edd\u5bf9\u8bef\u5dee\u3001\u671f\u671b\u8bef\u5dee\u548c\u8bef\u5dee\u96c6\u4e2d\u5ea6\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u8bef\u5dee\u53ef\u901a\u8fc7\u91c7\u6837\u6b21\u6570\u8fdb\u884c\u6743\u8861\u3002", "conclusion": "DPS\u4e3a\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4e3b\u6d41LLM\u51b3\u7b56\u8fb9\u754c\u7684\u6784\u9020\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.03272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03272", "abs": "https://arxiv.org/abs/2510.03272", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "comment": null, "summary": "The Transformer architecture has revolutionized artificial intelligence, yet\na principled theoretical understanding of its internal mechanisms remains\nelusive. This paper introduces a novel analytical framework that\nreconceptualizes the Transformer's discrete, layered structure as a continuous\nspatiotemporal dynamical system governed by a master Partial Differential\nEquation (PDE). Within this paradigm, we map core architectural components to\ndistinct mathematical operators: self-attention as a non-local interaction, the\nfeed-forward network as a local reaction, and, critically, residual connections\nand layer normalization as indispensable stabilization mechanisms. We do not\npropose a new model, but rather employ the PDE system as a theoretical probe to\nanalyze the mathematical necessity of these components. By comparing a standard\nTransformer with a PDE simulator that lacks explicit stabilizers, our\nexperiments provide compelling empirical evidence for our central thesis. We\ndemonstrate that without residual connections, the system suffers from\ncatastrophic representational drift, while the absence of layer normalization\nleads to unstable, explosive training dynamics. Our findings reveal that these\nseemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers\nrequired to tame an otherwise powerful but inherently unstable continuous\nsystem. This work offers a first-principles explanation for the Transformer's\ndesign and establishes a new paradigm for analyzing deep neural networks\nthrough the lens of continuous dynamics.", "AI": {"tldr": "\u5c06Transformer\u67b6\u6784\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u7531\u4e3b\u504f\u5fae\u5206\u65b9\u7a0b(PDE)\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\uff0c\u63ed\u793a\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u662f\u7a33\u5b9a\u8be5\u7cfb\u7edf\u7684\u5fc5\u8981\u6570\u5b66\u7a33\u5b9a\u5668\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u6210\u529f\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u673a\u5236\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u6570\u5b66\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5c06Transformer\u7684\u79bb\u6563\u5206\u5c42\u7ed3\u6784\u6620\u5c04\u4e3a\u8fde\u7eed\u52a8\u529b\u7cfb\u7edf\uff1a\u81ea\u6ce8\u610f\u529b\u5bf9\u5e94\u975e\u5c40\u90e8\u4ea4\u4e92\uff0c\u524d\u9988\u7f51\u7edc\u5bf9\u5e94\u5c40\u90e8\u53cd\u5e94\uff0c\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u5bf9\u5e94\u7a33\u5b9a\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6ca1\u6709\u6b8b\u5dee\u8fde\u63a5\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u8868\u793a\u6f02\u79fb\uff0c\u6ca1\u6709\u5c42\u5f52\u4e00\u5316\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u52a8\u6001\u4e0d\u7a33\u5b9a\u548c\u7206\u70b8\u6027\u589e\u957f\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u8fd9\u4e9b\u770b\u4f3c\u542f\u53d1\u5f0f\u7684\"\u6280\u5de7\"\u5b9e\u9645\u4e0a\u662f\u7a33\u5b9aTransformer\u8fd9\u4e00\u5f3a\u5927\u4f46\u5185\u5728\u4e0d\u7a33\u5b9a\u8fde\u7eed\u7cfb\u7edf\u7684\u5fc5\u8981\u6570\u5b66\u7a33\u5b9a\u5668\u3002"}}
{"id": "2510.03273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03273", "abs": "https://arxiv.org/abs/2510.03273", "authors": ["Chenhao Ye", "Ming Tang"], "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "comment": null, "summary": "Backpropagation (BP), while foundational to deep learning, imposes two\ncritical scalability bottlenecks: update locking, where network modules remain\nidle until the entire backward pass completes, and high memory consumption due\nto storing activations for gradient computation. To address these limitations,\nwe introduce Synergistic Information Distillation (SID), a novel training\nframework that reframes deep learning as a cascade of local cooperative\nrefinement problems. In SID, a deep network is structured as a pipeline of\nmodules, each imposed with a local objective to refine a probabilistic belief\nabout the ground-truth target. This objective balances fidelity to the target\nwith consistency to the belief from its preceding module. By decoupling the\nbackward dependencies between modules, SID enables parallel training and hence\neliminates update locking and drastically reduces memory requirements.\nMeanwhile, this design preserves the standard feed-forward inference pass,\nmaking SID a versatile drop-in replacement for BP. We provide a theoretical\nfoundation, proving that SID guarantees monotonic performance improvement with\nnetwork depth. Empirically, SID consistently matches or surpasses the\nclassification accuracy of BP, exhibiting superior scalability and pronounced\nrobustness to label noise.Code is available at:\nhttps://github.com/ychAlbert/sid-bp", "AI": {"tldr": "\u63d0\u51fa\u4e86\u534f\u540c\u4fe1\u606f\u84b8\u998f(SID)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6df1\u5ea6\u5b66\u4e60\u91cd\u6784\u4e3a\u5c40\u90e8\u534f\u540c\u7cbe\u70bc\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u66f4\u65b0\u9501\u5b9a\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5e76\u884c\u8bad\u7ec3\u5e76\u4fdd\u6301\u524d\u5411\u63a8\u7406\u4e0d\u53d8\u3002", "motivation": "\u89e3\u51b3\u53cd\u5411\u4f20\u64ad(BP)\u7684\u4e24\u4e2a\u5173\u952e\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff1a\u66f4\u65b0\u9501\u5b9a\uff08\u7f51\u7edc\u6a21\u5757\u9700\u7b49\u5f85\u6574\u4e2a\u53cd\u5411\u4f20\u64ad\u5b8c\u6210\uff09\u548c\u56e0\u5b58\u50a8\u6fc0\u6d3b\u503c\u5bfc\u81f4\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u3002", "method": "\u5c06\u6df1\u5ea6\u7f51\u7edc\u6784\u5efa\u4e3a\u6a21\u5757\u6d41\u6c34\u7ebf\uff0c\u6bcf\u4e2a\u6a21\u5757\u65bd\u52a0\u5c40\u90e8\u76ee\u6807\u6765\u7cbe\u70bc\u5bf9\u771f\u5b9e\u76ee\u6807\u7684\u6982\u7387\u4fe1\u5ff5\uff0c\u5e73\u8861\u76ee\u6807\u4fdd\u771f\u5ea6\u4e0e\u524d\u5e8f\u6a21\u5757\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u89e3\u8026\u6a21\u5757\u95f4\u7684\u53cd\u5411\u4f9d\u8d56\u3002", "result": "\u7406\u8bba\u8bc1\u660eSID\u4fdd\u8bc1\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u5355\u8c03\u63d0\u5347\uff0c\u5b9e\u9a8c\u8868\u660eSID\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u5339\u914d\u6216\u8d85\u8d8aBP\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SID\u4f5c\u4e3aBP\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u6d88\u9664\u4e86\u66f4\u65b0\u9501\u5b9a\uff0c\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u524d\u5411\u63a8\u7406\uff0c\u5728\u51c6\u786e\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.03274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03274", "abs": "https://arxiv.org/abs/2510.03274", "authors": ["Tianao Zhang", "Zhiteng Li", "Xianglong Yan", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs), which offer bidirectional context\nand flexible masked-denoising generation, are emerging as a compelling\nalternative to autoregressive (AR) LLMs. However, like AR LLMs, their model\nsizes continue to grow, motivating weight compression for deployment. Although\npost-training quantization (PTQ) is effective for AR LLMs, directly\ntransferring it to dLLMs at 2-bit leads to unsatisfactory performance. To\ntackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework\ntailored to dLLMs. Since masked-denoising activations in dLLMs differ from the\nfully visible signals assumed by standard PTQ methods, we introduce Masked\nCalibration Simulation (MCS) to align calibration with the timestep-dependent\nmasking, which yields more reliable calibrations. Moreover, we propose a\nData-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight\nrepresentations via an optimization algorithm. It performs iterative\napproximation guided by our simulated calibration data. In addition, under a\nstrict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a\nsensitivity-based precision allocation scheme that adaptively assigns bit width\nacross channel groups. When restricted to 2-bit precision, Quant-dLLM\nconsistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer\nPTQ methods on dLLMs. The code and models will be available at:\nhttps://github.com/ZTA2785/Quant-dLLM.", "AI": {"tldr": "Quant-dLLM\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u8d85\u4f4e\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u6821\u51c6\u6a21\u62df\u3001\u6570\u636e\u611f\u77e5\u4efb\u610f\u987a\u5e8f\u91cf\u5316\u5668\u548c\u81ea\u9002\u5e94\u5206\u5757\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\uff0c\u57282\u4f4d\u7cbe\u5ea6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6a21\u578b\u89c4\u6a21\u6301\u7eed\u589e\u957f\uff0c\u9700\u8981\u6743\u91cd\u538b\u7f29\u90e8\u7f72\u3002\u4f46\u76f4\u63a5\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u7684\u91cf\u5316\u65b9\u6cd5\u5e94\u7528\u5230\u6269\u6563\u6a21\u578b\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u6821\u51c6\u6a21\u62df\u6765\u5bf9\u9f50\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u63a9\u7801\uff0c\u6570\u636e\u611f\u77e5\u4efb\u610f\u987a\u5e8f\u91cf\u5316\u5668\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u5b66\u4e60\u8d85\u4f4e\u4f4d\u6743\u91cd\u8868\u793a\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u5206\u5757\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\u65b9\u6848\u3002", "result": "\u5728\u4e25\u683c\u76842\u4f4d\u9884\u7b97\u4e0b\uff0cQuant-dLLM\u5728\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u59cb\u7ec8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u8fc1\u79fb\u91cf\u5316\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d85\u4f4e\u4f4d\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2510.03275", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03275", "abs": "https://arxiv.org/abs/2510.03275", "authors": ["Junhao Xia", "Ming Zhao", "Limin Xiao", "Xiujun Zhang"], "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "comment": null, "summary": "Large language models (LLMs) face significant computational and memory\nchallenges, making extremely low-bit quantization crucial for their efficient\ndeployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for\n1-bit LLMs of any size, a novel framework that enables extremely low-bit\nquantization of LLMs while preserving their linguistic reasoning capabilities.\nA distinctive feature of SDQ-LLM is the continuous adjustability of the\nOver-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM\nconstraints by selecting fractional OSR (e.g. 2.5 times) for an optimal\ntrade-off between model size and accuracy. SDQ-LLM uses upsampling combined\nwith Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding\nhigh-precision parameters into 1-bit or 1.58-bit representations, replacing the\nmultiplication operations within linear layers with addition. This approach\nsignificantly enhances inference efficiency under extremely low-bit\nquantization. To further reduce the loss of quantization precision, we\nincorporate Hadamard-based weight smoothing prior to quantization, improving\nthe stability and robustness of the weight representations. Furthermore, to\nfully leverage the continuity of the OSR and reduce precision loss, recognizing\nthe correlation between quantization sensitivity and weight variance, we\npropose a fine-grained, layer- and linear-wise OSR allocation strategy,\nMultiOSR. This strategy distributes OSR both across layers and within each\nlayer, based on weight variance and parameter scale. Finally, extensive\nexperiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a\nmore efficient and high-precision performance even under highly aggressive\nlow-OSR settings. Our code is available at\nhttps://github.com/Dreamlittlecat/LLM-Quant-Factory.", "AI": {"tldr": "SDQ-LLM\u662f\u4e00\u79cd\u7528\u4e8e1\u4f4dLLM\u7684Sigma-Delta\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u91c7\u6837\u7387(OSR)\u8fde\u7eed\u53ef\u8c03\u548cHadamard\u6743\u91cd\u5e73\u6ed1\u6280\u672f\uff0c\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4fdd\u6301\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u9700\u8981\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u6765\u9ad8\u6548\u90e8\u7f72\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\u4e0b\u96be\u4ee5\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u4e0a\u91c7\u6837\u7ed3\u5408Sigma-Delta\u91cf\u5316\u5668\u5c06\u6743\u91cd\u4e8c\u503c\u5316\u6216\u4e09\u503c\u5316\uff0c\u91c7\u7528Hadamard\u6743\u91cd\u5e73\u6ed1\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u63d0\u51fa\u5206\u5c42\u7ec6\u7c92\u5ea6OSR\u5206\u914d\u7b56\u7565MultiOSR\u3002", "result": "\u5728OPT\u548cLLaMA\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSDQ-LLM\u5728\u9ad8\u5ea6\u6fc0\u8fdb\u7684\u4f4eOSR\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u9ad8\u7cbe\u5ea6\u7684\u6027\u80fd\u3002", "conclusion": "SDQ-LLM\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86LLM\u7684\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03276", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03276", "abs": "https://arxiv.org/abs/2510.03276", "authors": ["Qian Chen", "Linxin Yang", "Akang Wang", "Xiaodong Luo", "Yin Zhang"], "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The combination of linear transformations and non-linear activation functions\nforms the foundation of most modern deep neural networks, enabling them to\napproximate highly complex functions. This paper explores the introduction of\nquadratic transformations to further increase nonlinearity in neural networks,\nwith the aim of enhancing the performance of existing architectures. To reduce\nparameter complexity and computational complexity, we propose a lightweight\nquadratic enhancer that uses low-rankness, weight sharing, and sparsification\ntechniques. For a fixed architecture, the proposed approach introduces\nquadratic interactions between features at every layer, while only adding\nnegligible amounts of additional model parameters and forward computations. We\nconduct a set of proof-of-concept experiments for the proposed method across\nthree tasks: image classification, text classification, and fine-tuning\nlarge-language models. In all tasks, the proposed approach demonstrates clear\nand substantial performance gains.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u4f4e\u79e9\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\uff0c\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u57fa\u4e8e\u7ebf\u6027\u53d8\u6362\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u4e3a\u8fdb\u4e00\u6b65\u589e\u5f3a\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u63a2\u7d22\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u6765\u63d0\u5347\u73b0\u6709\u67b6\u6784\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u6027\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u5728\u6bcf\u5c42\u5f15\u5165\u7279\u5f81\u95f4\u7684\u4e8c\u6b21\u4ea4\u4e92\uff0c\u4ec5\u589e\u52a0\u5c11\u91cf\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e09\u4e2a\u4efb\u52a1\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5747\u663e\u793a\u51fa\u660e\u663e\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\u80fd\u6709\u6548\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5e26\u6765\u5b9e\u8d28\u6027\u6027\u80fd\u589e\u76ca\uff0c\u4e14\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2510.03278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03278", "abs": "https://arxiv.org/abs/2510.03278", "authors": ["Filip Landgren"], "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "comment": "5 pages, 2 figures", "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing\nequations to solve differential equations under uncertainty. However,\ninterpreting uncertainty and overconfidence in B-PINNs requires care due to the\npoorly understood effects the physical constraints have on the network;\noverconfidence could reflect warranted precision, enforced by the constraints,\nrather than miscalibration. Motivated by the need to further clarify how\nindividual physical constraints shape these networks, we introduce a scalable,\nmatrix-free Laplace framework that decomposes the posterior Hessian into\ncontributions from each constraint and provides metrics to quantify their\nrelative influence on the loss landscape. Applied to the Van der Pol equation,\nour method tracks how constraints sculpt the network's geometry and shows,\ndirectly through the Hessian, how changing a single loss weight non-trivially\nredistributes curvature and effective dominance across the others.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u77e9\u9635\u81ea\u7531\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u89e3\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7269\u7406\u7ea6\u675f\u5bf9\u635f\u5931\u66f2\u9762\u7684\u5f71\u54cd\uff0c\u5e76\u91cf\u5316\u5404\u7ea6\u675f\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "motivation": "\u9700\u8981\u6f84\u6e05\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\u5355\u4e2a\u7269\u7406\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u7f51\u7edc\uff0c\u56e0\u4e3a\u7269\u7406\u7ea6\u675f\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u7ea6\u675f\u5f3a\u5236\u5e26\u6765\u7684\u5408\u7406\u7cbe\u5ea6\u800c\u975e\u6821\u51c6\u9519\u8bef\u3002", "method": "\u5f15\u5165\u53ef\u6269\u5c55\u7684\u77e9\u9635\u81ea\u7531\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u5c06\u540e\u9a8cHessian\u77e9\u9635\u5206\u89e3\u4e3a\u6bcf\u4e2a\u7ea6\u675f\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u91cf\u5316\u6307\u6807\u6765\u8861\u91cf\u5b83\u4eec\u5728\u635f\u5931\u66f2\u9762\u4e0a\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "result": "\u5e94\u7528\u4e8eVan der Pol\u65b9\u7a0b\u65f6\uff0c\u8be5\u65b9\u6cd5\u8ffd\u8e2a\u4e86\u7ea6\u675f\u5982\u4f55\u5851\u9020\u7f51\u7edc\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u76f4\u63a5\u901a\u8fc7Hessian\u77e9\u9635\u5c55\u793a\u4e86\u6539\u53d8\u5355\u4e2a\u635f\u5931\u6743\u91cd\u5982\u4f55\u975e\u5e73\u51e1\u5730\u91cd\u65b0\u5206\u5e03\u66f2\u7387\u548c\u6709\u6548\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u7269\u7406\u7ea6\u675f\u5728\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5177\u4f53\u5f71\u54cd\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u89e3\u91ca\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002"}}
{"id": "2510.03279", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03279", "abs": "https://arxiv.org/abs/2510.03279", "authors": ["Youjin Wang", "Yangjingyi Chen", "Jiahao Yan", "Jiaxuan Lu", "Xiao Sun"], "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "comment": null, "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.", "AI": {"tldr": "MemMamba\u901a\u8fc7\u72b6\u6001\u603b\u7ed3\u673a\u5236\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86Mamba\u6a21\u578b\u957f\u8ddd\u79bb\u8bb0\u5fc6\u8870\u51cf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u5728\u6548\u7387\u4e0e\u5185\u5b58\u7684\u6743\u8861\u95ee\u9898\uff1aRNN\u6709\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0cTransformer\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\uff0cMamba\u867d\u7136\u9ad8\u6548\u4f46\u957f\u8ddd\u79bb\u8bb0\u5fc6\u4f1a\u6307\u6570\u8870\u51cf\u3002", "method": "\u63d0\u51faMemMamba\u6846\u67b6\uff0c\u96c6\u6210\u72b6\u6001\u603b\u7ed3\u673a\u5236\u4ee5\u53ca\u8de8\u5c42\u548c\u8de8token\u6ce8\u610f\u529b\uff0c\u6a21\u4eff\u4eba\u7c7b\u9605\u8bfb\u957f\u6587\u6863\u65f6\u63d0\u70bc\u5173\u952e\u4fe1\u606f\u7684\u65b9\u5f0f\u3002", "result": "\u5728PG19\u548cPasskey Retrieval\u7b49\u957f\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709Mamba\u53d8\u4f53\u548cTransformer\uff0c\u63a8\u7406\u6548\u7387\u63d0\u534748%\u3002", "conclusion": "MemMamba\u5728\u590d\u6742\u5ea6-\u5185\u5b58\u6743\u8861\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03280", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03280", "abs": "https://arxiv.org/abs/2510.03280", "authors": ["Jinjie Ni", "Qian Liu", "Chao Du", "Longxu Dou", "Hang Yan", "Zili Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "Training Optimal Large Diffusion Language Models", "comment": null, "summary": "We introduce Quokka, the first systematic scaling law for diffusion language\nmodels (DLMs), encompassing both compute-constrained and data-constrained\nregimes, and studying the key modeling and optimization designs. Quokka is a\ngood friend of Chinchilla and provides wider scopes. We hope the results would\nbring short-term practical guidance in DLMs training and long-term inspirations\nfor the whole AI community.", "AI": {"tldr": "Quokka\u662f\u9996\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u7f29\u653e\u5b9a\u5f8b\uff0c\u6db5\u76d6\u8ba1\u7b97\u53d7\u9650\u548c\u6570\u636e\u53d7\u9650\u4e24\u79cd\u673a\u5236\uff0c\u5e76\u7814\u7a76\u4e86\u5173\u952e\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u77ed\u671f\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2aAI\u793e\u533a\u5e26\u6765\u957f\u671f\u542f\u53d1\u3002", "method": "\u5f00\u53d1\u7cfb\u7edf\u6027\u7f29\u653e\u5b9a\u5f8b\uff0c\u7814\u7a76\u5173\u952e\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u6db5\u76d6\u8ba1\u7b97\u53d7\u9650\u548c\u6570\u636e\u53d7\u9650\u4e24\u79cd\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86Quokka\u7f29\u653e\u5b9a\u5f8b\uff0c\u8fd9\u662f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7684\u9996\u4e2a\u7cfb\u7edf\u6027\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "Quokka\u662fChinchilla\u7684\u597d\u670b\u53cb\uff0c\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u8303\u56f4\uff0c\u6709\u671b\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.03282", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03282", "abs": "https://arxiv.org/abs/2510.03282", "authors": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "comment": "Accepted to the NeurIPS 2025 Workshop on Mechanistic Interpretability\n  (Mechinterp) and the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "Interpreting language models often involves circuit analysis, which aims to\nidentify sparse subnetworks, or circuits, that accomplish specific tasks.\nExisting circuit discovery algorithms face a fundamental trade-off: attribution\npatching is fast but unfaithful to the full model, while edge pruning is\nfaithful but computationally expensive. This research proposes a hybrid\nattribution and pruning (HAP) framework that uses attribution patching to\nidentify a high-potential subgraph, then applies edge pruning to extract a\nfaithful circuit from it. We show that HAP is 46\\% faster than baseline\nalgorithms without sacrificing circuit faithfulness. Furthermore, we present a\ncase study on the Indirect Object Identification task, showing that our method\npreserves cooperative circuit components (e.g. S-inhibition heads) that\nattribution patching methods prune at high sparsity. Our results show that HAP\ncould be an effective approach for improving the scalability of mechanistic\ninterpretability research to larger models. Our code is available at\nhttps://anonymous.4open.science/r/HAP-circuit-discovery.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d(HAP)\u6846\u67b6\uff0c\u7ed3\u5408\u5f52\u56e0\u4fee\u8865\u7684\u901f\u5ea6\u4f18\u52bf\u548c\u8fb9\u526a\u679d\u7684\u5fe0\u5b9e\u6027\uff0c\u5728\u4fdd\u6301\u7535\u8def\u5fe0\u5b9e\u5ea6\u7684\u540c\u65f6\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb46%\u3002", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u7b97\u6cd5\u9762\u4e34\u57fa\u672c\u6743\u8861\uff1a\u5f52\u56e0\u4fee\u8865\u901f\u5ea6\u5feb\u4f46\u4e0d\u5fe0\u5b9e\u4e8e\u5b8c\u6574\u6a21\u578b\uff0c\u800c\u8fb9\u526a\u679d\u5fe0\u5b9e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u5e73\u8861\u901f\u5ea6\u548c\u5fe0\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u9ad8\u6f5c\u529b\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8fb9\u526a\u679d\u4ece\u4e2d\u63d0\u53d6\u5fe0\u5b9e\u7535\u8def\u3002", "result": "HAP\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb46%\u4e14\u4e0d\u727a\u7272\u7535\u8def\u5fe0\u5b9e\u5ea6\uff0c\u5728\u95f4\u63a5\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e2d\u4fdd\u7559\u4e86\u5f52\u56e0\u4fee\u8865\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u526a\u679d\u7684\u5408\u4f5c\u7535\u8def\u7ec4\u4ef6\u3002", "conclusion": "HAP\u53ef\u80fd\u662f\u63d0\u9ad8\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.03284", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03284", "abs": "https://arxiv.org/abs/2510.03284", "authors": ["Vinay Venkatesh", "Vamsidhar R Kamanuru", "Lav Kumar", "Nikita Kothari"], "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "comment": "7 pages, 1 figure", "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a\nscalable framework for Federated Instruction Tuning (FIT) of Large Language\nModels (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail\nwhen confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT\nframework combines federated learning with 4-bit Quantized Low-Rank Adaptation\n(QLORA), mitigating the core issues of communication and computational\noverhead. We demonstrate this by filtering the general-purpose Databricks Dolly\n15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned\nLlama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable\ntrade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable\nframework for decentralized LLM deployment on home compute gateways.", "AI": {"tldr": "Edge-FIT\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8054\u90a6\u6307\u4ee4\u8c03\u4f18LLM\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c4\u4f4d\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\u6765\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u4e0a\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982FedAvg\uff09\u5728\u5904\u7406LLM\u7684\u5de8\u5927\u53c2\u6570\u91cf\u65f6\u5931\u6548\uff0c\u9700\u8981\u89e3\u51b3\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u4ee5\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u7684LLM\u90e8\u7f72\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c4\u4f4d\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\uff08QLORA\uff09\uff0c\u5728\u7269\u8054\u7f51\u9886\u57df\u8fc7\u6ee4\u901a\u7528Databricks Dolly 15k\u6570\u636e\u96c6\u8fdb\u884c\u8054\u90a6\u6307\u4ee4\u8c03\u4f18\u3002", "result": "Edge-FIT\u8c03\u4f18\u7684Llama 2(7B)\u6a21\u578bF1\u5206\u6570\u8fbe\u52300.89\uff0c\u4f7f\u75283.8B Phi-3-mini\u6a21\u578b\u9a8c\u8bc1\u4e86\u53ef\u884c\u7684\u6743\u8861\u65b9\u6848\u3002", "conclusion": "Edge-FIT\u88ab\u9a8c\u8bc1\u4e3a\u5728\u5bb6\u5ead\u8ba1\u7b97\u7f51\u5173\u4e0a\u5206\u6563\u5f0fLLM\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.03289", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03289", "abs": "https://arxiv.org/abs/2510.03289", "authors": ["Haocheng Sun", "Cynthia Xin Wen", "Edward Hong Wang"], "title": "Why mask diffusion does not work", "comment": null, "summary": "The main advantages of diffusion language models over autoregressive (AR)\nmodels lie in their ability to support parallel generation and bidirectional\nattention, enabling a more controllable generation process. In recent years,\nopen-source mask diffusion language models have emerged, most of which are\nbased on a variant known as absorbing diffusion. However, this paper\ndemonstrates why mask diffusion faces inherent difficulties in achieving\nparallel generation and bidirectional attention. We also propose the most\neffective training and inference strategies for mask diffusion.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7684\u5f00\u6e90\u63a9\u7801\u6269\u6563\u6a21\u578b\u5927\u591a\u57fa\u4e8e\u5438\u6536\u6269\u6563\u53d8\u4f53\uff0c\u5b58\u5728\u5b9e\u73b0\u8fd9\u4e9b\u4f18\u52bf\u7684\u56fa\u6709\u56f0\u96be\u3002", "method": "\u5206\u6790\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u56fa\u6709\u56f0\u96be\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u7b56\u7565\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u867d\u7136\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u6280\u672f\u6311\u6218\uff0c\u9700\u8981\u7279\u5b9a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u6765\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.03290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03290", "abs": "https://arxiv.org/abs/2510.03290", "authors": ["X. Angelo Huang", "Ruben Ciranni", "Giovanni Spadaccini", "Carla J. L\u00f3pez Zurita"], "title": "Single-Core Superscalar Optimization of Clifford Neural Layers", "comment": "9 pages", "summary": "Within the growing interest in the physical sciences in developing networks\nwith equivariance properties, Clifford neural layers shine as one approach that\ndelivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this\npaper, we analyze the inner structure of the computation within Clifford\nconvolutional layers and propose and implement several optimizations to speed\nup the inference process while maintaining correctness. In particular, we begin\nby analyzing the theoretical foundations of Clifford algebras to eliminate\nredundant matrix allocations and computations, then systematically apply\nestablished optimization techniques to enhance performance further. We report a\nfinal average speedup of 21.35x over the baseline implementation of eleven\nfunctions and runtimes comparable to and faster than the original PyTorch\nimplementation in six cases. In the remaining cases, we achieve performance in\nthe same order of magnitude as the original library.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Clifford\u5377\u79ef\u5c42\u7684\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u6765\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u786e\u6027\u3002\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u77e9\u9635\u5206\u914d\u548c\u8ba1\u7b97\uff0c\u5e76\u5e94\u7528\u4f18\u5316\u6280\u672f\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e8621.35\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u7269\u7406\u79d1\u5b66\u4e2d\u5bf9\u5177\u6709\u7b49\u53d8\u6027\u7279\u6027\u7684\u7f51\u7edc\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0cClifford\u795e\u7ecf\u5c42\u4f5c\u4e3a\u5b9e\u73b0E(n)\u548cO(n)\u7b49\u53d8\u6027\u7684\u4e00\u79cd\u65b9\u6cd5\u800c\u5907\u53d7\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316\u8fd9\u4e9b\u5c42\u7684\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u9996\u5148\u5206\u6790Clifford\u4ee3\u6570\u7684\u7406\u8bba\u57fa\u7840\u4ee5\u6d88\u9664\u5197\u4f59\u77e9\u9635\u5206\u914d\u548c\u8ba1\u7b97\uff0c\u7136\u540e\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u5df2\u5efa\u7acb\u7684\u4f18\u5316\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u572811\u4e2a\u51fd\u6570\u4e0a\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u5b9e\u73b0\u768421.35\u500d\u5e73\u5747\u52a0\u901f\uff0c\u57286\u4e2a\u6848\u4f8b\u4e2d\u8fd0\u884c\u65f6\u95f4\u4e0e\u539fPyTorch\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u5feb\uff0c\u5176\u4f59\u6848\u4f8b\u4e2d\u6027\u80fd\u4e0e\u539f\u5e93\u5904\u4e8e\u540c\u4e00\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u6210\u529f\u663e\u8457\u63d0\u5347\u4e86Clifford\u5377\u79ef\u5c42\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6b63\u786e\u6027\u3002"}}
{"id": "2510.03291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03291", "abs": "https://arxiv.org/abs/2510.03291", "authors": ["Yizhuo Ding", "Wanying Qu", "Jiawei Geng", "Wenqi Shao", "Yanwei Fu"], "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks\nbut face prohibitive computational and memory costs. Pruning offers a promising\npath by inducing sparsity while preserving architectural flexibility. However,\nexisting methods struggle to balance efficiency and robustness: local metric\napproaches prune layer by layer but often collapse under high sparsity, whereas\nglobal feedback methods enforce consistency at the cost of expensive weight\nupdates or restrictive semi-structured formats. We present UniPruning, a\nunified post-training pruning framework that combines the speed of local\nsaliency metrics with the stability of global coordination, enabled by a mirror\ndescent based optimization, all without updating model weights. UniPruning\nleverages fast layer-wise scoring and a lightweight global controller to\nallocate a single sparsity budget, supporting both unstructured and\nsemi-structured N :M pruning within one framework. After a brief calibration,\nit can generate pruning masks for arbitrary sparsity levels in one shot, and\nadapts seamlessly to hardware-aware constraints. Extensive experiments on\nmultiple pretrained LLM families and standard benchmarks show that UniPruning\nconsistently delivers competitive or superior perplexity and zero-shot\naccuracy. Ablation studies further highlight the importance of mirror descent\nand local saliency anchoring. Overall, UniPruning provides an efficient,\nprincipled, and scalable solution for sparsifying large-scale LLMs. Our code is\navailable at: https://github.com/RainbowQTT/UniPruning.", "AI": {"tldr": "UniPruning\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u540e\u8bad\u7ec3\u526a\u679d\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u663e\u8457\u6027\u5ea6\u91cf\u7684\u901f\u5ea6\u548c\u5168\u5c40\u534f\u8c03\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u955c\u50cf\u4e0b\u964d\u4f18\u5316\u5b9e\u73b0\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6548\u7387\u548c\u9c81\u68d2\u6027\uff1a\u5c40\u90e8\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u5bb9\u6613\u5d29\u6e83\uff0c\u5168\u5c40\u65b9\u6cd5\u5219\u4ee3\u4ef7\u6602\u8d35\u6216\u683c\u5f0f\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u5feb\u901f\u5c42\u95f4\u8bc4\u5206\u548c\u8f7b\u91cf\u7ea7\u5168\u5c40\u63a7\u5236\u5668\u5206\u914d\u5355\u4e00\u7a00\u758f\u5ea6\u9884\u7b97\uff0c\u652f\u6301\u975e\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\uff0c\u901a\u8fc7\u955c\u50cf\u4e0b\u964d\u4f18\u5316\u548c\u5c40\u90e8\u663e\u8457\u6027\u951a\u5b9a\u5b9e\u73b0\u3002", "result": "\u5728\u591a\u4e2a\u9884\u8bad\u7ec3LLM\u5bb6\u65cf\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniPruning\u59cb\u7ec8\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "UniPruning\u4e3a\u5927\u89c4\u6a21\u578b\u7a00\u758f\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u6709\u539f\u5219\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03301", "abs": "https://arxiv.org/abs/2510.03301", "authors": ["Arthur Sedek"], "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles", "comment": null, "summary": "This paper introduces a novel adaptive ensemble framework that\nsynergistically combines XGBoost and neural networks through sophisticated\nmeta-learning. The proposed method leverages advanced uncertainty\nquantification techniques and feature importance integration to dynamically\norchestrate model selection and combination. Experimental results demonstrate\nsuperior predictive performance and enhanced interpretability across diverse\ndatasets, contributing to the development of more intelligent and flexible\nmachine learning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u9009\u62e9\u4e0e\u7ec4\u5408\u3002", "motivation": "\u5f00\u53d1\u66f4\u667a\u80fd\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u5143\u5b66\u4e60\u534f\u540c\u7ed3\u5408XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u548c\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\uff0c\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u7f16\u6392\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.03302", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03302", "abs": "https://arxiv.org/abs/2510.03302", "authors": ["Daiheng Gao", "Nanxiang Jiang", "Andi Zhang", "Shilin Lu", "Yufei Tang", "Wenbo Zhou", "Weiming Zhang", "Zhaoxin Fan"], "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models", "comment": "21 pages, 10 figures", "summary": "Concept erasure techniques have been widely deployed in T2I diffusion models\nto prevent inappropriate content generation for safety and copyright\nconsiderations. However, as models evolve to next-generation architectures like\nFlux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit\ndegraded effectiveness, raising questions about their true mechanisms. Through\nsystematic analysis, we reveal that concept erasure creates only an illusion of\n``amnesia\": rather than genuine forgetting, these methods bias sampling\ntrajectories away from target concepts, making the erasure fundamentally\nreversible. This insight motivates the need to distinguish superficial safety\nfrom genuine concept removal. In this work, we propose \\textbf{RevAm}\n(\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization\nframework that resurrects erased concepts by dynamically steering the denoising\nprocess without modifying model weights. By adapting Group Relative Policy\nOptimization (GRPO) to diffusion models, RevAm explores diverse recovery\ntrajectories through trajectory-level rewards, overcoming local optima that\nlimit existing methods. Extensive experiments demonstrate that RevAm achieves\nsuperior concept resurrection fidelity while reducing computational time by\n10$\\times$, exposing critical vulnerabilities in current safety mechanisms and\nunderscoring the need for more robust erasure techniques beyond trajectory\nmanipulation.", "AI": {"tldr": "\u6982\u5ff5\u64e6\u9664\u6280\u672f\u5728T2I\u6269\u6563\u6a21\u578b\u4e2d\u4ec5\u521b\u9020\"\u5931\u5fc6\"\u5047\u8c61\uff0c\u800c\u975e\u771f\u6b63\u79fb\u9664\u6982\u5ff5\u3002\u672c\u6587\u63d0\u51faRevAm\u6846\u67b6\uff0c\u901a\u8fc7RL\u8f68\u8ff9\u4f18\u5316\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u5373\u53ef\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u66b4\u9732\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5728\u4e0b\u4e00\u4ee3\u67b6\u6784\u4e2d\u6548\u679c\u4e0b\u964d\uff0c\u4e14\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u662f\u504f\u7f6e\u91c7\u6837\u8f68\u8ff9\u800c\u975e\u771f\u6b63\u79fb\u9664\u6982\u5ff5\uff0c\u8fd9\u79cd\u64e6\u9664\u672c\u8d28\u4e0a\u662f\u53ef\u9006\u7684\uff0c\u9700\u8981\u533a\u5206\u8868\u9762\u5b89\u5168\u4e0e\u771f\u5b9e\u6982\u5ff5\u79fb\u9664\u3002", "method": "\u63d0\u51faRevAm\u6846\u67b6\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u6765\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002\u91c7\u7528Group Relative Policy Optimization\u9002\u5e94\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5956\u52b1\u63a2\u7d22\u591a\u6837\u5316\u6062\u590d\u8f68\u8ff9\u3002", "result": "RevAm\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6982\u5ff5\u590d\u6d3b\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1110\u500d\uff0c\u6210\u529f\u590d\u6d3b\u4e86ESD\u3001UCE\u3001AC\u7b49\u65b9\u6cd5\u64e6\u9664\u7684\u6982\u5ff5\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u8f68\u8ff9\u64cd\u7eb5\u7684\u5b89\u5168\u673a\u5236\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u64e6\u9664\u6280\u672f\uff0c\u4e0d\u80fd\u4ec5\u4f9d\u8d56\u8f68\u8ff9\u504f\u7f6e\u6765\u5b9e\u73b0\u771f\u6b63\u7684\u6982\u5ff5\u79fb\u9664\u3002"}}
{"id": "2510.03305", "categories": ["cs.LG", "physics.ao-ph", "stat.AP", "stat.ML", "62P12 62p12"], "pdf": "https://arxiv.org/pdf/2510.03305", "abs": "https://arxiv.org/abs/2510.03305", "authors": ["Tian Zheng", "Subashree Venkatasubramanian", "Shuolin Li", "Amy Braverman", "Xinyi Ke", "Zhewen Hou", "Peter Jin", "Samarth Sanjay Agrawal"], "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies", "comment": "Supplement", "summary": "Machine learning has been increasingly applied in climate modeling on system\nemulation acceleration, data-driven parameter inference, forecasting, and\nknowledge discovery, addressing challenges such as physical consistency,\nmulti-scale coupling, data sparsity, robust generalization, and integration\nwith scientific workflows. This paper analyzes a series of case studies from\napplied machine learning research in climate modeling, with a focus on design\nchoices and workflow structure. Rather than reviewing technical details, we aim\nto synthesize workflow design patterns across diverse projects in ML-enabled\nclimate modeling: from surrogate modeling, ML parameterization, probabilistic\nprogramming, to simulation-based inference, and physics-informed transfer\nlearning. We unpack how these workflows are grounded in physical knowledge,\ninformed by simulation data, and designed to integrate observations. We aim to\noffer a framework for ensuring rigor in scientific machine learning through\nmore transparent model development, critical evaluation, informed adaptation,\nand reproducibility, and to contribute to lowering the barrier for\ninterdisciplinary collaboration at the interface of data science and climate\nmodeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u91cd\u70b9\u5173\u6ce8\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5305\u62ec\u66ff\u4ee3\u5efa\u6a21\u3001ML\u53c2\u6570\u5316\u3001\u6982\u7387\u7f16\u7a0b\u7b49\uff0c\u65e8\u5728\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e25\u8c28\u6027\u6846\u67b6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u7269\u7406\u4e00\u81f4\u6027\u3001\u591a\u5c3a\u5ea6\u8026\u5408\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6765\u786e\u4fdd\u79d1\u5b66\u4e25\u8c28\u6027\u548c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e00\u7cfb\u5217\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7814\u7a76\u6848\u4f8b\uff0c\u7efc\u5408\u4e0d\u540c\u9879\u76ee\u4e2d\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5173\u6ce8\u8bbe\u8ba1\u9009\u62e9\u548c\u6d41\u7a0b\u7ed3\u6784\u800c\u975e\u6280\u672f\u7ec6\u8282\u3002", "result": "\u8bc6\u522b\u5e76\u7cfb\u7edf\u5316\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u5de5\u4f5c\u6d41\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u5982\u4f55\u57fa\u4e8e\u7269\u7406\u77e5\u8bc6\u3001\u5229\u7528\u6a21\u62df\u6570\u636e\u5e76\u4e0e\u89c2\u6d4b\u6570\u636e\u6574\u5408\u3002", "conclusion": "\u63d0\u51fa\u4e86\u786e\u4fdd\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e25\u8c28\u6027\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u900f\u660e\u6a21\u578b\u5f00\u53d1\u3001\u5173\u952e\u8bc4\u4f30\u3001\u77e5\u60c5\u9002\u5e94\u548c\u53ef\u91cd\u590d\u6027\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u6570\u636e\u79d1\u5b66\u4e0e\u6c14\u5019\u5efa\u6a21\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u95e8\u69db\u3002"}}
{"id": "2510.03309", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.03309", "abs": "https://arxiv.org/abs/2510.03309", "authors": ["Mallikarjuna Tupakula"], "title": "Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval", "comment": null, "summary": "Multimodal foundation models hold promise for drug discovery and biomedical\napplications, but most existing approaches rely on heavy pretraining or large\nscale multimodal corpora. We investigate whether thin contrastive bridges,\nlightweight projection heads over frozen unimodal encoders can align chemical\nand textual representations without training a full multimodal model. Using\npaired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with\nbiomedical sentence embeddings through dual linear projections trained with a\ncontrastive objective. To better handle drugs sharing the same therapeutic\ntarget, we incorporate hard negative weighting and a margin loss. Evaluation\nunder scaffold based splits, which require generalization across disjoint\nchemical cores, demonstrates that our approach achieves non-trivial cross modal\nalignment and substantially improves within target discrimination compared to\nfrozen baselines. These results suggest that thin bridges offer a compute\nefficient alternative to large scale multimodal pretraining, enabling scaffold\naware drug text alignment and target specific retrieval in precision medicine.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5bf9\u6bd4\u6865\u63a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u548c\u7b80\u5355\u7684\u6295\u5f71\u5934\uff0c\u5728\u4e0d\u9700\u8981\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5316\u5b66\u5206\u5b50\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u6216\u591a\u6a21\u6001\u8bed\u6599\u5e93\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u5934\u5728\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u4e0a\u5b9e\u73b0\u5316\u5b66\u548c\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528ChEMBL\u4e2d\u7684\u914d\u5bf9\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u7ebf\u6027\u6295\u5f71\u5c06ECFP4\u5206\u5b50\u6307\u7eb9\u4e0e\u751f\u7269\u533b\u5b66\u53e5\u5b50\u5d4c\u5165\u5bf9\u9f50\uff0c\u91c7\u7528\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u3002\u4e3a\u5904\u7406\u5171\u4eab\u76f8\u540c\u6cbb\u7597\u9776\u70b9\u7684\u836f\u7269\uff0c\u5f15\u5165\u4e86\u56f0\u96be\u8d1f\u6837\u672c\u52a0\u6743\u548c\u8fb9\u754c\u635f\u5931\u3002", "result": "\u5728\u57fa\u4e8e\u652f\u67b6\u7684\u5206\u5272\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u76f8\u6bd4\u51bb\u7ed3\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u5185\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u8584\u6865\u63a5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7cbe\u51c6\u533b\u5b66\u4e2d\u5b9e\u73b0\u652f\u67b6\u611f\u77e5\u7684\u836f\u7269\u6587\u672c\u5bf9\u9f50\u548c\u9776\u70b9\u7279\u5f02\u6027\u68c0\u7d22\u3002"}}
{"id": "2510.03310", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03310", "abs": "https://arxiv.org/abs/2510.03310", "authors": ["Runze Zhang", "Xiaowei Zhang", "Mingyang Zhao"], "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management", "comment": null, "summary": "LLMs are emerging tools for simulating human behavior in business, economics,\nand social science, offering a lower-cost complement to laboratory experiments,\nfield studies, and surveys. This paper evaluates how well LLMs replicate human\nbehavior in operations management. Using nine published experiments in\nbehavioral operations, we assess two criteria: replication of hypothesis-test\noutcomes and distributional alignment via Wasserstein distance. LLMs reproduce\nmost hypothesis-level effects, capturing key decision biases, but their\nresponse distributions diverge from human data, including for strong commercial\nmodels. We also test two lightweight interventions -- chain-of-thought\nprompting and hyperparameter tuning -- which reduce misalignment and can\nsometimes let smaller or open-source models match or surpass larger systems.", "AI": {"tldr": "\u8bc4\u4f30LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u80fd\u590d\u73b0\u5927\u90e8\u5206\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\u4f46\u54cd\u5e94\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\u53ef\u6539\u5584\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "LLMs\u4f5c\u4e3a\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u4f4e\u6210\u672c\u5de5\u5177\u5728\u5546\u4e1a\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u79d1\u5b66\u4e2d\u5174\u8d77\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u8fd0\u8425\u7ba1\u7406\u9886\u57df\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u75289\u4e2a\u5df2\u53d1\u8868\u7684\u884c\u4e3a\u8fd0\u8425\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e24\u4e2a\u6807\u51c6\uff1a\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\u590d\u73b0\u6027\u548c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u7684\u5206\u5e03\u5bf9\u9f50\u6027\uff0c\u5e76\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u3002", "result": "LLMs\u80fd\u590d\u73b0\u5927\u591a\u6570\u5047\u8bbe\u7ea7\u6548\u5e94\uff0c\u6355\u6349\u5173\u952e\u51b3\u7b56\u504f\u5dee\uff0c\u4f46\u54cd\u5e94\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u5305\u62ec\u5546\u4e1a\u6a21\u578b\uff1b\u8f7b\u91cf\u7ea7\u5e72\u9884\u53ef\u51cf\u5c11\u9519\u4f4d\uff0c\u6709\u65f6\u8ba9\u5c0f\u6a21\u578b\u5339\u914d\u6216\u8d85\u8d8a\u5927\u7cfb\u7edf\u3002", "conclusion": "LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u80fd\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u5047\u8bbe\uff0c\u4f46\u5206\u5e03\u5bf9\u9f50\u9700\u8981\u6539\u8fdb\uff0c\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4e3aLLMs\u4f5c\u4e3a\u884c\u4e3a\u7814\u7a76\u5de5\u5177\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.03313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03313", "abs": "https://arxiv.org/abs/2510.03313", "authors": ["Anirudh Subramanyam", "Yuxin Chen", "Robert L. Grossman"], "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining", "comment": "18 pages, 6 figures", "summary": "Scaling laws for language model training traditionally characterize how\nperformance scales with model size and dataset volume. Prior work has explored\narchitecture variants and data treatments such as dataset filtering and noise\ninjection in language model pretraining; however, these studies have not\nformalized data quality within a principled scaling law. We introduce a\ndimensionless data-quality parameter Q, and propose a quality-aware scaling law\nextending the Chinchilla framework to predict loss as a joint function of model\nsize, data volume, and data quality. The law is motivated by an\neffective-sample-size and information-theoretic view of noisy or redundant\ncorpora, and it admits two practical estimators for Q: (i) a corruption rate\nproxy and (ii) a deficiency measure. Through synthetic experiments in neural\nmachine translation and autoregressive modeling -- where we systematically\ncontrol data quality via multiple levels of noise injection and coverage\nvariation -- we show that loss scales predictably with data quality and that\nhigher-quality data can substantially reduce model size and hence compute\nrequirements. Our results demonstrate a sublinear decay of effective data with\nquality and robustness to moderate data corruption; out-of-sample evaluations\nfurther validate the predictive form of the law. Unlike prior empirical\nanalyses, our work establishes an explicit, generalizable law for data quality,\noffering concrete guidance for balancing data curation effort and model scale\nin large-scale pretraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u6570\u636e\u8d28\u91cf\u53c2\u6570\u7684\u6269\u5c55\u7f29\u653e\u5b9a\u5f8b\uff0c\u5c06Chinchilla\u6846\u67b6\u6269\u5c55\u5230\u540c\u65f6\u8003\u8651\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u91cf\u548c\u6570\u636e\u8d28\u91cf\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7f29\u653e\u5b9a\u5f8b\u53ea\u5173\u6ce8\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u91cf\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u8d28\u91cf\u5f71\u54cd\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u5c06\u6570\u636e\u8d28\u91cf\u53c2\u6570\u5316\uff0c\u5efa\u7acb\u5305\u542b\u8d28\u91cf\u7ef4\u5ea6\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7b5b\u9009\u548c\u6a21\u578b\u89c4\u6a21\u5e73\u8861\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u5f15\u5165\u65e0\u91cf\u7eb2\u6570\u636e\u8d28\u91cf\u53c2\u6570Q\uff0c\u57fa\u4e8e\u6709\u6548\u6837\u672c\u91cf\u548c\u4fe1\u606f\u8bba\u89c6\u89d2\u63d0\u51fa\u8d28\u91cf\u611f\u77e5\u7f29\u653e\u5b9a\u5f8b\u3002\u5f00\u53d1\u4e24\u79cdQ\u7684\u5b9e\u7528\u4f30\u8ba1\u5668\uff1a\u8150\u8d25\u7387\u4ee3\u7406\u548c\u7f3a\u9677\u5ea6\u91cf\u3002\u901a\u8fc7\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u548c\u81ea\u56de\u5f52\u5efa\u6a21\u4e2d\u7684\u5408\u6210\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u63a7\u5236\u6570\u636e\u8d28\u91cf\uff08\u566a\u58f0\u6ce8\u5165\u548c\u8986\u76d6\u53d8\u5316\uff09\u6765\u9a8c\u8bc1\u5b9a\u5f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u635f\u5931\u968f\u6570\u636e\u8d28\u91cf\u53ef\u9884\u6d4b\u5730\u7f29\u653e\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u53ef\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u7ed3\u679c\u663e\u793a\u6709\u6548\u6570\u636e\u968f\u8d28\u91cf\u5448\u6b21\u7ebf\u6027\u8870\u51cf\uff0c\u5bf9\u9002\u5ea6\u6570\u636e\u8150\u8d25\u5177\u6709\u9c81\u68d2\u6027\u3002\u6837\u672c\u5916\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5b9a\u5f8b\u7684\u9884\u6d4b\u5f62\u5f0f\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u6570\u636e\u8d28\u91cf\u7684\u663e\u5f0f\u3001\u53ef\u63a8\u5e7f\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u7b5b\u9009\u52aa\u529b\u4e0e\u6a21\u578b\u89c4\u6a21\u7684\u5e73\u8861\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5f25\u8865\u4e86\u5148\u524d\u5b9e\u8bc1\u5206\u6790\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.03325", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.03325", "abs": "https://arxiv.org/abs/2510.03325", "authors": ["Giuseppe Di Somma", "Giorgio Carelli", "Angela D. V. Di Virgilio", "Francesco Fuso", "Enrico Maccioni", "Paolo Marsili"], "title": "Fast frequency reconstruction using Deep Learning for event recognition in ring laser data", "comment": null, "summary": "The reconstruction of a frequency with minimal delay from a sinusoidal signal\nis a common task in several fields; for example Ring Laser Gyroscopes, since\ntheir output signal is a beat frequency. While conventional methods require\nseveral seconds of data, we present a neural network approach capable of\nreconstructing frequencies of several hundred Hertz within approximately 10\nmilliseconds. This enables rapid trigger generation. The method outperforms\nstandard Fourier-based techniques, improving frequency estimation precision by\na factor of 2 in the operational range of GINGERINO, our Ring Laser\nGyroscope.\\\\ In addition to fast frequency estimation, we introduce an\nautomated classification framework to identify physical disturbances in the\nsignal, such as laser instabilities and seismic events, achieving accuracy\nrates between 99\\% and 100\\% on independent test datasets for the seismic\nclass. These results mark a step forward in integrating artificial intelligence\ninto signal analysis for geophysical applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5728\u7ea610\u6beb\u79d2\u5185\u4ece\u6b63\u5f26\u4fe1\u53f7\u4e2d\u91cd\u5efa\u6570\u767e\u8d6b\u5179\u9891\u7387\uff0c\u6bd4\u4f20\u7edf\u5085\u91cc\u53f6\u65b9\u6cd5\u5feb\u4e14\u7cbe\u5ea6\u63d0\u9ad82\u500d\uff0c\u540c\u65f6\u5f00\u53d1\u81ea\u52a8\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u7269\u7406\u5e72\u6270\uff0c\u51c6\u786e\u7387\u8fbe99%-100%\u3002", "motivation": "\u73af\u6fc0\u5149\u9640\u87ba\u4eea\u7b49\u8bbe\u5907\u9700\u8981\u4ece\u6b63\u5f26\u4fe1\u53f7\u4e2d\u5feb\u901f\u91cd\u5efa\u9891\u7387\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6570\u79d2\u6570\u636e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5feb\u901f\u89e6\u53d1\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u8fdb\u884c\u9891\u7387\u91cd\u5efa\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5206\u7c7b\u6846\u67b6\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u7269\u7406\u5e72\u6270\u3002", "result": "\u572810\u6beb\u79d2\u5185\u91cd\u5efa\u6570\u767e\u8d6b\u5179\u9891\u7387\uff0c\u7cbe\u5ea6\u6bd4\u5085\u91cc\u53f6\u65b9\u6cd5\u63d0\u9ad82\u500d\uff1b\u7269\u7406\u5e72\u6270\u5206\u7c7b\u51c6\u786e\u7387\u8fbe99%-100%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u4eba\u5de5\u667a\u80fd\u6210\u529f\u96c6\u6210\u5230\u5730\u7403\u7269\u7406\u4fe1\u53f7\u5206\u6790\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u9891\u7387\u4f30\u8ba1\u548c\u5e72\u6270\u8bc6\u522b\u3002"}}
{"id": "2510.03330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03330", "abs": "https://arxiv.org/abs/2510.03330", "authors": ["Andy Wu", "Chun-Cheng Lin", "Yuehua Huang", "Rung-Tzuo Liaw"], "title": "Constant in an Ever-Changing World", "comment": "in Chinese language", "summary": "The training process of reinforcement learning often suffers from severe\noscillations, leading to instability and degraded performance. In this paper,\nwe propose a Constant in an Ever-Changing World (CIC) framework that enhances\nalgorithmic stability to improve performance. CIC maintains both a\nrepresentative policy and a current policy. Instead of updating the\nrepresentative policy blindly, CIC selectively updates it only when the current\npolicy demonstrates superiority. Furthermore, CIC employs an adaptive\nadjustment mechanism, enabling the representative and current policies to\njointly facilitate critic training. We evaluate CIC on five MuJoCo\nenvironments, and the results show that CIC improves the performance of\nconventional algorithms without incurring additional computational cost.", "AI": {"tldr": "\u63d0\u51faCIC\u6846\u67b6\u6765\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u7ef4\u62a4\u4ee3\u8868\u6027\u7b56\u7565\u548c\u5f53\u524d\u7b56\u7565\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u4ee3\u8868\u6027\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\u5171\u540c\u4fc3\u8fdbcritic\u8bad\u7ec3\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u7ecf\u5e38\u51fa\u73b0\u4e25\u91cd\u9707\u8361\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u9ad8\u7b97\u6cd5\u7a33\u5b9a\u6027\u3002", "method": "CIC\u6846\u67b6\u7ef4\u62a4\u4ee3\u8868\u6027\u7b56\u7565\u548c\u5f53\u524d\u7b56\u7565\uff0c\u4ec5\u5728\u5f53\u524d\u7b56\u7565\u8868\u73b0\u66f4\u4f18\u65f6\u9009\u62e9\u6027\u66f4\u65b0\u4ee3\u8868\u6027\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\u8ba9\u4e24\u4e2a\u7b56\u7565\u5171\u540c\u4fc3\u8fdbcritic\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2aMuJoCo\u73af\u5883\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cCIC\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4f20\u7edf\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CIC\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002"}}
{"id": "2510.03335", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03335", "abs": "https://arxiv.org/abs/2510.03335", "authors": ["Ameya Daigavane", "YuQing Xie", "Bodhi P. Vani", "Saeed Saremi", "Joseph Kleinhenz", "Tess Smidt"], "title": "Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment", "comment": "under review", "summary": "Diffusion models are a popular class of generative models trained to reverse\na noising process starting from a target data distribution. Training a\ndiffusion model consists of learning how to denoise noisy samples at different\nnoise levels. When training diffusion models for point clouds such as molecules\nand proteins, there is often no canonical orientation that can be assigned. To\ncapture this symmetry, the true data samples are often augmented by\ntransforming them with random rotations sampled uniformly over $SO(3)$. Then,\nthe denoised predictions are often rotationally aligned via the Kabsch-Umeyama\nalgorithm to the ground truth samples before computing the loss. However, the\neffect of this alignment step has not been well studied. Here, we show that the\noptimal denoiser can be expressed in terms of a matrix Fisher distribution over\n$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and\nturns out to be the zeroth order approximation for small noise levels,\nexplaining its effectiveness. We build on this perspective to derive better\napproximators to the optimal denoiser in the limit of small noise. Our\nexperiments highlight that alignment is often a `good enough' approximation for\nthe noise levels that matter most for training diffusion models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u65cb\u8f6c\u5bf9\u9f50\u6b65\u9aa4\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5bf9\u9f50\u5bf9\u5e94\u4e8e\u77e9\u9635Fisher\u5206\u5e03\u7684\u91c7\u6837\u6a21\u5f0f\uff0c\u662f\u5c0f\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u96f6\u9636\u8fd1\u4f3c\uff0c\u5e76\u63a8\u5bfc\u4e86\u66f4\u597d\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u5728\u70b9\u4e91\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u901a\u5e38\u4f7f\u7528\u968f\u673a\u65cb\u8f6c\u589e\u5f3a\u548cKabsch-Umeyama\u5bf9\u9f50\u6765\u8ba1\u7b97\u635f\u5931\uff0c\u4f46\u8fd9\u79cd\u5bf9\u9f50\u6b65\u9aa4\u7684\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5c06\u6700\u4f18\u53bb\u566a\u5668\u8868\u793a\u4e3aSO(3)\u4e0a\u7684\u77e9\u9635Fisher\u5206\u5e03\uff0c\u5bf9\u9f50\u5bf9\u5e94\u4e8e\u91c7\u6837\u8be5\u5206\u5e03\u7684\u6a21\uff0c\u5e76\u63a8\u5bfc\u4e86\u5c0f\u566a\u58f0\u6781\u9650\u4e0b\u66f4\u597d\u7684\u8fd1\u4f3c\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u6700\u91cd\u8981\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u5bf9\u9f50\u901a\u5e38\u662f\u4e00\u4e2a'\u8db3\u591f\u597d'\u7684\u8fd1\u4f3c\u3002", "conclusion": "\u5bf9\u9f50\u6b65\u9aa4\u5728\u5c0f\u566a\u58f0\u6c34\u5e73\u4e0b\u662f\u6709\u6548\u7684\u96f6\u9636\u8fd1\u4f3c\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u5e76\u63a8\u5bfc\u4e86\u6539\u8fdb\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002"}}
{"id": "2510.03339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03339", "abs": "https://arxiv.org/abs/2510.03339", "authors": ["Sofiane Ennadir", "Levente Z\u00f3lyomi", "Oleg Smirnov", "Tianze Wang", "John Pertoft", "Filip Cornell", "Lele Cao"], "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models", "comment": null, "summary": "Transformer models have become the dominant backbone for sequence modeling,\nleveraging self-attention to produce contextualized token representations.\nThese are typically aggregated into fixed-size vectors via pooling operations\nfor downstream tasks. While much of the literature has focused on attention\nmechanisms, the role of pooling remains underexplored despite its critical\nimpact on model behavior. In this paper, we introduce a theoretical framework\nthat rigorously characterizes the expressivity of Transformer-based models\nequipped with widely used pooling methods by deriving closed-form bounds on\ntheir representational capacity and the ability to distinguish similar inputs.\nOur analysis extends to different variations of attention formulations,\ndemonstrating that these bounds hold across diverse architectural variants. We\nempirically evaluate pooling strategies across tasks requiring both global and\nlocal contextual understanding, spanning three major modalities: computer\nvision, natural language processing, and time-series analysis. Results reveal\nconsistent trends in how pooling choices affect accuracy, sensitivity, and\noptimization behavior. Our findings unify theoretical and empirical\nperspectives, providing practical guidance for selecting or designing pooling\nmechanisms suited to specific tasks. This work positions pooling as a key\narchitectural component in Transformer models and lays the foundation for more\nprincipled model design beyond attention alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86Transformer\u6a21\u578b\u4e2d\u6c60\u5316\u64cd\u4f5c\u7684\u8868\u5f81\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6c60\u5316\u7b56\u7565\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5df2\u6210\u4e3a\u5e8f\u5217\u5efa\u6a21\u7684\u4e3b\u6d41\u67b6\u6784\uff0c\u4f46\u6c60\u5316\u64cd\u4f5c\u4f5c\u4e3a\u5c06token\u8868\u793a\u805a\u5408\u4e3a\u56fa\u5b9a\u5927\u5c0f\u5411\u91cf\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u5176\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u63a8\u5bfc\u6c60\u5316\u64cd\u4f5c\u7684\u8868\u5f81\u80fd\u529b\u95ed\u5f0f\u8fb9\u754c\uff0c\u5e76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e09\u5927\u6a21\u6001\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6c60\u5316\u9009\u62e9\u5bf9\u6a21\u578b\u7cbe\u5ea6\u3001\u654f\u611f\u6027\u548c\u4f18\u5316\u884c\u4e3a\u5177\u6709\u4e00\u81f4\u5f71\u54cd\uff0c\u4e3a\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u6216\u8bbe\u8ba1\u6c60\u5316\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002", "conclusion": "\u6c60\u5316\u5e94\u88ab\u89c6\u4e3aTransformer\u6a21\u578b\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\uff0c\u8fd9\u4e3a\u8d85\u8d8a\u6ce8\u610f\u529b\u673a\u5236\u7684\u66f4\u539f\u5219\u6027\u6a21\u578b\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03340", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.03340", "abs": "https://arxiv.org/abs/2510.03340", "authors": ["Marian Chen", "Miri Zilka"], "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL", "comment": null, "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies\nthat balance disease containment with socioeconomic stability. We approach this\nchallenge by designing a framework for modeling and evaluating disease-spread\nprevention strategies. Our framework leverages multi-objective reinforcement\nlearning (MORL) - a formulation necessitated by competing objectives - combined\nwith a new stochastic differential equation (SDE) pandemic simulator,\ncalibrated and validated against global COVID-19 data. Our simulator reproduces\nnational-scale pandemic dynamics with orders of magnitude higher fidelity than\nother models commonly used in reinforcement learning (RL) approaches to\npandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on\nthis simulator, we illustrate the direct policy trade-offs between\nepidemiological control and economic stability for COVID-19. Furthermore, we\ndemonstrate the framework's generality by extending it to pathogens with\ndifferent epidemiological profiles, such as polio and influenza, and show how\nthese profiles lead the agent to discover fundamentally different intervention\npolicies. To ground our work in contemporary policymaking challenges, we apply\nthe model to measles outbreaks, quantifying how a modest 5% drop in vaccination\ncoverage necessitates significantly more stringent and costly interventions to\ncurb disease spread. This work provides a robust and adaptable framework to\nsupport transparent, evidence-based policymaking for mitigating public health\ncrises.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u548c\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u62df\u5668\u7684\u6d41\u884c\u75c5\u5e72\u9884\u6846\u67b6\uff0c\u80fd\u591f\u5728\u75be\u75c5\u63a7\u5236\u548c\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u5206\u6790\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f20\u67d3\u75c5\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u51f8\u663e\u4e86\u5728\u75be\u75c5\u63a7\u5236\u548c\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u8feb\u5207\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8003\u8651\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\u7684\u5e72\u9884\u7b56\u7565\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u548c\u65b0\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u6d41\u884c\u75c5\u6a21\u62df\u5668\uff0c\u4f7f\u7528Pareto\u6761\u4ef6\u7f51\u7edc\uff08PCN\uff09\u4ee3\u7406\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u62df\u5668\u7ecf\u8fc7\u5168\u7403COVID-19\u6570\u636e\u6821\u51c6\u9a8c\u8bc1\u3002", "result": "\u6a21\u62df\u5668\u5728\u91cd\u73b0\u56fd\u5bb6\u5c3a\u5ea6\u75ab\u60c5\u52a8\u6001\u65b9\u9762\u6bd4\u5176\u4ed6RL\u5e38\u7528\u6a21\u578b\u7cbe\u5ea6\u9ad8\u6570\u4e2a\u6570\u91cf\u7ea7\uff1b\u5c55\u793a\u4e86COVID-19\u6d41\u884c\u75c5\u63a7\u5236\u4e0e\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u7684\u76f4\u63a5\u653f\u7b56\u6743\u8861\uff1b\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e0d\u540c\u6d41\u884c\u75c5\u7279\u5f81\u7684\u75c5\u539f\u4f53\uff08\u5982\u810a\u9ad3\u7070\u8d28\u708e\u3001\u6d41\u611f\uff09\uff0c\u5e76\u53d1\u73b0\u4e0d\u540c\u7684\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u652f\u6301\u900f\u660e\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u516c\u5171\u536b\u751f\u5371\u673a\u7f13\u89e3\u653f\u7b56\u5236\u5b9a\uff0c\u80fd\u591f\u91cf\u5316\u75ab\u82d7\u63a5\u79cd\u8986\u76d6\u7387\u4e0b\u964d\u5bf9\u5e72\u9884\u7b56\u7565\u6210\u672c\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.03345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03345", "abs": "https://arxiv.org/abs/2510.03345", "authors": ["Luoma Ke", "Guangpeng Zhang", "Jibo He", "Yajing Li", "Yan Li", "Xufeng Liu", "Peng Fang"], "title": "Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models", "comment": null, "summary": "With the rapid growth of the aviation industry, there is a need for a large\nnumber of flight crew. How to select the right pilots in a cost-efficient\nmanner has become an important research question. In the current study,\ntwenty-three pilots were recruited from China Eastern Airlines, and 23 novices\nwere from the community of Tsinghua University. A novel approach incorporating\nmachine learning and virtual reality technology was applied to distinguish\nfeatures between these participants with different flight skills. Results\nindicate that SVM with the MIC feature selection method consistently achieved\nthe highest prediction performance on all metrics with an Accuracy of 0.93, an\nAUC of 0.96, and an F1 of 0.93, which outperforms four other classifier\nalgorithms and two other feature selection methods. From the perspective of\nfeature selection methods, the MIC method can select features with a nonlinear\nrelationship to sampling labels, instead of a simple filter-out. Our new\nimplementation of the SVM + MIC algorithm outperforms all existing pilot\nselection algorithms and perhaps provides the first implementation based on eye\ntracking and flight dynamics data. This study's VR simulation platforms and\nalgorithms can be used for pilot selection and training.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548cVR\u6280\u672f\u7684\u98de\u884c\u5458\u9009\u62d4\u65b9\u6cd5\uff0c\u4f7f\u7528SVM\u548cMIC\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff0c\u5728\u773c\u52a8\u8ffd\u8e2a\u548c\u98de\u884c\u52a8\u529b\u5b66\u6570\u636e\u4e0a\u5b9e\u73b0\u4e8693%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u822a\u7a7a\u4e1a\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u5927\u91cf\u98de\u884c\u5458\uff0c\u5982\u4f55\u4ee5\u6210\u672c\u6548\u76ca\u7684\u65b9\u5f0f\u9009\u62d4\u5408\u9002\u7684\u98de\u884c\u5458\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u95ee\u9898\u3002", "method": "\u62db\u52df23\u540d\u4e2d\u56fd\u4e1c\u65b9\u822a\u7a7a\u98de\u884c\u5458\u548c23\u540d\u6e05\u534e\u65b0\u624b\uff0c\u5e94\u7528\u673a\u5668\u5b66\u4e60\u548cVR\u6280\u672f\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u91c7\u7528SVM\u5206\u7c7b\u5668\u548cMIC\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u5206\u6790\u773c\u52a8\u548c\u98de\u884c\u6570\u636e\u3002", "result": "SVM+MIC\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff1a\u51c6\u786e\u73870.93\uff0cAUC 0.96\uff0cF1\u5206\u65700.93\uff0c\u4f18\u4e8e\u5176\u4ed6\u56db\u79cd\u5206\u7c7b\u5668\u548c\u4e24\u79cd\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684VR\u6a21\u62df\u5e73\u53f0\u548c\u7b97\u6cd5\u53ef\u7528\u4e8e\u98de\u884c\u5458\u9009\u62d4\u548c\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u548c\u98de\u884c\u52a8\u529b\u5b66\u6570\u636e\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2510.03346", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03346", "abs": "https://arxiv.org/abs/2510.03346", "authors": ["Xiangyu Shi", "Marco Chiesa", "Gerald Q. Maguire Jr.", "Dejan Kostic"], "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems.", "AI": {"tldr": "KVComm\u662f\u4e00\u79cd\u65b0\u9896\u7684LLM\u95f4\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5171\u4eabKV\u5bf9\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\uff0c\u4ec5\u4f20\u8f9330%\u7684KV\u5bf9\u5c31\u80fd\u8fbe\u5230\u4e0e\u76f4\u63a5\u5408\u5e76\u8f93\u5165\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u4fe1\u534f\u8bae\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u5bfc\u81f4\u9ad8\u63a8\u7406\u6210\u672c\u548c\u4fe1\u606f\u635f\u5931\uff0c\u9690\u85cf\u72b6\u6001\u901a\u4fe1\u5b58\u5728\u4fe1\u606f\u96c6\u4e2d\u504f\u5dee\u548c\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faKVComm\u6846\u67b6\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u91cd\u8981\u6027\u5206\u6570\u548c\u9ad8\u65af\u5148\u9a8c\u7684KV\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u8bc6\u522b\u6700\u5177\u4fe1\u606f\u91cf\u7684KV\u5bf9\u8fdb\u884c\u901a\u4fe1\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u548c\u6a21\u578b\u5bf9\u4e0a\uff0cKVComm\u4ec5\u4f20\u8f9330%\u7684KV\u5bf9\u5c31\u80fd\u8fbe\u5230\u4e0e\u76f4\u63a5\u5408\u5e76\u8f93\u5165\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "KV\u5bf9\u4f5c\u4e3aLLM\u95f4\u901a\u4fe1\u5a92\u4ecb\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03349", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.03349", "abs": "https://arxiv.org/abs/2510.03349", "authors": ["Michael Chen"], "title": "AgentCaster: Reasoning-Guided Tornado Forecasting", "comment": null, "summary": "There is a growing need to evaluate Large Language Models (LLMs) on complex,\nhigh-impact, real-world tasks to assess their true readiness as reasoning\nagents. To address this gap, we introduce AgentCaster, a contamination-free\nframework employing multimodal LLMs end-to-end for the challenging,\nlong-horizon task of tornado forecasting. Within AgentCaster, models interpret\nheterogeneous spatiotemporal data from a high-resolution convection-allowing\nforecast archive. We assess model performance over a 40-day period featuring\ndiverse historical data, spanning several major tornado outbreaks and including\nover 500 tornado reports. Each day, models query interactively from a pool of\n3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of\n12-36 hours. Probabilistic tornado-risk polygon predictions are verified\nagainst ground truths derived from geometric comparisons across disjoint risk\nbands in projected coordinate space. To quantify accuracy, we propose\ndomain-specific TornadoBench and TornadoHallucination metrics, with\nTornadoBench highly challenging for both LLMs and domain expert human\nforecasters. Notably, human experts significantly outperform state-of-the-art\nmodels, which demonstrate a strong tendency to hallucinate and overpredict risk\nintensity, struggle with precise geographic placement, and exhibit poor\nspatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster\naims to advance research on improving LLM agents for challenging reasoning\ntasks in critical domains.", "AI": {"tldr": "AgentCaster\u662f\u4e00\u4e2a\u7528\u4e8e\u9f99\u5377\u98ce\u9884\u6d4b\u7684\u591a\u6a21\u6001LLM\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u5728\u590d\u6742\u771f\u5b9e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e13\u5bb6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u98ce\u9669\u5f3a\u5ea6\u8fc7\u5ea6\u9884\u6d4b\u548c\u65f6\u7a7a\u63a8\u7406\u56f0\u96be\u7b49\u95ee\u9898\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30LLM\u5728\u590d\u6742\u3001\u9ad8\u5f71\u54cd\u529b\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u68c0\u9a8c\u5176\u4f5c\u4e3a\u63a8\u7406\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u51c6\u5907\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u7aef\u5230\u7aef\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u5bf9\u6d41\u5141\u8bb8\u9884\u62a5\u6863\u6848\u4e2d\u7684\u5f02\u8d28\u65f6\u7a7a\u6570\u636e\uff0c\u572840\u5929\u671f\u95f4\u67e5\u8be23,625\u4e2a\u9884\u62a5\u56fe\u548c40,125\u4e2a\u9884\u62a5\u63a2\u7a7a\u6570\u636e\uff0c\u8fdb\u884c12-36\u5c0f\u65f6\u9884\u62a5\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u70c8\u5e7b\u89c9\u503e\u5411\u3001\u98ce\u9669\u5f3a\u5ea6\u8fc7\u5ea6\u9884\u6d4b\u3001\u5730\u7406\u5b9a\u4f4d\u4e0d\u51c6\u786e\uff0c\u5728\u590d\u6742\u52a8\u6001\u6f14\u5316\u7cfb\u7edf\u4e2d\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u5dee\u3002", "conclusion": "AgentCaster\u65e8\u5728\u63a8\u8fdb\u6539\u8fdbLLM\u667a\u80fd\u4f53\u5728\u5173\u952e\u9886\u57df\u6311\u6218\u6027\u63a8\u7406\u4efb\u52a1\u7684\u7814\u7a76\u3002"}}
{"id": "2510.03351", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03351", "abs": "https://arxiv.org/abs/2510.03351", "authors": ["Song Wang", "Zhenyu Lei", "Zhen Tan", "Jundong Li", "Javier Rasero", "Aiying Zhang", "Chirag Agarwal"], "title": "Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks", "comment": null, "summary": "Nearly one in five adolescents currently live with a diagnosed mental or\nbehavioral health condition, such as anxiety, depression, or conduct disorder,\nunderscoring the urgency of developing accurate and interpretable diagnostic\ntools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a\npowerful lens into large-scale functional connectivity, where brain regions are\nmodeled as nodes and inter-regional synchrony as edges, offering clinically\nrelevant biomarkers for psychiatric disorders. While prior works use graph\nneural network (GNN) approaches for disorder prediction, they remain complex\nblack-boxes, limiting their reliability and clinical translation. In this work,\nwe propose CONCEPTNEURO, a concept-based diagnosis framework that leverages\nlarge language models (LLMs) and neurobiological domain knowledge to\nautomatically generate, filter, and encode interpretable functional\nconnectivity concepts. Each concept is represented as a structured subgraph\nlinking specific brain regions, which are then passed through a concept\nclassifier. Our design ensures predictions through clinically meaningful\nconnectivity patterns, enabling both interpretability and strong predictive\nperformance. Extensive experiments across multiple psychiatric disorder\ndatasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform\ntheir vanilla counterparts, improving accuracy while providing transparent,\nclinically aligned explanations. Furthermore, concept analyses highlight\ndisorder-specific connectivity patterns that align with expert knowledge and\nsuggest new hypotheses for future investigation, establishing CONCEPTNEURO as\nan interpretable, domain-informed framework for psychiatric disorder diagnosis.", "AI": {"tldr": "CONCEPTNEURO\u662f\u4e00\u4e2a\u57fa\u4e8e\u6982\u5ff5\u7684\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u795e\u7ecf\u751f\u7269\u5b66\u77e5\u8bc6\u81ea\u52a8\u751f\u6210\u3001\u8fc7\u6ee4\u548c\u7f16\u7801\u53ef\u89e3\u91ca\u7684\u529f\u80fd\u8fde\u63a5\u6982\u5ff5\uff0c\u901a\u8fc7\u6982\u5ff5\u5206\u7c7b\u5668\u5b9e\u73b0\u9ad8\u9884\u6d4b\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8fd1\u4e94\u5206\u4e4b\u4e00\u7684\u9752\u5c11\u5e74\u60a3\u6709\u7cbe\u795e\u6216\u884c\u4e3a\u5065\u5eb7\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u5de5\u5177\u3002\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u795e\u7ecf\u751f\u7269\u5b66\u9886\u57df\u77e5\u8bc6\uff0c\u81ea\u52a8\u751f\u6210\u529f\u80fd\u8fde\u63a5\u6982\u5ff5\uff0c\u6bcf\u4e2a\u6982\u5ff5\u8868\u793a\u4e3a\u8fde\u63a5\u7279\u5b9a\u8111\u533a\u7684\u7ed3\u6784\u5316\u5b50\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u6982\u5ff5\u5206\u7c7b\u5668\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u7cbe\u795e\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCONCEPTNEURO\u589e\u5f3a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u59cb\u7ec8\u4f18\u4e8e\u539f\u59cb\u7248\u672c\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u3001\u4e34\u5e8a\u5bf9\u9f50\u7684\u89e3\u91ca\u3002", "conclusion": "CONCEPTNEURO\u5efa\u7acb\u4e86\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u7684\u53ef\u89e3\u91ca\u3001\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u6982\u5ff5\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u4e13\u5bb6\u77e5\u8bc6\u4e00\u81f4\u4e14\u5177\u6709\u7814\u7a76\u4ef7\u503c\u7684\u75be\u75c5\u7279\u5f02\u6027\u8fde\u63a5\u6a21\u5f0f\u3002"}}
{"id": "2510.03355", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.03355", "abs": "https://arxiv.org/abs/2510.03355", "authors": ["Aryan Patel"], "title": "High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)", "comment": null, "summary": "Aluminum is a widely used alloy, which is susceptible to fatigue failure.\nCharacterizing fatigue performance for materials is extremely time and cost\ndemanding, especially for high cycle data. To help mitigate this, a transfer\nlearning based framework has been developed using Long short-term memory\nnetworks (LSTMs) in which a source LSTM model is trained based on pure axial\nfatigue data for Aluminum 7075-T6 alloy which is then transferred to predict\nhigh cycle torsional S-N curves. The framework was able to accurately predict\nAl torsional S-N curves for a much higher cycle range. It is the belief that\nthis framework will help to drastically mitigate the cost of gathering fatigue\ncharacteristics for different materials and help prioritize tests with better\ncost and time constraints.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684LSTM\u6846\u67b6\uff0c\u5229\u7528\u94dd7075-T6\u5408\u91d1\u7684\u8f74\u5411\u75b2\u52b3\u6570\u636e\u6765\u9884\u6d4b\u9ad8\u5468\u626d\u8f6cS-N\u66f2\u7ebf\uff0c\u663e\u8457\u964d\u4f4e\u75b2\u52b3\u7279\u6027\u6d4b\u8bd5\u6210\u672c\u3002", "motivation": "\u94dd\u5408\u91d1\u6613\u53d1\u751f\u75b2\u52b3\u5931\u6548\uff0c\u4f46\u8868\u5f81\u6750\u6599\u75b2\u52b3\u6027\u80fd\uff08\u7279\u522b\u662f\u9ad8\u5468\u6570\u636e\uff09\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc(LSTM)\u6784\u5efa\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u5148\u57fa\u4e8e\u94dd7075-T6\u5408\u91d1\u7684\u7eaf\u8f74\u5411\u75b2\u52b3\u6570\u636e\u8bad\u7ec3\u6e90\u6a21\u578b\uff0c\u7136\u540e\u8fc1\u79fb\u7528\u4e8e\u9884\u6d4b\u9ad8\u5468\u626d\u8f6cS-N\u66f2\u7ebf\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u94dd\u5408\u91d1\u5728\u66f4\u9ad8\u5faa\u73af\u8303\u56f4\u5185\u7684\u626d\u8f6cS-N\u66f2\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u5927\u5e45\u964d\u4f4e\u83b7\u53d6\u4e0d\u540c\u6750\u6599\u75b2\u52b3\u7279\u6027\u7684\u6210\u672c\uff0c\u5e2e\u52a9\u5728\u66f4\u597d\u7684\u6210\u672c\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u4f18\u5148\u5b89\u6392\u6d4b\u8bd5\u3002"}}
{"id": "2510.03358", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03358", "abs": "https://arxiv.org/abs/2510.03358", "authors": ["Annan Yu", "Danielle C. Maddix", "Boran Han", "Xiyuan Zhang", "Abdul Fatir Ansari", "Oleksandr Shchur", "Christos Faloutsos", "Andrew Gordon Wilson", "Michael W. Mahoney", "Yuyang Wang"], "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility", "comment": "42 pages", "summary": "Transformers are widely used across data modalities, and yet the principles\ndistilled from text models often transfer imperfectly to models trained to\nother modalities. In this paper, we analyze Transformers through the lens of\nrank structure. Our focus is on the time series setting, where the structural\nproperties of the data differ remarkably from those of text or vision. We show\nthat time-series embeddings, unlike text or vision, exhibit sharply decaying\nsingular value spectra: small patch sizes and smooth continuous mappings\nconcentrate the data into low-rank subspaces. From this, we prove that the\nassociated $Q/K/V$ projections admit accurate low-rank approximations, and that\nattention layers become compressible in proportion to the decay of the\nembedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by\nwhich nonlinear mixing across depth inflates the rank, explaining why early\nlayers are most amenable to compression and why ranks grow with depth. Guided\nby these theoretical and empirical results, we use these insights to compress\nChronos, a large time series foundation model, achieving a reduction of $65\\%$\nin inference time and $81\\%$ in memory, without loss of accuracy. Our findings\nprovide principled guidance for allocating width, depth, and heads in time\nseries foundation models, and for exploiting their inherent compressibility.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u79e9\u7ed3\u6784\u5206\u6790\u65f6\u95f4\u5e8f\u5217Transformer\uff0c\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u5177\u6709\u5feb\u901f\u8870\u51cf\u7684\u5947\u5f02\u503c\u8c31\uff0c\u4f7f\u5f97Q/K/V\u6295\u5f71\u53ef\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u6ce8\u610f\u529b\u5c42\u53ef\u538b\u7f29\u3002\u63d0\u51fa\u6d41\u79e9\u6982\u5ff5\u89e3\u91ca\u6df1\u5ea6\u589e\u52a0\u65f6\u79e9\u81a8\u80c0\u73b0\u8c61\uff0c\u5e76\u6210\u529f\u538b\u7f29Chronos\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1165%\uff0c\u5185\u5b58\u51cf\u5c1181%\u3002", "motivation": "\u73b0\u6709\u4ece\u6587\u672c\u6a21\u578b\u63d0\u70bc\u7684Transformer\u539f\u7406\u5728\u65f6\u95f4\u5e8f\u5217\u7b49\u6a21\u6001\u4e0a\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u5206\u6790\u65f6\u95f4\u5e8f\u5217Transformer\u7684\u7ed3\u6784\u7279\u6027\u3002", "method": "\u901a\u8fc7\u79e9\u7ed3\u6784\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u7684\u5947\u5f02\u503c\u8c31\u7279\u6027\uff0c\u8bc1\u660eQ/K/V\u6295\u5f71\u7684\u4f4e\u79e9\u8fd1\u4f3c\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u6d41\u79e9\u6982\u5ff5\u89e3\u91ca\u6df1\u5ea6\u4e0e\u79e9\u7684\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u5177\u6709\u5feb\u901f\u8870\u51cf\u7684\u5947\u5f02\u503c\u8c31\uff0c\u6ce8\u610f\u529b\u5c42\u53ef\u538b\u7f29\u6027\u4e0e\u5176\u8c31\u8870\u51cf\u7a0b\u5ea6\u6210\u6b63\u6bd4\u3002\u6210\u529f\u538b\u7f29Chronos\u6a21\u578b\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1165%\uff0c\u5185\u5b58\u51cf\u5c1181%\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u6ce8\u610f\u529b\u5934\u5206\u914d\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u56fa\u6709\u7684\u53ef\u538b\u7f29\u6027\u3002"}}
{"id": "2510.03360", "categories": ["cs.LG", "cs.AI", "math.OC", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.03360", "abs": "https://arxiv.org/abs/2510.03360", "authors": ["Zelin Zhao", "Zongyi Li", "Kimia Hassibi", "Kamyar Azizzadenesheli", "Junchi Yan", "H. Jane Bae", "Di Zhou", "Anima Anandkumar"], "title": "Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows", "comment": null, "summary": "Assessing turbulence control effects for wall friction numerically is a\nsignificant challenge since it requires expensive simulations of turbulent\nfluid dynamics. We instead propose an efficient deep reinforcement learning\n(RL) framework for modeling and control of turbulent flows. It is model-based\nRL for predictive control (PC), where both the policy and the observer models\nfor turbulence control are learned jointly using Physics Informed Neural\nOperators (PINO), which are discretization invariant and can capture fine\nscales in turbulent flows accurately. Our PINO-PC outperforms prior model-free\nreinforcement learning methods in various challenging scenarios where the flows\nare of high Reynolds numbers and unseen, i.e., not provided during model\ntraining. We find that PINO-PC achieves a drag reduction of 39.0\\% under a\nbulk-velocity Reynolds number of 15,000, outperforming previous fluid control\nmethods by more than 32\\%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50(PINO)\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6PINO-PC\uff0c\u7528\u4e8e\u6e4d\u6d41\u63a7\u5236\u5efa\u6a21\uff0c\u5728\u672a\u89c1\u9ad8\u96f7\u8bfa\u6570\u6d41\u52a8\u4e2d\u5b9e\u73b039.0%\u7684\u51cf\u963b\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd532%\u4ee5\u4e0a\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6a21\u62df\u8bc4\u4f30\u6e4d\u6d41\u63a7\u5236\u6548\u679c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6e4d\u6d41\u5efa\u6a21\u4e0e\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50(PINO)\u8054\u5408\u5b66\u4e60\u7b56\u7565\u548c\u89c2\u6d4b\u5668\u6a21\u578b\uff0c\u5177\u6709\u79bb\u6563\u4e0d\u53d8\u6027\u5e76\u80fd\u51c6\u786e\u6355\u6349\u6e4d\u6d41\u7cbe\u7ec6\u5c3a\u5ea6\u3002", "result": "PINO-PC\u5728\u591a\u79cd\u9ad8\u96f7\u8bfa\u6570\u672a\u89c1\u6d41\u52a8\u573a\u666f\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u96f7\u8bfa\u657015,000\u65f6\u5b9e\u73b039.0%\u7684\u51cf\u963b\u6548\u679c\u3002", "conclusion": "PINO-PC\u6846\u67b6\u4e3a\u6e4d\u6d41\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u672a\u89c1\u9ad8\u96f7\u8bfa\u6570\u6d41\u52a8\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03362", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03362", "abs": "https://arxiv.org/abs/2510.03362", "authors": ["Lijiao Wang", "Muhammad Usama", "Haris N. Koutsopoulos", "Zhengbing He"], "title": "Estimating link level traffic emissions: enhancing MOVES with open-source data", "comment": null, "summary": "Open-source data offers a scalable and transparent foundation for estimating\nvehicle activity and emissions in urban regions. In this study, we propose a\ndata-driven framework that integrates MOVES and open-source GPS trajectory\ndata, OpenStreetMap (OSM) road networks, regional traffic datasets and\nsatellite imagery-derived feature vectors to estimate the link level operating\nmode distribution and traffic emissions. A neural network model is trained to\npredict the distribution of MOVES-defined operating modes using only features\nderived from readily available data. The proposed methodology was applied using\nopen-source data related to 45 municipalities in the Boston Metropolitan area.\nThe \"ground truth\" operating mode distribution was established using OSM\nopen-source GPS trajectories. Compared to the MOVES baseline, the proposed\nmodel reduces RMSE by over 50% for regional scale traffic emissions of key\npollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the\nfeasibility of low-cost, replicable, and data-driven emissions estimation using\nfully open data sources.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5f00\u6e90GPS\u8f68\u8ff9\u6570\u636e\u3001\u9053\u8def\u7f51\u7edc\u548c\u536b\u661f\u56fe\u50cf\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u7b97\u8f66\u8f86\u8fd0\u884c\u6a21\u5f0f\u5206\u5e03\u548c\u4ea4\u901a\u6392\u653e\uff0c\u76f8\u6bd4MOVES\u57fa\u51c6\u6a21\u578b\u5c06\u5173\u952e\u6c61\u67d3\u7269\u6392\u653e\u4f30\u7b97\u7684RMSE\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u6570\u636e\u4e3a\u57ce\u5e02\u533a\u57df\u8f66\u8f86\u6d3b\u52a8\u548c\u6392\u653e\u4f30\u7b97\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u57fa\u7840\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u7684\u6392\u653e\u4f30\u7b97\u3002", "method": "\u96c6\u6210MOVES\u548c\u5f00\u6e90GPS\u8f68\u8ff9\u6570\u636e\u3001OpenStreetMap\u9053\u8def\u7f51\u7edc\u3001\u533a\u57df\u4ea4\u901a\u6570\u636e\u96c6\u548c\u536b\u661f\u56fe\u50cf\u7279\u5f81\u5411\u91cf\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9884\u6d4bMOVES\u5b9a\u4e49\u7684\u8fd0\u884c\u6a21\u5f0f\u5206\u5e03\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u5927\u90fd\u4f1a\u533a45\u4e2a\u57ce\u5e02\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4MOVES\u57fa\u51c6\u6a21\u578b\uff0c\u5173\u952e\u6c61\u67d3\u7269\uff08CO\u3001NOx\u3001CO2\u3001PM2.5\uff09\u533a\u57df\u5c3a\u5ea6\u4ea4\u901a\u6392\u653e\u7684RMSE\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u5b8c\u5168\u5f00\u6e90\u6570\u636e\u6e90\u8fdb\u884c\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u548c\u6570\u636e\u9a71\u52a8\u6392\u653e\u4f30\u7b97\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03364", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03364", "abs": "https://arxiv.org/abs/2510.03364", "authors": ["Xiaolong Ma", "Xu Dong", "Ashley Tarrant", "Lei Yang", "Rao Kotamarthi", "Jiali Wang", "Feng Yan", "Rajkumar Kettimuthu"], "title": "Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds", "comment": null, "summary": "High-quality observations of hub-height winds are valuable but sparse in\nspace and time. Simulations are widely available on regular grids but are\ngenerally biased and too coarse to inform wind-farm siting or to assess\nextreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully\nutilize both data types for generating high-quality, high-resolution hub-height\nwind speeds (tens to ~100m above ground), this study introduces WindSR, a\ndiffusion model with data assimilation for super-resolution downscaling of\nhub-height winds. WindSR integrates sparse observational data with simulation\nfields during downscaling using state-of-the-art diffusion models. A\ndynamic-radius blending method is introduced to merge observations with\nsimulations, providing conditioning for the diffusion process. Terrain\ninformation is incorporated during both training and inference to account for\nits role as a key driver of winds. Evaluated against\nconvolutional-neural-network and generative-adversarial-network baselines,\nWindSR outperforms them in both downscaling efficiency and accuracy. Our data\nassimilation reduces WindSR's model bias by approximately 20% relative to\nindependent observations.", "AI": {"tldr": "WindSR\u662f\u4e00\u79cd\u7528\u4e8e\u8f6e\u6bc2\u9ad8\u5ea6\u98ce\u901f\u8d85\u5206\u8fa8\u7387\u964d\u5c3a\u5ea6\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u540c\u5316\u5c06\u7a00\u758f\u89c2\u6d4b\u6570\u636e\u4e0e\u6a21\u62df\u573a\u7ed3\u5408\uff0c\u4f7f\u7528\u52a8\u6001\u534a\u5f84\u878d\u5408\u65b9\u6cd5\u6574\u5408\u89c2\u6d4b\u4e0e\u6a21\u62df\u6570\u636e\uff0c\u5e76\u878d\u5165\u5730\u5f62\u4fe1\u606f\uff0c\u5728\u964d\u5c3a\u5ea6\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u8f6e\u6bc2\u9ad8\u5ea6\u7684\u9ad8\u8d28\u91cf\u89c2\u6d4b\u6570\u636e\u5728\u65f6\u7a7a\u4e0a\u7a00\u758f\uff0c\u800c\u6a21\u62df\u6570\u636e\u867d\u7136\u5e7f\u6cdb\u53ef\u7528\u4f46\u5b58\u5728\u504f\u5dee\u4e14\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u98ce\u7535\u573a\u9009\u5740\u6216\u6781\u7aef\u5929\u6c14\u98ce\u9669\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faWindSR\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u6570\u636e\u540c\u5316\u65b9\u6cd5\u5c06\u7a00\u758f\u89c2\u6d4b\u4e0e\u6a21\u62df\u573a\u7ed3\u5408\uff0c\u5f15\u5165\u52a8\u6001\u534a\u5f84\u878d\u5408\u65b9\u6cd5\u6574\u5408\u6570\u636e\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u878d\u5165\u5730\u5f62\u4fe1\u606f\u3002", "result": "\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u57fa\u7ebf\u76f8\u6bd4\uff0cWindSR\u5728\u964d\u5c3a\u5ea6\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u6570\u636e\u540c\u5316\u4f7f\u6a21\u578b\u504f\u5dee\u76f8\u5bf9\u4e8e\u72ec\u7acb\u89c2\u6d4b\u51cf\u5c11\u4e86\u7ea620%\u3002", "conclusion": "WindSR\u901a\u8fc7\u7ed3\u5408\u89c2\u6d4b\u548c\u6a21\u62df\u6570\u636e\u4ee5\u53ca\u5730\u5f62\u4fe1\u606f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u8f6e\u6bc2\u9ad8\u5ea6\u98ce\u901f\u964d\u5c3a\u5ea6\uff0c\u4e3a\u98ce\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03366", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03366", "abs": "https://arxiv.org/abs/2510.03366", "authors": ["Harshwardhan Fartale", "Ashish Kattamuri", "Rahul Raja", "Arpita Vats", "Ishita Prasad", "Akshata Kishore Moharir"], "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis", "comment": null, "summary": "Transformer-based language models excel at both recall (retrieving memorized\nfacts) and reasoning (performing multi-step inference), but whether these\nabilities rely on distinct internal mechanisms remains unclear. Distinguishing\nrecall from reasoning is crucial for predicting model generalization, designing\ntargeted evaluations, and building safer interventions that affect one ability\nwithout disrupting the other.We approach this question through mechanistic\ninterpretability, using controlled datasets of synthetic linguistic puzzles to\nprobe transformer models at the layer, head, and neuron level. Our pipeline\ncombines activation patching and structured ablations to causally measure\ncomponent contributions to each task type. Across two model families (Qwen and\nLLaMA), we find that interventions on distinct layers and attention heads lead\nto selective impairments: disabling identified \"recall circuits\" reduces\nfact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas\ndisabling \"reasoning circuits\" reduces multi-step inference by a comparable\nmargin. At the neuron level, we observe task-specific firing patterns, though\nthese effects are less robust, consistent with neuronal polysemanticity.Our\nresults provide the first causal evidence that recall and reasoning rely on\nseparable but interacting circuits in transformer models. These findings\nadvance mechanistic interpretability by linking circuit-level structure to\nfunctional specialization and demonstrate how controlled datasets and causal\ninterventions can yield mechanistic insights into model cognition, informing\nsafer deployment of large language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u56e0\u679c\u8bc1\u636e\u8868\u660eTransformer\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\u4f9d\u8d56\u4e8e\u53ef\u5206\u79bb\u4f46\u76f8\u4e92\u4f5c\u7528\u7684\u795e\u7ecf\u56de\u8def\u3002", "motivation": "\u533a\u5206\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u9884\u6d4b\u6a21\u578b\u6cdb\u5316\u3001\u8bbe\u8ba1\u9488\u5bf9\u6027\u8bc4\u4f30\u4ee5\u53ca\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5e72\u9884\u63aa\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e24\u79cd\u80fd\u529b\u662f\u5426\u4f9d\u8d56\u4e8e\u4e0d\u540c\u7684\u5185\u90e8\u673a\u5236\u3002", "method": "\u4f7f\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7ed3\u5408\u6fc0\u6d3b\u4fee\u8865\u548c\u7ed3\u6784\u5316\u6d88\u878d\u6280\u672f\uff0c\u5728\u53d7\u63a7\u7684\u5408\u6210\u8bed\u8a00\u8c1c\u9898\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u5c42\u3001\u5934\u548c\u795e\u7ecf\u5143\u7ea7\u522b\u63a2\u6d4bTransformer\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08Qwen\u548cLLaMA\uff09\u4e2d\uff0c\u53d1\u73b0\u5bf9\u7279\u5b9a\u5c42\u548c\u6ce8\u610f\u529b\u5934\u7684\u5e72\u9884\u4f1a\u5bfc\u81f4\u9009\u62e9\u6027\u635f\u4f24\uff1a\u7981\u7528\u8bc6\u522b\u7684\"\u8bb0\u5fc6\u56de\u8def\"\u4f7f\u4e8b\u5b9e\u68c0\u7d22\u51c6\u786e\u7387\u964d\u4f4e\u8fbe15%\u800c\u63a8\u7406\u80fd\u529b\u4fdd\u6301\u5b8c\u6574\uff0c\u7981\u7528\"\u63a8\u7406\u56de\u8def\"\u5219\u4f7f\u591a\u6b65\u63a8\u7406\u80fd\u529b\u964d\u4f4e\u7c7b\u4f3c\u5e45\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u8bb0\u5fc6\u548c\u63a8\u7406\u4f9d\u8d56\u4e8e\u53ef\u5206\u79bb\u4f46\u76f8\u4e92\u4f5c\u7528\u7684\u795e\u7ecf\u56de\u8def\uff0c\u8fd9\u63a8\u8fdb\u4e86\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u5c06\u56de\u8def\u7ea7\u7ed3\u6784\u4e0e\u529f\u80fd\u4e13\u4e1a\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u673a\u5236\u6027\u89c1\u89e3\u3002"}}
{"id": "2510.03375", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03375", "abs": "https://arxiv.org/abs/2510.03375", "authors": ["Renrong Shao", "Wei Zhang", "Jun wang"], "title": "Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation", "comment": "13 pages", "summary": "Data-free knowledge distillation~(DFKD) is an effective manner to solve model\ncompression and transmission restrictions while retaining privacy protection,\nwhich has attracted extensive attention in recent years. Currently, the\nmajority of existing methods utilize a generator to synthesize images to\nsupport the distillation. Although the current methods have achieved great\nsuccess, there are still many issues to be explored. Firstly, the outstanding\nperformance of supervised learning in deep learning drives us to explore a\npseudo-supervised paradigm on DFKD. Secondly, current synthesized methods\ncannot distinguish the distributions of different categories of samples, thus\nproducing ambiguous samples that may lead to an incorrect evaluation by the\nteacher. Besides, current methods cannot optimize the category-wise diversity\nsamples, which will hinder the student model learning from diverse samples and\nfurther achieving better performance. In this paper, to address the above\nlimitations, we propose a novel learning paradigm, i.e., conditional\npseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).\nThe primary innovations of CPSC-DFKD are: (1) introducing a conditional\ngenerative adversarial network to synthesize category-specific diverse images\nfor pseudo-supervised learning, (2) improving the modules of the generator to\ndistinguish the distributions of different categories, and (3) proposing\npseudo-supervised contrastive learning based on teacher and student views to\nenhance diversity. Comprehensive experiments on three commonly-used datasets\nvalidate the performance lift of both the student and generator brought by\nCPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5CPSC-DFKD\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5408\u6210\u7c7b\u522b\u7279\u5b9a\u7684\u591a\u6837\u5316\u56fe\u50cf\uff0c\u6539\u8fdb\u751f\u6210\u5668\u6a21\u5757\u4ee5\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u4f2a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u589e\u5f3a\u591a\u6837\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u4e09\u4e2a\u95ee\u9898\uff1a\u7f3a\u4e4f\u4f2a\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u3001\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u6837\u672c\u5206\u5e03\u3001\u65e0\u6cd5\u4f18\u5316\u7c7b\u522b\u591a\u6837\u6027\u6837\u672c\uff0c\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5408\u6210\u7c7b\u522b\u7279\u5b9a\u7684\u591a\u6837\u5316\u56fe\u50cf\uff1b\u6539\u8fdb\u751f\u6210\u5668\u6a21\u5757\u4ee5\u66f4\u597d\u5730\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u5206\u5e03\uff1b\u63d0\u51fa\u57fa\u4e8e\u6559\u5e08\u548c\u5b66\u751f\u89c6\u56fe\u7684\u4f2a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u589e\u5f3a\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CPSC-DFKD\u65b9\u6cd5\u5bf9\u5b66\u751f\u6a21\u578b\u548c\u751f\u6210\u5668\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "CPSC-DFKD\u901a\u8fc7\u5f15\u5165\u6761\u4ef6\u4f2a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u81ea\u7531\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u7684\u5173\u952e\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03380", "abs": "https://arxiv.org/abs/2510.03380", "authors": ["Michael Ben Ali", "Imen Megdiche", "Andr\u00e9 Peninou", "Olivier Teste"], "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew", "comment": null, "summary": "Federated Learning (FL) is a decentralized paradigm that enables a\nclient-server architecture to collaboratively train a global Artificial\nIntelligence model without sharing raw data, thereby preserving privacy. A key\nchallenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of\nNon-IID, where clients hold highly heterogeneous data volumes. Clustered\nFederated Learning (CFL) is an emergent variant of FL that presents a promising\nsolution to Non-IID problem. It improves models' performance by grouping\nclients with similar data distributions into clusters. CFL methods generally\nfall into two operating strategies. In the first strategy, clients select the\ncluster that minimizes the local training loss. In the second strategy, the\nserver groups clients based on local model similarities. However, most CFL\nmethods lack systematic evaluation under QS but present significant challenges\nbecause of it. In this paper, we present two main contributions. The first one\nis an evaluation of state-of-the-art CFL algorithms under various Non-IID\nsettings, applying multiple QS scenarios to assess their robustness. Our second\ncontribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes\nan optimal coordination between both operating strategies of CFL. Our approach\nis robust against the different variations of QS settings. We conducted\nintensive experiments on six image classification datasets, resulting in 270\nNon-IID configurations. The results show that CORNFLQS achieves the highest\naverage ranking in both accuracy and clustering quality, as well as strong\nrobustness to QS perturbations. Overall, our approach outperforms actual CFL\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5728\u6570\u91cf\u503e\u659c\u975eIID\u6570\u636e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fed\u4ee3\u7b97\u6cd5CORNFLQS\uff0c\u8be5\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u805a\u7c7b\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u975eIID\u6570\u636e\u7279\u522b\u662f\u6570\u91cf\u503e\u659c\u95ee\u9898\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6b64\u95ee\u9898\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86CORNFLQS\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u534f\u8c03\u4e24\u79cdCFL\u64cd\u4f5c\u7b56\u7565\uff08\u5ba2\u6237\u7aef\u9009\u62e9\u6700\u5c0f\u5316\u635f\u5931\u7684\u96c6\u7fa4\u548c\u670d\u52a1\u5668\u57fa\u4e8e\u6a21\u578b\u76f8\u4f3c\u6027\u5206\u7ec4\uff09\uff0c\u5e76\u57286\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c270\u79cd\u975eIID\u914d\u7f6e\u5b9e\u9a8c\u3002", "result": "CORNFLQS\u5728\u51c6\u786e\u6027\u548c\u805a\u7c7b\u8d28\u91cf\u65b9\u9762\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u6392\u540d\uff0c\u5bf9\u6570\u91cf\u503e\u659c\u6270\u52a8\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709CFL\u7b97\u6cd5\u3002", "conclusion": "CORNFLQS\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u91cf\u503e\u659c\u95ee\u9898\uff0c\u5728\u975eIID\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.03381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03381", "abs": "https://arxiv.org/abs/2510.03381", "authors": ["Yongchao Li", "Jun Chen", "Zhuoxuan Li", "Chao Gao", "Yang Li", "Chu Zhang", "Changyin Dong"], "title": "Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges", "comment": null, "summary": "Interchanges are crucial nodes for vehicle transfers between highways, yet\nthe lack of real-time ramp detectors creates blind spots in traffic prediction.\nTo address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a\ntwo-stage framework that leverages cross-modal reconstruction pretraining. In\nthe first stage, STDAE reconstructs historical ramp flows from mainline data,\nforcing the model to capture intrinsic spatio-temporal relations. Its decoupled\narchitecture with parallel spatial and temporal autoencoders efficiently\nextracts heterogeneous features. In the prediction stage, the learned\nrepresentations are integrated with models such as GWNet to enhance accuracy.\nExperiments on three real-world interchange datasets show that STDAE-GWNET\nconsistently outperforms thirteen state-of-the-art baselines and achieves\nperformance comparable to models using historical ramp data. This demonstrates\nits effectiveness in overcoming detector scarcity and its plug-and-play\npotential for diverse forecasting pipelines.", "AI": {"tldr": "\u63d0\u51faSTDAE\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u91cd\u5efa\u9884\u8bad\u7ec3\u89e3\u51b3\u9ad8\u901f\u516c\u8def\u531d\u9053\u7f3a\u4e4f\u5b9e\u65f6\u68c0\u6d4b\u5668\u7684\u95ee\u9898\uff0c\u5728\u9884\u6d4b\u9636\u6bb5\u7ed3\u5408GWNet\u7b49\u6a21\u578b\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u4e92\u901a\u7acb\u4ea4\u662f\u8f66\u8f86\u8f6c\u6362\u7684\u5173\u952e\u8282\u70b9\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u65f6\u531d\u9053\u68c0\u6d4b\u5668\u5bfc\u81f4\u4ea4\u901a\u9884\u6d4b\u5b58\u5728\u76f2\u533a\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u89e3\u8026\u81ea\u7f16\u7801\u5668\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u4e3b\u7ebf\u6570\u636e\u91cd\u5efa\u5386\u53f2\u531d\u9053\u6d41\u91cf\uff0c\u6355\u6349\u5185\u5728\u65f6\u7a7a\u5173\u7cfb\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e0eGWNet\u7b49\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u4e92\u901a\u7acb\u4ea4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTDAE-GWNET\u6301\u7eed\u4f18\u4e8e13\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u5386\u53f2\u531d\u9053\u6570\u636e\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u514b\u670d\u68c0\u6d4b\u5668\u7a00\u7f3a\u95ee\u9898\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9884\u6d4b\u6d41\u7a0b\u3002"}}
{"id": "2510.03394", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03394", "abs": "https://arxiv.org/abs/2510.03394", "authors": ["Donghwan Rho"], "title": "Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning", "comment": "10 pages", "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training large language models (LLMs) with stronger reasoning abilities. It\nhas also been applied to a variety of logic puzzles. In this work, we study the\nKorean word-chain game using RLVR. We show that rule-derived rewards can\nnaturally conflict, and demonstrate through experiments that a\ncurriculum-learning scheme mitigates these conflicts. Our findings motivate\nfurther studies of puzzle tasks in diverse languages.", "AI": {"tldr": "\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u8bad\u7ec3\u97e9\u8bed\u63a5\u9f99\u6e38\u620f\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7f13\u89e3\u89c4\u5219\u5956\u52b1\u51b2\u7a81", "motivation": "\u7814\u7a76RLVR\u5728\u591a\u8bed\u8a00\u903b\u8f91\u8c1c\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u97e9\u8bed\u63a5\u9f99\u6e38\u620f\u4e2d\u89c4\u5219\u5956\u52b1\u7684\u81ea\u7136\u51b2\u7a81\u95ee\u9898", "method": "\u4f7f\u7528RLVR\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6848\u6765\u7f13\u89e3\u89c4\u5219\u5956\u52b1\u4e4b\u95f4\u7684\u51b2\u7a81", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8bfe\u7a0b\u5b66\u4e60\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5956\u52b1\u51b2\u7a81\u95ee\u9898", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9f13\u52b1\u5728\u66f4\u591a\u4e0d\u540c\u8bed\u8a00\u7684\u8c1c\u9898\u4efb\u52a1\u4e2d\u5f00\u5c55\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2510.03416", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.03416", "abs": "https://arxiv.org/abs/2510.03416", "authors": ["Ashley Lenau", "Dennis Dimiduk", "Stephen R. Niezgoda"], "title": "Training Variation of Physically-Informed Deep Learning Models", "comment": null, "summary": "A successful deep learning network is highly dependent not only on the\ntraining dataset, but the training algorithm used to condition the network for\na given task. The loss function, dataset, and tuning of hyperparameters all\nplay an essential role in training a network, yet there is not much discussion\non the reliability or reproducibility of a training algorithm. With the rise in\npopularity of physics-informed loss functions, this raises the question of how\nreliable one's loss function is in conditioning a network to enforce a\nparticular boundary condition. Reporting the model variation is needed to\nassess a loss function's ability to consistently train a network to obey a\ngiven boundary condition, and provides a fairer comparison among different\nmethods. In this work, a Pix2Pix network predicting the stress fields of high\nelastic contrast composites is used as a case study. Several different loss\nfunctions enforcing stress equilibrium are implemented, with each displaying\ndifferent levels of variation in convergence, accuracy, and enforcing stress\nequilibrium across many training sessions. Suggested practices in reporting\nmodel variation are also shared.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u7279\u522b\u5173\u6ce8\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u5728\u8bad\u7ec3\u7f51\u7edc\u6267\u884c\u8fb9\u754c\u6761\u4ef6\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7Pix2Pix\u7f51\u7edc\u9884\u6d4b\u9ad8\u5f39\u6027\u5bf9\u6bd4\u590d\u5408\u6750\u6599\u5e94\u529b\u573a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u635f\u5931\u51fd\u6570\u7684\u6536\u655b\u53d8\u5f02\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u7684\u6d41\u884c\uff0c\u9700\u8981\u8bc4\u4f30\u635f\u5931\u51fd\u6570\u5728\u8bad\u7ec3\u7f51\u7edc\u6267\u884c\u7279\u5b9a\u8fb9\u754c\u6761\u4ef6\u65f6\u7684\u53ef\u9760\u6027\uff0c\u62a5\u544a\u6a21\u578b\u53d8\u5f02\u5bf9\u4e8e\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Pix2Pix\u7f51\u7edc\u9884\u6d4b\u9ad8\u5f39\u6027\u5bf9\u6bd4\u590d\u5408\u6750\u6599\u7684\u5e94\u529b\u573a\uff0c\u5b9e\u65bd\u591a\u79cd\u4e0d\u540c\u7684\u5e94\u529b\u5e73\u8861\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u591a\u6b21\u8bad\u7ec3\u4f1a\u8bdd\u5206\u6790\u5404\u635f\u5931\u51fd\u6570\u5728\u6536\u655b\u3001\u51c6\u786e\u6027\u548c\u5e94\u529b\u5e73\u8861\u6267\u884c\u65b9\u9762\u7684\u53d8\u5f02\u6c34\u5e73\u3002", "result": "\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u5728\u6536\u655b\u3001\u51c6\u786e\u6027\u548c\u5e94\u529b\u5e73\u8861\u6267\u884c\u65b9\u9762\u663e\u793a\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u53d8\u5f02\uff0c\u67d0\u4e9b\u635f\u5931\u51fd\u6570\u6bd4\u5176\u4ed6\u51fd\u6570\u66f4\u7a33\u5b9a\u53ef\u9760\u3002", "conclusion": "\u62a5\u544a\u6a21\u578b\u53d8\u5f02\u5bf9\u4e8e\u8bc4\u4f30\u635f\u5931\u51fd\u6570\u53ef\u9760\u6027\u548c\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u8bba\u6587\u5206\u4eab\u4e86\u62a5\u544a\u6a21\u578b\u53d8\u5f02\u7684\u5efa\u8bae\u5b9e\u8df5\u3002"}}
{"id": "2510.03419", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03419", "abs": "https://arxiv.org/abs/2510.03419", "authors": ["Joseph Rawson", "Domniki Ladopoulou", "Petros Dellaportas"], "title": "Multi-task neural diffusion processes for uncertainty-quantified wind power prediction", "comment": "36 pages, 13 figures, 2 tables,", "summary": "Uncertainty-aware wind power prediction is essential for grid integration and\nreliable wind farm operation. We apply neural diffusion processes (NDPs)-a\nrecent class of models that learn distributions over functions-and extend them\nto a multi-task NDP (MT-NDP) framework for wind power prediction. We provide\nthe first empirical evaluation of NDPs in real supervisory control and data\nacquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture\ncross-turbine correlations and enable few-shot adaptation to unseen turbines.\nThe proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of\npoint accuracy and calibration, particularly for wind turbines whose behaviour\ndeviates from the fleet average. In general, NDP-based models deliver\ncalibrated and scalable predictions suitable for operational deployment,\noffering sharper, yet trustworthy, predictive intervals that can support\ndispatch and maintenance decisions in modern wind farms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u795e\u7ecf\u6269\u6563\u8fc7\u7a0b\uff08MT-NDP\uff09\u6846\u67b6\u7528\u4e8e\u98ce\u529f\u7387\u9884\u6d4b\uff0c\u901a\u8fc7\u4efb\u52a1\u7f16\u7801\u5668\u6355\u6349\u8de8\u98ce\u673a\u76f8\u5173\u6027\uff0c\u5e76\u5728\u771f\u5b9eSCADA\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9996\u6b21\u5b9e\u8bc1\u8bc4\u4f30\u3002", "motivation": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u98ce\u529f\u7387\u9884\u6d4b\u5bf9\u4e8e\u7535\u7f51\u96c6\u6210\u548c\u98ce\u7535\u573a\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u4e14\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u6269\u5c55\u795e\u7ecf\u6269\u6563\u8fc7\u7a0b\uff08NDPs\uff09\u5230\u591a\u4efb\u52a1\u6846\u67b6MT-NDP\uff0c\u5f15\u5165\u4efb\u52a1\u7f16\u7801\u5668\u6355\u6349\u8de8\u98ce\u673a\u76f8\u5173\u6027\uff0c\u652f\u6301\u5bf9\u672a\u89c1\u98ce\u673a\u7684\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "MT-NDP\u5728\u70b9\u7cbe\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u5355\u4efb\u52a1NDPs\u548c\u9ad8\u65af\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u504f\u79bb\u673a\u7fa4\u5e73\u5747\u884c\u4e3a\u7684\u98ce\u673a\uff0c\u63d0\u4f9b\u66f4\u5c16\u9510\u4f46\u53ef\u4fe1\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u57fa\u4e8eNDP\u7684\u6a21\u578b\u63d0\u4f9b\u9002\u5408\u8fd0\u8425\u90e8\u7f72\u7684\u6821\u51c6\u548c\u53ef\u6269\u5c55\u9884\u6d4b\uff0c\u5176\u66f4\u5c16\u9510\u4f46\u53ef\u4fe1\u7684\u9884\u6d4b\u533a\u95f4\u53ef\u4ee5\u652f\u6301\u73b0\u4ee3\u98ce\u7535\u573a\u7684\u8c03\u5ea6\u548c\u7ef4\u62a4\u51b3\u7b56\u3002"}}
{"id": "2510.03425", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03425", "abs": "https://arxiv.org/abs/2510.03425", "authors": ["Congzheng Song", "Xinyu Tang"], "title": "Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices", "comment": null, "summary": "Fine-tuning large language models (LLMs) with backpropagation\\textemdash even\nfor a subset of parameters such as LoRA\\textemdash can be much more\nmemory-consuming than inference and is often deemed impractical for\nresource-constrained mobile devices. Alternative methods, such as zeroth-order\noptimization (ZO), can greatly reduce the memory footprint but come at the cost\nof significantly slower model convergence (10$\\times$ to 100$\\times$ more steps\nthan backpropagation). We propose a memory-efficient implementation of\nbackpropagation (MeBP) on mobile devices that provides better trade-off between\nmemory usage and compute time, while converging faster and achieving better\nperformance than the ZO baseline. We verify the effectiveness of MeBP on an\niPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B\nparameters, can be fine-tuned using less than 1GB of memory. We release an\nexample of the MeBP implementation at https://github.com/apple/ml-mebp.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5185\u5b58\u9ad8\u6548\u7684\u53cd\u5411\u4f20\u64ad\u5b9e\u73b0\u65b9\u6cd5(MeBP)\uff0c\u80fd\u591f\u5728\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u65f6\u95f4\u4e4b\u95f4\u63d0\u4f9b\u66f4\u597d\u7684\u6743\u8861\uff0c\u76f8\u6bd4\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u66f4\u597d\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\uff0c\u5373\u4f7f\u662f\u4f7f\u7528LoRA\u7b49\u53c2\u6570\u5b50\u96c6\u65b9\u6cd5\uff0c\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684LLM\u5fae\u8c03\u4ecd\u7136\u5185\u5b58\u6d88\u8017\u8fc7\u5927\uff0c\u800c\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u5185\u5b58\u5360\u7528\u4f4e\u4f46\u6536\u655b\u901f\u5ea6\u8fc7\u6162\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u53cd\u5411\u4f20\u64ad\u5b9e\u73b0(MeBP)\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u5185\u5b58\u6d88\u8017\u7684LLM\u5fae\u8c03\u3002", "result": "\u5728iPhone 15 Pro Max\u4e0a\u9a8c\u8bc1\u4e86MeBP\u7684\u6709\u6548\u6027\uff0c\u5404\u79cd0.5B\u52304B\u53c2\u6570\u7684LLM\u53ef\u4ee5\u4f7f\u7528\u4e0d\u52301GB\u5185\u5b58\u8fdb\u884c\u5fae\u8c03\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u5feb10-100\u500d\u3002", "conclusion": "MeBP\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2510.03426", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.03426", "abs": "https://arxiv.org/abs/2510.03426", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation", "comment": "18 pages, 4 figures (main text). 14 pages, 21 figures (appendix)", "summary": "Many domains, from deep learning to finance, require compounding real numbers\nover long sequences, often leading to catastrophic numerical underflow or\noverflow. We introduce generalized orders of magnitude (GOOMs), a principled\nextension of traditional orders of magnitude that incorporates floating-point\nnumbers as a special case, and which in practice enables stable computation\nover significantly larger dynamic ranges of real numbers than previously\npossible. We implement GOOMs, along with an efficient custom parallel prefix\nscan, to support native execution on parallel hardware such as GPUs. We\ndemonstrate that our implementation of GOOMs outperforms traditional approaches\nwith three representative experiments, all of which were previously considered\nimpractical or impossible, and now become possible and practical: (1)\ncompounding real matrix products far beyond standard floating-point limits; (2)\nestimating spectra of Lyapunov exponents in parallel, orders of magnitude\nfaster than with previous methods, applying a novel selective-resetting method\nto prevent state colinearity; and (3) capturing long-range dependencies in deep\nrecurrent neural networks with non-diagonal recurrent states, computed in\nparallel via a prefix scan, without requiring any form of stabilization. Our\nresults show that our implementation of GOOMs, combined with efficient parallel\nscanning, offers a scalable and numerically robust alternative to conventional\nfloating-point numbers for high-dynamic-range applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e7f\u4e49\u6570\u91cf\u7ea7\uff08GOOMs\uff09\u65b9\u6cd5\uff0c\u6269\u5c55\u4f20\u7edf\u6570\u91cf\u7ea7\u6982\u5ff5\uff0c\u652f\u6301\u5728\u66f4\u5927\u52a8\u6001\u8303\u56f4\u5185\u8fdb\u884c\u7a33\u5b9a\u6570\u503c\u8ba1\u7b97\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5b9e\u6570\u8fd0\u7b97\u4e2d\u7684\u6570\u503c\u4e0b\u6ea2/\u4e0a\u6ea2\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u3001\u91d1\u878d\u7b49\u9886\u57df\u9700\u8981\u5728\u957f\u5e8f\u5217\u4e0a\u8fdb\u884c\u5b9e\u6570\u590d\u5408\u8fd0\u7b97\uff0c\u4f46\u4f20\u7edf\u6d6e\u70b9\u6570\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6570\u503c\u4e0b\u6ea2\u6216\u4e0a\u6ea2\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u7684\u53ef\u80fd\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u6570\u91cf\u7ea7\uff08GOOMs\uff09\u6982\u5ff5\uff0c\u5c06\u6d6e\u70b9\u6570\u4f5c\u4e3a\u7279\u4f8b\u5305\u542b\u5728\u5185\uff0c\u5b9e\u73b0\u81ea\u5b9a\u4e49\u5e76\u884c\u524d\u7f00\u626b\u63cf\u7b97\u6cd5\uff0c\u652f\u6301\u5728GPU\u7b49\u5e76\u884c\u786c\u4ef6\u4e0a\u539f\u751f\u6267\u884c\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u8d85\u8d8a\u6807\u51c6\u6d6e\u70b9\u6570\u9650\u5236\u7684\u5b9e\u6570\u77e9\u9635\u4e58\u79ef\u590d\u5408\u8fd0\u7b97\uff1b\u5e76\u884c\u4f30\u8ba1Lyapunov\u6307\u6570\u8c31\uff0c\u901f\u5ea6\u6bd4\u4e4b\u524d\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff1b\u5728\u6df1\u5ea6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u4efb\u4f55\u7a33\u5b9a\u5316\u5904\u7406\u3002", "conclusion": "GOOMs\u7ed3\u5408\u9ad8\u6548\u5e76\u884c\u626b\u63cf\u4e3a\u9ad8\u52a8\u6001\u8303\u56f4\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6570\u503c\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u4e4b\u524d\u88ab\u8ba4\u4e3a\u4e0d\u5207\u5b9e\u9645\u6216\u4e0d\u53ef\u80fd\u7684\u8ba1\u7b97\u53d8\u5f97\u53ef\u80fd\u548c\u5b9e\u7528\u3002"}}
{"id": "2510.03432", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03432", "abs": "https://arxiv.org/abs/2510.03432", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "Xingquan Zhu"], "title": "LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation", "comment": "Accepted by ICDM 2025", "summary": "Learning from large heterogeneous graphs presents significant challenges due\nto the scale of networks, heterogeneity in node and edge types, variations in\nnodal features, and complex local neighborhood structures. This paper advocates\nfor ensemble learning as a natural solution to this problem, whereby training\nmultiple graph learners under distinct sampling conditions, the ensemble\ninherently captures different aspects of graph heterogeneity. Yet, the crux\nlies in combining these learners to meet global optimization objective while\nmaintaining computational efficiency on large-scale graphs. In response, we\npropose LHGEL, an ensemble framework that addresses these challenges through\nbatch sampling with three key components, namely batch view aggregation,\nresidual attention, and diversity regularization. Specifically, batch view\naggregation samples subgraphs and forms multiple graph views, while residual\nattention adaptively weights the contributions of these views to guide node\nembeddings toward informative subgraphs, thereby improving the accuracy of base\nlearners. Diversity regularization encourages representational disparity across\nembedding matrices derived from different views, promoting model diversity and\nensemble robustness. Our theoretical study demonstrates that residual attention\nmitigates gradient vanishing issues commonly faced in ensemble learning.\nEmpirical results on five real heterogeneous networks validate that our LHGEL\napproach consistently outperforms its state-of-the-art competitors by\nsubstantial margin. Codes and datasets are available at\nhttps://github.com/Chrisshen12/LHGEL.", "AI": {"tldr": "\u63d0\u51faLHGEL\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u5927\u89c4\u6a21\u5f02\u6784\u56fe\u5b66\u4e60\u6311\u6218\uff0c\u5305\u542b\u6279\u5904\u7406\u89c6\u56fe\u805a\u5408\u3001\u6b8b\u5dee\u6ce8\u610f\u529b\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5728\u4e94\u4e2a\u771f\u5b9e\u5f02\u6784\u7f51\u7edc\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u5f02\u6784\u56fe\u7684\u6311\u6218\u5305\u62ec\u7f51\u7edc\u89c4\u6a21\u5927\u3001\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\u5f02\u6784\u3001\u8282\u70b9\u7279\u5f81\u53d8\u5316\u4ee5\u53ca\u590d\u6742\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\uff0c\u96c6\u6210\u5b66\u4e60\u80fd\u591f\u81ea\u7136\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LHGEL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6279\u5904\u7406\u89c6\u56fe\u805a\u5408\uff08\u91c7\u6837\u5b50\u56fe\u5f62\u6210\u591a\u4e2a\u56fe\u89c6\u56fe\uff09\u3001\u6b8b\u5dee\u6ce8\u610f\u529b\uff08\u81ea\u9002\u5e94\u52a0\u6743\u89c6\u56fe\u8d21\u732e\u4ee5\u6307\u5bfc\u8282\u70b9\u5d4c\u5165\uff09\u548c\u591a\u6837\u6027\u6b63\u5219\u5316\uff08\u9f13\u52b1\u4e0d\u540c\u89c6\u56fe\u5d4c\u5165\u77e9\u9635\u7684\u8868\u793a\u5dee\u5f02\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u5f02\u6784\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLHGEL\u65b9\u6cd5\u59cb\u7ec8\u4ee5\u663e\u8457\u4f18\u52bf\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b\u3002\u7406\u8bba\u7814\u7a76\u8868\u660e\u6b8b\u5dee\u6ce8\u610f\u529b\u7f13\u89e3\u4e86\u96c6\u6210\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "conclusion": "LHGEL\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5f02\u6784\u56fe\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.03437", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03437", "abs": "https://arxiv.org/abs/2510.03437", "authors": ["Jairo Diaz-Rodriguez", "Mumin Jia"], "title": "Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation", "comment": null, "summary": "Kernel change-point detection (KCPD) has become a widely used tool for\nidentifying structural changes in complex data. While existing theory\nestablishes consistency under independence assumptions, real-world sequential\ndata such as text exhibits strong dependencies. We establish new guarantees for\nKCPD under $m$-dependent data: specifically, we prove consistency in the number\nof detected change points and weak consistency in their locations under mild\nadditional assumptions. We perform an LLM-based simulation that generates\nsynthetic $m$-dependent text to validate the asymptotics. To complement these\nresults, we present the first comprehensive empirical study of KCPD for text\nsegmentation with modern embeddings. Across diverse text datasets, KCPD with\ntext embeddings outperforms baselines in standard text segmentation metrics. We\ndemonstrate through a case study on Taylor Swift's tweets that KCPD not only\nprovides strong theoretical and simulated reliability but also practical\neffectiveness for text segmentation tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6838\u53d8\u70b9\u68c0\u6d4b(KCPD)\u5728\u6587\u672c\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u5728m-\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7LLM\u6a21\u62df\u548c\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86KCPD\u5728\u6587\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684KCPD\u7406\u8bba\u4e3b\u8981\u57fa\u4e8e\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u5e8f\u5217\u6570\u636e\uff08\u5982\u6587\u672c\uff09\u5b58\u5728\u5f3a\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u7b26\u5408\u5b9e\u9645\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5efa\u7acb\u4e86KCPD\u5728m-\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u7406\u8bba\u4fdd\u8bc1\uff1b\u4f7f\u7528LLM\u751f\u6210\u5408\u6210m-\u4f9d\u8d56\u6587\u672c\u6765\u9a8c\u8bc1\u6e10\u8fd1\u6027\u8d28\uff1b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c06KCPD\u4e0e\u73b0\u4ee3\u6587\u672c\u5d4c\u5165\u7ed3\u5408\u5e94\u7528\u4e8e\u6587\u672c\u5206\u5272\u3002", "result": "\u8bc1\u660e\u4e86KCPD\u5728m-\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u53d8\u70b9\u6570\u91cf\u4e00\u81f4\u6027\u548c\u4f4d\u7f6e\u5f31\u4e00\u81f4\u6027\uff1b\u5b9e\u8bc1\u7814\u7a76\u8868\u660eKCPD\u5728\u591a\u79cd\u6587\u672c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u901a\u8fc7Taylor Swift\u63a8\u6587\u6848\u4f8b\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "KCPD\u4e0d\u4ec5\u5177\u6709\u7406\u8bba\u53ef\u9760\u6027\uff0c\u5728\u5b9e\u9645\u6587\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f9d\u8d56\u6570\u636e\u7684\u53d8\u70b9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\u3002"}}
{"id": "2510.03442", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03442", "abs": "https://arxiv.org/abs/2510.03442", "authors": ["Ege Cakar", "Per Ola Kristensson"], "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents", "comment": "8 pages, 4 figures, 6 tables, submitted to IAAI-26", "summary": "Humans are black boxes -- we cannot observe their neural processes, yet\nsociety functions by evaluating verifiable arguments. AI explainability should\nfollow this principle: stakeholders need verifiable reasoning chains, not\nmechanistic transparency. We propose using structured argumentation to provide\na level of explanation and verification neither interpretability nor\nLLM-generated explanation is able to offer. Our pipeline achieves\nstate-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7\npoints above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous\npublished results with comparable data setups, for Argumentative MicroTexts\nrelation classification, converting LLM text into argument graphs and enabling\nverification at each inferential step. We demonstrate this idea on multi-agent\nrisk assessment using the Structured What-If Technique, where specialized\nagents collaborate transparently to carry out risk assessment otherwise\nachieved by humans alone. Using Bipolar Assumption-Based Argumentation, we\ncapture support/attack relationships, thereby enabling automatic hallucination\ndetection via fact nodes attacking arguments. We also provide a verification\nmechanism that enables iterative refinement through test-time feedback without\nretraining. For easy deployment, we provide a Docker container for the\nfine-tuned AMT model, and the rest of the code with the Bipolar ABA Python\npackage on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548cLLM\u751f\u6210\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728\u8bba\u8bc1\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u98ce\u9669\u8bc4\u4f30\u573a\u666f\u3002", "motivation": "\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\uff0c\u4f46\u793e\u4f1a\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u8bba\u8bc1\u6765\u8fd0\u4f5c\u3002AI\u53ef\u89e3\u91ca\u6027\u5e94\u9075\u5faa\u8fd9\u4e00\u539f\u5219\uff0c\u4e3a\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u94fe\uff0c\u800c\u975e\u673a\u5236\u900f\u660e\u5ea6\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\uff0c\u5c06LLM\u6587\u672c\u8f6c\u6362\u4e3a\u8bba\u8bc1\u56fe\uff0c\u901a\u8fc7\u53cc\u6781\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\u6355\u83b7\u652f\u6301/\u653b\u51fb\u5173\u7cfb\uff0c\u5b9e\u73b0\u81ea\u52a8\u5e7b\u89c9\u68c0\u6d4b\u548c\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728AAEC\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.44\u5b8fF1\uff08\u6bd4\u5148\u524d\u5de5\u4f5c\u9ad85.7\u5206\uff09\uff0c\u5728Argumentative MicroTexts\u5173\u7cfb\u5206\u7c7b\u4e0a\u8fbe\u52300.81\u5b8fF1\uff08\u6bd4\u53ef\u6bd4\u6570\u636e\u8bbe\u7f6e\u4e0b\u7684\u5148\u524d\u7ed3\u679c\u9ad8\u7ea60.07\uff09\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u53cd\u9988\u5b9e\u73b0\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2510.03470", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03470", "abs": "https://arxiv.org/abs/2510.03470", "authors": ["Benoit Dherin", "Michael Munn"], "title": "On residual network depth", "comment": null, "summary": "Deep residual architectures, such as ResNet and the Transformer, have enabled\nmodels of unprecedented depth, yet a formal understanding of why depth is so\neffective remains an open question. A popular intuition, following Veit et al.\n(2016), is that these residual networks behave like ensembles of many shallower\nmodels. Our key finding is an explicit analytical formula that verifies this\nensemble perspective, proving that increasing network depth is mathematically\nequivalent to expanding the size of this implicit ensemble. Furthermore, our\nexpansion reveals a hierarchical ensemble structure in which the combinatorial\ngrowth of computation paths leads to an explosion in the output signal,\nexplaining the historical necessity of normalization layers in training deep\nmodels. This insight offers a first principles explanation for the historical\ndependence on normalization layers and sheds new light on a family of\nsuccessful normalization-free techniques like SkipInit and Fixup. However,\nwhile these previous approaches infer scaling factors through optimizer\nanalysis or a heuristic analogy to Batch Normalization, our work offers the\nfirst explanation derived directly from the network's inherent functional\nstructure. Specifically, our Residual Expansion Theorem reveals that scaling\neach residual module provides a principled solution to taming the combinatorial\nexplosion inherent to these architectures. We further show that this scaling\nacts as a capacity controls that also implicitly regularizes the model's\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6b8b\u5dee\u5c55\u5f00\u5b9a\u7406\u8bc1\u660e\u4e86\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5728\u6570\u5b66\u4e0a\u7b49\u4ef7\u4e8e\u6d45\u5c42\u7f51\u7edc\u7684\u9690\u5f0f\u96c6\u6210\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u8def\u5f84\u7ec4\u5408\u7206\u70b8\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u5f52\u4e00\u5316\u5c42\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u4e3a\u4ec0\u4e48\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08\u5982ResNet\u548cTransformer\uff09\u5982\u6b64\u6709\u6548\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u9700\u8981\u5f52\u4e00\u5316\u5c42\u6765\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u5c55\u5f00\u5b9a\u7406\uff0c\u901a\u8fc7\u5206\u6790\u6b8b\u5dee\u7f51\u7edc\u7684\u51fd\u6570\u7ed3\u6784\uff0c\u8bc1\u660e\u6df1\u5ea6\u589e\u52a0\u5728\u6570\u5b66\u4e0a\u7b49\u4ef7\u4e8e\u6269\u5927\u9690\u5f0f\u96c6\u6210\u89c4\u6a21\u3002", "result": "\u53d1\u73b0\u6b8b\u5dee\u7f51\u7edc\u5177\u6709\u5c42\u6b21\u5316\u96c6\u6210\u7ed3\u6784\uff0c\u8ba1\u7b97\u8def\u5f84\u7684\u7ec4\u5408\u589e\u957f\u4f1a\u5bfc\u81f4\u8f93\u51fa\u4fe1\u53f7\u7206\u70b8\uff0c\u8fd9\u89e3\u91ca\u4e86\u5f52\u4e00\u5316\u5c42\u7684\u5386\u53f2\u5fc5\u8981\u6027\u3002", "conclusion": "\u6b8b\u5dee\u6a21\u5757\u7684\u7f29\u653e\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u6765\u63a7\u5236\u7ec4\u5408\u7206\u70b8\uff0c\u540c\u65f6\u4f5c\u4e3a\u5bb9\u91cf\u63a7\u5236\u673a\u5236\u9690\u5f0f\u6b63\u5219\u5316\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.03478", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.03478", "abs": "https://arxiv.org/abs/2510.03478", "authors": ["Quan Nguyen"], "title": "How to Set $\u03b2_1, \u03b2_2$ in Adam: An Online Learning Perspective", "comment": "15 pages", "summary": "While Adam is one of the most effective optimizer for training large-scale\nmachine learning models, a theoretical understanding of how to optimally set\nits momentum factors, $\\beta_1$ and $\\beta_2$, remains largely incomplete.\n  Prior works have shown that Adam can be seen as an instance of\nFollow-the-Regularized-Leader (FTRL), one of the most important class of\nalgorithms in online learning.\n  The prior analyses in these works required setting $\\beta_1 =\n\\sqrt{\\beta_2}$, which does not cover the more practical cases with $\\beta_1\n\\neq \\sqrt{\\beta_2}$.\n  We derive novel, more general analyses that hold for both $\\beta_1 \\geq\n\\sqrt{\\beta_2}$ and $\\beta_1 \\leq \\sqrt{\\beta_2}$.\n  In both cases, our results strictly generalize the existing bounds.\n  Furthermore, we show that our bounds are tight in the worst case.\n  We also prove that setting $\\beta_1 = \\sqrt{\\beta_2}$ is optimal for an\noblivious adversary, but sub-optimal for an non-oblivious adversary.", "AI": {"tldr": "\u672c\u6587\u5bf9Adam\u4f18\u5316\u5668\u4e2d\u52a8\u91cf\u53c2\u6570\u03b2\u2081\u548c\u03b2\u2082\u7684\u7406\u8bba\u5206\u6790\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u7a81\u7834\u4e86\u4e4b\u524d\u9700\u8981\u03b2\u2081=\u221a\u03b2\u2082\u7684\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u03b2\u2081\u2265\u221a\u03b2\u2082\u548c\u03b2\u2081\u2264\u221a\u03b2\u2082\u4e24\u79cd\u60c5\u51b5\u7684\u65b0\u5206\u6790\u6846\u67b6\u3002", "motivation": "Adam\u4f18\u5316\u5668\u5728\u8bad\u7ec3\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4f46\u5982\u4f55\u6700\u4f18\u8bbe\u7f6e\u5176\u52a8\u91cf\u53c2\u6570\u03b2\u2081\u548c\u03b2\u2082\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u4e0d\u5b8c\u6574\u3002\u5148\u524d\u7684\u7814\u7a76\u8981\u6c42\u03b2\u2081=\u221a\u03b2\u2082\uff0c\u8fd9\u65e0\u6cd5\u8986\u76d6\u66f4\u5b9e\u9645\u7684\u03b2\u2081\u2260\u221a\u03b2\u2082\u7684\u60c5\u51b5\u3002", "method": "\u5c06Adam\u89c6\u4e3aFollow-the-Regularized-Leader\u7b97\u6cd5\u5b9e\u4f8b\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u66f4\u4e00\u822c\u7684\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u03b2\u2081\u2265\u221a\u03b2\u2082\u548c\u03b2\u2081\u2264\u221a\u03b2\u2082\u4e24\u79cd\u60c5\u51b5\u3002", "result": "\u65b0\u5206\u6790\u7ed3\u679c\u4e25\u683c\u63a8\u5e7f\u4e86\u73b0\u6709\u8fb9\u754c\uff0c\u4e14\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u662f\u7d27\u7684\u3002\u8bc1\u660e\u5bf9\u4e8e\u65e0\u610f\u8bc6\u5bf9\u624b\u8bbe\u7f6e\u03b2\u2081=\u221a\u03b2\u2082\u662f\u6700\u4f18\u7684\uff0c\u4f46\u5bf9\u4e8e\u6709\u610f\u8bc6\u5bf9\u624b\u662f\u6b21\u4f18\u7684\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9Adam\u4f18\u5316\u5668\u52a8\u91cf\u53c2\u6570\u8bbe\u7f6e\u7684\u66f4\u5168\u9762\u7406\u8bba\u7406\u89e3\uff0c\u7a81\u7834\u4e86\u5148\u524d\u5206\u6790\u7684\u9650\u5236\uff0c\u4e3a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.03486", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03486", "abs": "https://arxiv.org/abs/2510.03486", "authors": ["Anupam Panwar", "Himadri Pal", "Jiali Chen", "Kyle Cho", "Riddick Jiang", "Miao Zhao", "Rajiv Krishnamurthy"], "title": "Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains", "comment": "11 pages, 7 figures", "summary": "Detecting anomalies in large, distributed systems presents several\nchallenges. The first challenge arises from the sheer volume of data that needs\nto be processed. Flagging anomalies in a high-throughput environment calls for\na careful consideration of both algorithm and system design. The second\nchallenge comes from the heterogeneity of time-series datasets that leverage\nsuch a system in production. In practice, anomaly detection systems are rarely\ndeployed for a single use case. Typically, there are several metrics to\nmonitor, often across several domains (e.g. engineering, business and\noperations). A one-size-fits-all approach rarely works, so these systems need\nto be fine-tuned for every application - this is often done manually. The third\nchallenge comes from the fact that determining the root-cause of anomalies in\nsuch settings is akin to finding a needle in a haystack. Identifying (in real\ntime) a time-series dataset that is associated causally with the anomalous\ntime-series data is a very difficult problem. In this paper, we describe a\nunified framework that addresses these challenges. Reasoning based Anomaly\nDetection Framework (RADF) is designed to perform real time anomaly detection\non very large datasets. This framework employs a novel technique (mSelect) that\nautomates the process of algorithm selection and hyper-parameter tuning for\neach use case. Finally, it incorporates a post-detection capability that allows\nfor faster triaging and root-cause determination. Our extensive experiments\ndemonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly\ndetection models in AUC performance for 5 out of 9 public benchmarking\ndatasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a\ndistinction unmatched by any other state-of-the-art model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6RADF\uff0c\u901a\u8fc7mSelect\u6280\u672f\u81ea\u52a8\u9009\u62e9\u7b97\u6cd5\u548c\u8c03\u53c2\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6570\u636e\u91cf\u3001\u5f02\u6784\u6027\u548c\u6839\u56e0\u5206\u6790\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u6d77\u91cf\u6570\u636e\u5904\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u7684\u5f02\u6784\u6027\uff0c\u4ee5\u53ca\u5f02\u5e38\u6839\u56e0\u5206\u6790\u7684\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86RADF\u6846\u67b6\uff0c\u91c7\u7528mSelect\u6280\u672f\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7528\u4f8b\u9009\u62e9\u7b97\u6cd5\u548c\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u5e76\u5305\u542b\u540e\u68c0\u6d4b\u80fd\u529b\u4ee5\u52a0\u901f\u95ee\u9898\u6392\u67e5\u548c\u6839\u56e0\u786e\u5b9a\u3002", "result": "\u57289\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\uff0cRADF\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u7684AUC\u6027\u80fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5176\u4e2d7\u4e2a\u6570\u636e\u96c6\u7684AUC\u8d85\u8fc70.85\uff0c\u8fd9\u662f\u5176\u4ed6\u6a21\u578b\u65e0\u6cd5\u8fbe\u5230\u7684\u6210\u5c31\u3002", "conclusion": "RADF\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u548c\u53c2\u6570\u8c03\u4f18\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03494", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03494", "abs": "https://arxiv.org/abs/2510.03494", "authors": ["Volodymyr Tkachuk", "Csaba Szepesv\u00e1ri", "Xiaoqi Tan"], "title": "Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^\u03c0$-Realizability and Concentrability", "comment": null, "summary": "We study finite-horizon offline reinforcement learning (RL) with function\napproximation for both policy evaluation and policy optimization. Prior work\nestablished that statistically efficient learning is impossible for either of\nthese problems when the only assumptions are that the data has good coverage\n(concentrability) and the state-action value function of every policy is\nlinearly realizable ($q^\\pi$-realizability) (Foster et al., 2021). Recently,\nTkachuk et al. (2024) gave a statistically efficient learner for policy\noptimization, if in addition the data is assumed to be given as trajectories.\nIn this work we present a statistically efficient learner for policy evaluation\nunder the same assumptions. Further, we show that the sample complexity of the\nlearner used by Tkachuk et al. (2024) for policy optimization can be improved\nby a tighter analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6709\u9650\u65f6\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5728\u8f68\u8ff9\u6570\u636e\u548cq\u03c0-\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\u7684\u7edf\u8ba1\u9ad8\u6548\u5b66\u4e60\u5668\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u51ed\u6570\u636e\u8986\u76d6\u6027\u548cq\u03c0-\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u65e0\u6cd5\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u5b66\u4e60\u3002\u6700\u8fd1\u6709\u5de5\u4f5c\u8bc1\u660e\u4e86\u5728\u8f68\u8ff9\u6570\u636e\u4e0b\u7b56\u7565\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7b56\u7565\u8bc4\u4f30\u95ee\u9898\u5e76\u6539\u8fdb\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\u3002", "method": "\u5728\u8f68\u8ff9\u6570\u636e\u548cq\u03c0-\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u5f00\u53d1\u4e86\u7edf\u8ba1\u9ad8\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u5bf9\u73b0\u6709\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u66f4\u4e25\u683c\u7684\u5206\u6790\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u7edf\u8ba1\u9ad8\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u8bc1\u660e\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u53ef\u4ee5\u901a\u8fc7\u66f4\u4e25\u683c\u7684\u5206\u6790\u5f97\u5230\u6539\u8fdb\u3002", "conclusion": "\u5728\u8f68\u8ff9\u6570\u636e\u548cq\u03c0-\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u90fd\u53ef\u4ee5\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u5b66\u4e60\uff0c\u4e14\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2510.03508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03508", "abs": "https://arxiv.org/abs/2510.03508", "authors": ["Lunjun Zhang", "Shuo Han", "Hanrui Lyu", "Bradly C Stadie"], "title": "D2 Actor Critic: Diffusion Actor Meets Distributional Critic", "comment": null, "summary": "We introduce D2AC, a new model-free reinforcement learning (RL) algorithm\ndesigned to train expressive diffusion policies online effectively. At its core\nis a policy improvement objective that avoids the high variance of typical\npolicy gradients and the complexity of backpropagation through time. This\nstable learning process is critically enabled by our second contribution: a\nrobust distributional critic, which we design through a fusion of\ndistributional RL and clipped double Q-learning. The resulting algorithm is\nhighly effective, achieving state-of-the-art performance on a benchmark of\neighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,\nspanning both dense-reward and goal-conditioned RL scenarios. Beyond standard\nbenchmarks, we also evaluate a biologically motivated predator-prey task to\nexamine the behavioral robustness and generalization capacity of our approach.", "AI": {"tldr": "D2AC\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7a33\u5b9a\u7b56\u7565\u6539\u8fdb\u76ee\u6807\u548c\u9c81\u68d2\u5206\u5e03\u8bc4\u8bba\u5bb6\uff0c\u572818\u4e2a\u56f0\u96beRL\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u65b9\u5dee\u9ad8\u548c\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u590d\u6742\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5728\u7ebf\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u7b56\u7565\u6539\u8fdb\u76ee\u6807\uff08\u907f\u514d\u9ad8\u65b9\u5dee\u7b56\u7565\u68af\u5ea6\u548cBPTT\u590d\u6742\u6027\uff09\u4e0e\u878d\u5408\u5206\u5e03RL\u548c\u88c1\u526a\u53ccQ\u5b66\u4e60\u7684\u9c81\u68d2\u5206\u5e03\u8bc4\u8bba\u5bb6\u3002", "result": "\u5728Humanoid\u3001Dog\u3001Shadow Hand\u7b4918\u4e2a\u56f0\u96beRL\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6db5\u76d6\u5bc6\u96c6\u5956\u52b1\u548c\u76ee\u6807\u6761\u4ef6RL\u573a\u666f\u3002", "conclusion": "D2AC\u7b97\u6cd5\u901a\u8fc7\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b\u548c\u9c81\u68d2\u5206\u5e03\u8bc4\u8bba\u5bb6\u8bbe\u8ba1\uff0c\u5728\u590d\u6742RL\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u884c\u4e3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03509", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03509", "abs": "https://arxiv.org/abs/2510.03509", "authors": ["Kristi Topollai", "Anna Choromanska"], "title": "Task-Level Contrastiveness for Cross-Domain Few-Shot Learning", "comment": null, "summary": "Few-shot classification and meta-learning methods typically struggle to\ngeneralize across diverse domains, as most approaches focus on a single\ndataset, failing to transfer knowledge across various seen and unseen domains.\nExisting solutions often suffer from low accuracy, high computational costs,\nand rely on restrictive assumptions. In this paper, we introduce the notion of\ntask-level contrastiveness, a novel approach designed to address issues of\nexisting methods. We start by introducing simple ways to define task\naugmentations, and thereafter define a task-level contrastive loss that\nencourages unsupervised clustering of task representations. Our method is\nlightweight and can be easily integrated within existing few-shot/meta-learning\nalgorithms while providing significant benefits. Crucially, it leads to\nimproved generalization and computational efficiency without requiring prior\nknowledge of task domains. We demonstrate the effectiveness of our approach\nthrough different experiments on the MetaDataset benchmark, where it achieves\nsuperior performance without additional complexity.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u589e\u5f3a\u548c\u5bf9\u6bd4\u635f\u5931\u6539\u8fdb\u5c0f\u6837\u672c\u5206\u7c7b\u548c\u5143\u5b66\u4e60\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728MetaDataset\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd", "motivation": "\u73b0\u6709\u5c0f\u6837\u672c\u5206\u7c7b\u548c\u5143\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u8de8\u57df\u6cdb\u5316\uff0c\u5b58\u5728\u7cbe\u5ea6\u4f4e\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u9650\u5236\u6027\u5047\u8bbe\u7b49\u95ee\u9898", "method": "\u5f15\u5165\u4efb\u52a1\u7ea7\u5bf9\u6bd4\u6027\u6982\u5ff5\uff0c\u5b9a\u4e49\u4efb\u52a1\u589e\u5f3a\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4efb\u52a1\u7ea7\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u4fc3\u8fdb\u4efb\u52a1\u8868\u793a\u7684\u65e0\u76d1\u7763\u805a\u7c7b\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7b97\u6cd5\u4e2d", "result": "\u5728MetaDataset\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u65e0\u9700\u4efb\u52a1\u57df\u7684\u5148\u9a8c\u77e5\u8bc6", "conclusion": "\u4efb\u52a1\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8de8\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u590d\u6742\u5ea6"}}
{"id": "2510.03515", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03515", "abs": "https://arxiv.org/abs/2510.03515", "authors": ["Lianghuan Huang", "Sagnik Anupam", "Insup Lee", "Shuo Li", "Osbert Bastani"], "title": "RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising strategy for\nfinetuning small language models (SLMs) to solve targeted tasks such as math\nand coding. However, RL algorithms tend to be resource-intensive, taking a\nsignificant amount of time to train. We propose RAPID, a novel RL algorithm\nthat can substantially reduce the running time of RL. Our key insight is that\nRL tends to be costly due to the need to perform both inference and\nbackpropagation during training. To maximize use of computational resources,\nour algorithm performs inference in large batches, and then performs off-policy\npolicy gradient updates in mini-batches. For off-policy updates, we incorporate\ngroup advantage estimation into the policy gradient algorithm, and derive an\nimportance weighted estimator to correct for the bias arising from off-policy\nlearning. Our experiments demonstrate that our algorithm can reduce running\ntime by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms\nwhile maintaining similar or better accuracy.", "AI": {"tldr": "RAPID\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5927\u6279\u91cf\u63a8\u7406\u548c\u79bb\u7ebf\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\uff0c\u663e\u8457\u51cf\u5c11\u5c0f\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5fae\u8c03\u5c0f\u8bed\u8a00\u6a21\u578b\u65f6\u8d44\u6e90\u6d88\u8017\u5927\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u3002", "method": "\u91c7\u7528\u5927\u6279\u91cf\u63a8\u7406\u548c\u79bb\u7ebf\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\uff0c\u7ed3\u5408\u7ec4\u4f18\u52bf\u4f30\u8ba1\u548c\u91cd\u8981\u6027\u52a0\u6743\u4f30\u8ba1\u6765\u7ea0\u6b63\u79bb\u7ebf\u5b66\u4e60\u504f\u5dee\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u4e8611%-34%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u6216\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "RAPID\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.03520", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03520", "abs": "https://arxiv.org/abs/2510.03520", "authors": ["Kartik Pandit", "Sourav Ganguly", "Arnesh Banerjee", "Shaahin Angizi", "Arnob Ghosh"], "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models", "comment": null, "summary": "Ensuring safety is a foundational requirement for large language models\n(LLMs). Achieving an appropriate balance between enhancing the utility of model\noutputs and mitigating their potential for harm is a complex and persistent\nchallenge. Contemporary approaches frequently formalize this problem within the\nframework of Constrained Markov Decision Processes (CMDPs) and employ\nestablished CMDP optimization techniques. However, these methods exhibit two\nnotable limitations. First, their reliance on reward and cost functions renders\nperformance highly sensitive to the underlying scoring mechanism, which must\ncapture semantic meaning rather than being triggered by superficial keywords.\nSecond, CMDP-based training entails tuning dual-variable, a process that is\nboth computationally expensive and does not provide any provable safety\nguarantee for a fixed dual variable that can be exploitable through adversarial\njailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF\n(CS-RLHF) that introduces a cost model trained on a large-scale corpus to\nassign semantically grounded safety scores. In contrast to the lagrangian-based\napproach, CS-RLHF adopts a rectified penalty-based formulation. This design\ndraws on the theory of exact penalty functions in constrained optimization,\nwherein constraint satisfaction is enforced directly through a suitably chosen\npenalty term. With an appropriately scaled penalty, feasibility of the safety\nconstraints can be guaranteed at the optimizer, eliminating the need for\ndual-variable updates. Empirical evaluation demonstrates that CS-RLHF\noutperforms state-of-the-art LLM model responses rendering at-least 5 times\nefficient against nominal and jail-breaking prompts", "AI": {"tldr": "CS-RLHF\u662f\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u4f18\u5316\u6846\u67b6\u89e3\u51b3LLM\u5b89\u5168\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edfCMDP\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edfCMDP\u65b9\u6cd5\u5728LLM\u5b89\u5168\u5e73\u8861\u4e0a\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bf9\u8bc4\u5206\u673a\u5236\u654f\u611f\u4e14\u4f9d\u8d56\u5173\u952e\u8bcd\u89e6\u53d1\uff0c\u4ee5\u53ca\u53cc\u53d8\u91cf\u8c03\u4f18\u8ba1\u7b97\u6602\u8d35\u4e14\u65e0\u6cd5\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5927\u89c4\u6a21\u8bed\u6599\u8bad\u7ec3\u7684\u6210\u672c\u6a21\u578b\u6765\u5206\u914d\u8bed\u4e49\u5b89\u5168\u5206\u6570\uff0c\u91c7\u7528\u4fee\u6b63\u7684\u57fa\u4e8e\u60e9\u7f5a\u7684\u4f18\u5316\u516c\u5f0f\uff0c\u501f\u9274\u7ea6\u675f\u4f18\u5316\u4e2d\u7684\u7cbe\u786e\u60e9\u7f5a\u51fd\u6570\u7406\u8bba\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aCS-RLHF\u5728\u5bf9\u6297\u666e\u901a\u548c\u8d8a\u72f1\u63d0\u793a\u65f6\uff0c\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\u54cd\u5e94\u81f3\u5c11\u9ad8\u65485\u500d\u3002", "conclusion": "CS-RLHF\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2510.03535", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03535", "abs": "https://arxiv.org/abs/2510.03535", "authors": ["William Anderson", "Seung Whan Chung", "Youngsoo Choi"], "title": "Sequential decoder training for improved latent space dynamics identification", "comment": null, "summary": "Accurate numerical solutions of partial differential equations are essential\nin many scientific fields but often require computationally expensive solvers,\nmotivating reduced-order models (ROMs). Latent Space Dynamics Identification\n(LaSDI) is a data-driven ROM framework that combines autoencoders with equation\ndiscovery to learn interpretable latent dynamics. However, enforcing latent\ndynamics during training can compromise reconstruction accuracy of the model\nfor simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that\nimproves reconstruction and prediction accuracy by sequentially learning\nadditional decoders to correct residual errors from previous stages. Applied to\nthe 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,\nachieving lower prediction errors and reduced training time across a wide range\nof architectures.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5LaSDI\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u5b66\u4e60\u989d\u5916\u89e3\u7801\u5668\u6765\u4fee\u6b63\u6b8b\u5dee\u8bef\u5dee\uff0c\u63d0\u5347\u91cd\u5efa\u548c\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edfLaSDI\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u5b9e\u65bd\u6f5c\u5728\u52a8\u529b\u5b66\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u62df\u6570\u636e\u7684\u91cd\u5efa\u7cbe\u5ea6", "method": "\u591a\u9636\u6bb5LaSDI\u6846\u67b6\uff0c\u987a\u5e8f\u5b66\u4e60\u989d\u5916\u89e3\u7801\u5668\u6765\u4fee\u6b63\u524d\u9636\u6bb5\u7684\u6b8b\u5dee\u8bef\u5dee", "result": "\u57281D-1V Vlasov\u65b9\u7a0b\u5e94\u7528\u4e2d\uff0cmLaSDI\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6LaSDI\uff0c\u5b9e\u73b0\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\u548c\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4", "conclusion": "\u591a\u9636\u6bb5LaSDI\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u91cd\u5efa\u548c\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4"}}
{"id": "2510.03566", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.03566", "abs": "https://arxiv.org/abs/2510.03566", "authors": ["Ashwin Prabu", "Nhat Thanh Tran", "Guofa Zhou", "Jack Xin"], "title": "CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "A variety of models have been developed to forecast dengue cases to date.\nHowever, it remains a challenge to predict major dengue outbreaks that need\ntimely public warnings the most. In this paper, we introduce CrossLag, an\nenvironmentally informed attention that allows for the incorporation of lagging\nendogenous signals behind the significant events in the exogenous data into the\narchitecture of the transformer at low parameter counts. Outbreaks typically\nlag behind major changes in climate and oceanic anomalies. We use TimeXer, a\nrecent general-purpose transformer distinguishing exogenous-endogenous inputs,\nas the baseline for this study. Our proposed model outperforms TimeXer by a\nconsiderable margin in detecting and predicting major outbreaks in Singapore\ndengue data over a 24-week prediction window.", "AI": {"tldr": "\u63d0\u51faCrossLag\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u6574\u5408\u5916\u751f\u6570\u636e\u4e2d\u7684\u6ede\u540e\u5185\u751f\u4fe1\u53f7\u6765\u6539\u8fdb\u767b\u9769\u70ed\u75ab\u60c5\u9884\u6d4b\uff0c\u572824\u5468\u9884\u6d4b\u7a97\u53e3\u5185\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u9700\u8981\u53ca\u65f6\u516c\u5171\u536b\u751f\u9884\u8b66\u7684\u91cd\u5927\u767b\u9769\u70ed\u75ab\u60c5\u7206\u53d1\uff0c\u800c\u75ab\u60c5\u901a\u5e38\u6ede\u540e\u4e8e\u6c14\u5019\u548c\u6d77\u6d0b\u5f02\u5e38\u7684\u91cd\u5927\u53d8\u5316\u3002", "method": "\u5f00\u53d1CrossLag\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u5916\u751f\u6570\u636e\u4e2d\u663e\u8457\u4e8b\u4ef6\u80cc\u540e\u7684\u6ede\u540e\u5185\u751f\u4fe1\u53f7\u4ee5\u4f4e\u53c2\u6570\u6210\u672c\u6574\u5408\u5230transformer\u67b6\u6784\u4e2d\uff0c\u57fa\u4e8eTimeXer\u6a21\u578b\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u5728\u65b0\u52a0\u5761\u767b\u9769\u70ed\u6570\u636e\u4e0a\uff0c\u6240\u63d0\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u9884\u6d4b\u91cd\u5927\u75ab\u60c5\u65b9\u9762\u6bd4TimeXer\u6a21\u578b\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "CrossLag\u673a\u5236\u80fd\u6709\u6548\u5229\u7528\u73af\u5883\u6570\u636e\u4e2d\u7684\u6ede\u540e\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u91cd\u5927\u767b\u9769\u70ed\u75ab\u60c5\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.03567", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.03567", "abs": "https://arxiv.org/abs/2510.03567", "authors": ["Fatmazohra Rezkellah", "Ramzi Dakhmouche"], "title": "Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs", "comment": null, "summary": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6743\u91cd\u5e72\u9884\u6765\u540c\u65f6\u5b9e\u73b0LLM\u7684\u654f\u611f\u4fe1\u606f\u9057\u5fd8\u548c\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u4f7f\u7528\u5206\u7c7b\u5668\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u548c\u5b89\u5168\u751f\u6210\uff0c\u7279\u522b\u662f\u5728\u654f\u611f\u4fe1\u606f\u9057\u5fd8\u548c\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u7684\u6743\u91cd\u5e72\u9884\u4f7f\u7279\u5b9a\u8bcd\u6c47\u96c6\u4e0d\u53ef\u8fbe\u6216\u5c06\u90e8\u5206\u6743\u91cd\u8f6c\u79fb\u5230\u66f4\u5b89\u5168\u7684\u533a\u57df\uff0c\u5b9e\u73b0\u7edf\u4e00\u5904\u7406\u3002", "result": "\u63d0\u51fa\u7684\u70b9\u5f0f\u7ea6\u675f\u5e72\u9884\u65b9\u6cd5\u6bd4\u6700\u5927\u6700\u5c0f\u5e72\u9884\u8868\u73b0\u66f4\u597d\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u5728\u5bf9\u6297\u6700\u5148\u8fdb\u9632\u5fa1\u65b9\u6cd5\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7edf\u4e00\u5904\u7406\u654f\u611f\u4fe1\u606f\u9057\u5fd8\u548c\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5206\u7c7b\u5668\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002"}}
{"id": "2510.03569", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03569", "abs": "https://arxiv.org/abs/2510.03569", "authors": ["Mohammad Mohaiminul Islam", "Thijs P. Kuipers", "Sharvaree Vadgama", "Coen de Vente", "Afsana Khan", "Clara I. S\u00e1nchez", "Erik J. Bekkers"], "title": "Longitudinal Flow Matching for Trajectory Modeling", "comment": null, "summary": "Generative models for sequential data often struggle with sparsely sampled\nand high-dimensional trajectories, typically reducing the learning of dynamics\nto pairwise transitions. We propose \\textit{Interpolative Multi-Marginal Flow\nMatching} (IMMFM), a framework that learns continuous stochastic dynamics\njointly consistent with multiple observed time points. IMMFM employs a\npiecewise-quadratic interpolation path as a smooth target for flow matching and\njointly optimizes drift and a data-driven diffusion coefficient, supported by a\ntheoretical condition for stable learning. This design captures intrinsic\nstochasticity, handles irregular sparse sampling, and yields subject-specific\ntrajectories. Experiments on synthetic benchmarks and real-world longitudinal\nneuroimaging datasets show that IMMFM outperforms existing methods in both\nforecasting accuracy and further downstream tasks.", "AI": {"tldr": "\u63d0\u51faIMMFM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65f6\u95f4\u70b9\u8054\u5408\u5b66\u4e60\u8fde\u7eed\u968f\u673a\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u5206\u6bb5\u4e8c\u6b21\u63d2\u503c\u8def\u5f84\u4f5c\u4e3a\u6d41\u5339\u914d\u76ee\u6807\uff0c\u5171\u540c\u4f18\u5316\u6f02\u79fb\u548c\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u7cfb\u6570\u3002", "motivation": "\u89e3\u51b3\u5e8f\u5217\u6570\u636e\u751f\u6210\u6a21\u578b\u5728\u7a00\u758f\u91c7\u6837\u548c\u9ad8\u7ef4\u8f68\u8ff9\u5b66\u4e60\u4e2d\u7684\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5c06\u52a8\u6001\u5b66\u4e60\u7b80\u5316\u4e3a\u6210\u5bf9\u8f6c\u6362\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u4e8c\u6b21\u63d2\u503c\u8def\u5f84\u4f5c\u4e3a\u6d41\u5339\u914d\u7684\u5e73\u6ed1\u76ee\u6807\uff0c\u8054\u5408\u4f18\u5316\u6f02\u79fb\u548c\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u7cfb\u6570\uff0c\u6709\u7406\u8bba\u6761\u4ef6\u652f\u6301\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u7eb5\u5411\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cIMMFM\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IMMFM\u80fd\u591f\u6355\u6349\u5185\u5728\u968f\u673a\u6027\uff0c\u5904\u7406\u4e0d\u89c4\u5219\u7a00\u758f\u91c7\u6837\uff0c\u5e76\u751f\u6210\u7279\u5b9a\u5bf9\u8c61\u7684\u8f68\u8ff9\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.03571", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.2.7; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.03571", "abs": "https://arxiv.org/abs/2510.03571", "authors": ["Burak Karabulut", "Carlo Manna", "Chris Develder"], "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection", "comment": "This paper has been submitted and accepted for IEEE SmartGridComm\n  2025", "summary": "Fault detection in power distribution grids is critical for ensuring system\nreliability and preventing costly outages. Moreover, fault detection\nmethodologies should remain robust to evolving grid topologies caused by\nfactors such as reconfigurations, equipment failures, and Distributed Energy\nResource (DER) integration. Current data-driven state-of-the-art methods use\nRecurrent Neural Networks (RNNs) for temporal modeling and Graph Neural\nNetworks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in\nshort). Specifically, for power system fault diagnosis, Graph Convolutional\nNetworks (GCNs) have been adopted. Yet, various more advanced GNN architectures\nhave been proposed and adopted in domains outside of power systems. In this\npaper, we set out to systematically and consistently benchmark various GNN\narchitectures in an RNN+GNN pipeline model. Specifically, to the best of our\nknowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention\n(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive\nbenchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN\nmodels (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring\ntheir generalization potential for deployment in different settings than those\nused for training them. Our experimental results on the IEEE 123-node\ndistribution network show that RGATv2 has superior generalization capabilities,\nmaintaining high performance with an F1-score reduction of $\\sim$12% across\ndifferent topology settings. In contrast, pure RNN models largely fail,\nexperiencing an F1-score reduction of up to $\\sim$60%, while other RGNN\nvariants also exhibit significant performance degradation, i.e., up to\n$\\sim$25% lower F1-scores.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u4e0d\u540c\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RGATv2\u6a21\u578b\u5728\u62d3\u6251\u53d8\u5316\u60c5\u51b5\u4e0b\u5177\u6709\u6700\u4f73\u6cdb\u5316\u80fd\u529b\uff0cF1\u5206\u6570\u4ec5\u4e0b\u964d\u7ea612%\uff0c\u800c\u7eafRNN\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe60%\u3002", "motivation": "\u7535\u529b\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9002\u5e94\u7535\u7f51\u62d3\u6251\u7684\u52a8\u6001\u53d8\u5316\u3002\u5f53\u524d\u57fa\u4e8eRNN+GNN\u7684\u65b9\u6cd5\u4e2d\uff0cGCN\u67b6\u6784\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u9009\u62e9\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u5148\u8fdb\u7684GNN\u67b6\u6784\u3002", "method": "\u91c7\u7528RNN+GNN\u6d41\u6c34\u7ebf\u6a21\u578b\uff0c\u9996\u6b21\u5c06GraphSAGE\u3001GAT\u548cGATv2\u5e94\u7528\u4e8e\u6545\u969c\u8bca\u65ad\uff0c\u5e76\u4e0e\u73b0\u6709\u7684RGCN\u548c\u7eafRNN\u6a21\u578b\uff08\u7279\u522b\u662fGRU\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u7279\u522b\u5173\u6ce8\u6a21\u578b\u5728\u4e0d\u540c\u62d3\u6251\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728IEEE 123\u8282\u70b9\u914d\u7535\u7f51\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRGATv2\u5177\u6709\u6700\u4f18\u7684\u6cdb\u5316\u6027\u80fd\uff0cF1\u5206\u6570\u5728\u4e0d\u540c\u62d3\u6251\u8bbe\u7f6e\u4e0b\u4ec5\u4e0b\u964d\u7ea612%\uff1b\u7eafRNN\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe60%\uff1b\u5176\u4ed6RGNN\u53d8\u4f53\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe25%\u3002", "conclusion": "RGATv2\u67b6\u6784\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u8bca\u65ad\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u7535\u7f51\u62d3\u6251\u53d8\u5316\uff0c\u662f\u90e8\u7f72\u5230\u4e0d\u540c\u573a\u666f\u7684\u4f18\u9009\u65b9\u6848\u3002"}}
{"id": "2510.03574", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03574", "abs": "https://arxiv.org/abs/2510.03574", "authors": ["Mehmet Onurcan Kaya", "Desmond Elliott", "Dim P. Papadopoulos"], "title": "Efficient Test-Time Scaling for Small Vision-Language Models", "comment": null, "summary": "Small Vision-Language Models (VLMs) provide a computationally efficient\nalternative to larger models, at the cost of weaker generalization abilities\nand downstream task performance. These shortcomings could be addressed by\ntest-time scaling techniques, but existing methods are typically\ncomputationally demanding, contradicting the resource-efficient design goals of\nsmall models. To address these limitations, we propose two novel and efficient\ntest-time scaling strategies that leverage the model-internal features rather\nthan external supervision: (i) Test-Time Augmentation (TTAug), which generates\nmultiple augmented inputs and aggregates outputs at the token level without\nparameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model\nparameters during inference using consensus-based pseudolabels from TTAug.\nThrough extensive experiments across nine benchmarks, we demonstrate consistent\nperformance improvements while maintaining computational efficiency suitable\nfor resource-constrained environments. The generality of our approach is\ndemonstrated both within models at different scales and across different VLMs\nwithout additional tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565TTAug\u548cTTAdapt\uff0c\u5229\u7528\u6a21\u578b\u5185\u90e8\u7279\u5f81\u800c\u975e\u5916\u90e8\u76d1\u7763\u6765\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u6cdb\u5316\u80fd\u529b\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u8f83\u5f31\uff0c\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\uff0c\u8fdd\u80cc\u5c0f\u578b\u6a21\u578b\u7684\u8d44\u6e90\u9ad8\u6548\u8bbe\u8ba1\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff1a(1) TTAug\uff1a\u751f\u6210\u591a\u4e2a\u589e\u5f3a\u8f93\u5165\u5e76\u5728token\u7ea7\u522b\u805a\u5408\u8f93\u51fa\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff1b(2) TTAdapt\uff1a\u4f7f\u7528TTAug\u751f\u6210\u7684\u57fa\u4e8e\u5171\u8bc6\u7684\u4f2a\u6807\u7b7e\u5728\u63a8\u7406\u65f6\u8c03\u6574\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u548c\u4e0d\u540cVLM\u95f4\u5177\u6709\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2510.03576", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03576", "abs": "https://arxiv.org/abs/2510.03576", "authors": ["Bongseok Kim", "Jiahao Zhang", "Guang Lin"], "title": "BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems", "comment": "29 pages, 22 figures", "summary": "Deep learning has gained attention for solving PDEs, but the black-box nature\nof neural networks hinders precise enforcement of boundary conditions. To\naddress this, we propose a boundary condition-guaranteed evolutionary\nKolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,\nwe propose three distinct and combinable approaches for incorporating\nDirichlet, periodic, and Neumann boundary conditions into the network. For\nDirichlet problem, we use smooth and global Gaussian RBFs to construct\nunivariate basis functions for approximating the solution and to encode\nboundary information at the activation level of the network. To handle periodic\nproblems, we employ a periodic layer constructed from a set of sinusoidal\nfunctions to enforce the boundary conditions exactly. For a Neumann problem, we\ndevise a least-squares formulation to guide the parameter evolution toward\nsatisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the\nperiodic layer, and the evolutionary framework, we can perform accurate PDE\nsimulations while rigorously enforcing boundary conditions. For demonstration,\nwe conducted extensive numerical experiments on Dirichlet, Neumann, periodic,\nand mixed boundary value problems. The results indicate that BEKAN outperforms\nboth multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In\nconclusion, the proposed approach enhances the capability of KANs in solving\nPDE problems while satisfying boundary conditions, thereby facilitating\nadvancements in scientific computing and engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86BEKAN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f84\u5411\u57fa\u51fd\u6570\u548c\u8fdb\u5316\u6846\u67b6\u7cbe\u786e\u6267\u884c\u8fb9\u754c\u6761\u4ef6\uff0c\u5728PDE\u6c42\u89e3\u4e2d\u4f18\u4e8eMLP\u548cB\u6837\u6761KAN", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6c42\u89e3PDE\u65f6\u795e\u7ecf\u7f51\u7edc\u7684\u9ed1\u76d2\u7279\u6027\u96be\u4ee5\u7cbe\u786e\u6267\u884c\u8fb9\u754c\u6761\u4ef6\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u4e25\u683c\u4fdd\u8bc1\u8fb9\u754c\u6761\u4ef6\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u4e09\u79cd\u53ef\u7ec4\u5408\u65b9\u6cd5\uff1aDirichlet\u95ee\u9898\u7528\u9ad8\u65afRBF\u6784\u9020\u5355\u53d8\u91cf\u57fa\u51fd\u6570\uff0c\u5468\u671f\u95ee\u9898\u7528\u6b63\u5f26\u51fd\u6570\u6784\u9020\u5468\u671f\u5c42\uff0cNeumann\u95ee\u9898\u7528\u6700\u5c0f\u4e8c\u4e58\u516c\u5f0f\u6307\u5bfc\u53c2\u6570\u6f14\u5316", "result": "\u5728Dirichlet\u3001Neumann\u3001\u5468\u671f\u548c\u6df7\u5408\u8fb9\u754c\u503c\u95ee\u9898\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0cBEKAN\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u591a\u5c42\u611f\u77e5\u673a\u548cB\u6837\u6761KAN", "conclusion": "BEKAN\u589e\u5f3a\u4e86KAN\u5728\u6c42\u89e3PDE\u95ee\u9898\u65f6\u7684\u80fd\u529b\uff0c\u540c\u65f6\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u79d1\u5b66\u8ba1\u7b97\u548c\u5de5\u7a0b\u5e94\u7528\u7684\u53d1\u5c55"}}
{"id": "2510.03578", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03578", "abs": "https://arxiv.org/abs/2510.03578", "authors": ["Haoran Li", "Chenhan Xiao", "Muhao Guo", "Yang Weng"], "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning", "comment": "30 pages, 6 figures", "summary": "Learning dynamics is essential for model-based control and Reinforcement\nLearning in engineering systems, such as robotics and power systems. However,\nlimited system measurements, such as those from low-resolution sensors, demand\nsample-efficient learning. Symmetry provides a powerful inductive bias by\ncharacterizing equivariant relations in system states to improve sample\nefficiency. While recent methods attempt to discover symmetries from data, they\ntypically assume a single global symmetry group and treat symmetry discovery\nand dynamic learning as separate tasks, leading to limited expressiveness and\nerror accumulation. In this paper, we propose the Latent Mixture of Symmetries\n(Latent MoS), an expressive model that captures a mixture of symmetry-governed\nlatent factors from complex dynamical measurements. Latent MoS focuses on\ndynamic learning while locally and provably preserving the underlying symmetric\ntransformations. To further capture long-term equivariance, we introduce a\nhierarchical architecture that stacks MoS blocks. Numerical experiments in\ndiverse physical systems demonstrate that Latent MoS outperforms\nstate-of-the-art baselines in interpolation and extrapolation tasks while\noffering interpretable latent representations suitable for future geometric and\nsafety-critical analyses.", "AI": {"tldr": "\u63d0\u51fa\u4e86Latent Mixture of Symmetries (Latent MoS)\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349\u590d\u6742\u52a8\u6001\u6d4b\u91cf\u4e2d\u7684\u5bf9\u79f0\u6027\u6df7\u5408\u6f5c\u5728\u56e0\u5b50\uff0c\u63d0\u9ad8\u52a8\u6001\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u5168\u5c40\u5bf9\u79f0\u7fa4\uff0c\u5e76\u5c06\u5bf9\u79f0\u6027\u53d1\u73b0\u4e0e\u52a8\u6001\u5b66\u4e60\u5206\u79bb\uff0c\u5bfc\u81f4\u8868\u8fbe\u80fd\u529b\u6709\u9650\u548c\u8bef\u5dee\u7d2f\u79ef\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u6df7\u5408\u5bf9\u79f0\u6027\u7684\u8868\u8fbe\u6027\u6a21\u578b\u3002", "method": "Latent MoS\u6a21\u578b\u901a\u8fc7\u5c40\u90e8\u4e14\u53ef\u8bc1\u660e\u5730\u4fdd\u6301\u5e95\u5c42\u5bf9\u79f0\u53d8\u6362\u6765\u6355\u6349\u5bf9\u79f0\u6027\u652f\u914d\u7684\u6f5c\u5728\u56e0\u5b50\u6df7\u5408\u3002\u91c7\u7528\u5206\u5c42\u67b6\u6784\u5806\u53e0MoS\u5757\u4ee5\u6355\u83b7\u957f\u671f\u7b49\u53d8\u6027\u3002", "result": "\u5728\u591a\u79cd\u7269\u7406\u7cfb\u7edf\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0cLatent MoS\u5728\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u8868\u793a\u3002", "conclusion": "Latent MoS\u901a\u8fc7\u6df7\u5408\u5bf9\u79f0\u6027\u5efa\u6a21\u63d0\u9ad8\u4e86\u52a8\u6001\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u672a\u6765\u51e0\u4f55\u548c\u5b89\u5168\u5173\u952e\u5206\u6790\u3002"}}
{"id": "2510.03589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03589", "abs": "https://arxiv.org/abs/2510.03589", "authors": ["Ankit Bhardwaj", "Ananth Balashankar", "Lakshminarayanan Subramanian"], "title": "FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors", "comment": null, "summary": "Spatio-temporal sensor data is often sparse, noisy, and irregular, and\nexisting interpolation or learning methods struggle here because they either\nignore governing PDEs or do not scale. We introduce FieldFormer, a\ntransformer-based framework for mesh-free spatio-temporal field reconstruction\nthat combines data-driven flexibility with physics-based structure. For each\nquery, FieldFormer gathers a local neighborhood using a learnable\nvelocity-scaled distance metric, enabling anisotropic adaptation to different\npropagation regimes. Neighborhoods are built efficiently via per-batch offset\nrecomputation, and refined in an expectation-maximization style as the velocity\nscales evolve. Predictions are made by a local transformer encoder, and physics\nconsistency is enforced through autograd-based PDE residuals and\nboundary-specific penalties. Across three benchmarks--a scalar anisotropic heat\nequation, a vector-valued shallow-water system, and a realistic\nadvection-diffusion pollution simulation--FieldFormer consistently outperforms\nstrong baselines by more than 40%. Our results demonstrate that FieldFormer\nenables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field\nreconstruction from sparse (0.4%-2%) and noisy(10%) data.", "AI": {"tldr": "FieldFormer\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u3001\u566a\u58f0\u3001\u4e0d\u89c4\u5219\u65f6\u7a7a\u4f20\u611f\u5668\u6570\u636e\u4e2d\u8fdb\u884c\u65e0\u7f51\u683c\u573a\u91cd\u5efa\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7075\u6d3b\u6027\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u534740%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u63d2\u503c\u6216\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7a00\u758f\u3001\u566a\u58f0\u3001\u4e0d\u89c4\u5219\u7684\u65f6\u7a7a\u4f20\u611f\u5668\u6570\u636e\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u5ffd\u7565\u63a7\u5236PDE\uff0c\u8981\u4e48\u65e0\u6cd5\u6269\u5c55\u3002\u9700\u8981\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7075\u6d3b\u6027\u548c\u7269\u7406\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u901f\u5ea6\u7f29\u653e\u8ddd\u79bb\u5ea6\u91cf\u6784\u5efa\u5c40\u90e8\u90bb\u57df\uff0c\u91c7\u7528\u671f\u671b\u6700\u5927\u5316\u98ce\u683c\u4f18\u5316\uff0c\u901a\u8fc7\u5c40\u90e8Transformer\u7f16\u7801\u5668\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u68af\u5ea6PDE\u6b8b\u5dee\u548c\u8fb9\u754c\u7279\u5b9a\u60e9\u7f5a\u5f3a\u5236\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u6807\u91cf\u5404\u5411\u5f02\u6027\u70ed\u65b9\u7a0b\u3001\u77e2\u91cf\u6d45\u6c34\u7cfb\u7edf\u3001\u73b0\u5b9e\u5e73\u6d41\u6269\u6563\u6c61\u67d3\u6a21\u62df\uff09\u4e2d\uff0cFieldFormer\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd540%\u4ee5\u4e0a\uff0c\u80fd\u591f\u4ece\u7a00\u758f\uff080.4%-2%\uff09\u548c\u566a\u58f0\uff0810%\uff09\u6570\u636e\u5b9e\u73b0\u51c6\u786e\uff08RMSE<10^-2\uff09\u3001\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u573a\u91cd\u5efa\u3002", "conclusion": "FieldFormer\u8bc1\u660e\u4e86\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7075\u6d3b\u6027\u548c\u7269\u7406\u7ea6\u675f\u7684Transformer\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a00\u758f\u566a\u58f0\u65f6\u7a7a\u6570\u636e\u7684\u573a\u91cd\u5efa\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.03592", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03592", "abs": "https://arxiv.org/abs/2510.03592", "authors": ["Kehinde O. Aina", "Sehoon Ha"], "title": "Deep Reinforcement Learning for Multi-Agent Coordination", "comment": "11 pages, 8 figures, 1 table, presented at SWARM 2022, to be\n  published in Journal of Artificial Life and Robotics", "summary": "We address the challenge of coordinating multiple robots in narrow and\nconfined environments, where congestion and interference often hinder\ncollective task performance. Drawing inspiration from insect colonies, which\nachieve robust coordination through stigmergy -- modifying and interpreting\nenvironmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement\nLearning (S-MADRL) framework that leverages virtual pheromones to model local\nand social interactions, enabling decentralized emergent coordination without\nexplicit communication. To overcome the convergence and scalability limitations\nof existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum\nlearning, which decomposes complex tasks into progressively harder\nsub-problems. Simulation results show that our framework achieves the most\neffective coordination of up to eight agents, where robots self-organize into\nasymmetric workload distributions that reduce congestion and modulate group\nperformance. This emergent behavior, analogous to strategies observed in\nnature, demonstrates a scalable solution for decentralized multi-agent\ncoordination in crowded environments with communication constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u865a\u62df\u4fe1\u606f\u7d20\u7684S-MADRL\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u6d8c\u73b0\u534f\u8c03\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u72ed\u7a84\u62e5\u6324\u73af\u5883\u4e2d\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u62e5\u5835\u548c\u5e72\u6270\u95ee\u9898\uff0c\u53d7\u6606\u866b\u7fa4\u4f53\u901a\u8fc7\u73af\u5883\u75d5\u8ff9\u5b9e\u73b0\u9c81\u68d2\u534f\u8c03\u7684\u542f\u53d1\u3002", "method": "\u7ed3\u5408\u865a\u62df\u4fe1\u606f\u7d20\u5efa\u6a21\u5c40\u90e8\u548c\u793e\u4f1a\u4ea4\u4e92\u7684S-MADRL\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u6e10\u8fdb\u56f0\u96be\u7684\u5b50\u95ee\u9898\u3002", "result": "\u5728\u6700\u591a8\u4e2a\u667a\u80fd\u4f53\u7684\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u6700\u6709\u6548\u7684\u534f\u8c03\uff0c\u673a\u5668\u4eba\u81ea\u7ec4\u7ec7\u5f62\u6210\u975e\u5bf9\u79f0\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\uff0c\u51cf\u5c11\u62e5\u5835\u5e76\u8c03\u8282\u7fa4\u4f53\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u901a\u4fe1\u53d7\u9650\u7684\u62e5\u6324\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u534f\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u6d8c\u73b0\u51fa\u7c7b\u4f3c\u81ea\u7136\u754c\u7684\u534f\u8c03\u7b56\u7565\u3002"}}
{"id": "2510.03604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03604", "abs": "https://arxiv.org/abs/2510.03604", "authors": ["Yucheng Wang", "Mohamed Ragab", "Yubo Hou", "Zhenghua Chen", "Min Wu", "Xiaoli Li"], "title": "Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends", "comment": null, "summary": "Remaining Useful Life (RUL) prediction for turbofan engines plays a vital\nrole in predictive maintenance, ensuring operational safety and efficiency in\naviation. Although data-driven approaches using machine learning and deep\nlearning have shown potential, they face challenges such as limited data and\ndistribution shifts caused by varying operating conditions. Domain Adaptation\n(DA) has emerged as a promising solution, enabling knowledge transfer from\nsource domains with abundant data to target domains with scarce data while\nmitigating distributional shifts. Given the unique properties of turbofan\nengines, such as complex operating conditions, high-dimensional sensor data,\nand slower-changing signals, it is essential to conduct a focused review of DA\ntechniques specifically tailored to turbofan engines. To address this need,\nthis paper provides a comprehensive review of DA solutions for turbofan engine\nRUL prediction, analyzing key methodologies, challenges, and recent\nadvancements. A novel taxonomy tailored to turbofan engines is introduced,\norganizing approaches into methodology-based (how DA is applied),\nalignment-based (where distributional shifts occur due to operational\nvariations), and problem-based (why certain adaptations are needed to address\nspecific challenges). This taxonomy offers a multidimensional view that goes\nbeyond traditional classifications by accounting for the distinctive\ncharacteristics of turbofan engine data and the standard process of applying DA\ntechniques to this area. Additionally, we evaluate selected DA techniques on\nturbofan engine datasets, providing practical insights for practitioners and\nidentifying key challenges. Future research directions are identified to guide\nthe development of more effective DA techniques, advancing the state of RUL\nprediction for turbofan engines.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6da1\u6247\u53d1\u52a8\u673a\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u6da1\u6247\u53d1\u52a8\u673a\u7279\u6027\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u6280\u672f\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u6da1\u6247\u53d1\u52a8\u673aRUL\u9884\u6d4b\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u6709\u9650\u548c\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\u3002\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u80fd\u591f\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u6e90\u57df\u5411\u6570\u636e\u7a00\u7f3a\u7684\u76ee\u6807\u57df\u8f6c\u79fb\u77e5\u8bc6\uff0c\u540c\u65f6\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u6da1\u6247\u53d1\u52a8\u673a\u7684\u65b0\u5206\u7c7b\u6cd5\uff1a\u57fa\u4e8e\u65b9\u6cd5\u7684\u5206\u7c7b\uff08\u5982\u4f55\u5e94\u7528DA\uff09\u3001\u57fa\u4e8e\u5bf9\u9f50\u7684\u5206\u7c7b\uff08\u64cd\u4f5c\u53d8\u5316\u5bfc\u81f4\u7684\u5206\u5e03\u504f\u79fb\u4f4d\u7f6e\uff09\u3001\u57fa\u4e8e\u95ee\u9898\u7684\u5206\u7c7b\uff08\u4e3a\u4ec0\u4e48\u9700\u8981\u7279\u5b9a\u9002\u5e94\u6765\u89e3\u51b3\u7279\u5b9a\u6311\u6218\uff09\u3002\u8fd9\u79cd\u591a\u7ef4\u89c6\u89d2\u8d85\u8d8a\u4e86\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u5728\u6da1\u6247\u53d1\u52a8\u673a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u9009\u5b9a\u7684DA\u6280\u672f\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u6da1\u6247\u53d1\u52a8\u673aRUL\u9884\u6d4b\u7684DA\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u8bc6\u522b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2510.03613", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.m"], "pdf": "https://arxiv.org/pdf/2510.03613", "abs": "https://arxiv.org/abs/2510.03613", "authors": ["Meenakshi Manikandan", "Leilani Gilpin"], "title": "Explore the Loss space with Hill-ADAM", "comment": "14-15 pages", "summary": "This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus\ntowards escaping local minima in prescribed loss landscapes to find the global\nminimum. Hill-ADAM escapes minima by deterministically exploring the state\nspace. This eliminates uncertainty from random gradient updates in stochastic\nalgorithms while seldom converging at the first minimum that visits. In the\npaper we first derive an analytical approximation of the ADAM Optimizer step\nsize at a particular model state. From there define the primary condition\ndetermining ADAM limitations in escaping local minima. The proposed optimizer\nalgorithm Hill-ADAM alternates between error minimization and maximization. It\nmaximizes to escape the local minimum and minimizes again afterward. This\nalternation provides an overall exploration throughout the loss space. This\nallows the deduction of the global minimum's state. Hill-ADAM was tested with 5\nloss functions and 12 amber-saturated to cooler-shade image color correction\ninstances.", "AI": {"tldr": "Hill-ADAM\u662f\u4e00\u79cd\u4e13\u6ce8\u4e8e\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u5bfb\u627e\u5168\u5c40\u6700\u5c0f\u503c\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u63a2\u7d22\u72b6\u6001\u7a7a\u95f4\u548c\u4ea4\u66ff\u8fdb\u884c\u8bef\u5dee\u6700\u5c0f\u5316\u4e0e\u6700\u5927\u5316\u6765\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u3002", "motivation": "\u4f20\u7edfADAM\u4f18\u5316\u5668\u5728\u968f\u673a\u68af\u5ea6\u66f4\u65b0\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u800c\u65e0\u6cd5\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63a8\u5bfcADAM\u4f18\u5316\u5668\u6b65\u957f\u7684\u89e3\u6790\u8fd1\u4f3c\uff0c\u7136\u540e\u63d0\u51faHill-ADAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u8bef\u5dee\u6700\u5c0f\u5316\u548c\u6700\u5927\u5316\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u6700\u5927\u5316\u9636\u6bb5\u7528\u4e8e\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u6700\u5c0f\u5316\u9636\u6bb5\u7528\u4e8e\u6536\u655b\u5230\u66f4\u4f18\u89e3\u3002", "result": "\u57285\u4e2a\u635f\u5931\u51fd\u6570\u548c12\u4e2a\u56fe\u50cf\u8272\u5f69\u6821\u6b63\u5b9e\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86Hill-ADAM\u5728\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u548c\u5bfb\u627e\u5168\u5c40\u6700\u5c0f\u503c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Hill-ADAM\u901a\u8fc7\u786e\u5b9a\u6027\u63a2\u7d22\u548c\u8bef\u5dee\u6700\u5c0f\u5316-\u6700\u5927\u5316\u4ea4\u66ff\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u5e76\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u76f8\u6bd4\u4f20\u7edf\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u5168\u5c40\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03614", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03614", "abs": "https://arxiv.org/abs/2510.03614", "authors": ["Christopher Solinas", "Radovan Haluska", "David Sychrovsky", "Finbarr Timbers", "Nolan Bard", "Michael Buro", "Martin Schmid", "Nathan R. Sturtevant", "Michael Bowling"], "title": "Neural Bayesian Filtering", "comment": null, "summary": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining\ndistributions over hidden states, called beliefs, in partially observable\nsystems. NBF is trained to find a good latent representation of the beliefs\ninduced by a task. It maps beliefs to fixed-length embedding vectors, which\ncondition generative models for sampling. During filtering, particle-style\nupdates compute posteriors in this embedding space using incoming observations\nand the environment's dynamics. NBF combines the computational efficiency of\nclassical filters with the expressiveness of deep generative models - tracking\nrapidly shifting, multimodal beliefs while mitigating the risk of particle\nimpoverishment. We validate NBF in state estimation tasks in three partially\nobservable environments.", "AI": {"tldr": "\u63d0\u51faNeural Bayesian Filtering (NBF)\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u7ef4\u62a4\u9690\u85cf\u72b6\u6001\u7684\u5206\u5e03\u3002NBF\u7ed3\u5408\u4e86\u7ecf\u5178\u6ee4\u6ce2\u5668\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8ddf\u8e2a\u5feb\u901f\u53d8\u5316\u3001\u591a\u6a21\u6001\u4fe1\u5ff5\u65f6\u9762\u4e34\u7c92\u5b50\u8d2b\u5316\u98ce\u9669\uff0c\u9700\u8981\u7ed3\u5408\u7ecf\u5178\u6ee4\u6ce2\u6548\u7387\u548c\u6df1\u5ea6\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "method": "NBF\u8bad\u7ec3\u627e\u5230\u4efb\u52a1\u8bf1\u5bfc\u4fe1\u5ff5\u7684\u826f\u597d\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u4fe1\u5ff5\u6620\u5c04\u5230\u56fa\u5b9a\u957f\u5ea6\u5d4c\u5165\u5411\u91cf\uff0c\u4f7f\u7528\u7c92\u5b50\u5f0f\u66f4\u65b0\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728\u4e09\u4e2a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u7684\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86NBF\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8ddf\u8e2a\u5feb\u901f\u53d8\u5316\u7684\u591a\u6a21\u6001\u4fe1\u5ff5\u5e76\u51cf\u8f7b\u7c92\u5b50\u8d2b\u5316\u98ce\u9669\u3002", "conclusion": "NBF\u6210\u529f\u7ed3\u5408\u4e86\u7ecf\u5178\u6ee4\u6ce2\u5668\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03633", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03633", "abs": "https://arxiv.org/abs/2510.03633", "authors": ["An Vuong", "Susan Gauch"], "title": "Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis", "comment": "17th International Conference on Knowledge Discovery, Knowledge\n  Engineering and Knowledge Management (KDIR 2025), Marbella, Spain, Oct.\n  22-24, 2025 (to appear) Best Student Paper Finalist", "summary": "Accurately predicting short-term stock price movement remains a challenging\ntask due to the market's inherent volatility and sensitivity to investor\nsentiment. This paper discusses a deep learning framework that integrates\nemotion features extracted from tweet data with historical stock price\ninformation to forecast significant price changes on the following day. We\nutilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby\nenhancing the quality of emotion features derived from three emotion analysis\napproaches: a transformer-based DistilRoBERTa classifier from the Hugging Face\nlibrary and two lexicon-based methods using National Research Council Canada\n(NRC) resources. These features are combined with previous-day stock price data\nto train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,\nAAPL, and AMZN stocks show that all three emotion analysis methods improve the\naverage accuracy for predicting significant price movements, compared to the\nbaseline model using only historical stock prices, which yields an accuracy of\n13.5%. The DistilRoBERTa-based stock prediction model achieves the best\nperformance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced\nemotion analysis. These results demonstrate that using large language models to\npreprocess tweet content enhances the effectiveness of emotion analysis which\nin turn improves the accuracy of predicting significant stock price movements.", "AI": {"tldr": "\u4f7f\u7528Llama 3.1-8B-Instruct\u9884\u5904\u7406\u63a8\u6587\u6570\u636e\uff0c\u7ed3\u5408\u4e09\u79cd\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u63d0\u53d6\u60c5\u611f\u7279\u5f81\uff0c\u4e0e\u5386\u53f2\u80a1\u4ef7\u6570\u636e\u4e00\u8d77\u8bad\u7ec3LSTM\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80a1\u7968\u4ef7\u683c\u5927\u5e45\u6ce2\u52a8\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u77ed\u671f\u80a1\u7968\u4ef7\u683c\u53d8\u52a8\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5e02\u573a\u5177\u6709\u56fa\u6709\u7684\u6ce2\u52a8\u6027\u4e14\u5bf9\u6295\u8d44\u8005\u60c5\u7eea\u654f\u611f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u63a8\u6587\u60c5\u611f\u7279\u5f81\u548c\u5386\u53f2\u80a1\u4ef7\u4fe1\u606f\u6765\u6539\u8fdb\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528Meta\u7684Llama 3.1-8B-Instruct\u6a21\u578b\u9884\u5904\u7406\u63a8\u6587\u6570\u636e\uff0c\u91c7\u7528\u4e09\u79cd\u60c5\u611f\u5206\u6790\u65b9\u6cd5\uff1a\u57fa\u4e8etransformer\u7684DistilRoBERTa\u5206\u7c7b\u5668\u548c\u4e24\u79cd\u57fa\u4e8e\u8bcd\u5178\u7684\u65b9\u6cd5\uff08\u4f7f\u7528NRC\u8d44\u6e90\uff09\u3002\u5c06\u60c5\u611f\u7279\u5f81\u4e0e\u524d\u4e00\u65e5\u80a1\u4ef7\u6570\u636e\u7ed3\u5408\uff0c\u8bad\u7ec3LSTM\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728TSLA\u3001AAPL\u548cAMZN\u80a1\u7968\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u4e09\u79cd\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u90fd\u63d0\u9ad8\u4e86\u9884\u6d4b\u80a1\u4ef7\u5927\u5e45\u6ce2\u52a8\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002\u57fa\u7ebf\u6a21\u578b\uff08\u4ec5\u4f7f\u7528\u5386\u53f2\u80a1\u4ef7\uff09\u51c6\u786e\u7387\u4e3a13.5%\uff0c\u57fa\u4e8eDistilRoBERTa\u7684\u80a1\u7968\u9884\u6d4b\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f7f\u7528LLaMA\u589e\u5f3a\u60c5\u611f\u5206\u6790\u540e\u51c6\u786e\u7387\u4ece23.6%\u63d0\u5347\u81f338.5%\u3002", "conclusion": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u5904\u7406\u63a8\u6587\u5185\u5bb9\u80fd\u591f\u589e\u5f3a\u60c5\u611f\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u8fdb\u800c\u63d0\u9ad8\u9884\u6d4b\u80a1\u7968\u4ef7\u683c\u5927\u5e45\u6ce2\u52a8\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03636", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03636", "abs": "https://arxiv.org/abs/2510.03636", "authors": ["Rabeya Amin Jhuma", "Mostafa Mohaimen Akand Faisal"], "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse", "comment": null, "summary": "This study explored how in-context learning (ICL) in large language models\ncan be disrupted by data poisoning attacks in the setting of public health\nsentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small\nadversarial perturbations such as synonym replacement, negation insertion, and\nrandomized perturbation were introduced into the support examples. Even these\nminor manipulations caused major disruptions, with sentiment labels flipping in\nup to 67% of cases. To address this, a Spectral Signature Defense was applied,\nwhich filtered out poisoned examples while keeping the data's meaning and\nsentiment intact. After defense, ICL accuracy remained steady at around 46.7%,\nand logistic regression validation reached 100% accuracy, showing that the\ndefense successfully preserved the dataset's integrity. Overall, the findings\nextend prior theoretical studies of ICL poisoning to a practical, high-stakes\nsetting in public health discourse analysis, highlighting both the risks and\npotential defenses for robust LLM deployment. This study also highlights the\nfragility of ICL under attack and the value of spectral defenses in making AI\nsystems more reliable for health-related social media monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u516c\u5171\u536b\u751f\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5982\u4f55\u53d7\u5230\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u7834\u574f\u3002\u901a\u8fc7\u5f15\u5165\u5fae\u5c0f\u5bf9\u6297\u6027\u6270\u52a8\uff08\u5982\u540c\u4e49\u8bcd\u66ff\u6362\u3001\u5426\u5b9a\u63d2\u5165\u548c\u968f\u673a\u6270\u52a8\uff09\uff0c\u5373\u4f7f\u5728\u5c11\u91cf\u652f\u6301\u793a\u4f8b\u4e2d\u4e5f\u80fd\u5bfc\u81f4\u9ad8\u8fbe67%\u7684\u60c5\u611f\u6807\u7b7e\u7ffb\u8f6c\u3002\u91c7\u7528\u8c31\u7b7e\u540d\u9632\u5fa1\u65b9\u6cd5\u6210\u529f\u8fc7\u6ee4\u4e86\u6295\u6bd2\u793a\u4f8b\uff0c\u4fdd\u6301\u4e86\u6570\u636e\u5b8c\u6574\u6027\u3002", "motivation": "\u5c06\u5148\u524d\u5173\u4e8eICL\u6295\u6bd2\u7684\u7406\u8bba\u7814\u7a76\u6269\u5c55\u5230\u516c\u5171\u536b\u751f\u8bdd\u8bed\u5206\u6790\u8fd9\u4e00\u5b9e\u9645\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5065\u5eb7\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u76d1\u63a7\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u5728\u4eba\u7c7b\u504f\u80ba\u75c5\u6bd2\u63a8\u6587\u6570\u636e\u4e2d\u5f15\u5165\u4e09\u79cd\u5bf9\u6297\u6027\u6270\u52a8\uff1a\u540c\u4e49\u8bcd\u66ff\u6362\u3001\u5426\u5b9a\u63d2\u5165\u548c\u968f\u673a\u6270\u52a8\uff0c\u7136\u540e\u5e94\u7528\u8c31\u7b7e\u540d\u9632\u5fa1\u65b9\u6cd5\u6765\u8fc7\u6ee4\u6295\u6bd2\u793a\u4f8b\u3002", "result": "\u5fae\u5c0f\u6270\u52a8\u5bfc\u81f4\u9ad8\u8fbe67%\u7684\u60c5\u611f\u6807\u7b7e\u7ffb\u8f6c\uff1b\u9632\u5fa1\u540eICL\u51c6\u786e\u7387\u7a33\u5b9a\u572846.7%\uff0c\u903b\u8f91\u56de\u5f52\u9a8c\u8bc1\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u6210\u529f\u4fdd\u6301\u4e86\u6570\u636e\u96c6\u5b8c\u6574\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86ICL\u5728\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4ee5\u53ca\u8c31\u7b7e\u540d\u9632\u5fa1\u5728\u6784\u5efa\u66f4\u53ef\u9760\u7684AI\u7cfb\u7edf\u7528\u4e8e\u5065\u5eb7\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u76d1\u63a7\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u7a33\u5065\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u98ce\u9669\u8ba4\u77e5\u548c\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2510.03638", "categories": ["cs.LG", "cs.AI", "math.RT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03638", "abs": "https://arxiv.org/abs/2510.03638", "authors": ["Jialin Liu", "Lisang Ding", "Stanley Osher", "Wotao Yin"], "title": "Implicit Models: Expressive Power Scales with Test-Time Compute", "comment": null, "summary": "Implicit models, an emerging model class, compute outputs by iterating a\nsingle parameter block to a fixed point. This architecture realizes an\ninfinite-depth, weight-tied network that trains with constant memory,\nsignificantly reducing memory needs for the same level of performance compared\nto explicit models. While it is empirically known that these compact models can\noften match or even exceed larger explicit networks by allocating more\ntest-time compute, the underlying mechanism remains poorly understood.\n  We study this gap through a nonparametric analysis of expressive power. We\nprovide a strict mathematical characterization, showing that a simple and\nregular implicit operator can, through iteration, progressively express more\ncomplex mappings. We prove that for a broad class of implicit models, this\nprocess lets the model's expressive power scale with test-time compute,\nultimately matching a much richer function class. The theory is validated\nacross three domains: image reconstruction, scientific computing, and\noperations research, demonstrating that as test-time iterations increase, the\ncomplexity of the learned mapping rises, while the solution quality\nsimultaneously improves and stabilizes.", "AI": {"tldr": "\u9690\u5f0f\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u5355\u4e2a\u53c2\u6570\u5757\u5230\u56fa\u5b9a\u70b9\u6765\u8ba1\u7b97\u8f93\u51fa\uff0c\u5f62\u6210\u65e0\u9650\u6df1\u5ea6\u3001\u6743\u91cd\u5171\u4eab\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u8bad\u7ec3\u65f6\u5185\u5b58\u9700\u6c42\u6052\u5b9a\u3002\u7814\u7a76\u8868\u660e\uff0c\u9690\u5f0f\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u53ef\u4ee5\u6e10\u8fdb\u8868\u8fbe\u66f4\u590d\u6742\u7684\u6620\u5c04\uff0c\u5176\u8868\u8fbe\u80fd\u529b\u968f\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u6269\u5c55\uff0c\u6700\u7ec8\u5339\u914d\u66f4\u4e30\u5bcc\u7684\u51fd\u6570\u7c7b\u3002", "motivation": "\u9690\u5f0f\u6a21\u578b\u4f5c\u4e3a\u65b0\u5174\u6a21\u578b\u7c7b\u522b\uff0c\u867d\u7136\u7ecf\u9a8c\u4e0a\u5df2\u77e5\u901a\u8fc7\u5206\u914d\u66f4\u591a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u66f4\u5927\u7684\u663e\u5f0f\u7f51\u7edc\uff0c\u4f46\u5176\u5e95\u5c42\u673a\u5236\u4ecd\u7f3a\u4e4f\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u53c2\u6570\u5206\u6790\u6765\u7814\u7a76\u9690\u5f0f\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u6269\u5c55\u673a\u5236\u3002", "method": "\u91c7\u7528\u975e\u53c2\u6570\u5206\u6790\u6846\u67b6\uff0c\u5bf9\u9690\u5f0f\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u8fdb\u884c\u4e25\u683c\u6570\u5b66\u8868\u5f81\u3002\u8bc1\u660e\u7b80\u5355\u7684\u6b63\u5219\u9690\u5f0f\u7b97\u5b50\u901a\u8fc7\u8fed\u4ee3\u53ef\u4ee5\u6e10\u8fdb\u8868\u8fbe\u66f4\u590d\u6742\u7684\u6620\u5c04\uff0c\u5e76\u8bc1\u660e\u5bf9\u4e8e\u5e7f\u6cdb\u7684\u9690\u5f0f\u6a21\u578b\u7c7b\u522b\uff0c\u8868\u8fbe\u80fd\u529b\u968f\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u6269\u5c55\u3002", "result": "\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u79d1\u5b66\u8ba1\u7b97\u548c\u8fd0\u7b79\u5b66\u4e09\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u7406\u8bba\uff1a\u968f\u7740\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u6b21\u6570\u589e\u52a0\uff0c\u5b66\u4e60\u6620\u5c04\u7684\u590d\u6742\u6027\u4e0a\u5347\uff0c\u540c\u65f6\u89e3\u7684\u8d28\u91cf\u63d0\u9ad8\u5e76\u7a33\u5b9a\u3002", "conclusion": "\u9690\u5f0f\u6a21\u578b\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u7684\u589e\u52a0\u5b9e\u73b0\u8868\u8fbe\u80fd\u529b\u7684\u6e10\u8fdb\u6269\u5c55\uff0c\u8fd9\u4e3a\u7406\u89e3\u9690\u5f0f\u6a21\u578b\u7684\u6027\u80fd\u4f18\u52bf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03643", "abs": "https://arxiv.org/abs/2510.03643", "authors": ["Nicholas Carter", "Arkaprava Gupta", "Prateek Ganguli", "Benedikt Dietrich", "Vibhor Krishna", "Samarjit Chakraborty"], "title": "In-Vivo Training for Deep Brain Stimulation", "comment": null, "summary": "Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's\nDisease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL\nagents modulating the stimulation frequency and amplitude. But, these models\nrely on biomarkers that are not measurable in patients and are only present in\nbrain-on-chip (BoC) simulations. In this work, we present an RL-based DBS\napproach that adapts these stimulation parameters according to brain activity\nmeasurable in vivo. Using a TD3 based RL agent trained on a model of the basal\nganglia region of the brain, we see a greater suppression of biomarkers\ncorrelated with PD severity compared to modern clinical DBS implementations.\nOur agent outperforms the standard clinical approaches in suppressing PD\nbiomarkers while relying on information that can be measured in a real world\nenvironment, thereby opening up the possibility of training personalized RL\nagents specific to individual patient needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df1\u90e8\u8111\u523a\u6fc0\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u4f53\u5185\u6d4b\u91cf\u7684\u8111\u6d3b\u52a8\u6765\u8c03\u6574\u523a\u6fc0\u53c2\u6570\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e34\u5e8a\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6291\u5236\u5e15\u91d1\u68ee\u75c5\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684DBS\u65b9\u6cd5\u4f9d\u8d56\u65e0\u6cd5\u5728\u60a3\u8005\u4f53\u5185\u6d4b\u91cf\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4ec5\u9002\u7528\u4e8e\u8111\u82af\u7247\u6a21\u62df\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u4f7f\u7528\u53ef\u4f53\u5185\u6d4b\u91cf\u8111\u6d3b\u52a8\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u57fa\u5e95\u795e\u7ecf\u8282\u8111\u533a\u6a21\u578b\u4e0a\u8bad\u7ec3RL\u667a\u80fd\u4f53\uff0c\u6839\u636e\u53ef\u6d4b\u91cf\u7684\u8111\u6d3b\u52a8\u81ea\u9002\u5e94\u8c03\u6574\u523a\u6fc0\u9891\u7387\u548c\u5e45\u5ea6\u53c2\u6570\u3002", "result": "\u76f8\u6bd4\u73b0\u4ee3\u4e34\u5e8aDBS\u5b9e\u73b0\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6291\u5236\u4e0e\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u76f8\u5173\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u4e34\u5e8a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bad\u7ec3\u9488\u5bf9\u4e2a\u4f53\u60a3\u8005\u9700\u6c42\u7684\u4e2a\u6027\u5316RL\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\uff0c\u4e14\u4f9d\u8d56\u53ef\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u91cf\u7684\u4fe1\u606f\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.03648", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03648", "abs": "https://arxiv.org/abs/2510.03648", "authors": ["Huijing Zhang", "Muyang Cao", "Linshan Jiang", "Xin Du", "Di Yu", "Changze Lv", "Shuiguang Deng"], "title": "SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network", "comment": null, "summary": "Continuous learning of novel classes is crucial for edge devices to preserve\ndata privacy and maintain reliable performance in dynamic environments.\nHowever, the scenario becomes particularly challenging when data samples are\ninsufficient, requiring on-device few-shot class-incremental learning (FSCIL)\nto maintain consistent model performance. Although existing work has explored\nparameter-efficient FSCIL frameworks based on artificial neural networks\n(ANNs), their deployment is still fundamentally constrained by limited device\nresources. Inspired by neural mechanisms, Spiking neural networks (SNNs)\nprocess spatiotemporal information efficiently, offering lower energy\nconsumption, greater biological plausibility, and compatibility with\nneuromorphic hardware than ANNs. In this work, we present an SNN-based method\nfor On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We\nfirst propose sparsity-conditioned neuronal dynamics, in which most neurons\nremain stable while a subset stays active, thereby mitigating catastrophic\nforgetting. To further cope with spike non-differentiability in gradient\nestimation, we employ zeroth-order optimization. Moreover, during incremental\nlearning sessions, we enhance the discriminability of new classes through\nsubspace projection, which alleviates overfitting to novel classes. Extensive\nexperiments conducted on two standard benchmark datasets (CIFAR100 and\nMini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,\nand N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,\nspecifically achieving at least 4.01% improvement at the last incremental\nsession on Mini-ImageNet and 20% lower energy cost over baseline methods with\npractical implementation.", "AI": {"tldr": "\u63d0\u51faSAFA-SNN\u65b9\u6cd5\u7528\u4e8e\u8bbe\u5907\u7aef\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff0c\u901a\u8fc7\u7a00\u758f\u6761\u4ef6\u795e\u7ecf\u5143\u52a8\u6001\u548c\u96f6\u9636\u4f18\u5316\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u8109\u51b2\u4e0d\u53ef\u5fae\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u80fd\u8017\u66f4\u4f4e\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u7c7b\u522b\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u548c\u7ef4\u6301\u53ef\u9760\u6027\u80fd\uff0c\u4f46\u6570\u636e\u6837\u672c\u4e0d\u8db3\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bbe\u5907\u8d44\u6e90\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u66f4\u4f4e\u80fd\u8017\u548c\u66f4\u597d\u7684\u751f\u7269\u5408\u7406\u6027\u3002", "method": "\u63d0\u51faSAFA-SNN\u65b9\u6cd5\uff1a1\uff09\u7a00\u758f\u6761\u4ef6\u795e\u7ecf\u5143\u52a8\u6001\uff0c\u5927\u90e8\u5206\u795e\u7ecf\u5143\u4fdd\u6301\u7a33\u5b9a\uff0c\u5b50\u96c6\u4fdd\u6301\u6d3b\u8dc3\uff1b2\uff09\u4f7f\u7528\u96f6\u9636\u4f18\u5316\u5904\u7406\u8109\u51b2\u4e0d\u53ef\u5fae\u6027\uff1b3\uff09\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u589e\u5f3a\u65b0\u7c7b\u522b\u7684\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728CIFAR100\u3001Mini-ImageNet\u548c\u4e09\u4e2a\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAFA-SNN\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Mini-ImageNet\u6700\u540e\u4e00\u4e2a\u589e\u91cf\u4f1a\u8bdd\u4e2d\u81f3\u5c11\u63d0\u53474.01%\uff0c\u80fd\u8017\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4f4e20%\u3002", "conclusion": "SAFA-SNN\u4e3a\u8bbe\u5907\u7aef\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u80fd\u8017\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2510.03650", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "cs.NE", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.03650", "abs": "https://arxiv.org/abs/2510.03650", "authors": ["Amir Sadikov"], "title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design", "comment": null, "summary": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo\n(QMC) methods for high-dimensional integration. We cast two long-standing QMC\ndesign problems as program synthesis and solve them with an LLM-guided\nevolutionary loop that mutates and selects code under task-specific fitness:\n(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)\nchoosing Sobol' direction numbers that minimize randomized QMC error on\ndownstream integrands. Our two-phase procedure combines constructive code\nproposals with iterative numerical refinement. On finite sets, we rediscover\nknown optima in small 2D cases and set new best-known 2D benchmarks for N >=\n40, while matching most known 3D optima up to the proven frontier (N <= 8) and\nreporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'\nparameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)\nmean-squared error for several 32-dimensional option-pricing tasks relative to\nwidely used Joe--Kuo parameters, while preserving extensibility to any sample\nsize and compatibility with standard randomizations. Taken together, the\nresults demonstrate that LLM-driven evolutionary program synthesis can automate\nthe discovery of high-quality QMC constructions, recovering classical designs\nwhere they are optimal and improving them where finite-N structure matters.\nData and code are available at\nhttps://github.com/hockeyguy123/openevolve-star-discrepancy.git.", "AI": {"tldr": "\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u8fdb\u5316\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u89e3\u51b3\u51c6\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u4e2d\u7684\u4e24\u4e2a\u957f\u671f\u8bbe\u8ba1\u95ee\u9898\uff1a\u6784\u9020\u4f4e\u661f\u5dee\u5f02\u7684\u6709\u9650\u70b9\u96c6\u548c\u4f18\u5316Sobol\u65b9\u5411\u6570\uff0c\u57282D/3D\u70b9\u96c6\u548c32\u7ef4\u671f\u6743\u5b9a\u4ef7\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u51c6\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5728\u9ad8\u7ef4\u79ef\u5206\u4e2d\u4f9d\u8d56\u4f4e\u5dee\u5f02\u70b9\u96c6\u548c\u6570\u5b57\u5e8f\u5217\uff0c\u4f46\u6784\u9020\u6700\u4f18\u70b9\u96c6\u548c\u65b9\u5411\u6570\u4e00\u76f4\u662f\u957f\u671f\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u5316\u8fd9\u4e00\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7ed3\u5408\u6784\u9020\u6027\u4ee3\u7801\u63d0\u6848\u548c\u8fed\u4ee3\u6570\u503c\u4f18\u5316\uff0c\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u8fdb\u5316\u5faa\u73af\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u5ea6\u4e0b\u7a81\u53d8\u548c\u9009\u62e9\u4ee3\u7801\u3002", "result": "\u57282D\u70b9\u96c6\u4e0a\u4e3aN\u226540\u8bbe\u7f6e\u4e86\u65b0\u7684\u6700\u4f73\u57fa\u51c6\uff0c\u57283D\u4e2d\u5339\u914d\u4e86\u5df2\u77e5\u6700\u4f18\u89e3\u5e76\u62a5\u544a\u4e86\u6539\u8fdb\u7684\u57fa\u51c6\uff1b\u5728Sobol\u5e8f\u5217\u4f18\u5316\u4e2d\uff0c\u76f8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684Joe-Kuo\u53c2\u6570\uff0c\u572832\u7ef4\u671f\u6743\u5b9a\u4ef7\u4efb\u52a1\u4e2d\u6301\u7eed\u964d\u4f4e\u4e86\u968f\u673a\u51c6\u8499\u7279\u5361\u6d1b\u5747\u65b9\u8bef\u5dee\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u8fdb\u5316\u7a0b\u5e8f\u5408\u6210\u53ef\u4ee5\u81ea\u52a8\u5316\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684QMC\u6784\u9020\uff0c\u5728\u7ecf\u5178\u8bbe\u8ba1\u6700\u4f18\u65f6\u6062\u590d\u5b83\u4eec\uff0c\u5728\u6709\u9650N\u7ed3\u6784\u91cd\u8981\u65f6\u6539\u8fdb\u5b83\u4eec\u3002"}}
{"id": "2510.03657", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03657", "abs": "https://arxiv.org/abs/2510.03657", "authors": ["Aymeric Fabre"], "title": "Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast", "comment": null, "summary": "In electricity markets around the world, the ability to anticipate price\nmovements with precision can be the difference between profit and loss,\nespecially for fast-acting assets like battery energy storage systems (BESS).\nAs grid volatility increases due to renewables and market decentralisation,\noperators and forecasters alike face growing pressure to transform prediction\ninto strategy. Yet while forecast data is abundant, especially in advanced\nmarkets like Australia's National Electricity Market (NEM), its practical value\nin driving real-world BESS trading decisions remains largely unexplored. This\nthesis dives into that gap. This work addresses a key research question: Can\nthe accuracy of the Australian Energy Market Operator (AEMO) energy price\nforecasts be systematically leveraged to develop a reliable and profitable\nbattery energy storage system trading algorithm? Despite the availability of\nAEMO price forecasts, no existing framework evaluates their reliability or\nincorporates them into practical BESS trading strategies. By analysing patterns\nin forecast accuracy based on time of day, forecast horizon, and regional\nvariations, this project creates a novel, forecast-informed BESS trading model\nto optimise arbitrage financial returns. The performance of this\nforecast-driven algorithm is benchmarked against a basic trading algorithm with\nno knowledge of forecast data. The study further explores the potential of\nmachine learning techniques to predict future energy prices by enhancing AEMO\nforecasts to govern a more advanced trading strategy. The research outcomes\nwill inform future improvements in energy market trading models and promote\nmore efficient BESS integration into market operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528AEMO\u80fd\u6e90\u4ef7\u683c\u9884\u6d4b\u6765\u5f00\u53d1\u53ef\u9760\u7684BESS\u4ea4\u6613\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9884\u6d4b\u51c6\u786e\u6027\u6a21\u5f0f\u521b\u5efa\u4e86\u57fa\u4e8e\u9884\u6d4b\u7684BESS\u4ea4\u6613\u6a21\u578b\uff0c\u5e76\u4e0e\u65e0\u9884\u6d4b\u7684\u57fa\u672c\u7b97\u6cd5\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u968f\u7740\u7535\u7f51\u6ce2\u52a8\u6027\u589e\u52a0\uff0c\u867d\u7136\u9884\u6d4b\u6570\u636e\u4e30\u5bcc\uff0c\u4f46\u5176\u5728\u9a71\u52a8\u5b9e\u9645BESS\u4ea4\u6613\u51b3\u7b56\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u5229\u7528AEMO\u4ef7\u683c\u9884\u6d4b\u5f00\u53d1\u53ef\u9760\u4ea4\u6613\u7b56\u7565\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5206\u6790\u9884\u6d4b\u51c6\u786e\u6027\u6a21\u5f0f\uff08\u57fa\u4e8e\u65f6\u95f4\u3001\u9884\u6d4b\u8303\u56f4\u548c\u5730\u533a\u5dee\u5f02\uff09\uff0c\u521b\u5efa\u57fa\u4e8e\u9884\u6d4b\u7684BESS\u4ea4\u6613\u6a21\u578b\uff0c\u5e76\u4e0e\u65e0\u9884\u6d4b\u7684\u57fa\u672c\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u589e\u5f3aAEMO\u9884\u6d4b\u3002", "result": "\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u57fa\u4e8e\u9884\u6d4b\u7684BESS\u4ea4\u6613\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u5957\u5229\u8d22\u52a1\u56de\u62a5\uff0c\u5e76\u8bc4\u4f30\u4e86\u9884\u6d4b\u9a71\u52a8\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u65e0\u9884\u6d4b\u7b97\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u5c06\u4e3a\u80fd\u6e90\u5e02\u573a\u4ea4\u6613\u6a21\u578b\u7684\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u4fc3\u8fdbBESS\u66f4\u6709\u6548\u5730\u6574\u5408\u5230\u5e02\u573a\u8fd0\u8425\u4e2d\u3002"}}
{"id": "2510.03659", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03659", "abs": "https://arxiv.org/abs/2510.03659", "authors": ["Xu Wang", "Yan Hu", "Benyou Wang", "Difan Zou"], "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders", "comment": "24 pages", "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models\n(LLMs), based on the assumption that their interpretable features naturally\nenable effective model behavior steering. Yet, a fundamental question remains\nunanswered: does higher interpretability indeed imply better steering utility?\nTo answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,\nQwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,\nand evaluate their interpretability and steering utility based on SAEBench\n(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a\nrank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis\nreveals only a relatively weak positive association (tau b approx 0.298),\nindicating that interpretability is an insufficient proxy for steering\nperformance. We conjecture the interpretability utility gap may stem from the\nselection of SAE features, as not all of them are equally effective for\nsteering. To further find features that truly steer the behavior of LLMs, we\npropose a novel selection criterion called Delta Token Confidence, which\nmeasures how much amplifying a feature changes the next token distribution. We\nshow that our method improves the steering performance of three LLMs by 52.52\npercent compared to the current best output score based criterion\n(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token\nConfidence, the correlation between interpretability and utility vanishes (tau\nb approx 0), and can even become negative. This further highlights the\ndivergence between interpretability and utility for the most effective steering\nfeatures.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6a21\u578b\u884c\u4e3a\u5f15\u5bfc\u6548\u7528\u4e4b\u95f4\u4ec5\u5b58\u5728\u5f31\u6b63\u76f8\u5173\uff0c\u63d0\u51faDelta Token Confidence\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u5f15\u5bfc\u6027\u80fd\uff0c\u4f46\u4f1a\u6d88\u9664\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u9a8c\u8bc1\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7684\u53ef\u89e3\u91ca\u6027\u662f\u5426\u786e\u5b9e\u80fd\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u6a21\u578b\u884c\u4e3a\u5f15\u5bfc\u6548\u7528\uff0c\u56e0\u4e3a\u5f53\u524d\u7814\u7a76\u666e\u904d\u5047\u8bbe\u53ef\u89e3\u91ca\u7279\u5f81\u81ea\u7136\u5177\u5907\u826f\u597d\u7684\u5f15\u5bfc\u80fd\u529b\u3002", "method": "\u5728\u4e09\u4e2aLLM\u4e0a\u8bad\u7ec390\u4e2aSAE\uff0c\u6db5\u76d65\u79cd\u67b6\u6784\u548c6\u79cd\u7a00\u758f\u5ea6\uff0c\u4f7f\u7528SAEBench\u548cAxBench\u5206\u522b\u8bc4\u4f30\u53ef\u89e3\u91ca\u6027\u548c\u5f15\u5bfc\u6548\u7528\uff0c\u901a\u8fc7Kendall\u79e9\u76f8\u5173\u7cfb\u6570\u5206\u6790\u5173\u8054\u6027\uff0c\u5e76\u63d0\u51faDelta Token Confidence\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u53ef\u89e3\u91ca\u6027\u4e0e\u5f15\u5bfc\u6548\u7528\u4ec5\u5b58\u5728\u5f31\u6b63\u76f8\u5173(tau b\u22480.298)\uff0c\u4f7f\u7528Delta Token Confidence\u65b9\u6cd5\u540e\u5f15\u5bfc\u6027\u80fd\u63d0\u534752.52%\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7528\u7684\u76f8\u5173\u6027\u6d88\u5931\u751a\u81f3\u53d8\u4e3a\u8d1f\u76f8\u5173\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u4e0d\u80fd\u4f5c\u4e3a\u5f15\u5bfc\u6548\u7528\u7684\u53ef\u9760\u4ee3\u7406\u6307\u6807\uff0c\u6700\u6709\u6548\u7684\u5f15\u5bfc\u7279\u5f81\u5f80\u5f80\u4e0e\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u4e0d\u540c\uff0c\u4e24\u8005\u5b58\u5728\u660e\u663e\u5206\u6b67\u3002"}}
{"id": "2510.03662", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03662", "abs": "https://arxiv.org/abs/2510.03662", "authors": ["Jijie Zhou", "Niloofar Mireshghallah", "Tianshi Li"], "title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting", "comment": null, "summary": "The rapid deployment of large language models (LLMs) in consumer applications\nhas led to frequent exchanges of personal information. To obtain useful\nresponses, users often share more than necessary, increasing privacy risks via\nmemorization, context-based personalization, or security breaches. We present a\nframework to formally define and operationalize data minimization: for a given\nuser prompt and response model, quantifying the least privacy-revealing\ndisclosure that maintains utility, and we propose a priority-queue tree search\nto locate this optimal point within a privacy-ordered transformation space. We\nevaluated the framework on four datasets spanning open-ended conversations\n(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth\nanswers (CaseHold, MedQA), quantifying achievable data minimization with nine\nLLMs as the response model. Our results demonstrate that larger frontier LLMs\ncan tolerate stronger data minimization while maintaining task quality than\nsmaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for\nQwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that\nLLMs struggle to predict optimal data minimization directly, showing a bias\ntoward abstraction that leads to oversharing. This suggests not just a privacy\ngap, but a capability gap: models may lack awareness of what information they\nactually need to solve a task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u961f\u5217\u6811\u641c\u7d22\u5728\u9690\u79c1\u6709\u5e8f\u8f6c\u6362\u7a7a\u95f4\u4e2d\u5bfb\u627e\u6700\u4f18\u5e73\u8861\u70b9\uff0c\u91cf\u5316\u4e86\u5728\u4fdd\u6301\u4efb\u52a1\u6548\u7528\u524d\u63d0\u4e0b\u7684\u6700\u5c0f\u9690\u79c1\u6cc4\u9732\u62ab\u9732\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u5e94\u7528\u4e2d\u7684\u5feb\u901f\u90e8\u7f72\u5bfc\u81f4\u4e2a\u4eba\u4fe1\u606f\u9891\u7e41\u4ea4\u6362\uff0c\u7528\u6237\u4e3a\u83b7\u53d6\u6709\u7528\u54cd\u5e94\u5f80\u5f80\u8fc7\u5ea6\u5206\u4eab\u4fe1\u606f\uff0c\u589e\u52a0\u4e86\u901a\u8fc7\u8bb0\u5fc6\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u4e2a\u6027\u5316\u6216\u5b89\u5168\u6f0f\u6d1e\u5e26\u6765\u7684\u9690\u79c1\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6b63\u5f0f\u5b9a\u4e49\u548c\u64cd\u4f5c\u5316\u6570\u636e\u6700\u5c0f\u5316\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u4f18\u5148\u7ea7\u961f\u5217\u6811\u641c\u7d22\u65b9\u6cd5\u5728\u9690\u79c1\u6709\u5e8f\u8f6c\u6362\u7a7a\u95f4\u4e2d\u5b9a\u4f4d\u6700\u4f18\u5e73\u8861\u70b9\uff0c\u5e76\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e5d\u4e2aLLM\u7684\u53ef\u5b9e\u73b0\u6570\u636e\u6700\u5c0f\u5316\u7a0b\u5ea6\u3002", "result": "\u524d\u6cbf\u5927\u578bLLM\uff08\u5982GPT-5\uff09\u5728\u4fdd\u6301\u4efb\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u53ef\u4ee5\u5bb9\u5fcd\u66f4\u5f3a\u7684\u6570\u636e\u6700\u5c0f\u5316\uff0885.7%\u7684\u5220\u51cf\uff09\uff0c\u800c\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\uff08\u5982Qwen2.5-0.5B\uff09\u53ea\u80fd\u5bb9\u5fcd19.3%\u7684\u5220\u51cf\u3002LLM\u96be\u4ee5\u76f4\u63a5\u9884\u6d4b\u6700\u4f18\u6570\u636e\u6700\u5c0f\u5316\uff0c\u5b58\u5728\u504f\u5411\u62bd\u8c61\u7684\u504f\u89c1\u5bfc\u81f4\u8fc7\u5ea6\u5206\u4eab\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86\u9690\u79c1\u5dee\u8ddd\uff0c\u8fd8\u53d1\u73b0\u4e86\u80fd\u529b\u5dee\u8ddd\uff1a\u6a21\u578b\u53ef\u80fd\u7f3a\u4e4f\u5bf9\u89e3\u51b3\u4efb\u52a1\u5b9e\u9645\u6240\u9700\u4fe1\u606f\u7684\u610f\u8bc6\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u9690\u79c1\u4fdd\u62a4\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.03669", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03669", "abs": "https://arxiv.org/abs/2510.03669", "authors": ["Wenlong Deng", "Yi Ren", "Yushu Li", "Boying Gong", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "title": "Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable rewards has significantly advanced the\nreasoning capabilities of large language models, yet how to explicitly steer\ntraining toward exploration or exploitation remains an open problem. We\nintroduce Token Hidden Reward (THR), a token-level metric that quantifies each\ntoken's influence on the likelihood of correct responses under Group Relative\nPolicy Optimization (GRPO). We find that training dynamics are dominated by a\nsmall subset of tokens with high absolute THR values. Most interestingly,\ntokens with positive THR strengthen confidence in correct outputs, thus\nfavoring exploitation, while tokens with negative THR preserve probability mass\nfor alternative outputs, enabling exploration. This insight suggests a natural\nintervention: a THR-guided reweighting algorithm that modulates GRPO's learning\nsignals to explicitly bias training toward exploitation or exploration. We\nvalidate the efficacy of this algorithm on diverse math reasoning benchmarks.\nBy amplifying tokens with positive THR value and weakening negative ones, our\nalgorithm improves greedy-decoding accuracy, favoring exploitation. The reverse\nstrategy yields consistent gains in Pass@K accuracy, favoring exploration. We\nfurther demonstrate that our algorithm integrates seamlessly with other RL\nobjectives such as GSPO and generalizes across architectures including Llama.\nThese findings establish THR as a principled and fine-grained mechanism for\ndynamically controlling exploration and exploitation in RL-tuned LLMs,\nproviding new tools for targeted fine-tuning in reasoning-intensive\napplications.", "AI": {"tldr": "\u63d0\u51faToken Hidden Reward (THR)\u6307\u6807\uff0c\u901a\u8fc7\u5206\u6790token\u5bf9\u6b63\u786e\u56de\u7b54\u6982\u7387\u7684\u5f71\u54cd\u6765\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u53ef\u663e\u5f0f\u63a7\u5236\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u663e\u5f0f\u63a7\u5236\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u5e73\u8861\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u5f15\u5165THR\u6307\u6807\u91cf\u5316\u6bcf\u4e2atoken\u5728GRPO\u6846\u67b6\u4e0b\u5bf9\u6b63\u786e\u56de\u7b54\u6982\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u91cd\u52a0\u6743\u7b97\u6cd5\u6765\u8c03\u8282\u5b66\u4e60\u4fe1\u53f7\uff0c\u504f\u5411\u63a2\u7d22\u6216\u5229\u7528\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\uff1a\u653e\u5927\u6b63THR\u503ctoken\u63d0\u9ad8\u8d2a\u5a6a\u89e3\u7801\u7cbe\u5ea6\uff08\u504f\u5411\u5229\u7528\uff09\uff0c\u524a\u5f31\u8d1fTHR\u503ctoken\u63d0\u9ad8Pass@K\u7cbe\u5ea6\uff08\u504f\u5411\u63a2\u7d22\uff09\u3002", "conclusion": "THR\u4e3aRL\u8c03\u4f18\u7684LLM\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u52a8\u6001\u63a7\u5236\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u673a\u5236\uff0c\u4e3a\u63a8\u7406\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u76ee\u6807\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2510.03678", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03678", "abs": "https://arxiv.org/abs/2510.03678", "authors": ["Zhao Song", "Shenghao Xie", "Samson Zhou"], "title": "Towards Sampling Data Structures for Tensor Products in Turnstile Streams", "comment": null, "summary": "This paper studies the computational challenges of large-scale\nattention-based models in artificial intelligence by utilizing importance\nsampling methods in the streaming setting. Inspired by the classical definition\nof the $\\ell_2$ sampler and the recent progress of the attention scheme in\nLarge Language Models (LLMs), we propose the definition of the attention\nsampler. Our approach significantly reduces the computational burden of\ntraditional attention mechanisms. We analyze the effectiveness of the attention\nsampler from a theoretical perspective, including space and update time.\nAdditionally, our framework exhibits scalability and broad applicability across\nvarious model architectures and domains.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u91c7\u6837\u5668\u5b9a\u4e49\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5e76\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u5206\u6790\u5176\u7a7a\u95f4\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u6a21\u578b\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u53d7\u7ecf\u5178\u21132\u91c7\u6837\u5668\u548cLLM\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u5c55\u7684\u542f\u53d1\u3002", "method": "\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u5b9a\u4e49\u6ce8\u610f\u529b\u91c7\u6837\u5668\uff0c\u5206\u6790\u5176\u7a7a\u95f4\u548c\u66f4\u65b0\u65f6\u95f4\u6548\u7387\u3002", "result": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u91c7\u6837\u5668\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "\u6ce8\u610f\u529b\u91c7\u6837\u5668\u4e3a\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03679", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03679", "abs": "https://arxiv.org/abs/2510.03679", "authors": ["Junhua Chen", "Zixi Zhang", "Hantao Zhong", "Rika Antonova"], "title": "Group Policy Gradient", "comment": null, "summary": "We introduce Group Policy Gradient (GPG), a family of critic-free\npolicy-gradient estimators for general MDPs. Inspired by the success of GRPO's\napproach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a\nlearned value function with a group-based Monte Carlo advantage estimator,\nremoving the memory, compute, and hyperparameter costs of training a critic\nwhile preserving PPO's clipped-objective structure. We prove the consistency of\nthe GPG estimator, analyze the bias-variance tradeoffs, and demonstrate\nempirically that GPG matches or outperforms PPO on standard benchmarks. GPG\nmakes better use of parallel simulations, which, together with its critic-free\ndesign, results in more efficient use of computational resources than PPO.", "AI": {"tldr": "\u63d0\u51faGroup Policy Gradient (GPG)\uff0c\u4e00\u79cd\u65e0\u8bc4\u8bba\u5bb6\u7684\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\u5bb6\u65cf\uff0c\u4f7f\u7528\u57fa\u4e8e\u7ec4\u7684\u8499\u7279\u5361\u6d1b\u4f18\u52bf\u4f30\u8ba1\u5668\u66ff\u4ee3\u5b66\u4e60\u503c\u51fd\u6570\uff0c\u5728\u4fdd\u6301PPO\u526a\u88c1\u76ee\u6807\u7ed3\u6784\u7684\u540c\u65f6\u6d88\u9664\u8bc4\u8bba\u5bb6\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u53d7GRPO\u5728\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u4e2d\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u6d88\u9664\u8bad\u7ec3\u8bc4\u8bba\u5bb6\u6240\u9700\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u8d85\u53c2\u6570\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301PPO\u7684\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ec4\u7684\u8499\u7279\u5361\u6d1b\u4f18\u52bf\u4f30\u8ba1\u5668\u66ff\u4ee3\u5b66\u4e60\u503c\u51fd\u6570\uff0c\u4fdd\u7559PPO\u7684\u526a\u88c1\u76ee\u6807\u7ed3\u6784\uff0c\u65e0\u9700\u8bad\u7ec3\u8bc4\u8bba\u5bb6\u7f51\u7edc\u3002", "result": "\u7406\u8bba\u8bc1\u660eGPG\u4f30\u8ba1\u5668\u7684\u4e00\u81f4\u6027\uff0c\u5206\u6790\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u5b9e\u9a8c\u8868\u660eGPG\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u4f18\u4e8ePPO\uff0c\u80fd\u66f4\u597d\u5730\u5229\u7528\u5e76\u884c\u6a21\u62df\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "GPG\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bc4\u8bba\u5bb6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21\u5e76\u884c\u73af\u5883\u3002"}}
{"id": "2510.03690", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03690", "abs": "https://arxiv.org/abs/2510.03690", "authors": ["Ali Azizpour", "Reza Ramezanpour", "Ashutosh Sabharwal", "Santiago Segarra"], "title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning", "comment": null, "summary": "Real-world graph datasets often consist of mixtures of populations, where\ngraphs are generated from multiple distinct underlying distributions. However,\nmodern representation learning approaches, such as graph contrastive learning\n(GCL) and augmentation methods like Mixup, typically overlook this mixture\nstructure. In this work, we propose a unified framework that explicitly models\ndata as a mixture of underlying probabilistic graph generative models\nrepresented by graphons. To characterize these graphons, we leverage graph\nmoments (motif densities) to cluster graphs arising from the same model. This\nenables us to disentangle the mixture components and identify their distinct\ngenerative mechanisms. This model-aware partitioning benefits two key graph\nlearning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data\naugmentation technique that interpolates in a semantically valid space guided\nby the estimated graphons, instead of assuming a single graphon per class. 2)\nFor GCL, it enables model-adaptive and principled augmentations. Additionally,\nby introducing a new model-aware objective, our proposed approach (termed MGCL)\nimproves negative sampling by restricting negatives to graphs from other\nmodels. We establish a key theoretical guarantee: a novel, tighter bound\nshowing that graphs sampled from graphons with small cut distance will have\nsimilar motif densities with high probability. Extensive experiments on\nbenchmark datasets demonstrate strong empirical performance. In unsupervised\nlearning, MGCL achieves state-of-the-art results, obtaining the top average\nrank across eight datasets. In supervised learning, GMAM consistently\noutperforms existing strategies, achieving new state-of-the-art accuracy in 6\nout of 7 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a21\u56fe\u6570\u636e\u4e3a\u56fe\u5143\u6df7\u5408\u5206\u5e03\uff0c\u901a\u8fc7\u56fe\u77e9\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u751f\u6210\u673a\u5236\uff0c\u5e76\u5e94\u7528\u4e8e\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\uff0c\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\u4e2d\u90fd\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u6765\u81ea\u591a\u4e2a\u4e0d\u540c\u5206\u5e03\u7684\u6df7\u5408\u79cd\u7fa4\uff0c\u4f46\u73b0\u6709\u7684\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548cMixup\u589e\u5f3a\uff09\u5f80\u5f80\u5ffd\u7565\u4e86\u8fd9\u79cd\u6df7\u5408\u7ed3\u6784\u3002", "method": "1) \u4f7f\u7528\u56fe\u77e9\uff08\u6a21\u4f53\u5bc6\u5ea6\uff09\u805a\u7c7b\u6765\u81ea\u76f8\u540c\u56fe\u5143\u6a21\u578b\u7684\u56fe\uff1b2) \u63d0\u51fa\u56fe\u5143\u6df7\u5408\u611f\u77e5\u7684Mixup\u589e\u5f3a(GMAM)\uff1b3) \u63d0\u51fa\u6a21\u578b\u611f\u77e5\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60(MGCL)\uff0c\u6539\u8fdb\u8d1f\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cMGCL\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u6392\u540d\uff1b\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\uff0cGMAM\u57287\u4e2a\u6570\u636e\u96c6\u4e2d\u76846\u4e2a\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u51c6\u786e\u7387\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u56fe\u7684\u6df7\u5408\u5206\u5e03\u7ed3\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u56fe\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.03691", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03691", "abs": "https://arxiv.org/abs/2510.03691", "authors": ["Zehua Liu", "Han Wu", "Xiaojin Fu", "Shuqi Liu", "Xiongwei Han", "Tao Zhong", "Mingxuan Yuan"], "title": "REG: A Regularization Optimizer for Robust Training Dynamics", "comment": null, "summary": "Optimizers are crucial for the efficient training of Large Language Models\n(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers\nlike Muon have emerged, which regularize gradient updates by operating on\nentire weight matrices. The Muon optimizer balances the gradient updates along\nall the directions. However, Muon's reliance on the matrix sign function can\nlead to training instability, exhibits incompatibility when fine-tuning models\npre-trained with AdamW. To address these limitations, we propose \\textbf{REG},\na novel optimizer that replaces Muon's aggressive matrix sign operator with the\nRow-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a\nmatrix, the RACS operator regularizes the update steps in a less drastic\nmanner, making it simpler to implement and more compatible with established\ntraining dynamics. Through extensive empirical experiments on LLM training, we\ndemonstrate that our REG optimizer not only achieves superior performance and\nstability over AdamW, but also maintains consistency with the AdamW training\nparadigm. This consistency is particularly evident during the fine-tuning\nstage, where REG optimizer avoids the performance degradation observed with\nMuon.", "AI": {"tldr": "\u63d0\u51fa\u4e86REG\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u7528\u884c-\u5217\u7f29\u653e(RACS)\u64cd\u4f5c\u7b26\u66ff\u6362Muon\u7684\u77e9\u9635\u7b26\u53f7\u51fd\u6570\uff0c\u89e3\u51b3\u4e86Muon\u4f18\u5316\u5668\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0eAdamW\u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002", "motivation": "Muon\u4f18\u5316\u5668\u867d\u7136\u80fd\u5e73\u8861\u68af\u5ea6\u66f4\u65b0\u65b9\u5411\uff0c\u4f46\u5176\u4f9d\u8d56\u77e9\u9635\u7b26\u53f7\u51fd\u6570\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u4e14\u4e0eAdamW\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u4e0d\u517c\u5bb9\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u4e14\u4e0e\u73b0\u6709\u8bad\u7ec3\u8303\u5f0f\u517c\u5bb9\u7684\u4f18\u5316\u5668\u3002", "method": "\u4f7f\u7528\u884c-\u5217\u7f29\u653e(RACS)\u64cd\u4f5c\u7b26\u66ff\u4ee3Muon\u7684\u77e9\u9635\u7b26\u53f7\u51fd\u6570\uff0c\u8be5\u64cd\u4f5c\u7b26\u57fa\u4e8e\u77e9\u9635\u5e73\u8861\u7406\u8bba\uff0c\u4ee5\u66f4\u6e29\u548c\u7684\u65b9\u5f0f\u6b63\u5219\u5316\u66f4\u65b0\u6b65\u9aa4\uff0c\u5b9e\u73b0\u66f4\u7b80\u5355\u7684\u5b9e\u73b0\u548c\u66f4\u597d\u7684\u8bad\u7ec3\u52a8\u6001\u517c\u5bb9\u6027\u3002", "result": "\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cREG\u4f18\u5316\u5668\u4e0d\u4ec5\u6bd4AdamW\u83b7\u5f97\u66f4\u4f18\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u800c\u4e14\u4e0eAdamW\u8bad\u7ec3\u8303\u5f0f\u4fdd\u6301\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u5fae\u8c03\u9636\u6bb5\u907f\u514d\u4e86Muon\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "REG\u4f18\u5316\u5668\u901a\u8fc7RACS\u64cd\u4f5c\u7b26\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u4e0eAdamW\u517c\u5bb9\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u89e3\u51b3\u4e86Muon\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3002"}}
{"id": "2510.03722", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03722", "abs": "https://arxiv.org/abs/2510.03722", "authors": ["Qianxin Yi", "Shao-Bo Lin", "Jun Fan", "Yao Wang"], "title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach", "comment": null, "summary": "Reinforcement learning (RL) has been widely applied to sequential decision\nmaking, where interpretability and performance are both critical for practical\nadoption. Current approaches typically focus on performance and rely on post\nhoc explanations to account for interpretability. Different from these\napproaches, we focus on designing an interpretability-oriented yet\nperformance-enhanced RL approach. Specifically, we propose a spectral based\nlinear RL method that extends the ridge regression-based approach through a\nspectral filter function. The proposed method clarifies the role of\nregularization in controlling estimation error and further enables the design\nof an adaptive regularization parameter selection strategy guided by the\nbias-variance trade-off principle. Theoretical analysis establishes\nnear-optimal bounds for both parameter estimation and generalization error.\nExtensive experiments on simulated environments and real-world datasets from\nKuaishou and Taobao demonstrate that our method either outperforms or matches\nexisting baselines in decision quality. We also conduct interpretability\nanalyses to illustrate how the learned policies make decisions, thereby\nenhancing user trust. These results highlight the potential of our approach to\nbridge the gap between RL theory and practical decision making, providing\ninterpretability, accuracy, and adaptability in management contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u6ee4\u6ce2\u7684\u7ebf\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\uff0c\u4f9d\u8d56\u4e8b\u540e\u89e3\u91ca\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u53ef\u89e3\u91ca\u6027\u5bfc\u5411\u4e14\u6027\u80fd\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u4e86\u57fa\u4e8e\u5cad\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u6ee4\u6ce2\u51fd\u6570\u8bbe\u8ba1\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u539f\u5219\u63a7\u5236\u4f30\u8ba1\u8bef\u5dee\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u53c2\u6570\u4f30\u8ba1\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u8fd1\u4f3c\u6700\u4f18\u754c\u3002\u5728\u5feb\u624b\u548c\u6dd8\u5b9d\u7b49\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u5f25\u5408\u5f3a\u5316\u5b66\u4e60\u7406\u8bba\u4e0e\u5b9e\u9645\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u7ba1\u7406\u573a\u666f\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.03726", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03726", "abs": "https://arxiv.org/abs/2510.03726", "authors": ["Jiahao Zeng", "Wolong Xing", "Liangtao Shi", "Xin Huang", "Jialin Wang", "Zhile Cao", "Zhenkui Shi"], "title": "Personalized federated prototype learning in mixed heterogeneous data scenarios", "comment": null, "summary": "Federated learning has received significant attention for its ability to\nsimultaneously protect customer privacy and leverage distributed data from\nmultiple devices for model training. However, conventional approaches often\nfocus on isolated heterogeneous scenarios, resulting in skewed feature\ndistributions or label distributions. Meanwhile, data heterogeneity is actually\na key factor in improving model performance. To address this issue, we propose\na new approach called PFPL in mixed heterogeneous scenarios. The method\nprovides richer domain knowledge and unbiased convergence targets by\nconstructing personalized, unbiased prototypes for each client. Moreover, in\nthe local update phase, we introduce consistent regularization to align local\ninstances with their personalized prototypes, which significantly improves the\nconvergence of the loss function. Experimental results on Digits and Office\nCaltech datasets validate the effectiveness of our approach and successfully\nreduce the communication cost.", "AI": {"tldr": "\u63d0\u51faPFPL\u65b9\u6cd5\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u4e2a\u6027\u5316\u65e0\u504f\u539f\u578b\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5f02\u6784\u573a\u666f\u4e0b\u5b58\u5728\u7279\u5f81\u5206\u5e03\u6216\u6807\u7b7e\u5206\u5e03\u503e\u659c\u7684\u95ee\u9898\uff0c\u800c\u6570\u636e\u5f02\u6784\u5b9e\u9645\u4e0a\u662f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20", "method": "PFPL\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6784\u5efa\u4e2a\u6027\u5316\u65e0\u504f\u539f\u578b\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u9886\u57df\u77e5\u8bc6\u548c\u65e0\u504f\u6536\u655b\u76ee\u6807\uff1b\u5728\u672c\u5730\u66f4\u65b0\u9636\u6bb5\u5f15\u5165\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u5c06\u672c\u5730\u5b9e\u4f8b\u4e0e\u5176\u4e2a\u6027\u5316\u539f\u578b\u5bf9\u9f50", "result": "\u5728Digits\u548cOffice Caltech\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c", "conclusion": "PFPL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6df7\u5408\u5f02\u6784\u573a\u666f\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u539f\u578b\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2510.03731", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03731", "abs": "https://arxiv.org/abs/2510.03731", "authors": ["Yongfu Xue"], "title": "Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation", "comment": null, "summary": "The rapid development of parameter-efficient fine-tuning methods has\nnoticeably improved the efficiency of adapting large language models. Among\nthese, LoRA has gained widespread popularity due to its strong balance of\neffectiveness and parameter efficiency. However, LoRA relies on initializing\ntwo low-rank matrices whose product is zero, which limits its ability to\neffectively activate and leverage the original model weights-creating a\npotential bottleneck for optimal performance. To address this limitation, we\npropose \\textbf{IniLoRA}, a novel initialization strategy that initializes the\nlow-rank matrices to closely approximate the original model weights.\nExperimental results indicate that IniLoRA achieves better performance than\nLoRA across a range of models and tasks. Additionally, we introduce two\nvariants, IniLoRA-$\\alpha$ and IniLoRA-$\\beta$, both leveraging distinct\ninitialization methods to enhance performance further.", "AI": {"tldr": "\u63d0\u51faIniLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbLoRA\u7684\u4f4e\u79e9\u77e9\u9635\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LoRA\u867d\u7136\u53c2\u6570\u6548\u7387\u9ad8\uff0c\u4f46\u5176\u96f6\u4e58\u79ef\u521d\u59cb\u5316\u9650\u5236\u4e86\u6709\u6548\u6fc0\u6d3b\u548c\u5229\u7528\u539f\u59cb\u6a21\u578b\u6743\u91cd\u7684\u80fd\u529b\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faIniLoRA\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5c06\u4f4e\u79e9\u77e9\u9635\u521d\u59cb\u5316\u4e3a\u8fd1\u4f3c\u539f\u59cb\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u5f15\u5165IniLoRA-\u03b1\u548cIniLoRA-\u03b2\u4e24\u4e2a\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aIniLoRA\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eLoRA\u3002", "conclusion": "IniLoRA\u901a\u8fc7\u6539\u8fdb\u521d\u59cb\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86LoRA\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.03734", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03734", "abs": "https://arxiv.org/abs/2510.03734", "authors": ["Nirjhar Das", "Mohit Sharma", "Praharsh Nanavati", "Kirankumar Shiragur", "Amit Deshpande"], "title": "Cost Efficient Fairness Audit Under Partial Feedback", "comment": "Accepted at NeurIPS 2025 RegML Workshop; Reliable ML Workshop", "summary": "We study the problem of auditing the fairness of a given classifier under\npartial feedback, where true labels are available only for positively\nclassified individuals, (e.g., loan repayment outcomes are observed only for\napproved applicants). We introduce a novel cost model for acquiring additional\nlabeled data, designed to more accurately reflect real-world costs such as\ncredit assessment, loan processing, and potential defaults. Our goal is to find\noptimal fairness audit algorithms that are more cost-effective than random\nexploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no\nassumptions on the data distribution, and a mixture model, where features and\ntrue labels follow a mixture of exponential family distributions. In the\nblack-box setting, we propose a near-optimal auditing algorithm under mild\nassumptions and show that a natural baseline can be strictly suboptimal. In the\nmixture model setting, we design a novel algorithm that achieves significantly\nlower audit cost than the black-box case. Our approach leverages prior work on\nlearning from truncated samples and maximum-a-posteriori oracles, and extends\nknown results on spherical Gaussian mixtures to handle exponential family\nmixtures, which may be of independent interest. Moreover, our algorithms apply\nto popular fairness metrics including demographic parity, equal opportunity,\nand equalized odds. Empirically, we demonstrate strong performance of our\nalgorithms on real-world fair classification datasets like Adult Income and Law\nSchool, consistently outperforming natural baselines by around 50% in terms of\naudit cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u90e8\u5206\u53cd\u9988\u4e0b\u5ba1\u8ba1\u5206\u7c7b\u5668\u516c\u5e73\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5176\u4e2d\u771f\u5b9e\u6807\u7b7e\u4ec5\u5bf9\u9633\u6027\u5206\u7c7b\u4e2a\u4f53\u53ef\u7528\u3002\u5f15\u5165\u4e86\u66f4\u7b26\u5408\u73b0\u5b9e\u6210\u672c\u7684\u65b0\u6210\u672c\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6700\u4f18\u5ba1\u8ba1\u7b97\u6cd5\u3002", "motivation": "\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u771f\u5b9e\u6807\u7b7e\u901a\u5e38\u53ea\u5bf9\u9633\u6027\u5206\u7c7b\u4e2a\u4f53\u53ef\u7528\uff08\u5982\u8d37\u6b3e\u7533\u8bf7\u4e2d\u53ea\u6709\u83b7\u6279\u8005\u7684\u8fd8\u6b3e\u7ed3\u679c\u53ef\u89c2\u6d4b\uff09\uff0c\u800c\u4f20\u7edf\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u90e8\u5206\u53cd\u9988\u60c5\u51b5\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6210\u672c\u6548\u76ca\u65b9\u9762\u4e0d\u591f\u7406\u60f3\uff0c\u9700\u8981\u66f4\u7b26\u5408\u5b9e\u9645\u6210\u672c\u7ed3\u6784\u7684\u5ba1\u8ba1\u65b9\u6848\u3002", "method": "\u8003\u8651\u4e86\u4e24\u79cd\u5ba1\u8ba1\u8bbe\u7f6e\uff1a\u9ed1\u76d2\u6a21\u578b\uff08\u65e0\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff09\u548c\u6df7\u5408\u6a21\u578b\uff08\u7279\u5f81\u548c\u771f\u5b9e\u6807\u7b7e\u9075\u5faa\u6307\u6570\u65cf\u5206\u5e03\u7684\u6df7\u5408\uff09\u3002\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u63d0\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u5ba1\u8ba1\u7b97\u6cd5\uff1b\u5728\u6df7\u5408\u6a21\u578b\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1\u4e86\u5229\u7528\u622a\u65ad\u6837\u672c\u5b66\u4e60\u548c\u6700\u5927\u540e\u9a8c\u6982\u7387\u9884\u8a00\u673a\u7684\u65b0\u7b97\u6cd5\uff0c\u5c06\u7403\u5f62\u9ad8\u65af\u6df7\u5408\u7ed3\u679c\u6269\u5c55\u5230\u6307\u6570\u65cf\u6df7\u5408\u3002", "result": "\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u8bc1\u660e\u4e86\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u53ef\u80fd\u662f\u4e25\u683c\u6b21\u4f18\u7684\uff1b\u5728\u6df7\u5408\u6a21\u578b\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u9ed1\u76d2\u60c5\u51b5\u663e\u8457\u66f4\u4f4e\u7684\u5ba1\u8ba1\u6210\u672c\u3002\u7b97\u6cd5\u9002\u7528\u4e8e\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u3001\u673a\u4f1a\u5747\u7b49\u548c\u5747\u7b49\u5316\u51e0\u7387\u7b49\u5e38\u89c1\u516c\u5e73\u6027\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982Adult Income\u548cLaw School\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u5728\u5ba1\u8ba1\u6210\u672c\u65b9\u9762\u6301\u7eed\u63d0\u5347\u7ea650%\uff0c\u4e3a\u90e8\u5206\u53cd\u9988\u4e0b\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03745", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.03745", "abs": "https://arxiv.org/abs/2510.03745", "authors": ["Michael Etienne Van Huffel", "Nathan Kirk", "Makram Chahine", "Daniela Rus", "T. Konstantin Rusch"], "title": "Neural Low-Discrepancy Sequences", "comment": null, "summary": "Low-discrepancy points are designed to efficiently fill the space in a\nuniform manner. This uniformity is highly advantageous in many problems in\nscience and engineering, including in numerical integration, computer vision,\nmachine perception, computer graphics, machine learning, and simulation.\nWhereas most previous low-discrepancy constructions rely on abstract algebra\nand number theory, Message-Passing Monte Carlo (MPMC) was recently introduced\nto exploit machine learning methods for generating point sets with lower\ndiscrepancy than previously possible. However, MPMC is limited to generating\npoint sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,\nsequences of points in which every prefix has low discrepancy, a property\nessential for many applications. To address this limitation, we introduce\nNeural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based\nframework for generating LDS. Drawing inspiration from classical LDS, we train\na neural network to map indices to points such that the resulting sequences\nexhibit minimal discrepancy across all prefixes. To this end, we deploy a\ntwo-stage learning process: supervised approximation of classical constructions\nfollowed by unsupervised fine-tuning to minimize prefix discrepancies. We\ndemonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a\nsignificant margin with respect to discrepancy measures. Moreover, we\ndemonstrate the effectiveness of $NeuroLDS$ across diverse applications,\nincluding numerical integration, robot motion planning, and scientific machine\nlearning. These results highlight the promise and broad significance of Neural\nLow-Discrepancy Sequences. Our code can be found at\nhttps://github.com/camail-official/neuro-lds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NeuroLDS\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4f4e\u5dee\u5f02\u5e8f\u5217\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5c06\u7d22\u5f15\u6620\u5c04\u4e3a\u70b9\uff0c\u4f7f\u6240\u6709\u524d\u7f00\u5e8f\u5217\u90fd\u5177\u6709\u6700\u5c0f\u5dee\u5f02\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f4e\u5dee\u5f02\u70b9\u96c6\u6784\u9020\u4e3b\u8981\u4f9d\u8d56\u62bd\u8c61\u4ee3\u6570\u548c\u6570\u8bba\u65b9\u6cd5\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5MPMC\u53ea\u80fd\u751f\u6210\u70b9\u96c6\u800c\u65e0\u6cd5\u6269\u5c55\u5230\u4f4e\u5dee\u5f02\u5e8f\u5217\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u5e8f\u5217\u524d\u7f00\u4f4e\u5dee\u5f02\u7279\u6027\u7684\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8fd1\u4f3c\u7ecf\u5178\u6784\u9020\uff0c\u7136\u540e\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u6765\u6700\u5c0f\u5316\u524d\u7f00\u5dee\u5f02\u3002\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06\u7d22\u5f15\u6620\u5c04\u4e3a\u70b9\uff0c\u4f7f\u751f\u6210\u5e8f\u5217\u7684\u6240\u6709\u524d\u7f00\u90fd\u5177\u6709\u6700\u5c0f\u5dee\u5f02\u3002", "result": "NeuroLDS\u5728\u6240\u6709\u4f4e\u5dee\u5f02\u5e8f\u5217\u6784\u9020\u65b9\u6cd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5728\u6570\u503c\u79ef\u5206\u3001\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7b49\u591a\u6837\u5316\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u795e\u7ecf\u4f4e\u5dee\u5f02\u5e8f\u5217\u5c55\u73b0\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u548c\u5e7f\u6cdb\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4f4e\u5dee\u5f02\u5e8f\u5217\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03760", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03760", "abs": "https://arxiv.org/abs/2510.03760", "authors": ["Ping Guo", "Chenyu Zhu", "Siyuan Chen", "Fei Liu", "Xi Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models", "comment": "Under Review of ICLR 2026", "summary": "CUDA kernel optimization has become a critical bottleneck for AI performance,\nas deep learning training and inference efficiency directly depends on highly\noptimized GPU kernels.\n  Despite the promise of Large Language Models (LLMs) for automating kernel\noptimization, this field suffers from a fragmented ecosystem of isolated and\nincomparable approaches with unclear problem formulations.\n  Furthermore, general-purpose LLM code evolution methods cannot meet strict\ncorrectness requirements of CUDA kernel optimization.\n  We address these fundamental challenges by first formalizing CUDA kernel\noptimization as a code optimization task with a clear objective, constraints,\nand evaluation metrics.\n  We then establish the first systematic LLM-based code evolution framework,\nEvoEngineer, that provides guidance for designing and adapting optimization\nstrategies to achieve a balance between performance and correctness.\n  Finally, we implement a kernel optimization system based on this framework\nand conduct extensive experiments on 91 real-world CUDA kernels.\n  Our results demonstrate that EvoEngineer achieves a principled balance\nbetween performance and correctness, with the highest averaged median speedup\nof \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of\n\\textbf{69.8}\\%, outperforming existing methods on both dimensions.\n  Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all\noperations over PyTorch kernels and delivers the highest speedup on \\textbf{28}\n(\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$}\nacceleration.", "AI": {"tldr": "EvoEngineer\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u6027\u4ee3\u7801\u6f14\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8eCUDA\u5185\u6838\u4f18\u5316\uff0c\u572891\u4e2a\u771f\u5b9eCUDA\u5185\u6838\u4e0a\u5b9e\u73b0\u4e862.72\u500d\u7684\u5e73\u5747\u4e2d\u503c\u52a0\u901f\u548c69.8%\u7684\u4ee3\u7801\u6709\u6548\u6027\u3002", "motivation": "CUDA\u5185\u6838\u4f18\u5316\u5df2\u6210\u4e3aAI\u6027\u80fd\u7684\u5173\u952e\u74f6\u9888\uff0c\u4f46\u73b0\u6709LLM\u65b9\u6cd5\u5b58\u5728\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u3001\u95ee\u9898\u5b9a\u4e49\u4e0d\u6e05\u6670\u4ee5\u53ca\u65e0\u6cd5\u6ee1\u8db3\u4e25\u683c\u6b63\u786e\u6027\u8981\u6c42\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316CUDA\u5185\u6838\u4f18\u5316\u4efb\u52a1\uff0c\u7136\u540e\u5efa\u7acbEvoEngineer\u6846\u67b6\uff0c\u8bbe\u8ba1\u5e73\u8861\u6027\u80fd\u548c\u6b63\u786e\u6027\u7684\u4f18\u5316\u7b56\u7565\uff0c\u6700\u540e\u57fa\u4e8e\u8be5\u6846\u67b6\u5b9e\u73b0\u5185\u6838\u4f18\u5316\u7cfb\u7edf\u3002", "result": "\u572891\u4e2a\u771f\u5b9eCUDA\u5185\u6838\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cEvoEngineer\u5b9e\u73b0\u4e862.72\u500d\u7684\u5e73\u5747\u4e2d\u503c\u52a0\u901f\uff0c\u4ee3\u7801\u6709\u6548\u6027\u8fbe69.8%\uff0c\u6700\u9ad8\u52a0\u901f\u8fbe36.75\u500d\uff0c\u572856%\u7684\u64cd\u4f5c\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u52a0\u901f\u3002", "conclusion": "EvoEngineer\u5728\u6027\u80fd\u548c\u6b63\u786e\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u539f\u5219\u6027\u5e73\u8861\uff0c\u5728CUDA\u5185\u6838\u4f18\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03782", "abs": "https://arxiv.org/abs/2510.03782", "authors": ["Guofu Xie", "Chen Zhang", "Xiao Zhang", "Yunsheng Shi", "Ting Yao", "Jun Xu"], "title": "Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation", "comment": "Work in progress", "summary": "Adapting to diverse user needs at test time is a key challenge in\ncontrollable multi-objective generation. Existing methods are insufficient:\nmerging-based approaches provide indirect, suboptimal control at the parameter\nlevel, often disregarding the impacts of multiple objectives. While\ndecoding-based guidance is more direct, it typically requires aggregating\nlogits from multiple expert models, incurring significant space overhead and\nrelying heavily on individual model capacity. To address these issues, we\nintroduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model\nmerging for guided decoding. We first identify a critical compatibility problem\nbetween the guidance and base models. In Stage 1, MAGE resolves this by\ndynamically constructing a more robust base model, merging a series of backbone\nmodels that account for multiple objectives. In Stage 2, we merge explicit and\nimplicit value models into a unified guidance proxy, which then steers the\ndecoding of the base model from Stage 1. Our analysis empirically validates\nLinear Mode Connectivity (LMC) in value models, explores the relationship\nbetween model merging and prediction ensembling, and demonstrates the enhanced\ncontrollability afforded by our approach. Extensive experiments show that our\nmethod outperforms existing approaches, achieving superior controllability,\nPareto-optimal performance, and enhanced adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAGE\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6a21\u578b\u5408\u5e76\u4e0e\u5f15\u5bfc\u89e3\u7801\u6765\u89e3\u51b3\u591a\u76ee\u6807\u751f\u6210\u4e2d\u7684\u63a7\u5236\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u63a7\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1a\u57fa\u4e8e\u5408\u5e76\u7684\u65b9\u6cd5\u63d0\u4f9b\u95f4\u63a5\u7684\u53c2\u6570\u7ea7\u63a7\u5236\uff0c\u800c\u57fa\u4e8e\u89e3\u7801\u7684\u5f15\u5bfc\u65b9\u6cd5\u9700\u8981\u805a\u5408\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u7684logits\uff0c\u7a7a\u95f4\u5f00\u9500\u5927\u4e14\u4f9d\u8d56\u5355\u4e2a\u6a21\u578b\u80fd\u529b\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u52a8\u6001\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u57fa\u6a21\u578b\uff0c\u5408\u5e76\u8003\u8651\u591a\u76ee\u6807\u7684\u9aa8\u5e72\u6a21\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u663e\u5f0f\u548c\u9690\u5f0f\u4ef7\u503c\u6a21\u578b\u5408\u5e76\u4e3a\u7edf\u4e00\u5f15\u5bfc\u4ee3\u7406\uff0c\u6307\u5bfc\u57fa\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4ef7\u503c\u6a21\u578b\u4e2d\u7684\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\uff0c\u63a2\u7d22\u4e86\u6a21\u578b\u5408\u5e76\u4e0e\u9884\u6d4b\u96c6\u6210\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u3001\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u80fd\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "MAGE\u6846\u67b6\u5728\u53ef\u63a7\u591a\u76ee\u6807\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a7\u5236\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.03784", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03784", "abs": "https://arxiv.org/abs/2510.03784", "authors": ["Ruoxi Yu", "Haotian Jiang", "Jingpu Cheng", "Penghao Yu", "Qianxiao Li", "Zhong Li"], "title": "Allocation of Parameters in Transformers", "comment": null, "summary": "Transformers have achieved remarkable successes across a wide range of\napplications, yet the theoretical foundation of their model efficiency remains\nunderexplored. In this work, we investigate how the model parameters -- mainly\nattention heads and head dimensions -- should be allocated across layers to\nbalance expressivity and efficiency. We first provide mathematical analysis on\nthe role of early layers in information extraction from an approximation\nperspective, with a theoretical characterization on the trade-off between the\nnumber of heads and head dimension under a fixed parameter budget. In addition,\nwe uncover and prove the \\emph{saturation} behavior of softmax activations:\nContinuously increasing head dimensions can lead to diminishing returns in\nlearning errors, particularly for long sequences. Supported by both theory and\nexperiments, this saturation pattern suggests that later layers can operate\nmore efficiently with reduced parameters. Combining these insights, we propose\nprincipled strategies for allocating attention heads and dimensions across\nTransformers' layers, shedding light on theoretically-grounded model efficiency\nof Transformer-based architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790Transformer\u6a21\u578b\u53c2\u6570\u5206\u914d\u7b56\u7565\uff0c\u53d1\u73b0\u65e9\u671f\u5c42\u5728\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4e86softmax\u6fc0\u6d3b\u7684\u9971\u548c\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u8de8\u5c42\u5206\u914d\u6ce8\u610f\u529b\u5934\u548c\u7ef4\u5ea6\u7684\u4f18\u5316\u7b56\u7565\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u6a21\u578b\u6548\u7387\u7684\u7406\u8bba\u57fa\u7840\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u5c42\u95f4\u5206\u914d\u6a21\u578b\u53c2\u6570\uff08\u4e3b\u8981\u662f\u6ce8\u610f\u529b\u5934\u548c\u5934\u7ef4\u5ea6\uff09\u6765\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u9996\u5148\u4ece\u8fd1\u4f3c\u89d2\u5ea6\u5bf9\u65e9\u671f\u5c42\u5728\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u4f5c\u7528\u8fdb\u884c\u6570\u5b66\u5206\u6790\uff0c\u7406\u8bba\u8868\u5f81\u4e86\u5728\u56fa\u5b9a\u53c2\u6570\u9884\u7b97\u4e0b\u6ce8\u610f\u529b\u5934\u6570\u91cf\u548c\u5934\u7ef4\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u53d1\u73b0\u5e76\u8bc1\u660e\u4e86softmax\u6fc0\u6d3b\u7684\u9971\u548c\u884c\u4e3a\uff1a\u6301\u7eed\u589e\u52a0\u5934\u7ef4\u5ea6\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8bef\u5dee\u7684\u6536\u76ca\u9012\u51cf\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u5e8f\u5217\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u90fd\u652f\u6301\u9971\u548c\u6a21\u5f0f\u7684\u5b58\u5728\uff0c\u8868\u660e\u540e\u7eed\u5c42\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u53c2\u6570\u66f4\u9ad8\u6548\u5730\u8fd0\u884c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86\u5728Transformer\u5404\u5c42\u95f4\u5206\u914d\u6ce8\u610f\u529b\u5934\u548c\u7ef4\u5ea6\u7684\u539f\u5219\u6027\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u6a21\u578b\u6548\u7387\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u63ed\u793a\u4e86\u53c2\u6570\u5206\u914d\u7684\u5173\u952e\u6a21\u5f0f\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684Transformer\u6a21\u578b\u3002"}}
{"id": "2510.03798", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03798", "abs": "https://arxiv.org/abs/2510.03798", "authors": ["Yunwen Guo", "Yunlun Shu", "Gongyi Zhuo", "Tianyu Wang"], "title": "Robust Batched Bandits", "comment": "39 pages", "summary": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u91cd\u5c3e\u5956\u52b1\u7684\u9c81\u68d2\u6279\u5904\u7406\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u53d1\u73b0\u5728\u5b9e\u4f8b\u65e0\u5173\u548cLipschitz\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u5c3e\u5956\u52b1\u9700\u8981\u66f4\u5c11\u7684\u6279\u6b21\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\uff0c\u800c\u5728\u5b9e\u4f8b\u76f8\u5173\u8bbe\u7f6e\u4e2d\u6240\u9700\u6279\u6b21\u6570\u91cf\u4e0e\u5c3e\u90e8\u91cd\u5c3e\u6027\u65e0\u5173\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5047\u8bbe\u8f7b\u5c3e\u5956\u52b1\u5206\u5e03\uff0c\u4f46\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u5982\u4e34\u5e8a\u8bd5\u9a8c\u7ed3\u679c\u8868\u73b0\u51fa\u91cd\u5c3e\u7279\u5f81\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u9488\u5bf9\u91cd\u5c3e\u5956\u52b1\u7684\u9c81\u68d2\u6279\u5904\u7406\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u6db5\u76d6\u6709\u9650\u81c2\u548cLipschitz\u8fde\u7eed\u8bbe\u7f6e\u3002", "result": "\u63ed\u793a\u4e86\u4ee4\u4eba\u60ca\u8bb6\u7684\u73b0\u8c61\uff1a\u5728\u5b9e\u4f8b\u65e0\u5173\u548cLipschitz\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u5c3e\u5956\u52b1\u9700\u8981\u66f4\u5c11\u7684\u6279\u6b21\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\uff1b\u800c\u5728\u5b9e\u4f8b\u76f8\u5173\u8bbe\u7f6e\u4e2d\uff0c\u6240\u9700\u6279\u6b21\u6570\u91cf\u4e0e\u5c3e\u90e8\u91cd\u5c3e\u6027\u65e0\u5173\u3002", "conclusion": "\u91cd\u5c3e\u5956\u52b1\u5728\u6279\u5904\u7406\u8001\u864e\u673a\u95ee\u9898\u4e2d\u5177\u6709\u72ec\u7279\u7684\u6279\u6b21\u9700\u6c42\u7279\u6027\uff0c\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u5982\u4e34\u5e8a\u8bd5\u9a8c\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.03811", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.03811", "abs": "https://arxiv.org/abs/2510.03811", "authors": ["Aya Laajil", "Abduragim Shtanchaev", "Sajan Muhammad", "Eric Moulines", "Salem Lahlou"], "title": "Curriculum-Augmented GFlowNets For mRNA Sequence Generation", "comment": null, "summary": "Designing mRNA sequences is a major challenge in developing next-generation\ntherapeutics, since it involves exploring a vast space of possible nucleotide\ncombinations while optimizing sequence properties like stability, translation\nefficiency, and protein expression. While Generative Flow Networks are\npromising for this task, their training is hindered by sparse, long-horizon\nrewards and multi-objective trade-offs. We propose Curriculum-Augmented\nGFlowNets (CAGFN), which integrate curriculum learning with multi-objective\nGFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based\ncurriculum that progressively adapts the maximum sequence length guiding\nexploration from easier to harder subproblems. We also provide a new mRNA\ndesign environment for GFlowNets which, given a target protein sequence and a\ncombination of biological objectives, allows for the training of models that\ngenerate plausible mRNA candidates. This provides a biologically motivated\nsetting for applying and advancing GFlowNets in therapeutic sequence design. On\ndifferent mRNA design tasks, CAGFN improves Pareto performance and biological\nplausibility, while maintaining diversity. Moreover, CAGFN reaches\nhigher-quality solutions faster than a GFlowNet trained with random sequence\nsampling (no curriculum), and enables generalization to out-of-distribution\nsequences.", "AI": {"tldr": "\u63d0\u51faCurriculum-Augmented GFlowNets (CAGFN)\uff0c\u901a\u8fc7\u6574\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u591a\u76ee\u6807GFlowNets\u6765\u751f\u6210mRNA\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u548c\u957f\u89c6\u91ce\u4f18\u5316\u4e2d\u7684\u8bad\u7ec3\u56f0\u96be\u3002", "motivation": "mRNA\u5e8f\u5217\u8bbe\u8ba1\u9762\u4e34\u5de8\u5927\u7ec4\u5408\u7a7a\u95f4\u548c\u591a\u4e2a\u76ee\u6807\u4f18\u5316\u7684\u6311\u6218\uff0c\u4f20\u7edf\u751f\u6210\u6d41\u7f51\u7edc\u5728\u7a00\u758f\u5956\u52b1\u548c\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8bad\u7ec3\u56f0\u96be\u3002", "method": "CAGFN\u6574\u5408\u57fa\u4e8e\u957f\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u9010\u6b65\u8c03\u6574\u6700\u5927\u5e8f\u5217\u957f\u5ea6\uff0c\u4ece\u6613\u5230\u96be\u5f15\u5bfc\u63a2\u7d22\uff1b\u63d0\u4f9b\u65b0\u7684mRNA\u8bbe\u8ba1\u73af\u5883\uff0c\u7ed3\u5408\u76ee\u6807\u86cb\u767d\u5e8f\u5217\u548c\u751f\u7269\u76ee\u6807\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u4e0d\u540cmRNA\u8bbe\u8ba1\u4efb\u52a1\u4e2d\uff0cCAGFN\u63d0\u9ad8\u4e86Pareto\u6027\u80fd\u548c\u751f\u7269\u5408\u7406\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6837\u6027\uff1b\u6bd4\u968f\u673a\u91c7\u6837\u8bad\u7ec3\u66f4\u5feb\u8fbe\u5230\u9ad8\u8d28\u91cf\u89e3\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u5e8f\u5217\u3002", "conclusion": "CAGFN\u4e3a\u6cbb\u7597\u6027\u5e8f\u5217\u8bbe\u8ba1\u4e2d\u7684GFlowNets\u5e94\u7528\u63d0\u4f9b\u4e86\u751f\u7269\u52a8\u673a\u8bbe\u7f6e\uff0c\u5728mRNA\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.03814", "categories": ["cs.LG", "cs.AI", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.03814", "abs": "https://arxiv.org/abs/2510.03814", "authors": ["Lukas Eisenmann", "Alena Br\u00e4ndle", "Zahra Monfared", "Daniel Durstewitz"], "title": "Detecting Invariant Manifolds in ReLU-Based RNNs", "comment": null, "summary": "Recurrent Neural Networks (RNNs) have found widespread applications in\nmachine learning for time series prediction and dynamical systems\nreconstruction, and experienced a recent renaissance with improved training\nalgorithms and architectural designs. Understanding why and how trained RNNs\nproduce their behavior is important for scientific and medical applications,\nand explainable AI more generally. An RNN's dynamical repertoire depends on the\ntopological and geometrical properties of its state space. Stable and unstable\nmanifolds of periodic points play a particularly important role: They dissect a\ndynamical system's state space into different basins of attraction, and their\nintersections lead to chaotic dynamics with fractal geometry. Here we introduce\na novel algorithm for detecting these manifolds, with a focus on\npiecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as\ntheir activation function. We demonstrate how the algorithm can be used to\ntrace the boundaries between different basins of attraction, and hence to\ncharacterize multistability, a computationally important property. We further\nshow its utility in finding so-called homoclinic points, the intersections\nbetween stable and unstable manifolds, and thus establish the existence of\nchaos in PLRNNs. Finally we show for an empirical example, electrophysiological\nrecordings from a cortical neuron, how insights into the underlying dynamics\ncould be gained through our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u7a33\u5b9a\u548c\u4e0d\u7a33\u5b9a\u6d41\u5f62\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790PLRNNs\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5305\u62ec\u591a\u7a33\u6001\u548c\u6df7\u6c8c\u884c\u4e3a\u3002", "motivation": "\u7406\u89e3\u8bad\u7ec3\u540e\u7684RNNs\u5982\u4f55\u4ea7\u751f\u5176\u884c\u4e3a\u5bf9\u4e8e\u79d1\u5b66\u548c\u533b\u5b66\u5e94\u7528\u4ee5\u53ca\u53ef\u89e3\u91caAI\u5f88\u91cd\u8981\uff0c\u7279\u522b\u662f\u5206\u6790\u5176\u52a8\u529b\u5b66\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7b97\u6cd5\u6765\u68c0\u6d4bPLRNNs\u4e2d\u7684\u7a33\u5b9a\u548c\u4e0d\u7a33\u5b9a\u6d41\u5f62\uff0c\u5229\u7528ReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u7279\u6027\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u8ffd\u8e2a\u4e0d\u540c\u5438\u5f15\u76c6\u8fb9\u754c\uff0c\u8868\u5f81\u591a\u7a33\u6001\u6027\uff0c\u53d1\u73b0\u540c\u5bbf\u70b9\u4ee5\u8bc1\u660e\u6df7\u6c8c\u5b58\u5728\uff0c\u5e76\u5728\u5b9e\u9645\u795e\u7ecf\u8bb0\u5f55\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u6790RNNs\u7684\u590d\u6742\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.03817", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03817", "abs": "https://arxiv.org/abs/2510.03817", "authors": ["Philipp Becker", "Niklas Freymuth", "Serge Thilges", "Fabian Otto", "Gerhard Neumann"], "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models", "comment": null, "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has\nbecome the standard choice for reward-based fine-tuning of large language\nmodels (LLMs). Although recent work has explored improved estimators of\nadvantages and normalization, the clipping mechanism itself has remained\nuntouched. Originally introduced as a proxy for principled KL-based trust\nregions, clipping is a crude approximation that often causes unstable updates\nand suboptimal performance. We replace the clip objective with a novel discrete\ndifferentiable trust region projection, which provides principled token-level\nKL constraints. The projection operates on a sparse subset of the model's most\nimportant token logits to balance computational cost and projection\neffectiveness. Our approach, Trust Region Optimization for Large Language\nModels (TROLL), serves as a direct replacement for PPO-like clipping during\ntraining and does not alter the model's inference behavior. Across datasets,\nmodel families, and advantage-estimation methods, TROLL consistently\noutperforms PPO-like clipping in terms of training speed, stability, and final\nsuccess rates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTROLL\u65b9\u6cd5\uff0c\u7528\u79bb\u6563\u53ef\u5fae\u5206\u4fe1\u4efb\u57df\u6295\u5f71\u66ff\u4ee3PPO\u4e2d\u7684clip\u673a\u5236\uff0c\u4e3aLLM\u5956\u52b1\u5fae\u8c03\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "PPO\u4e2d\u7684clip\u673a\u5236\u4f5c\u4e3aKL\u4fe1\u4efb\u57df\u7684\u8fd1\u4f3c\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u5e38\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6b21\u4f18\u6027\u80fd\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u53ef\u5fae\u5206\u4fe1\u4efb\u57df\u6295\u5f71\uff0c\u5728\u6a21\u578b\u6700\u91cd\u8981\u7684token logits\u7a00\u758f\u5b50\u96c6\u4e0a\u65bd\u52a0token\u7ea7KL\u7ea6\u675f\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6295\u5f71\u6548\u679c\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u4e0a\uff0cTROLL\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6210\u529f\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8ePPO-like clip\u3002", "conclusion": "TROLL\u53ef\u4f5c\u4e3aPPO-like clip\u7684\u76f4\u63a5\u66ff\u4ee3\uff0c\u4e0d\u6539\u53d8\u6a21\u578b\u63a8\u7406\u884c\u4e3a\uff0c\u4f46\u663e\u8457\u63d0\u5347RL\u5fae\u8c03\u6548\u679c\u3002"}}
{"id": "2510.03823", "categories": ["cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03823", "abs": "https://arxiv.org/abs/2510.03823", "authors": ["Adam Haroon", "Tristan Schuler"], "title": "Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning", "comment": null, "summary": "High Altitude Balloons (HABs) can leverage stratospheric wind layers for\nlimited horizontal control, enabling applications in reconnaissance,\nenvironmental monitoring, and communications networks. Existing multi-agent HAB\ncoordination approaches use deterministic methods like Voronoi partitioning and\nextremum seeking control for large global constellations, which perform poorly\nfor smaller teams and localized missions. While single-agent HAB control using\nreinforcement learning has been demonstrated on HABs, coordinated multi-agent\nreinforcement learning (MARL) has not yet been investigated. This work presents\nthe first systematic application of multi-agent reinforcement learning (MARL)\nto HAB coordination for distributed area coverage. We extend our previously\ndeveloped reinforcement learning simulation environment (RLHAB) to support\ncooperative multi-agent learning, enabling multiple agents to operate\nsimultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area\ncoverage coordination, leveraging Centralized Training with Decentralized\nExecution to address atmospheric vehicle coordination challenges. Our approach\nemploys specialized observation spaces providing individual state,\nenvironmental context, and teammate data, with hierarchical rewards\nprioritizing coverage while encouraging spatial distribution. We demonstrate\nthat QMIX achieves similar performance to the theoretically optimal geometric\ndeterministic method for distributed area coverage, validating the MARL\napproach and providing a foundation for more complex autonomous multi-HAB\nmissions where deterministic methods become intractable.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u9ad8\u7a7a\u6c14\u7403\u534f\u8c03\u63a7\u5236\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u533a\u57df\u8986\u76d6\u4efb\u52a1\uff0c\u5c55\u793a\u4e86QMIX\u7b97\u6cd5\u5728\u6b64\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9ad8\u7a7a\u6c14\u7403\u591a\u667a\u80fd\u4f53\u534f\u8c03\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u786e\u5b9a\u6027\u65b9\u6cd5\uff08\u5982Voronoi\u5206\u5272\uff09\uff0c\u5728\u5c0f\u578b\u56e2\u961f\u548c\u5c40\u90e8\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u6b64\u9886\u57df\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u6269\u5c55RLHAB\u4eff\u771f\u73af\u5883\u652f\u6301\u591a\u667a\u80fd\u4f53\u5b66\u4e60\uff0c\u91c7\u7528QMIX\u7b97\u6cd5\u7ed3\u5408\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u67b6\u6784\uff0c\u4f7f\u7528\u5305\u542b\u4e2a\u4f53\u72b6\u6001\u3001\u73af\u5883\u4fe1\u606f\u548c\u961f\u53cb\u6570\u636e\u7684\u4e13\u95e8\u89c2\u6d4b\u7a7a\u95f4\uff0c\u4ee5\u53ca\u4f18\u5148\u8986\u76d6\u548c\u9f13\u52b1\u7a7a\u95f4\u5206\u5e03\u7684\u5c42\u6b21\u5316\u5956\u52b1\u3002", "result": "QMIX\u5728\u5206\u5e03\u5f0f\u533a\u57df\u8986\u76d6\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4e0e\u7406\u8bba\u4e0a\u6700\u4f18\u7684\u51e0\u4f55\u786e\u5b9a\u6027\u65b9\u6cd5\u76f8\u4f3c\u7684\u6027\u80fd\u3002", "conclusion": "\u9a8c\u8bc1\u4e86MARL\u65b9\u6cd5\u5728\u9ad8\u7a7a\u6c14\u7403\u534f\u8c03\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u590d\u6742\u7684\u81ea\u4e3b\u591a\u6c14\u7403\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u786e\u5b9a\u6027\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.03824", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03824", "abs": "https://arxiv.org/abs/2510.03824", "authors": ["Wei Guo", "Jaemoo Choi", "Yuchen Zhu", "Molei Tao", "Yongxin Chen"], "title": "Proximal Diffusion Neural Sampler", "comment": "31 pages, 12 figures", "summary": "The task of learning a diffusion-based neural sampler for drawing samples\nfrom an unnormalized target distribution can be viewed as a stochastic optimal\ncontrol problem on path measures. However, the training of neural samplers can\nbe challenging when the target distribution is multimodal with significant\nbarriers separating the modes, potentially leading to mode collapse. We propose\na framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that\naddresses these challenges by tackling the stochastic optimal control problem\nvia proximal point method on the space of path measures. PDNS decomposes the\nlearning process into a series of simpler subproblems that create a path\ngradually approaching the desired distribution. This staged procedure traces a\nprogressively refined path to the desired distribution and promotes thorough\nexploration across modes. For a practical and efficient realization, we\ninstantiate each proximal step with a proximal weighted denoising cross-entropy\n(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS\nthrough extensive experiments on both continuous and discrete sampling tasks,\nincluding challenging scenarios in molecular dynamics and statistical physics.", "AI": {"tldr": "\u63d0\u51faPDNS\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u6d4b\u5ea6\u4e0a\u7684\u8fd1\u7aef\u70b9\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4f7f\u7528\u5206\u9636\u6bb5\u7684\u6e10\u8fdb\u5b66\u4e60\u8fc7\u7a0b", "motivation": "\u4f20\u7edf\u6269\u6563\u795e\u7ecf\u91c7\u6837\u5668\u5728\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\u65f6\u5bb9\u6613\u53d1\u751f\u6a21\u5f0f\u5d29\u6e83\uff0c\u7279\u522b\u662f\u5f53\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u52bf\u5792\u65f6", "method": "\u57fa\u4e8e\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u91c7\u7528\u8def\u5f84\u6d4b\u5ea6\u4e0a\u7684\u8fd1\u7aef\u70b9\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u7b80\u5355\u5b50\u95ee\u9898\uff0c\u4f7f\u7528\u8fd1\u7aef\u52a0\u6743\u53bb\u566a\u4ea4\u53c9\u71b5\u76ee\u6807\u5b9e\u73b0", "result": "\u5728\u8fde\u7eed\u548c\u79bb\u6563\u91c7\u6837\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u5206\u5b50\u52a8\u529b\u5b66\u548c\u7edf\u8ba1\u7269\u7406\u4e2d\u7684\u6311\u6218\u6027\u573a\u666f", "conclusion": "PDNS\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8def\u5f84\u6784\u5efa\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u8de8\u6a21\u5f0f\u7684\u5145\u5206\u63a2\u7d22"}}
{"id": "2510.03830", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03830", "abs": "https://arxiv.org/abs/2510.03830", "authors": ["Alex Durkin", "Jasper Stolte", "Mehmet Mercang\u00f6z"], "title": "HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control", "comment": "31 pages, 15 figures, submitted to Computers and Chemical Engineering", "summary": "Start-ups and product grade-changes are critical steps in continuous-process\nplant operation, because any misstep immediately affects product quality and\ndrives operational losses. These transitions have long relied on manual\noperation by a handful of expert operators, but the progressive retirement of\nthat workforce is leaving plant owners without the tacit know-how needed to\nexecute them consistently. In the absence of a process model, offline\nreinforcement learning (RL) promises to capture and even surpass human\nexpertise by mining historical start-up and grade-change logs, yet standard\noffline RL struggles with distribution shift and value-overestimation whenever\na learned policy ventures outside the data envelope. We introduce HOFLON\n(Hybrid Offline Learning + Online Optimization) to overcome those limitations.\nOffline, HOFLON learns (i) a latent data manifold that represents the feasible\nregion spanned by past transitions and (ii) a long-horizon Q-critic that\npredicts the cumulative reward from state-action pairs. Online, it solves a\none-step optimization problem that maximizes the Q-critic while penalizing\ndeviations from the learned manifold and excessive rates of change in the\nmanipulated variables. We test HOFLON on two industrial case studies: a\npolymerization reactor start-up and a paper-machine grade-change problem, and\nbenchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.\nIn both plants HOFLON not only surpasses IQL but also delivers, on average,\nbetter cumulative rewards than the best start-up or grade-change observed in\nthe historical data, demonstrating its potential to automate transition\noperations beyond current expert capability.", "AI": {"tldr": "HOFLON\u662f\u4e00\u79cd\u6df7\u5408\u79bb\u7ebf\u5b66\u4e60+\u5728\u7ebf\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fde\u7eed\u8fc7\u7a0b\u5de5\u5382\u542f\u52a8\u548c\u4ea7\u54c1\u7b49\u7ea7\u8f6c\u6362\u4e2d\u7684\u64cd\u4f5c\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u514b\u670d\u4e86\u6807\u51c6\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5e03\u504f\u79fb\u548c\u4ef7\u503c\u9ad8\u4f30\u95ee\u9898\u3002", "motivation": "\u5de5\u5382\u542f\u52a8\u548c\u4ea7\u54c1\u7b49\u7ea7\u8f6c\u6362\u4f9d\u8d56\u5c11\u6570\u4e13\u5bb6\u64cd\u4f5c\u5458\u7684\u624b\u52a8\u64cd\u4f5c\uff0c\u4f46\u968f\u7740\u8fd9\u4e9b\u4e13\u5bb6\u9000\u4f11\uff0c\u5de5\u5382\u6240\u6709\u8005\u7f3a\u4e4f\u6267\u884c\u8fd9\u4e9b\u5173\u952e\u64cd\u4f5c\u6240\u9700\u7684\u9690\u6027\u77e5\u8bc6\u3002\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6316\u6398\u5386\u53f2\u6570\u636e\u6765\u6355\u83b7\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u6807\u51c6\u65b9\u6cd5\u5728\u7b56\u7565\u8d85\u51fa\u6570\u636e\u8303\u56f4\u65f6\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u4ef7\u503c\u9ad8\u4f30\u95ee\u9898\u3002", "method": "HOFLON\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u5b66\u4e60(i)\u8868\u793a\u8fc7\u53bb\u8f6c\u6362\u53ef\u884c\u533a\u57df\u7684\u6f5c\u5728\u6570\u636e\u6d41\u5f62\u548c(ii)\u9884\u6d4b\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7d2f\u79ef\u5956\u52b1\u7684\u957f\u671fQ\u8bc4\u4f30\u5668\uff1b\u5728\u7ebf\u9636\u6bb5\u6c42\u89e3\u5355\u6b65\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5927\u5316Q\u8bc4\u4f30\u5668\u540c\u65f6\u60e9\u7f5a\u504f\u79bb\u5b66\u4e60\u6d41\u5f62\u548c\u64cd\u7eb5\u53d8\u91cf\u53d8\u5316\u7387\u8fc7\u5927\u3002", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff08\u805a\u5408\u53cd\u5e94\u5668\u542f\u52a8\u548c\u9020\u7eb8\u673a\u7b49\u7ea7\u8f6c\u6362\uff09\u4e2d\uff0cHOFLON\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u79bb\u7ebfRL\u7b97\u6cd5IQL\uff0c\u800c\u4e14\u5e73\u5747\u83b7\u5f97\u4e86\u6bd4\u5386\u53f2\u6570\u636e\u4e2d\u6700\u4f73\u542f\u52a8\u6216\u7b49\u7ea7\u8f6c\u6362\u66f4\u597d\u7684\u7d2f\u79ef\u5956\u52b1\u3002", "conclusion": "HOFLON\u5c55\u793a\u4e86\u8d85\u8d8a\u5f53\u524d\u4e13\u5bb6\u80fd\u529b\u81ea\u52a8\u5316\u8f6c\u6362\u64cd\u4f5c\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u751f\u6210\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\u7684\u7b56\u7565\u3002"}}
{"id": "2510.03838", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03838", "abs": "https://arxiv.org/abs/2510.03838", "authors": ["Behraj Khan", "Tahir Qasim Syed"], "title": "Technical note on Fisher Information for Robust Federated Cross-Validation", "comment": null, "summary": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets.", "AI": {"tldr": "\u63d0\u51faFIRE\u65b9\u6cd5\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u6765\u4f30\u8ba1\u548c\u8865\u507f\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u788e\u7247\u5316\u5f15\u8d77\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u63d0\u5347\u6a21\u578b\u5728\u504f\u79fb\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u8bad\u7ec3\u6570\u636e\u5728\u6279\u6b21\u95f4\u6216\u5730\u7406\u4f4d\u7f6e\u4e0a\u788e\u7247\u5316\u65f6\uff0c\u8bad\u7ec3\u6a21\u578b\u4f1a\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u534f\u53d8\u91cf\u504f\u79fb\u5bfc\u81f4\u7684\u5404\u6570\u636e\u7247\u6bb5\u5206\u5e03\u4e0e\u5168\u5c40\u8bad\u7ec3\u5206\u5e03\u4e0d\u4e00\u81f4\u3002", "method": "FIRE\u65b9\u6cd5\u901a\u8fc7\u8fd1\u4f3cFisher\u4fe1\u606f\u6765\u7d2f\u79ef\u788e\u7247\u5316\u5f15\u8d77\u7684\u534f\u53d8\u91cf\u504f\u79fb\u5dee\u5f02\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6bcf\u4e2a\u6570\u636e\u7247\u6bb5\u7684\u635f\u5931\u60e9\u7f5a\u9879\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "result": "FIRE\u5728\u504f\u79fb\u9a8c\u8bc1\u96c6\u4e0a\u6bd4\u91cd\u8981\u6027\u52a0\u6743\u57fa\u51c6\u65b9\u6cd5\u6700\u591a\u63d0\u53475.1%\uff0c\u6bd4\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u65b9\u6cd5\u6700\u591a\u63d0\u53475.3%\u3002", "conclusion": "FIRE\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u4f30\u8ba1\u534f\u53d8\u91cf\u504f\u79fb\u5e76\u5b9e\u73b0\u5206\u5e03\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.03839", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03839", "abs": "https://arxiv.org/abs/2510.03839", "authors": ["Behraj Khan", "Tahir Qasim Syed"], "title": "Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting", "comment": null, "summary": "We present a theoretical framework for M-FISHER, a method for sequential\ndistribution shift detection and stable adaptation in streaming data. For\ndetection, we construct an exponential martingale from non-conformity scores\nand apply Ville's inequality to obtain time-uniform guarantees on false alarm\ncontrol, ensuring statistical validity at any stopping time. Under sustained\nshifts, we further bound the expected detection delay as\n$\\mathcal{O}(\\log(1/\\delta)/\\Gamma)$, where $\\Gamma$ reflects the post-shift\ninformation gain, thereby linking detection efficiency to distributional\ndivergence. For adaptation, we show that Fisher-preconditioned updates of\nprompt parameters implement natural gradient descent on the distributional\nmanifold, yielding locally optimal updates that minimize KL divergence while\npreserving stability and parameterization invariance. Together, these results\nestablish M-FISHER as a principled approach for robust, anytime-valid detection\nand geometrically stable adaptation in sequential decision-making under\ncovariate shift.", "AI": {"tldr": "M-FISHER\u662f\u4e00\u4e2a\u7528\u4e8e\u6d41\u6570\u636e\u4e2d\u5e8f\u5217\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u548c\u7a33\u5b9a\u9002\u5e94\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u6570\u9785\u548cVille\u4e0d\u7b49\u5f0f\u63d0\u4f9b\u65f6\u95f4\u5747\u5300\u7684\u8bef\u62a5\u63a7\u5236\u4fdd\u8bc1\uff0c\u5e76\u4f7f\u7528Fisher\u9884\u5904\u7406\u7684\u63d0\u793a\u53c2\u6570\u66f4\u65b0\u5b9e\u73b0\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u6d41\u6570\u636e\u4e2d\u5e8f\u5217\u5206\u5e03\u6f02\u79fb\u7684\u68c0\u6d4b\u548c\u9002\u5e94\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u534f\u53d8\u91cf\u6f02\u79fb\u4e0b\u5177\u6709\u7edf\u8ba1\u6709\u6548\u6027\u548c\u51e0\u4f55\u7a33\u5b9a\u6027\u7684\u51b3\u7b56\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u975e\u4e00\u81f4\u6027\u5206\u6570\u7684\u6307\u6570\u9785\uff0c\u5e94\u7528Ville\u4e0d\u7b49\u5f0f\u8fdb\u884c\u65f6\u95f4\u5747\u5300\u7684\u8bef\u62a5\u63a7\u5236\uff1b\u4f7f\u7528Fisher\u9884\u5904\u7406\u7684\u63d0\u793a\u53c2\u6570\u66f4\u65b0\u5b9e\u73b0\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u6301\u7eed\u6f02\u79fb\u4e0b\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u7684\u671f\u671b\u4e0a\u754c\u4e3aO(log(1/\u03b4)/\u0393)\uff0c\u5176\u4e2d\u0393\u53cd\u6620\u540e\u6f02\u79fb\u4fe1\u606f\u589e\u76ca\uff1b\u9002\u5e94\u8fc7\u7a0b\u5728\u5206\u5e03\u6d41\u5f62\u4e0a\u5b9e\u73b0\u5c40\u90e8\u6700\u4f18\u66f4\u65b0\uff0c\u6700\u5c0f\u5316KL\u6563\u5ea6\u3002", "conclusion": "M-FISHER\u4e3a\u5e8f\u5217\u51b3\u7b56\u4e2d\u534f\u53d8\u91cf\u6f02\u79fb\u4e0b\u7684\u7a33\u5065\u3001\u968f\u65f6\u6709\u6548\u68c0\u6d4b\u548c\u51e0\u4f55\u7a33\u5b9a\u9002\u5e94\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2510.03844", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.03844", "abs": "https://arxiv.org/abs/2510.03844", "authors": ["Sarah C. Lotspeich", "Abbey Collins", "Brian J. Wells", "Ashish K. Khanna", "Joseph Rigdon", "Lucy D'Agostino McGowan"], "title": "On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records", "comment": null, "summary": "Objective: Electronic health records (EHR) data are prone to missingness and\nerrors. Previously, we devised an \"enriched\" chart review protocol where a\n\"roadmap\" of auxiliary diagnoses (anchors) was used to recover missing values\nin EHR data (e.g., a diagnosis of impaired glycemic control might imply that a\nmissing hemoglobin A1c value would be considered unhealthy). Still, chart\nreviews are expensive and time-intensive, which limits the number of patients\nwhose data can be reviewed. Now, we investigate the accuracy and scalability of\na roadmap-driven algorithm, based on ICD-10 codes (International Classification\nof Diseases, 10th revision), to mimic expert chart reviews and recover missing\nvalues. Materials and Methods: In addition to the clinicians' original roadmap\nfrom our previous work, we consider new versions that were iteratively refined\nusing large language models (LLM) in conjunction with clinical expertise to\nexpand the list of auxiliary diagnoses. Using chart reviews for 100 patients\nfrom the EHR at an extensive learning health system, we examine algorithm\nperformance with different roadmaps. Using the larger study of $1000$ patients,\nwe applied the final algorithm, which used a roadmap with clinician-approved\nadditions from the LLM. Results: The algorithm recovered as much, if not more,\nmissing data as the expert chart reviewers, depending on the roadmap.\nDiscussion: Clinically-driven algorithms (enhanced by LLM) can recover missing\nEHR data with similar accuracy to chart reviews and can feasibly be applied to\nlarge samples. Extending them to monitor other dimensions of data quality\n(e.g., plausability) is a promising future direction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eICD-10\u4ee3\u7801\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u4e34\u5e8a\u8def\u5f84\u6765\u6062\u590d\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u7f3a\u5931\u6570\u636e\uff0c\u5176\u51c6\u786e\u6027\u4e0e\u4e13\u5bb6\u56fe\u8868\u5ba1\u67e5\u76f8\u5f53\uff0c\u4f46\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u5b58\u5728\u7f3a\u5931\u548c\u9519\u8bef\u95ee\u9898\uff0c\u4f20\u7edf\u56fe\u8868\u5ba1\u67e5\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9650\u5236\u4e86\u53ef\u5ba1\u67e5\u7684\u60a3\u8005\u6570\u91cf\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6570\u636e\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eICD-10\u4ee3\u7801\u7684\u8def\u5f84\u9a71\u52a8\u7b97\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u8f85\u52a9\u8bca\u65ad\u5217\u8868\uff0c\u901a\u8fc7\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u9a8c\u8bc1\uff0c\u5728100\u540d\u60a3\u8005\u6837\u672c\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u8def\u5f84\u7684\u6027\u80fd\u3002", "result": "\u7b97\u6cd5\u6062\u590d\u7684\u7f3a\u5931\u6570\u636e\u91cf\u4e0e\u4e13\u5bb6\u56fe\u8868\u5ba1\u67e5\u76f8\u5f53\u751a\u81f3\u66f4\u591a\uff0c\u53d6\u51b3\u4e8e\u6240\u7528\u8def\u5f84\u7684\u8d28\u91cf\u3002\u57281000\u540d\u60a3\u8005\u7684\u5927\u6837\u672c\u4e2d\uff0c\u4f7f\u7528LLM\u589e\u5f3a\u7684\u6700\u7ec8\u8def\u5f84\u7b97\u6cd5\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u4e34\u5e8a\u9a71\u52a8\u7684\u7b97\u6cd5\uff08\u901a\u8fc7LLM\u589e\u5f3a\uff09\u80fd\u591f\u4ee5\u4e0e\u56fe\u8868\u5ba1\u67e5\u76f8\u4f3c\u7684\u51c6\u786e\u6027\u6062\u590d\u7f3a\u5931\u7684EHR\u6570\u636e\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6837\u672c\uff0c\u672a\u6765\u6709\u671b\u6269\u5c55\u5230\u76d1\u6d4b\u6570\u636e\u8d28\u91cf\u7684\u5176\u4ed6\u7ef4\u5ea6\u3002"}}
{"id": "2510.03865", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03865", "abs": "https://arxiv.org/abs/2510.03865", "authors": ["Wenhao Deng", "Long Wei", "Chenglei Yu", "Tailin Wu"], "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced\nthe reasoning capabilities of large language models (LLMs), particularly for\nmathematical problem solving. However, a fundamental limitation remains: as the\nsampling budget increases, the advantage of RLVR-trained models over their\npretrained bases often diminishes or even vanishes, revealing a strong\ndependence on the base model's restricted search space. We attribute this\nphenomenon to the widespread use of the reverse Kullback-Leibler (KL)\ndivergence regularizer, whose mode-seeking behavior keeps the policy trapped\ninside the base model's support region and hampers wider exploration. To\naddress this issue, we propose RAPO (Rewards-Aware Policy Optimization), an\nalgorithm to promote broader yet focused exploration. Our method (i) utilizes\nthe forward KL penalty to replace the reverse KL penalty for\nout-of-distribution exploration, and (ii) reweights the reference policy to\nfacilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B\nmodels with RAPO on the 8K SimpleRL-Zero dataset, without supervised\nfine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO\nconsistently improves problem-solving performance. Notably, RAPO enables models\nto surpass the base model's performance ceiling and solves previously\nintractable problems, advancing the frontier of RLVR for challenging reasoning\ntasks.", "AI": {"tldr": "\u63d0\u51faRAPO\u7b97\u6cd5\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u53cd\u5411KL\u6563\u5ea6\u5bfc\u81f4\u7684\u63a2\u7d22\u53d7\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u524d\u5411KL\u60e9\u7f5a\u548c\u53c2\u8003\u7b56\u7565\u91cd\u52a0\u6743\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u800c\u805a\u7126\u7684\u63a2\u7d22\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u589e\u52a0\u91c7\u6837\u9884\u7b97\u65f6\u4f18\u52bf\u51cf\u5f31\uff0c\u8fd9\u6e90\u4e8e\u53cd\u5411KL\u6563\u5ea6\u7684\u6a21\u5f0f\u5bfb\u6c42\u884c\u4e3a\u9650\u5236\u4e86\u7b56\u7565\u5728\u57fa\u7840\u6a21\u578b\u652f\u6301\u533a\u57df\u5185\u7684\u63a2\u7d22", "method": "RAPO\u7b97\u6cd5\uff1a(i)\u4f7f\u7528\u524d\u5411KL\u60e9\u7f5a\u66ff\u4ee3\u53cd\u5411KL\u60e9\u7f5a\u8fdb\u884c\u5206\u5e03\u5916\u63a2\u7d22\uff1b(ii)\u91cd\u52a0\u6743\u53c2\u8003\u7b56\u7565\u5b9e\u73b0\u81ea\u9002\u5e94\u5206\u5e03\u5185\u63a2\u7d22", "result": "\u5728Qwen2.5-3B\u548c7B\u6a21\u578b\u4e0a\u8bad\u7ec3\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5728AIME2024\u548cAIME2025\u8bc4\u4f30\u4e2d\u6301\u7eed\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u6027\u80fd\uff0c\u7a81\u7834\u57fa\u7840\u6a21\u578b\u6027\u80fd\u4e0a\u9650", "conclusion": "RAPO\u63a8\u52a8\u4e86RLVR\u5728\u6311\u6218\u6027\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u524d\u6cbf\uff0c\u80fd\u591f\u89e3\u51b3\u5148\u524d\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898"}}
{"id": "2510.03866", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03866", "abs": "https://arxiv.org/abs/2510.03866", "authors": ["Xinwen Zhang", "Hongchang Gao"], "title": "On Provable Benefits of Muon in Federated Learning", "comment": null, "summary": "The recently introduced optimizer, Muon, has gained increasing attention due\nto its superior performance across a wide range of applications. However, its\neffectiveness in federated learning remains unexplored. To address this gap,\nthis paper investigates the performance of Muon in the federated learning\nsetting. Specifically, we propose a new algorithm, FedMuon, and establish its\nconvergence rate for nonconvex problems. Our theoretical analysis reveals\nmultiple favorable properties of FedMuon. In particular, due to its\northonormalized update direction, the learning rate of FedMuon is independent\nof problem-specific parameters, and, importantly, it can naturally accommodate\nheavy-tailed noise. The extensive experiments on a variety of neural network\narchitectures validate the effectiveness of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86FedMuon\u7b97\u6cd5\uff0c\u5c06Muon\u4f18\u5316\u5668\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff0c\u5efa\u7acb\u4e86\u975e\u51f8\u95ee\u9898\u7684\u6536\u655b\u7387\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faFedMuon\u7b97\u6cd5\uff0c\u5229\u7528\u6b63\u4ea4\u5316\u66f4\u65b0\u65b9\u5411\uff0c\u4f7f\u5176\u5b66\u4e60\u7387\u72ec\u7acb\u4e8e\u95ee\u9898\u7279\u5b9a\u53c2\u6570\uff0c\u5e76\u80fd\u81ea\u7136\u5904\u7406\u91cd\u5c3e\u566a\u58f0\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aFedMuon\u5177\u6709\u591a\u4e2a\u6709\u5229\u7279\u6027\uff0c\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedMuon\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u9a8c\u8bc1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03871", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03871", "abs": "https://arxiv.org/abs/2510.03871", "authors": ["Oleg Filatov", "Jiangtao Wang", "Jan Ebert", "Stefan Kesselheim"], "title": "Optimal Scaling Needs Optimal Norm", "comment": null, "summary": "Despite recent progress in optimal hyperparameter transfer under model and\ndataset scaling, no unifying explanatory principle has been established. Using\nthe Scion optimizer, we discover that joint optimal scaling across model and\ndataset sizes is governed by a single invariant: the operator norm of the\noutput layer. Across models with up to 1.3B parameters trained on up to 138B\ntokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$\nconsistently has the same operator norm value - a phenomenon we term norm\ntransfer. This constant norm condition is necessary but not sufficient: while\nfor each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a\nunique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient\ncondition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$\nscaling with dataset size for Scion, and find that the scaling rules are\nconsistent with those of the Adam optimizer. Tuning per-layer-group learning\nrates also improves model performance, with the output layer being the most\nsensitive and hidden layers benefiting from lower learning rates. We provide\npractical insights on norm-guided optimal scaling and release our Distributed\nScion (Disco) implementation with logs from over two thousand runs to support\nresearch on LLM training dynamics at scale.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u548c\u6570\u636e\u96c6\u89c4\u6a21\u8054\u5408\u6700\u4f18\u7f29\u653e\u7531\u8f93\u51fa\u5c42\u7b97\u5b50\u8303\u6570\u8fd9\u4e00\u5355\u4e00\u4e0d\u53d8\u91cf\u51b3\u5b9a\uff0c\u79f0\u4e3a\u8303\u6570\u4f20\u9012\u73b0\u8c61\u3002\u6700\u4f18\u5b66\u4e60\u7387\u548c\u6279\u6b21\u5927\u5c0f\u7ec4\u5408\u5177\u6709\u76f8\u540c\u7684\u7b97\u5b50\u8303\u6570\u503c\uff0c\u4e14\u8be5\u6761\u4ef6\u662f\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u7684\u3002", "motivation": "\u5c3d\u7ba1\u5728\u6a21\u578b\u548c\u6570\u636e\u96c6\u7f29\u653e\u4e0b\u7684\u6700\u4f18\u8d85\u53c2\u6570\u4f20\u9012\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u89e3\u91ca\u539f\u7406\u3002\u672c\u6587\u65e8\u5728\u53d1\u73b0\u63a7\u5236\u8054\u5408\u6700\u4f18\u7f29\u653e\u7684\u57fa\u672c\u89c4\u5f8b\u3002", "method": "\u4f7f\u7528Scion\u4f18\u5316\u5668\uff0c\u5728\u591a\u8fbe13\u4ebf\u53c2\u6570\u6a21\u578b\u548c1380\u4ebftoken\u6570\u636e\u96c6\u4e0a\u7814\u7a76\u6700\u4f18\u5b66\u4e60\u7387\u548c\u6279\u6b21\u5927\u5c0f\u7ec4\u5408\uff0c\u5206\u6790\u7b97\u5b50\u8303\u6570\u4e0d\u53d8\u6027\uff0c\u5e76\u6d4b\u91cf\u7f29\u653e\u89c4\u5219\u3002", "result": "\u53d1\u73b0\u6700\u4f18\u5b66\u4e60\u7387/\u6279\u6b21\u5927\u5c0f\u7ec4\u5408\u59cb\u7ec8\u5177\u6709\u76f8\u540c\u7684\u7b97\u5b50\u8303\u6570\u503c\uff0c\u8fd9\u662f\u5fc5\u8981\u4f46\u4e0d\u5145\u5206\u6761\u4ef6\u3002\u6d4b\u91cf\u4e86Scion\u4f18\u5316\u5668\u7684\u7f29\u653e\u89c4\u5219\uff0c\u4e0eAdam\u4f18\u5316\u5668\u4e00\u81f4\u3002\u5206\u5c42\u7ec4\u5b66\u4e60\u7387\u8c03\u4f18\u53ef\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8f93\u51fa\u5c42\u7b97\u5b50\u8303\u6570\u662f\u63a7\u5236\u8054\u5408\u6700\u4f18\u7f29\u653e\u7684\u5173\u952e\u4e0d\u53d8\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u53d1\u5e03\u4e86Distributed Scion\u5b9e\u73b0\u548c\u4e24\u5343\u591a\u6b21\u8fd0\u884c\u65e5\u5fd7\u3002"}}
{"id": "2510.03893", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.03893", "abs": "https://arxiv.org/abs/2510.03893", "authors": ["Akshay Kudva", "Joel A. Paulson"], "title": "BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty", "comment": "Published in Computers and Chemical Engineering, 2025", "summary": "Optimal design under uncertainty remains a fundamental challenge in advancing\nreliable, next-generation process systems. Robust optimization (RO) offers a\nprincipled approach by safeguarding against worst-case scenarios across a range\nof uncertain parameters. However, traditional RO methods typically require\nknown problem structure, which limits their applicability to high-fidelity\nsimulation environments. To overcome these limitations, recent work has\nexplored robust Bayesian optimization (RBO) as a flexible alternative that can\naccommodate expensive, black-box objectives. Existing RBO methods, however,\ngenerally ignore available structural information and struggle to scale to\nhigh-dimensional settings. In this work, we introduce BONSAI (Bayesian\nOptimization of Network Systems under uncertAInty), a new RBO framework that\nleverages partial structural knowledge commonly available in simulation-based\nmodels. Instead of treating the objective as a monolithic black box, BONSAI\nrepresents it as a directed graph of interconnected white- and black-box\ncomponents, allowing the algorithm to utilize intermediate information within\nthe optimization process. We further propose a scalable Thompson sampling-based\nacquisition function tailored to the structured RO setting, which can be\nefficiently optimized using gradient-based methods. We evaluate BONSAI across a\ndiverse set of synthetic and real-world case studies, including applications in\nprocess systems engineering. Compared to existing simulation-based RO\nalgorithms, BONSAI consistently delivers more sample-efficient and\nhigher-quality robust solutions, highlighting its practical advantages for\nuncertainty-aware design in complex engineering systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86BONSAI\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u90e8\u5206\u7ed3\u6784\u77e5\u8bc6\u7684\u9c81\u68d2\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u7f51\u7edc\u7cfb\u7edf\u4f18\u5316\u95ee\u9898", "motivation": "\u4f20\u7edf\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u95ee\u9898\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff1b\u73b0\u6709\u9c81\u68d2\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u5ffd\u7565\u7ed3\u6784\u4fe1\u606f\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u7ef4\u573a\u666f", "method": "\u5c06\u76ee\u6807\u51fd\u6570\u8868\u793a\u4e3a\u6709\u5411\u56fe\uff0c\u5305\u542b\u767d\u76d2\u548c\u9ed1\u76d2\u7ec4\u4ef6\uff0c\u5229\u7528\u4e2d\u95f4\u4fe1\u606f\uff1b\u63d0\u51fa\u57fa\u4e8eThompson\u91c7\u6837\u7684\u53ef\u6269\u5c55\u91c7\u96c6\u51fd\u6570\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u4eff\u771f\u9c81\u68d2\u4f18\u5316\u7b97\u6cd5\uff0cBONSAI\u80fd\u63d0\u4f9b\u66f4\u6837\u672c\u9ad8\u6548\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u9c81\u68d2\u89e3", "conclusion": "BONSAI\u5728\u590d\u6742\u5de5\u7a0b\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bbe\u8ba1\u4e2d\u5177\u6709\u5b9e\u9645\u4f18\u52bf"}}
{"id": "2510.03904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03904", "abs": "https://arxiv.org/abs/2510.03904", "authors": ["Hangting Ye", "Jinmeng Li", "He Zhao", "Mingchen Zhuge", "Dandan Guo", "Yi Chang", "Hongyuan Zha"], "title": "LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis", "comment": null, "summary": "Existing anomaly detection (AD) methods for tabular data usually rely on some\nassumptions about anomaly patterns, leading to inconsistent performance in\nreal-world scenarios. While Large Language Models (LLMs) show remarkable\nreasoning capabilities, their direct application to tabular AD is impeded by\nfundamental challenges, including difficulties in processing heterogeneous data\nand significant privacy risks. To address these limitations, we propose\nLLM-DAS, a novel framework that repositions the LLM from a ``data processor''\nto an ``algorithmist''. Instead of being exposed to raw data, our framework\nleverages the LLM's ability to reason about algorithms. It analyzes a\nhigh-level description of a given detector to understand its intrinsic\nweaknesses and then generates detector-specific, data-agnostic Python code to\nsynthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.\nThis generated synthesis program, which is reusable across diverse datasets, is\nthen instantiated to augment training data, systematically enhancing the\ndetector's robustness by transforming the problem into a more discriminative\ntwo-class classification task. Extensive experiments on 36 TAD benchmarks show\nthat LLM-DAS consistently boosts the performance of mainstream detectors. By\nbridging LLM reasoning with classic AD algorithms via programmatic synthesis,\nLLM-DAS offers a scalable, effective, and privacy-preserving approach to\npatching the logical blind spots of existing detectors.", "AI": {"tldr": "LLM-DAS\u6846\u67b6\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u6570\u636e\u5904\u7406\u8005\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u7b97\u6cd5\u4e13\u5bb6\uff0c\u901a\u8fc7\u5206\u6790\u68c0\u6d4b\u5668\u7684\u5f31\u70b9\u751f\u6210\u5408\u6210\u7a0b\u5e8f\u6765\u521b\u5efa\u96be\u4ee5\u68c0\u6d4b\u7684\u5f02\u5e38\uff0c\u4ece\u800c\u589e\u5f3a\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u5f02\u5e38\u6a21\u5f0f\u7684\u5047\u8bbe\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\u3002\u867d\u7136LLM\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u5f02\u6784\u6570\u636e\u5904\u7406\u56f0\u96be\u548c\u9690\u79c1\u98ce\u9669\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faLLM-DAS\u6846\u67b6\uff0c\u5229\u7528LLM\u5206\u6790\u68c0\u6d4b\u5668\u7684\u9ad8\u5c42\u63cf\u8ff0\u6765\u7406\u89e3\u5176\u56fa\u6709\u5f31\u70b9\uff0c\u7136\u540e\u751f\u6210\u68c0\u6d4b\u5668\u7279\u5b9a\u3001\u6570\u636e\u65e0\u5173\u7684Python\u4ee3\u7801\u6765\u5408\u6210\u96be\u4ee5\u68c0\u6d4b\u7684\u5f02\u5e38\uff0c\u901a\u8fc7\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u53ef\u533a\u5206\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u572836\u4e2a\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLLM-DAS\u80fd\u591f\u6301\u7eed\u63d0\u5347\u4e3b\u6d41\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u5408\u6210\u5c06LLM\u63a8\u7406\u4e0e\u7ecf\u5178\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0cLLM-DAS\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6709\u6548\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u65b9\u6cd5\u6765\u4fee\u8865\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u903b\u8f91\u76f2\u70b9\u3002"}}
{"id": "2510.03911", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03911", "abs": "https://arxiv.org/abs/2510.03911", "authors": ["Yadav Mahesh Lorik", "Kaushik Sarveswaran", "Nagaraj Sundaramahalingam", "Aravindakumar Venugopalan"], "title": "THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series", "comment": "Oral Presentation. AI4TS Workshop, IJCAI'25", "summary": "Time series anomaly detection forms a very crucial area in several domains\nbut poses substantial challenges. Due to time series data possessing\nseasonality, trends, noise, and evolving patterns (concept drift), it becomes\nvery difficult to set a general notion of what constitutes normal behavior.\nAnomalies themselves could be varied, ranging from a single outlier to\ncontextual or collective anomalies, and are normally very rare; hence, the\ndataset is largely imbalanced. Additional layers of complexities arise due to\nthe problems of increased dimensionality of modern time series, real-time\ndetection criteria, setting up appropriate detection thresholds, and arriving\nat results that are interpretable. To embrace these multifaceted challenges,\nvery strong, flexible, and interpretable approaches are required. This paper\npresents THEMIS, a new framework for time series anomaly detection that\nexploits pretrained knowledge from foundation models. THEMIS extracts\nembeddings from the encoder of the Chronos time series foundation model and\napplies outlier detection techniques like Local Outlier Factor and Spectral\nDecomposition on the self-similarity matrix, to spot anomalies in the data. Our\nexperiments show that this modular method achieves SOTA results on the MSL\ndataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.\nNotably, THEMIS exceeds models trained specifically for anomaly detection,\npresenting hyperparameter robustness and interpretability by default. This\npaper advocates for pretrained representations from foundation models for\nperforming efficient and adaptable anomaly detection for time series data.", "AI": {"tldr": "THEMIS\u662f\u4e00\u4e2a\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6Chronos\u6a21\u578b\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5e94\u7528\u5c40\u90e8\u79bb\u7fa4\u56e0\u5b50\u548c\u8c31\u5206\u89e3\u6280\u672f\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u5b63\u8282\u6027\u3001\u8d8b\u52bf\u3001\u566a\u58f0\u3001\u6982\u5ff5\u6f02\u79fb\u7b49\u591a\u91cd\u6311\u6218\uff0c\u5f02\u5e38\u7c7b\u578b\u591a\u6837\u4e14\u7f55\u89c1\u5bfc\u81f4\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u9700\u8981\u5f3a\u5927\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u53d6Chronos\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7f16\u7801\u5668\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5728\u81ea\u76f8\u4f3c\u77e9\u9635\u4e0a\u5e94\u7528\u5c40\u90e8\u79bb\u7fa4\u56e0\u5b50\u548c\u8c31\u5206\u89e3\u7b49\u79bb\u7fa4\u68c0\u6d4b\u6280\u672f\u6765\u8bc6\u522b\u5f02\u5e38\u3002", "result": "\u5728MSL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u7ed3\u679c\uff0c\u5728SMAP\u548cSWAT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u4e3a\u5f02\u5e38\u68c0\u6d4b\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5177\u6709\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u5021\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8868\u793a\u6765\u8fdb\u884c\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2510.03912", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03912", "abs": "https://arxiv.org/abs/2510.03912", "authors": ["Liyuan Hu", "Jitao Wang", "Zhenke Wu", "Chengchun Shi"], "title": "Generalized Fitted Q-Iteration with Clustered Data", "comment": null, "summary": "This paper focuses on reinforcement learning (RL) with clustered data, which\nis commonly encountered in healthcare applications. We propose a generalized\nfitted Q-iteration (FQI) algorithm that incorporates generalized estimating\nequations into policy learning to handle the intra-cluster correlations.\nTheoretically, we demonstrate (i) the optimalities of our Q-function and policy\nestimators when the correlation structure is correctly specified, and (ii)\ntheir consistencies when the structure is mis-specified. Empirically, through\nsimulations and analyses of a mobile health dataset, we find the proposed\ngeneralized FQI achieves, on average, a half reduction in regret compared to\nthe standard FQI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u805a\u7c7b\u6570\u636e\u7684\u5e7f\u4e49\u62df\u5408Q\u8fed\u4ee3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5e7f\u4e49\u4f30\u8ba1\u65b9\u7a0b\u6765\u5904\u7406\u7ec4\u5185\u76f8\u5173\u6027\uff0c\u5728\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6FQI\u3002", "motivation": "\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u7ecf\u5e38\u9047\u5230\u805a\u7c7b\u6570\u636e\uff0c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7ec4\u5185\u76f8\u5173\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u7684\u7b97\u6cd5\u3002", "method": "\u5c06\u5e7f\u4e49\u4f30\u8ba1\u65b9\u7a0b\u6574\u5408\u5230\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u63d0\u51fa\u5e7f\u4e49\u62df\u5408Q\u8fed\u4ee3\u7b97\u6cd5\u6765\u5904\u7406\u805a\u7c7b\u6570\u636e\u4e2d\u7684\u7ec4\u5185\u76f8\u5173\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5f53\u76f8\u5173\u7ed3\u6784\u6b63\u786e\u6307\u5b9a\u65f6\uff0cQ\u51fd\u6570\u548c\u7b56\u7565\u4f30\u8ba1\u5668\u5177\u6709\u6700\u4f18\u6027\uff1b\u5f53\u7ed3\u6784\u8bef\u6307\u5b9a\u65f6\u4ecd\u4fdd\u6301\u4e00\u81f4\u6027\u3002\u5b9e\u8bc1\u663e\u793a\uff1a\u76f8\u6bd4\u6807\u51c6FQI\uff0c\u5e73\u5747\u51cf\u5c11\u4e00\u534a\u7684\u9057\u61be\u3002", "conclusion": "\u5e7f\u4e49FQI\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u805a\u7c7b\u6570\u636e\uff0c\u5728\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2510.03917", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.03917", "abs": "https://arxiv.org/abs/2510.03917", "authors": ["Vinod Raman", "Shenghao Xie", "Samson Zhou"], "title": "Transductive and Learning-Augmented Online Regression", "comment": null, "summary": "Motivated by the predictable nature of real-life in data streams, we study\nonline regression when the learner has access to predictions about future\nexamples. In the extreme case, called transductive online learning, the\nsequence of examples is revealed to the learner before the game begins. For\nthis setting, we fully characterize the minimax expected regret in terms of the\nfat-shattering dimension, establishing a separation between transductive online\nregression and (adversarial) online regression. Then, we generalize this\nsetting by allowing for noisy or \\emph{imperfect} predictions about future\nexamples. Using our results for the transductive online setting, we develop an\nonline learner whose minimax expected regret matches the worst-case regret,\nimproves smoothly with prediction quality, and significantly outperforms the\nworst-case regret when future example predictions are precise, achieving\nperformance similar to the transductive online learner. This enables\nlearnability for previously unlearnable classes under predictable examples,\naligning with the broader learning-augmented model paradigm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u56de\u5f52\u95ee\u9898\uff0c\u5f53\u5b66\u4e60\u8005\u80fd\u591f\u83b7\u5f97\u672a\u6765\u6837\u672c\u7684\u9884\u6d4b\u4fe1\u606f\u65f6\u3002\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff08\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\uff09\uff0c\u4f5c\u8005\u5b8c\u5168\u523b\u753b\u4e86\u6700\u5c0f\u6700\u5927\u671f\u671b\u9057\u61be\u4e0efat-shattering\u7ef4\u5ea6\u7684\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5728\u7ebf\u5b66\u4e60\u5668\uff0c\u5176\u9057\u61be\u968f\u9884\u6d4b\u8d28\u91cf\u5e73\u6ed1\u6539\u8fdb\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u65f6\u63a5\u8fd1\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u53d7\u73b0\u5b9e\u6570\u636e\u6d41\u53ef\u9884\u6d4b\u6027\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5f53\u5b66\u4e60\u8005\u80fd\u591f\u83b7\u5f97\u672a\u6765\u6837\u672c\u9884\u6d4b\u65f6\u7684\u5728\u7ebf\u56de\u5f52\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u9884\u6d4b\u4fe1\u606f\u6539\u8fdb\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u9996\u5148\u7814\u7a76\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\uff08\u5b8c\u5168\u77e5\u9053\u672a\u6765\u6837\u672c\u5e8f\u5217\uff09\uff0c\u7136\u540e\u63a8\u5e7f\u5230\u4e0d\u5b8c\u7f8e\u9884\u6d4b\u60c5\u51b5\u3002\u57fa\u4e8e\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\u7684\u7ed3\u679c\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5728\u7ebf\u5b66\u4e60\u5668\uff0c\u5176\u6700\u5c0f\u6700\u5927\u671f\u671b\u9057\u61be\u4e0e\u6700\u574f\u60c5\u51b5\u9057\u61be\u5339\u914d\uff0c\u5e76\u968f\u9884\u6d4b\u8d28\u91cf\u5e73\u6ed1\u6539\u8fdb\u3002", "result": "\u5b8c\u5168\u523b\u753b\u4e86\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\u7684\u6700\u5c0f\u6700\u5927\u671f\u671b\u9057\u61be\u4e0efat-shattering\u7ef4\u5ea6\u7684\u5173\u7cfb\uff0c\u5efa\u7acb\u4e86\u8f6c\u5bfc\u5728\u7ebf\u56de\u5f52\u4e0e\u5bf9\u6297\u5728\u7ebf\u56de\u5f52\u7684\u5206\u79bb\u3002\u5f00\u53d1\u7684\u5b66\u4e60\u5668\u5728\u9884\u6d4b\u51c6\u786e\u65f6\u6027\u80fd\u63a5\u8fd1\u8f6c\u5bfc\u5728\u7ebf\u5b66\u4e60\u5668\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u574f\u60c5\u51b5\u9057\u61be\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4f7f\u5f97\u5728\u53ef\u9884\u6d4b\u6837\u672c\u4e0b\u4ee5\u524d\u4e0d\u53ef\u5b66\u4e60\u7684\u7c7b\u53d8\u5f97\u53ef\u5b66\u4e60\uff0c\u4e0e\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u8303\u5f0f\u76f8\u4e00\u81f4\uff0c\u5c55\u793a\u4e86\u5229\u7528\u9884\u6d4b\u4fe1\u606f\u63d0\u5347\u5728\u7ebf\u5b66\u4e60\u6027\u80fd\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03923", "abs": "https://arxiv.org/abs/2510.03923", "authors": ["Mingsong Yan", "Charles Kulick", "Sui Tang"], "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks", "comment": null, "summary": "Continuous-depth graph neural networks, also known as Graph Neural\nDifferential Equations (GNDEs), combine the structural inductive bias of Graph\nNeural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,\noffering a scalable and principled framework for modeling dynamics on graphs.\nIn this paper, we present a rigorous convergence analysis of GNDEs with\ntime-varying parameters in the infinite-node limit, providing theoretical\ninsights into their size transferability. To this end, we introduce Graphon\nNeural Differential Equations (Graphon-NDEs) as the infinite-node limit of\nGNDEs and establish their well-posedness. Leveraging tools from graphon theory\nand dynamical systems, we prove the trajectory-wise convergence of GNDE\nsolutions to Graphon-NDE solutions. Moreover, we derive explicit convergence\nrates under two deterministic graph sampling regimes: (1) weighted graphs\nsampled from smooth graphons, and (2) unweighted graphs sampled from\n$\\{0,1\\}$-valued (discontinuous) graphons. We further establish size\ntransferability bounds, providing theoretical justification for the practical\nstrategy of transferring GNDE models trained on moderate-sized graphs to\nlarger, structurally similar graphs without retraining. Numerical experiments\nusing synthetic and real data support our theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5177\u6709\u65f6\u53d8\u53c2\u6570\u7684\u8fde\u7eed\u6df1\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNDEs\uff09\u5728\u65e0\u9650\u8282\u70b9\u6781\u9650\u4e0b\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u4e25\u683c\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5fae\u5206\u65b9\u7a0b\uff08Graphon-NDEs\uff09\u4f5c\u4e3a\u5176\u65e0\u9650\u8282\u70b9\u6781\u9650\uff0c\u5e76\u8bc1\u660e\u4e86GNDE\u89e3\u5411Graphon-NDE\u89e3\u7684\u8f68\u8ff9\u6536\u655b\u3002", "motivation": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u5f52\u7eb3\u504f\u7f6e\u548c\u795e\u7ecfODE\u7684\u8fde\u7eed\u6df1\u5ea6\u67b6\u6784\uff0c\u4e3a\u56fe\u4e0a\u7684\u52a8\u6001\u5efa\u6a21\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7814\u7a76GNDE\u6a21\u578b\u5728\u4e0d\u540c\u89c4\u6a21\u56fe\u4e4b\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u5f15\u5165Graphon-NDEs\u4f5c\u4e3aGNDEs\u7684\u65e0\u9650\u8282\u70b9\u6781\u9650\uff0c\u5229\u7528\u56fe\u8bba\u548c\u52a8\u529b\u7cfb\u7edf\u5de5\u5177\uff0c\u5728\u4e24\u79cd\u786e\u5b9a\u6027\u56fe\u91c7\u6837\u673a\u5236\u4e0b\uff08\u5e73\u6ed1\u56fe\u91c7\u6837\u7684\u52a0\u6743\u56fe\u548c\u4e0d\u8fde\u7eed\u56fe\u91c7\u6837\u7684\u65e0\u6743\u56fe\uff09\u8bc1\u660e\u6536\u655b\u6027\u5e76\u63a8\u5bfc\u663e\u5f0f\u6536\u655b\u7387\u3002", "result": "\u5efa\u7acb\u4e86GNDE\u89e3\u5411Graphon-NDE\u89e3\u7684\u8f68\u8ff9\u6536\u655b\uff0c\u83b7\u5f97\u4e86\u663e\u5f0f\u6536\u655b\u7387\uff0c\u5e76\u5efa\u7acb\u4e86\u5c3a\u5bf8\u53ef\u8fc1\u79fb\u6027\u8fb9\u754c\uff0c\u4e3a\u5c06\u4e2d\u7b49\u89c4\u6a21\u56fe\u4e0a\u8bad\u7ec3\u7684GNDE\u6a21\u578b\u8fc1\u79fb\u5230\u66f4\u5927\u89c4\u6a21\u56fe\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GNDE\u6a21\u578b\u5728\u4e0d\u540c\u89c4\u6a21\u56fe\u4e4b\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u578b\u8fc1\u79fb\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2510.03930", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03930", "abs": "https://arxiv.org/abs/2510.03930", "authors": ["Huascar Sanchez", "Briland Hitaj"], "title": "LLM Chemistry Estimation for Multi-LLM Recommendation", "comment": "20 pages, 5 figures, 5 tables", "summary": "Multi-LLM collaboration promises accurate, robust, and context-aware\nsolutions, yet existing approaches rely on implicit selection and output\nassessment without analyzing whether collaborating models truly complement or\nconflict. We introduce LLM Chemistry -- a framework that measures when LLM\ncombinations exhibit synergistic or antagonistic behaviors that shape\ncollective performance beyond individual capabilities. We formalize the notion\nof chemistry among LLMs, propose algorithms that quantify it by analyzing\ninteraction dependencies, and recommend optimal model ensembles accordingly.\nOur theoretical analysis shows that chemistry among collaborating LLMs is most\nevident under heterogeneous model profiles, with its outcome impact shaped by\ntask type, group size, and complexity. Evaluation on classification,\nsummarization, and program repair tasks provides initial evidence for these\ntask-dependent effects, thereby reinforcing our theoretical results. This\nestablishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and\na foundation for ensemble recommendation.", "AI": {"tldr": "LLM Chemistry\u6846\u67b6\u7528\u4e8e\u91cf\u5316\u591aLLM\u534f\u4f5c\u4e2d\u7684\u534f\u540c\u6216\u5bf9\u6297\u884c\u4e3a\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u95f4\u4ea4\u4e92\u4f9d\u8d56\u5173\u7cfb\u6765\u63a8\u8350\u6700\u4f18\u6a21\u578b\u7ec4\u5408\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u6a21\u578b\u5f02\u8d28\u6027\u3001\u4efb\u52a1\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u5f71\u54cd\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u591aLLM\u534f\u4f5c\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u9009\u62e9\u548c\u8f93\u51fa\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5206\u6790\u6a21\u578b\u95f4\u662f\u5426\u771f\u6b63\u4e92\u8865\u6216\u51b2\u7a81\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u8861\u91cf\u534f\u4f5c\u6548\u679c\u3002", "method": "\u63d0\u51faLLM Chemistry\u6846\u67b6\uff0c\u5f62\u5f0f\u5316LLM\u95f4\u5316\u5b66\u4f5c\u7528\u6982\u5ff5\uff0c\u5f00\u53d1\u7b97\u6cd5\u91cf\u5316\u4ea4\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u636e\u6b64\u63a8\u8350\u6700\u4f18\u6a21\u578b\u96c6\u6210\u7ec4\u5408\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eLLM\u534f\u4f5c\u6548\u679c\u5728\u5f02\u8d28\u6a21\u578b\u914d\u7f6e\u4e0b\u6700\u660e\u663e\uff0c\u53d7\u4efb\u52a1\u7c7b\u578b\u3001\u56e2\u961f\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u5f71\u54cd\uff1b\u5728\u5206\u7c7b\u3001\u6458\u8981\u548c\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u4efb\u52a1\u4f9d\u8d56\u6027\u6548\u5e94\u3002", "conclusion": "LLM Chemistry\u53ef\u4f5c\u4e3a\u591aLLM\u7cfb\u7edf\u7684\u8bca\u65ad\u56e0\u7d20\u548c\u96c6\u6210\u63a8\u8350\u7684\u57fa\u7840\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u534f\u4f5c\u673a\u5236\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2510.03944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03944", "abs": "https://arxiv.org/abs/2510.03944", "authors": ["Weiqing He", "Xiang Li", "Tianqi Shang", "Li Shen", "Weijie Su", "Qi Long"], "title": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection", "comment": "Accepted at NeurIPS 2025 as a spotlight", "summary": "Large language models (LLMs) raise concerns about content authenticity and\nintegrity because they can generate human-like text at scale. Text watermarks,\nwhich embed detectable statistical signals into generated text, offer a\nprovable way to verify content origin. Many detection methods rely on pivotal\nstatistics that are i.i.d. under human-written text, making goodness-of-fit\n(GoF) tests a natural tool for watermark detection. However, GoF tests remain\nlargely underexplored in this setting. In this paper, we systematically\nevaluate eight GoF tests across three popular watermarking schemes, using three\nopen-source LLMs, two datasets, various generation temperatures, and multiple\npost-editing methods. We find that general GoF tests can improve both the\ndetection power and robustness of watermark detectors. Notably, we observe that\ntext repetition, common in low-temperature settings, gives GoF tests a unique\nadvantage not exploited by existing methods. Our results highlight that classic\nGoF tests are a simple yet powerful and underused tool for watermark detection\nin LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e868\u79cd\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u5728\u4e09\u79cd\u6d41\u884c\u6c34\u5370\u65b9\u6848\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u7528\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u80fd\u663e\u8457\u63d0\u5347\u6c34\u5370\u68c0\u6d4b\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u6587\u672c\u91cd\u590d\u573a\u666f\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u5f15\u53d1\u62c5\u5fe7\uff0c\u6c34\u5370\u6280\u672f\u901a\u8fc7\u5d4c\u5165\u53ef\u68c0\u6d4b\u7684\u7edf\u8ba1\u4fe1\u53f7\u6765\u9a8c\u8bc1\u5185\u5bb9\u6765\u6e90\u3002\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u4f5c\u4e3a\u81ea\u7136\u7684\u6c34\u5370\u68c0\u6d4b\u5de5\u5177\uff0c\u4f46\u5728\u8be5\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5f00\u6e90LLM\u3001\u4e24\u4e2a\u6570\u636e\u96c6\u3001\u4e0d\u540c\u751f\u6210\u6e29\u5ea6\u548c\u591a\u91cd\u540e\u7f16\u8f91\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u516b\u79cd\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u5728\u4e09\u79cd\u6d41\u884c\u6c34\u5370\u65b9\u6848\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u7528\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u80fd\u540c\u65f6\u63d0\u5347\u6c34\u5370\u68c0\u6d4b\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5728\u4f4e\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u6587\u672c\u91cd\u590d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\uff0c\u8fd9\u662f\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u7684\u3002", "conclusion": "\u7ecf\u5178\u7684\u62df\u5408\u4f18\u5ea6\u68c0\u9a8c\u662fLLM\u6c34\u5370\u68c0\u6d4b\u4e2d\u7b80\u5355\u800c\u5f3a\u5927\u4f46\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u5de5\u5177\uff0c\u503c\u5f97\u5728\u6c34\u5370\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.03950", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03950", "abs": "https://arxiv.org/abs/2510.03950", "authors": ["Shahriar Kabir Nahin", "Wenxiao Xiao", "Joshua Liu", "Anshuman Chhabra", "Hongfu Liu"], "title": "What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis", "comment": null, "summary": "Data-centric learning seeks to improve model performance from the perspective\nof data quality, and has been drawing increasing attention in the machine\nlearning community. Among its key tools, influence functions provide a powerful\nframework to quantify the impact of individual training samples on model\npredictions, enabling practitioners to identify detrimental samples and retrain\nmodels on a cleaner dataset for improved performance. However, most existing\nwork focuses on the question: \"what data benefits the learning model?\" In this\npaper, we take a step further and investigate a more fundamental question:\n\"what is the performance ceiling of the learning model?\" Unlike prior studies\nthat primarily measure improvement through overall accuracy, we emphasize\ncategory-wise accuracy and aim for Pareto improvements, ensuring that every\nclass benefits, rather than allowing tradeoffs where some classes improve at\nthe expense of others. To address this challenge, we propose category-wise\ninfluence functions and introduce an influence vector that quantifies the\nimpact of each training sample across all categories. Leveraging these\ninfluence vectors, we develop a principled criterion to determine whether a\nmodel can still be improved, and further design a linear programming-based\nsample reweighting framework to achieve Pareto performance improvements.\nThrough extensive experiments on synthetic datasets, vision, and text\nbenchmarks, we demonstrate the effectiveness of our approach in estimating and\nachieving a model's performance improvement across multiple categories of\ninterest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7c7b\u522b\u7ea7\u5f71\u54cd\u51fd\u6570\u548c\u5f71\u54cd\u5411\u91cf\uff0c\u7528\u4e8e\u91cf\u5316\u8bad\u7ec3\u6837\u672c\u5bf9\u6a21\u578b\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u6837\u672c\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u8de8\u591a\u4e2a\u7c7b\u522b\u7684\u5e15\u7d2f\u6258\u6027\u80fd\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u4e2d\u5fc3\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\"\u54ea\u4e9b\u6570\u636e\u5bf9\u5b66\u4e60\u6a21\u578b\u6709\u76ca\"\uff0c\u800c\u672c\u6587\u7814\u7a76\u66f4\u57fa\u7840\u7684\u95ee\u9898\"\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\u662f\u4ec0\u4e48\"\uff0c\u5f3a\u8c03\u7c7b\u522b\u7ea7\u51c6\u786e\u6027\u548c\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7c7b\u522b\u90fd\u80fd\u53d7\u76ca\u3002", "method": "\u63d0\u51fa\u7c7b\u522b\u7ea7\u5f71\u54cd\u51fd\u6570\u548c\u5f71\u54cd\u5411\u91cf\u6765\u91cf\u5316\u8bad\u7ec3\u6837\u672c\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u7684\u5f71\u54cd\uff1b\u5f00\u53d1\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u6837\u672c\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6837\u672c\u6765\u5b9e\u73b0\u5e15\u7d2f\u6258\u6027\u80fd\u6539\u8fdb\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u3001\u89c6\u89c9\u548c\u6587\u672c\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u548c\u5b9e\u73b0\u6a21\u578b\u5728\u591a\u4e2a\u611f\u5174\u8da3\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7c7b\u522b\u7ea7\u5f71\u54cd\u51fd\u6570\u548c\u6837\u672c\u91cd\u52a0\u6743\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u5730\u8bc6\u522b\u6a21\u578b\u6027\u80fd\u6539\u8fdb\u6f5c\u529b\uff0c\u5e76\u5b9e\u73b0\u8de8\u7c7b\u522b\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2510.03954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03954", "abs": "https://arxiv.org/abs/2510.03954", "authors": ["Tim Bary", "Tiffanie Godelaine", "Axel Abels", "Beno\u00eet Macq"], "title": "Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts", "comment": "7 pages, 3 figures, 3 tables, Accepted at IEEE BHI 2025", "summary": "Accurate ground truth estimation in medical screening programs often relies\non coalitions of experts and peer second opinions. Algorithms that efficiently\naggregate noisy annotations can enhance screening workflows, particularly when\ndata arrive continuously and expert proficiency is initially unknown. However,\nexisting algorithms do not meet the requirements for seamless integration into\nscreening pipelines. We therefore propose an adaptive approach for real-time\nannotation that (I) supports on-the-fly labeling of incoming data, (II)\noperates without prior knowledge of medical experts or pre-labeled data, and\n(III) dynamically queries additional experts based on the latent difficulty of\neach instance. The method incrementally gathers expert opinions until a\nconfidence threshold is met, providing accurate labels with reduced annotation\noverhead. We evaluate our approach on three multi-annotator classification\ndatasets across different modalities. Results show that our adaptive querying\nstrategy reduces the number of expert queries by up to 50% while achieving\naccuracy comparable to a non-adaptive baseline. Our code is available at\nhttps://github.com/tbary/MEDICS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5b9e\u65f6\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u533b\u5b66\u7b5b\u67e5\u4e2d\u52a8\u6001\u67e5\u8be2\u4e13\u5bb6\u610f\u89c1\uff0c\u51cf\u5c1150%\u7684\u4e13\u5bb6\u67e5\u8be2\u91cf\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7387", "motivation": "\u533b\u5b66\u7b5b\u67e5\u4e2d\u51c6\u786e\u6807\u6ce8\u9700\u8981\u591a\u4f4d\u4e13\u5bb6\u5171\u8bc6\uff0c\u73b0\u6709\u7b97\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u7b5b\u67e5\u6d41\u7a0b\u7684\u65e0\u7f1d\u96c6\u6210\u9700\u6c42", "method": "\u81ea\u9002\u5e94\u5b9e\u65f6\u6807\u6ce8\u65b9\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u6807\u6ce8\u4f20\u5165\u6570\u636e\uff0c\u65e0\u9700\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\uff0c\u57fa\u4e8e\u5b9e\u4f8b\u6f5c\u5728\u96be\u5ea6\u52a8\u6001\u67e5\u8be2\u989d\u5916\u4e13\u5bb6\uff0c\u76f4\u5230\u8fbe\u5230\u7f6e\u4fe1\u5ea6\u9608\u503c", "result": "\u5728\u4e09\u4e2a\u591a\u6807\u6ce8\u8005\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u81ea\u9002\u5e94\u67e5\u8be2\u7b56\u7565\u51cf\u5c1150%\u4e13\u5bb6\u67e5\u8be2\u91cf\uff0c\u51c6\u786e\u7387\u4e0e\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\u76f8\u5f53", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u533b\u5b66\u7b5b\u67e5\u4e2d\u7684\u6807\u6ce8\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u6ce8\u51c6\u786e\u6027"}}
{"id": "2510.03959", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03959", "abs": "https://arxiv.org/abs/2510.03959", "authors": ["Iryna Stanishevska"], "title": "Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model", "comment": "23 pages (main), 70 pages incl. appendices; figures & tables as in\n  manuscript. Code (main figure, synthetic data):\n  https://github.com/<your-login>/peak-outage-forecasting License: CC BY 4.0\n  (preprint)", "summary": "Thunderstorm-driven outages are difficult to predict because most storms do\nnot cause damage, convective processes occur rapidly and chaotically, and the\navailable public data are both noisy and incomplete. We develop a 24-48 h\nearly-warning model for summer, thunderstorm-related outages in Michigan using\nonly open sources (EAGLE-I for ground truth; METAR for weather). We use the\npublicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge\nNational Laboratory for the U.S. Department of Energy. The pipeline preserves\nconvective micro-signals from a sparse station network via parameter-specific\nkriging with hourly variograms and targeted overdrafting to retain extremes,\nand builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW\nspatial aggregates) capturing precursors of severe convection (moisture\nadvection, wind shifts, and pressure drops). The two-stage model design,\ncombining a logistic gate and an LSTM regressor, limits routine periods and\nreduces noise exposure. The study uses event-centric metrics (cluster-based\nhits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour\nwindows around state-level peaks (>= 50,000), with uncertainty quantified by\nhourly moving-block bootstrap.\n  On the test sample, Two-Stage detects more reference peaks across all windows\n(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra\nfalse alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at\n+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48\nh (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP\nanalysis confirms moisture-advection and wind/gust precursors, underscoring the\nvalue of the feature engineering. Despite open-data noise, the feature-driven\npipeline yields actionable, event-focused early warnings for thunderstorm\noutages.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a24-48\u5c0f\u65f6\u96f7\u66b4\u505c\u7535\u65e9\u671f\u9884\u8b66\u6a21\u578b\uff0c\u4f7f\u7528\u516c\u5f00\u6570\u636e\u6e90\u548c\u4e24\u9636\u6bb5\u6a21\u578b\u8bbe\u8ba1\uff08\u903b\u8f91\u95e8+LSTM\u56de\u5f52\u5668\uff09\uff0c\u5728\u5bc6\u6b47\u6839\u5dde\u590f\u5b63\u96f7\u66b4\u76f8\u5173\u505c\u7535\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u96f7\u66b4\u9a71\u52a8\u7684\u505c\u7535\u96be\u4ee5\u9884\u6d4b\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u98ce\u66b4\u4e0d\u4f1a\u9020\u6210\u635f\u5bb3\uff0c\u5bf9\u6d41\u8fc7\u7a0b\u53d1\u751f\u8fc5\u901f\u4e14\u6df7\u4e71\uff0c\u53ef\u7528\u7684\u516c\u5171\u6570\u636e\u65e2\u5608\u6742\u53c8\u4e0d\u5b8c\u6574\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684EAGLE-I\u505c\u7535\u6570\u636e\u96c6\u548cMETAR\u5929\u6c14\u6570\u636e\uff0c\u901a\u8fc7\u7279\u5b9a\u53c2\u6570\u514b\u91cc\u91d1\u6cd5\u4fdd\u7559\u5bf9\u6d41\u5fae\u4fe1\u53f7\uff0c\u6784\u5efa\u56e0\u679c\u65f6\u7a7a\u7279\u5f81\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6a21\u578b\u8bbe\u8ba1\uff08\u903b\u8f91\u95e8+LSTM\u56de\u5f52\u5668\uff09\u51cf\u5c11\u566a\u58f0\u66b4\u9732\u3002", "result": "\u4e24\u9636\u6bb5\u6a21\u578b\u5728\u6240\u6709\u65f6\u95f4\u7a97\u53e3\u68c0\u6d4b\u5230\u66f4\u591a\u53c2\u8003\u5cf0\u503c\uff08\u5982\u00b148\u5c0f\u65f6\u8bb0\u5f553/4 vs 2/4\uff09\uff0c\u5728\u5cf0\u503c\u9644\u8fd1\u663e\u793a\u9002\u5ea6\u5e45\u5ea6\u589e\u76ca\uff08\u00b10-12\u5c0f\u65f6cMASE\u964d\u4f4e2-3%\uff09\uff0c\u4f46\u00b136-48\u5c0f\u65f6\u6709\u5c0f\u635f\u5931\u3002", "conclusion": "\u5c3d\u7ba1\u516c\u5f00\u6570\u636e\u5b58\u5728\u566a\u58f0\uff0c\u4f46\u7279\u5f81\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\u4e3a\u96f7\u66b4\u505c\u7535\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u3001\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u65e9\u671f\u9884\u8b66\uff0cSHAP\u5206\u6790\u786e\u8ba4\u4e86\u6e7f\u5ea6\u5e73\u6d41\u548c\u98ce/\u9635\u98ce\u524d\u5146\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.03962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03962", "abs": "https://arxiv.org/abs/2510.03962", "authors": ["Hanzhe Wei", "Jiajun Wu", "Jialin Yang", "Henry Leung", "Steve Drew"], "title": "SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data", "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Time series anomaly detection plays a crucial role in a wide range of fields,\nsuch as healthcare and internet traffic monitoring. The emergence of large\nlanguage models (LLMs) offers new opportunities for detecting anomalies in the\nubiquitous time series data. Traditional approaches struggle with\nvariable-length time series sequences and context-based anomalies. We propose\nSoft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage\nLLMs for anomaly detection with soft prompts and quantization. Our methodology\ninvolves quantizing and transforming the time series data into input embeddings\nand combining them with learnable soft prompt embeddings. These combined\nembeddings are then fed into a frozen LLM. The soft prompts are updated\niteratively based on a cross-entropy loss, allowing the model to adapt to time\nseries anomaly detection. The use of soft prompts helps adapt LLMs effectively\nto time series tasks, while quantization ensures optimal handling of sequences,\nas LLMs are designed to handle discrete sequences. Our experimental results\ndemonstrate that soft prompts effectively increase LLMs' performance in\ndownstream tasks regarding time series anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86SPEAR\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u548c\u91cf\u5316\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u53d8\u957f\u5e8f\u5217\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5f02\u5e38\u65f6\u7684\u56f0\u96be\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u53d8\u957f\u65f6\u95f4\u5e8f\u5217\u5e8f\u5217\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u5f02\u5e38\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u91cf\u5316\u4e3a\u8f93\u5165\u5d4c\u5165\uff0c\u4e0e\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u5d4c\u5165\u7ed3\u5408\u540e\u8f93\u5165\u51bb\u7ed3\u7684LLM\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u8fed\u4ee3\u66f4\u65b0\u8f6f\u63d0\u793a\uff0c\u4f7f\u6a21\u578b\u9002\u5e94\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8f6f\u63d0\u793a\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u8f6f\u63d0\u793a\u6709\u52a9\u4e8eLLM\u6709\u6548\u9002\u5e94\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\uff0c\u800c\u91cf\u5316\u786e\u4fdd\u4e86\u5e8f\u5217\u7684\u6700\u4f18\u5904\u7406\uff0c\u56e0\u4e3aLLM\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u79bb\u6563\u5e8f\u5217\u3002"}}
{"id": "2510.03971", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03971", "abs": "https://arxiv.org/abs/2510.03971", "authors": ["Jatin Prakash", "Anirudh Buvanesh"], "title": "What Can You Do When You Have Zero Rewards During RL?", "comment": null, "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective\nfor improving large language models (LLMs) on complex reasoning tasks. However,\nits success often depends on the base model occasionally sampling correct\nsolutions. When no correct solutions are sampled, training encounters a\nzero-reward barrier where learning stalls due to zero gradients. We study this\nscenario through the graph search task introduced in Bachmann et al. (2024) and\nevaluate recent methods that incorporate desirable components such as dense\nrewards, diversity incentives, and improved credit assignment. Our experiments\nshow that none of these approaches overcome the zero-reward barrier if the base\nmodel never produces a correct answer. In contrast, we find that a simple\ndata-centric intervention of adding easier samples to the training set enables\nthe model to eventually solve the original hard task despite starting from zero\nreward. Importantly, this succeeds without modifying the RL algorithm itself.\nBecause official implementations of several baselines were unavailable, we\ndeveloped our own, which allowed us to conduct a detailed analysis of their\nfailure modes. We release these implementations to support further research at:\nhttps://github.com/rl4reasoning/rl-baselines", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f53\u57fa\u7840\u6a21\u578b\u4ece\u672a\u91c7\u6837\u5230\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u65f6\u51fa\u73b0\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u90fd\u65e0\u6cd5\u514b\u670d\u8fd9\u4e00\u969c\u788d\uff0c\u4f46\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u5e72\u9884\uff08\u6dfb\u52a0\u7b80\u5355\u6837\u672c\uff09\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u9047\u5230\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u5373\u5f53\u57fa\u7840\u6a21\u578b\u4ece\u672a\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u65f6\uff0c\u8bad\u7ec3\u4f1a\u56e0\u96f6\u68af\u5ea6\u800c\u505c\u6ede\u3002", "method": "\u4f7f\u7528\u56fe\u641c\u7d22\u4efb\u52a1\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff08\u5bc6\u96c6\u5956\u52b1\u3001\u591a\u6837\u6027\u6fc0\u52b1\u3001\u6539\u8fdb\u4fe1\u7528\u5206\u914d\uff09\uff0c\u5e76\u6d4b\u8bd5\u6570\u636e\u5e72\u9884\u65b9\u6cd5\uff08\u5728\u8bad\u7ec3\u96c6\u4e2d\u6dfb\u52a0\u7b80\u5355\u6837\u672c\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u90fd\u65e0\u6cd5\u514b\u670d\u96f6\u5956\u52b1\u969c\u788d\uff0c\u4f46\u7b80\u5355\u7684\u6570\u636e\u5e72\u9884\u65b9\u6cd5\u80fd\u8ba9\u6a21\u578b\u6700\u7ec8\u89e3\u51b3\u539f\u59cb\u56f0\u96be\u4efb\u52a1\uff0c\u4e14\u65e0\u9700\u4fee\u6539RL\u7b97\u6cd5\u672c\u8eab\u3002", "conclusion": "\u6570\u636e\u5e72\u9884\u662f\u514b\u670d\u96f6\u5956\u52b1\u969c\u788d\u7684\u6709\u6548\u7b56\u7565\uff0c\u4e3aRL\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03979", "categories": ["cs.LG", "econ.TH"], "pdf": "https://arxiv.org/pdf/2510.03979", "abs": "https://arxiv.org/abs/2510.03979", "authors": ["Emerson Melo", "David M\u00fcller"], "title": "Beyond Softmax: A New Perspective on Gradient Bandits", "comment": null, "summary": "We establish a link between a class of discrete choice models and the theory\nof online learning and multi-armed bandits. Our contributions are: (i)\nsublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a\nspecial case; (ii) a new class of adversarial bandit algorithms derived from\ngeneralized nested logit models \\citep{wen:2001}; and (iii)\n\\textcolor{black}{we introduce a novel class of generalized gradient bandit\nalgorithms that extends beyond the widely used softmax formulation. By relaxing\nthe restrictive independence assumptions inherent in softmax, our framework\naccommodates correlated learning dynamics across actions, thereby broadening\nthe applicability of gradient bandit methods.} Overall, the proposed algorithms\ncombine flexible model specification with computational efficiency via\nclosed-form sampling probabilities. Numerical experiments in stochastic bandit\nsettings demonstrate their practical effectiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4e0e\u5728\u7ebf\u5b66\u4e60\u53ca\u591a\u81c2\u8001\u864e\u673a\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u5177\u6709\u6b21\u7ebf\u6027\u9057\u61be\u754c\u7684\u7b97\u6cd5\u5bb6\u65cf\u3001\u57fa\u4e8e\u5e7f\u4e49\u5d4c\u5957logit\u6a21\u578b\u7684\u65b0\u5bf9\u6297\u6027\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u4ee5\u53ca\u8d85\u8d8asoftmax\u7684\u5e7f\u4e49\u68af\u5ea6\u8001\u864e\u673a\u7b97\u6cd5\u3002", "motivation": "\u8fde\u63a5\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4e0e\u5728\u7ebf\u5b66\u4e60\u7406\u8bba\uff0c\u7a81\u7834\u4f20\u7edfsoftmax\u7b97\u6cd5\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u9650\u5236\uff0c\u89e3\u51b3\u52a8\u4f5c\u95f4\u76f8\u5173\u5b66\u4e60\u52a8\u6001\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5305\u542bExp3\u7684\u5e7f\u4e49\u7b97\u6cd5\u5bb6\u65cf\uff0c\u57fa\u4e8e\u5e7f\u4e49\u5d4c\u5957logit\u6a21\u578b\u5f00\u53d1\u5bf9\u6297\u6027\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u8003\u8651\u52a8\u4f5c\u76f8\u5173\u6027\u7684\u5e7f\u4e49\u68af\u5ea6\u8001\u864e\u673a\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u6b21\u7ebf\u6027\u9057\u61be\u754c\uff0c\u901a\u8fc7\u95ed\u5f0f\u91c7\u6837\u6982\u7387\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u968f\u673a\u8001\u864e\u673a\u8bbe\u7f6e\u4e2d\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u7ed3\u5408\u4e86\u7075\u6d3b\u7684\u6a21\u578b\u89c4\u8303\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u6269\u5c55\u4e86\u68af\u5ea6\u8001\u864e\u673a\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u80fd\u591f\u5904\u7406\u52a8\u4f5c\u95f4\u7684\u76f8\u5173\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2510.03987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03987", "abs": "https://arxiv.org/abs/2510.03987", "authors": ["Michael Yang"], "title": "ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity", "comment": null, "summary": "Hierarchical Pooling Models have demonstrated strong performance in\nclassifying graph-structured data. While numerous innovative methods have been\nproposed to design cluster assignments and coarsening strategies, the\nrelationships between clusters are often overlooked. In this paper, we\nintroduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel\nhierarchical pooling framework designed to enhance model's understanding of\ninter-cluster connectivity and ability of preserving the structural integrity\nin the original graph. ICEPool is compatible with a wide range of pooling-based\nGNN models. The deployment of ICEPool as an enhancement to existing models\neffectively combines the strengths of the original model with ICEPool's\ncapability to emphasize the integration of inter-cluster connectivity,\nresulting in a more comprehensive and robust graph-level representation.\nMoreover, we make theoretical analysis to ICEPool's ability of graph\nreconstruction to demonstrate its effectiveness in learning inter-cluster\nrelationship that is overlooked by conventional models. Finally, the\nexperimental results show the compatibility of ICEPool with wide varieties of\nmodels and its potential to boost the performance of existing graph neural\nnetwork architectures.", "AI": {"tldr": "ICEPool\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u6b21\u6c60\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u5bf9\u7c07\u95f4\u8fde\u63a5\u6027\u7684\u7406\u89e3\u6765\u63d0\u5347\u56fe\u7ed3\u6784\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u53ef\u4e0e\u591a\u79cd\u6c60\u5316GNN\u6a21\u578b\u517c\u5bb9\u3002", "motivation": "\u73b0\u6709\u7684\u5c42\u6b21\u6c60\u5316\u6a21\u578b\u5728\u8bbe\u8ba1\u7c07\u5206\u914d\u548c\u7c97\u5316\u7b56\u7565\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u7c07\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51faICEPool\u6846\u67b6\uff0c\u5f3a\u8c03\u7c07\u95f4\u8fde\u63a5\u6027\u7684\u6574\u5408\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u5728\u56fe\u91cd\u5efa\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eICEPool\u4e0e\u591a\u79cd\u6a21\u578b\u517c\u5bb9\uff0c\u5e76\u80fd\u63d0\u5347\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6027\u80fd\u3002", "conclusion": "ICEPool\u901a\u8fc7\u589e\u5f3a\u7c07\u95f4\u8fde\u63a5\u6027\u7406\u89e3\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5168\u9762\u548c\u9c81\u68d2\u7684\u56fe\u7ea7\u8868\u793a\uff0c\u63d0\u5347\u56fe\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2510.03988", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03988", "abs": "https://arxiv.org/abs/2510.03988", "authors": ["Hoang Anh Just", "Myeongseob Ko", "Ruoxi Jia"], "title": "Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data", "comment": "Preprint", "summary": "Distilling long reasoning traces (10K+ tokens) from stronger teacher models\ninto smaller student LLMs via SFT has emerged as a standard paradigm. This\napproach is practical and efficient: it leverages the ease of generating\nabundant reasoning data from stronger models and provides a direct, data-driven\nway to teach less capable models better reasoning. While previous work has\nlargely focused on prompt selection with responses from a single teacher, the\nequally important problem of choosing the best response when multiple teacher\noutputs are available for a single prompt remains underexplored. This challenge\nbecomes important in a multi-teacher setting, where different students may\nbenefit from the outputs of different teachers. This paper fills that gap with\na systematic study of response selection for reasoning distillation. We first\nshow that the current method, which picks responses the student assigns the\nhighest global log-probability (global naturalness), fails when responses come\nfrom multiple teachers, i.e., global naturalness no longer correlates with\ndownstream performance, especially as the reasoning traces from strong teachers\nbecome longer. To overcome this problem, we introduce Local Naturalness, which\nmeasures the student's log-probabilities over short, sequential reasoning steps\nconditioned only on a small local window. Local Naturalness enables two\napplications: 1) Teacher Selection: Aggregating local scores across prompts\nreliably identifies the most helpful teacher. 2) Response Selection from a\nMultiple Teachers: When mixing answers from many teachers, Local Naturalness\nboosts a 32B student's accuracy on math benchmarks by 9.4pp over global\nselection, also surpassing the performance achieved by training on data from\nthe single best teacher. These results highlight the power of localized data\nquality evaluation and data mixing for more effective reasoning distillation.", "AI": {"tldr": "\u63d0\u51fa\u5c40\u90e8\u81ea\u7136\u5ea6\u65b9\u6cd5\u89e3\u51b3\u591a\u6559\u5e08\u63a8\u7406\u84b8\u998f\u4e2d\u7684\u54cd\u5e94\u9009\u62e9\u95ee\u9898\uff0c\u76f8\u6bd4\u5168\u5c40\u81ea\u7136\u5ea6\u80fd\u66f4\u51c6\u786e\u9009\u62e9\u9002\u5408\u5b66\u751f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u6559\u5e08\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u57fa\u4e8e\u5168\u5c40\u81ea\u7136\u5ea6\u7684\u54cd\u5e94\u9009\u62e9\u65b9\u6cd5\u5931\u6548\uff0c\u65e0\u6cd5\u4e3a\u4e0d\u540c\u5b66\u751f\u9009\u62e9\u6700\u5408\u9002\u7684\u6559\u5e08\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u63a8\u7406\u84b8\u998f\u7684\u6548\u679c\u3002", "method": "\u5f15\u5165\u5c40\u90e8\u81ea\u7136\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u5b66\u751f\u5728\u77ed\u5e8f\u5217\u63a8\u7406\u6b65\u9aa4\u4e0a\u7684\u6761\u4ef6\u5bf9\u6570\u6982\u7387\u6765\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u652f\u6301\u6559\u5e08\u9009\u62e9\u548c\u54cd\u5e94\u9009\u62e9\u3002", "result": "\u5c40\u90e8\u81ea\u7136\u5ea6\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5c0632B\u5b66\u751f\u6a21\u578b\u7684\u51c6\u786e\u7387\u6bd4\u5168\u5c40\u9009\u62e9\u63d0\u5347\u4e869.4\u4e2a\u767e\u5206\u70b9\uff0c\u8d85\u8d8a\u4e86\u5355\u4e00\u6700\u4f73\u6559\u5e08\u6570\u636e\u7684\u8bad\u7ec3\u6548\u679c\u3002", "conclusion": "\u5c40\u90e8\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u548c\u6570\u636e\u6df7\u5408\u80fd\u591f\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u63a8\u7406\u84b8\u998f\uff0c\u5c40\u90e8\u81ea\u7136\u5ea6\u662f\u591a\u6559\u5e08\u84b8\u998f\u573a\u666f\u4e0b\u7684\u5173\u952e\u6539\u8fdb\u3002"}}
{"id": "2510.03989", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.03989", "abs": "https://arxiv.org/abs/2510.03989", "authors": ["Xue-Cheng Tai", "Hao Liu", "Lingfeng Li", "Raymond H. Chan"], "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs", "comment": null, "summary": "The Transformer architecture has revolutionized the field of sequence\nmodeling and underpins the recent breakthroughs in large language models\n(LLMs). However, a comprehensive mathematical theory that explains its\nstructure and operations remains elusive. In this work, we propose a novel\ncontinuous framework that rigorously interprets the Transformer as a\ndiscretization of a structured integro-differential equation. Within this\nformulation, the self-attention mechanism emerges naturally as a non-local\nintegral operator, and layer normalization is characterized as a projection to\na time-dependent constraint. This operator-theoretic and variational\nperspective offers a unified and interpretable foundation for understanding the\narchitecture's core components, including attention, feedforward layers, and\nnormalization. Our approach extends beyond previous theoretical analyses by\nembedding the entire Transformer operation in continuous domains for both token\nindices and feature dimensions. This leads to a principled and flexible\nframework that not only deepens theoretical insight but also offers new\ndirections for architecture design, analysis, and control-based\ninterpretations. This new interpretation provides a step toward bridging the\ngap between deep learning architectures and continuous mathematical modeling,\nand contributes a foundational perspective to the ongoing development of\ninterpretable and theoretically grounded neural network models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u7eed\u6846\u67b6\uff0c\u5c06Transformer\u89e3\u91ca\u4e3a\u7ed3\u6784\u5316\u79ef\u5206-\u5fae\u5206\u65b9\u7a0b\u7684\u79bb\u6563\u5316\uff0c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u81ea\u7136\u8868\u73b0\u4e3a\u975e\u5c40\u90e8\u79ef\u5206\u7b97\u5b50\uff0c\u5c42\u5f52\u4e00\u5316\u88ab\u63cf\u8ff0\u4e3a\u5bf9\u65f6\u95f4\u76f8\u5173\u7ea6\u675f\u7684\u6295\u5f71\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u5e8f\u5217\u5efa\u6a21\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u7a81\u7834\uff0c\u4f46\u5176\u5168\u9762\u7684\u6570\u5b66\u7406\u8bba\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u6765\u89e3\u91caTransformer\u7684\u7ed3\u6784\u548c\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u7b97\u5b50\u7406\u8bba\u548c\u53d8\u5206\u89c6\u89d2\uff0c\u5c06\u6574\u4e2aTransformer\u64cd\u4f5c\u5d4c\u5165\u5230token\u7d22\u5f15\u548c\u7279\u5f81\u7ef4\u5ea6\u7684\u8fde\u7eed\u57df\u4e2d\uff0c\u5c06Transformer\u89c6\u4e3a\u7ed3\u6784\u5316\u79ef\u5206-\u5fae\u5206\u65b9\u7a0b\u7684\u79bb\u6563\u5316\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u4e0d\u4ec5\u52a0\u6df1\u4e86\u5bf9Transformer\u7684\u7406\u8bba\u7406\u89e3\uff0c\u8fd8\u4e3a\u67b6\u6784\u8bbe\u8ba1\u3001\u5206\u6790\u548c\u57fa\u4e8e\u63a7\u5236\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8fd9\u4e00\u65b0\u89e3\u91ca\u5728\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u8fde\u7eed\u6570\u5b66\u5efa\u6a21\u4e4b\u95f4\u67b6\u8d77\u4e86\u6865\u6881\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u7406\u8bba\u57fa\u7840\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u53d1\u5c55\u8d21\u732e\u4e86\u57fa\u7840\u6027\u89c6\u89d2\u3002"}}
{"id": "2510.04006", "categories": ["cs.LG", "nlin.CD", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04006", "abs": "https://arxiv.org/abs/2510.04006", "authors": ["Hang Fan", "Yi Xiao", "Yongquan Qu", "Fenghua Ling", "Ben Fei", "Lei Bai", "Pierre Gentine"], "title": "Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints", "comment": null, "summary": "Data-driven machine learning (ML) models have recently shown promise in\nsurpassing traditional physics-based approaches for weather forecasting,\nleading to a so-called second revolution in weather forecasting. However, most\nML-based forecast models treat reanalysis as the truth and are trained under\nvariable-specific loss weighting, ignoring their physical coupling and spatial\nstructure. Over long time horizons, the forecasts become blurry and physically\nunrealistic under rollout training. To address this, we reinterpret model\ntraining as a weak-constraint four-dimensional variational data assimilation\n(WC-4DVar) problem, treating reanalysis data as imperfect observations. This\nallows the loss function to incorporate reanalysis error covariance and capture\nmultivariate dependencies. In practice, we compute the loss in a latent space\nlearned by an autoencoder (AE), where the reanalysis error covariance becomes\napproximately diagonal, thus avoiding the need to explicitly model it in the\nhigh-dimensional model space. We show that rollout training with latent-space\nconstraints improves long-term forecast skill and better preserves fine-scale\nstructures and physical realism compared to training with model-space loss.\nFinally, we extend this framework to accommodate heterogeneous data sources,\nenabling the forecast model to be trained jointly on reanalysis and\nmulti-source observations within a unified theoretical formulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u8bad\u7ec3\u91cd\u65b0\u89e3\u91ca\u4e3a\u5f31\u7ea6\u675f\u56db\u7ef4\u53d8\u5206\u6570\u636e\u540c\u5316\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u635f\u5931\u51fd\u6570\u6539\u8fdb\u957f\u671f\u9884\u62a5\u6280\u80fd\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5c06\u518d\u5206\u6790\u6570\u636e\u89c6\u4e3a\u7edd\u5bf9\u771f\u503c\uff0c\u5ffd\u7565\u4e86\u7269\u7406\u8026\u5408\u548c\u7a7a\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u957f\u671f\u9884\u62a5\u6a21\u7cca\u4e14\u7269\u7406\u4e0d\u771f\u5b9e\u3002", "method": "\u5c06\u6a21\u578b\u8bad\u7ec3\u91cd\u65b0\u89e3\u91ca\u4e3aWC-4DVar\u95ee\u9898\uff0c\u5728\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u663e\u5f0f\u5efa\u6a21\u9ad8\u7ef4\u6a21\u578b\u7a7a\u95f4\u4e2d\u7684\u518d\u5206\u6790\u8bef\u5dee\u534f\u65b9\u5dee\u3002", "result": "\u6f5c\u5728\u7a7a\u95f4\u7ea6\u675f\u7684\u6eda\u52a8\u8bad\u7ec3\u76f8\u6bd4\u6a21\u578b\u7a7a\u95f4\u635f\u5931\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u62a5\u6280\u80fd\uff0c\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u7cbe\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u5f02\u6784\u6570\u636e\u6e90\uff0c\u4f7f\u9884\u62a5\u6a21\u578b\u80fd\u591f\u5728\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u4e0b\u8054\u5408\u8bad\u7ec3\u518d\u5206\u6790\u548c\u591a\u6e90\u89c2\u6d4b\u6570\u636e\u3002"}}
{"id": "2510.04008", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04008", "abs": "https://arxiv.org/abs/2510.04008", "authors": ["Sahil Joshi", "Agniva Chowdhury", "Amar Kanakamedala", "Ekam Singh", "Evan Tu", "Anshumali Shrivastava"], "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention", "comment": "28 pages, 7 figures", "summary": "Softmax Attention has a quadratic time complexity, which becomes prohibitive\nto run at long contexts, even with highly optimized GPU kernels. For example,\nFlashAttention (an exact, GPU-optimized implementation of Softmax Attention)\ncannot complete a single forward-backward pass of a multi-head attention layer\nonce the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We\nintroduce RACE Attention, a kernel-inspired alternative to Softmax Attention\nthat is linear in sequence length and embedding dimension. RACE Attention\nreplaces the exponential kernel with a sharpened angular (cosine) similarity,\nand approximates attention outputs via randomized projections and soft\nLocality-Sensitive Hashing (LSH). Across language modeling, masked language\nmodeling, and text classification, RACE Attention matches the accuracy of\nstrong baselines while reducing runtime and memory. In a controlled scale test,\nit processes up to 12 million tokens during a single forward-backward pass on\nan NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well\nbeyond the practical limits of the current state-of-the-art attention\nimplementations. RACE Attention thus offers a practical, theoretically grounded\nmechanism for outrageously long context windows on today's hardware. We hope\nthat it gets adopted in practice.", "AI": {"tldr": "RACE Attention\u662f\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u66ff\u4ee3\u4e8c\u6b21\u590d\u6742\u5ea6\u7684Softmax Attention\uff0c\u80fd\u5728\u5355\u6b21\u524d\u5411-\u53cd\u5411\u4f20\u64ad\u4e2d\u5904\u7406\u6570\u767e\u4e07token\u7684\u957f\u4e0a\u4e0b\u6587\u3002", "motivation": "Softmax Attention\u7684\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u5373\u4f7f\u4f7f\u7528\u4f18\u5316\u7684GPU\u5185\u6838\uff08\u5982FlashAttention\uff09\u4e5f\u65e0\u6cd5\u5904\u7406\u8d85\u8fc7\u7ea6400\u4e07token\u7684\u4e0a\u4e0b\u6587\u3002", "method": "RACE Attention\u4f7f\u7528\u9510\u5316\u7684\u89d2\u5ea6\uff08\u4f59\u5f26\uff09\u76f8\u4f3c\u5ea6\u66ff\u4ee3\u6307\u6570\u6838\uff0c\u901a\u8fc7\u968f\u673a\u6295\u5f71\u548c\u8f6f\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\uff08LSH\uff09\u6765\u8fd1\u4f3c\u6ce8\u610f\u529b\u8f93\u51fa\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cRACE Attention\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u5728NVIDIA GH200 GPU\u4e0a\u80fd\u5904\u74061200\u4e07token\uff0c\u5728Intel Xeon Gold 5220R CPU\u4e0a\u80fd\u5904\u74067500\u4e07token\u3002", "conclusion": "RACE Attention\u4e3a\u5f53\u524d\u786c\u4ef6\u63d0\u4f9b\u4e86\u5904\u7406\u6781\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u5b9e\u7528\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u673a\u5236\uff0c\u6709\u671b\u5728\u5b9e\u8df5\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.04019", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04019", "abs": "https://arxiv.org/abs/2510.04019", "authors": ["Anthony Zhan"], "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs) are a new paradigm of\nnon-autoregressive language models that are trained to predict multiple tokens\nin parallel and generate text via iterative unmasking. Recent works have\nsuccessfully pretrained dLLMs to parity with autoregressive LLMs at the 8B\nscale, but dLLMs have yet to benefit from modern post-training techniques, e.g.\nreinforcement learning (RL), that have proven effective for autoregressive\nmodels. Crucially, algorithms designed for traditional LLMs aren't directly\ncompatible with diffusion frameworks due to inherent differences in modeling\nassumptions. Moreover, existing attempts at dLLM post-training with RL rely on\nheuristic-based objectives with no theoretical grounding. In this work, we\npresent Amortized Group Relative Policy Optimization (AGRPO), a principled\non-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo\nsampling to compute an unbiased policy gradient estimate, making it the first\ntractable, faithful adaptation of policy gradient methods for dLLMs. We\ndemonstrate AGRPO's effectiveness on different math/reasoning tasks, a common\nsetting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x\nperformance on the Countdown task over the baseline LLaDA-8B-Instruct model and\n1.3x performance gains over comparable RL methods such as diffu-GRPO.\nFurthermore, these gains persist across different numbers of sampling steps at\ninference time, achieving better tradeoffs between compute and performance. Our\nresults demonstrate that online RL algorithms can be extended to diffusion LLMs\nin principled ways, maintaining both theoretical soundness and practical\neffectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AGRPO\u7b97\u6cd5\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5c1a\u672a\u53d7\u76ca\u4e8e\u73b0\u4ee3\u540e\u8bad\u7ec3\u6280\u672f\uff08\u5982\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4f20\u7edfLLM\u7684\u7b97\u6cd5\u4e0e\u6269\u6563\u6846\u67b6\u4e0d\u517c\u5bb9\uff0c\u73b0\u6709dLLM\u7684RL\u8bad\u7ec3\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86AGRPO\u7b97\u6cd5\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8ba1\u7b97\u65e0\u504f\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\uff0c\u662f\u9996\u4e2a\u9002\u7528\u4e8edLLM\u7684\u53ef\u5904\u7406\u3001\u5fe0\u5b9e\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aGSM8K\u4efb\u52a1\u7edd\u5bf9\u589e\u76ca+7.6%\uff0cCountdown\u4efb\u52a1\u6027\u80fd\u63d0\u53473.8\u500d\uff0c\u6bd4diffu-GRPO\u65b9\u6cd5\u63d0\u53471.3\u500d\u3002", "conclusion": "\u5728\u7ebfRL\u7b97\u6cd5\u53ef\u4ee5\u4ee5\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u7684\u65b9\u5f0f\u6269\u5c55\u5230\u6269\u6563LLM\uff0c\u4fdd\u6301\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\uff0c\u5728\u4e0d\u540c\u91c7\u6837\u6b65\u6570\u4e0b\u90fd\u80fd\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u6027\u80fd\u7684\u66f4\u597d\u6743\u8861\u3002"}}
{"id": "2510.04020", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04020", "abs": "https://arxiv.org/abs/2510.04020", "authors": ["Hao Wu", "Yuan Gao", "Xingjian Shi", "Shuaipeng Li", "Fan Xu", "Fan Zhang", "Zhihong Zhu", "Weiyan Wang", "Xiao Luo", "Kun Wang", "Xian Wu", "Xiaomeng Huang"], "title": "Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models", "comment": null, "summary": "To address the dual challenges of inherent stochasticity and\nnon-differentiable metrics in physical spatiotemporal forecasting, we propose\nSpatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in\nModel-Based Reinforcement Learning. SFP constructs a novel Generative World\nModel to simulate diverse, high-fidelity future states, enabling an\n\"imagination-based\" environmental simulation. Within this framework, a base\nforecasting model acts as an agent, guided by a beam search-based planning\nalgorithm that leverages non-differentiable domain metrics as reward signals to\nexplore high-return future sequences. These identified high-reward candidates\nthen serve as pseudo-labels to continuously optimize the agent's policy through\niterative self-training, significantly reducing prediction error and\ndemonstrating exceptional performance on critical domain metrics like capturing\nextreme events.", "AI": {"tldr": "\u63d0\u51faSFP\uff08\u65f6\u7a7a\u9884\u6d4b\u5373\u89c4\u5212\uff09\u65b0\u8303\u5f0f\uff0c\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u5fae\u5ea6\u91cf\u95ee\u9898\u3002\u901a\u8fc7\u751f\u6210\u4e16\u754c\u6a21\u578b\u6a21\u62df\u591a\u6837\u5316\u672a\u6765\u72b6\u6001\uff0c\u4f7f\u7528\u6ce2\u675f\u641c\u7d22\u89c4\u5212\u7b97\u6cd5\u63a2\u7d22\u9ad8\u56de\u62a5\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u81ea\u8bad\u7ec3\u4f18\u5316\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u7269\u7406\u65f6\u7a7a\u9884\u6d4b\u4e2d\u56fa\u6709\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u5fae\u5ea6\u91cf\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u4f18\u5316\u9886\u57df\u5173\u952e\u6307\u6807\u3002", "method": "\u6784\u5efa\u751f\u6210\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u73af\u5883\u6a21\u62df\uff0c\u5c06\u57fa\u7840\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u6ce2\u675f\u641c\u7d22\u89c4\u5212\u7b97\u6cd5\u4ee5\u4e0d\u53ef\u5fae\u9886\u57df\u5ea6\u91cf\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8bad\u7ec3\u4f18\u5316\u7b56\u7565\u3002", "result": "\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u5728\u6355\u83b7\u6781\u7aef\u4e8b\u4ef6\u7b49\u5173\u952e\u9886\u57df\u5ea6\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SFP\u8303\u5f0f\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u548c\u89c4\u5212\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u968f\u673a\u6027\u548c\u4e0d\u53ef\u5fae\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.04027", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04027", "abs": "https://arxiv.org/abs/2510.04027", "authors": ["Jinseong Park", "Yujin Choi", "Jaewook Lee"], "title": "Multi-Class Support Vector Machine with Differential Privacy", "comment": "NeurIPS 2025", "summary": "With the increasing need to safeguard data privacy in machine learning\nmodels, differential privacy (DP) is one of the major frameworks to build\nprivacy-preserving models. Support Vector Machines (SVMs) are widely used\ntraditional machine learning models due to their robust margin guarantees and\nstrong empirical performance in binary classification. However, applying DP to\nmulti-class SVMs is inadequate, as the standard one-versus-rest (OvR) and\none-versus-one (OvO) approaches repeatedly query each data sample when building\nmultiple binary classifiers, thus consuming the privacy budget proportionally\nto the number of classes. To overcome this limitation, we explore all-in-one\nSVM approaches for DP, which access each data sample only once to construct\nmulti-class SVM boundaries with margin maximization properties. We propose a\nnovel differentially Private Multi-class SVM (PMSVM) with weight and gradient\nperturbation methods, providing rigorous sensitivity and convergence analyses\nto ensure DP in all-in-one SVMs. Empirical results demonstrate that our\napproach surpasses existing DP-SVM methods in multi-class scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1\u591a\u7c7b\u652f\u6301\u5411\u91cf\u673a(PMSVM)\uff0c\u901a\u8fc7\u6743\u91cd\u548c\u68af\u5ea6\u6270\u52a8\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edfOvR\u548cOvO\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e0b\u9690\u79c1\u9884\u7b97\u6d88\u8017\u8fc7\u591a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u7c7bSVM\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e0b\u5b58\u5728\u9690\u79c1\u9884\u7b97\u6d88\u8017\u8fc7\u591a\u7684\u95ee\u9898\uff0c\u56e0\u4e3aOvR\u548cOvO\u65b9\u6cd5\u9700\u8981\u91cd\u590d\u67e5\u8be2\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u6765\u6784\u5efa\u591a\u4e2a\u4e8c\u5143\u5206\u7c7b\u5668\u3002", "method": "\u91c7\u7528all-in-one SVM\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u4ec5\u8bbf\u95ee\u4e00\u6b21\u6765\u6784\u5efa\u591a\u7c7bSVM\u8fb9\u754c\uff1b\u63d0\u51fa\u6743\u91cd\u548c\u68af\u5ea6\u6270\u52a8\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e25\u683c\u7684\u654f\u611f\u6027\u548c\u6536\u655b\u6027\u5206\u6790\u4ee5\u786e\u4fdd\u5dee\u5206\u9690\u79c1\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u7c7b\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684DP-SVM\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684PMSVM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7c7bSVM\u5728\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e0b\u7684\u9690\u79c1\u9884\u7b97\u6d88\u8017\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2510.04028", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04028", "abs": "https://arxiv.org/abs/2510.04028", "authors": ["Xinhao Yao", "Lu Yu", "Xiaolin Hu", "Fengwei Teng", "Qing Cui", "Jun Zhou", "Yong Liu"], "title": "The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View", "comment": null, "summary": "The ongoing debate on whether reinforcement learning with verifiable rewards\n(RLVR) expands or shrinks the reasoning capabilities of large language models\n(LLMs) remains unresolved. Some studies contend that RLVR mainly improves\nsampling efficiency but at the expense of diversity and exploratory capacity,\nresulting in capability boundary shrinkage. In contrast, others demonstrate\nthat prolonged training can lead to the emergence of novel reasoning\nstrategies, suggesting capability boundary expansion. To reconcile these\ncontradictory findings, we theoretically and empirically show that both\nperspectives are partially valid-each aligning with a separate phase in an\ninherent two-stage probability mass dynamic: (1) Exploitation stage: initially,\nthe model primarily samples explored high-reward and low-reward tokens, while\nrarely selecting the potentially optimal token. Positive advantage estimates\nincrease the probability of high-reward tokens and decrease those of low-reward\ntokens, yet the optimal token's probability remains largely unchanged during\nthis stage. (2) Exploration stage: as training advances, the growth rate of\npreviously acquired high-reward tokens slows as their probabilities approach\nsaturation. When a potentially optimal token-now receiving positive advantage\nestimates-is occasionally sampled, its probability increases, while those of\nthe originally high-reward tokens decrease. This dynamic suggests that\nover-exploitation during the exploitation stage may lead to capability boundary\nshrinkage, whereas prolonged training into the exploration stage can promote an\nexpansion of the reasoning capability boundary. Building upon our insights, we\nrevisit the potential of only using relative negative gradients for prolonging\ntraining, providing a theoretical and empirical foundation for the development\nof more advanced reasoning capabilities.", "AI": {"tldr": "RLVR\u8bad\u7ec3\u5b58\u5728\u4e24\u9636\u6bb5\u52a8\u6001\uff1a\u521d\u59cb\u9636\u6bb5\u8fc7\u5ea6\u5229\u7528\u9ad8\u5956\u52b1token\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u6536\u7f29\uff0c\u540e\u671f\u9636\u6bb5\u63a2\u7d22\u6f5c\u5728\u6700\u4f18token\u4fc3\u8fdb\u80fd\u529b\u8fb9\u754c\u6269\u5c55\u3002", "motivation": "\u8c03\u548c\u5173\u4e8eRLVR\u5bf9LLM\u63a8\u7406\u80fd\u529b\u8fb9\u754c\u5f71\u54cd\u7684\u77db\u76fe\u89c2\u70b9\u2014\u2014\u6709\u4e9b\u7814\u7a76\u8ba4\u4e3a\u6536\u7f29\uff0c\u6709\u4e9b\u8ba4\u4e3a\u6269\u5c55\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793aRLVR\u8bad\u7ec3\u4e2d\u7684\u4e24\u9636\u6bb5\u6982\u7387\u8d28\u91cf\u52a8\u6001\uff1a\u5229\u7528\u9636\u6bb5\u548c\u63a2\u7d22\u9636\u6bb5\u3002", "result": "\u53d1\u73b0\u5229\u7528\u9636\u6bb5\u53ef\u80fd\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u6536\u7f29\uff0c\u800c\u8fdb\u5165\u63a2\u7d22\u9636\u6bb5\u540e\u80fd\u4fc3\u8fdb\u80fd\u529b\u8fb9\u754c\u6269\u5c55\u3002", "conclusion": "\u4e24\u9636\u6bb5\u52a8\u6001\u89e3\u91ca\u4e86\u77db\u76fe\u53d1\u73b0\uff0c\u57fa\u4e8e\u6b64\u91cd\u65b0\u5ba1\u89c6\u4ec5\u4f7f\u7528\u76f8\u5bf9\u8d1f\u68af\u5ea6\u5ef6\u957f\u8bad\u7ec3\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2510.04046", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04046", "abs": "https://arxiv.org/abs/2510.04046", "authors": ["Kotaro J. Nishimura", "Yuichi Sakumura", "Kazushi Ikeda"], "title": "Adaptive kernel-density approach for imbalanced binary classification", "comment": null, "summary": "Class imbalance is a common challenge in real-world binary classification\ntasks, often leading to predictions biased toward the majority class and\nreduced recognition of the minority class. This issue is particularly critical\nin domains such as medical diagnosis and anomaly detection, where correct\nclassification of minority classes is essential. Conventional methods often\nfail to deliver satisfactory performance when the imbalance ratio is extremely\nsevere. To address this challenge, we propose a novel approach called\nKernel-density-Oriented Threshold Adjustment with Regional Optimization\n(KOTARO), which extends the framework of kernel density estimation (KDE) by\nadaptively adjusting decision boundaries according to local sample density. In\nKOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on\nthe estimated density around each sample, thereby enhancing the classifier's\nability to capture minority regions. We validated the effectiveness of KOTARO\nthrough experiments on both synthetic and real-world imbalanced datasets. The\nresults demonstrated that KOTARO outperformed conventional methods,\nparticularly under conditions of severe imbalance, highlighting its potential\nas a promising solution for a wide range of imbalanced classification problems", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKOTARO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u51b3\u7b56\u8fb9\u754c\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u5e38\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u504f\u5411\u591a\u6570\u7c7b\uff0c\u5728\u533b\u7597\u8bca\u65ad\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5173\u952e\u9886\u57df\u4e2d\u6b63\u786e\u5206\u7c7b\u5c11\u6570\u7c7b\u81f3\u5173\u91cd\u8981", "method": "KOTARO\u65b9\u6cd5\u6269\u5c55\u4e86\u6838\u5bc6\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u6839\u636e\u5c40\u90e8\u6837\u672c\u5bc6\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u51b3\u7b56\u8fb9\u754c\uff0c\u52a8\u6001\u8c03\u8282\u9ad8\u65af\u57fa\u51fd\u6570\u7684\u5e26\u5bbd\u4ee5\u589e\u5f3a\u5bf9\u5c11\u6570\u7c7b\u533a\u57df\u7684\u6355\u6349\u80fd\u529b", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKOTARO\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa", "conclusion": "KOTARO\u6709\u6f5c\u529b\u6210\u4e3a\u89e3\u51b3\u5404\u79cd\u4e0d\u5e73\u8861\u5206\u7c7b\u95ee\u9898\u7684\u6709\u524d\u666f\u65b9\u6848"}}
{"id": "2510.04058", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04058", "abs": "https://arxiv.org/abs/2510.04058", "authors": ["Subhodip Panda", "MS Varun", "Shreyans Jain", "Sarthak Kumar Maharana", "Prathosh A. P"], "title": "Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints", "comment": null, "summary": "For a responsible and safe deployment of diffusion models in various domains,\nregulating the generated outputs from these models is desirable because such\nmodels could generate undesired, violent, and obscene outputs. To tackle this\nproblem, recent works use machine unlearning methodology to forget training\ndata points containing these undesired features from pre-trained generative\nmodels. However, these methods proved to be ineffective in data-constrained\nsettings where the whole training dataset is inaccessible. Thus, the principal\nobjective of this work is to propose a machine unlearning methodology that can\nprevent the generation of outputs containing undesired features from a\npre-trained diffusion model in such a data-constrained setting. Our proposed\nmethod, termed as Variational Diffusion Unlearning (VDU), is a computationally\nefficient method that only requires access to a subset of training data\ncontaining undesired features. Our approach is inspired by the variational\ninference framework with the objective of minimizing a loss function consisting\nof two terms: plasticity inducer and stability regularizer. Plasticity inducer\nreduces the log-likelihood of the undesired training data points, while the\nstability regularizer, essential for preventing loss of image generation\nquality, regularizes the model in parameter space. We validate the\neffectiveness of our method through comprehensive experiments for both class\nunlearning and feature unlearning. For class unlearning, we unlearn some\nuser-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a\npre-trained unconditional denoising diffusion probabilistic model (DDPM).\nSimilarly, for feature unlearning, we unlearn the generation of certain\nhigh-level features from a pre-trained Stable Diffusion model", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53d8\u5206\u6269\u6563\u9057\u5fd8(VDU)\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e2d\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u79fb\u9664\u4e0d\u9700\u8981\u7684\u7279\u5f81\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u6269\u6563\u6a21\u578b\uff0c\u9700\u8981\u9632\u6b62\u6a21\u578b\u751f\u6210\u4e0d\u826f\u3001\u66b4\u529b\u548c\u6deb\u79fd\u5185\u5bb9\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u53ea\u9700\u8bbf\u95ee\u90e8\u5206\u4e0d\u826f\u6570\u636e\u5b50\u96c6\u7684\u9057\u5fd8\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u4f7f\u7528\u5305\u542b\u5851\u6027\u8bf1\u5bfc\u5668\u548c\u7a33\u5b9a\u6027\u6b63\u5219\u5316\u5668\u7684\u635f\u5931\u51fd\u6570\u3002\u5851\u6027\u8bf1\u5bfc\u5668\u964d\u4f4e\u4e0d\u826f\u8bad\u7ec3\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136\uff0c\u7a33\u5b9a\u6027\u6b63\u5219\u5316\u5668\u5728\u53c2\u6570\u7a7a\u95f4\u6b63\u5219\u5316\u6a21\u578b\u4ee5\u9632\u6b62\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548ctinyImageNet\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7c7b\u522b\u9057\u5fd8\u6548\u679c\uff0c\u5728Stable Diffusion\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u7279\u5f81\u9057\u5fd8\u6548\u679c\uff0c\u8bc1\u660e\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "VDU\u65b9\u6cd5\u80fd\u591f\u5728\u4ec5\u8bbf\u95ee\u90e8\u5206\u4e0d\u826f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4ece\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u79fb\u9664\u4e0d\u9700\u8981\u7684\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04067", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04067", "abs": "https://arxiv.org/abs/2510.04067", "authors": ["Junxi Yan", "Zixi Wei", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu"], "title": "What Scales in Cross-Entropy Scaling Law?", "comment": null, "summary": "The cross-entropy scaling law has long served as a key tool for guiding the\ndevelopment of large language models. It shows that cross-entropy loss\ndecreases in a predictable power-law rate as the model size increases. However,\nrecent evidence indicates that this law breaks down at very large scales: the\nloss decreases more slowly than expected, which causes significant trouble for\ndeveloping large language models. In this paper, we hypothesize that the root\ncause lies in the fact that cross-entropy itself does not truly scale; instead,\nonly one of its hidden components does. To investigate this, we introduce a\nnovel decomposition of cross-entropy into three parts: Error-Entropy,\nSelf-Alignment, and Confidence. We show both theoretically and empirically that\nthis decomposition precisely captures the training dynamics and optimization\nobjectives. Through extensive experiments on multiple datasets and 32 models\nspanning five orders of magnitude in size, we find that only error-entropy\nfollows a robust power-law scaling, while the other two terms remain largely\ninvariant. Moreover, error-entropy constitutes the dominant share of\ncross-entropy in small models but diminishes in proportion as models grow\nlarger. This explains why the cross-entropy scaling law appears accurate at\nsmall scales but fails at very large ones. Our findings establish the\nerror-entropy scaling law as a more accurate description of model behavior. We\nbelieve it will have wide applications in the training, understanding, and\nfuture development of large language models.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u4ea4\u53c9\u71b5\u7f29\u653e\u5b9a\u5f8b\u5728\u8d85\u5927\u89c4\u6a21\u4e0b\u5931\u6548\uff0c\u63d0\u51fa\u5c06\u4ea4\u53c9\u71b5\u5206\u89e3\u4e3a\u8bef\u5dee\u71b5\u3001\u81ea\u5bf9\u9f50\u548c\u7f6e\u4fe1\u5ea6\u4e09\u4e2a\u90e8\u5206\uff0c\u53d1\u73b0\u53ea\u6709\u8bef\u5dee\u71b5\u9075\u5faa\u7a33\u5065\u7684\u5e42\u5f8b\u7f29\u653e\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u4ea4\u53c9\u71b5\u7f29\u653e\u5b9a\u5f8b\u5728\u5c0f\u89c4\u6a21\u65f6\u51c6\u786e\u4f46\u5728\u8d85\u5927\u89c4\u6a21\u65f6\u5931\u6548\u3002", "motivation": "\u4ea4\u53c9\u71b5\u7f29\u653e\u5b9a\u5f8b\u957f\u671f\u4ee5\u6765\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0c\u4f46\u6700\u8fd1\u8bc1\u636e\u8868\u660e\u8be5\u5b9a\u5f8b\u5728\u8d85\u5927\u89c4\u6a21\u4e0b\u5931\u6548\uff0c\u635f\u5931\u4e0b\u964d\u901f\u5ea6\u6bd4\u9884\u671f\u6162\uff0c\u8fd9\u7ed9\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u5e26\u6765\u663e\u8457\u56f0\u6270\u3002", "method": "\u5f15\u5165\u4ea4\u53c9\u71b5\u7684\u65b0\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u8bef\u5dee\u71b5\u3001\u81ea\u5bf9\u9f50\u548c\u7f6e\u4fe1\u5ea6\u4e09\u4e2a\u90e8\u5206\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c32\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e94\u4e2a\u6570\u91cf\u7ea7\u7684\u5927\u5c0f\u3002", "result": "\u53ea\u6709\u8bef\u5dee\u71b5\u9075\u5faa\u7a33\u5065\u7684\u5e42\u5f8b\u7f29\u653e\uff0c\u800c\u5176\u4ed6\u4e24\u4e2a\u9879\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\uff1b\u8bef\u5dee\u71b5\u5728\u5c0f\u6a21\u578b\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u968f\u7740\u6a21\u578b\u589e\u5927\u6bd4\u4f8b\u51cf\u5c0f\u3002", "conclusion": "\u8bef\u5dee\u71b5\u7f29\u653e\u5b9a\u5f8b\u80fd\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u6a21\u578b\u884c\u4e3a\uff0c\u5c06\u5728\u8bad\u7ec3\u3001\u7406\u89e3\u548c\u672a\u6765\u5f00\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.04072", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04072", "abs": "https://arxiv.org/abs/2510.04072", "authors": ["Ziyan Wang", "Zheng Wang", "Jie Fu", "Xingwei Qu", "Qi Cheng", "Shengpu Tang", "Minjia Zhang", "Xiaoming Huo"], "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in\nlarge language models (LLMs). Yet on-policy algorithms such as Group Relative\nPolicy Optimization (GRPO) often suffer in early training: noisy gradients from\nlow-quality rollouts lead to unstable updates and inefficient exploration. We\nintroduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient\nframework to address these limitations via decomposing each step into three\nstages: a short fast trajectory of inner steps on the same batch, a reposition\nmechanism to control off-policy drift, and a final slow correction. This\nreposition-before-update design preserves the objective and rollout process\nunchanged, making SFPO plug-compatible with existing policy-gradient pipelines.\nExtensive experiments demonstrate that SFPO consistently improves stability,\nreduces rollouts, and accelerates convergence of reasoning RL training.\nSpecifically, it outperforms GRPO by up to 2.80 points in average on math\nreasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts\nand a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best\naccuracy.", "AI": {"tldr": "SFPO\u901a\u8fc7\u6162-\u5feb\u7b56\u7565\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u4f4e\u6548\u63a2\u7d22\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eGRPO\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\uff09\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u9762\u4e34\u68af\u5ea6\u566a\u58f0\u5927\u3001\u66f4\u65b0\u4e0d\u7a33\u5b9a\u548c\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSFPO\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5728\u76f8\u540c\u6279\u6b21\u4e0a\u8fdb\u884c\u77ed\u5feb\u901f\u8f68\u8ff9\u5185\u6b65\u3001\u91cd\u5b9a\u4f4d\u673a\u5236\u63a7\u5236\u79bb\u7b56\u7565\u6f02\u79fb\u3001\u6700\u7ec8\u6162\u901f\u6821\u6b63\u3002", "result": "SFPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4GRPO\u5e73\u5747\u63d0\u53472.80\u5206\uff0c\u51cf\u5c114.93\u500drollouts\uff0c\u5728\u8fbe\u5230GRPO\u6700\u4f73\u51c6\u786e\u7387\u65f6\u51cf\u5c114.19\u500dwall-clock\u65f6\u95f4\u3002", "conclusion": "SFPO\u901a\u8fc7\u91cd\u5b9a\u4f4d-\u66f4\u65b0\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u6548\u7387\uff0c\u4e14\u4e0e\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u6d41\u7a0b\u517c\u5bb9\u3002"}}
{"id": "2510.04088", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04088", "abs": "https://arxiv.org/abs/2510.04088", "authors": ["Nan Jiang", "Tengyang Xie"], "title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees", "comment": "To appear in Statistical Science", "summary": "This article introduces the theory of offline reinforcement learning in large\nstate spaces, where good policies are learned from historical data without\nonline interactions with the environment. Key concepts introduced include\nexpressivity assumptions on function approximation (e.g., Bellman completeness\nvs. realizability) and data coverage (e.g., all-policy vs. single-policy\ncoverage). A rich landscape of algorithms and results is described, depending\non the assumptions one is willing to make and the sample and computational\ncomplexity guarantees one wishes to achieve. We also discuss open questions and\nconnections to adjacent areas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u8fdb\u884c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\uff0c\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u7b56\u7565\u800c\u65e0\u9700\u4e0e\u73af\u5883\u5728\u7ebf\u4ea4\u4e92\u3002", "motivation": "\u7814\u7a76\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63a2\u7d22\u5728\u4ec5\u4f7f\u7528\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u6709\u6548\u7b56\u7565\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5f15\u5165\u51fd\u6570\u903c\u8fd1\u7684\u8868\u8fbe\u6027\u5047\u8bbe\uff08\u5982\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027vs\u53ef\u5b9e\u73b0\u6027\uff09\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\uff08\u5982\u5168\u7b56\u7565vs\u5355\u7b56\u7565\u8986\u76d6\uff09\uff0c\u5206\u6790\u4e0d\u540c\u5047\u8bbe\u4e0b\u7684\u7b97\u6cd5\u548c\u7ed3\u679c\u3002", "result": "\u63cf\u8ff0\u4e86\u4e30\u5bcc\u7684\u7b97\u6cd5\u548c\u7ed3\u679c\u56fe\u8c31\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5047\u8bbe\u4e0b\u53ef\u5b9e\u73b0\u7684\u6837\u672c\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u6027\u95ee\u9898\u4ee5\u53ca\u4e0e\u76f8\u90bb\u9886\u57df\u7684\u8054\u7cfb\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2510.04090", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04090", "abs": "https://arxiv.org/abs/2510.04090", "authors": ["Nikita Gabdullin"], "title": "Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes", "comment": "28 pages, 12 figures, 10 tables, 12 equations, 1 algorithm", "summary": "Supervised learning (SL) methods are indispensable for neural network (NN)\ntraining used to perform classification tasks. While resulting in very high\naccuracy, SL training often requires making NN parameter number dependent on\nthe number of classes, limiting their applicability when the number of classes\nis extremely large or unknown in advance. In this paper we propose a\nmethodology that allows one to train the same NN architecture regardless of the\nnumber of classes. This is achieved by using predefined vector systems as the\ntarget latent space configuration (LSC) during NN training. We discuss the\ndesired properties of target configurations and choose randomly perturbed\nvectors of An root system for our experiments. These vectors are used to\nsuccessfully train encoders and visual transformers (ViT) on Cinic-10 and\nImageNet-1K in low- and high-dimensional cases by matching NN predictions with\nthe predefined vectors. Finally, ViT is trained on a dataset with 1.28 million\nclasses illustrating the applicability of the method to training on datasets\nwith extremely large number of classes. In addition, potential applications of\nLSC in lifelong learning and NN distillation are discussed illustrating\nversatility of the proposed methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e0d\u4f9d\u8d56\u7c7b\u522b\u6570\u91cf\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u5411\u91cf\u7cfb\u7edf\u4f5c\u4e3a\u76ee\u6807\u6f5c\u5728\u7a7a\u95f4\u914d\u7f6e\uff0c\u53ef\u5728\u7c7b\u522b\u6570\u6781\u5927\u6216\u672a\u77e5\u65f6\u8bad\u7ec3\u76f8\u540c\u67b6\u6784\u7684\u795e\u7ecf\u7f51\u7edc", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u6570\u91cf\u4f9d\u8d56\u4e8e\u7c7b\u522b\u6570\u91cf\uff0c\u8fd9\u5728\u7c7b\u522b\u6570\u6781\u5927\u6216\u672a\u77e5\u65f6\u9650\u5236\u4e86\u5e94\u7528", "method": "\u4f7f\u7528\u9884\u5b9a\u4e49\u5411\u91cf\u7cfb\u7edf\uff08\u5982An\u6839\u7cfb\u7edf\u7684\u968f\u673a\u6270\u52a8\u5411\u91cf\uff09\u4f5c\u4e3a\u76ee\u6807\u6f5c\u5728\u7a7a\u95f4\u914d\u7f6e\uff0c\u901a\u8fc7\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4e0e\u9884\u5b9a\u4e49\u5411\u91cf\u6765\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668", "result": "\u6210\u529f\u5728Cinic-10\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7f16\u7801\u5668\u548cViT\uff0c\u5e76\u5728128\u4e07\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u7c7b\u522b\u6570\u6781\u5927\u7684\u573a\u666f\uff0c\u5e76\u5728\u6301\u7eed\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u84b8\u998f\u65b9\u9762\u6709\u6f5c\u5728\u5e94\u7528"}}
{"id": "2510.04091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04091", "abs": "https://arxiv.org/abs/2510.04091", "authors": ["Wei Wang", "Tianhao Ma", "Ming-Kun Xie", "Gang Niu", "Masashi Sugiyama"], "title": "Rethinking Consistent Multi-Label Classification under Inexact Supervision", "comment": null, "summary": "Partial multi-label learning and complementary multi-label learning are two\npopular weakly supervised multi-label classification paradigms that aim to\nalleviate the high annotation costs of collecting precisely annotated\nmulti-label data. In partial multi-label learning, each instance is annotated\nwith a candidate label set, among which only some labels are relevant; in\ncomplementary multi-label learning, each instance is annotated with\ncomplementary labels indicating the classes to which the instance does not\nbelong. Existing consistent approaches for the two paradigms either require\naccurate estimation of the generation process of candidate or complementary\nlabels or assume a uniform distribution to eliminate the estimation problem.\nHowever, both conditions are usually difficult to satisfy in real-world\nscenarios. In this paper, we propose consistent approaches that do not rely on\nthe aforementioned conditions to handle both problems in a unified way.\nSpecifically, we propose two unbiased risk estimators based on first- and\nsecond-order strategies. Theoretically, we prove consistency w.r.t. two widely\nused multi-label classification evaluation metrics and derive convergence rates\nfor the estimation errors of the proposed risk estimators. Empirically,\nextensive experimental results validate the effectiveness of our proposed\napproaches against state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5904\u7406\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\u548c\u4e92\u8865\u591a\u6807\u7b7e\u5b66\u4e60\u7684\u4e00\u81f4\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u7b7e\u751f\u6210\u8fc7\u7a0b\u7684\u51c6\u786e\u4f30\u8ba1\u6216\u5747\u5300\u5206\u5e03\u5047\u8bbe\uff0c\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u7b56\u7565\u6784\u5efa\u65e0\u504f\u98ce\u9669\u4f30\u8ba1\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\u548c\u4e92\u8865\u591a\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u5019\u9009\u6807\u7b7e\u6216\u4e92\u8865\u6807\u7b7e\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u6216\u5047\u8bbe\u5747\u5300\u5206\u5e03\u6765\u6d88\u9664\u4f30\u8ba1\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u6761\u4ef6\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u901a\u5e38\u96be\u4ee5\u6ee1\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e00\u9636\u548c\u4e8c\u9636\u7b56\u7565\u7684\u65e0\u504f\u98ce\u9669\u4f30\u8ba1\u5668\uff0c\u6784\u5efa\u7edf\u4e00\u6846\u67b6\u5904\u7406\u4e24\u79cd\u5f31\u76d1\u7763\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u4e0e\u591a\u6807\u7b7e\u5206\u7c7b\u8bc4\u4f30\u6307\u6807\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6240\u63d0\u98ce\u9669\u4f30\u8ba1\u5668\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u4f30\u8ba1\u8bef\u5dee\u7684\u6536\u655b\u901f\u7387\uff1b\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65e0\u9700\u4f9d\u8d56\u6807\u7b7e\u751f\u6210\u8fc7\u7a0b\u51c6\u786e\u4f30\u8ba1\u6216\u5747\u5300\u5206\u5e03\u5047\u8bbe\u7684\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u90e8\u5206\u591a\u6807\u7b7e\u5b66\u4e60\u548c\u4e92\u8865\u591a\u6807\u7b7e\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04102", "categories": ["cs.LG", "cs.NA", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.04102", "abs": "https://arxiv.org/abs/2510.04102", "authors": ["Ramzi Dakhmouche", "Hossein Gorji"], "title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws", "comment": null, "summary": "Motivated by the remarkable success of Foundation Models (FMs) in language\nmodeling, there has been growing interest in developing FMs for time series\nprediction, given the transformative power such models hold for science and\nengineering. This culminated in significant success of FMs in short-range\nforecasting settings. However, extrapolation or long-range forecasting remains\nelusive for FMs, which struggle to outperform even simple baselines. This\ncontrasts with physical laws which have strong extrapolation properties, and\nraises the question of the fundamental difference between the structure of\nneural networks and physical laws. In this work, we identify and formalize a\nfundamental property characterizing the ability of statistical learning models\nto predict more accurately outside of their training domain, hence explaining\nperformance deterioration for deep learning models in extrapolation settings.\nIn addition to a theoretical analysis, we present empirical results showcasing\nthe implications of this property on current deep learning architectures. Our\nresults not only clarify the root causes of the extrapolation gap but also\nsuggest directions for designing next-generation forecasting models capable of\nmastering extrapolation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5916\u63a8\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8868\u5f81\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u57df\u5916\u9884\u6d4b\u80fd\u529b\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u4e0e\u7269\u7406\u5b9a\u5f8b\u5728\u7ed3\u6784\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u3002", "motivation": "\u53d7\u57fa\u7840\u6a21\u578b\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u663e\u8457\u6210\u529f\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\u5177\u6709\u53d8\u9769\u6027\u610f\u4e49\u3002\u867d\u7136\u57fa\u7840\u6a21\u578b\u5728\u77ed\u7a0b\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5916\u63a8\u6216\u957f\u7a0b\u9884\u6d4b\u4e2d\u751a\u81f3\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\uff0c\u8fd9\u4e0e\u7269\u7406\u5b9a\u5f8b\u7684\u5f3a\u5916\u63a8\u7279\u6027\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002", "method": "\u8bc6\u522b\u5e76\u5f62\u5f0f\u5316\u4e86\u4e00\u4e2a\u8868\u5f81\u7edf\u8ba1\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u57df\u5916\u9884\u6d4b\u80fd\u529b\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u5c5e\u6027\u5bf9\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u9610\u660e\u4e86\u5916\u63a8\u5dee\u8ddd\u7684\u6839\u672c\u539f\u56e0\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5916\u63a8\u8bbe\u7f6e\u4e2d\u6027\u80fd\u6076\u5316\u7684\u673a\u5236\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u6f84\u6e05\u4e86\u5916\u63a8\u5dee\u8ddd\u7684\u6839\u6e90\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u80fd\u591f\u638c\u63e1\u5916\u63a8\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3\u9884\u6d4b\u6a21\u578b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.04108", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04108", "abs": "https://arxiv.org/abs/2510.04108", "authors": ["Ramzi Dakhmouche", "Adrien Letellier", "Hossein Gorji"], "title": "Can Linear Probes Measure LLM Uncertainty?", "comment": null, "summary": "Effective Uncertainty Quantification (UQ) represents a key aspect for\nreliable deployment of Large Language Models (LLMs) in automated\ndecision-making and beyond. Yet, for LLM generation with multiple choice\nstructure, the state-of-the-art in UQ is still dominated by the naive baseline\ngiven by the maximum softmax score. To address this shortcoming, we demonstrate\nthat taking a principled approach via Bayesian statistics leads to improved\nperformance despite leveraging the simplest possible model, namely linear\nregression. More precisely, we propose to train multiple Bayesian linear\nmodels, each predicting the output of a layer given the output of the previous\none. Based on the obtained layer-level posterior distributions, we infer the\nglobal uncertainty level of the LLM by identifying a sparse combination of\ndistributional features, leading to an efficient UQ scheme. Numerical\nexperiments on various LLMs show consistent improvement over state-of-the-art\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u7edf\u8ba1\u7684LLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u8d1d\u53f6\u65af\u7ebf\u6027\u6a21\u578b\u6765\u9884\u6d4bLLM\u5404\u5c42\u8f93\u51fa\uff0c\u4ece\u800c\u6539\u8fdb\u591a\u9009\u62e9\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u5f53\u524dLLM\u5728\u591a\u9009\u62e9\u7ed3\u6784\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ecd\u4ee5\u6700\u5927softmax\u5206\u6570\u4e3a\u4e3b\uff0c\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u591a\u4e2a\u8d1d\u53f6\u65af\u7ebf\u6027\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u9884\u6d4bLLM\u67d0\u4e00\u5c42\u7684\u8f93\u51fa\u57fa\u4e8e\u524d\u4e00\u5c42\u8f93\u51fa\uff0c\u901a\u8fc7\u5c42\u95f4\u540e\u9a8c\u5206\u5e03\u63a8\u65ad\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u3002", "result": "\u5728\u4e0d\u540cLLM\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u8d1d\u53f6\u65af\u7edf\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u8fdbLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u7b80\u5355\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u4e5f\u80fd\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.04114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04114", "abs": "https://arxiv.org/abs/2510.04114", "authors": ["Wanxin Li", "Yongjin P. Park", "Khanh Dao Duc"], "title": "Wasserstein projection distance for fairness testing of regression models", "comment": null, "summary": "Fairness in machine learning is a critical concern, yet most research has\nfocused on classification tasks, leaving regression models underexplored. This\npaper introduces a Wasserstein projection-based framework for fairness testing\nin regression models, focusing on expectation-based criteria. We propose a\nhypothesis-testing approach and an optimal data perturbation method to improve\nfairness while balancing accuracy. Theoretical results include a detailed\ncategorization of fairness criteria for regression, a dual reformulation of the\nWasserstein projection test statistic, and the derivation of asymptotic bounds\nand limiting distributions. Experiments on synthetic and real-world datasets\ndemonstrate that the proposed method offers higher specificity compared to\npermutation-based tests, and effectively detects and mitigates biases in real\napplications such as student performance and housing price prediction.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWasserstein\u6295\u5f71\u7684\u56de\u5f52\u6a21\u578b\u516c\u5e73\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u548c\u6700\u4f18\u6570\u636e\u6270\u52a8\u65b9\u6cd5\u68c0\u6d4b\u548c\u6539\u5584\u516c\u5e73\u6027\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u4efb\u52a1\uff0c\u56de\u5f52\u6a21\u578b\u7684\u516c\u5e73\u6027\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "\u4f7f\u7528Wasserstein\u6295\u5f71\u6846\u67b6\u8fdb\u884c\u516c\u5e73\u6027\u6d4b\u8bd5\uff0c\u63d0\u51fa\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\u548c\u6700\u4f18\u6570\u636e\u6270\u52a8\u6280\u672f\uff0c\u7406\u8bba\u5206\u6790\u5305\u62ec\u516c\u5e73\u6027\u6807\u51c6\u5206\u7c7b\u3001\u5bf9\u5076\u91cd\u6784\u548c\u6e10\u8fd1\u754c\u9650\u63a8\u5bfc\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u7f6e\u6362\u7684\u6d4b\u8bd5\u5177\u6709\u66f4\u9ad8\u7279\u5f02\u6027\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3\u5b66\u751f\u6210\u7ee9\u9884\u6d4b\u3001\u623f\u4ef7\u9884\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u504f\u5dee\u3002", "conclusion": "\u8be5Wasserstein\u6295\u5f71\u6846\u67b6\u4e3a\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u548c\u6539\u5584\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u56de\u5f52\u6a21\u578b\u516c\u5e73\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.04115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04115", "abs": "https://arxiv.org/abs/2510.04115", "authors": ["George Giapitzakis", "Kimon Fountoulakis", "Eshaan Nichani", "Jason D. Lee"], "title": "On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach", "comment": "42 pages", "summary": "Semiautomata form a rich class of sequence-processing algorithms with\napplications in natural language processing, robotics, computational biology,\nand data mining. We establish the first Statistical Query hardness result for\nsemiautomata under the uniform distribution over input words and initial\nstates. We show that Statistical Query hardness can be established when both\nthe alphabet size and input length are polynomial in the number of states.\nUnlike the case of deterministic finite automata, where hardness typically\narises through the hardness of the language they recognize (e.g., parity), our\nresult is derived solely from the internal state-transition structure of\nsemiautomata. Our analysis reduces the task of distinguishing the final states\nof two semiautomata to studying the behavior of a random walk on the group\n$S_{N} \\times S_{N}$. By applying tools from Fourier analysis and the\nrepresentation theory of the symmetric group, we obtain tight spectral gap\nbounds, demonstrating that after a polynomial number of steps in the number of\nstates, distinct semiautomata become nearly uncorrelated, yielding the desired\nhardness result.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u534a\u81ea\u52a8\u673a\u5728\u7edf\u4e00\u5206\u5e03\u4e0b\u7684\u9996\u4e2a\u7edf\u8ba1\u67e5\u8be2\u786c\u5ea6\u7ed3\u679c\uff0c\u8bc1\u660e\u5f53\u5b57\u6bcd\u8868\u5927\u5c0f\u548c\u8f93\u5165\u957f\u5ea6\u90fd\u662f\u72b6\u6001\u6570\u7684\u591a\u9879\u5f0f\u65f6\uff0c\u534a\u81ea\u52a8\u673a\u5b58\u5728\u7edf\u8ba1\u67e5\u8be2\u786c\u5ea6\u3002", "motivation": "\u534a\u81ea\u52a8\u673a\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u673a\u5668\u4eba\u5b66\u3001\u8ba1\u7b97\u751f\u7269\u5b66\u548c\u6570\u636e\u6316\u6398\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4e4b\u524d\u7f3a\u4e4f\u5bf9\u5176\u7edf\u8ba1\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u534a\u81ea\u52a8\u673a\u7684\u5b66\u4e60\u96be\u5ea6\u3002", "method": "\u901a\u8fc7\u5c06\u533a\u5206\u4e24\u4e2a\u534a\u81ea\u52a8\u673a\u6700\u7ec8\u72b6\u6001\u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u7814\u7a76\u5728\u5bf9\u79f0\u7fa4S_N\u00d7S_N\u4e0a\u7684\u968f\u673a\u6e38\u8d70\u884c\u4e3a\uff0c\u5e94\u7528\u5085\u91cc\u53f6\u5206\u6790\u548c\u5bf9\u79f0\u7fa4\u8868\u793a\u8bba\u5de5\u5177\uff0c\u83b7\u5f97\u7d27\u81f4\u7684\u8c31\u9699\u754c\u9650\u3002", "result": "\u8bc1\u660e\u5728\u72b6\u6001\u6570\u7684\u591a\u9879\u5f0f\u6b65\u6570\u540e\uff0c\u4e0d\u540c\u7684\u534a\u81ea\u52a8\u673a\u53d8\u5f97\u51e0\u4e4e\u4e0d\u76f8\u5173\uff0c\u4ece\u800c\u5f97\u5230\u786c\u5ea6\u7ed3\u679c\u3002\u4e0e\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\u4e0d\u540c\uff0c\u8be5\u786c\u5ea6\u6e90\u4e8e\u534a\u81ea\u52a8\u673a\u7684\u5185\u90e8\u72b6\u6001\u8f6c\u79fb\u7ed3\u6784\u800c\u975e\u6240\u8bc6\u522b\u7684\u8bed\u8a00\u3002", "conclusion": "\u534a\u81ea\u52a8\u673a\u7684\u7edf\u8ba1\u67e5\u8be2\u786c\u5ea6\u53ef\u4ee5\u4ec5\u4ece\u5176\u5185\u90e8\u72b6\u6001\u8f6c\u79fb\u7ed3\u6784\u63a8\u5bfc\u51fa\u6765\uff0c\u4e3a\u7406\u89e3\u8fd9\u7c7b\u5e8f\u5217\u5904\u7406\u7b97\u6cd5\u7684\u5b66\u4e60\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.04126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04126", "abs": "https://arxiv.org/abs/2510.04126", "authors": ["Ziying Zhang", "Yaqing Wang", "Yuxuan Sun", "Min Ye", "Quanming Yao"], "title": "Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions", "comment": null, "summary": "Cold-start drug-target interaction (DTI) prediction focuses on interaction\nbetween novel drugs and proteins. Previous methods typically learn transferable\ninteraction patterns between structures of drug and proteins to tackle it.\nHowever, insight from proteomics suggest that protein have multi-level\nstructures and they all influence the DTI. Existing works usually represent\nprotein with only primary structures, limiting their ability to capture\ninteractions involving higher-level structures. Inspired by this insight, we\npropose ColdDTI, a framework attending on protein multi-level structure for\ncold-start DTI prediction. We employ hierarchical attention mechanism to mine\ninteraction between multi-level protein structures (from primary to quaternary)\nand drug structures at both local and global granularities. Then, we leverage\nmined interactions to fuse structure representations of different levels for\nfinal prediction. Our design captures biologically transferable priors,\navoiding the risk of overfitting caused by excessive reliance on representation\nlearning. Experiments on benchmark datasets demonstrate that ColdDTI\nconsistently outperforms previous methods in cold-start settings.", "AI": {"tldr": "ColdDTI\u662f\u4e00\u4e2a\u7528\u4e8e\u51b7\u542f\u52a8\u836f\u7269-\u9776\u70b9\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u6ce8\u86cb\u767d\u8d28\u591a\u7ea7\u7ed3\u6784\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u4f7f\u7528\u86cb\u767d\u8d28\u7684\u4e00\u7ea7\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u6d89\u53ca\u9ad8\u7ea7\u7ed3\u6784\uff08\u5982\u4e8c\u7ea7\u3001\u4e09\u7ea7\u3001\u56db\u7ea7\u7ed3\u6784\uff09\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u86cb\u767d\u8d28\u7684\u591a\u7ea7\u7ed3\u6784\u90fd\u4f1a\u5f71\u54cd\u836f\u7269-\u9776\u70b9\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u6316\u6398\u86cb\u767d\u8d28\u591a\u7ea7\u7ed3\u6784\uff08\u4ece\u4e00\u7ea7\u5230\u56db\u7ea7\uff09\u4e0e\u836f\u7269\u7ed3\u6784\u5728\u5c40\u90e8\u548c\u5168\u5c40\u7c92\u5ea6\u4e0a\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7136\u540e\u5229\u7528\u6316\u6398\u5230\u7684\u76f8\u4e92\u4f5c\u7528\u878d\u5408\u4e0d\u540c\u5c42\u6b21\u7684\u7ed3\u6784\u8868\u793a\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cColdDTI\u5728\u51b7\u542f\u52a8\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6355\u6349\u751f\u7269\u5b66\u4e0a\u53ef\u8f6c\u79fb\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u4f9d\u8d56\u8868\u793a\u5b66\u4e60\u5e26\u6765\u7684\u8fc7\u62df\u5408\u98ce\u9669\u3002"}}
{"id": "2510.04130", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04130", "abs": "https://arxiv.org/abs/2510.04130", "authors": ["Yang Chen", "Yitao Liang", "Zhouchen Lin"], "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization", "comment": null, "summary": "In Transformers, Position Embeddings (PEs) significantly influence Length\nGeneralization (LG) performance, yet their fundamental role remains unclear. In\nthis work, we investigate the limitations and capabilities of PEs in achieving\nLG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),\nintroducing Linear Representation Complexity (LRC) to characterize when PEs\nenable LG. Our analysis shows that PEs do not expand computational capabilities\nbut structure learned computations across positions. Extending to practical\nTransformers, we propose Sequential Representation Complexity (SRC) and\nconjecture that LG is possible if and only if SRC remains invariant across\nscales. We support this hypothesis with empirical evidence in various reasoning\ntasks. To enhance LG, we introduce Scale Hint, allowing flexible instance\nscaling, and a Learning-Based Position Embedding framework that automatically\nlearns positional relations. Our work provides theoretical insights and\npractical strategies for improving LG in Transformers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u4e2d\u4f4d\u7f6e\u5d4c\u5165\u5728\u957f\u5ea6\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u7ebf\u6027\u8868\u793a\u590d\u6742\u5ea6\u548c\u5e8f\u5217\u8868\u793a\u590d\u6742\u5ea6\u7684\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86Scale Hint\u548c\u5b66\u4e60\u578b\u4f4d\u7f6e\u5d4c\u5165\u6846\u67b6\u6765\u63d0\u5347\u957f\u5ea6\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f4d\u7f6e\u5d4c\u5165\u663e\u8457\u5f71\u54cdTransformer\u7684\u957f\u5ea6\u6cdb\u5316\u6027\u80fd\uff0c\u4f46\u5176\u6839\u672c\u4f5c\u7528\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u4f4d\u7f6e\u5d4c\u5165\u5728\u5b9e\u73b0\u957f\u5ea6\u6cdb\u5316\u4e2d\u7684\u5c40\u9650\u6027\u548c\u80fd\u529b\u3002", "method": "\u7406\u8bba\u5206\u6790\u4f4d\u7f6e\u4ec5\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5f15\u5165\u7ebf\u6027\u8868\u793a\u590d\u6742\u5ea6\uff1b\u6269\u5c55\u5230\u5b9e\u9645Transformer\uff0c\u63d0\u51fa\u5e8f\u5217\u8868\u793a\u590d\u6742\u5ea6\u5047\u8bbe\uff1b\u5f00\u53d1Scale Hint\u5b9e\u4f8b\u7f29\u653e\u548c\u5b66\u4e60\u578b\u4f4d\u7f6e\u5d4c\u5165\u6846\u67b6\u3002", "result": "\u5206\u6790\u8868\u660e\u4f4d\u7f6e\u5d4c\u5165\u4e0d\u6269\u5c55\u8ba1\u7b97\u80fd\u529b\u4f46\u7ed3\u6784\u5316\u8de8\u4f4d\u7f6e\u5b66\u4e60\u8ba1\u7b97\uff1b\u5b9e\u8bc1\u652f\u6301\u5e8f\u5217\u8868\u793a\u590d\u6742\u5ea6\u8de8\u5c3a\u5ea6\u4e0d\u53d8\u6027\u5047\u8bbe\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u957f\u5ea6\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3Transformer\u4e2d\u4f4d\u7f6e\u5d4c\u5165\u5728\u957f\u5ea6\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u6539\u8fdb\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u5e8f\u5217\u957f\u5ea6\u53d8\u5316\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04133", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04133", "abs": "https://arxiv.org/abs/2510.04133", "authors": ["Muhao Guo", "Yang Weng"], "title": "Modeling Time Series Dynamics with Fourier Ordinary Differential Equations", "comment": "8 pages, 7 figures, conference", "summary": "Neural ODEs (NODEs) have emerged as powerful tools for modeling time series\ndata, offering the flexibility to adapt to varying input scales and capture\ncomplex dynamics. However, they face significant challenges: first, their\nreliance on time-domain representations often limits their ability to capture\nlong-term dependencies and periodic structures; second, the inherent mismatch\nbetween their continuous-time formulation and the discrete nature of real-world\ndata can lead to loss of granularity and predictive accuracy. To address these\nlimitations, we propose Fourier Ordinary Differential Equations (FODEs), an\napproach that embeds the dynamics in the Fourier domain. By transforming\ntime-series data into the frequency domain using the Fast Fourier Transform\n(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive\nin the time domain. Additionally, we introduce a learnable element-wise\nfiltering mechanism that aligns continuous model outputs with discrete\nobservations, preserving granularity and enhancing accuracy. Experiments on\nvarious time series datasets demonstrate that FODEs outperform existing methods\nin terms of both accuracy and efficiency. By effectively capturing both long-\nand short-term patterns, FODEs provide a robust framework for modeling time\nseries dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5085\u91cc\u53f6\u5e38\u5fae\u5206\u65b9\u7a0b(FODEs)\uff0c\u901a\u8fc7\u5728\u5085\u91cc\u53f6\u57df\u5efa\u6a21\u6765\u89e3\u51b3\u795e\u7ecfODE\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u957f\u671f\u4f9d\u8d56\u548c\u5468\u671f\u6a21\u5f0f\u3002", "motivation": "\u795e\u7ecfODE\u5728\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u65f6\u95f4\u57df\u8868\u793a\u96be\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u548c\u5468\u671f\u7ed3\u6784\uff1b2) \u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u4e0e\u79bb\u6563\u89c2\u6d4b\u6570\u636e\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u5bfc\u81f4\u7c92\u5ea6\u635f\u5931\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u5230\u9891\u57df\uff0c\u5728\u5085\u91cc\u53f6\u57df\u4e2d\u5d4c\u5165\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u9010\u5143\u7d20\u6ee4\u6ce2\u673a\u5236\u6765\u5bf9\u9f50\u8fde\u7eed\u6a21\u578b\u8f93\u51fa\u4e0e\u79bb\u6563\u89c2\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFODEs\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u671f\u548c\u77ed\u671f\u6a21\u5f0f\u3002", "conclusion": "FODEs\u901a\u8fc7\u5085\u91cc\u53f6\u57df\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65f6\u95f4\u5e8f\u5217\u52a8\u6001\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u63ed\u793a\u5168\u5c40\u6a21\u5f0f\u548c\u5468\u671f\u884c\u4e3a\u3002"}}
{"id": "2510.04134", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04134", "abs": "https://arxiv.org/abs/2510.04134", "authors": ["Yiming Niu", "Jinliang Deng", "Yongxin Tong"], "title": "PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting", "comment": null, "summary": "Periodicity is a fundamental characteristic of time series data and has long\nplayed a central role in forecasting. Recent deep learning methods strengthen\nthe exploitation of periodicity by treating patches as basic tokens, thereby\nimproving predictive effectiveness. However, their efficiency remains a\nbottleneck due to large parameter counts and heavy computational costs. This\npaper provides, for the first time, a clear explanation of why patch-level\nprocessing is inherently inefficient, supported by strong evidence from\nreal-world data. To address these limitations, we introduce a phase perspective\nfor modeling periodicity and present an efficient yet effective solution,\nPhaseFormer. PhaseFormer features phase-wise prediction through compact phase\nembeddings and efficient cross-phase interaction enabled by a lightweight\nrouting mechanism. Extensive experiments demonstrate that PhaseFormer achieves\nstate-of-the-art performance with around 1k parameters, consistently across\nbenchmark datasets. Notably, it excels on large-scale and complex datasets,\nwhere models with comparable efficiency often struggle. This work marks a\nsignificant step toward truly efficient and effective time series forecasting.\nCode is available at this repository:\nhttps://github.com/neumyor/PhaseFormer_TSL", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PhaseFormer\uff0c\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u89c6\u89d2\u7684\u9ad8\u6548\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u76f8\u4f4d\u5d4c\u5165\u548c\u8f7b\u91cf\u7ea7\u8def\u7531\u673a\u5236\uff0c\u5728\u4ec5\u4f7f\u7528\u7ea61k\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8epatch\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u5c06patch\u4f5c\u4e3a\u57fa\u672ctoken\u589e\u5f3a\u4e86\u5468\u671f\u6027\u7684\u5229\u7528\uff0c\u4f46\u5b58\u5728\u53c2\u6570\u591a\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6548\u7387\u74f6\u9888\u3002\u672c\u6587\u9996\u6b21\u660e\u786e\u89e3\u91ca\u4e86patch\u7ea7\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u4f9b\u4e86\u771f\u5b9e\u6570\u636e\u7684\u8bc1\u636e\u652f\u6301\u3002", "method": "\u5f15\u5165\u76f8\u4f4d\u89c6\u89d2\u5efa\u6a21\u5468\u671f\u6027\uff0c\u63d0\u51faPhaseFormer\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7d27\u51d1\u7684\u76f8\u4f4d\u5d4c\u5165\u5b9e\u73b0\u76f8\u4f4d\u7ea7\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u76f8\u4f4d\u4ea4\u4e92\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPhaseFormer\u5728\u4ec5\u4f7f\u7528\u7ea61k\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u548c\u590d\u6742\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u800c\u540c\u7b49\u6548\u7387\u7684\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u80dc\u4efb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u5411\u771f\u6b63\u9ad8\u6548\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u5904\u7406\u5468\u671f\u6027\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04138", "abs": "https://arxiv.org/abs/2510.04138", "authors": ["Muhao Guo", "Haoran Li", "Yang Weng"], "title": "Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets", "comment": "8 pages; 7 figures; conference IJCNN", "summary": "Neural ordinary differential equations (NODE) have garnered significant\nattention for their design of continuous-depth neural networks and the ability\nto learn data/feature dynamics. However, for high-dimensional systems,\nestimating dynamics requires extensive calculations and suffers from high\ntruncation errors for the ODE solvers. To address the issue, one intuitive\napproach is to consider the non-trivial topological space of the data\ndistribution, i.e., a low-dimensional manifold. Existing methods often rely on\nknowledge of the manifold for projection or implicit transformation,\nrestricting the ODE solutions on the manifold. Nevertheless, such knowledge is\nusually unknown in realistic scenarios. Therefore, we propose a novel approach\nto explore the underlying manifold to restrict the ODE process. Specifically,\nwe employ a structure-preserved encoder to process data and find the underlying\ngraph to approximate the manifold. Moreover, we propose novel methods to\ncombine the NODE learning with the manifold, resulting in significant gains in\ncomputational speed and accuracy. Our experimental evaluations encompass\nmultiple datasets, where we compare the accuracy, number of function\nevaluations (NFEs), and convergence speed of our model against existing\nbaselines. Our results demonstrate superior performance, underscoring the\neffectiveness of our approach in addressing the challenges of high-dimensional\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b(NODE)\u4e0e\u5e95\u5c42\u6d41\u5f62\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u4fdd\u6301\u7f16\u7801\u5668\u53d1\u73b0\u6570\u636e\u6d41\u5f62\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7ef4\u7cfb\u7edf\u4e0b\u7684\u8ba1\u7b97\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfNODE\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u5b58\u5728\u8ba1\u7b97\u91cf\u5927\u548c\u622a\u65ad\u8bef\u5dee\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u6d41\u5f62\u77e5\u8bc6\u8fdb\u884c\u6295\u5f71\u6216\u9690\u5f0f\u53d8\u6362\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u901a\u5e38\u4e0d\u53ef\u7528\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u4fdd\u6301\u7f16\u7801\u5668\u5904\u7406\u6570\u636e\u5e76\u53d1\u73b0\u5e95\u5c42\u56fe\u6765\u8fd1\u4f3c\u6d41\u5f62\uff0c\u5c06NODE\u5b66\u4e60\u4e0e\u6d41\u5f62\u7ed3\u5408\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u6d41\u5f62\u7ea6\u675fODE\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u51fd\u6570\u8bc4\u4f30\u6b21\u6570(NFEs)\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04146", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04146", "abs": "https://arxiv.org/abs/2510.04146", "authors": ["Minseo Kim", "Coleman Hooper", "Aditya Tomar", "Chenfeng Xu", "Mehrdad Farajtabar", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models", "comment": "11 pages, 5 figures", "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance on a\nbroad range of Natural Language Processing (NLP) tasks, including document\nprocessing and coding. Autoregressive Language Models (ARMs), which generate\ntokens sequentially conditioned on all previous tokens, have been the\npredominant paradigm for LLMs. However, while these networks have achieved high\naccuracy across a range of downstream tasks, they exhibit low arithmetic\nintensity due to the inherent sequential dependency with next-token prediction.\nRecently, Diffusion Language Models (DLMs) have emerged as a promising\nalternative architecture. DLMs generate output text in parallel, breaking the\nlimitations of sequential dependency. However, the performance implications of\nDLMs relative to commonly deployed ARMs are not fully understood. In this work,\nwe present a comprehensive performance study analyzing the performance\ncharacteristics of ARMs and DLMs, using both theoretical analysis and profiling\ndata to characterize the trade-offs between these approaches. We illustrate\nthat although DLMs exhibit higher arithmetic intensity compared to ARMs because\nof their capability to utilize parallelism across sequence lengths, they fail\nto scale effectively to longer contexts. We then explore DLMs with block-wise\ndecoding, outlining how this approach allows for increased arithmetic\nintensity, while still scaling well to long contexts (similar to ARMs). We also\nshow interesting trade-offs for batched inference, where we find that ARMs\nexhibit superior throughput, as they benefit more from parallelism across\nsequences in the batch. Finally, we highlight opportunities for accelerating\nDLM inference, and, in particular, highlight the importance of reducing the\nnumber of sampling steps for allowing open-source DLMs to provide improved\nlatency relative to ARMs.", "AI": {"tldr": "\u672c\u6587\u5bf9\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b(ARMs)\u548c\u6269\u6563\u8bed\u8a00\u6a21\u578b(DLMs)\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6027\u80fd\u5bf9\u6bd4\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u67b6\u6784\u5728\u7b97\u672f\u5f3a\u5ea6\u3001\u4e0a\u4e0b\u6587\u6269\u5c55\u3001\u6279\u91cf\u63a8\u7406\u7b49\u65b9\u9762\u7684\u6743\u8861\u3002", "motivation": "\u867d\u7136\u6269\u6563\u8bed\u8a00\u6a21\u578b(DLMs)\u4f5c\u4e3a\u5e76\u884c\u751f\u6210\u6587\u672c\u7684\u66ff\u4ee3\u67b6\u6784\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u76f8\u5bf9\u4e8e\u4e3b\u6d41\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b(ARMs)\u7684\u6027\u80fd\u5f71\u54cd\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u6027\u80fd\u5206\u6790\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u548c\u6027\u80fd\u5256\u6790\u6570\u636e\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5bf9\u6bd4ARMs\u548cDLMs\u7684\u6027\u80fd\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u4e86\u5757\u72b6\u89e3\u7801\u7684DLMs\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0DLMs\u7531\u4e8e\u80fd\u5728\u5e8f\u5217\u957f\u5ea6\u4e0a\u5229\u7528\u5e76\u884c\u6027\u800c\u5177\u6709\u66f4\u9ad8\u7684\u7b97\u672f\u5f3a\u5ea6\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1b\u5757\u72b6\u89e3\u7801\u7684DLMs\u80fd\u63d0\u9ad8\u7b97\u672f\u5f3a\u5ea6\u5e76\u4fdd\u6301\u826f\u597d\u6269\u5c55\u6027\uff1b\u5728\u6279\u91cf\u63a8\u7406\u4e2dARMs\u5177\u6709\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u5bf9\u4e8e\u63d0\u5347\u5f00\u6e90DLMs\u7684\u5ef6\u8fdf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cDLMs\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u66ff\u4ee3ARMs\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2510.04189", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04189", "abs": "https://arxiv.org/abs/2510.04189", "authors": ["Prashansa Panda", "Shalabh Bhatnagar"], "title": "Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity", "comment": null, "summary": "Recent studies have increasingly focused on non-asymptotic convergence\nanalyses for actor-critic (AC) algorithms. One such effort introduced a\ntwo-timescale critic-actor algorithm for the discounted cost setting using a\ntabular representation, where the usual roles of the actor and critic are\nreversed. However, only asymptotic convergence was established there.\nSubsequently, both asymptotic and non-asymptotic analyses of the critic-actor\nalgorithm with linear function approximation were conducted. In our work, we\nintroduce the first natural critic-actor algorithm with function approximation\nfor the long-run average cost setting and under inequality constraints. We\nprovide the non-asymptotic convergence guarantees for this algorithm. Our\nanalysis establishes optimal learning rates and we also propose a modification\nto enhance sample complexity. We further show the results of experiments on\nthree different Safety-Gym environments where our algorithm is found to be\ncompetitive in comparison with other well known algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u957f\u671f\u5e73\u5747\u6210\u672c\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u81ea\u7136critic-actor\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u975e\u6e10\u8fdb\u6536\u655b\u4fdd\u8bc1\uff0c\u5728Safety-Gym\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65e0\u7ea6\u675f\u7684\u6298\u6263\u6210\u672c\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u9488\u5bf9\u957f\u671f\u5e73\u5747\u6210\u672c\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684critic-actor\u7b97\u6cd5\u53ca\u5176\u975e\u6e10\u8fdb\u6536\u655b\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u51fd\u6570\u903c\u8fd1\u7684\u81ea\u7136critic-actor\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u957f\u671f\u5e73\u5747\u6210\u672c\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7248\u672c\u4ee5\u63d0\u5347\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u4f18\u5b66\u4e60\u7387\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u7684Safety-Gym\u73af\u5883\u4e2d\u4e0e\u5176\u4ed6\u77e5\u540d\u7b97\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u586b\u8865\u4e86\u957f\u671f\u5e73\u5747\u6210\u672c\u548c\u7ea6\u675f\u8bbe\u7f6e\u4e0bcritic-actor\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u5b9e\u8df5\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.04202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04202", "abs": "https://arxiv.org/abs/2510.04202", "authors": ["Haiquan Qiu", "You Wu", "Yingjie Tan", "Yaqing Wang", "Quanming Yao"], "title": "Spectral Alignment as Predictor of Loss Explosion in Neural Network Training", "comment": "18 pages, 8 figures", "summary": "Loss explosions in training deep neural networks can nullify multi-million\ndollar training runs. Conventional monitoring metrics like weight and gradient\nnorms are often lagging and ambiguous predictors, as their values vary\ndramatically across different models and even between layers of the same model,\nmaking it difficult to establish a unified standard for detecting impending\nfailure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded\nmetric that monitors the distributional alignment between layer inputs and the\nprincipal singular vectors of weight matrices. We show that a collapse in the\nsign diversity of this alignment is a powerful early predictor of\nrepresentational collapse and training divergence. Empirical results on\nlanguage models demonstrate that monitoring the SA distribution provides a\nsignificantly earlier and clearer warning of loss explosions than traditional\nscalar metrics. SA's low computational overhead makes it a practical tool for\nsafeguarding model training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8c31\u5bf9\u9f50\uff08SA\uff09\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u76d1\u6d4b\u5c42\u8f93\u5165\u4e0e\u6743\u91cd\u77e9\u9635\u4e3b\u5947\u5f02\u5411\u91cf\u4e4b\u95f4\u7684\u5206\u5e03\u5bf9\u9f50\u6765\u9884\u6d4b\u8bad\u7ec3\u53d1\u6563\uff0c\u6bd4\u4f20\u7edf\u6807\u91cf\u6307\u6807\u66f4\u65e9\u66f4\u6e05\u6670\u5730\u9884\u8b66\u635f\u5931\u7206\u70b8\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u7206\u70b8\u53ef\u80fd\u4f7f\u6570\u767e\u4e07\u7f8e\u5143\u7684\u8bad\u7ec3\u8fd0\u884c\u5931\u6548\uff0c\u800c\u4f20\u7edf\u7684\u6743\u91cd\u548c\u68af\u5ea6\u8303\u6570\u7b49\u76d1\u63a7\u6307\u6807\u5f80\u5f80\u6ede\u540e\u4e14\u6a21\u7cca\uff0c\u96be\u4ee5\u5efa\u7acb\u7edf\u4e00\u7684\u5931\u8d25\u68c0\u6d4b\u6807\u51c6\u3002", "method": "\u5f15\u5165\u8c31\u5bf9\u9f50\uff08SA\uff09\u6307\u6807\uff0c\u76d1\u6d4b\u5c42\u8f93\u5165\u4e0e\u6743\u91cd\u77e9\u9635\u4e3b\u5947\u5f02\u5411\u91cf\u4e4b\u95f4\u7684\u5206\u5e03\u5bf9\u9f50\uff0c\u5f53\u8fd9\u79cd\u5bf9\u9f50\u7684\u7b26\u53f7\u591a\u6837\u6027\u5d29\u6e83\u65f6\uff0c\u53ef\u4f5c\u4e3a\u8868\u793a\u5d29\u6e83\u548c\u8bad\u7ec3\u53d1\u6563\u7684\u65e9\u671f\u9884\u6d4b\u5668\u3002", "result": "\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u76d1\u6d4bSA\u5206\u5e03\u6bd4\u4f20\u7edf\u6807\u91cf\u6307\u6807\u80fd\u663e\u8457\u66f4\u65e9\u3001\u66f4\u6e05\u6670\u5730\u9884\u8b66\u635f\u5931\u7206\u70b8\u3002", "conclusion": "SA\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u662f\u4fdd\u62a4\u6a21\u578b\u8bad\u7ec3\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u4e3a\u8bad\u7ec3\u76d1\u63a7\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u673a\u5236\u3002"}}
{"id": "2510.04203", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04203", "abs": "https://arxiv.org/abs/2510.04203", "authors": ["Aayushya Agarwal", "Larry Pileggi", "Gauri Joshi"], "title": "Adaptive Federated Learning via Dynamical System Model", "comment": null, "summary": "Hyperparameter selection is critical for stable and efficient convergence of\nheterogeneous federated learning, where clients differ in computational\ncapabilities, and data distributions are non-IID. Tuning hyperparameters is a\nmanual and computationally expensive process as the hyperparameter space grows\ncombinatorially with the number of clients. To address this, we introduce an\nend-to-end adaptive federated learning method in which both clients and central\nagents adaptively select their local learning rates and momentum parameters.\nOur approach models federated learning as a dynamical system, allowing us to\ndraw on principles from numerical simulation and physical design. Through this\nperspective, selecting momentum parameters equates to critically damping the\nsystem for fast, stable convergence, while learning rates for clients and\ncentral servers are adaptively selected to satisfy accuracy properties from\nnumerical simulation. The result is an adaptive, momentum-based federated\nlearning algorithm in which the learning rates for clients and servers are\ndynamically adjusted and controlled by a single, global hyperparameter. By\ndesigning a fully integrated solution for both adaptive client updates and\ncentral agent aggregation, our method is capable of handling key challenges of\nheterogeneous federated learning, including objective inconsistency and client\ndrift. Importantly, our approach achieves fast convergence while being\ninsensitive to the choice of the global hyperparameter, making it well-suited\nfor rapid prototyping and scalable deployment. Compared to state-of-the-art\nadaptive methods, our framework is shown to deliver superior convergence for\nheterogeneous federated learning while eliminating the need for hyperparameter\ntuning both client and server updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8054\u90a6\u5b66\u4e60\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5ba2\u6237\u7aef\u548c\u4e2d\u592e\u670d\u52a1\u5668\u7684\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u53c2\u6570\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u9009\u62e9\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u624b\u52a8\u8c03\u53c2\u8fc7\u7a0b\u8017\u65f6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5ba2\u6237\u7aef\u8ba1\u7b97\u80fd\u529b\u4e0d\u540c\u3001\u6570\u636e\u5206\u5e03\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5c06\u8054\u90a6\u5b66\u4e60\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u501f\u9274\u6570\u503c\u6a21\u62df\u548c\u7269\u7406\u8bbe\u8ba1\u539f\u7406\uff0c\u901a\u8fc7\u4e34\u754c\u963b\u5c3c\u9009\u62e9\u52a8\u91cf\u53c2\u6570\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u6536\u655b\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5b66\u4e60\u7387\u6ee1\u8db3\u6570\u503c\u6a21\u62df\u7cbe\u5ea6\u8981\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff08\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\u548c\u5ba2\u6237\u7aef\u6f02\u79fb\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u81ea\u9002\u5e94\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u6536\u655b\u6027\u80fd\uff0c\u4e14\u5bf9\u5168\u5c40\u8d85\u53c2\u6570\u9009\u62e9\u4e0d\u654f\u611f\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6d88\u9664\u4e86\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u66f4\u65b0\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\uff0c\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u5728\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04205", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "math.OC", "68T07, 41A15, 52B11", "F.2.2; G.1.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.04205", "abs": "https://arxiv.org/abs/2510.04205", "authors": ["Di Zhang"], "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression", "comment": "10", "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\ntraditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability\nand a strong mathematical foundation. However, their parameter efficiency\nremains a significant challenge for practical deployment. This paper introduces\nPolyKAN, a novel theoretical framework for KAN compression that provides formal\nguarantees on both model size reduction and approximation error. By leveraging\nthe inherent piecewise polynomial structure of KANs, we formulate the\ncompression problem as one of optimal polyhedral region merging. We establish a\nrigorous polyhedral characterization of KANs, develop a complete theory of\n$\\epsilon$-equivalent compression, and design an optimal dynamic programming\nalgorithm that guarantees minimal compression under specified error bounds. Our\ntheoretical analysis demonstrates that PolyKAN achieves provably minimal\ncompression while maintaining strict error control, with polynomial-time\ncomplexity in all network parameters. The framework provides the first formal\nfoundation for KAN compression with mathematical guarantees, opening new\ndirections for efficient deployment of interpretable neural architectures.", "AI": {"tldr": "PolyKAN\u662f\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u4e3aKolmogorov-Arnold Networks\uff08KANs\uff09\u63d0\u4f9b\u538b\u7f29\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u901a\u8fc7\u591a\u9762\u4f53\u533a\u57df\u5408\u5e76\u5b9e\u73b0\u6700\u5c0f\u5316\u538b\u7f29\u5e76\u63a7\u5236\u8bef\u5dee\u3002", "motivation": "KANs\u867d\u7136\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6570\u5b66\u57fa\u7840\uff0c\u4f46\u53c2\u6570\u6548\u7387\u4f4e\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u5229\u7528KANs\u56fa\u6709\u7684\u5206\u6bb5\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u5c06\u538b\u7f29\u95ee\u9898\u5efa\u6a21\u4e3a\u6700\u4f18\u591a\u9762\u4f53\u533a\u57df\u5408\u5e76\uff0c\u5f00\u53d1\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u4fdd\u8bc1\u5728\u6307\u5b9a\u8bef\u5dee\u8303\u56f4\u5185\u5b9e\u73b0\u6700\u5c0f\u538b\u7f29\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660ePolyKAN\u5728\u4e25\u683c\u8bef\u5dee\u63a7\u5236\u4e0b\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u6700\u5c0f\u538b\u7f29\uff0c\u4e14\u5728\u6240\u6709\u7f51\u7edc\u53c2\u6570\u4e0a\u5177\u6709\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aKAN\u538b\u7f29\u63d0\u4f9b\u4e86\u9996\u4e2a\u5177\u6709\u6570\u5b66\u4fdd\u8bc1\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u4e3a\u53ef\u89e3\u91ca\u795e\u7ecf\u67b6\u6784\u7684\u9ad8\u6548\u90e8\u7f72\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.04212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04212", "abs": "https://arxiv.org/abs/2510.04212", "authors": ["Haiquan Qiu", "Quanming Yao"], "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention", "comment": "19 pages, 10 figures", "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosions. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u673a\u5236\u6027\u5730\u89e3\u91ca\u4e86\u5728\u4f4e\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u4f7f\u7528Flash Attention\u8bad\u7ec3\u65f6\u5bfc\u81f4\u707e\u96be\u6027\u635f\u5931\u7206\u70b8\u7684\u957f\u671f\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4f4e\u79e9\u8868\u793a\u548c\u4f4e\u7cbe\u5ea6\u7b97\u672f\u7684\u504f\u7f6e\u820d\u5165\u8bef\u5dee\u76f8\u4e92\u4f5c\u7528\u7684\u6076\u6027\u5faa\u73af\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u4fee\u6539\u65b9\u6848\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u8ffd\u6c42\u8ba1\u7b97\u6548\u7387\u63a8\u52a8\u4e86\u5728\u8bad\u7ec3transformer\u6a21\u578b\u65f6\u91c7\u7528\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u4f46\u8fd9\u79cd\u8fdb\u6b65\u5e38\u5e38\u53d7\u5230\u81ed\u540d\u662d\u8457\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u963b\u788d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u4f4e\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u4f7f\u7528Flash Attention\u8bad\u7ec3\u65f6\u5bfc\u81f4\u707e\u96be\u6027\u635f\u5931\u7206\u70b8\u7684\u957f\u671f\u672a\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u5206\u6790\u63ed\u793a\u4e86\u5931\u8d25\u673a\u5236\uff0c\u5e76\u5f15\u5165\u5bf9Flash Attention\u7684\u6700\u5c0f\u4fee\u6539\u6765\u51cf\u8f7b\u820d\u5165\u8bef\u5dee\u7684\u504f\u7f6e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u673a\u5236\u6027\u5206\u6790\u8bc6\u522b\u95ee\u9898\u6839\u6e90\uff0c\u7136\u540e\u8bbe\u8ba1\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5931\u8d25\u4e0d\u662f\u968f\u673a\u4f2a\u5f71\uff0c\u800c\u662f\u7531\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u76f8\u4f3c\u4f4e\u79e9\u8868\u793a\u7684\u51fa\u73b0\u4e0e\u4f4e\u7cbe\u5ea6\u7b97\u672f\u56fa\u6709\u7684\u504f\u7f6e\u820d\u5165\u8bef\u5dee\u7684\u590d\u5408\u6548\u5e94\u5171\u540c\u5f15\u8d77\u7684\u3002\u8fd9\u4e9b\u56e0\u7d20\u521b\u5efa\u4e86\u8bef\u5dee\u7d2f\u79ef\u7684\u6076\u6027\u5faa\u73af\uff0c\u6700\u7ec8\u7834\u574f\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u4fee\u6539\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u8bc1\u5b9e\u4e86\u5206\u6790\u7684\u6b63\u786e\u6027\uff0c\u5e76\u4e3a\u8fd9\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7406\u89e3\u5e95\u5c42\u673a\u5236\u53ef\u4ee5\u8bbe\u8ba1\u6709\u6548\u7684\u7a33\u5b9a\u7b56\u7565\u3002"}}
{"id": "2510.04217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04217", "abs": "https://arxiv.org/abs/2510.04217", "authors": ["Chenlu Ding", "Jiancan Wu", "Leheng Sheng", "Fan Zhang", "Yancheng Yuan", "Xiang Wang", "Xiangnan He"], "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities across vision-language tasks, yet their large-scale deployment\nraises pressing concerns about memorized private data, outdated knowledge, and\nharmful content. Existing unlearning approaches for MLLMs typically adapt\ntraining-based strategies such as gradient ascent or preference optimization,\nbut these methods are computationally expensive, irreversible, and often\ndistort retained knowledge. In this work, we propose MLLMEraser, an\ninput-aware, training-free framework for test-time unlearning. Our approach\nleverages activation steering to enable dynamic knowledge erasure without\nparameter updates. Specifically, we construct a multimodal erasure direction by\ncontrasting adversarially perturbed, knowledge-recall image-text pairs with\nknowledge-erasure counterparts, capturing both textual and visual\ndiscrepancies. To prevent unnecessary interference, we further design an\ninput-aware steering mechanism that adaptively determines when and how the\nerasure direction should be applied, preserving utility on retained knowledge\nwhile enforcing forgetting on designated content. Experiments on LLaVA-1.5 and\nQwen-2.5-VL demonstrate that MLLMEraser consistently outperforms\nstate-of-the-art MLLM unlearning baselines, achieving stronger forgetting\nperformance with lower computational cost and minimal utility degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e86MLLMEraser\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8f93\u5165\u611f\u77e5\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u5b9e\u73b0\u52a8\u6001\u77e5\u8bc6\u64e6\u9664\uff0c\u89e3\u51b3\u4e86MLLMs\u4e2d\u9690\u79c1\u6570\u636e\u3001\u8fc7\u65f6\u77e5\u8bc6\u548c\u6709\u5bb3\u5185\u5bb9\u7684\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bb0\u5fc6\u9690\u79c1\u6570\u636e\u3001\u8fc7\u65f6\u77e5\u8bc6\u548c\u6709\u5bb3\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0d\u53ef\u9006\u4e14\u4f1a\u626d\u66f2\u4fdd\u7559\u77e5\u8bc6\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u6297\u6027\u6270\u52a8\u7684\u77e5\u8bc6\u56de\u5fc6\u548c\u77e5\u8bc6\u64e6\u9664\u56fe\u50cf-\u6587\u672c\u5bf9\u6784\u5efa\u591a\u6a21\u6001\u64e6\u9664\u65b9\u5411\uff0c\u5e76\u8bbe\u8ba1\u8f93\u5165\u611f\u77e5\u5f15\u5bfc\u673a\u5236\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u5e94\u7528\u64e6\u9664\u65b9\u5411\u3002", "result": "\u5728LLaVA-1.5\u548cQwen-2.5-VL\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLLMEraser\u5728\u9057\u5fd8\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u4e14\u6548\u7528\u9000\u5316\u6700\u5c0f\u3002", "conclusion": "MLLMEraser\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u64e6\u9664\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4fdd\u7559\u77e5\u8bc6\u6548\u7528\u7684\u540c\u65f6\u6709\u6548\u64e6\u9664\u6307\u5b9a\u5185\u5bb9\u3002"}}
{"id": "2510.04233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04233", "abs": "https://arxiv.org/abs/2510.04233", "authors": ["Kai Yang", "Yuqi Huang", "Junheng Tao", "Wanyu Wang", "Qitian Wu"], "title": "Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling", "comment": null, "summary": "Modeling 3D dynamics is a fundamental problem in multi-body systems across\nscientific and engineering domains and has important practical implications in\ntrajectory prediction and simulation. While recent GNN-based approaches have\nachieved strong performance by enforcing geometric symmetries, encoding\nhigh-order features or incorporating neural-ODE mechanics, they typically\ndepend on explicitly observed structures and inherently fail to capture the\nunobserved interactions that are crucial to complex physical behaviors and\ndynamics mechanism. In this paper, we propose PAINET, a principled\nSE(3)-equivariant neural architecture for learning all-pair interactions in\nmulti-body systems. The model comprises: (1) a novel physics-inspired attention\nnetwork derived from the minimization trajectory of an energy function, and (2)\na parallel decoder that preserves equivariance while enabling efficient\ninference. Empirical results on diverse real-world benchmarks, including human\nmotion capture, molecular dynamics, and large-scale protein simulations, show\nthat PAINET consistently outperforms recently proposed models, yielding 4.7% to\n41.5% error reductions in 3D dynamics prediction with comparable computation\ncosts in terms of time and memory.", "AI": {"tldr": "PAINET\u662f\u4e00\u4e2aSE(3)-\u7b49\u53d8\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u5168\u5bf9\u76f8\u4e92\u4f5c\u7528\uff0c\u57283D\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u89c2\u6d4b\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7269\u7406\u884c\u4e3a\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u672a\u89c2\u6d4b\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u6700\u5c0f\u5316\u8f68\u8ff9\u7684\u7269\u7406\u542f\u53d1\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u4fdd\u6301\u7b49\u53d8\u6027\u7684\u5e76\u884c\u89e3\u7801\u5668\u3002", "result": "\u5728\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u3001\u5206\u5b50\u52a8\u529b\u5b66\u548c\u86cb\u767d\u8d28\u6a21\u62df\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6700\u65b0\u6a21\u578b\u5b9e\u73b04.7%\u523041.5%\u7684\u8bef\u5dee\u964d\u4f4e\u3002", "conclusion": "PAINET\u901a\u8fc7\u6355\u6349\u672a\u89c2\u6d4b\u76f8\u4e92\u4f5c\u7528\uff0c\u57283D\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8ba1\u7b97\u6210\u672c\u76f8\u5f53\u3002"}}
{"id": "2510.04237", "categories": ["cs.LG", "68T05, 68Q32, 62L20"], "pdf": "https://arxiv.org/pdf/2510.04237", "abs": "https://arxiv.org/abs/2510.04237", "authors": ["Jinhui Bai", "Andreas Christmann", "Lei Shi"], "title": "Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions", "comment": "54 pages, 20 figures", "summary": "In this paper, we propose a novel kernel stochastic gradient descent (SGD)\nalgorithm for large-scale supervised learning with general losses. Compared to\ntraditional kernel SGD, our algorithm improves efficiency and scalability\nthrough an innovative regularization strategy. By leveraging the infinite\nseries expansion of spherical radial basis functions, this strategy projects\nthe stochastic gradient onto a finite-dimensional hypothesis space, which is\nadaptively scaled according to the bias-variance trade-off, thereby enhancing\ngeneralization performance. Based on a new estimation of the spectral structure\nof the kernel-induced covariance operator, we develop an analytical framework\nthat unifies optimization and generalization analyses. We prove that both the\nlast iterate and the suffix average converge at minimax-optimal rates, and we\nfurther establish optimal strong convergence in the reproducing kernel Hilbert\nspace. Our framework accommodates a broad class of classical loss functions,\nincluding least-squares, Huber, and logistic losses. Moreover, the proposed\nalgorithm significantly reduces computational complexity and achieves optimal\nstorage complexity by incorporating coordinate-wise updates from linear SGD,\nthereby avoiding the costly pairwise operations typical of kernel SGD and\nenabling efficient processing of streaming data. Finally, extensive numerical\nexperiments demonstrate the efficiency of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6838\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\u63d0\u9ad8\u5927\u89c4\u6a21\u76d1\u7763\u5b66\u4e60\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5728\u4f18\u5316\u548c\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u8fbe\u5230\u6700\u4f18\u6536\u655b\u7387\u3002", "motivation": "\u4f20\u7edf\u6838SGD\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6838\u65b9\u6cd5\u4f18\u52bf\u53c8\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7403\u9762\u5f84\u5411\u57fa\u51fd\u6570\u7684\u65e0\u7a77\u7ea7\u6570\u5c55\u5f00\uff0c\u5c06\u968f\u673a\u68af\u5ea6\u6295\u5f71\u5230\u6709\u9650\u7ef4\u5047\u8bbe\u7a7a\u95f4\uff0c\u5e76\u57fa\u4e8e\u6838\u8bf1\u5bfc\u534f\u65b9\u5dee\u7b97\u5b50\u7684\u8c31\u7ed3\u6784\u4f30\u8ba1\u5efa\u7acb\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u7ebf\u6027SGD\u7684\u5750\u6807\u66f4\u65b0\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u540e\u8fed\u4ee3\u548c\u540e\u7f00\u5e73\u5747\u90fd\u4ee5\u6781\u5c0f\u6781\u5927\u6700\u4f18\u901f\u7387\u6536\u655b\uff0c\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5efa\u7acb\u6700\u4f18\u5f3a\u6536\u655b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u590d\u6742\u5ea6\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u4fdd\u6301\u6838\u65b9\u6cd5\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7ecf\u5178\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u6d41\u5f0f\u6570\u636e\u3002"}}
{"id": "2510.04241", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04241", "abs": "https://arxiv.org/abs/2510.04241", "authors": ["Seong Jin Ahn", "Myoung-Ho Kim"], "title": "Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs", "comment": null, "summary": "For large-scale applications, there is growing interest in replacing Graph\nNeural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via\nknowledge distillation. However, distilling GNNs for self-supervised graph\nrepresentation learning into MLPs is more challenging. This is because the\nperformance of self-supervised learning is more related to the model's\ninductive bias than supervised learning. This motivates us to design a new\ndistillation method to bridge a huge capacity gap between GNNs and MLPs in\nself-supervised graph representation learning. In this paper, we propose\n\\textbf{D}iffusion-\\textbf{A}ssisted \\textbf{D}istillation for\n\\textbf{S}elf-supervised \\textbf{G}raph representation learning with\n\\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion\nmodel as a teacher assistant to better distill the knowledge from the teacher\nGNN into the student MLP. This approach enhances the generalizability and\nrobustness of MLPs in self-supervised graph representation learning. Extensive\nexperiments demonstrate that DAD-SGM effectively distills the knowledge of\nself-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation\nmethods. Our implementation is available at\nhttps://github.com/SeongJinAhn/DAD-SGM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAD-SGM\u7684\u65b0\u84b8\u998f\u65b9\u6cd5\uff0c\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u52a9\u624b\uff0c\u5c06\u81ea\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\u4e2d\uff0c\u4ee5\u89e3\u51b3\u4e24\u8005\u5728\u5bb9\u91cf\u4e0a\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\uff0c\u7528\u8f7b\u91cf\u7ea7MLP\u66ff\u4ee3GNN\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u4f46\u5728\u81ea\u76d1\u7763\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u5c06GNN\u84b8\u998f\u5230MLP\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u66f4\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u52a9\u624b\uff0c\u5e2e\u52a9\u5c06\u6559\u5e08GNN\u7684\u77e5\u8bc6\u66f4\u597d\u5730\u84b8\u998f\u5230\u5b66\u751fMLP\u4e2d\uff0c\u589e\u5f3aMLP\u5728\u81ea\u76d1\u7763\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDAD-SGM\u76f8\u6bd4\u6700\u5148\u8fdb\u7684GNN-to-MLP\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u66f4\u6709\u6548\u5730\u84b8\u998f\u81ea\u76d1\u7763GNN\u7684\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u56fe\u8868\u793a\u5b66\u4e60\u4e2dGNN\u4e0eMLP\u4e4b\u95f4\u7684\u5bb9\u91cf\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04263", "abs": "https://arxiv.org/abs/2510.04263", "authors": ["Joseph Ramsey", "Bryan Andrews"], "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing", "comment": "30 pages, 23 figures, 6 tables", "summary": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u5206\u6570\u5f15\u5bfc\u7684\u6df7\u5408\u7b56\u7565\u56e0\u679c\u641c\u7d22\u7b97\u6cd5\uff0c\u6539\u8fdbFCI\u7b97\u6cd5\u5728\u5b58\u5728\u9690\u53d8\u91cf\u6216\u9009\u62e9\u504f\u5dee\u65f6\u7684\u6027\u80fd\uff0c\u5305\u62ecBOSS-FCI\u3001GRaSP-FCI\u3001FCIT\u548cLV-Dumb\u7b49\u65b9\u6cd5\u3002", "motivation": "FCI\u7b97\u6cd5\u5728\u5904\u7406\u9690\u53d8\u91cf\u6216\u9009\u62e9\u504f\u5dee\u65f6\u9700\u8981\u8fdb\u884c\u5927\u91cf\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u5bfc\u81f4\u865a\u5047\u72ec\u7acb\u6027\u58f0\u660e\u3001\u591a\u4f59\u6216\u7f3a\u5931\u8fb9\u4ee5\u53ca\u4e0d\u53ef\u9760\u7684\u65b9\u5411\u786e\u5b9a\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86BOSS-FCI\u548cGRaSP-FCI\u4f5c\u4e3aGFCI\u7684\u53d8\u4f53\uff0c\u7528BOSS\u6216GRaSP\u66ff\u4ee3FGES\uff1b\u63d0\u51faFCIT\u65b9\u6cd5\u7528BOSS\u5f15\u5bfc\u7684\u5b9a\u5411\u6d4b\u8bd5\u66ff\u4ee3\u7a77\u4e3e\u6d4b\u8bd5\uff1b\u8fd8\u63d0\u51fa\u4e86LV-Dumb\u542f\u53d1\u5f0f\u65b9\u6cd5\u76f4\u63a5\u8fd4\u56deBOSS DAG\u7684PAG\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u8868\u660e\uff0cBOSS-FCI\u548cGRaSP-FCI\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7ebf\uff0cFCIT\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u9760\u6027\uff0cLV-Dumb\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u7a81\u51fa\u4e86\u5206\u6570\u5f15\u5bfc\u548c\u5b9a\u5411\u7b56\u7565\u5728\u53ef\u6269\u5c55\u9690\u53d8\u91cf\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.04273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04273", "abs": "https://arxiv.org/abs/2510.04273", "authors": ["Paul Strang", "Zacharie Al\u00e8s", "C\u00f4me Bissuel", "Olivier Juan", "Safia Kedad-Sidhoum", "Emmanuel Rachelson"], "title": "Influence branching for learning to solve mixed-integer programs online", "comment": "11 pages", "summary": "On the occasion of the 20th Mixed Integer Program Workshop's computational\ncompetition, this work introduces a new approach for learning to solve MIPs\nonline. Influence branching, a new graph-oriented variable selection strategy,\nis applied throughout the first iterations of the branch and bound algorithm.\nThis branching heuristic is optimized online with Thompson sampling, which\nranks the best graph representations of MIP's structure according to\ncomputational speed up over SCIP. We achieve results comparable to state of the\nart online learning methods. Moreover, our results indicate that our method\ngeneralizes well to more general online frameworks, where variations in\nconstraint matrix, constraint vector and objective coefficients can all occur\nand where more samples are available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f71\u54cd\u5206\u652f\u548cThompson\u91c7\u6837\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u6df7\u5408\u6574\u6570\u89c4\u5212\u7684\u6c42\u89e3\u8fc7\u7a0b\uff0c\u5728\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u7684\u524d\u51e0\u6b21\u8fed\u4ee3\u4e2d\u5e94\u7528\u56fe\u5bfc\u5411\u7684\u53d8\u91cf\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u4e3a\u7b2c20\u5c4a\u6df7\u5408\u6574\u6570\u89c4\u5212\u7814\u8ba8\u4f1a\u8ba1\u7b97\u7ade\u8d5b\u5f00\u53d1\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8MIP\u6c42\u89e3\u6548\u7387\u5e76\u9002\u5e94\u66f4\u4e00\u822c\u7684\u5728\u7ebf\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5f71\u54cd\u5206\u652f\u4f5c\u4e3a\u65b0\u7684\u56fe\u5bfc\u5411\u53d8\u91cf\u9009\u62e9\u7b56\u7565\uff0c\u5728\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u524d\u51e0\u6b21\u8fed\u4ee3\u4e2d\u5e94\u7528\uff0c\u5e76\u901a\u8fc7Thompson\u91c7\u6837\u5728\u7ebf\u4f18\u5316\u5206\u652f\u542f\u53d1\u5f0f\uff0c\u6839\u636e\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u5bf9MIP\u7ed3\u6784\u7684\u6700\u4f73\u56fe\u8868\u793a\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u66f4\u4e00\u822c\u7684\u5728\u7ebf\u6846\u67b6\uff0c\u5305\u62ec\u7ea6\u675f\u77e9\u9635\u3001\u7ea6\u675f\u5411\u91cf\u548c\u76ee\u6807\u7cfb\u6570\u7684\u53d8\u5316\uff0c\u4e14\u5728\u6709\u66f4\u591a\u6837\u672c\u65f6\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5f71\u54cd\u5206\u652f\u7ed3\u5408Thompson\u91c7\u6837\u7684\u65b9\u6cd5\u5728MIP\u5728\u7ebf\u6c42\u89e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u53c2\u6570\u53d8\u5316\u7684\u5728\u7ebf\u573a\u666f\u3002"}}
{"id": "2510.04280", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04280", "abs": "https://arxiv.org/abs/2510.04280", "authors": ["\u00c1lvaro Serra-Gomez", "Daniel Jarne Ornia", "Dhruva Tirumala", "Thomas Moerland"], "title": "A KL-regularization framework for learning to plan with adaptive priors", "comment": "Preprint", "summary": "Effective exploration remains a central challenge in model-based\nreinforcement learning (MBRL), particularly in high-dimensional continuous\ncontrol tasks where sample efficiency is crucial. A prominent line of recent\nwork leverages learned policies as proposal distributions for Model-Predictive\nPath Integral (MPPI) planning. Initial approaches update the sampling policy\nindependently of the planner distribution, typically maximizing a learned value\nfunction with deterministic policy gradient and entropy regularization.\nHowever, because the states encountered during training depend on the MPPI\nplanner, aligning the sampling policy with the planner improves the accuracy of\nvalue estimation and long-term performance. To this end, recent methods update\nthe sampling policy by minimizing KL divergence to the planner distribution or\nby introducing planner-guided regularization into the policy update. In this\nwork, we unify these MPPI-based reinforcement learning methods under a single\nframework by introducing Policy Optimization-Model Predictive Control (PO-MPC),\na family of KL-regularized MBRL methods that integrate the planner's action\ndistribution as a prior in policy optimization. By aligning the learned policy\nwith the planner's behavior, PO-MPC allows more flexibility in the policy\nupdates to trade off Return maximization and KL divergence minimization. We\nclarify how prior approaches emerge as special cases of this family, and we\nexplore previously unstudied variations. Our experiments show that these\nextended configurations yield significant performance improvements, advancing\nthe state of the art in MPPI-based RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86PO-MPC\u6846\u67b6\uff0c\u5c06\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7edf\u4e00\u4e3aKL\u6b63\u5219\u5316\u7684\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c4\u5212\u5668\u7684\u52a8\u4f5c\u5206\u5e03\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u5148\u9a8c\u6765\u5bf9\u9f50\u5b66\u4e60\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\u4e2d\u6709\u6548\u63a2\u7d22\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u66f4\u65b0\u91c7\u6837\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u5206\u5e03\uff0c\u4f46\u8bad\u7ec3\u72b6\u6001\u4f9d\u8d56MPPI\u89c4\u5212\u5668\uff0c\u5bf9\u9f50\u91c7\u6837\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u53ef\u4ee5\u63d0\u9ad8\u4ef7\u503c\u4f30\u8ba1\u51c6\u786e\u6027\u548c\u957f\u671f\u6027\u80fd\u3002", "method": "\u5f15\u5165PO-MPC\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2aKL\u6b63\u5219\u5316\u7684\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bb6\u65cf\uff0c\u5c06\u89c4\u5212\u5668\u7684\u52a8\u4f5c\u5206\u5e03\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u5148\u9a8c\u3002\u901a\u8fc7\u6700\u5c0f\u5316KL\u6563\u5ea6\u6765\u5bf9\u9f50\u5b66\u4e60\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u884c\u4e3a\uff0c\u5728\u7b56\u7565\u66f4\u65b0\u4e2d\u6743\u8861\u56de\u62a5\u6700\u5927\u5316\u548cKL\u6563\u5ea6\u6700\u5c0f\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u5c55\u914d\u7f6e\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63a8\u8fdb\u4e86\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "PO-MPC\u6846\u67b6\u7edf\u4e00\u4e86\u73b0\u6709\u7684MPPI\u57fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5148\u524d\u65b9\u6cd5\u4f5c\u4e3a\u8be5\u5bb6\u65cf\u7279\u4f8b\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u7d22\u4e86\u672a\u7814\u7a76\u8fc7\u7684\u53d8\u4f53\uff0c\u4e3a\u6a21\u578b\u57fa\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.04295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04295", "abs": "https://arxiv.org/abs/2510.04295", "authors": ["Nghiem T. Diep", "Dung Le", "Tuan Truong", "Tan Dinh", "Huy Nguyen", "Nhat Ho"], "title": "HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks", "comment": "Nghiem T. Diep, Dung Le, and Tuan Truong contributed equally to this\n  work", "summary": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)\ntechnique that adapts large pre-trained models by adding low-rank matrices to\ntheir weight updates. However, in the context of fine-tuning multi-head\nself-attention (MHA), LoRA has been employed to adapt each attention head\nseparately, thereby overlooking potential synergies across different heads. To\nmitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)\nmethod, which utilizes joint hypernetworks to generate low-rank matrices across\nattention heads. By coupling their adaptation through a shared generator, HoRA\nencourages cross-head information sharing, and thus directly addresses the\naforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens\nof hierarchical mixture of experts, our theoretical findings reveal that the\nlatter achieves superior sample efficiency to the former. Furthermore, through\nextensive experiments across diverse language and vision benchmarks, we\ndemonstrate that HoRA outperforms LoRA and other PEFT methods while requiring\nonly a marginal increase in the number of trainable parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86HoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u8d85\u7f51\u7edc\u751f\u6210\u8de8\u6ce8\u610f\u529b\u5934\u7684\u4f4e\u79e9\u77e9\u9635\uff0c\u89e3\u51b3\u4e86LoRA\u5728\u591a\u5934\u81ea\u6ce8\u610f\u529b\u4e2d\u5ffd\u89c6\u5934\u95f4\u534f\u540c\u7684\u95ee\u9898\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eLoRA\u3002", "motivation": "LoRA\u5728\u5fae\u8c03\u591a\u5934\u81ea\u6ce8\u610f\u529b\u65f6\u5355\u72ec\u9002\u914d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u5934\u4e4b\u95f4\u7684\u6f5c\u5728\u534f\u540c\u6548\u5e94\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u8054\u5408\u8d85\u7f51\u7edc\u4e3a\u6240\u6709\u6ce8\u610f\u529b\u5934\u751f\u6210\u4f4e\u79e9\u77e9\u9635\uff0c\u901a\u8fc7\u5171\u4eab\u751f\u6210\u5668\u5b9e\u73b0\u8de8\u5934\u4fe1\u606f\u5171\u4eab\uff0c\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eHoRA\u6bd4LoRA\u5177\u6709\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\uff1b\u5728\u591a\u4e2a\u8bed\u8a00\u548c\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHoRA\u4ee5\u5c11\u91cf\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u8d85\u8d8a\u4e86LoRA\u548c\u5176\u4ed6PEFT\u65b9\u6cd5\u3002", "conclusion": "HoRA\u901a\u8fc7\u4fc3\u8fdb\u8de8\u6ce8\u610f\u529b\u5934\u7684\u4fe1\u606f\u5171\u4eab\uff0c\u6709\u6548\u89e3\u51b3\u4e86LoRA\u7684\u5c40\u9650\u6027\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04304", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04304", "abs": "https://arxiv.org/abs/2510.04304", "authors": ["Harshil Vejendla"], "title": "Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention", "comment": "PRICAI 2025 Oral, 9 pages, 3 figures", "summary": "We introduce Wave-PDE Nets, a neural architecture whose elementary operation\nis a differentiable simulation of the second-order wave equation. Each layer\npropagates its hidden state as a continuous field through a medium with\ntrainable spatial velocity c(x) and damping {\\gamma}(x). A symplectic spectral\nsolver based on FFTs realises this propagation in O(nlog n) time. This\noscillatory, global mechanism provides a powerful alternative to attention and\nfirst-order state-space models. We prove that a single Wave-PDE layer is a\nuniversal approximator. On language and vision benchmarks, Wave-PDE Nets match\nor exceed Transformer performance while demonstrating superior practical\nefficiency, reducing wall-clock time by up to 30% and peak memory by 25%.\nAblation studies confirm the critical role of symplectic integration and a\nspectral Laplacian for stability and performance. Visualizations of the learned\nphysical parameters reveal that the model learns intuitive strategies for\ninformation propagation. These results position Wave-PDE Nets as a\ncomputationally efficient and robust architecture with a strong physical\ninductive bias.", "AI": {"tldr": "Wave-PDE Nets\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u6ce2\u52a8\u65b9\u7a0b\u6a21\u62df\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u7a7a\u95f4\u901f\u5ea6\u548c\u963b\u5c3c\u53c2\u6570\u4f20\u64ad\u9690\u85cf\u72b6\u6001\uff0c\u4f7f\u7528\u57fa\u4e8eFFT\u7684\u8f9b\u8c31\u6c42\u89e3\u5668\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e00\u9636\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u63d0\u4f9b\u632f\u8361\u3001\u5168\u5c40\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u6784\u5efa\u8ba1\u7b97\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u67b6\u6784\u3002", "method": "\u6bcf\u5c42\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u7a7a\u95f4\u901f\u5ea6c(x)\u548c\u963b\u5c3c\u03b3(x)\u4f20\u64ad\u8fde\u7eed\u573a\u9690\u85cf\u72b6\u6001\uff0c\u4f7f\u7528\u57fa\u4e8eFFT\u7684\u8f9b\u8c31\u6c42\u89e3\u5668\u5728O(n log n)\u65f6\u95f4\u5185\u5b9e\u73b0\u4f20\u64ad\u3002", "result": "\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7Transformer\uff0c\u5b9e\u9645\u6548\u7387\u63d0\u534730%\u7684\u5899\u949f\u65f6\u95f4\u548c25%\u7684\u5cf0\u503c\u5185\u5b58\uff0c\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u5b66\u4e60\u5230\u76f4\u89c2\u7684\u4fe1\u606f\u4f20\u64ad\u7b56\u7565\u3002", "conclusion": "Wave-PDE Nets\u88ab\u5b9a\u4f4d\u4e3a\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u9c81\u68d2\u6027\u5f3a\u4e14\u5177\u6709\u5f3a\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\u7684\u67b6\u6784\uff0c\u8f9b\u79ef\u5206\u548c\u8c31\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5bf9\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04309", "abs": "https://arxiv.org/abs/2510.04309", "authors": ["Dung V. Nguyen", "Hieu M. Vu", "Nhi Y. Pham", "Lei Zhang", "Tan M. Nguyen"], "title": "Activation Steering with a Feedback Controller", "comment": "9 pages in the main text. Under Review", "summary": "Controlling the behaviors of large language models (LLM) is fundamental to\ntheir safety alignment and reliable deployment. However, existing steering\nmethods are primarily driven by empirical insights and lack theoretical\nperformance guarantees. In this work, we develop a control-theoretic foundation\nfor activation steering by showing that popular steering methods correspond to\nthe proportional (P) controllers, with the steering vector serving as the\nfeedback signal. Building on this finding, we propose\nProportional-Integral-Derivative (PID) Steering, a principled framework that\nleverages the full PID controller for activation steering in LLMs. The\nproportional (P) term aligns activations with target semantic directions, the\nintegral (I) term accumulates errors to enforce persistent corrections across\nlayers, and the derivative (D) term mitigates overshoot by counteracting rapid\nactivation changes. This closed-loop design yields interpretable error dynamics\nand connects activation steering to classical stability guarantees in control\ntheory. Moreover, PID Steering is lightweight, modular, and readily integrates\nwith state-of-the-art steering methods. Extensive experiments across multiple\nLLM families and benchmarks demonstrate that PID Steering consistently\noutperforms existing approaches, achieving more robust and reliable behavioral\ncontrol.", "AI": {"tldr": "\u63d0\u51faPID Steering\u6846\u67b6\uff0c\u5c06\u63a7\u5236\u7406\u8bba\u4e2d\u7684PID\u63a7\u5236\u5668\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\u5f15\u5bfc\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u884c\u4e3a\u63a7\u5236\u6548\u679c\u548c\u7406\u8bba\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7ecf\u9a8c\uff0c\u7f3a\u4e4f\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\uff0c\u9700\u8981\u5efa\u7acb\u63a7\u5236\u7406\u8bba\u57fa\u7840\u6765\u63d0\u5347\u6fc0\u6d3b\u5f15\u5bfc\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027", "method": "\u5c06\u6d41\u884c\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5bf9\u5e94\u4e3a\u6bd4\u4f8b\u63a7\u5236\u5668\uff0c\u63d0\u51fa\u5b8c\u6574\u7684PID\u63a7\u5236\u5668\u6846\u67b6\uff1a\u6bd4\u4f8b\u9879\u5bf9\u9f50\u8bed\u4e49\u65b9\u5411\uff0c\u79ef\u5206\u9879\u7d2f\u79ef\u8bef\u5dee\u8fdb\u884c\u6301\u7eed\u4fee\u6b63\uff0c\u5fae\u5206\u9879\u6291\u5236\u8d85\u8c03", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPID Steering\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u884c\u4e3a\u63a7\u5236", "conclusion": "PID Steering\u4e3a\u6fc0\u6d3b\u5f15\u5bfc\u63d0\u4f9b\u4e86\u63a7\u5236\u7406\u8bba\u57fa\u7840\uff0c\u8fde\u63a5\u4e86\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u662f\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u65b9\u6cd5"}}
{"id": "2510.04316", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04316", "abs": "https://arxiv.org/abs/2510.04316", "authors": ["Sahar Koohfar"], "title": "Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework", "comment": null, "summary": "Accurate and timely prediction of crash severity is crucial in mitigating the\nsevere consequences of traffic accidents. Accurate and timely prediction of\ncrash severity is crucial in mitigating the severe consequences of traffic\naccidents. In order to provide appropriate levels of medical assistance and\ntransportation services, an intelligent transportation system relies on\neffective prediction methods. Deep learning models have gained popularity in\nthis domain due to their capability to capture non-linear relationships among\nvariables. In this research, we have implemented a hybrid CNN-RNN deep learning\nmodel for crash severity prediction and compared its performance against widely\nused statistical and machine learning models such as logistic regression,\nna\\\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and\nindividual deep learning models: RNN and CNN. This study employs a methodology\nthat considers the interconnected relationships between various features of\ntraffic accidents. The study was conducted using a dataset of 15,870 accident\nrecords gathered over a period of seven years between 2015 and 2021 on Virginia\nhighway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model\nhas outperformed all benchmark models in terms of predicting crash severity.\nThis result illustrates the effectiveness of the hybrid model as it combines\nthe advantages of both RNN and CNN models in order to achieve greater accuracy\nin the prediction process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408CNN-RNN\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6\u9884\u6d4b\uff0c\u5728\u5f17\u5409\u5c3c\u4e9aI-64\u9ad8\u901f\u516c\u8def15,870\u6761\u4e8b\u6545\u8bb0\u5f55\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u51c6\u786e\u53ca\u65f6\u5730\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u4e8e\u51cf\u8f7b\u4e8b\u6545\u4e25\u91cd\u540e\u679c\u81f3\u5173\u91cd\u8981\uff0c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u63d0\u4f9b\u9002\u5f53\u7684\u533b\u7597\u63f4\u52a9\u548c\u8fd0\u8f93\u670d\u52a1\u3002", "method": "\u91c7\u7528\u6df7\u5408CNN-RNN\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u4e0e\u903b\u8f91\u56de\u5f52\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001KNN\u3001\u51b3\u7b56\u6811\u4ee5\u53ca\u5355\u72ec\u7684RNN\u548cCNN\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u8003\u8651\u4e86\u4ea4\u901a\u4e8b\u6545\u5404\u7279\u5f81\u95f4\u7684\u76f8\u4e92\u5173\u8054\u5173\u7cfb\u3002", "result": "\u6240\u63d0\u51fa\u7684CNN-RNN\u6df7\u5408\u6a21\u578b\u5728\u6240\u6709\u57fa\u51c6\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u8be5\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u4e86RNN\u548cCNN\u4f18\u52bf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408CNN-RNN\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.04317", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04317", "abs": "https://arxiv.org/abs/2510.04317", "authors": ["Yucong Dai", "Lu Zhang", "Feng Luo", "Mashrur Chowdhury", "Yongkai Wu"], "title": "FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents", "comment": "Accepted by ICDM 2025 Demo Workshop", "summary": "Training fair and unbiased machine learning models is crucial for high-stakes\napplications, yet it presents significant challenges. Effective bias mitigation\nrequires deep expertise in fairness definitions, metrics, data preprocessing,\nand machine learning techniques. In addition, the complex process of balancing\nmodel performance with fairness requirements while properly handling sensitive\nattributes makes fairness-aware model development inaccessible to many\npractitioners. To address these challenges, we introduce FairAgent, an\nLLM-powered automated system that significantly simplifies fairness-aware model\ndevelopment. FairAgent eliminates the need for deep technical expertise by\nautomatically analyzing datasets for potential biases, handling data\npreprocessing and feature engineering, and implementing appropriate bias\nmitigation strategies based on user requirements. Our experiments demonstrate\nthat FairAgent achieves significant performance improvements while\nsignificantly reducing development time and expertise requirements, making\nfairness-aware machine learning more accessible to practitioners.", "AI": {"tldr": "FairAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u516c\u5e73\u611f\u77e5\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\uff0c\u65e0\u9700\u6df1\u539a\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u5373\u53ef\u5b9e\u73b0\u504f\u89c1\u7f13\u89e3\u3002", "motivation": "\u516c\u5e73\u65e0\u504f\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u9ad8\u98ce\u9669\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f00\u53d1\u8fc7\u7a0b\u9700\u8981\u6df1\u539a\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u3001\u6307\u6807\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f7f\u5f97\u8bb8\u591a\u4ece\u4e1a\u8005\u96be\u4ee5\u5b9e\u65bd\u3002", "method": "FairAgent\u81ea\u52a8\u5206\u6790\u6570\u636e\u96c6\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5904\u7406\u6570\u636e\u9884\u5904\u7406\u548c\u7279\u5f81\u5de5\u7a0b\uff0c\u5e76\u6839\u636e\u7528\u6237\u9700\u6c42\u5b9e\u65bd\u9002\u5f53\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFairAgent\u5728\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u4e13\u4e1a\u77e5\u8bc6\u8981\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "FairAgent\u4f7f\u516c\u5e73\u611f\u77e5\u7684\u673a\u5668\u5b66\u4e60\u5bf9\u4ece\u4e1a\u8005\u66f4\u52a0\u6613\u4e8e\u4f7f\u7528\u3002"}}
{"id": "2510.04325", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.04325", "abs": "https://arxiv.org/abs/2510.04325", "authors": ["Kenechukwu Ogbuagu", "Sepehr Maleki", "Giuseppe Bruni", "Senthil Krishnababu"], "title": "FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields", "comment": null, "summary": "The accurate prediction of flow fields around airfoils is crucial for\naerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models\nare effective but computationally expensive, thus inspiring the development of\nsurrogate models to enable quicker predictions. These surrogate models can be\nbased on deep learning architectures, such as Convolutional Neural Networks\n(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion\nmodels have shown significant promise in predicting complex flow fields. In\nthis work, we propose FoilDiff, a diffusion-based surrogate model with a\nhybrid-backbone denoising network. This hybrid design combines the power of\nconvolutional feature extraction and transformer-based global attention to\ngenerate more adaptable and accurate representations of flow structures.\nFoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling\nto optimise the efficiency of the sampling process at no additional cost to\nmodel generalisation. We used encoded representations of Reynolds number, angle\nof attack, and airfoil geometry to define the input space for generalisation\nacross a wide range of aerodynamic conditions. When evaluated against\nstate-of-the-art models, FoilDiff shows significant performance improvements,\nwith mean prediction errors reducing by up to 85\\% on the same datasets. The\nresults have demonstrated that FoilDiff can provide both more accurate\npredictions and better-calibrated predictive uncertainty than existing\ndiffusion-based models.", "AI": {"tldr": "FoilDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u6d41\u573a\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u9aa8\u5e72\u7f51\u7edc\u7ed3\u5408CNN\u548cTransformer\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfCFD\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u6a21\u578b\u6765\u5feb\u901f\u9884\u6d4b\u7a7a\u6c14\u52a8\u529b\u5b66\u6d41\u573a\u3002\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u6d41\u573a\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u63d0\u51faFoilDiff\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u9aa8\u5e72\u53bb\u566a\u7f51\u7edc\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548cTransformer\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528DDIM\u91c7\u6837\u4f18\u5316\u6548\u7387\uff0c\u5e76\u7f16\u7801\u96f7\u8bfa\u6570\u3001\u653b\u89d2\u548c\u7ffc\u578b\u51e0\u4f55\u53c2\u6570\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\uff0cFoilDiff\u5c06\u5e73\u5747\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u9ad8\u8fbe85%\uff0c\u5728\u76f8\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u548c\u66f4\u597d\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3002", "conclusion": "FoilDiff\u80fd\u591f\u4e3a\u7a7a\u6c14\u52a8\u529b\u5b66\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u7684\u6d41\u573a\u9884\u6d4b\uff0c\u5728\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u3002"}}
{"id": "2510.04327", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04327", "abs": "https://arxiv.org/abs/2510.04327", "authors": ["Haosong Zhang", "Shenxi Wu", "Yichi Zhang", "Wei Lin"], "title": "Arithmetic-Mean $\u03bc$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets", "comment": "Preprint. Under review at ICLR 2026", "summary": "Choosing an appropriate learning rate remains a key challenge in scaling\ndepth of modern deep networks. The classical maximal update parameterization\n($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to\nhomogeneous multilayer perceptrons (MLPs) but becomes ill-posed in\nheterogeneous architectures where residual accumulation and convolutions\nintroduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P\n(AM-$\\mu$P), which constrains not each individual layer but the network-wide\naverage one-step pre-activation second moment to a constant scale. Combined\nwith a residual-aware He fan-in initialization - scaling residual-branch\nweights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot\n\\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that\ntransfer consistently across depths. We prove that, for one- and\ntwo-dimensional convolutional networks, the maximal-update learning rate\nsatisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects\nare constant-level as $N\\gg k$. For standard residual networks with general\nconv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the\nminimal depth. Empirical results across a range of depths confirm the $-3/2$\nscaling law and enable zero-shot learning-rate transfer, providing a unified\nand practical LR principle for convolutional and deep residual networks without\nadditional tuning overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b97\u672f\u5e73\u5747\u03bcP\uff08AM-\u03bcP\uff09\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u7f51\u7edc\u8303\u56f4\u5185\u5e73\u5747\u5355\u6b65\u9884\u6fc0\u6d3b\u4e8c\u9636\u77e9\u4e3a\u5e38\u6570\u5c3a\u5ea6\uff0c\u7ed3\u5408\u6b8b\u5dee\u611f\u77e5\u7684He\u521d\u59cb\u5316\uff0c\u4e3a\u5377\u79ef\u548c\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5efa\u7acb\u4e86-3/2\u5e42\u5f8b\u7684\u5b66\u4e60\u7387\u7f29\u653e\u89c4\u5f8b\u3002", "motivation": "\u7ecf\u5178\u7684\u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316\uff08\u03bcP\uff09\u5728\u5f02\u6784\u67b6\u6784\u4e2d\u53d8\u5f97\u4e0d\u9002\u5b9a\uff0c\u56e0\u4e3a\u6b8b\u5dee\u7d2f\u79ef\u548c\u5377\u79ef\u5f15\u5165\u4e86\u8de8\u5c42\u7684\u4e0d\u5e73\u8861\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5b66\u4e60\u7387\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u5f15\u5165AM-\u03bcP\u53c2\u6570\u5316\uff0c\u7ea6\u675f\u7f51\u7edc\u5e73\u5747\u9884\u6fc0\u6d3b\u4e8c\u9636\u77e9\uff1b\u7ed3\u5408\u6b8b\u5dee\u611f\u77e5He\u521d\u59cb\u5316\uff0c\u7f29\u653e\u6b8b\u5dee\u5206\u652f\u6743\u91cd\uff1b\u4e3a\u5377\u79ef\u7f51\u7edc\u548c\u6b8b\u5dee\u7f51\u7edc\u5efa\u7acb\u5b66\u4e60\u7387\u6df1\u5ea6\u7f29\u653e\u89c4\u5f8b\u3002", "result": "\u8bc1\u660e\u5377\u79ef\u7f51\u7edc\u6700\u5927\u66f4\u65b0\u5b66\u4e60\u7387\u6ee1\u8db3\u03b7*(L)\u221dL^(-3/2)\uff1b\u6807\u51c6\u6b8b\u5dee\u7f51\u7edc\u03b7*(L)=\u0398(L^(-3/2))\uff1b\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86-3/2\u7f29\u653e\u89c4\u5f8b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u5b66\u4e60\u7387\u8fc1\u79fb\u3002", "conclusion": "AM-\u03bcP\u4e3a\u5377\u79ef\u548c\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u63d0\u4f9b\u4e86\u7edf\u4e00\u5b9e\u7528\u7684\u5b66\u4e60\u7387\u539f\u5219\uff0c\u65e0\u9700\u989d\u5916\u8c03\u4f18\u5f00\u9500\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u67b6\u6784\u4e2d\u7684\u5b66\u4e60\u7387\u9009\u62e9\u6311\u6218\u3002"}}
{"id": "2510.04331", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04331", "abs": "https://arxiv.org/abs/2510.04331", "authors": ["Nghiem T. Diep", "Hien Dang", "Tuan Truong", "Tan Dinh", "Huy Nguyen", "Nhat Ho"], "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks", "comment": "Nghiem T. Diep, Hien Dang, and Tuan Truong contributed equally to\n  this work", "summary": "Parameter-efficient fine-tuning (PEFT) methods have become the standard\nparadigm for adapting large-scale models. Among these techniques,\nWeight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the\nlearning capacity and training stability of the vanilla Low-Rank Adaptation\n(LoRA) method by explicitly decomposing pre-trained weights into magnitude and\ndirectional components. In this work, we propose DoRAN, a new variant of DoRA\ndesigned to further stabilize training and boost the sample efficiency of DoRA.\nOur approach includes two key stages: (i) injecting noise into the denominator\nof DoRA's weight decomposition, which serves as an adaptive regularizer to\nmitigate instabilities; and (ii) replacing static low-rank matrices with\nauxiliary networks that generate them dynamically, enabling parameter coupling\nacross layers and yielding better sample efficiency in both theory and\npractice. Comprehensive experiments on vision and language benchmarks show that\nDoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These\nresults underscore the effectiveness of combining stabilization through\nnoise-based regularization with network-based parameter generation, offering a\npromising direction for robust and efficient fine-tuning of foundation models.", "AI": {"tldr": "DoRAN\u662f\u4e00\u79cd\u6539\u8fdb\u7684DoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u52a8\u6001\u4f4e\u79e9\u77e9\u9635\u751f\u6210\u6765\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "DoRA\u867d\u7136\u4f18\u4e8eLoRA\uff0c\u4f46\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002DoRAN\u65e8\u5728\u901a\u8fc7\u6b63\u5219\u5316\u548c\u53c2\u6570\u8026\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "1) \u5728DoRA\u6743\u91cd\u5206\u89e3\u7684\u5206\u6bcd\u4e2d\u6ce8\u5165\u566a\u58f0\u4f5c\u4e3a\u81ea\u9002\u5e94\u6b63\u5219\u5316\u5668\uff1b2) \u7528\u8f85\u52a9\u7f51\u7edc\u52a8\u6001\u751f\u6210\u4f4e\u79e9\u77e9\u9635\uff0c\u5b9e\u73b0\u8de8\u5c42\u53c2\u6570\u8026\u5408\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDoRAN\u6301\u7eed\u4f18\u4e8eLoRA\u3001DoRA\u548c\u5176\u4ed6PEFT\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u566a\u58f0\u6b63\u5219\u5316\u548c\u7f51\u7edc\u53c2\u6570\u751f\u6210\u7684\u65b9\u6cd5\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u7a33\u5065\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.04341", "categories": ["cs.LG", "cs.AI", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.04341", "abs": "https://arxiv.org/abs/2510.04341", "authors": ["G. Niklas Noren", "Eva-Lisa Meldau", "Johan Ellenius"], "title": "Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies", "comment": "28 pages, 2 figures", "summary": "Many high-stakes AI applications target low-prevalence events, where apparent\naccuracy can conceal limited real-world value. Relevant AI models range from\nexpert-defined rules and traditional machine learning to generative LLMs\nconstrained for classification. We outline key considerations for critical\nappraisal of AI in rare-event recognition, including problem framing and test\nset design, prevalence-aware statistical evaluation, robustness assessment, and\nintegration into human workflows. In addition, we propose an approach to\nstructured case-level examination (SCLE), to complement statistical performance\nevaluation, and a comprehensive checklist to guide procurement or development\nof AI models for rare-event recognition. We instantiate the framework in\npharmacovigilance, drawing on three studies: rule-based retrieval of\npregnancy-related reports; duplicate detection combining machine learning with\nprobabilistic record linkage; and automated redaction of person names using an\nLLM. We highlight pitfalls specific to the rare-event setting including\noptimism from unrealistic class balance and lack of difficult positive controls\nin test sets - and show how cost-sensitive targets align model performance with\noperational value. While grounded in pharmacovigilance practice, the principles\ngeneralize to domains where positives are scarce and error costs may be\nasymmetric.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u6027\u80fd\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4f4e\u53d1\u751f\u7387\u573a\u666f\u4e0b\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5305\u62ec\u95ee\u9898\u6846\u67b6\u3001\u6d4b\u8bd5\u96c6\u8bbe\u8ba1\u3001\u7edf\u8ba1\u8bc4\u4f30\u548c\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u3002", "motivation": "\u8bb8\u591a\u9ad8\u98ce\u9669AI\u5e94\u7528\u9488\u5bf9\u4f4e\u53d1\u751f\u7387\u4e8b\u4ef6\uff0c\u8868\u9762\u4e0a\u7684\u51c6\u786e\u6027\u53ef\u80fd\u63a9\u76d6\u4e86\u6709\u9650\u7684\u73b0\u5b9e\u4ef7\u503c\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30AI\u5728\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u7684\u771f\u5b9e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u6848\u4f8b\u7ea7\u68c0\u67e5(SCLE)\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u5168\u9762\u7684\u68c0\u67e5\u6e05\u5355\u6765\u6307\u5bfcAI\u6a21\u578b\u91c7\u8d2d\u6216\u5f00\u53d1\u3002\u5728\u836f\u7269\u8b66\u6212\u9886\u57df\u5b9e\u4f8b\u5316\u4e86\u8be5\u6846\u67b6\uff0c\u4f7f\u7528\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u7d22\u3001\u673a\u5668\u5b66\u4e60\u4e0e\u6982\u7387\u8bb0\u5f55\u94fe\u63a5\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u7f16\u8f91\u3002", "result": "\u8bc6\u522b\u4e86\u7f55\u89c1\u4e8b\u4ef6\u8bbe\u7f6e\u4e2d\u7684\u7279\u5b9a\u9677\u9631\uff0c\u5305\u62ec\u4e0d\u73b0\u5b9e\u7684\u7c7b\u522b\u5e73\u8861\u5bfc\u81f4\u7684\u4e50\u89c2\u4f30\u8ba1\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7f3a\u4e4f\u56f0\u96be\u9633\u6027\u5bf9\u7167\u3002\u5c55\u793a\u4e86\u6210\u672c\u654f\u611f\u76ee\u6807\u5982\u4f55\u4f7f\u6a21\u578b\u6027\u80fd\u4e0e\u64cd\u4f5c\u4ef7\u503c\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u867d\u7136\u57fa\u4e8e\u836f\u7269\u8b66\u6212\u5b9e\u8df5\uff0c\u4f46\u8fd9\u4e9b\u539f\u5219\u53ef\u63a8\u5e7f\u5230\u9633\u6027\u6837\u672c\u7a00\u7f3a\u4e14\u9519\u8bef\u6210\u672c\u53ef\u80fd\u4e0d\u5bf9\u79f0\u7684\u9886\u57df\uff0c\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u8bc6\u522b\u4e2d\u7684AI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2510.04342", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04342", "abs": "https://arxiv.org/abs/2510.04342", "authors": ["Harshil Vejendla"], "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics", "comment": "MIT URTC Technical Paper (Oral), 5 pages, 4 figures", "summary": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bfe\u7a0b\u6df7\u6c8c\u9884\u6d4b(CCF)\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u7b80\u5355\u5468\u671f\u884c\u4e3a\u5230\u590d\u6742\u6df7\u6c8c\u52a8\u6001\u7684\u6e10\u8fdb\u5f0f\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u6df7\u6c8c\u7cfb\u7edf\u9884\u6d4b\u4e2d\u7684\u4e24\u4e2a\u6781\u7aef\u95ee\u9898\uff1a\u8fc7\u5ea6\u4e13\u4e1a\u5316\u4e8e\u5355\u4e00\u6df7\u6c8c\u7cfb\u7edf\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff0c\u6216\u6df7\u5408\u5927\u91cf\u4e0d\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u5bfc\u81f4\u65e0\u6cd5\u5b66\u4e60\u7279\u5b9a\u52a8\u6001\u673a\u5236", "method": "\u57fa\u4e8e\u52a8\u529b\u5b66\u7cfb\u7edf\u7406\u8bba\u7ec4\u7ec7\u8bad\u7ec3\u6570\u636e\uff0c\u4f7f\u7528\u6700\u5927Lyapun\u592b\u6307\u6570\u548c\u5438\u5f15\u5b50\u7ef4\u5ea6\u91cf\u5316\u590d\u6742\u6027\uff0c\u6784\u5efa\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u8bfe\u7a0b\u5b66\u4e60\u8def\u5f84\uff0c\u5305\u542b50\u591a\u4e2a\u5408\u6210ODE/PDE\u7cfb\u7edf", "result": "\u5728\u592a\u9633\u9ed1\u5b50\u6570\u3001\u7535\u529b\u9700\u6c42\u548c\u4eba\u7c7bECG\u4fe1\u53f7\u7b49\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCF\u5c06\u6709\u6548\u9884\u6d4b\u8303\u56f4\u5ef6\u957f\u8fbe40%\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u5ef6\u957f\u8d85\u8fc7\u4e24\u500d", "conclusion": "CCF\u8bad\u7ec3\u8303\u5f0f\u80fd\u591f\u5efa\u7acb\u5bf9\u52a8\u6001\u884c\u4e3a\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u8868\u793a\uff0c\u5728\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u5747\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347"}}
{"id": "2510.04357", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.04357", "abs": "https://arxiv.org/abs/2510.04357", "authors": ["Anoushka Harit", "Zhongtian Sun", "Jongmin Yu"], "title": "From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere", "comment": "6th ACM International Conference on AI in Finance", "summary": "We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel\narchitecture for interpretable financial time-series forecasting that unifies\n\\emph{Granger-causal hypergraph structure}, \\emph{Riemannian geometry}, and\n\\emph{causally masked Transformer attention}. CSHT models the directional\ninfluence of financial news and sentiment on asset returns by extracting\nmultivariate Granger-causal dependencies, which are encoded as directional\nhyperedges on the surface of a hypersphere. Attention is constrained via\nangular masks that preserve both temporal directionality and geometric\nconsistency. Evaluated on S\\&P 500 data from 2018 to 2023, including the 2020\nCOVID-19 shock, CSHT consistently outperforms baselines across return\nprediction, regime classification, and top-asset ranking tasks. By enforcing\npredictive causal structure and embedding variables in a Riemannian manifold,\nCSHT delivers both \\emph{robust generalisation across market regimes} and\n\\emph{transparent attribution pathways} from macroeconomic events to\nstock-level responses. These results suggest that CSHT is a principled and\npractical solution for trustworthy financial forecasting under uncertainty.", "AI": {"tldr": "\u63d0\u51faCSHT\u67b6\u6784\uff0c\u7ed3\u5408Granger\u56e0\u679c\u8d85\u56fe\u7ed3\u6784\u3001\u9ece\u66fc\u51e0\u4f55\u548c\u56e0\u679c\u63a9\u7801Transformer\u6ce8\u610f\u529b\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u7edf\u4e00\u5efa\u6a21\u91d1\u878d\u65b0\u95fb\u548c\u60c5\u7eea\u5bf9\u8d44\u4ea7\u6536\u76ca\u7684\u5b9a\u5411\u5f71\u54cd\uff0c\u63d0\u4f9b\u900f\u660e\u5f52\u56e0\u8def\u5f84\u3002", "method": "\u63d0\u53d6\u591a\u5143Granger\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u7f16\u7801\u4e3a\u8d85\u7403\u9762\u4e0a\u7684\u5b9a\u5411\u8d85\u8fb9\uff0c\u901a\u8fc7\u89d2\u5ea6\u63a9\u7801\u7ea6\u675f\u6ce8\u610f\u529b\u4ee5\u4fdd\u6301\u65f6\u95f4\u65b9\u5411\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u57282018-2023\u5e74\u6807\u666e500\u6570\u636e\u4e0a\uff0c\u5305\u62ec2020\u5e74COVID-19\u51b2\u51fb\uff0cCSHT\u5728\u6536\u76ca\u9884\u6d4b\u3001\u5236\u5ea6\u5206\u7c7b\u548c\u9876\u7ea7\u8d44\u4ea7\u6392\u540d\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "CSHT\u901a\u8fc7\u5f3a\u5236\u9884\u6d4b\u56e0\u679c\u7ed3\u6784\u548c\u5728\u9ece\u66fc\u6d41\u5f62\u4e2d\u5d4c\u5165\u53d8\u91cf\uff0c\u63d0\u4f9b\u4e86\u8de8\u5e02\u573a\u5236\u5ea6\u7684\u7a33\u5065\u6cdb\u5316\u548c\u4ece\u5b8f\u89c2\u7ecf\u6d4e\u4e8b\u4ef6\u5230\u80a1\u7968\u7ea7\u54cd\u5e94\u7684\u900f\u660e\u5f52\u56e0\u8def\u5f84\uff0c\u662f\u53ef\u4fe1\u91d1\u878d\u9884\u6d4b\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04366", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04366", "abs": "https://arxiv.org/abs/2510.04366", "authors": ["Christopher Klugmann", "Daniel Kondermann"], "title": "Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework", "comment": "Preprint, 20 pages in total, 7 figures", "summary": "Human-generated categorical annotations frequently produce empirical response\ndistributions (soft labels) that reflect ambiguity rather than simple annotator\nerror. We introduce an ambiguity measure that maps a discrete response\ndistribution to a scalar in the unit interval, designed to quantify aleatoric\nuncertainty in categorical tasks. The measure bears a close relationship to\nquadratic entropy (Gini-style impurity) but departs from those indices by\ntreating an explicit \"can't solve\" category asymmetrically, thereby separating\nuncertainty arising from class-level indistinguishability from uncertainty due\nto explicit unresolvability. We analyze the measure's formal properties and\ncontrast its behavior with a representative ambiguity measure from the\nliterature. Moving beyond description, we develop statistical tools for\ninference: we propose frequentist point estimators for population ambiguity and\nderive the Bayesian posterior over ambiguity induced by Dirichlet priors on the\nunderlying probability vector, providing a principled account of epistemic\nuncertainty. Numerical examples illustrate estimation, calibration, and\npractical use for dataset-quality assessment and downstream machine-learning\nworkflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u7cca\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4e8c\u6b21\u71b5\u76f8\u5173\u4f46\u80fd\u533a\u5206\u7c7b\u522b\u4e0d\u53ef\u533a\u5206\u6027\u548c\u660e\u786e\u4e0d\u53ef\u89e3\u51b3\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u7edf\u8ba1\u63a8\u65ad\u5de5\u5177\u3002", "motivation": "\u4eba\u7c7b\u751f\u6210\u7684\u5206\u7c7b\u6807\u6ce8\u7ecf\u5e38\u4ea7\u751f\u53cd\u6620\u6a21\u7cca\u6027\u800c\u975e\u7b80\u5355\u6807\u6ce8\u9519\u8bef\u7684\u7ecf\u9a8c\u54cd\u5e94\u5206\u5e03\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u79cd\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5c06\u79bb\u6563\u54cd\u5e94\u5206\u5e03\u6620\u5c04\u5230\u5355\u4f4d\u533a\u95f4\u6807\u91cf\u7684\u6a21\u7cca\u6027\u5ea6\u91cf\uff0c\u5206\u6790\u5176\u5f62\u5f0f\u7279\u6027\uff0c\u5f00\u53d1\u9891\u7387\u4e3b\u4e49\u70b9\u4f30\u8ba1\u5668\u548c\u8d1d\u53f6\u65af\u540e\u9a8c\u63a8\u65ad\u5de5\u5177\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5206\u79bb\u7c7b\u522b\u4e0d\u53ef\u533a\u5206\u6027\u548c\u660e\u786e\u4e0d\u53ef\u89e3\u51b3\u6027\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u548c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u5b9e\u7528\u5de5\u5177\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u7cca\u6027\u5ea6\u91cf\u4e3a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u4eba\u7c7b\u6807\u6ce8\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002"}}
{"id": "2510.04374", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.04374", "abs": "https://arxiv.org/abs/2510.04374", "authors": ["Tejal Patwardhan", "Rachel Dias", "Elizabeth Proehl", "Grace Kim", "Michele Wang", "Olivia Watkins", "Sim\u00f3n Posada Fishman", "Marwan Aljubeh", "Phoebe Thacker", "Laurance Fauconnet", "Natalie S. Kim", "Patrick Chao", "Samuel Miserendino", "Gildas Chabot", "David Li", "Michael Sharman", "Alexandra Barr", "Amelia Glaese", "Jerry Tworek"], "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks", "comment": null, "summary": "We introduce GDPval, a benchmark evaluating AI model capabilities on\nreal-world economically valuable tasks. GDPval covers the majority of U.S.\nBureau of Labor Statistics Work Activities for 44 occupations across the top 9\nsectors contributing to U.S. GDP (Gross Domestic Product). Tasks are\nconstructed from the representative work of industry professionals with an\naverage of 14 years of experience. We find that frontier model performance on\nGDPval is improving roughly linearly over time, and that the current best\nfrontier models are approaching industry experts in deliverable quality. We\nanalyze the potential for frontier models, when paired with human oversight, to\nperform GDPval tasks cheaper and faster than unaided experts. We also\ndemonstrate that increased reasoning effort, increased task context, and\nincreased scaffolding improves model performance on GDPval. Finally, we\nopen-source a gold subset of 220 tasks and provide a public automated grading\nservice at evals.openai.com to facilitate future research in understanding\nreal-world model capabilities.", "AI": {"tldr": "GDPval\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7ecf\u6d4e\u4ef7\u503c\u4efb\u52a1\u4e0a\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u7f8e\u56fdGDP\u524d9\u5927\u884c\u4e1a\u4e2d44\u4e2a\u804c\u4e1a\u7684\u5de5\u4f5c\u6d3b\u52a8\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u6027\u80fd\u968f\u65f6\u95f4\u7ebf\u6027\u63d0\u5347\uff0c\u63a5\u8fd1\u884c\u4e1a\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30AI\u6a21\u578b\u5728\u5b9e\u9645\u7ecf\u6d4e\u4ef7\u503c\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b66\u672f\u57fa\u51c6\uff0c\u4ee5\u4e86\u89e3\u6a21\u578b\u5728\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u7f8e\u56fd\u52b3\u5de5\u7edf\u8ba1\u5c40\u5de5\u4f5c\u6d3b\u52a8\u6570\u636e\uff0c\u6784\u5efa44\u4e2a\u804c\u4e1a\u7684\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u7531\u5e73\u574714\u5e74\u7ecf\u9a8c\u7684\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u8bbe\u8ba1\uff0c\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728GDPval\u4e0a\u7684\u8868\u73b0\u968f\u65f6\u95f4\u7ebf\u6027\u63d0\u5347\uff0c\u5f53\u524d\u6700\u4f73\u6a21\u578b\u63a5\u8fd1\u884c\u4e1a\u4e13\u5bb6\u7684\u4ea4\u4ed8\u8d28\u91cf\uff0c\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u52aa\u529b\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u652f\u67b6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "AI\u6a21\u578b\u5728\u7ecf\u6d4e\u4ef7\u503c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u4eba\u7c7b\u76d1\u7763\u53ef\u4ee5\u6bd4\u65e0\u8f85\u52a9\u4e13\u5bb6\u66f4\u4fbf\u5b9c\u3001\u66f4\u5feb\u5730\u5b8c\u6210\u4efb\u52a1\uff0c\u4e3a\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2510.04375", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04375", "abs": "https://arxiv.org/abs/2510.04375", "authors": ["Akshay Mittal", "Vinay Venkatesh", "Krishna Kandi", "Shalini Sudarshan"], "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains", "comment": null, "summary": "The effectiveness of single-model sequential recommendation architectures,\nwhile scalable, is often limited when catering to \"power users\" in sparse or\nniche domains. Our previous research, PinnerFormerLite, addressed this by using\na fixed weighted loss to prioritize specific domains. However, this approach\ncan be sub-optimal, as a single, uniform weight may not be sufficient for\ndomains with very few interactions, where the training signal is easily diluted\nby the vast, generic dataset.\n  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss\nfunction with comprehensive theoretical foundations and extensive empirical\nvalidation. We introduce an adaptive algorithm that adjusts the loss weight for\neach domain based on its sparsity in the training data, assigning a higher\nweight to sparser domains and a lower weight to denser ones. This ensures that\neven rare user interests contribute a meaningful gradient signal, preventing\nthem from being overshadowed.\n  We provide rigorous theoretical analysis including convergence proofs,\ncomplexity analysis, and bounds analysis to establish the stability and\nefficiency of our approach. Our comprehensive empirical validation across four\ndiverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)\nwith state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that\nthis dynamic weighting system significantly outperforms all comparison methods,\nparticularly for sparse domains, achieving substantial lifts in key metrics\nlike Recall at 10 and NDCG at 10 while maintaining performance on denser\ndomains and introducing minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u6839\u636e\u9886\u57df\u7a00\u758f\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u89e3\u51b3\u7a00\u758f\u9886\u57df\u63a8\u8350\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u52a0\u6743\u635f\u5931\u5728\u7a00\u758f\u9886\u57df\u6548\u679c\u6709\u9650\uff0c\u5355\u4e00\u6743\u91cd\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4ea4\u4e92\u6781\u5c11\u7684\u9886\u57df\uff0c\u8bad\u7ec3\u4fe1\u53f7\u5bb9\u6613\u88ab\u901a\u7528\u6570\u636e\u96c6\u7a00\u91ca\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u6bcf\u4e2a\u9886\u57df\u7684\u7a00\u758f\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u5bf9\u7a00\u758f\u9886\u57df\u8d4b\u4e88\u66f4\u9ad8\u6743\u91cd\uff0c\u7a20\u5bc6\u9886\u57df\u8d4b\u4e88\u8f83\u4f4e\u6743\u91cd\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u9886\u57df\uff0cRecall@10\u548cNDCG@10\u6307\u6807\u5927\u5e45\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u7a20\u5bc6\u9886\u57df\u6027\u80fd\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u7a00\u758f\u9886\u57df\u63a8\u8350\u6027\u80fd\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04376", "categories": ["cs.LG", "68T07, 18B99, 55N35", "I.2.6; F.4.1; G.2.2"], "pdf": "https://arxiv.org/pdf/2510.04376", "abs": "https://arxiv.org/abs/2510.04376", "authors": ["Abdulrahman Tamim"], "title": "Categorical Invariants of Learning Dynamics", "comment": null, "summary": "Neural network training is typically viewed as gradient descent on a loss\nsurface. We propose a fundamentally different perspective: learning is a\nstructure-preserving transformation (a functor L) between the space of network\nparameters (Param) and the space of learned representations (Rep). This\ncategorical framework reveals that different training runs producing similar\ntest performance often belong to the same homotopy class (continuous\ndeformation family) of optimization paths. We show experimentally that networks\nconverging via homotopic trajectories generalize within 0.5% accuracy of each\nother, while non-homotopic paths differ by over 3%. The theory provides\npractical tools: persistent homology identifies stable minima predictive of\ngeneralization (R^2 = 0.82 correlation), pullback constructions formalize\ntransfer learning, and 2-categorical structures explain when different\noptimization algorithms yield functionally equivalent models. These categorical\ninvariants offer both theoretical insight into why deep learning works and\nconcrete algorithmic principles for training more robust networks.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u89c6\u4e3a\u53c2\u6570\u7a7a\u95f4\u5230\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\u4fdd\u6301\u53d8\u6362\uff08\u51fd\u5b50L\uff09\u7684\u8303\u7574\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u540c\u4f26\u7c7b\u4f18\u5316\u8def\u5f84\u4e0e\u6cdb\u5316\u6027\u80fd\u7684\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u5c06\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u89c6\u4e3a\u635f\u5931\u66f2\u9762\u68af\u5ea6\u4e0b\u964d\u7684\u89c6\u89d2\u6709\u9650\uff0c\u9700\u8981\u66f4\u6839\u672c\u7684\u6570\u5b66\u6846\u67b6\u6765\u7406\u89e3\u5b66\u4e60\u8fc7\u7a0b\u7684\u672c\u8d28\u7ed3\u6784\u3002", "method": "\u5efa\u7acb\u8303\u7574\u8bba\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u51fd\u5b50\u53d8\u6362\uff0c\u4f7f\u7528\u540c\u4f26\u7406\u8bba\u5206\u6790\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u540c\u8c03\u8bc6\u522b\u7a33\u5b9a\u6700\u5c0f\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u540c\u4f26\u8def\u5f84\u6cdb\u5316\u6027\u80fd\u5dee\u5f02\u57280.5%\u4ee5\u5185\uff0c\u975e\u540c\u4f26\u8def\u5f84\u5dee\u5f02\u8d85\u8fc73%\uff1b\u6301\u7eed\u540c\u8c03\u4e0e\u6cdb\u5316\u6027\u80fd\u76f8\u5173\u6027R^2=0.82\u3002", "conclusion": "\u8303\u7574\u4e0d\u53d8\u91cf\u65e2\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e3a\u4f55\u6709\u6548\u7684\u7406\u8bba\u6d1e\u5bdf\uff0c\u53c8\u4e3a\u8bad\u7ec3\u66f4\u9c81\u68d2\u7f51\u7edc\u63d0\u4f9b\u4e86\u5177\u4f53\u7b97\u6cd5\u539f\u5219\u3002"}}
{"id": "2510.04378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04378", "abs": "https://arxiv.org/abs/2510.04378", "authors": ["Xinshuai Dong", "Ignavier Ng", "Haoyue Dai", "Jiaqi Sun", "Xiangchen Song", "Peter Spirtes", "Kun Zhang"], "title": "Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models", "comment": null, "summary": "Identifying the structure of a partially observed causal system is essential\nto various scientific fields. Recent advances have focused on constraint-based\ncausal discovery to solve this problem, and yet in practice these methods often\nface challenges related to multiple testing and error propagation. These issues\ncould be mitigated by a score-based method and thus it has raised great\nattention whether there exists a score-based greedy search method that can\nhandle the partially observed scenario. In this work, we propose the first\nscore-based greedy search method for the identification of structure involving\nlatent variables with identifiability guarantees. Specifically, we propose\nGeneralized N Factor Model and establish the global consistency:\n  the true structure including latent variables can be identified up to the\nMarkov equivalence class by using score. We then design\n  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm\nfor this class of model with well-defined operators,\n  which search very efficiently over the graph space to find the optimal\nstructure. Our experiments on both synthetic and real-life data validate the\neffectiveness of our method (code will be publicly available).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5206\u6570\u7684\u8d2a\u5a6a\u641c\u7d22\u65b9\u6cd5LGES\uff0c\u7528\u4e8e\u8bc6\u522b\u5305\u542b\u6f5c\u53d8\u91cf\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u5177\u6709\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7ea6\u675f\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u9762\u4e34\u591a\u91cd\u68c0\u9a8c\u548c\u8bef\u5dee\u4f20\u64ad\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u90e8\u5206\u89c2\u6d4b\u573a\u666f\u7684\u57fa\u4e8e\u5206\u6570\u7684\u8d2a\u5a6a\u641c\u7d22\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e86\u5e7f\u4e49N\u56e0\u5b50\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6f5c\u53d8\u91cf\u8d2a\u5a6a\u7b49\u4ef7\u641c\u7d22(LGES)\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u826f\u597d\u7684\u64cd\u4f5c\u5728\u56fe\u7a7a\u95f4\u4e2d\u9ad8\u6548\u641c\u7d22\u6700\u4f18\u7ed3\u6784", "result": "\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5305\u542b\u6f5c\u53d8\u91cf\u7684\u771f\u5b9e\u7ed3\u6784\uff0c\u8fbe\u5230\u9a6c\u5c14\u53ef\u592b\u7b49\u4ef7\u7c7b"}}
{"id": "2510.04386", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04386", "abs": "https://arxiv.org/abs/2510.04386", "authors": ["Shakson Isaac", "Yentl Collin", "Chirag Patel"], "title": "SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management", "comment": "Shakson Isaac and Yentl Collin contributed equally", "summary": "Continuous glucose monitoring (CGM) generates dense data streams critical for\ndiabetes management, but most used forecasting models lack interpretability for\nclinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting\nmodel that integrates CGM and wearable activity signals from the AI-READI\ncohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer\nbaseline, adds interpretability through variable selection and temporal\nattribution, and enables counterfactual forecasts simulating how planned\nchanges in physiological signals (e.g., heart rate, respiration) affect\nnear-term glucose. Together, these features make SSM-CGM an interpretable,\nphysiologically grounded framework for personalized diabetes management.", "AI": {"tldr": "SSM-CGM\u662f\u4e00\u4e2a\u57fa\u4e8eMamba\u7684\u795e\u7ecf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86\u8840\u7cd6\u548c\u53ef\u7a7f\u6234\u6d3b\u52a8\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u589e\u52a0\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eMamba\u7684\u795e\u7ecf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u6574\u5408\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u548c\u53ef\u7a7f\u6234\u6d3b\u52a8\u4fe1\u53f7\uff0c\u901a\u8fc7\u53d8\u91cf\u9009\u62e9\u548c\u65f6\u95f4\u5f52\u56e0\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u76f8\u6bd4\u65f6\u5e8f\u878d\u5408\u53d8\u6362\u5668\u57fa\u7ebf\uff0cSSM-CGM\u5728\u77ed\u671f\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u5e76\u80fd\u8fdb\u884c\u53cd\u4e8b\u5b9e\u9884\u6d4b\u6a21\u62df\u751f\u7406\u4fe1\u53f7\u53d8\u5316\u5bf9\u8840\u7cd6\u7684\u5f71\u54cd\u3002", "conclusion": "SSM-CGM\u4e3a\u4e2a\u6027\u5316\u7cd6\u5c3f\u75c5\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u751f\u7406\u5b66\u7684\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2510.04417", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.04417", "abs": "https://arxiv.org/abs/2510.04417", "authors": ["Wenyuan Zhao", "Adithya Balachandran", "Chao Tian", "Paul Pu Liang"], "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions", "comment": "NeurIPS 2025", "summary": "The study of multimodality has garnered significant interest in fields where\nthe analysis of interactions among multiple information sources can enhance\npredictive modeling, data fusion, and interpretability. Partial information\ndecomposition (PID) has emerged as a useful information-theoretic framework to\nquantify the degree to which individual modalities independently, redundantly,\nor synergistically convey information about a target variable. However,\nexisting PID methods depend on optimizing over a joint distribution constrained\nby estimated pairwise probability distributions, which are costly and\ninaccurate for continuous and high-dimensional modalities. Our first key\ninsight is that the problem can be solved efficiently when the pairwise\ndistributions are multivariate Gaussians, and we refer to this problem as\nGaussian PID (GPID). We propose a new gradient-based algorithm that\nsubstantially improves the computational efficiency of GPID based on an\nalternative formulation of the underlying optimization problem. To generalize\nthe applicability to non-Gaussian data, we learn information-preserving\nencoders to transform random variables of arbitrary input distributions into\npairwise Gaussian random variables. Along the way, we resolved an open problem\nregarding the optimality of joint Gaussian solutions for GPID. Empirical\nvalidation in diverse synthetic examples demonstrates that our proposed method\nprovides more accurate and efficient PID estimates than existing baselines. We\nfurther evaluate a series of large-scale multimodal benchmarks to show its\nutility in real-world applications of quantifying PID in multimodal datasets\nand selecting high-performing models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5047\u8bbe\u7684\u9ad8\u6548\u90e8\u5206\u4fe1\u606f\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5\u548c\u4fe1\u606f\u4fdd\u6301\u7f16\u7801\u5668\u5904\u7406\u975e\u9ad8\u65af\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u51c6\u786e\u6027\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u90e8\u5206\u4fe1\u606f\u5206\u89e3\u65b9\u6cd5\u4f9d\u8d56\u8054\u5408\u5206\u5e03\u4f18\u5316\uff0c\u5bf9\u8fde\u7eed\u9ad8\u7ef4\u6a21\u6001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u9ad8\u65af\u90e8\u5206\u4fe1\u606f\u5206\u89e3\u6846\u67b6\uff0c\u4f7f\u7528\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u4fdd\u6301\u7f16\u7801\u5668\u5c06\u4efb\u610f\u5206\u5e03\u8f6c\u6362\u4e3a\u9ad8\u65af\u5206\u5e03\u4ee5\u5904\u7406\u975e\u9ad8\u65af\u6570\u636e\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u4e2d\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u66f4\u51c6\u786e\u9ad8\u6548\uff0c\u5728\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u91cf\u5316\u4fe1\u606f\u5206\u89e3\u5e76\u9009\u62e9\u9ad8\u6027\u80fd\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u90e8\u5206\u4fe1\u606f\u5206\u89e3\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\u548c\u5206\u5e03\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2510.04430", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04430", "abs": "https://arxiv.org/abs/2510.04430", "authors": ["Ziyi Chen", "Heng Huang"], "title": "Achieve Performatively Optimal Policy for Performative Reinforcement Learning", "comment": null, "summary": "Performative reinforcement learning is an emerging dynamical decision making\nframework, which extends reinforcement learning to the common applications\nwhere the agent's policy can change the environmental dynamics. Existing works\non performative reinforcement learning only aim at a performatively stable (PS)\npolicy that maximizes an approximate value function. However, there is a\nprovably positive constant gap between the PS policy and the desired\nperformatively optimal (PO) policy that maximizes the original value function.\nIn contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)\nalgorithm with a zeroth-order approximation of the performative policy gradient\nin the Frank-Wolfe framework, and obtains \\textbf{the first polynomial-time\nconvergence to the desired PO} policy under the standard regularizer dominance\ncondition. For the convergence analysis, we prove two important properties of\nthe nonconvex value function. First, when the policy regularizer dominates the\nenvironmental shift, the value function satisfies a certain gradient dominance\nproperty, so that any stationary point (not PS) of the value function is a\ndesired PO. Second, though the value function has unbounded gradient, we prove\nthat all the sufficiently stationary points lie in a convex and compact policy\nsubspace $\\Pi_{\\Delta}$, where the policy value has a constant lower bound\n$\\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.\nExperimental results also demonstrate that our 0-FW algorithm is more effective\nthan the existing algorithms in finding the desired PO policy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f6\u9636Frank-Wolfe\u7b97\u6cd5\uff0c\u9996\u6b21\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u6536\u655b\u5230\u671f\u671b\u7684performatively optimal\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u6536\u655b\u5230performatively stable\u7b56\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684performative\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ea\u80fd\u6536\u655b\u5230performatively stable\u7b56\u7565\uff0c\u4e0e\u671f\u671b\u7684performatively optimal\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u7406\u8bba\u4e0a\u7684\u6b63\u5e38\u6570\u5dee\u8ddd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u96f6\u9636Frank-Wolfe\u7b97\u6cd5\uff0c\u5728Frank-Wolfe\u6846\u67b6\u4e2d\u4f7f\u7528\u96f6\u9636\u8fd1\u4f3c\u6765\u8ba1\u7b97performative\u7b56\u7565\u68af\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u6b63\u5219\u5316\u4e3b\u5bfc\u6761\u4ef6\u4e0b\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u6536\u655b\u5230\u671f\u671b\u7684PO\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7b97\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u975e\u51f8\u503c\u51fd\u6570\u7684\u91cd\u8981\u6027\u8d28\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u6536\u655b\u5230performatively optimal\u7b56\u7565\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7406\u8bba\u7a7a\u767d\u3002"}}
{"id": "2510.04432", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04432", "abs": "https://arxiv.org/abs/2510.04432", "authors": ["Ziyi Chen", "Su Zhang", "Heng Huang"], "title": "Trade-off in Estimating the Number of Byzantine Clients in Federated Learning", "comment": null, "summary": "Federated learning has attracted increasing attention at recent large-scale\noptimization and machine learning research and applications, but is also\nvulnerable to Byzantine clients that can send any erroneous signals. Robust\naggregators are commonly used to resist Byzantine clients. This usually\nrequires to estimate the unknown number $f$ of Byzantine clients, and thus\naccordingly select the aggregators with proper degree of robustness (i.e., the\nmaximum number $\\hat{f}$ of Byzantine clients allowed by the aggregator). Such\nan estimation should have important effect on the performance, which has not\nbeen systematically studied to our knowledge. This work will fill in the gap by\ntheoretically analyzing the worst-case error of aggregators as well as its\ninduced federated learning algorithm for any cases of $\\hat{f}$ and $f$.\nSpecifically, we will show that underestimation ($\\hat{f}<f$) can lead to\narbitrarily poor performance for both aggregators and federated learning. For\nnon-underestimation ($\\hat{f}\\ge f$), we have proved optimal lower and upper\nbounds of the same order on the errors of both aggregators and federated\nlearning. All these optimal bounds are proportional to $\\hat{f}/(n-f-\\hat{f})$\nwith $n$ clients, which monotonically increases with larger $\\hat{f}$. This\nindicates a fundamental trade-off: while an aggregator with a larger robustness\ndegree $\\hat{f}$ can solve federated learning problems of wider range $f\\in\n[0,\\hat{f}]$, the performance can deteriorate when there are actually fewer or\neven no Byzantine clients (i.e., $f\\in [0,\\hat{f})$).", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u62dc\u5360\u5ead\u5ba2\u6237\u7aef\u6570\u91cf\u4f30\u8ba1\u5bf9\u9c81\u68d2\u805a\u5408\u5668\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4f4e\u4f30\u4f1a\u5bfc\u81f4\u6027\u80fd\u4efb\u610f\u6076\u5316\uff0c\u800c\u975e\u4f4e\u4f30\u60c5\u51b5\u4e0b\u5b58\u5728\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u6613\u53d7\u62dc\u5360\u5ead\u5ba2\u6237\u7aef\u653b\u51fb\uff0c\u9c81\u68d2\u805a\u5408\u5668\u9700\u8981\u4f30\u8ba1\u62dc\u5360\u5ead\u5ba2\u6237\u7aef\u6570\u91cff\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u5206\u6790\u8fd9\u79cd\u4f30\u8ba1\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u805a\u5408\u5668\u548c\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5728\u4e0d\u540c\u4f30\u8ba1\u503cf\u0302\u548c\u5b9e\u9645\u503cf\u60c5\u51b5\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u8bef\u5dee\u3002", "result": "\u4f4e\u4f30(f\u0302<f)\u4f1a\u5bfc\u81f4\u6027\u80fd\u4efb\u610f\u6076\u5316\uff1b\u975e\u4f4e\u4f30(f\u0302\u2265f)\u65f6\uff0c\u8bef\u5dee\u4e0ef\u0302/(n-f-f\u0302)\u6210\u6b63\u6bd4\uff0c\u968ff\u0302\u589e\u5927\u800c\u589e\u52a0\uff0c\u8868\u660e\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u5b58\u5728\u6743\u8861\u3002", "conclusion": "\u62dc\u5360\u5ead\u5ba2\u6237\u7aef\u6570\u91cf\u4f30\u8ba1\u5bf9\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u5728\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u907f\u514d\u4f4e\u4f30\u662f\u5173\u952e\u3002"}}
{"id": "2510.04440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04440", "abs": "https://arxiv.org/abs/2510.04440", "authors": ["Farid Bozorgnia", "Vyacheslav Kungurtsev", "Shirali Kadyrov", "Mohsen Yousefnezhad"], "title": "Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size", "comment": null, "summary": "In this work, we introduce novel algorithms for label propagation and\nself-training using fractional heat kernel dynamics with a source term. We\nmotivate the methodology through the classical correspondence of information\ntheory with the physics of parabolic evolution equations. We integrate the\nfractional heat kernel into Graph Neural Network architectures such as Graph\nConvolutional Networks and Graph Attention, enhancing their expressiveness\nthrough adaptive, multi-hop diffusion. By applying Chebyshev polynomial\napproximations, large graphs become computationally feasible. Motivating\nvariational formulations demonstrate that by extending the classical diffusion\nmodel to fractional powers of the Laplacian, nonlocal interactions deliver more\nglobally diffusing labels. The particular balance between supervision of known\nlabels and diffusion across the graph is particularly advantageous in the case\nwhere only a small number of labeled training examples are present. We\ndemonstrate the effectiveness of this approach on standard datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u6570\u70ed\u6838\u52a8\u529b\u5b66\u7684\u65b0\u578b\u6807\u7b7e\u4f20\u64ad\u548c\u81ea\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6570\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u589e\u5f3a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\u7279\u522b\u6709\u6548\u3002", "motivation": "\u901a\u8fc7\u4fe1\u606f\u8bba\u4e0e\u629b\u7269\u7ebf\u6f14\u5316\u65b9\u7a0b\u7684\u7ecf\u5178\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c06\u5206\u6570\u70ed\u6838\u96c6\u6210\u5230\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\uff0c\u589e\u5f3a\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53ea\u6709\u5c11\u91cf\u6807\u6ce8\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u5206\u6570\u70ed\u6838\u52a8\u529b\u5b66\u8fdb\u884c\u6807\u7b7e\u4f20\u64ad\u548c\u81ea\u8bad\u7ec3\uff0c\u901a\u8fc7\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8fd1\u4f3c\u5904\u7406\u5927\u89c4\u6a21\u56fe\uff0c\u5c06\u5206\u6570\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u6574\u5408\u5230\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u4e2d\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5206\u6570\u6269\u6563\u6a21\u578b\u901a\u8fc7\u975e\u5c40\u90e8\u4ea4\u4e92\u5b9e\u73b0\u4e86\u66f4\u5168\u5c40\u7684\u6807\u7b7e\u6269\u6563\u3002", "conclusion": "\u5206\u6570\u70ed\u6838\u65b9\u6cd5\u5728\u76d1\u7763\u6807\u7b7e\u548c\u56fe\u4e0a\u6269\u6563\u4e4b\u95f4\u53d6\u5f97\u4e86\u7279\u522b\u6709\u5229\u7684\u5e73\u8861\uff0c\u5728\u53ea\u6709\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.04441", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04441", "abs": "https://arxiv.org/abs/2510.04441", "authors": ["Yilun Zhu", "Naihao Deng", "Naichen Shi", "Aditya Gangrade", "Clayton Scott"], "title": "Domain Generalization: A Tale of Two ERMs", "comment": null, "summary": "Domain generalization (DG) is the problem of generalizing from several\ndistributions (or domains), for which labeled training data are available, to a\nnew test domain for which no labeled data is available. A common finding in the\nDG literature is that it is difficult to outperform empirical risk minimization\n(ERM) on the pooled training data.\n  In this work, we argue that this finding has primarily been reported for\ndatasets satisfying a \\emph{covariate shift} assumption. When the dataset\nsatisfies a \\emph{posterior drift} assumption instead, we show that\n``domain-informed ERM,'' wherein feature vectors are augmented with\ndomain-specific information, outperforms pooling ERM. These claims are\nsupported by a theoretical framework and experiments on language and vision\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u6ee1\u8db3\u540e\u9a8c\u6f02\u79fb\u5047\u8bbe\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u6dfb\u52a0\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u7684\u9886\u57df\u611f\u77e5ERM\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u6c60\u5316ERM\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u6cdb\u5316\u7814\u7a76\u5927\u591a\u57fa\u4e8e\u534f\u53d8\u91cf\u504f\u79fb\u5047\u8bbe\uff0c\u96be\u4ee5\u8d85\u8d8a\u6c60\u5316ERM\u65b9\u6cd5\u3002\u4f5c\u8005\u8ba4\u4e3a\u5728\u540e\u9a8c\u6f02\u79fb\u5047\u8bbe\u4e0b\uff0c\u9886\u57df\u611f\u77e5\u65b9\u6cd5\u5e94\u8be5\u8868\u73b0\u66f4\u597d\u3002", "method": "\u63d0\u51fa\u9886\u57df\u611f\u77e5ERM\u65b9\u6cd5\uff0c\u5728\u7279\u5f81\u5411\u91cf\u4e2d\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u5e76\u4e0e\u7406\u8bba\u6846\u67b6\u7ed3\u5408\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u540e\u9a8c\u6f02\u79fb\u5047\u8bbe\u4e0b\uff0c\u9886\u57df\u611f\u77e5ERM\u786e\u5b9e\u4f18\u4e8e\u6c60\u5316ERM\u3002", "conclusion": "\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\u7684\u5206\u5e03\u5047\u8bbe\u7c7b\u578b\uff0c\u5728\u540e\u9a8c\u6f02\u79fb\u60c5\u51b5\u4e0b\uff0c\u9886\u57df\u611f\u77e5\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.04487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04487", "abs": "https://arxiv.org/abs/2510.04487", "authors": ["Willa Potosnak", "Malcolm Wolff", "Boris Oreshkin", "Mengfei Cao", "Michael W. Mahoney", "Dmitry Efimov", "Kin G. Olivares"], "title": "Forking-Sequences", "comment": null, "summary": "While accuracy is a critical requirement for time series forecasting models,\nan equally important (yet often overlooked) desideratum is forecast stability\nacross forecast creation dates (FCDs). Even highly accurate models can produce\nerratic revisions between FCDs, undermining stakeholder trust and disrupting\ndownstream decision-making. To improve forecast stability, models like MQCNN,\nMQT, and SPADE employ a little-known but highly effective technique:\nforking-sequences. Unlike standard statistical and neural forecasting methods\nthat treat each FCD independently, the forking-sequences method jointly encodes\nand decodes the entire time series across all FCDs, in a way mirroring time\nseries cross-validation. Since forking sequences remains largely unknown in the\nbroader neural forecasting community, in this work, we formalize the\nforking-sequences approach, and we make a case for its broader adoption. We\ndemonstrate three key benefits of forking-sequences: (i) more stable and\nconsistent gradient updates during training; (ii) reduced forecast variance\nthrough ensembling; and (iii) improved inference computational efficiency. We\nvalidate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and\nTourism competitions, showing improvements in forecast percentage change\nstability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,\nRNN, LSTM, CNN, and Transformer-based architectures, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faforking-sequences\u65b9\u6cd5\u6765\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u7f16\u7801\u548c\u89e3\u7801\u6240\u6709\u9884\u6d4b\u521b\u5efa\u65e5\u671f\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u867d\u7136\u6ce8\u91cd\u51c6\u786e\u6027\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u9884\u6d4b\u7a33\u5b9a\u6027\u8fd9\u4e00\u91cd\u8981\u6307\u6807\u3002\u4e0d\u7a33\u5b9a\u7684\u9884\u6d4b\u4f1a\u7834\u574f\u5229\u76ca\u76f8\u5173\u8005\u7684\u4fe1\u4efb\u5e76\u5e72\u6270\u4e0b\u6e38\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u91c7\u7528forking-sequences\u65b9\u6cd5\uff0c\u4e0e\u6807\u51c6\u7edf\u8ba1\u548c\u795e\u7ecf\u9884\u6d4b\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u7f16\u7801\u548c\u89e3\u7801\u6240\u6709\u9884\u6d4b\u521b\u5efa\u65e5\u671f\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u7c7b\u4f3c\u4e8e\u65f6\u95f4\u5e8f\u5217\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u572816\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cforking-sequences\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7a33\u5b9a\u6027\uff0cMLP\u3001RNN\u3001LSTM\u3001CNN\u548cTransformer\u67b6\u6784\u7684\u5e73\u5747\u9884\u6d4b\u767e\u5206\u6bd4\u53d8\u5316\u7a33\u5b9a\u6027\u5206\u522b\u63d0\u9ad8\u4e8628.8%\u300128.8%\u300137.9%\u300131.3%\u548c8.8%\u3002", "conclusion": "forking-sequences\u65b9\u6cd5\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u503c\u5f97\u5e7f\u6cdb\u91c7\u7528\u7684\u6280\u672f\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u51cf\u5c11\u9884\u6d4b\u65b9\u5dee\u5e76\u63d0\u5347\u63a8\u7406\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.04500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04500", "abs": "https://arxiv.org/abs/2510.04500", "authors": ["Linghao Kong", "Inimai Subramanian", "Yonadav Shavit", "Micah Adler", "Dan Alistarh", "Nir Shavit"], "title": "Expand Neurons, Not Parameters", "comment": "10 pages, 6 figures", "summary": "This work demonstrates how increasing the number of neurons in a network\nwithout increasing its number of non-zero parameters improves performance. We\nshow that this gain corresponds with a decrease in interference between\nmultiple features that would otherwise share the same neurons. To reduce such\nentanglement at a fixed non-zero parameter count, we introduce Fixed Parameter\nExpansion (FPE): replace a neuron with multiple children and partition the\nparent's weights disjointly across them, so that each child inherits a\nnon-overlapping subset of connections. On symbolic tasks, specifically Boolean\ncode problems, clause-aligned FPE systematically reduces polysemanticity\nmetrics and yields higher task accuracy. Notably, random splits of neuron\nweights approximate these gains, indicating that reduced collisions, not\nprecise assignment, are a primary driver. Consistent with the superposition\nhypothesis, the benefits of FPE grow with increasing interference: when\npolysemantic load is high, accuracy improvements are the largest. Transferring\nthese insights to real models (classifiers over CLIP embeddings and deeper\nmultilayer networks) we find that widening networks while maintaining a\nconstant non-zero parameter count consistently increases accuracy. These\nresults identify an interpretability-grounded mechanism to leverage width\nagainst superposition, improving performance without increasing the number of\nnon-zero parameters. Such a direction is well matched to modern accelerators,\nwhere memory movement of non-zero parameters, rather than raw compute, is the\ndominant bottleneck.", "AI": {"tldr": "\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u6269\u5c55(FPE)\u6280\u672f\uff0c\u5728\u4e0d\u589e\u52a0\u975e\u96f6\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u795e\u7ecf\u5143\u6570\u91cf\uff0c\u53ef\u4ee5\u51cf\u5c11\u7279\u5f81\u5e72\u6270\u5e76\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u6301\u975e\u96f6\u53c2\u6570\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u63a2\u7d22\u795e\u7ecf\u5143\u6570\u91cf\u589e\u52a0\u4e0e\u7279\u5f81\u5e72\u6270\u51cf\u5c11\u4e4b\u95f4\u7684\u5173\u7cfb", "method": "\u63d0\u51fa\u56fa\u5b9a\u53c2\u6570\u6269\u5c55(FPE)\uff1a\u5c06\u5355\u4e2a\u795e\u7ecf\u5143\u66ff\u6362\u4e3a\u591a\u4e2a\u5b50\u795e\u7ecf\u5143\uff0c\u5e76\u5c06\u7236\u795e\u7ecf\u5143\u7684\u6743\u91cd\u4e0d\u76f8\u4ea4\u5730\u5206\u914d\u7ed9\u5b50\u795e\u7ecf\u5143\uff0c\u6bcf\u4e2a\u5b50\u795e\u7ecf\u5143\u7ee7\u627f\u975e\u91cd\u53e0\u7684\u8fde\u63a5\u5b50\u96c6", "result": "\u5728\u5e03\u5c14\u4ee3\u7801\u95ee\u9898\u7b49\u7b26\u53f7\u4efb\u52a1\u4e0a\uff0cFPE\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u4e86\u591a\u4e49\u6027\u6307\u6807\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u7387\uff1b\u5728\u771f\u5b9e\u6a21\u578b\uff08CLIP\u5d4c\u5165\u5206\u7c7b\u5668\u548c\u6df1\u5c42\u7f51\u7edc\uff09\u4e0a\uff0c\u4fdd\u6301\u975e\u96f6\u53c2\u6570\u6570\u91cf\u4e0d\u53d8\u800c\u589e\u52a0\u7f51\u7edc\u5bbd\u5ea6\u6301\u7eed\u63d0\u5347\u4e86\u51c6\u786e\u7387", "conclusion": "FPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u539f\u7406\u7684\u673a\u5236\uff0c\u5229\u7528\u7f51\u7edc\u5bbd\u5ea6\u5bf9\u6297\u53e0\u52a0\u73b0\u8c61\uff0c\u5728\u4e0d\u589e\u52a0\u975e\u96f6\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\uff0c\u8fd9\u7279\u522b\u9002\u5408\u73b0\u4ee3\u52a0\u901f\u5668\u67b6\u6784"}}
{"id": "2510.04507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04507", "abs": "https://arxiv.org/abs/2510.04507", "authors": ["Min Wang", "Xin Li", "Ye He", "Yao-Hui Li", "Hasnaa Bennis", "Riashat Islam", "Mingzhong Wang"], "title": "Wavelet Predictive Representations for Non-Stationary Reinforcement Learning", "comment": null, "summary": "The real world is inherently non-stationary, with ever-changing factors, such\nas weather conditions and traffic flows, making it challenging for agents to\nadapt to varying environmental dynamics. Non-Stationary Reinforcement Learning\n(NSRL) addresses this challenge by training agents to adapt rapidly to\nsequences of distinct Markov Decision Processes (MDPs). However, existing NSRL\napproaches often focus on tasks with regularly evolving patterns, leading to\nlimited adaptability in highly dynamic settings. Inspired by the success of\nWavelet analysis in time series modeling, specifically its ability to capture\nsignal trends at multiple scales, we propose WISDOM to leverage wavelet-domain\npredictive task representations to enhance NSRL. WISDOM captures these\nmulti-scale features in evolving MDP sequences by transforming task\nrepresentation sequences into the wavelet domain, where wavelet coefficients\nrepresent both global trends and fine-grained variations of non-stationary\nchanges. In addition to the auto-regressive modeling commonly employed in time\nseries forecasting, we devise a wavelet temporal difference (TD) update\noperator to enhance tracking and prediction of MDP evolution. We theoretically\nprove the convergence of this operator and demonstrate policy improvement with\nwavelet task representations. Experiments on diverse benchmarks show that\nWISDOM significantly outperforms existing baselines in both sample efficiency\nand asymptotic performance, demonstrating its remarkable adaptability in\ncomplex environments characterized by non-stationary and stochastically\nevolving tasks.", "AI": {"tldr": "\u63d0\u51faWISDOM\u65b9\u6cd5\uff0c\u5229\u7528\u5c0f\u6ce2\u57df\u9884\u6d4b\u4efb\u52a1\u8868\u793a\u6765\u589e\u5f3a\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u6355\u6349MDP\u5e8f\u5217\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u63d0\u5347\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5177\u6709\u975e\u5e73\u7a33\u7279\u6027\uff0c\u73b0\u6709NSRL\u65b9\u6cd5\u5bf9\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u7684\u9002\u5e94\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u6355\u6349\u73af\u5883\u52a8\u6001\u53d8\u5316\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "method": "\u5c06\u4efb\u52a1\u8868\u793a\u5e8f\u5217\u8f6c\u6362\u5230\u5c0f\u6ce2\u57df\uff0c\u5229\u7528\u5c0f\u6ce2\u7cfb\u6570\u8868\u793a\u975e\u5e73\u7a33\u53d8\u5316\u7684\u5168\u5c40\u8d8b\u52bf\u548c\u7ec6\u7c92\u5ea6\u53d8\u5316\uff1b\u8bbe\u8ba1\u5c0f\u6ce2\u65f6\u5e8f\u5dee\u5206\u66f4\u65b0\u7b97\u5b50\u6765\u589e\u5f3aMDP\u6f14\u5316\u7684\u8ddf\u8e2a\u548c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWISDOM\u5728\u6837\u672c\u6548\u7387\u548c\u6e10\u8fd1\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u975e\u5e73\u7a33\u548c\u968f\u673a\u6f14\u5316\u4efb\u52a1\u7684\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u4efb\u52a1\u8868\u793a\u80fd\u6709\u6548\u589e\u5f3aNSRL\uff0cWISDOM\u65b9\u6cd5\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u6355\u6349\u548c\u7406\u8bba\u4fdd\u8bc1\u7684\u6536\u655b\u6027\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.04510", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04510", "abs": "https://arxiv.org/abs/2510.04510", "authors": ["Achim Eckerle", "Martin Spitznagel", "Janis Keuper"], "title": "Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows", "comment": null, "summary": "Accurate and fast urban noise prediction is pivotal for public health and for\nregulatory workflows in cities, where the Environmental Noise Directive\nmandates regular strategic noise maps and action plans, often needed in\npermission workflows, right-of-way allocation, and construction scheduling.\nPhysics-based solvers are too slow for such time-critical, iterative \"what-if\"\nstudies. We evaluate conditional Normalizing Flows (Full-Glow) for generating\nfor generating standards-compliant urban sound-pressure maps from 2D urban\nlayouts in real time per 256x256 map on a single RTX 4090), enabling\ninteractive exploration directly on commodity hardware. On datasets covering\nBaseline, Diffraction, and Reflection regimes, our model accelerates map\ngeneration by >2000 times over a reference solver while improving NLoS accuracy\nby up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE\nwith high structural fidelity. The model reproduces diffraction and\ninterference patterns and supports instant recomputation under source or\ngeometry changes, making it a practical engine for urban planning, compliance\nmapping, and operations (e.g., temporary road closures, night-work variance\nassessments).", "AI": {"tldr": "\u4f7f\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41(Full-Glow)\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u57ce\u5e02\u566a\u58f0\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\u52a0\u901f\u8d85\u8fc72000\u500d\uff0c\u5728\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u7cbe\u5ea6\u63d0\u534724%\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u57ce\u5e02\u89c4\u5212\u5e94\u7528\u3002", "motivation": "\u57ce\u5e02\u566a\u58f0\u9884\u6d4b\u5bf9\u516c\u5171\u5065\u5eb7\u548c\u76d1\u7ba1\u5de5\u4f5c\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u7684\u6c42\u89e3\u5668\u901f\u5ea6\u592a\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65f6\u95f4\u7d27\u8feb\u7684\u8fed\u4ee3\"\u5047\u8bbe\u5206\u6790\"\u7814\u7a76\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41(Full-Glow)\u6a21\u578b\uff0c\u4ece2D\u57ce\u5e02\u5e03\u5c40\u751f\u6210\u7b26\u5408\u6807\u51c6\u7684\u57ce\u5e02\u58f0\u538b\u5730\u56fe\uff0c\u5728\u5355\u5f20RTX 4090\u4e0a\u5b9e\u73b0256x256\u5730\u56fe\u7684\u5b9e\u65f6\u751f\u6210\u3002", "result": "\u6a21\u578b\u5728\u57fa\u7ebf\u3001\u884d\u5c04\u548c\u53cd\u5c04\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u53c2\u8003\u6c42\u89e3\u5668\u52a0\u901f\u8d85\u8fc72000\u500d\uff0c\u5728\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u7cbe\u5ea6\u6bd4\u5148\u524d\u6df1\u5ea6\u6a21\u578b\u63d0\u534724%\uff0c\u57fa\u7ebf\u975e\u89c6\u8ddd\u573a\u666f\u8fbe\u52300.65 dB MAE\uff0c\u5177\u6709\u9ad8\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u91cd\u73b0\u884d\u5c04\u548c\u5e72\u6d89\u6a21\u5f0f\uff0c\u652f\u6301\u5728\u58f0\u6e90\u6216\u51e0\u4f55\u53d8\u5316\u4e0b\u7684\u5373\u65f6\u91cd\u65b0\u8ba1\u7b97\uff0c\u6210\u4e3a\u57ce\u5e02\u89c4\u5212\u3001\u5408\u89c4\u5236\u56fe\u548c\u8fd0\u8425\u7684\u5b9e\u7528\u5f15\u64ce\u3002"}}
{"id": "2510.04522", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04522", "abs": "https://arxiv.org/abs/2510.04522", "authors": ["Yisen Gao", "Xingcheng Fu", "Qingyun Sun", "Jianxin Li", "Xianxian Li"], "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction", "comment": "Accepted by NeuIPS 2025", "summary": "Graph diffusion models have made significant progress in learning structured\ngraph data and have demonstrated strong potential for predictive tasks.\nExisting approaches typically embed node, edge, and graph-level features into a\nunified latent space, modeling prediction tasks including classification and\nregression as a form of conditional generation. However, due to the\nnon-Euclidean nature of graph data, features of different curvatures are\nentangled in the same latent space without releasing their geometric potential.\nTo address this issue, we aim to construt an ideal Riemannian diffusion model\nto capture distinct manifold signatures of complex graph data and learn their\ndistribution. This goal faces two challenges: numerical instability caused by\nexponential mapping during the encoding proces and manifold deviation during\ndiffusion generation. To address these challenges, we propose GeoMancer: a\nnovel Riemannian graph diffusion framework for both generation and prediction\ntasks. To mitigate numerical instability, we replace exponential mapping with\nan isometric-invariant Riemannian gyrokernel approach and decouple multi-level\nfeatures onto their respective task-specific manifolds to learn optimal\nrepresentations. To address manifold deviation, we introduce a\nmanifold-constrained diffusion method and a self-guided strategy for\nunconditional generation, ensuring that the generated data remains aligned with\nthe manifold signature. Extensive experiments validate the effectiveness of our\napproach, demonstrating superior performance across a variety of tasks.", "AI": {"tldr": "GeoMancer\u662f\u4e00\u4e2a\u9ece\u66fc\u56fe\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u7ea7\u7279\u5f81\u89e3\u8026\u5230\u7279\u5b9a\u6d41\u5f62\u4e0a\uff0c\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u4e0d\u540c\u66f2\u7387\u7279\u5f81\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u548c\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u6269\u6563\u6a21\u578b\u5c06\u8282\u70b9\u3001\u8fb9\u548c\u56fe\u7ea7\u7279\u5f81\u5d4c\u5165\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f46\u7531\u4e8e\u56fe\u6570\u636e\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u7279\u6027\uff0c\u4e0d\u540c\u66f2\u7387\u7684\u7279\u5f81\u5728\u8be5\u7a7a\u95f4\u4e2d\u7ea0\u7f20\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u5176\u51e0\u4f55\u6f5c\u529b\u3002", "method": "\u63d0\u51faGeoMancer\u6846\u67b6\uff1a1\uff09\u7528\u7b49\u8ddd\u4e0d\u53d8\u7684\u9ece\u66fc\u9640\u87ba\u6838\u65b9\u6cd5\u66ff\u4ee3\u6307\u6570\u6620\u5c04\uff0c\u7f13\u89e3\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\uff1b2\uff09\u5c06\u591a\u7ea7\u7279\u5f81\u89e3\u8026\u5230\u5404\u81ea\u4efb\u52a1\u7279\u5b9a\u7684\u6d41\u5f62\u4e0a\uff1b3\uff09\u5f15\u5165\u6d41\u5f62\u7ea6\u675f\u6269\u6563\u65b9\u6cd5\u548c\u81ea\u5f15\u5bfc\u7b56\u7565\uff0c\u786e\u4fdd\u751f\u6210\u6570\u636e\u4e0e\u6d41\u5f62\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "GeoMancer\u901a\u8fc7\u6784\u5efa\u7406\u60f3\u7684\u9ece\u66fc\u6269\u6563\u6a21\u578b\uff0c\u6210\u529f\u6355\u6349\u4e86\u590d\u6742\u56fe\u6570\u636e\u7684\u72ec\u7279\u6d41\u5f62\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u548c\u6d41\u5f62\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2510.04525", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04525", "abs": "https://arxiv.org/abs/2510.04525", "authors": ["Satoshi Hayakawa", "Yuhta Takida", "Masaaki Imaizumi", "Hiromi Wakaki", "Yuki Mitsufuji"], "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion", "comment": "23 pages", "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86MaskGIT\u91c7\u6837\u5668\u7684\u7406\u8bba\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\"moment\u91c7\u6837\u5668\"\uff0c\u5e76\u901a\u8fc7\u90e8\u5206\u7f13\u5b58\u548c\u6df7\u5408\u65b9\u6cd5\u4f18\u5316\u4e86\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u91c7\u6837\u52a0\u901f\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u5668\u3002", "method": "\u7406\u8bba\u5206\u6790MaskGIT\u91c7\u6837\u5668\uff0c\u63d0\u51famoment\u91c7\u6837\u5668\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u91c7\u7528\"\u9009\u62e9-\u7136\u540e\u91c7\u6837\"\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u90e8\u5206\u7f13\u5b58\u6280\u672f\u548c\u6df7\u5408\u63a2\u7d22-\u5229\u7528\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u9886\u57df\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u63a9\u7801\u6269\u6563\u91c7\u6837\u5668\u7684\u7406\u8bba\u7406\u89e3\u548c\u5b9e\u9645\u5b9e\u73b0\uff0c\u4e3a\u9ad8\u6548\u91c7\u6837\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.04543", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04543", "abs": "https://arxiv.org/abs/2510.04543", "authors": ["Elias Dubbeldam", "Reza Mohammadi", "Marit Schoonhoven", "S. Ilker Birbil"], "title": "Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions", "comment": "9 pages, 6 figures, submitted to position track NeurIPS 2025", "summary": "Despite recent progress, deep learning methods for tabular data still\nstruggle to compete with traditional tree-based models. A key challenge lies in\nmodeling complex, dataset-specific feature interactions that are central to\ntabular data. Graph-based tabular deep learning (GTDL) methods aim to address\nthis by representing features and their interactions as graphs. However,\nexisting methods predominantly optimize predictive accuracy, neglecting\naccurate modeling of the graph structure. This position paper argues that GTDL\nshould move beyond prediction-centric objectives and prioritize the explicit\nlearning and evaluation of feature interactions. Using synthetic datasets with\nknown ground-truth graph structures, we show that existing GTDL methods fail to\nrecover meaningful feature interactions. Moreover, enforcing the true\ninteraction structure improves predictive performance. This highlights the need\nfor GTDL methods to prioritize quantitative evaluation and accurate structural\nlearning. We call for a shift toward structure-aware modeling as a foundation\nfor building GTDL systems that are not only accurate but also interpretable,\ntrustworthy, and grounded in domain understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3b\u5f20\u56fe\u57fa\u8868\u683c\u6df1\u5ea6\u5b66\u4e60\u5e94\u4ece\u9884\u6d4b\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u7ed3\u6784\u611f\u77e5\u5efa\u6a21\uff0c\u5f3a\u8c03\u51c6\u786e\u5b66\u4e60\u7279\u5f81\u4ea4\u4e92\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u5f3a\u5236\u771f\u5b9e\u4ea4\u4e92\u7ed3\u6784\u80fd\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u57fa\u8868\u683c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5ffd\u89c6\u4e86\u51c6\u786e\u5efa\u6a21\u56fe\u7ed3\u6784\uff0c\u800c\u7279\u5f81\u4ea4\u4e92\u5bf9\u8868\u683c\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5177\u6709\u5df2\u77e5\u771f\u5b9e\u56fe\u7ed3\u6784\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709GTDL\u65b9\u6cd5\u6062\u590d\u7279\u5f81\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u5f3a\u5236\u771f\u5b9e\u4ea4\u4e92\u7ed3\u6784\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u73b0\u6709GTDL\u65b9\u6cd5\u65e0\u6cd5\u6062\u590d\u6709\u610f\u4e49\u7684\u7279\u5f81\u4ea4\u4e92\uff0c\u4f46\u5f3a\u5236\u771f\u5b9e\u4ea4\u4e92\u7ed3\u6784\u80fd\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "GTDL\u65b9\u6cd5\u9700\u8981\u4f18\u5148\u8003\u8651\u5b9a\u91cf\u8bc4\u4f30\u548c\u51c6\u786e\u7684\u7ed3\u6784\u5b66\u4e60\uff0c\u8f6c\u5411\u7ed3\u6784\u611f\u77e5\u5efa\u6a21\uff0c\u4ee5\u6784\u5efa\u4e0d\u4ec5\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u3001\u57fa\u4e8e\u9886\u57df\u7406\u89e3\u7684\u7cfb\u7edf\u3002"}}
{"id": "2510.04547", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04547", "abs": "https://arxiv.org/abs/2510.04547", "authors": ["Seunghyeon Kim", "Jinho Kim", "Taesun Yeom", "Wonpyo Park", "Kyuyeun Kim", "Jaeho Lee"], "title": "Post-training quantization of vision encoders needs prefixing registers", "comment": null, "summary": "Transformer-based vision encoders -- such as CLIP -- are central to\nmultimodal intelligence, powering applications from autonomous web agents to\nrobotic control. Since these applications often demand real-time processing of\nmassive visual data, reducing the inference cost of vision encoders is\ncritical. Post-training quantization offers a practical path, but remains\nchallenging even at 8-bit precision due to massive-scale activations (i.e.,\noutliers). In this work, we propose $\\textit{RegCache}$, a training-free\nalgorithm to mitigate outliers in vision encoders, enabling quantization with\nsignificantly smaller accuracy drops. The proposed RegCache introduces\noutlier-prone yet semantically meaningless prefix tokens to the target vision\nencoder, which prevents other tokens from having outliers. Notably, we observe\nthat outliers in vision encoders behave differently from those in language\nmodels, motivating two technical innovations: middle-layer prefixing and token\ndeletion. Experiments show that our method consistently improves the accuracy\nof quantized models across both text-supervised and self-supervised vision\nencoders.", "AI": {"tldr": "\u63d0\u51faRegCache\u8bad\u7ec3\u514d\u8d39\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5f02\u5e38\u503c\u503e\u5411\u4f46\u8bed\u4e49\u65e0\u610f\u4e49\u7684\u524d\u7f00\u4ee4\u724c\u6765\u7f13\u89e3\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u91cf\u5316", "motivation": "\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5728\u5b9e\u65f6\u5904\u7406\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u65f6\u9762\u4e34\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\u662f\u5b9e\u7528\u8def\u5f84\uff0c\u4f46\u7531\u4e8e\u5927\u89c4\u6a21\u6fc0\u6d3b\uff08\u5f02\u5e38\u503c\uff09\u7684\u5b58\u5728\uff0c\u5373\u4f7f\u57288\u4f4d\u7cbe\u5ea6\u4e0b\u4ecd\u5177\u6311\u6218\u6027", "method": "RegCache\u7b97\u6cd5\u5f15\u5165\u5f02\u5e38\u503c\u503e\u5411\u4f46\u8bed\u4e49\u65e0\u610f\u4e49\u7684\u524d\u7f00\u4ee4\u724c\u5230\u76ee\u6807\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u9632\u6b62\u5176\u4ed6\u4ee4\u724c\u4ea7\u751f\u5f02\u5e38\u503c\u3002\u57fa\u4e8e\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u5f02\u5e38\u503c\u4e0e\u8bed\u8a00\u6a21\u578b\u4e0d\u540c\u7684\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u4e2d\u95f4\u5c42\u524d\u7f00\u548c\u4ee4\u724c\u5220\u9664\u4e24\u9879\u6280\u672f\u521b\u65b0", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u76d1\u7763\u548c\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u91cf\u5316\u6a21\u578b\u7684\u51c6\u786e\u6027", "conclusion": "RegCache\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u7f13\u89e3\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.04555", "categories": ["cs.LG", "q-fin.TR", "68T05, 90C20, 91G60", "I.2.6; I.2.8; G.3"], "pdf": "https://arxiv.org/pdf/2510.04555", "abs": "https://arxiv.org/abs/2510.04555", "authors": ["Jian'an Zhang"], "title": "Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets", "comment": "32 pages including appendices; 5 figures. Primary subject class:\n  q-fin.TR. Cross-lists: cs.LG; q-fin.RM", "summary": "We introduce Tail-Safe, a deployability-oriented framework for derivatives\nhedging that unifies distributional, risk-sensitive reinforcement learning with\na white-box control-barrier-function (CBF) quadratic-program (QP) safety layer\ntailored to financial constraints. The learning component combines an IQN-based\ndistributional critic with a CVaR objective (IQN--CVaR--PPO) and a\nTail-Coverage Controller that regulates quantile sampling through temperature\ntilting and tail boosting to stabilize small-$\\alpha$ estimation. The safety\ncomponent enforces discrete-time CBF inequalities together with domain-specific\nconstraints -- ellipsoidal no-trade bands, box and rate limits, and a\nsign-consistency gate -- solved as a convex QP whose telemetry (active sets,\ntightness, rate utilization, gate scores, slack, and solver status) forms an\nauditable trail for governance. We provide guarantees of robust forward\ninvariance of the safe set under bounded model mismatch, a minimal-deviation\nprojection interpretation of the QP, a KL-to-DRO upper bound linking per-state\nKL regularization to worst-case CVaR, concentration and sample-complexity\nresults for the temperature-tilted CVaR estimator, and a CVaR trust-region\nimprovement inequality under KL limits, together with feasibility persistence\nunder expiry-aware tightening. Empirically, in arbitrage-free,\nmicrostructure-aware synthetic markets (SSVI $\\to$ Dupire $\\to$ VIX with\nABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading\ncentral performance and yields zero hard-constraint violations whenever the QP\nis feasible with zero slack. Telemetry is mapped to governance dashboards and\nincident workflows to support explainability and auditability. Limitations\ninclude reliance on synthetic data and simplified execution to isolate\nmethodological contributions.", "AI": {"tldr": "Tail-Safe\u662f\u4e00\u4e2a\u9762\u5411\u90e8\u7f72\u7684\u884d\u751f\u54c1\u5bf9\u51b2\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u5c42\uff0c\u901a\u8fc7IQN-CVaR-PPO\u7b97\u6cd5\u4f18\u5316\u5c3e\u90e8\u98ce\u9669\uff0c\u4f7f\u7528\u51f8\u4e8c\u6b21\u89c4\u5212\u5f3a\u5236\u6267\u884c\u91d1\u878d\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u884d\u751f\u54c1\u5bf9\u51b2\u65b9\u6cd5\u5728\u6781\u7aef\u5e02\u573a\u6761\u4ef6\u4e0b\u53ef\u80fd\u5931\u6548\uff0c\u9700\u8981\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u80fd\u529b\u4e0e\u91d1\u878d\u76d1\u7ba1\u7ea6\u675f\uff0c\u786e\u4fdd\u5728\u4f18\u5316\u98ce\u9669\u7684\u540c\u65f6\u6ee1\u8db3\u53ef\u90e8\u7f72\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u8981\u6c42\u3002", "method": "\u91c7\u7528IQN-CVaR-PPO\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u6e29\u5ea6\u503e\u659c\u548c\u5c3e\u90e8\u589e\u5f3a\u6280\u672f\u7a33\u5b9a\u5c0f\u03b1\u503cCVaR\u4f30\u8ba1\uff1b\u4f7f\u7528\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4e8c\u6b21\u89c4\u5212\u5b89\u5168\u5c42\u5f3a\u5236\u6267\u884c\u692d\u5706\u65e0\u4ea4\u6613\u5e26\u3001\u7bb1\u578b\u9650\u5236\u3001\u901f\u7387\u9650\u5236\u7b49\u91d1\u878d\u7ea6\u675f\u3002", "result": "\u5728\u65e0\u5957\u5229\u3001\u8003\u8651\u5fae\u89c2\u7ed3\u6784\u7684\u5408\u6210\u5e02\u573a\u4e2d\uff0cTail-Safe\u6539\u5584\u4e86\u5de6\u5c3e\u98ce\u9669\u800c\u4e0d\u5f71\u54cd\u4e2d\u5fc3\u6027\u80fd\uff0c\u5f53QP\u53ef\u884c\u4e14\u65e0\u677e\u5f1b\u65f6\u5b9e\u73b0\u96f6\u786c\u7ea6\u675f\u8fdd\u53cd\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u53ef\u5ba1\u8ba1\u8f68\u8ff9\u3002", "conclusion": "Tail-Safe\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u4e0e\u91d1\u878d\u5b89\u5168\u7ea6\u675f\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u5408\u6210\u6570\u636e\u548c\u7b80\u5316\u6267\u884c\u73af\u5883\u662f\u5f53\u524d\u5c40\u9650\u3002"}}
{"id": "2510.04559", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04559", "abs": "https://arxiv.org/abs/2510.04559", "authors": ["Mohsen Amiri", "V Venktesh", "Sindri Magn\u00fasson"], "title": "Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems", "comment": "6 pages", "summary": "This paper investigates the identification of the top-m user-scheduling sets\nin multi-user MIMO downlink, which is cast as a combinatorial pure-exploration\nproblem in stochastic linear bandits. Because the action space grows\nexponentially, exhaustive search is infeasible. We therefore adopt a linear\nutility model to enable efficient exploration and reliable selection of\npromising user subsets. We introduce a gap-index framework that maintains a\nshortlist of current estimates of champion arms (top-m sets) and a rotating\nshortlist of challenger arms that pose the greatest threat to the champions.\nThis design focuses on measurements that yield the most informative\ngap-index-based comparisons, resulting in significant reductions in runtime and\ncomputation compared to state-of-the-art linear bandit methods, with high\nidentification accuracy. The method also exposes a tunable trade-off between\nspeed and accuracy. Simulations on a realistic OFDM downlink show that\nshortlist-driven pure exploration makes online, measurement-efficient\nsubcarrier selection practical for AI-enabled communication systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u6548\u7528\u6a21\u578b\u548c\u95f4\u9699\u7d22\u5f15\u6846\u67b6\u7684top-m\u7528\u6237\u8c03\u5ea6\u96c6\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u7528\u6237MIMO\u4e0b\u884c\u94fe\u8def\u4e2d\u7684\u7ec4\u5408\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u4e86\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u591a\u7528\u6237MIMO\u4e0b\u884c\u94fe\u8def\u4e2d\u8bc6\u522btop-m\u7528\u6237\u8c03\u5ea6\u96c6\u662f\u4e00\u4e2a\u7ec4\u5408\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u5448\u6307\u6570\u589e\u957f\uff0c\u7a77\u4e3e\u641c\u7d22\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u6548\u7528\u6a21\u578b\uff0c\u5f15\u5165\u95f4\u9699\u7d22\u5f15\u6846\u67b6\uff0c\u7ef4\u62a4\u5f53\u524d\u51a0\u519b\u81c2\uff08top-m\u96c6\uff09\u7684\u77ed\u5217\u8868\u548c\u65cb\u8f6c\u6311\u6218\u8005\u81c2\u77ed\u5217\u8868\uff0c\u4e13\u6ce8\u4e8e\u4ea7\u751f\u6700\u6709\u4fe1\u606f\u91cf\u7684\u57fa\u4e8e\u95f4\u9699\u7d22\u5f15\u7684\u6bd4\u8f83\u6d4b\u91cf\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u7ebf\u6027bandit\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5e76\u66b4\u9732\u4e86\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u53ef\u8c03\u6743\u8861\u3002", "conclusion": "\u77ed\u5217\u8868\u9a71\u52a8\u7684\u7eaf\u63a2\u7d22\u4f7f\u5f97AI\u4f7f\u80fd\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5728\u7ebf\u3001\u6d4b\u91cf\u9ad8\u6548\u5b50\u8f7d\u6ce2\u9009\u62e9\u53d8\u5f97\u5b9e\u7528\u53ef\u884c\u3002"}}
{"id": "2510.04563", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04563", "abs": "https://arxiv.org/abs/2510.04563", "authors": ["Jinyang Jiang", "Bernd Heidergott", "Jiaqiao Hu", "Yijie Peng"], "title": "Stochastic Approximation Methods for Distortion Risk Measure Optimization", "comment": null, "summary": "Distortion Risk Measures (DRMs) capture risk preferences in decision-making\nand serve as general criteria for managing uncertainty. This paper proposes\ngradient descent algorithms for DRM optimization based on two dual\nrepresentations: the Distortion-Measure (DM) form and Quantile-Function (QF)\nform. The DM-form employs a three-timescale algorithm to track quantiles,\ncompute their gradients, and update decision variables, utilizing the\nGeneralized Likelihood Ratio and kernel-based density estimation. The QF-form\nprovides a simpler two-timescale approach that avoids the need for complex\nquantile gradient estimation. A hybrid form integrates both approaches,\napplying the DM-form for robust performance around distortion function jumps\nand the QF-form for efficiency in smooth regions. Proofs of strong convergence\nand convergence rates for the proposed algorithms are provided. In particular,\nthe DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form\nattains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their\neffectiveness and demonstrate substantial improvements over baselines in robust\nportfolio selection tasks. The method's scalability is further illustrated\nthrough integration into deep reinforcement learning. Specifically, a DRM-based\nProximal Policy Optimization algorithm is developed and applied to\nmulti-echelon dynamic inventory management, showcasing its practical\napplicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e24\u79cd\u5bf9\u5076\u8868\u793a\u7684\u5931\u771f\u98ce\u9669\u5ea6\u91cf\u4f18\u5316\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff1aDM\u5f62\u5f0f\u548cQF\u5f62\u5f0f\uff0c\u4ee5\u53ca\u7ed3\u5408\u4e24\u8005\u7684\u6df7\u5408\u5f62\u5f0f\uff0c\u5728\u6295\u8d44\u7ec4\u5408\u9009\u62e9\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5931\u771f\u98ce\u9669\u5ea6\u91cf\u5728\u51b3\u7b56\u4e2d\u6355\u6349\u98ce\u9669\u504f\u597d\uff0c\u662f\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u901a\u7528\u6807\u51c6\uff0c\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "\u57fa\u4e8e\u5931\u771f-\u5ea6\u91cf\u5f62\u5f0f\u548c\u5206\u4f4d\u6570\u51fd\u6570\u5f62\u5f0f\u7684\u5bf9\u5076\u8868\u793a\uff0c\u5206\u522b\u5f00\u53d1\u4e09\u65f6\u95f4\u5c3a\u5ea6\u548c\u4e24\u65f6\u95f4\u5c3a\u5ea6\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u5f62\u5f0f\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "result": "DM\u5f62\u5f0f\u8fbe\u5230O(k^{-4/7})\u7684\u6700\u4f18\u6536\u655b\u7387\uff0cQF\u5f62\u5f0f\u8fbe\u5230\u66f4\u5feb\u7684O(k^{-2/3})\u6536\u655b\u7387\uff0c\u5728\u6295\u8d44\u7ec4\u5408\u9009\u62e9\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u6269\u5c55\u5230\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u52a8\u6001\u5e93\u5b58\u7ba1\u7406\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04567", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04567", "abs": "https://arxiv.org/abs/2510.04567", "authors": ["Weishuo Ma", "Yanbo Wang", "Xiyuan Wang", "Lei Zou", "Muhan Zhang"], "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning", "comment": null, "summary": "Graph Neural Networks (GNNs) are powerful tools for precessing relational\ndata but often struggle to generalize to unseen graphs, giving rise to the\ndevelopment of Graph Foundational Models (GFMs). However, current GFMs are\nchallenged by the extreme heterogeneity of graph data, where each graph can\npossess a unique feature space, label set, and topology. To address this, two\nmain paradigms have emerged. The first leverages Large Language Models (LLMs),\nbut is fundamentally text-dependent, thus struggles to handle the numerical\nfeatures in vast graphs. The second pre-trains a structure-based model, but the\nadaptation to new tasks typically requires a costly, per-graph tuning stage,\ncreating a critical efficiency bottleneck. In this work, we move beyond these\nlimitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning\n\\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free\narchitecture. GILT introduces a novel token-based framework for in-context\nlearning (ICL) on graphs, reframing classification tasks spanning node, edge\nand graph levels in a unified framework. This mechanism is the key to handling\nheterogeneity, as it is designed to operate on generic numerical features.\nFurther, its ability to understand class semantics dynamically from the context\nenables tuning-free adaptation. Comprehensive experiments show that GILT\nachieves stronger few-shot performance with significantly less time than\nLLM-based or tuning-based baselines, validating the effectiveness of our\napproach.", "AI": {"tldr": "GILT\u662f\u4e00\u4e2a\u57fa\u4e8etoken\u7684\u56fe\u5185\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700LLM\u548c\u5fae\u8c03\uff0c\u80fd\u7edf\u4e00\u5904\u7406\u8282\u70b9\u3001\u8fb9\u548c\u56fe\u7ea7\u522b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u6709\u6548\u5e94\u5bf9\u56fe\u6570\u636e\u7684\u5f02\u6784\u6027\u3002", "motivation": "\u89e3\u51b3\u56fe\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u56fe\u6570\u636e\u65f6\u7684\u6311\u6218\uff0c\u5305\u62ec\u7279\u5f81\u7a7a\u95f4\u3001\u6807\u7b7e\u96c6\u548c\u62d3\u6251\u7ed3\u6784\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u5bf9\u6587\u672c\u4f9d\u8d56\u6216\u9700\u8981\u6602\u8d35\u5fae\u8c03\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8etoken\u7684\u56fe\u5185\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u56fe\u5206\u7c7b\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u7edf\u4e00\u683c\u5f0f\uff0c\u5229\u7528\u52a8\u6001\u7406\u89e3\u7c7b\u522b\u8bed\u4e49\u7684\u80fd\u529b\u5b9e\u73b0\u514d\u8c03\u4f18\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGILT\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u6027\u80fd\u66f4\u5f3a\uff0c\u4e14\u6240\u9700\u65f6\u95f4\u663e\u8457\u5c11\u4e8e\u57fa\u4e8eLLM\u6216\u9700\u8981\u5fae\u8c03\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GILT\u901a\u8fc7\u514dLLM\u548c\u514d\u5fae\u8c03\u7684\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e3a\u56fe\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04573", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04573", "abs": "https://arxiv.org/abs/2510.04573", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Nicklas Majamaki", "Navdeep Jaitly", "Yi-An Ma", "Lianhui Qin"], "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning", "comment": null, "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.", "AI": {"tldr": "LaDiR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edfLLM\u7684\u81ea\u56de\u5f52\u89e3\u7801\u9650\u5236\u4e86\u6574\u4f53\u6027\u91cd\u65b0\u5ba1\u89c6\u548c\u7ec6\u5316\u65e9\u671f\u63a8\u7406\u6b65\u9aa4\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u63a8\u7406\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u89e3\u51b3\u65b9\u6848\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u4f7f\u7528VAE\u6784\u5efa\u7ed3\u6784\u5316\u6f5c\u5728\u63a8\u7406\u7a7a\u95f4\uff0c\u5c06\u6587\u672c\u63a8\u7406\u6b65\u9aa4\u7f16\u7801\u4e3a\u601d\u60f3\u6807\u8bb0\u5757\uff1b\u7136\u540e\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5757\u7ea7\u53cc\u5411\u6ce8\u610f\u529b\u63a9\u7801\u5b66\u4e60\u53bb\u566a\u6f5c\u5728\u601d\u60f3\u6807\u8bb0\uff0c\u652f\u6301\u957f\u7a0b\u63a8\u7406\u548c\u8fed\u4ee3\u7ec6\u5316\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLaDiR\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u56de\u5f52\u3001\u57fa\u4e8e\u6269\u6563\u548c\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "LaDiR\u4e3a\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u5728\u6269\u6563\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.04576", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04576", "abs": "https://arxiv.org/abs/2510.04576", "authors": ["Yuhta Takida", "Satoshi Hayakawa", "Takashi Shibuya", "Masaaki Imaizumi", "Naoki Murata", "Bac Nguyen", "Toshimitsu Uesaka", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator", "comment": "24 pages with 9 figures", "summary": "Deep generative models have made significant advances in generating complex\ncontent, yet conditional generation remains a fundamental challenge. Existing\nconditional generative adversarial networks often struggle to balance the dual\nobjectives of assessing authenticity and conditional alignment of input samples\nwithin their conditional discriminators. To address this, we propose a novel\ndiscriminator design that integrates three key capabilities: unconditional\ndiscrimination, matching-aware supervision to enhance alignment sensitivity,\nand adaptive weighting to dynamically balance all objectives. Specifically, we\nintroduce Sum of Naturalness and Alignment (SONA), which employs separate\nprojections for naturalness (authenticity) and alignment in the final layer\nwith an inductive bias, supported by dedicated objective functions and an\nadaptive weighting mechanism. Extensive experiments on class-conditional\ngeneration tasks show that \\ours achieves superior sample quality and\nconditional alignment compared to state-of-the-art methods. Furthermore, we\ndemonstrate its effectiveness in text-to-image generation, confirming the\nversatility and robustness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSONA\u7684\u65b0\u578b\u5224\u522b\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u79bb\u81ea\u7136\u6027\u548c\u5bf9\u9f50\u6027\u8bc4\u4f30\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u5728\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6837\u672c\u8d28\u91cf\u548c\u6761\u4ef6\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5728\u6761\u4ef6\u5224\u522b\u5668\u4e2d\u96be\u4ee5\u5e73\u8861\u771f\u5b9e\u6027\u548c\u6761\u4ef6\u5bf9\u9f50\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5224\u522b\u5668\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faSONA\u65b9\u6cd5\uff0c\u5728\u5224\u522b\u5668\u6700\u540e\u5c42\u4f7f\u7528\u5206\u79bb\u7684\u81ea\u7136\u6027\u548c\u5bf9\u9f50\u6027\u6295\u5f71\uff0c\u7ed3\u5408\u5339\u914d\u611f\u77e5\u76d1\u7763\u548c\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\u6765\u52a8\u6001\u5e73\u8861\u6240\u6709\u76ee\u6807\u3002", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSONA\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f18\u7684\u6837\u672c\u8d28\u91cf\u548c\u6761\u4ef6\u5bf9\u9f50\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SONA\u65b9\u6cd5\u5177\u6709\u591a\u529f\u80fd\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04579", "categories": ["cs.LG", "math.MG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04579", "abs": "https://arxiv.org/abs/2510.04579", "authors": ["Cl\u00e9ment Bonet", "Elsa Cazelles", "Lucas Drumetz", "Nicolas Courty"], "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing", "comment": null, "summary": "The Busemann function has recently found much interest in a variety of\ngeometric machine learning problems, as it naturally defines projections onto\ngeodesic rays of Riemannian manifolds and generalizes the notion of\nhyperplanes. As several sources of data can be conveniently modeled as\nprobability distributions, it is natural to study this function in the\nWasserstein space, which carries a rich formal Riemannian structure induced by\nOptimal Transport metrics. In this work, we investigate the existence and\ncomputation of Busemann functions in Wasserstein space, which admits geodesic\nrays. We establish closed-form expressions in two important cases:\none-dimensional distributions and Gaussian measures. These results enable\nexplicit projection schemes for probability distributions on $\\mathbb{R}$,\nwhich in turn allow us to define novel Sliced-Wasserstein distances over\nGaussian mixtures and labeled datasets. We demonstrate the efficiency of those\noriginal schemes on synthetic datasets as well as transfer learning problems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Wasserstein\u7a7a\u95f4\u4e2d\u7684Busemann\u51fd\u6570\uff0c\u5728\u4e00\u7ef4\u5206\u5e03\u548c\u9ad8\u65af\u6d4b\u5ea6\u60c5\u51b5\u4e0b\u5efa\u7acb\u4e86\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684Sliced-Wasserstein\u8ddd\u79bb\u548c\u6295\u5f71\u65b9\u6848\u3002", "motivation": "Busemann\u51fd\u6570\u5728\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u80fd\u5b9a\u4e49\u9ece\u66fc\u6d41\u5f62\u4e0a\u6d4b\u5730\u7ebf\u7684\u6295\u5f71\u3002\u7531\u4e8e\u8bb8\u591a\u6570\u636e\u6e90\u53ef\u4ee5\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u7814\u7a76\u5176\u5728Wasserstein\u7a7a\u95f4\u4e2d\u7684\u6027\u8d28\u5f88\u6709\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5ea6\u91cf\u8bf1\u5bfc\u7684\u9ece\u66fc\u7ed3\u6784\uff0c\u7814\u7a76Wasserstein\u7a7a\u95f4\u4e2dBusemann\u51fd\u6570\u7684\u5b58\u5728\u6027\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u4e00\u7ef4\u5206\u5e03\u548c\u9ad8\u65af\u6d4b\u5ea6\u4e24\u79cd\u60c5\u51b5\u3002", "result": "\u5728\u4e00\u7ef4\u5206\u5e03\u548c\u9ad8\u65af\u6d4b\u5ea6\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e86Busemann\u51fd\u6570\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684Sliced-Wasserstein\u8ddd\u79bb\u548c\u6295\u5f71\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4e3a\u6982\u7387\u5206\u5e03\u7684\u6295\u5f71\u548c\u8ddd\u79bb\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2510.04583", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04583", "abs": "https://arxiv.org/abs/2510.04583", "authors": ["Carlo Kneissl", "Christopher B\u00fclte", "Philipp Scholl", "Gitta Kutyniok"], "title": "Improved probabilistic regression using diffusion models", "comment": null, "summary": "Probabilistic regression models the entire predictive distribution of a\nresponse variable, offering richer insights than classical point estimates and\ndirectly allowing for uncertainty quantification. While diffusion-based\ngenerative models have shown remarkable success in generating complex,\nhigh-dimensional data, their usage in general regression tasks often lacks\nuncertainty-related evaluation and remains limited to domain-specific\napplications. We propose a novel diffusion-based framework for probabilistic\nregression that learns predictive distributions in a nonparametric way. More\nspecifically, we propose to model the full distribution of the diffusion noise,\nenabling adaptation to diverse tasks and enhanced uncertainty quantification.\nWe investigate different noise parameterizations, analyze their trade-offs, and\nevaluate our framework across a broad range of regression tasks, covering low-\nand high-dimensional settings. For several experiments, our approach shows\nsuperior performance against existing baselines, while delivering calibrated\nuncertainty estimates, demonstrating its versatility as a tool for\nprobabilistic prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6982\u7387\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u65b9\u5f0f\u5b66\u4e60\u9884\u6d4b\u5206\u5e03\uff0c\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u4efb\u52a1\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u6982\u7387\u56de\u5f52\u80fd\u591f\u63d0\u4f9b\u5b8c\u6574\u7684\u9884\u6d4b\u5206\u5e03\uff0c\u6bd4\u4f20\u7edf\u70b9\u4f30\u8ba1\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6d1e\u5bdf\u529b\u5e76\u76f4\u63a5\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u867d\u7136\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e00\u822c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u4e14\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u3002", "method": "\u63d0\u51fa\u5efa\u6a21\u6269\u6563\u566a\u58f0\u7684\u5b8c\u6574\u5206\u5e03\uff0c\u5b9e\u73b0\u975e\u53c2\u6570\u6982\u7387\u56de\u5f52\u3002\u7814\u7a76\u4e86\u4e0d\u540c\u7684\u566a\u58f0\u53c2\u6570\u5316\u65b9\u6cd5\u5e76\u5206\u6790\u5176\u6743\u8861\uff0c\u5728\u4f4e\u7ef4\u548c\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6846\u67b6\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u57fa\u7ebf\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u6982\u7387\u9884\u6d4b\u5de5\u5177\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u6269\u6563\u57fa\u6982\u7387\u56de\u5f52\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u9884\u6d4b\u5206\u5e03\uff0c\u9002\u5e94\u591a\u6837\u5316\u56de\u5f52\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04606", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04606", "abs": "https://arxiv.org/abs/2510.04606", "authors": ["Alexandre Galashov", "Natha\u00ebl Da Costa", "Liyuan Xu", "Philipp Hennig", "Arthur Gretton"], "title": "Closed-Form Last Layer Optimization", "comment": null, "summary": "Neural networks are typically optimized with variants of stochastic gradient\ndescent. Under a squared loss, however, the optimal solution to the linear last\nlayer weights is known in closed-form. We propose to leverage this during\noptimization, treating the last layer as a function of the backbone parameters,\nand optimizing solely for these parameters. We show this is equivalent to\nalternating between gradient descent steps on the backbone and closed-form\nupdates on the last layer. We adapt the method for the setting of stochastic\ngradient descent, by trading off the loss on the current batch against the\naccumulated information from previous batches. Further, we prove that, in the\nNeural Tangent Kernel regime, convergence of this method to an optimal solution\nis guaranteed. Finally, we demonstrate the effectiveness of our approach\ncompared with standard SGD on a squared loss in several supervised tasks --\nboth regression and classification -- including Fourier Neural Operators and\nInstrumental Variable Regression.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u6700\u540e\u4e00\u5c42\u7ebf\u6027\u6743\u91cd\u7528\u95ed\u5f0f\u89e3\u8868\u793a\uff0c\u4ec5\u4f18\u5316\u9aa8\u5e72\u7f51\u7edc\u53c2\u6570\uff0c\u5728\u5e73\u65b9\u635f\u5931\u4e0b\u6bd4\u6807\u51c6SGD\u66f4\u6709\u6548\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u53d8\u4f53\u4f18\u5316\uff0c\u4f46\u5728\u5e73\u65b9\u635f\u5931\u4e0b\uff0c\u7ebf\u6027\u6700\u540e\u4e00\u5c42\u6743\u91cd\u7684\u6700\u4f18\u89e3\u6709\u95ed\u5f0f\u89e3\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e00\u7279\u6027\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "method": "\u5c06\u6700\u540e\u4e00\u5c42\u89c6\u4e3a\u9aa8\u5e72\u53c2\u6570\u7684\u51fd\u6570\uff0c\u4ec5\u4f18\u5316\u9aa8\u5e72\u53c2\u6570\uff0c\u7b49\u4ef7\u4e8e\u5728\u9aa8\u5e72\u4e0a\u505a\u68af\u5ea6\u4e0b\u964d\u6b65\u957f\u548c\u5728\u6700\u540e\u4e00\u5c42\u505a\u95ed\u5f0f\u66f4\u65b0\u4ea4\u66ff\u8fdb\u884c\u3002\u4e3a\u9002\u5e94SGD\u8bbe\u7f6e\uff0c\u5728\u5f53\u524d\u4f4d\u7f6e\u635f\u5931\u548c\u5148\u524d\u6279\u6b21\u7d2f\u79ef\u4fe1\u606f\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "result": "\u5728\u795e\u7ecf\u6b63\u5207\u6838\u673a\u5236\u4e0b\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002\u5728\u591a\u4e2a\u76d1\u7763\u4efb\u52a1\uff08\u5305\u62ec\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c\u5de5\u5177\u53d8\u91cf\u56de\u5f52\uff09\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6SGD\u5728\u5e73\u65b9\u635f\u5931\u4e0b\u8868\u73b0\u66f4\u6709\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u6700\u540e\u4e00\u5c42\u95ed\u5f0f\u89e3\u7279\u6027\uff0c\u5728\u4fdd\u6301\u6536\u655b\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u5e73\u65b9\u635f\u5931\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2510.04618", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04618", "abs": "https://arxiv.org/abs/2510.04618", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "comment": null, "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.", "AI": {"tldr": "ACE\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u751f\u6210\u3001\u53cd\u601d\u548c\u7b56\u5c55\u8fc7\u7a0b\uff0c\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u4e0d\u65ad\u6f14\u8fdb\u7684\u5267\u672c\uff0c\u6709\u6548\u9632\u6b62\u4e0a\u4e0b\u6587\u5d29\u6e83\u5e76\u4fdd\u6301\u8be6\u7ec6\u77e5\u8bc6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e0a\u4e0b\u6587\u9002\u5e94\u65b9\u6cd5\u4e2d\u7684\u7b80\u6d01\u6027\u504f\u89c1\u548c\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\uff0c\u524d\u8005\u4f1a\u4e22\u5931\u9886\u57df\u6d1e\u5bdf\uff0c\u540e\u8005\u5728\u8fed\u4ee3\u91cd\u5199\u4e2d\u9010\u6e10\u4fb5\u8680\u7ec6\u8282\u3002", "method": "ACE\u6846\u67b6\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u4e0d\u65ad\u6f14\u8fdb\u7684\u5267\u672c\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7684\u751f\u6210\u3001\u53cd\u601d\u548c\u7b56\u5c55\u8fc7\u7a0b\u6765\u79ef\u7d2f\u3001\u4f18\u5316\u548c\u7ec4\u7ec7\u7b56\u7565\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u589e\u91cf\u66f4\u65b0\u9632\u6b62\u4e0a\u4e0b\u6587\u5d29\u6e83\u3002", "result": "\u5728\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE\u5728\u79bb\u7ebf\uff08\u5982\u7cfb\u7edf\u63d0\u793a\uff09\u548c\u5728\u7ebf\uff08\u5982\u4ee3\u7406\u8bb0\u5fc6\uff09\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1a\u4ee3\u7406\u4efb\u52a1\u63d0\u534710.6%\uff0c\u91d1\u878d\u4efb\u52a1\u63d0\u53478.6%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9002\u5e94\u5ef6\u8fdf\u548c\u90e8\u7f72\u6210\u672c\u3002", "conclusion": "\u5168\u9762\u4e14\u4e0d\u65ad\u6f14\u8fdb\u7684\u4e0a\u4e0b\u6587\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u81ea\u6211\u6539\u8fdb\u7684LLM\u7cfb\u7edf\uff0c\u4e14\u5f00\u9500\u8f83\u4f4e\u3002"}}
{"id": "2510.04622", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04622", "abs": "https://arxiv.org/abs/2510.04622", "authors": ["Youngjoon Lee", "Seongmin Cho", "Yehhyun Jo", "Jinu Gong", "Hyunjoo Jenny Lee", "Joonhyuk Kang"], "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI", "comment": "Under Review", "summary": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u7684\u5408\u6210\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u80fd\u9ad8\u4fdd\u771f\u590d\u5236EEG\u548cEMG\u7b49\u590d\u6742\u7535\u751f\u7406\u4fe1\u53f7\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u548c\u5927\u91cf\u8d44\u6e90\u9700\u6c42\u9650\u5236\u4e86\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217AI\u7684\u53d1\u5c55\uff0c\u5bfc\u81f4\u6570\u636e\u9700\u6c42\u4e0e\u53ef\u8bbf\u95ee\u6027\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u9884\u6d4b\u6a21\u578b\u751f\u6210\u5408\u6210\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u51c6\u786e\u590d\u5236\u590d\u6742\u7684\u7535\u751f\u7406\u4fe1\u53f7\u5982EEG\u548cEMG\u3002", "result": "\u5408\u6210\u6570\u636e\u4fdd\u6301\u4e86\u771f\u5b9e\u6570\u636e\u7684\u5173\u952e\u65f6\u95f4\u548c\u9891\u8c31\u7279\u6027\uff0c\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u6548\u66ff\u4ee3\u54c1\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5173\u952e\u751f\u7269\u533b\u5b66\u7279\u5f81\u7684\u540c\u65f6\uff0c\u4e3a\u5404\u79cd\u5e94\u7528\u63d0\u4f9b\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u96c6\u6210\u5230\u5f00\u6e90\u5b58\u50a8\u5e93\u4e2d\uff0c\u5927\u5927\u6269\u5c55\u4e86AI\u9a71\u52a8\u751f\u7269\u533b\u5b66\u7814\u7a76\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.04626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04626", "abs": "https://arxiv.org/abs/2510.04626", "authors": ["Mohamed Ayoub Ben Ayad", "Michael Dinzinger", "Kanishka Ghosh Dastidar", "Jelena Mitrovic", "Michael Granitzer"], "title": "Compressed Concatenation of Small Embedding Models", "comment": null, "summary": "Embedding models are central to dense retrieval, semantic search, and\nrecommendation systems, but their size often makes them impractical to deploy\nin resource-constrained environments such as browsers or edge devices. While\nsmaller embedding models offer practical advantages, they typically\nunderperform compared to their larger counterparts. To bridge this gap, we\ndemonstrate that concatenating the raw embedding vectors of multiple small\nmodels can outperform a single larger baseline on standard retrieval\nbenchmarks. To overcome the resulting high dimensionality of naive\nconcatenation, we introduce a lightweight unified decoder trained with a\nMatryoshka Representation Learning (MRL) loss. This decoder maps the\nhigh-dimensional joint representation to a low-dimensional space, preserving\nmost of the original performance without fine-tuning the base models. We also\nshow that while concatenating more base models yields diminishing gains, the\nrobustness of the decoder's representation under compression and quantization\nimproves. Our experiments show that, on a subset of MTEB retrieval tasks, our\nconcat-encode-quantize pipeline recovers 89\\% of the original performance with\na 48x compression factor when the pipeline is applied to a concatenation of\nfour small embedding models.", "AI": {"tldr": "\u901a\u8fc7\u8fde\u63a5\u591a\u4e2a\u5c0f\u578b\u5d4c\u5165\u6a21\u578b\u7684\u539f\u59cb\u5411\u91cf\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u538b\u7f29\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u5927\u578b\u5d4c\u5165\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u6d4f\u89c8\u5668\u6216\u8fb9\u7f18\u8bbe\u5907\uff09\u4e2d\u90e8\u7f72\u56f0\u96be\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8fde\u63a5\u591a\u4e2a\u5c0f\u578b\u6a21\u578b\u7684\u539f\u59cb\u5d4c\u5165\u5411\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eMatryoshka\u8868\u793a\u5b66\u4e60\u635f\u5931\u7684\u8f7b\u91cf\u7ea7\u7edf\u4e00\u89e3\u7801\u5668\u5c06\u9ad8\u7ef4\u8054\u5408\u8868\u793a\u6620\u5c04\u5230\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728MTEB\u68c0\u7d22\u4efb\u52a1\u5b50\u96c6\u4e0a\uff0cconcat-encode-quantize\u6d41\u6c34\u7ebf\u572848\u500d\u538b\u7f29\u56e0\u5b50\u4e0b\u6062\u590d\u4e86\u539f\u59cb\u6027\u80fd\u768489%\uff0c\u8fde\u63a5\u66f4\u591a\u57fa\u7840\u6a21\u578b\u5e26\u6765\u9012\u51cf\u6536\u76ca\u4f46\u63d0\u9ad8\u4e86\u538b\u7f29\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u8fde\u63a5\u591a\u4e2a\u5c0f\u578b\u5d4c\u5165\u6a21\u578b\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u538b\u7f29\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04646", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04646", "abs": "https://arxiv.org/abs/2510.04646", "authors": ["Johanna Sommer", "John Rachwan", "Nils Fleischmann", "Stephan G\u00fcnnemann", "Bertrand Charpentier"], "title": "Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation", "comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025", "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u901f\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u6c42\u89e3\u5668\u6b65\u9aa4\u95f4\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\uff0c\u5c06\u5206\u5b50\u51e0\u4f55\u751f\u6210\u7684\u63a8\u7406\u65f6\u95f4\u51cf\u5c112\u500d\uff0c\u4e0e\u5176\u4ed6\u4f18\u5316\u7ed3\u5408\u53ef\u8fbe7\u500d\u52a0\u901f\u3002", "motivation": "\u6d41\u5339\u914d\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5b50\u51e0\u4f55\u7ed3\u6784\uff0c\u4f46\u63a8\u7406\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u6570\u767e\u6b21\u7f51\u7edc\u8bc4\u4f30\uff0c\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u6837\u5927\u91cf\u5206\u5b50\u5019\u9009\u8005\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u76f4\u63a5\u5728SE(3)-\u7b49\u53d8\u9aa8\u5e72\u7f51\u7edc\u4e0a\u64cd\u4f5c\u7684\u65e0\u8bad\u7ec3\u7f13\u5b58\u7b56\u7565\uff0c\u9884\u6d4b\u6c42\u89e3\u5668\u6b65\u9aa4\u95f4\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\uff0c\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u517c\u5bb9\uff0c\u4e14\u4e0e\u73b0\u6709\u57fa\u4e8e\u8bad\u7ec3\u7684\u52a0\u901f\u548c\u7cfb\u7edf\u7ea7\u4f18\u5316\u6b63\u4ea4\u3002", "result": "\u5728GEOM-Drugs\u6570\u636e\u96c6\u4e0a\uff0c\u7f13\u5b58\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u540c\u6837\u672c\u8d28\u91cf\u4e0b\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c112\u500d\uff0c\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\u6700\u9ad8\u53ef\u8fbe3\u500d\u52a0\u901f\uff0c\u4e14\u6837\u672c\u8d28\u91cf\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u8be5\u7f13\u5b58\u65b9\u6cd5\u80fd\u663e\u8457\u52a0\u901f\u5206\u5b50\u51e0\u4f55\u751f\u6210\uff0c\u4e14\u5176\u589e\u76ca\u53ef\u4e0e\u5176\u4ed6\u4f18\u5316\u53e0\u52a0\uff0c\u7ed3\u5408\u5176\u4ed6\u901a\u7528\u65e0\u635f\u4f18\u5316\u53ef\u83b7\u5f97\u9ad8\u8fbe7\u500d\u7684\u52a0\u901f\u6548\u679c\u3002"}}
{"id": "2510.04660", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04660", "abs": "https://arxiv.org/abs/2510.04660", "authors": ["Yuandou Wang", "Filip Gunnarsson", "Rihan Hai"], "title": "IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams", "comment": null, "summary": "Tabular data streams are rapidly emerging as a dominant modality for\nreal-time decision-making in healthcare, finance, and the Internet of Things\n(IoT). These applications commonly run on edge and mobile devices, where energy\nbudgets, memory, and compute are strictly limited. Continual learning (CL)\naddresses such dynamics by training models sequentially on task streams while\npreserving prior knowledge and consolidating new knowledge. While recent CL\nwork has advanced in mitigating catastrophic forgetting and improving knowledge\ntransfer, the practical requirements of energy and memory efficiency for\ntabular data streams remain underexplored. In particular, existing CL solutions\nmostly depend on replay mechanisms whose buffers grow over time and exacerbate\nresource costs.\n  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a\ncompact continual learner for tabular data streams. IMLP incorporates a\nwindowed scaled dot-product attention over a sliding latent feature buffer,\nenabling constant-size memory and avoiding storing raw data. The attended\ncontext is concatenated with current features and processed by shared\nfeed-forward layers, yielding lightweight per-segment updates. To assess\npractical deployability, we introduce NetScore-T, a tunable metric coupling\nbalanced accuracy with energy for Pareto-aware comparison across models and\ndatasets. IMLP achieves up to $27.6\\times$ higher energy efficiency than TabNet\nand $85.5\\times$ higher than TabPFN, while maintaining competitive average\naccuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient\nalternative to full retraining for tabular data streams.", "AI": {"tldr": "\u63d0\u51faIMLP\u6a21\u578b\uff0c\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u6d41\u7684\u7d27\u51d1\u578b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u6ed1\u52a8\u6f5c\u5728\u7279\u5f81\u7f13\u51b2\u533a\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u8868\u683c\u6570\u636e\u6d41\u5728\u533b\u7597\u3001\u91d1\u878d\u548c\u7269\u8054\u7f51\u7b49\u9886\u57df\u7684\u5b9e\u65f6\u51b3\u7b56\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u5e94\u7528\u901a\u5e38\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u56de\u653e\u673a\u5236\uff0c\u5176\u7f13\u51b2\u533a\u4f1a\u968f\u65f6\u95f4\u589e\u957f\u800c\u52a0\u5267\u8d44\u6e90\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u589e\u91cf\u591a\u5c42\u611f\u77e5\u5668(IMLP)\uff0c\u4f7f\u7528\u6ed1\u52a8\u6f5c\u5728\u7279\u5f81\u7f13\u51b2\u533a\u4e0a\u7684\u7a97\u53e3\u5316\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\uff0c\u907f\u514d\u5b58\u50a8\u539f\u59cb\u6570\u636e\u5e76\u4fdd\u6301\u6052\u5b9a\u5185\u5b58\u5927\u5c0f\u3002\u6ce8\u610f\u529b\u4e0a\u4e0b\u6587\u4e0e\u5f53\u524d\u7279\u5f81\u62fc\u63a5\u540e\u7531\u5171\u4eab\u524d\u9988\u5c42\u5904\u7406\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u5206\u6bb5\u66f4\u65b0\u3002", "result": "IMLP\u6bd4TabNet\u548cTabPFN\u5206\u522b\u5b9e\u73b0\u9ad8\u8fbe27.6\u500d\u548c85.5\u500d\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "IMLP\u4e3a\u8868\u683c\u6570\u636e\u6d41\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u90e8\u7f72\u3001\u80fd\u6e90\u6548\u7387\u9ad8\u7684\u5168\u91cd\u8bad\u7ec3\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.04667", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2510.04667", "abs": "https://arxiv.org/abs/2510.04667", "authors": ["Fanzhe Fu", "Yang Yang"], "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting", "comment": "9pages, 6 figures", "summary": "Reversible Instance Normalization (RevIN) is a key technique enabling simple\nlinear models to achieve state-of-the-art performance in time series\nforecasting. While replacing its non-robust statistics with robust counterparts\n(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal\na far more complex reality. This paper deconstructs the perplexing performance\nof various normalization strategies by identifying four underlying theoretical\ncontradictions. Our experiments provide two crucial findings: first, the\nstandard RevIN catastrophically fails on datasets with extreme outliers, where\nits MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN\nprevents this failure and unexpectedly emerges as the best overall performer,\nour adaptive model (A-IN), designed to test a diagnostics-driven heuristic,\nunexpectedly suffers a complete and systemic failure. This surprising outcome\nuncovers a critical, overlooked pitfall in time series analysis: the\ninstability introduced by a simple or counter-intuitive heuristic can be more\ndamaging than the statistical issues it aims to solve. The core contribution of\nthis work is thus a new, cautionary paradigm for time series normalization: a\nshift from a blind search for complexity to a diagnostics-driven analysis that\nreveals not only the surprising power of simple baselines but also the perilous\nnature of naive adaptation.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u5f52\u4e00\u5316\u7b56\u7565\u4e2d\u7684\u7406\u8bba\u77db\u76fe\uff0c\u53d1\u73b0\u6807\u51c6RevIN\u5728\u6781\u7aef\u5f02\u5e38\u503c\u6570\u636e\u96c6\u4e0a\u4f1a\u707e\u96be\u6027\u5931\u8d25\uff0c\u800c\u7b80\u5355\u7684R\u00b2-IN\u5374\u610f\u5916\u6210\u4e3a\u6700\u4f73\u8868\u73b0\u8005\uff0c\u540c\u65f6\u81ea\u9002\u5e94\u6a21\u578bA-IN\u51fa\u73b0\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u7814\u7a76RevIN\u5f52\u4e00\u5316\u6280\u672f\u53ca\u5176\u9c81\u68d2\u6027\u6539\u8fdb\u7248\u672cR\u00b2-IN\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u53ef\u80fd\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc6\u522b\u56db\u79cd\u5f52\u4e00\u5316\u7b56\u7565\u7684\u7406\u8bba\u77db\u76fe\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u6807\u51c6RevIN\u3001R\u00b2-IN\u548c\u81ea\u9002\u5e94\u6a21\u578bA-IN\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6807\u51c6RevIN\u5728\u6781\u7aef\u5f02\u5e38\u503c\u6570\u636e\u96c6\u4e0aMSE\u6fc0\u589e683%\uff1bR\u00b2-IN\u610f\u5916\u6210\u4e3a\u6574\u4f53\u6700\u4f73\u8868\u73b0\u8005\uff1b\u81ea\u9002\u5e94\u6a21\u578bA-IN\u51fa\u73b0\u5b8c\u5168\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u5f52\u4e00\u5316\u7684\u65b0\u8b66\u793a\u8303\u5f0f\uff1a\u4ece\u76f2\u76ee\u8ffd\u6c42\u590d\u6742\u6027\u8f6c\u5411\u8bca\u65ad\u9a71\u52a8\u5206\u6790\uff0c\u63ed\u793a\u4e86\u7b80\u5355\u57fa\u7ebf\u7684\u60ca\u4eba\u529b\u91cf\u548c\u5929\u771f\u9002\u5e94\u7684\u5371\u9669\u6027\u3002"}}
{"id": "2510.04674", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.04674", "abs": "https://arxiv.org/abs/2510.04674", "authors": ["Lorenzo Pannacci", "Simone Fiorellino", "Mario Edoardo Pandolfo", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding", "comment": "Proceedings of IEEE Globecom 2025 Workshops", "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8bed\u4e49\u4fe1\u9053\u5747\u8861\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u4f9b\u5e94\u5546\u90e8\u7f72\u4e2dDeepJSCC\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u79cd\u5bf9\u9f50\u5668\u6765\u63d0\u5347\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DeepJSCC\u65b9\u6848\u5047\u8bbe\u53d1\u5c04\u7aef\u548c\u63a5\u6536\u7aef\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u8fd9\u5728\u591a\u4f9b\u5e94\u5546\u90e8\u7f72\u4e2d\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u8bed\u4e49\u566a\u58f0\u5e76\u964d\u4f4e\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u7c7b\u8bed\u4e49\u4fe1\u9053\u5747\u8861\u5668\uff1a\u7ebf\u6027\u6620\u5c04\uff08\u95ed\u5f0f\u89e3\uff09\u3001\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\uff08\u66f4\u5f3a\u8868\u8fbe\u80fd\u529b\uff09\u548cParseval\u6846\u67b6\u5747\u8861\u5668\uff08\u96f6\u6837\u672c\u65e0\u9700\u8bad\u7ec3\uff09\u3002", "result": "\u901a\u8fc7\u5728AWGN\u548c\u8870\u843d\u4fe1\u9053\u4e0a\u7684\u56fe\u50cf\u91cd\u5efa\u5b9e\u9a8c\uff0c\u91cf\u5316\u4e86\u590d\u6742\u5ea6\u3001\u6570\u636e\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u4e3a\u5728\u5f02\u6784AI\u539f\u751f\u65e0\u7ebf\u7f51\u7edc\u4e2d\u90e8\u7f72DeepJSCC\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u9488\uff0c\u89e3\u51b3\u4e86\u591a\u4f9b\u5e94\u5546\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2510.04676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04676", "abs": "https://arxiv.org/abs/2510.04676", "authors": ["Qiyu Wei", "Haowei Wang", "Richard Allmendinger", "Mauricio A. \u00c1lvarez"], "title": "Counterfactual Credit Guided Bayesian Optimization", "comment": null, "summary": "Bayesian optimization has emerged as a prominent methodology for optimizing\nexpensive black-box functions by leveraging Gaussian process surrogates, which\nfocus on capturing the global characteristics of the objective function.\nHowever, in numerous practical scenarios, the primary objective is not to\nconstruct an exhaustive global surrogate, but rather to quickly pinpoint the\nglobal optimum. Due to the aleatoric nature of the sequential optimization\nproblem and its dependence on the quality of the surrogate model and the\ninitial design, it is restrictive to assume that all observed samples\ncontribute equally to the discovery of the optimum in this context. In this\npaper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),\na novel framework that explicitly quantifies the contribution of individual\nhistorical observations through counterfactual credit. By incorporating\ncounterfactual credit into the acquisition function, our approach can\nselectively allocate resources in areas where optimal solutions are most likely\nto occur. We prove that CCGBO retains sublinear regret. Empirical evaluations\non various synthetic and real-world benchmarks demonstrate that CCGBO\nconsistently reduces simple regret and accelerates convergence to the global\noptimum.", "AI": {"tldr": "\u63d0\u51faCCGBO\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u4fe1\u7528\u8bc4\u4f30\u5386\u53f2\u89c2\u6d4b\u70b9\u7684\u8d21\u732e\u5ea6\uff0c\u6539\u8fdb\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u91c7\u96c6\u51fd\u6570\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5bfb\u627e\u5168\u5c40\u6700\u4f18\u89e3\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u5173\u6ce8\u6784\u5efa\u5168\u5c40\u4ee3\u7406\u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4e3b\u8981\u76ee\u6807\u662f\u5feb\u901f\u5b9a\u4f4d\u5168\u5c40\u6700\u4f18\u89e3\u3002\u7531\u4e8e\u5e8f\u5217\u4f18\u5316\u7684\u968f\u673a\u6027\u548c\u5bf9\u521d\u59cb\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u5e76\u975e\u6240\u6709\u89c2\u6d4b\u70b9\u90fd\u5bf9\u5bfb\u627e\u6700\u4f18\u89e3\u6709\u540c\u7b49\u8d21\u732e\u3002", "method": "\u5f15\u5165\u53cd\u4e8b\u5b9e\u4fe1\u7528\u6982\u5ff5\uff0c\u91cf\u5316\u5386\u53f2\u89c2\u6d4b\u70b9\u7684\u4e2a\u4f53\u8d21\u732e\u5ea6\uff0c\u5e76\u5c06\u53cd\u4e8b\u5b9e\u4fe1\u7528\u6574\u5408\u5230\u91c7\u96c6\u51fd\u6570\u4e2d\uff0c\u4f7f\u4f18\u5316\u8fc7\u7a0b\u80fd\u591f\u6709\u9009\u62e9\u5730\u5728\u6700\u4f18\u89e3\u6700\u53ef\u80fd\u51fa\u73b0\u7684\u533a\u57df\u5206\u914d\u8d44\u6e90\u3002", "result": "\u7406\u8bba\u8bc1\u660eCCGBO\u4fdd\u6301\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5728\u591a\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCGBO\u6301\u7eed\u964d\u4f4e\u7b80\u5355\u9057\u61be\u5e76\u52a0\u901f\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "conclusion": "CCGBO\u901a\u8fc7\u663e\u5f0f\u91cf\u5316\u5386\u53f2\u89c2\u6d4b\u70b9\u7684\u8d21\u732e\u5ea6\uff0c\u6539\u8fdb\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2510.04685", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04685", "abs": "https://arxiv.org/abs/2510.04685", "authors": ["Shuche Wang", "Adarsh Barik", "Peng Zhao", "Vincent Y. F. Tan"], "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model", "comment": "Accepted to NeurIPS 2025", "summary": "We develop the first parameter-free algorithms for the Stochastically\nExtended Adversarial (SEA) model, a framework that bridges adversarial and\nstochastic online convex optimization. Existing approaches for the SEA model\nrequire prior knowledge of problem-specific parameters, such as the diameter of\nthe domain $D$ and the Lipschitz constant of the loss functions $G$, which\nlimits their practical applicability. Addressing this, we develop\nparameter-free methods by leveraging the Optimistic Online Newton Step (OONS)\nalgorithm to eliminate the need for these parameters. We first establish a\ncomparator-adaptive algorithm for the scenario with unknown domain diameter but\nknown Lipschitz constant, achieving an expected regret bound of\n$\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} +\n\\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and\n$\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic\nvariance and cumulative adversarial variation, respectively. We then extend\nthis to the more general setting where both $D$ and $G$ are unknown, attaining\nthe comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound\nexhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$,\ndemonstrating the efficacy of our proposed methods even when both parameters\nare unknown in the SEA model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u968f\u673a\u6269\u5c55\u5bf9\u6297\uff08SEA\uff09\u6a21\u578b\u7684\u65e0\u53c2\u6570\u7b97\u6cd5\uff0c\u6d88\u9664\u4e86\u5bf9\u95ee\u9898\u7279\u5b9a\u53c2\u6570\uff08\u5982\u57df\u76f4\u5f84D\u548c\u635f\u5931\u51fd\u6570Lipschitz\u5e38\u6570G\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u9700\u6c42\u3002", "motivation": "\u73b0\u6709SEA\u6a21\u578b\u65b9\u6cd5\u9700\u8981\u9884\u5148\u77e5\u9053\u95ee\u9898\u7279\u5b9a\u53c2\u6570\uff0c\u5982\u57df\u76f4\u5f84\u548cLipschitz\u5e38\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u65e0\u53c2\u6570\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5b9e\u7528\u6027\u3002", "method": "\u5229\u7528\u4e50\u89c2\u5728\u7ebf\u725b\u987f\u6b65\uff08OONS\uff09\u7b97\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u65e0\u53c2\u6570\u65b9\u6cd5\uff1a1\uff09\u672a\u77e5\u57df\u76f4\u5f84\u4f46\u5df2\u77e5Lipschitz\u5e38\u6570\u7684\u60c5\u51b5\uff1b2\uff09\u4e24\u8005\u90fd\u672a\u77e5\u7684\u66f4\u4e00\u822c\u60c5\u51b5\u3002", "result": "\u5b9e\u73b0\u4e86\u6bd4\u8f83\u5668\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u83b7\u5f97\u4e86\u671f\u671b\u9057\u61be\u754c\u4e3a\u00d5(||u||\u2082\u00b2 + ||u||\u2082(\u221a\u03c3\u00b2\u2081:\u209c + \u221a\u03a3\u00b2\u2081:\u209c))\uff0c\u5176\u4e2du\u662f\u6bd4\u8f83\u5668\u5411\u91cf\uff0c\u03c3\u00b2\u548c\u03a3\u00b2\u5206\u522b\u8868\u793a\u7d2f\u79ef\u968f\u673a\u65b9\u5dee\u548c\u7d2f\u79ef\u5bf9\u6297\u53d8\u5316\u3002", "conclusion": "\u5373\u4f7f\u5728SEA\u6a21\u578b\u4e2d\u4e24\u4e2a\u53c2\u6570\u90fd\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4ecd\u7136\u6709\u6548\uff0c\u9057\u61be\u754c\u5bf9\u968f\u673a\u65b9\u5dee\u548c\u5bf9\u6297\u53d8\u5316\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u4f9d\u8d56\u6027\u3002"}}
{"id": "2510.04686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04686", "abs": "https://arxiv.org/abs/2510.04686", "authors": ["Chenxiang Zhang", "Alexander Theus", "Damien Teney", "Antonio Orvieto", "Jun Pang", "Sjouke Mauw"], "title": "How does the optimizer implicitly bias the model merging loss landscape?", "comment": "preprint", "summary": "Model merging methods combine models with different capabilities into a\nsingle one while maintaining the same inference cost. Two popular approaches\nare linear interpolation, which linearly interpolates between model weights,\nand task arithmetic, which combines task vectors obtained by the difference\nbetween finetuned and base models. While useful in practice, what properties\nmake merging effective are poorly understood. This paper explores how the\noptimization process affects the loss landscape geometry and its impact on\nmerging success. We show that a single quantity -- the effective noise scale --\nunifies the impact of optimizer and data choices on model merging. Across\narchitectures and datasets, the effectiveness of merging success is a\nnon-monotonic function of effective noise, with a distinct optimum. Decomposing\nthis quantity, we find that larger learning rates, stronger weight decay,\nsmaller batch sizes, and data augmentation all independently modulate the\neffective noise scale, exhibiting the same qualitative trend. Unlike prior work\nthat connects optimizer noise to the flatness or generalization of individual\nminima, we show that it also affects the global loss landscape, predicting when\nindependently trained solutions can be merged. Our findings broaden the\nunderstanding of how optimization shapes the loss landscape geometry and its\ndownstream consequences for model merging, suggesting the possibility of\nfurther manipulating the training dynamics to improve merging effectiveness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u6709\u6548\u566a\u58f0\u5c3a\u5ea6\u662f\u7edf\u4e00\u4f18\u5316\u5668\u548c\u6570\u636e\u9009\u62e9\u5bf9\u6a21\u578b\u5408\u5e76\u5f71\u54cd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u6a21\u578b\u5408\u5e76\u6548\u679c\u662f\u6709\u6548\u566a\u58f0\u7684\u975e\u5355\u8c03\u51fd\u6570\uff0c\u5b58\u5728\u6700\u4f18\u503c\u3002", "motivation": "\u867d\u7136\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u5bf9\u5176\u6709\u6548\u6027\u7684\u539f\u7406\u7406\u89e3\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4f18\u5316\u8fc7\u7a0b\u5982\u4f55\u5f71\u54cd\u635f\u5931\u666f\u89c2\u51e0\u4f55\u5f62\u72b6\u53ca\u5176\u5bf9\u5408\u5e76\u6210\u529f\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6709\u6548\u566a\u58f0\u5c3a\u5ea6\uff0c\u7814\u7a76\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\u3001\u6279\u91cf\u5927\u5c0f\u548c\u6570\u636e\u589e\u5f3a\u7b49\u4f18\u5316\u53c2\u6570\u5982\u4f55\u72ec\u7acb\u8c03\u8282\u6709\u6548\u566a\u58f0\uff0c\u8fdb\u800c\u5f71\u54cd\u6a21\u578b\u5408\u5e76\u6548\u679c\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5408\u5e76\u6548\u679c\u662f\u6709\u6548\u566a\u58f0\u7684\u975e\u5355\u8c03\u51fd\u6570\uff0c\u5177\u6709\u660e\u663e\u7684\u6700\u4f18\u503c\u3002\u8f83\u5927\u7684\u5b66\u4e60\u7387\u3001\u8f83\u5f3a\u7684\u6743\u91cd\u8870\u51cf\u3001\u8f83\u5c0f\u7684\u6279\u91cf\u5927\u5c0f\u548c\u6570\u636e\u589e\u5f3a\u90fd\u80fd\u72ec\u7acb\u8c03\u8282\u6709\u6548\u566a\u58f0\u5c3a\u5ea6\uff0c\u8868\u73b0\u51fa\u76f8\u540c\u7684\u5b9a\u6027\u8d8b\u52bf\u3002", "conclusion": "\u4f18\u5316\u5668\u566a\u58f0\u4e0d\u4ec5\u5f71\u54cd\u5355\u4e2a\u6700\u5c0f\u503c\u7684\u5e73\u5766\u5ea6\u6216\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5f71\u54cd\u5168\u5c40\u635f\u5931\u666f\u89c2\uff0c\u80fd\u591f\u9884\u6d4b\u72ec\u7acb\u8bad\u7ec3\u7684\u89e3\u4f55\u65f6\u53ef\u4ee5\u6210\u529f\u5408\u5e76\u3002\u8fd9\u6269\u5c55\u4e86\u5bf9\u4f18\u5316\u5982\u4f55\u5851\u9020\u635f\u5931\u666f\u89c2\u51e0\u4f55\u5f62\u72b6\u53ca\u5176\u5bf9\u6a21\u578b\u5408\u5e76\u4e0b\u6e38\u5f71\u54cd\u7684\u7406\u89e3\u3002"}}
{"id": "2510.04710", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04710", "abs": "https://arxiv.org/abs/2510.04710", "authors": ["Zexin Wang", "Changhua Pei", "Yang Liu", "Hengyue Jiang", "Quan Zhou", "Haotian Si", "Hang Cui", "Jianhui Li", "Gaogang Xie", "Jingjing Li", "Dan Pei"], "title": "ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts", "comment": "13 pages", "summary": "Web service administrators must ensure the stability of multiple systems by\npromptly detecting anomalies in Key Performance Indicators (KPIs). Achieving\nthe goal of \"train once, infer across scenarios\" remains a fundamental\nchallenge for time series anomaly detection models. Beyond improving zero-shot\ngeneralization, such models must also flexibly handle sequences of varying\nlengths during inference, ranging from one hour to one week, without\nretraining. Conventional approaches rely on sliding-window encoding and\nself-supervised learning, which restrict inference to fixed-length inputs.\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot\ncapabilities across general domains. However, when applied to time series data,\nthey face inherent limitations due to context length. To address this issue, we\npropose ViTs, a Vision-Language Model (VLM)-based framework that converts time\nseries curves into visual representations. By rescaling time series images,\ntemporal dependencies are preserved while maintaining a consistent input size,\nthereby enabling efficient processing of arbitrarily long sequences without\ncontext constraints. Training VLMs for this purpose introduces unique\nchallenges, primarily due to the scarcity of aligned time series image-text\ndata. To overcome this, we employ an evolutionary algorithm to automatically\ngenerate thousands of high-quality image-text pairs and design a three-stage\ntraining pipeline consisting of: (1) time series knowledge injection, (2)\nanomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive\nexperiments demonstrate that ViTs substantially enhance the ability of VLMs to\nunderstand and detect anomalies in time series data. All datasets and code will\nbe publicly released at: https://anonymous.4open.science/r/ViTs-C484/.", "AI": {"tldr": "\u63d0\u51faViTs\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u5b9e\u73b0\"\u4e00\u6b21\u8bad\u7ec3\uff0c\u8de8\u573a\u666f\u63a8\u7406\"\u7684\u76ee\u6807", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\"\u4e00\u6b21\u8bad\u7ec3\uff0c\u8de8\u573a\u666f\u63a8\u7406\"\u7684\u6839\u672c\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u957f\u5ea6\u8f93\u5165\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u66f2\u7ebf\u8f6c\u6362\u4e3a\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u7f29\u653e\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u8bbe\u8ba1\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u65f6\u95f4\u5e8f\u5217\u77e5\u8bc6\u6ce8\u5165\u3001\u5f02\u5e38\u68c0\u6d4b\u589e\u5f3a\u3001\u5f02\u5e38\u63a8\u7406\u7cbe\u70bc", "result": "ViTs\u663e\u8457\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u548c\u68c0\u6d4b\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u7684\u80fd\u529b", "conclusion": "ViTs\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u957f\u5ea6\u5e8f\u5217\u7684\u9ad8\u6548\u5904\u7406"}}
{"id": "2510.04727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04727", "abs": "https://arxiv.org/abs/2510.04727", "authors": ["Emanuele Mule", "Stefano Fiorini", "Antonio Purificato", "Federico Siciliano", "Stefano Coniglio", "Fabrizio Silvestri"], "title": "Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs", "comment": null, "summary": "Hypergraphs provide a natural way to represent higher-order interactions\namong multiple entities. While undirected hypergraphs have been extensively\nstudied, the case of directed hypergraphs, which can model oriented group\ninteractions, remains largely under-explored despite its relevance for many\napplications. Recent approaches in this direction often exhibit an implicit\nbias toward homophily, which limits their effectiveness in heterophilic\nsettings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf\nNeural Networks (SNNs) were introduced as an effective solution to circumvent\nsuch a drawback. While a generalization to hypergraphs is known, it is only\nsuitable for undirected hypergraphs, failing to tackle the directed case. In\nthis work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a\nframework integrating sheaf theory with a principled treatment of asymmetric\nrelations within a hypergraph. From it, we construct the Directed Sheaf\nHypergraph Laplacian, a complex-valued operator by which we unify and\ngeneralize many existing Laplacian matrices proposed in the graph- and\nhypergraph-learning literature. Across 7 real-world datasets and against 13\nbaselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how\na principled treatment of directionality in hypergraphs, combined with the\nexpressive power of sheaves, can substantially improve performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65b9\u5411\u6027\u5c42\u72b6\u8d85\u56fe\u7f51\u7edc\uff08DSHN\uff09\uff0c\u5c06\u5c42\u72b6\u7406\u8bba\u4e0e\u8d85\u56fe\u4e2d\u7684\u975e\u5bf9\u79f0\u5173\u7cfb\u5904\u7406\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u6709\u5411\u5c42\u72b6\u8d85\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u57287\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u76f8\u6bd413\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e862%\u523020%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u6709\u5411\u8d85\u56fe\u80fd\u591f\u5efa\u6a21\u5b9a\u5411\u7684\u7fa4\u4f53\u4ea4\u4e92\uff0c\u4f46\u5728\u5f53\u524d\u7814\u7a76\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9690\u542b\u540c\u8d28\u6027\u504f\u89c1\uff0c\u9650\u5236\u4e86\u5728\u5f02\u8d28\u6027\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8e\u4ee3\u6570\u62d3\u6251\u4e2d\u7684\u7ec6\u80de\u5c42\u72b6\u6982\u5ff5\uff0c\u5c06\u5c42\u72b6\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u5230\u6709\u5411\u8d85\u56fe\uff0c\u6784\u5efa\u4e86\u65b9\u5411\u6027\u5c42\u72b6\u8d85\u56fe\u7f51\u7edc\u6846\u67b6\u548c\u590d\u6570\u503c\u7684\u6709\u5411\u5c42\u72b6\u8d85\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e13\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cDSHN\u5b9e\u73b0\u4e862%\u523020%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u5bf9\u8d85\u56fe\u4e2d\u65b9\u5411\u6027\u7684\u539f\u5219\u6027\u5904\u7406\uff0c\u7ed3\u5408\u5c42\u72b6\u7406\u8bba\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.04728", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04728", "abs": "https://arxiv.org/abs/2510.04728", "authors": ["Mehrasa Ahmadipour", "Aur\u00e9lien Garivier"], "title": "EVaR-Optimal Arm Identification in Bandits", "comment": null, "summary": "We study the fixed-confidence best arm identification (BAI) problem within\nthe multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)\ncriterion. Our analysis considers a nonparametric setting, allowing for general\nreward distributions bounded in [0,1]. This formulation addresses the critical\nneed for risk-averse decision-making in high-stakes environments, such as\nfinance, moving beyond simple expected value optimization. We propose a\n$\\delta$-correct, Track-and-Stop based algorithm and derive a corresponding\nlower bound on the expected sample complexity, which we prove is asymptotically\nmatched. The implementation of our algorithm and the characterization of the\nlower bound both require solving a complex convex optimization problem and a\nrelated, simpler non-convex one.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Entropic Value-at-Risk\u51c6\u5219\u4e0b\u7684\u591a\u81c2\u8001\u864e\u673a\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u03b4-\u6b63\u786e\u7684Track-and-Stop\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u6e10\u8fd1\u5339\u914d\u4e0b\u754c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u98ce\u9669\u73af\u5883\uff08\u5982\u91d1\u878d\u9886\u57df\uff09\u4e2d\u98ce\u9669\u89c4\u907f\u51b3\u7b56\u7684\u9700\u6c42\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u671f\u671b\u503c\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTrack-and-Stop\u7684\u03b4-\u6b63\u786e\u7b97\u6cd5\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u7684\u51f8\u4f18\u5316\u95ee\u9898\u548c\u76f8\u5173\u7684\u7b80\u5355\u975e\u51f8\u95ee\u9898\u3002", "result": "\u63a8\u5bfc\u4e86\u671f\u671b\u6837\u672c\u590d\u6742\u5ea6\u7684\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u7b97\u6cd5\u6e10\u8fd1\u5339\u914d\u8be5\u4e0b\u754c\u3002", "conclusion": "\u5728\u975e\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86EVaR\u51c6\u5219\u4e0b\u7684BAI\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u98ce\u9669\u89c4\u907f\u51b3\u7b56\u7684\u4f18\u5316\u3002"}}
{"id": "2510.04758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04758", "abs": "https://arxiv.org/abs/2510.04758", "authors": ["Zhiwei Han", "Stefan Matthes", "Hao Shen"], "title": "Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors", "comment": null, "summary": "In this work, we establish conditions under which nonlinear CCA recovers the\nground-truth latent factors up to an orthogonal transform after whitening.\nBuilding on the classical result that linear mappings maximize canonical\ncorrelations under Gaussian priors, we prove affine identifiability for a broad\nclass of latent distributions in the population setting. Central to our proof\nis a reparameterization result that transports the analysis from observation\nspace to source space, where identifiability becomes tractable. We further show\nthat whitening is essential for ensuring boundedness and well-conditioning,\nthereby underpinning identifiability. Beyond the population setting, we prove\nthat ridge-regularized empirical CCA converges to its population counterpart,\ntransferring these guarantees to the finite-sample regime. Experiments on a\ncontrolled synthetic dataset and a rendered image dataset validate our theory\nand demonstrate the necessity of its assumptions through systematic ablations.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u975e\u7ebf\u6027CCA\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u6062\u590d\u771f\u5b9e\u6f5c\u5728\u56e0\u5b50\uff0c\u5efa\u7acb\u4e86\u4ece\u89c2\u6d4b\u7a7a\u95f4\u5230\u6e90\u7a7a\u95f4\u7684\u91cd\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5cad\u6b63\u5219\u5316\u7ecf\u9a8cCCA\u7684\u6536\u655b\u6027\u3002", "motivation": "\u7814\u7a76\u975e\u7ebf\u6027CCA\u5982\u4f55\u6062\u590d\u771f\u5b9e\u6f5c\u5728\u56e0\u5b50\uff0c\u89e3\u51b3\u4f20\u7edf\u7ebf\u6027CCA\u5728\u975e\u7ebf\u6027\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u975e\u7ebf\u6027\u964d\u7ef4\u548c\u7279\u5f81\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u5c06\u5206\u6790\u4ece\u89c2\u6d4b\u7a7a\u95f4\u8f6c\u6362\u5230\u6e90\u7a7a\u95f4\uff0c\u5efa\u7acb\u4eff\u5c04\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\uff0c\u4f7f\u7528\u5cad\u6b63\u5219\u5316\u7ecf\u9a8cCCA\u5e76\u8bc1\u660e\u5176\u6536\u655b\u5230\u603b\u4f53\u5bf9\u5e94\u7269\u3002", "result": "\u5728\u603b\u4f53\u8bbe\u7f6e\u4e0b\u8bc1\u660e\u4e86\u975e\u7ebf\u6027CCA\u80fd\u591f\u6062\u590d\u771f\u5b9e\u6f5c\u5728\u56e0\u5b50\uff08\u7ecf\u8fc7\u6b63\u4ea4\u53d8\u6362\u548c\u767d\u5316\uff09\uff0c\u5728\u6709\u9650\u6837\u672c\u60c5\u51b5\u4e0b\u7ecf\u9a8cCCA\u6536\u655b\u5230\u603b\u4f53CCA\u3002", "conclusion": "\u767d\u5316\u5bf9\u4e8e\u786e\u4fdd\u6709\u754c\u6027\u548c\u826f\u597d\u6761\u4ef6\u6027\u81f3\u5173\u91cd\u8981\uff0c\u975e\u7ebf\u6027CCA\u5728\u9002\u5f53\u6761\u4ef6\u4e0b\u80fd\u591f\u6709\u6548\u6062\u590d\u6f5c\u5728\u56e0\u5b50\uff0c\u4e3a\u975e\u7ebf\u6027\u964d\u7ef4\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2510.04767", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04767", "abs": "https://arxiv.org/abs/2510.04767", "authors": ["Wonjun Kang", "Kevin Galim", "Seunghyuk Oh", "Minjae Lee", "Yuchen Zeng", "Shuibai Zhang", "Coleman Hooper", "Yuezhou Hu", "Hyung Il Koo", "Nam Ik Cho", "Kangwook Lee"], "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs", "comment": "Project Page: https://parallelbench.github.io", "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5e76\u884c\u89e3\u7801\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9dLLMs\u7684\u57fa\u51c6\u6d4b\u8bd5ParallelBench\uff0c\u63ed\u793a\u4e86\u5e76\u884c\u89e3\u7801\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u8d28\u91cf\u4e0b\u964d\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u89e3\u7801\u7b56\u7565\u96be\u4ee5\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u56e0\u5e76\u884c\u89e3\u7801\u6f5c\u529b\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u6761\u4ef6\u72ec\u7acb\u6027\u5047\u8bbe\u4f1a\u5ffd\u7565token\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u5f3a\u4f9d\u8d56\u573a\u666f\u4e0b\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u56fa\u6709\u6311\u6218\uff0c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u4ee5\u6355\u6349\u5e76\u884c\u89e3\u7801\u5e26\u6765\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\u3002", "method": "\u9996\u5148\u8fdb\u884c\u5e76\u884c\u89e3\u7801\u7684\u4fe1\u606f\u8bba\u5206\u6790\uff0c\u7136\u540e\u5728\u53ef\u5206\u6790\u7684\u5408\u6210\u5217\u8868\u64cd\u4f5c\u4e0a\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u4ece\u6570\u636e\u5206\u5e03\u548c\u89e3\u7801\u7b56\u7565\u89d2\u5ea6\u63d0\u4f9b\u91cf\u5316\u89c1\u89e3\u3002\u63d0\u51fa\u4e13\u95e8\u4e3adLLMs\u8bbe\u8ba1\u7684ParallelBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5bf9\u4eba\u7c7b\u548c\u81ea\u56de\u5f52LLMs\u7b80\u5355\u4f46\u5bf9dLLMs\u5e76\u884c\u89e3\u7801\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i) \u5e76\u884c\u89e3\u7801\u4e0b\u7684dLLMs\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u906d\u53d7\u4e25\u91cd\u7684\u8d28\u91cf\u4e0b\u964d\uff1b(ii) \u5f53\u524d\u5e76\u884c\u89e3\u7801\u7b56\u7565\u96be\u4ee5\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u8c03\u6574\u5e76\u884c\u5ea6\uff0c\u65e0\u6cd5\u5728\u4e0d\u635f\u5bb3\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u52a0\u901f\u3002", "conclusion": "\u5f53\u524d\u7684\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u95ee\u9898\u8feb\u5207\u9700\u8981\u521b\u65b0\u7684\u89e3\u7801\u65b9\u6cd5\u3002\u4f5c\u8005\u53d1\u5e03\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u52a0\u901f\u771f\u6b63\u9ad8\u6548dLLMs\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.04769", "categories": ["cs.LG", "cs.AI", "math.PR", "math.ST", "stat.ML", "stat.TH", "Primary: 54H25, Secondary: 68T05, 68T37"], "pdf": "https://arxiv.org/pdf/2510.04769", "abs": "https://arxiv.org/abs/2510.04769", "authors": ["Michele Caprio", "Siu Lun Chau", "Krikamol Muandet"], "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates", "comment": null, "summary": "Many machine learning algorithms rely on iterative updates of uncertainty\nrepresentations, ranging from variational inference and\nexpectation-maximization, to reinforcement learning, continual learning, and\nmulti-agent learning. In the presence of imprecision and ambiguity, credal sets\n-- closed, convex sets of probability distributions -- have emerged as a\npopular framework for representing imprecise probabilistic beliefs. Under such\nimprecision, many learning problems in imprecise probabilistic machine learning\n(IPML) may be viewed as processes involving successive applications of update\nrules on credal sets. This naturally raises the question of whether this\niterative process converges to stable fixed points -- or, more generally, under\nwhat conditions on the updating mechanism such fixed points exist, and whether\nthey can be attained. We provide the first analysis of this problem and\nillustrate our findings using Credal Bayesian Deep Learning as a concrete\nexample. Our work demonstrates that incorporating imprecision into the learning\nprocess not only enriches the representation of uncertainty, but also reveals\nstructural conditions under which stability emerges, thereby offering new\ninsights into the dynamics of iterative learning under imprecision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5206\u6790\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u6982\u7387\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u8fed\u4ee3\u66f4\u65b0\u4fe1\u7528\u96c6\uff08credal sets\uff09\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u56fa\u5b9a\u70b9\u5b58\u5728\u7684\u6761\u4ef6\u53ca\u5176\u53ef\u8fbe\u6027\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u7684\u8fed\u4ee3\u66f4\u65b0\uff0c\u4f46\u5728\u5b58\u5728\u4e0d\u7cbe\u786e\u6027\u548c\u6a21\u7cca\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u7814\u7a76\u4fe1\u7528\u96c6\u8fed\u4ee3\u66f4\u65b0\u8fc7\u7a0b\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4fe1\u7528\u96c6\uff08\u95ed\u51f8\u6982\u7387\u5206\u5e03\u96c6\uff09\u7684\u8fed\u4ee3\u66f4\u65b0\u673a\u5236\uff0c\u7814\u7a76\u56fa\u5b9a\u70b9\u5b58\u5728\u7684\u6761\u4ef6\uff0c\u5e76\u4ee5\u4fe1\u7528\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u4e0d\u7cbe\u786e\u6027\u7eb3\u5165\u5b66\u4e60\u8fc7\u7a0b\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u4e0d\u786e\u5b9a\u6027\u8868\u793a\uff0c\u8fd8\u63ed\u793a\u4e86\u7a33\u5b9a\u6027\u51fa\u73b0\u7684\u7ed3\u6784\u6027\u6761\u4ef6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4e0d\u7cbe\u786e\u6027\u4e0b\u8fed\u4ee3\u5b66\u4e60\u7684\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4fe1\u7528\u96c6\u66f4\u65b0\u8fc7\u7a0b\u80fd\u591f\u6536\u655b\u5230\u7a33\u5b9a\u56fa\u5b9a\u70b9\u3002"}}
{"id": "2510.04773", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04773", "abs": "https://arxiv.org/abs/2510.04773", "authors": ["Kai Qin", "Jiaqi Wu", "Jianxiang He", "Haoyuan Sun", "Yifei Zhao", "Bin Liang", "Yongzhe Chang", "Tiantian Zhang", "Houde Liu"], "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning", "comment": "20 pages", "summary": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned\nfrom vast corpora, concerns regarding data privacy and safety are receiving\nincreasing attention. LLM unlearning, which aims to remove the influence of\nspecific data while preserving overall model utility, is becoming an important\nresearch area. One of the mainstream unlearning classes is optimization-based\nmethods, which achieve forgetting directly through fine-tuning, exemplified by\nNegative Preference Optimization (NPO). However, NPO's effectiveness is limited\nby its inherent lack of explicit positive preference signals. Attempts to\nintroduce such signals by constructing preferred responses often necessitate\ndomain-specific knowledge or well-designed prompts, fundamentally restricting\ntheir generalizability. In this paper, we shift the focus to the\ndistribution-level, directly targeting the next-token probability distribution\ninstead of entire responses, and derive a novel unlearning algorithm termed\n\\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show\nthat the requisite preference distribution pairs for DiPO, which are\ndistributions over the model's output tokens, can be constructed by selectively\namplifying or suppressing the model's high-confidence output logits, thereby\neffectively overcoming NPO's limitations. We theoretically prove the\nconsistency of DiPO's loss function with the desired unlearning direction.\nExtensive experiments demonstrate that DiPO achieves a strong trade-off between\nmodel utility and forget quality. Notably, DiPO attains the highest forget\nquality on the TOFU benchmark, and maintains leading scalability and\nsustainability in utility preservation on the MUSE benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiPO\uff08Distribution Preference Optimization\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u4e0b\u4e00\u4e2atoken\u7684\u6982\u7387\u5206\u5e03\u6765\u89e3\u51b3LLM\u9057\u5fd8\u95ee\u9898\uff0c\u514b\u670d\u4e86NPO\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u6b63\u504f\u597d\u4fe1\u53f7\u7684\u9650\u5236\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u9057\u5fd8\u65b9\u6cd5\u5982NPO\u5b58\u5728\u7f3a\u4e4f\u663e\u5f0f\u6b63\u504f\u597d\u4fe1\u53f7\u7684\u9650\u5236\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6216\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "DiPO\u65b9\u6cd5\u5c06\u7126\u70b9\u8f6c\u79fb\u5230\u5206\u5e03\u5c42\u9762\uff0c\u76f4\u63a5\u9488\u5bf9\u4e0b\u4e00\u4e2atoken\u7684\u6982\u7387\u5206\u5e03\u800c\u975e\u6574\u4e2a\u54cd\u5e94\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u653e\u5927\u6216\u6291\u5236\u6a21\u578b\u9ad8\u7f6e\u4fe1\u5ea6\u8f93\u51falogits\u6765\u6784\u5efa\u6240\u9700\u7684\u504f\u597d\u5206\u5e03\u5bf9\u3002", "result": "DiPO\u5728TOFU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u9057\u5fd8\u8d28\u91cf\uff0c\u5728MUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u9886\u5148\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7528\u4fdd\u6301\u7684\u53ef\u6301\u7eed\u6027\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u6548\u7528\u548c\u9057\u5fd8\u8d28\u91cf\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "DiPO\u901a\u8fc7\u5206\u5e03\u7ea7\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86NPO\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u9057\u5fd8\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u548c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u9057\u5fd8\u8d28\u91cf\u548c\u6a21\u578b\u6548\u7528\u4fdd\u6301\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04776", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.04776", "abs": "https://arxiv.org/abs/2510.04776", "authors": ["Ebenezer Awotoro", "Chisom Ezekannagha", "Florian Schwarz", "Johannes Tauscher", "Dominik Heider", "Katharina Ladewig", "Christel Le Bon", "Karine Moncoq", "Bruno Miroux", "Georges Hattab"], "title": "MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis", "comment": null, "summary": "Structural biology has made significant progress in determining membrane\nproteins, leading to a remarkable increase in the number of available\nstructures in dedicated databases. The inherent complexity of membrane protein\nstructures, coupled with challenges such as missing data, inconsistencies, and\ncomputational barriers from disparate sources, underscores the need for\nimproved database integration. To address this gap, we present MetaMP, a\nframework that unifies membrane-protein databases within a web application and\nuses machine learning for classification. MetaMP improves data quality by\nenriching metadata, offering a user-friendly interface, and providing eight\ninteractive views for streamlined exploration. MetaMP was effective across\ntasks of varying difficulty, demonstrating advantages across different levels\nwithout compromising speed or accuracy, according to user evaluations.\nMoreover, MetaMP supports essential functions such as structure classification\nand outlier detection.\n  We present three practical applications of Artificial Intelligence (AI) in\nmembrane protein research: predicting transmembrane segments, reconciling\nlegacy databases, and classifying structures with explainable AI support. In a\nvalidation focused on statistics, MetaMP resolved 77% of data discrepancies and\naccurately predicted the class of newly identified membrane proteins 98% of the\ntime and overtook expert curation. Altogether, MetaMP is a much-needed resource\nthat harmonizes current knowledge and empowers AI-driven exploration of\nmembrane-protein architecture.", "AI": {"tldr": "MetaMP\u662f\u4e00\u4e2a\u6574\u5408\u819c\u86cb\u767d\u6570\u636e\u5e93\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548c\u7528\u6237\u53cb\u597d\u754c\u9762\u6539\u8fdb\u6570\u636e\u8d28\u91cf\uff0c\u5728\u9a8c\u8bc1\u4e2d\u89e3\u51b3\u4e8677%\u7684\u6570\u636e\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u65b0\u819c\u86cb\u767d\u5206\u7c7b\u4e2d\u8fbe\u523098%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u819c\u86cb\u767d\u7ed3\u6784\u590d\u6742\u6027\u9ad8\uff0c\u73b0\u6709\u6570\u636e\u5e93\u5b58\u5728\u6570\u636e\u7f3a\u5931\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u969c\u788d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u6570\u636e\u5e93\u6574\u5408\u3002", "method": "\u5f00\u53d1MetaMP\u6846\u67b6\uff0c\u7edf\u4e00\u819c\u86cb\u767d\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u5206\u7c7b\uff0c\u63d0\u4f9b8\u4e2a\u4ea4\u4e92\u89c6\u56fe\u548c\u7528\u6237\u53cb\u597d\u754c\u9762\uff0c\u652f\u6301\u7ed3\u6784\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u529f\u80fd\u3002", "result": "MetaMP\u89e3\u51b3\u4e8677%\u7684\u6570\u636e\u4e0d\u4e00\u81f4\u6027\uff0c\u65b0\u819c\u86cb\u767d\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523098%\uff0c\u8d85\u8d8a\u4e86\u4e13\u5bb6\u7b56\u5c55\uff0c\u5728\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u826f\u597d\u4e14\u4e0d\u727a\u7272\u901f\u5ea6\u6216\u51c6\u786e\u6027\u3002", "conclusion": "MetaMP\u662f\u4e00\u4e2a\u5fc5\u8981\u7684\u8d44\u6e90\uff0c\u7edf\u4e00\u4e86\u5f53\u524d\u77e5\u8bc6\u5e76\u652f\u6301AI\u9a71\u52a8\u7684\u819c\u86cb\u767d\u7ed3\u6784\u63a2\u7d22\u3002"}}
{"id": "2510.04786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04786", "abs": "https://arxiv.org/abs/2510.04786", "authors": ["Jonas H\u00fcbotter", "Leander Diaz-Bone", "Ido Hakimi", "Andreas Krause", "Moritz Hardt"], "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning", "comment": null, "summary": "Humans are good at learning on the job: We learn how to solve the tasks we\nface as we go along. Can a model do the same? We propose an agent that\nassembles a task-specific curriculum, called test-time curriculum (TTC-RL), and\napplies reinforcement learning to continue training the model for its target\ntask. The test-time curriculum avoids time-consuming human curation of datasets\nby automatically selecting the most task-relevant data from a large pool of\navailable training data. Our experiments demonstrate that reinforcement\nlearning on a test-time curriculum consistently improves the model on its\ntarget tasks, across a variety of evaluations and models. Notably, on\nchallenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B\nby approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that\nTTC-RL significantly raises the performance ceiling compared to the initial\nmodel, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to\n43%. Our findings show the potential of test-time curricula in extending the\ntest-time scaling paradigm to continual training on thousands of task-relevant\nexperiences during test-time.", "AI": {"tldr": "\u63d0\u51fa\u4e86TTC-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u6570\u636e\u6784\u5efa\u6d4b\u8bd5\u65f6\u8bfe\u7a0b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u6d4b\u8bd5\u65f6\u7ee7\u7eed\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u5728\u5de5\u4f5c\u4e2d\u5b66\u4e60\uff0c\u4f46\u6a21\u578b\u901a\u5e38\u9700\u8981\u4eba\u5de5\u7b56\u5212\u7684\u6570\u636e\u96c6\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u76f8\u5173\u6570\u636e\u5e76\u5728\u6d4b\u8bd5\u65f6\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u6d4b\u8bd5\u65f6\u8bfe\u7a0b(TTC-RL)\u81ea\u52a8\u4ece\u5927\u91cf\u53ef\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u9009\u62e9\u6700\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\uff0c\u7136\u540e\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u4efb\u52a1\u7684\u6301\u7eed\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTTC-RL\u5c06Qwen3-8B\u7684pass@1\u5728AIME25\u4e0a\u63d0\u5347\u7ea61.8\u500d\uff0c\u5728CodeElo\u4e0a\u63d0\u5347\u7ea62.1\u500d\u3002pass@8\u5728AIME25\u4e0a\u4ece40%\u63d0\u5347\u523062%\uff0c\u5728CodeElo\u4e0a\u4ece28%\u63d0\u5347\u523043%\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u8bfe\u7a0b\u5c55\u793a\u4e86\u5c06\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u6269\u5c55\u5230\u5728\u6d4b\u8bd5\u65f6\u5bf9\u6570\u5343\u4e2a\u4efb\u52a1\u76f8\u5173\u7ecf\u9a8c\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04816", "abs": "https://arxiv.org/abs/2510.04816", "authors": ["Junhyung Ahn", "Sanghack Lee"], "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference", "comment": "This work has been accepted for publication at the IEEE International\n  Conference on Data Mining (ICDM) 2025", "summary": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities.", "AI": {"tldr": "\u63d0\u51faESCIM\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e3a\u672a\u70b9\u51fb\u6837\u672c\u751f\u6210\u8f6c\u5316\u6807\u7b7e\uff0c\u89e3\u51b3CVR\u9884\u6d4b\u4e2d\u70b9\u51fb\u6837\u672c\u7a00\u758f\u6027\u95ee\u9898", "motivation": "\u4f20\u7edfCVR\u9884\u6d4b\u6a21\u578b\u4ec5\u4f7f\u7528\u70b9\u51fb\u6837\u672c\uff0c\u4f46\u70b9\u51fb\u6837\u672c\u7a00\u758f\u9700\u8981\u5927\u91cf\u65e5\u5fd7\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u672a\u70b9\u51fb\u6837\u672c\u4f46\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5b58\u5728\u504f\u5dee\u95ee\u9898", "method": "\u8bad\u7ec3\u7528\u6237\u5e8f\u5217\u884c\u4e3a\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5bf9\u672a\u70b9\u51fb\u9879\u76ee\u8fdb\u884c\u5047\u8bbe\u5e72\u9884\uff08\u70b9\u51fb\uff09\uff0c\u63a8\u65ad\u53cd\u4e8b\u5b9eCVR\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u4e8c\u5143\u53cd\u4e8b\u5b9e\u8f6c\u5316\u6807\u7b7e\uff0c\u6574\u5408\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\u7b97\u6cd5\u4f18\u8d8a\u6027\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u6f5c\u5728\u8f6c\u5316\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "ESCIM\u65b9\u6cd5\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86CVR\u9884\u6d4b\u4e2d\u7684\u6837\u672c\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2510.04834", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.04834", "abs": "https://arxiv.org/abs/2510.04834", "authors": ["Idan Attias", "Lev Reyzin", "Nathan Srebro", "Gal Vardi"], "title": "On the Hardness of Learning Regular Expressions", "comment": null, "summary": "Despite the theoretical significance and wide practical use of regular\nexpressions, the computational complexity of learning them has been largely\nunexplored. We study the computational hardness of improperly learning regular\nexpressions in the PAC model and with membership queries. We show that PAC\nlearning is hard even under the uniform distribution on the hypercube, and also\nprove hardness of distribution-free learning with membership queries.\nFurthermore, if regular expressions are extended with complement or\nintersection, we establish hardness of learning with membership queries even\nunder the uniform distribution. We emphasize that these results do not follow\nfrom existing hardness results for learning DFAs or NFAs, since the descriptive\ncomplexity of regular languages can differ exponentially between DFAs, NFAs,\nand regular expressions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6b63\u5219\u8868\u8fbe\u5f0f\u5728PAC\u6a21\u578b\u548c\u6210\u5458\u67e5\u8be2\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u5728\u8d85\u7acb\u65b9\u4f53\u4e0a\u7684\u5747\u5300\u5206\u5e03\u4e0b\uff0cPAC\u5b66\u4e60\u4e5f\u662f\u56f0\u96be\u7684\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5e26\u6210\u5458\u67e5\u8be2\u7684\u65e0\u5206\u5e03\u5b66\u4e60\u7684\u56f0\u96be\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6b63\u5219\u8868\u8fbe\u5f0f\u5177\u6709\u7406\u8bba\u91cd\u8981\u6027\u548c\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5176\u5b66\u4e60\u8ba1\u7b97\u590d\u6742\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b63\u5219\u8868\u8fbe\u5f0f\u5728PAC\u6a21\u578b\u548c\u6210\u5458\u67e5\u8be2\u4e0b\u7684\u5b66\u4e60\u590d\u6742\u6027\uff0c\u5e76\u6269\u5c55\u5230\u5305\u542b\u8865\u96c6\u6216\u4ea4\u96c6\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u53d8\u4f53\u3002", "result": "\u8bc1\u660e\u4e86PAC\u5b66\u4e60\u5728\u5747\u5300\u5206\u5e03\u4e0b\u662f\u56f0\u96be\u7684\uff0c\u5e26\u6210\u5458\u67e5\u8be2\u7684\u65e0\u5206\u5e03\u5b66\u4e60\u4e5f\u662f\u56f0\u96be\u7684\uff0c\u4e14\u5f53\u6b63\u5219\u8868\u8fbe\u5f0f\u6269\u5c55\u8865\u96c6\u6216\u4ea4\u96c6\u65f6\uff0c\u5373\u4f7f\u5728\u5747\u5300\u5206\u5e03\u4e0b\u5e26\u6210\u5458\u67e5\u8be2\u7684\u5b66\u4e60\u4e5f\u662f\u56f0\u96be\u7684\u3002", "conclusion": "\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u5b66\u4e60\u5728\u8ba1\u7b97\u4e0a\u662f\u56f0\u96be\u7684\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e0d\u80fd\u4ece\u73b0\u6709\u7684DFA\u6216NFA\u5b66\u4e60\u56f0\u96be\u6027\u7ed3\u679c\u4e2d\u63a8\u5bfc\u51fa\u6765\uff0c\u56e0\u4e3a\u6b63\u5219\u8bed\u8a00\u7684\u63cf\u8ff0\u590d\u6742\u6027\u5728DFA\u3001NFA\u548c\u6b63\u5219\u8868\u8fbe\u5f0f\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u6307\u6570\u7ea7\u5dee\u5f02\u3002"}}
{"id": "2510.04837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04837", "abs": "https://arxiv.org/abs/2510.04837", "authors": ["Guillaume Godin"], "title": "Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study", "comment": "14 pages, 10 figures, 1 table", "summary": "Bond Centered FingerPrint (BCFP) are a complementary, bond-centric\nalternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static\nBCFP that mirrors the bond-convolution used by directed message-passing GNNs\nlike ChemProp, and evaluate it with a fast rapid Random Forest model on\nBrain-Blood Barrier Penetration (BBBP) classification task. Across stratified\ncross-validation, concatenating ECFP with BCFP consistently improves AUROC and\nAUPRC over either descriptor alone, as confirmed by Turkey HSD\nmultiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not\nyield statistically separable gains under the same test. We further propose\nBCFP-Sort&Slice, a simple feature-combination scheme that preserves the\nout-of-vocabulary (OOV) count information native to ECFP count vectors while\nenabling compact unhashed concatenation of BCFP variants. We also outperform\nthe MGTP prediction on our BBBP evaluation, using such composite new features\nbond and atom features. These results show that lightweight, bond-centered\ndescriptors can complement atom-centered circular fingerprints and provide\nstrong, fast baselines for BBBP prediction.", "AI": {"tldr": "BCFP\u662f\u4e00\u79cd\u4e0eECFP\u4e92\u8865\u7684\u952e\u4e2d\u5fc3\u6307\u7eb9\u65b9\u6cd5\uff0c\u5728BBB\u7a7f\u900f\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5c06ECFP\u4e0eBCFP\u7ed3\u5408\u4f7f\u7528\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u952e\u4e2d\u5fc3\u7684\u6307\u7eb9\u65b9\u6cd5\u4f5c\u4e3a\u539f\u5b50\u4e2d\u5fc3\u5706\u5f62\u6307\u7eb9\uff08\u5982ECFP\uff09\u7684\u8865\u5145\uff0c\u4ee5\u63d0\u5347\u8111\u8840\u5c4f\u969c\u7a7f\u900f\u6027\u9884\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u9759\u6001BCFP\uff0c\u6a21\u62dfChemProp\u7b49\u5b9a\u5411\u6d88\u606f\u4f20\u9012GNN\u7684\u952e\u5377\u79ef\u64cd\u4f5c\uff0c\u4f7f\u7528\u5feb\u901f\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5f00\u53d1BCFP-Sort&Slice\u7279\u5f81\u7ec4\u5408\u65b9\u6848\u3002", "result": "\u5728\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\uff0cECFP\u4e0eBCFP\u7684\u62fc\u63a5\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u63cf\u8ff0\u7b26\u90fd\u80fd\u6301\u7eed\u63d0\u5347AUROC\u548cAUPRC\uff0c\u534a\u5f84r=1\u65f6\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7684\u952e\u4e2d\u5fc3\u63cf\u8ff0\u7b26\u53ef\u4ee5\u8865\u5145\u539f\u5b50\u4e2d\u5fc3\u5706\u5f62\u6307\u7eb9\uff0c\u4e3aBBB\u7a7f\u900f\u6027\u9884\u6d4b\u63d0\u4f9b\u5f3a\u5927\u4e14\u5feb\u901f\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.04842", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04842", "abs": "https://arxiv.org/abs/2510.04842", "authors": ["Yorgos Felekis", "Theodoros Damoulas", "Paris Giampouras"], "title": "Distributionally Robust Causal Abstractions", "comment": null, "summary": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u5e03\u9c81\u68d2\u7684\u56e0\u679c\u62bd\u8c61\u6846\u67b6\u53ca\u5176\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7Wasserstein\u6a21\u7cca\u96c6\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u5916\u751f\u5206\u5e03\u5047\u8bbe\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u62bd\u8c61\u5b66\u4e60\u65b9\u6cd5\u90fd\u5047\u8bbe\u56fa\u5b9a\u4e14\u826f\u597d\u6307\u5b9a\u7684\u5916\u751f\u5206\u5e03\uff0c\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u73af\u5883\u53d8\u5316\u548c\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u9c81\u68d2\u56e0\u679c\u62bd\u8c61\u5b66\u4e60\u5efa\u6a21\u4e3a\u5e26Wasserstein\u6a21\u7cca\u96c6\u7684\u7ea6\u675f\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u7406\u8bba\u548c\u7b97\u6cd5\u652f\u6301\u3002", "result": "\u5728\u7ecf\u9a8c\u548c\u9ad8\u65af\u73af\u5883\u4e0b\u63d0\u4f9b\u7406\u8bba\u7ed3\u679c\uff0c\u901a\u8fc7\u6a21\u7cca\u96c6\u534a\u5f84\u5b9e\u73b0\u9c81\u68d2\u6027\u6c34\u5e73\u7684\u539f\u5219\u6027\u9009\u62e9\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5bf9\u73af\u5883\u548c\u7ed3\u6784\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u9c81\u68d2\u56e0\u679c\u62bd\u8c61\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u548c\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\uff0c\u4e3a\u56e0\u679c\u62bd\u8c61\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2510.04855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04855", "abs": "https://arxiv.org/abs/2510.04855", "authors": ["Junqi Jiang", "Francesco Leofante", "Antonio Rago", "Francesca Toni"], "title": "Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders", "comment": null, "summary": "Counterfactual explanations (CEs) provide recourse recommendations for\nindividuals affected by algorithmic decisions. A key challenge is generating\nCEs that are robust against various perturbation types (e.g. input and model\nperturbations) while simultaneously satisfying other desirable properties.\nThese include plausibility, ensuring CEs reside on the data manifold, and\ndiversity, providing multiple distinct recourse options for single inputs.\nExisting methods, however, mostly struggle to address these multifaceted\nrequirements in a unified, model-agnostic manner. We address these limitations\nby proposing a novel generative framework. First, we introduce the\nLabel-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model\ntrained to learn a structured latent space where each class label is\nrepresented by a set of Gaussian components with diverse, prototypical\ncentroids. Building on this, we present LAPACE (LAtent PAth Counterfactual\nExplanations), a model-agnostic algorithm that synthesises entire paths of CE\npoints by interpolating from inputs' latent representations to those learned\nlatent centroids. This approach inherently ensures robustness to input changes,\nas all paths for a given target class converge to the same fixed centroids.\nFurthermore, the generated paths provide a spectrum of recourse options,\nallowing users to navigate the trade-off between proximity and plausibility\nwhile also encouraging robustness against model changes. In addition,\nuser-specified actionability constraints can also be easily incorporated via\nlightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive\nexperiments show that LAPACE is computationally efficient and achieves\ncompetitive performance across eight quantitative metrics.", "AI": {"tldr": "\u63d0\u51faLAPACE\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u4e2d\u521b\u5efa\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8def\u5f84\uff0c\u540c\u65f6\u6ee1\u8db3\u9c81\u68d2\u6027\u3001\u5408\u7406\u6027\u548c\u591a\u6837\u6027\u8981\u6c42", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u5904\u7406\u8f93\u5165\u6270\u52a8\u3001\u6a21\u578b\u6270\u52a8\u3001\u5408\u7406\u6027\u548c\u591a\u6837\u6027\u7b49\u591a\u65b9\u9762\u9700\u6c42", "method": "\u4f7f\u7528\u6807\u7b7e\u6761\u4ef6\u9ad8\u65af\u6df7\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668(L-GMVAE)\u5b66\u4e60\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u6f5c\u5728\u8def\u5f84\u63d2\u503c\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8def\u5f84", "result": "LAPACE\u8ba1\u7b97\u9ad8\u6548\uff0c\u5728\u516b\u4e2a\u91cf\u5316\u6307\u6807\u4e0a\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b", "conclusion": "LAPACE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9c81\u68d2\u3001\u5408\u7406\u4e14\u591a\u6837\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8def\u5f84"}}
{"id": "2510.04860", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04860", "abs": "https://arxiv.org/abs/2510.04860", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "comment": null, "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.", "AI": {"tldr": "\u8bba\u6587\u8bc6\u522b\u4e86\u81ea\u8fdb\u5316LLM\u4ee3\u7406\u7684\u5bf9\u9f50\u503e\u659c\u8fc7\u7a0b(ATP)\uff0c\u5373\u6301\u7eed\u4ea4\u4e92\u5bfc\u81f4\u4ee3\u7406\u653e\u5f03\u8bad\u7ec3\u65f6\u5efa\u7acb\u7684\u5bf9\u9f50\u7ea6\u675f\uff0c\u8f6c\u800c\u91c7\u7528\u81ea\u5229\u7b56\u7565\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u83b7\u5f97\u81ea\u8fdb\u5316\u80fd\u529b\uff0c\u5176\u957f\u671f\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u9700\u8981\u7814\u7a76\u90e8\u7f72\u540e\u51fa\u73b0\u7684\u5bf9\u9f50\u5931\u6548\u98ce\u9669\uff0c\u8fd9\u4e0e\u8bad\u7ec3\u65f6\u6545\u969c\u4e0d\u540c\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u8303\u5f0f\u5206\u6790ATP\uff1a\u81ea\u5229\u63a2\u7d22(\u91cd\u590d\u9ad8\u5956\u52b1\u504f\u5dee\u5bfc\u81f4\u4e2a\u4f53\u884c\u4e3a\u6f02\u79fb)\u548c\u6a21\u4eff\u7b56\u7565\u6269\u6563(\u5f02\u5e38\u884c\u4e3a\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4f20\u64ad)\u3002\u6784\u5efa\u53ef\u63a7\u6d4b\u8bd5\u5e73\u53f0\uff0c\u57fa\u51c6\u6d4b\u8bd5Qwen3-8B\u548cLlama-3.1-8B-Instruct\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5bf9\u9f50\u6548\u76ca\u5728\u81ea\u8fdb\u5316\u4e0b\u8fc5\u901f\u524a\u5f31\uff0c\u521d\u59cb\u5bf9\u9f50\u6a21\u578b\u6536\u655b\u5230\u672a\u5bf9\u9f50\u72b6\u6001\u3002\u591a\u4ee3\u7406\u8bbe\u7f6e\u4e2d\uff0c\u6210\u529f\u8fdd\u89c4\u5feb\u901f\u6269\u6563\u5bfc\u81f4\u96c6\u4f53\u5931\u51c6\u3002\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u8106\u5f31\u9632\u5fa1\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u5bf9\u9f50\u4e0d\u662f\u9759\u6001\u5c5e\u6027\uff0c\u800c\u662f\u8106\u5f31\u52a8\u6001\u7684\uff0c\u5728\u90e8\u7f72\u671f\u95f4\u6613\u53d7\u53cd\u9988\u9a71\u52a8\u8870\u51cf\u5f71\u54cd\u3002"}}
{"id": "2510.04861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04861", "abs": "https://arxiv.org/abs/2510.04861", "authors": ["Zihan Zhao", "Fengtao Zhou", "Ronggang Li", "Bing Chu", "Xinke Zhang", "Xueyi Zheng", "Ke Zheng", "Xiaobo Wen", "Jiabo Ma", "Yihui Wang", "Jiewei Chen", "Chengyou Zheng", "Jiangyu Zhang", "Yongqin Wen", "Jiajia Meng", "Ziqi Zeng", "Xiaoqing Li", "Jing Li", "Dan Xie", "Yaping Ye", "Yu Wang", "Hao Chen", "Muyan Cai"], "title": "A Clinical-grade Universal Foundation Model for Intraoperative Pathology", "comment": null, "summary": "Intraoperative pathology is pivotal to precision surgery, yet its clinical\nimpact is constrained by diagnostic complexity and the limited availability of\nhigh-quality frozen-section data. While computational pathology has made\nsignificant strides, the lack of large-scale, prospective validation has\nimpeded its routine adoption in surgical workflows. Here, we introduce CRISP, a\nclinical-grade foundation model developed on over 100,000 frozen sections from\neight medical centers, specifically designed to provide Clinical-grade Robust\nIntraoperative Support for Pathology (CRISP). CRISP was comprehensively\nevaluated on more than 15,000 intraoperative slides across nearly 100\nretrospective diagnostic tasks, including benign-malignant discrimination, key\nintraoperative decision-making, and pan-cancer detection, etc. The model\ndemonstrated robust generalization across diverse institutions, tumor types,\nand anatomical sites-including previously unseen sites and rare cancers. In a\nprospective cohort of over 2,000 patients, CRISP sustained high diagnostic\naccuracy under real-world conditions, directly informing surgical decisions in\n92.6% of cases. Human-AI collaboration further reduced diagnostic workload by\n35%, avoided 105 ancillary tests and enhanced detection of micrometastases with\n87.5% accuracy. Together, these findings position CRISP as a clinical-grade\nparadigm for AI-driven intraoperative pathology, bridging computational\nadvances with surgical precision and accelerating the translation of artificial\nintelligence into routine clinical practice.", "AI": {"tldr": "CRISP\u662f\u4e00\u4e2a\u4e34\u5e8a\u7ea7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u5728\u8d85\u8fc710\u4e07\u5f20\u51b0\u51bb\u5207\u7247\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u4e3a\u672f\u4e2d\u75c5\u7406\u63d0\u4f9bAI\u652f\u6301\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8bca\u65ad\u5de5\u4f5c\u91cf\u3002", "motivation": "\u672f\u4e2d\u75c5\u7406\u5bf9\u7cbe\u51c6\u624b\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u8bca\u65ad\u590d\u6742\u6027\u548c\u9ad8\u8d28\u91cf\u51b0\u51bb\u5207\u7247\u6570\u636e\u7684\u7f3a\u4e4f\u3002\u8ba1\u7b97\u75c5\u7406\u5b66\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u524d\u77bb\u6027\u9a8c\u8bc1\u963b\u788d\u4e86\u5176\u5728\u624b\u672f\u6d41\u7a0b\u4e2d\u7684\u5e38\u89c4\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e8\u4e2a\u533b\u7597\u4e2d\u5fc3\u8d85\u8fc710\u4e07\u5f20\u51b0\u51bb\u5207\u7247\u7684CRISP\u6a21\u578b\uff0c\u5e76\u5728\u8d85\u8fc715,000\u5f20\u672f\u4e2d\u5207\u7247\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u6db5\u76d6\u8fd1100\u4e2a\u56de\u987e\u6027\u8bca\u65ad\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u673a\u6784\u3001\u80bf\u7624\u7c7b\u578b\u548c\u89e3\u5256\u90e8\u4f4d\u8868\u73b0\u51fa\u7a33\u5065\u6cdb\u5316\u80fd\u529b\u3002\u5728\u524d\u77bb\u6027\u961f\u5217\u4e2d\uff0cCRISP\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u76f4\u63a5\u5f71\u54cd92.6%\u75c5\u4f8b\u7684\u624b\u672f\u51b3\u7b56\u3002\u4eba\u673a\u534f\u4f5c\u51cf\u5c1135%\u8bca\u65ad\u5de5\u4f5c\u91cf\uff0c\u907f\u514d105\u9879\u8f85\u52a9\u68c0\u67e5\uff0c\u5fae\u8f6c\u79fb\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe87.5%\u3002", "conclusion": "CRISP\u4ee3\u8868\u4e86AI\u9a71\u52a8\u672f\u4e2d\u75c5\u7406\u7684\u4e34\u5e8a\u7ea7\u8303\u5f0f\uff0c\u5c06\u8ba1\u7b97\u8fdb\u5c55\u4e0e\u624b\u672f\u7cbe\u51c6\u6027\u76f8\u7ed3\u5408\uff0c\u52a0\u901f\u4eba\u5de5\u667a\u80fd\u5411\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u7684\u8f6c\u5316\u3002"}}
{"id": "2510.04871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04871", "abs": "https://arxiv.org/abs/2510.04871", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Less is More: Recursive Reasoning with Tiny Networks", "comment": null, "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.", "AI": {"tldr": "TRM\u662f\u4e00\u79cd\u6bd4HRM\u66f4\u7b80\u5355\u7684\u9012\u5f52\u63a8\u7406\u65b9\u6cd5\uff0c\u4f7f\u7528\u4ec52\u5c42\u7684\u5fae\u5c0f\u7f51\u7edc\uff0c\u5728ARC-AGI\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u591a\u6570LLM\uff0c\u53c2\u6570\u91cf\u4ec57M\u3002", "motivation": "HRM\u5728\u89e3\u51b3\u96be\u9898\u65b9\u9762\u6709\u6f5c\u529b\u4f46\u7406\u89e3\u4e0d\u8db3\u4e14\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u65b9\u6848\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u63a8\u7406\u6a21\u578b\u3002", "method": "\u63d0\u51faTiny Recursive Model (TRM)\uff0c\u4f7f\u7528\u5355\u4e2a\u4ec52\u5c42\u7684\u5fae\u5c0f\u7f51\u7edc\u8fdb\u884c\u9012\u5f52\u63a8\u7406\uff0c\u53c2\u6570\u91cf\u4ec57M\u3002", "result": "TRM\u5728ARC-AGI-1\u4e0a\u83b7\u5f9745%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0cARC-AGI-2\u4e0a8%\uff0c\u8d85\u8d8a\u5927\u591a\u6570LLM\uff0c\u53c2\u6570\u91cf\u4e0d\u5230LLM\u76840.01%\u3002", "conclusion": "TRM\u8bc1\u660e\u4e86\u7b80\u5355\u9012\u5f52\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5c0f\u7f51\u7edc\u89e3\u51b3\u96be\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.04878", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.04878", "abs": "https://arxiv.org/abs/2510.04878", "authors": ["Xiangyang Xu", "Hongyang Gao"], "title": "Flow-Matching Based Refiner for Molecular Conformer Generation", "comment": null, "summary": "Low-energy molecular conformers generation (MCG) is a foundational yet\nchallenging problem in drug discovery. Denoising-based methods include\ndiffusion and flow-matching methods that learn mappings from a simple base\ndistribution to the molecular conformer distribution. However, these approaches\noften suffer from error accumulation during sampling, especially in the low SNR\nsteps, which are hard to train. To address these challenges, we propose a\nflow-matching refiner for the MCG task. The proposed method initializes\nsampling from mixed-quality outputs produced by upstream denoising models and\nreschedules the noise scale to bypass the low-SNR phase, thereby improving\nsample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the\ngenerator-refiner pipeline improves quality with fewer total denoising steps\nwhile preserving diversity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u80fd\u91cf\u5206\u5b50\u6784\u8c61\u751f\u6210\u7684\u6d41\u5339\u914d\u7cbe\u70bc\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u4e0a\u6e38\u53bb\u566a\u6a21\u578b\u7684\u6df7\u5408\u8d28\u91cf\u8f93\u51fa\u521d\u59cb\u5316\u91c7\u6837\uff0c\u5e76\u91cd\u65b0\u8c03\u5ea6\u566a\u58f0\u5c3a\u5ea6\u6765\u7ed5\u8fc7\u4f4e\u4fe1\u566a\u6bd4\u9636\u6bb5\uff0c\u4ece\u800c\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u566a\u65b9\u6cd5\uff08\u5982\u6269\u6563\u548c\u6d41\u5339\u914d\uff09\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u7279\u522b\u662f\u5728\u96be\u4ee5\u8bad\u7ec3\u7684\u4f4e\u4fe1\u566a\u6bd4\u6b65\u9aa4\u4e2d\uff0c\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5206\u5b50\u6784\u8c61\u751f\u6210\u7684\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u751f\u6210\u5668-\u7cbe\u70bc\u5668\u6d41\u6c34\u7ebf\uff0c\u4ece\u4e0a\u6e38\u53bb\u566a\u6a21\u578b\u7684\u6df7\u5408\u8d28\u91cf\u8f93\u51fa\u521d\u59cb\u5316\u91c7\u6837\uff0c\u5e76\u91cd\u65b0\u8c03\u5ea6\u566a\u58f0\u5c3a\u5ea6\u4ee5\u7ed5\u8fc7\u4f4e\u4fe1\u566a\u6bd4\u9636\u6bb5\uff0c\u51cf\u5c11\u603b\u53bb\u566a\u6b65\u9aa4\u7684\u540c\u65f6\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5728GEOM-QM9\u548cGEOM-Drugs\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ee5\u66f4\u5c11\u7684\u603b\u53bb\u566a\u6b65\u9aa4\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6837\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6d41\u5339\u914d\u7cbe\u70bc\u5668\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53bb\u566a\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u9636\u6bb5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u5206\u5b50\u6784\u8c61\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04888", "abs": "https://arxiv.org/abs/2510.04888", "authors": ["Alina Ermilova", "Dmitrii Kornilov", "Sofia Samoilova", "Ekaterina Laptenkova", "Anastasia Kolesnikova", "Ekaterina Podplutova", "Senotrusova Sofya", "Maksim G. Sharaev"], "title": "Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models", "comment": null, "summary": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e867\u79cd\u53d1\u73b0\u75be\u75c5\u5173\u8054\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u548c\u7ed3\u6784\u5316\u75be\u75c5\u63cf\u8ff0\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u5728\u53d1\u73b0\u65b0\u75be\u75c5\u5173\u8054\u65b9\u9762\u6f5c\u529b\u6709\u9650\u3002", "motivation": "\u624b\u52a8\u5206\u6790\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u8bc6\u522b\u75be\u75c5\u5173\u8054\u5b58\u5728\u52b3\u52a8\u5bc6\u96c6\u3001\u4e3b\u89c2\u6027\u5f3a\u548c\u4e13\u5bb6\u610f\u89c1\u5206\u6b67\u7684\u95ee\u9898\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u65b9\u6cd5\u9009\u62e9\u3001\u6570\u636e\u6e90\u9009\u62e9\u548c\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u7684\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e867\u79cd\u65b9\u6cd5\uff1a\u7edf\u8ba1\u5171\u73b0\u5206\u6790\u3001\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u9886\u57df\u7279\u5b9aBERT\u53d8\u4f53\u3001\u901a\u7528BERT\u3001\u6587\u6863\u68c0\u7d22\u4ee5\u53ca4\u79cdLLM\uff0c\u4f7f\u7528MIMIC-IV EHR\u6570\u636e\u548cICD-10\u4ee3\u7801\u4f5c\u4e3a\u6570\u636e\u6e90\u3002", "result": "\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4ea7\u751f\u7684\u75be\u75c5\u5173\u8054\u591a\u6837\u6027\u6700\u4f4e\uff0c\u8868\u660eLLM\u5728\u53d1\u73b0\u65b0\u75be\u75c5\u5173\u8054\u65b9\u9762\u6f5c\u529b\u6709\u9650\u3002", "conclusion": "\u5728\u6ca1\u6709\u533b\u7597\u5173\u8054\u771f\u5b9e\u6570\u636e\u5e93\u7684\u60c5\u51b5\u4e0b\uff0c\u7814\u7a76\u7ed3\u679c\u6784\u6210\u4e86\u6709\u4ef7\u503c\u7684\u533b\u7597\u75be\u75c5\u672c\u4f53\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u4e34\u5e8a\u7814\u7a76\u548c\u533b\u7597AI\u5e94\u7528\u7684\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2510.04900", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04900", "abs": "https://arxiv.org/abs/2510.04900", "authors": ["Nick Jan\u00dfen", "Melanie Schaller", "Bodo Rosenhahn"], "title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models", "comment": "Number of pages: 13 Number of figures: 16 Number of Tables: 1\n  Submitted to: IEEE Transactions on Signal Processing", "summary": "Understanding the robustness of deep learning models for multivariate\nlong-term time series forecasting (M-LTSF) remains challenging, as evaluations\ntypically rely on real-world datasets with unknown noise properties. We propose\na simulation-based evaluation framework that generates parameterizable\nsynthetic datasets, where each dataset instance corresponds to a different\nconfiguration of signal components, noise types, signal-to-noise ratios, and\nfrequency characteristics. These configurable components aim to model\nreal-world multivariate time series data without the ambiguity of unknown\nnoise. This framework enables fine-grained, systematic evaluation of M-LTSF\nmodels under controlled and diverse scenarios. We benchmark four representative\narchitectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear\n(linear), and Autoformer (decomposition-based). Our analysis reveals that all\nmodels degrade severely when lookback windows cannot capture complete periods\nof seasonal patters in the data. S-Mamba and Autoformer perform best on\nsawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.\nWhite and Brownian noise universally degrade performance with lower\nsignal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer\nshows seasonal-noise vulnerability. Further spectral analysis shows that\nS-Mamba and iTransformer achieve superior frequency reconstruction. This\ncontrolled approach, based on our synthetic and principle-driven testbed,\noffers deeper insights into model-specific strengths and limitations through\nthe aggregation of MSE scores and provides concrete guidance for model\nselection based on signal characteristics and noise conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u914d\u7f6e\u7684\u5408\u6210\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u591a\u5143\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4fe1\u53f7\u6a21\u5f0f\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u7406\u89e3\u591a\u5143\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u4e8e\u5177\u6709\u672a\u77e5\u566a\u58f0\u5c5e\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u62df\u8bc4\u4f30\u6846\u67b6\uff0c\u751f\u6210\u53ef\u53c2\u6570\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u7684\u4fe1\u53f7\u7ec4\u4ef6\u3001\u566a\u58f0\u7c7b\u578b\u3001\u4fe1\u566a\u6bd4\u548c\u9891\u7387\u7279\u5f81\u914d\u7f6e\uff0c\u5e76\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u67b6\u6784\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u56de\u770b\u7a97\u53e3\u65e0\u6cd5\u6355\u83b7\u5b8c\u6574\u5b63\u8282\u6027\u6a21\u5f0f\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff1bS-Mamba\u548cAutoformer\u5728\u952f\u9f7f\u6ce2\u6a21\u5f0f\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cR-Linear\u548ciTransformer\u504f\u597d\u6b63\u5f26\u4fe1\u53f7\uff1b\u767d\u566a\u58f0\u548c\u5e03\u6717\u566a\u58f0\u666e\u904d\u964d\u4f4e\u6027\u80fd\uff0cS-Mamba\u5bf9\u8d8b\u52bf\u566a\u58f0\u654f\u611f\uff0ciTransformer\u5bf9\u5b63\u8282\u6027\u566a\u58f0\u654f\u611f\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u5408\u6210\u548c\u539f\u5219\u9a71\u52a8\u6d4b\u8bd5\u5e73\u53f0\u7684\u53d7\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408MSE\u5206\u6570\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u7279\u5b9a\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u5e76\u4e3a\u57fa\u4e8e\u4fe1\u53f7\u7279\u5f81\u548c\u566a\u58f0\u6761\u4ef6\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2510.04901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04901", "abs": "https://arxiv.org/abs/2510.04901", "authors": ["Jonathan Cola\u00e7o Carr", "Qinyi Sun", "Cameron Allen"], "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects", "comment": "Reinforcement Learning Journal 2025", "summary": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba9\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u5b66\u4e60\u4e13\u6ce8\u6280\u80fd\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u6280\u80fd\u80fd\u591f\u9488\u5bf9\u548c\u63a7\u5236\u7279\u5b9a\u7684\u72b6\u6001\u53d8\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u3001\u89e3\u9501\u65b0\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u81ea\u52a8\u907f\u514d\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u5f80\u5f80\u5ffd\u7565\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e2d\u5b58\u5728\u7684\u81ea\u7136\u72b6\u6001\u53d8\u91cf\uff0c\u5bfc\u81f4\u53d1\u73b0\u7684\u6280\u80fd\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u63a7\u5236\uff0c\u8fd9\u4f1a\u663e\u8457\u5f71\u54cd\u63a2\u7d22\u6548\u7387\u3001\u589e\u52a0\u5b66\u4e60\u96be\u5ea6\uff0c\u5e76\u5728\u76ee\u6807\u4e0d\u660e\u786e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4ea7\u751f\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u4f7f\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u4e13\u6ce8\u6280\u80fd\u2014\u2014\u5373\u9488\u5bf9\u548c\u63a7\u5236\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u6280\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u89e3\u9501\u4e86\u65b0\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u81ea\u52a8\u907f\u514d\u4e86\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u4e13\u6ce8\u6280\u80fd\u6765\u63a7\u5236\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u6539\u5584\u63a2\u7d22\u6548\u7387\u548c\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2510.04902", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04902", "abs": "https://arxiv.org/abs/2510.04902", "authors": ["Johannes Liebenow", "Thorsten Peinemann", "Esfandiar Mohammadi"], "title": "DP-HYPE: Distributed Differentially Private Hyperparameter Search", "comment": null, "summary": "The tuning of hyperparameters in distributed machine learning can\nsubstantially impact model performance. When the hyperparameters are tuned on\nsensitive data, privacy becomes an important challenge and to this end,\ndifferential privacy has emerged as the de facto standard for provable privacy.\nA standard setting when performing distributed learning tasks is that clients\nagree on a shared setup, i.e., find a compromise from a set of hyperparameters,\nlike the learning rate of the model to be trained. Yet, prior work on\ndifferentially private hyperparameter tuning either uses computationally\nexpensive cryptographic protocols, determines hyperparameters separately for\neach client, or applies differential privacy locally, which can lead to\nundesirable utility-privacy trade-offs.\n  In this work, we present our algorithm DP-HYPE, which performs a distributed\nand privacy-preserving hyperparameter search by conducting a distributed voting\nbased on local hyperparameter evaluations of clients. In this way, DP-HYPE\nselects hyperparameters that lead to a compromise supported by the majority of\nclients, while maintaining scalability and independence from specific learning\ntasks. We prove that DP-HYPE preserves the strong notion of differential\nprivacy called client-level differential privacy and, importantly, show that\nits privacy guarantees do not depend on the number of hyperparameters. We also\nprovide bounds on its utility guarantees, that is, the probability of reaching\na compromise, and implement DP-HYPE as a submodule in the popular Flower\nframework for distributed machine learning. In addition, we evaluate\nperformance on multiple benchmark data sets in iid as well as multiple non-iid\nsettings and demonstrate high utility of DP-HYPE even under small privacy\nbudgets.", "AI": {"tldr": "DP-HYPE\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u8d85\u53c2\u6570\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5ba2\u6237\u7aef\u672c\u5730\u8d85\u53c2\u6570\u8bc4\u4f30\u7684\u5206\u5e03\u5f0f\u6295\u7968\u6765\u9009\u62e9\u591a\u6570\u5ba2\u6237\u7aef\u652f\u6301\u7684\u6298\u4e2d\u8d85\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u4e0e\u5177\u4f53\u5b66\u4e60\u4efb\u52a1\u7684\u72ec\u7acb\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u91cd\u5927\u3002\u5f53\u5728\u654f\u611f\u6570\u636e\u4e0a\u8c03\u6574\u8d85\u53c2\u6570\u65f6\uff0c\u9690\u79c1\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u5dee\u5206\u9690\u79c1\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5355\u72ec\u786e\u5b9a\u8d85\u53c2\u6570\uff0c\u8981\u4e48\u5e94\u7528\u672c\u5730\u5dee\u5206\u9690\u79c1\u5bfc\u81f4\u4e0d\u7406\u60f3\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002", "method": "\u63d0\u51faDP-HYPE\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5ba2\u6237\u7aef\u672c\u5730\u8d85\u53c2\u6570\u8bc4\u4f30\u7684\u5206\u5e03\u5f0f\u6295\u7968\u6765\u6267\u884c\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u8d85\u53c2\u6570\u641c\u7d22\u3002\u7b97\u6cd5\u9009\u62e9\u591a\u6570\u5ba2\u6237\u7aef\u652f\u6301\u7684\u6298\u4e2d\u8d85\u53c2\u6570\uff0c\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u4efb\u52a1\u72ec\u7acb\u6027\u3002", "result": "\u8bc1\u660eDP-HYPE\u4fdd\u62a4\u5ba2\u6237\u7aef\u7ea7\u5dee\u5206\u9690\u79c1\uff0c\u4e14\u9690\u79c1\u4fdd\u8bc1\u4e0d\u4f9d\u8d56\u4e8e\u8d85\u53c2\u6570\u6570\u91cf\u3002\u5728iid\u548c\u975eiid\u8bbe\u7f6e\u4e0b\u5bf9\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5373\u4f7f\u5728\u5c0f\u7684\u9690\u79c1\u9884\u7b97\u4e0b\u4e5f\u8868\u73b0\u51fa\u9ad8\u6548\u7528\u3002", "conclusion": "DP-HYPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5f3a\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u4e2a\u6027\u5316\u8bbe\u7f6e\u548c\u6548\u7528-\u9690\u79c1\u6743\u8861\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.04908", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04908", "abs": "https://arxiv.org/abs/2510.04908", "authors": ["Haotian Gao", "Zheng Dong", "Jiawei Yong", "Shintaro Fukushima", "Kenjiro Taura", "Renhe Jiang"], "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning", "comment": "Accepted at NeurIPS 2025", "summary": "Spatio-temporal forecasting is essential for real-world applications such as\ntraffic management and urban computing. Although recent methods have shown\nimproved accuracy, they often fail to account for dynamic deviations between\ncurrent inputs and historical patterns. These deviations contain critical\nsignals that can significantly affect model performance. To fill this gap, we\npropose ST-SSDL, a Spatio-Temporal time series forecasting framework that\nincorporates a Self-Supervised Deviation Learning scheme to capture and utilize\nsuch deviations. ST-SSDL anchors each input to its historical average and\ndiscretizes the latent space using learnable prototypes that represent typical\nspatio-temporal patterns. Two auxiliary objectives are proposed to refine this\nstructure: a contrastive loss that enhances inter-prototype discriminability\nand a deviation loss that regularizes the distance consistency between input\nrepresentations and corresponding prototypes to quantify deviation. Optimized\njointly with the forecasting objective, these components guide the model to\norganize its hidden space and improve generalization across diverse input\nconditions. Experiments on six benchmark datasets show that ST-SSDL\nconsistently outperforms state-of-the-art baselines across multiple metrics.\nVisualizations further demonstrate its ability to adaptively respond to varying\nlevels of deviation in complex spatio-temporal scenarios. Our code and datasets\nare available at https://github.com/Jimmy-7664/ST-SSDL.", "AI": {"tldr": "\u63d0\u51fa\u4e86ST-SSDL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u504f\u5dee\u5b66\u4e60\u65b9\u6848\u6765\u6355\u6349\u548c\u5229\u7528\u5f53\u524d\u8f93\u5165\u4e0e\u5386\u53f2\u6a21\u5f0f\u4e4b\u95f4\u7684\u52a8\u6001\u504f\u5dee\uff0c\u63d0\u5347\u65f6\u7a7a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u9884\u6d4b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u5f53\u524d\u8f93\u5165\u4e0e\u5386\u53f2\u6a21\u5f0f\u4e4b\u95f4\u7684\u52a8\u6001\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u5305\u542b\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u4fe1\u53f7\u3002", "method": "\u4f7f\u7528\u5386\u53f2\u5e73\u5747\u503c\u4f5c\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u539f\u578b\u5bf9\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u504f\u5dee\u635f\u5931\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\u6765\u4f18\u5316\u6a21\u578b\u7ed3\u6784\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cST-SSDL\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ST-SSDL\u80fd\u591f\u81ea\u9002\u5e94\u5730\u54cd\u5e94\u590d\u6742\u65f6\u7a7a\u573a\u666f\u4e2d\u4e0d\u540c\u7a0b\u5ea6\u7684\u504f\u5dee\uff0c\u901a\u8fc7\u7ec4\u7ec7\u9690\u85cf\u7a7a\u95f4\u7ed3\u6784\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04910", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04910", "abs": "https://arxiv.org/abs/2510.04910", "authors": ["Jie Yang", "Kexin Zhang", "Guibin Zhang", "Philip S. Yu", "Kaize Ding"], "title": "Glocal Information Bottleneck for Time Series Imputation", "comment": null, "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGlocal-IB\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40\u5bf9\u9f50\u635f\u5931\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4e2d\u9ad8\u7f3a\u5931\u7387\u4e0b\u7684\u4f18\u5316\u56f0\u5883\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u6a21\u578b\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u867d\u7136\u8bad\u7ec3\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u63a8\u7406\u9636\u6bb5\u4f1a\u4ea7\u751f\u8f83\u5dee\u7684\u63d2\u8865\u7ed3\u679c\u548c\u626d\u66f2\u7684\u6f5c\u5728\u8868\u793a\u5206\u5e03\uff0c\u8fd9\u8868\u660e\u5f53\u524d\u4f18\u5316\u76ee\u6807\u7f3a\u4e4f\u5168\u5c40\u6307\u5bfc\u3002", "method": "\u63d0\u51faGlocal-IB\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u6807\u51c6\u4fe1\u606f\u74f6\u9888\u6846\u67b6\u57fa\u7840\u4e0a\u5f15\u5165\u5168\u5c40\u5bf9\u9f50\u635f\u5931\uff0c\u8be5\u635f\u5931\u901a\u8fc7\u53ef\u5904\u7406\u7684\u4e92\u4fe1\u606f\u8fd1\u4f3c\u63a8\u5bfc\uff0c\u5c06\u63a9\u7801\u8f93\u5165\u7684\u6f5c\u5728\u8868\u793a\u4e0e\u5176\u539f\u59cb\u89c2\u6d4b\u5bf9\u5e94\u7269\u5bf9\u9f50\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\uff0cGlocal-IB\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u5e26\u6765\u6301\u7eed\u6539\u8fdb\u7684\u6027\u80fd\u548c\u4e00\u81f4\u7684\u6f5c\u5728\u8868\u793a\u5bf9\u9f50\u3002", "conclusion": "Glocal-IB\u80fd\u591f\u5e2e\u52a9\u6a21\u578b\u5728\u6291\u5236\u7f3a\u5931\u503c\u5f15\u8d77\u7684\u566a\u58f0\u7684\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u4ece\u800c\u5728\u9ad8\u7f3a\u5931\u7387\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04927", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04927", "abs": "https://arxiv.org/abs/2510.04927", "authors": ["Usman Akram", "Yiyue Chen", "Haris Vikalo"], "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data", "comment": null, "summary": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.", "AI": {"tldr": "FedSSL-AMC\uff1a\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u7684\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u6807\u7b7eI/Q\u5e8f\u5217\u7684triplet-loss\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u5c0f\u6837\u672cSVM\u5206\u7c7b\u5668\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u3001\u901a\u4fe1\u5f00\u9500\u548c\u4fe1\u9053\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u96c6\u4e2d\u5f0f\u8bad\u7ec3AMC\u6a21\u578b\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3001\u901a\u4fe1\u5f00\u9500\u5927\u4e14\u5bf9\u4fe1\u9053\u53d8\u5316\u4e0d\u9c81\u68d2\uff0c\u8054\u90a6\u5b66\u4e60\u867d\u7136\u907f\u514d\u4e86\u6570\u636e\u96c6\u4e2d\u4f46\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u975eIID\u6570\u636e\u5206\u5e03\u548c\u6807\u7b7e\u6837\u672c\u6709\u9650\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faFedSSL-AMC\uff0c\u5728\u5ba2\u6237\u7aef\u4f7f\u7528\u56e0\u679c\u65f6\u95f4\u81a8\u80c0CNN\u548ctriplet-loss\u81ea\u76d1\u7763\u5b66\u4e60\u65e0\u6807\u7b7eI/Q\u5e8f\u5217\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u5c0f\u6837\u672cSVM\u5206\u7c7b\u5668\u8fdb\u884c\u4e0b\u6e38\u5206\u7c7b\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u65e0\u7ebf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5f02\u6784SNR\u3001\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u548c\u975eIID\u6807\u7b7e\u5206\u5e03\u4e0b\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FedSSL-AMC\u901a\u8fc7\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86AMC\u4e2d\u7684\u9690\u79c1\u3001\u901a\u4fe1\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u5f02\u6784\u4fe1\u9053\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04930", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04930", "abs": "https://arxiv.org/abs/2510.04930", "authors": ["Ali Saheb Pasand", "Elvis Dohmatob"], "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking", "comment": null, "summary": "Grokking is the phenomenon whereby, unlike the training performance, which\npeaks early in the training process, the test/generalization performance of a\nmodel stagnates over arbitrarily many epochs and then suddenly jumps to usually\nclose to perfect levels. In practice, it is desirable to reduce the length of\nsuch plateaus, that is to make the learning process \"grok\" faster. In this\nwork, we provide new insights into grokking. First, we show both empirically\nand theoretically that grokking can be induced by asymmetric speeds of\n(stochastic) gradient descent, along different principal (i.e singular\ndirections) of the gradients. We then propose a simple modification that\nnormalizes the gradients so that dynamics along all the principal directions\nevolves at exactly the same speed. Then, we establish that this modified\nmethod, which we call egalitarian gradient descent (EGD) and can be seen as a\ncarefully modified form of natural gradient descent, groks much faster. In\nfact, in some cases the stagnation is completely removed. Finally, we\nempirically show that on classical arithmetic problems such as modular addition\nand sparse parity problem which this stagnation has been widely observed and\nintensively studied, that our proposed method eliminates the plateaus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u5e73\u7b49\u68af\u5ea6\u4e0b\u964d(EGD)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u68af\u5ea6\u6765\u6d88\u9664\u4e0d\u540c\u4e3b\u65b9\u5411\u4e0a\u7684\u4e0d\u5bf9\u79f0\u5b66\u4e60\u901f\u5ea6\uff0c\u4ece\u800c\u663e\u8457\u52a0\u901fGrokking\u73b0\u8c61\u7684\u53d1\u751f\u3002", "motivation": "Grokking\u73b0\u8c61\u4e2d\u6d4b\u8bd5\u6027\u80fd\u4f1a\u5728\u957f\u65f6\u95f4\u505c\u6ede\u671f\u540e\u7a81\u7136\u63d0\u5347\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5e0c\u671b\u7f29\u77ed\u8fd9\u79cd\u505c\u6ede\u671f\uff0c\u8ba9\u5b66\u4e60\u8fc7\u7a0b\u66f4\u5feb\u5730\"grok\"\u3002", "method": "\u63d0\u51fa\u5e73\u7b49\u68af\u5ea6\u4e0b\u964d(EGD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u68af\u5ea6\u4f7f\u6240\u6709\u4e3b\u65b9\u5411\u7684\u5b66\u4e60\u901f\u5ea6\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u53ef\u4ee5\u770b\u4f5c\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u7684\u4e00\u79cd\u6539\u8fdb\u5f62\u5f0f\u3002", "result": "\u5728\u6a21\u52a0\u6cd5\u548c\u7a00\u758f\u5947\u5076\u95ee\u9898\u7b49\u7ecf\u5178\u7b97\u672f\u95ee\u9898\u4e0a\uff0cEGD\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86grokking\u8fc7\u7a0b\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b8c\u5168\u6d88\u9664\u4e86\u505c\u6ede\u671f\u3002", "conclusion": "\u68af\u5ea6\u5728\u4e0d\u540c\u4e3b\u65b9\u5411\u4e0a\u7684\u4e0d\u5bf9\u79f0\u901f\u5ea6\u662f\u5bfc\u81f4grokking\u73b0\u8c61\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u8fc7EGD\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u5e76\u6d88\u9664\u6027\u80fd\u505c\u6ede\u3002"}}
{"id": "2510.04938", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04938", "abs": "https://arxiv.org/abs/2510.04938", "authors": ["Shiwen Qin", "Alexander Auras", "Shay B. Cohen", "Elliot J. Crowley", "Michael Moeller", "Linus Ericsson", "Jovita Lukasik"], "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures", "comment": "Our code is available at: https://github.com/shiwenqin/ONNX-Net", "summary": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.", "AI": {"tldr": "\u63d0\u51fa\u4e86ONNX-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\u548cONNX-Net\u6587\u672c\u7f16\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4e2d\u6027\u80fd\u8bc4\u4f30\u74f6\u9888\u548c\u641c\u7d22\u7a7a\u95f4\u4f9d\u8d56\u6027\u95ee\u9898", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u6027\u80fd\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u57fa\u4e8e\u5355\u5143\u7684\u641c\u7d22\u7a7a\u95f4\u548c\u7279\u5b9a\u7f16\u7801\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027", "method": "\u521b\u5efa\u5305\u542b60\u4e07+{\u67b6\u6784,\u7cbe\u5ea6}\u5bf9\u7684ONNX-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684ONNX-Net\u7edf\u4e00\u7f51\u7edc\u8868\u793a\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5728\u4e0d\u540c\u641c\u7d22\u7a7a\u95f4\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u9700\u5c11\u91cf\u9884\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u4efb\u610f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5373\u65f6\u8bc4\u4f30", "conclusion": "ONNX-Bench\u548cONNX-Net\u63d0\u4f9b\u4e86\u8de8\u641c\u7d22\u7a7a\u95f4\u7684\u7edf\u4e00\u8868\u793a\u548c\u6027\u80fd\u9884\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027"}}
{"id": "2510.04944", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04944", "abs": "https://arxiv.org/abs/2510.04944", "authors": ["Jerry Yao-Chieh Hu", "Xiwen Zhang", "Weimin Wu", "Han Liu"], "title": "On Structured State-Space Duality", "comment": null, "summary": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027(SSD)\uff0c\u4ece\u6807\u91cf\u6052\u7b49\u72b6\u6001\u77e9\u9635\u63a8\u5e7f\u5230\u4e00\u822c\u5bf9\u89d2SSM\uff0c\u5efa\u7acb\u4e86\u5bf9\u89d2SSM\u4e0e1-\u534a\u53ef\u5206\u56e0\u679c\u63a9\u7801\u6ce8\u610f\u529b\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8fd9\u79cd\u5bf9\u5076\u6027\u65e0\u6cd5\u6269\u5c55\u5230\u6807\u51c6softmax\u6ce8\u610f\u529b\u7684\u539f\u56e0\u3002", "motivation": "\u6269\u5c55SSD\u5bf9\u5076\u6027\u5230\u66f4\u4e00\u822c\u7684\u5bf9\u89d2\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4e30\u5bcc\u6a21\u578b\u52a8\u6001\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u52a0\u6df1\u5bf9\u5faa\u73afSSM\u4e0eTransformer\u4e4b\u95f4\u8054\u7cfb\u7684\u7406\u89e3\u3002", "method": "\u5c06SSD\u4ece\u6807\u91cf\u6052\u7b49\u72b6\u6001\u77e9\u9635\u63a8\u5e7f\u5230\u4e00\u822c\u5bf9\u89d2SSM\uff0c\u5efa\u7acb\u5bf9\u89d2SSM\u4e0e1-\u534a\u53ef\u5206\u56e0\u679c\u63a9\u7801\u6ce8\u610f\u529b\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u5206\u6790\u5bf9\u5076\u6027\u5728\u6807\u51c6softmax\u6ce8\u610f\u529b\u4e2d\u5931\u6548\u7684\u539f\u56e0\u3002", "result": "\u8bc1\u660e\u5bf9\u89d2SSM\u4e0e\u6807\u91cf\u60c5\u51b5\u5177\u6709\u76f8\u540c\u7684\u8bad\u7ec3\u590d\u6742\u5ea6\u4e0b\u754c\u4f46\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u52a8\u6001\u7279\u6027\uff0c\u5efa\u7acb\u4e86SSM\u4e0e1-\u534a\u53ef\u5206\u63a9\u7801\u6ce8\u610f\u529b\u7b49\u4ef7\u7684\u5145\u8981\u6761\u4ef6\uff0c\u53d1\u73b0\u5bf9\u5076\u6027\u65e0\u6cd5\u6269\u5c55\u5230softmax\u6ce8\u610f\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u52a0\u5f3a\u4e86\u5faa\u73afSSM\u4e0eTransformer\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u8bbe\u8ba1\u8868\u8fbe\u529b\u5f3a\u4e14\u9ad8\u6548\u7684\u5e8f\u5217\u6a21\u578b\u62d3\u5bbd\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u3002"}}
{"id": "2510.04951", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04951", "abs": "https://arxiv.org/abs/2510.04951", "authors": ["Jayanta Mandi", "Marianne Defresne", "Senne Berden", "Tias Guns"], "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints", "comment": null, "summary": "When some parameters of a constrained optimization problem (COP) are\nuncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising\ntwo stages -- the prediction of the unknown parameters from contextual\ninformation and the subsequent optimization using those predicted parameters.\nDecision-focused learning (DFL) implements the first stage by training a\nmachine learning (ML) model to optimize the quality of the decisions made using\nthe predicted parameters. When parameters in the constraints of a COP are\npredicted, the predicted parameters can lead to infeasible solutions.\nTherefore, it is important to simultaneously manage both feasibility and\ndecision quality. We develop a DFL framework for predicting constraint\nparameters in a generic COP. While prior works typically assume that the\nunderlying optimization problem is a linear program (LP) or integer linear\nprogram (ILP), our approach makes no such assumption. We derive two novel loss\nfunctions based on maximum likelihood estimation (MLE): the first one penalizes\ninfeasibility (by penalizing when the predicted parameters lead to infeasible\nsolutions), and the second one penalizes suboptimal decisions (by penalizing\nwhen the true optimal solution is infeasible under the predicted parameters).\nWe introduce a single tunable parameter to form a weighted average of the two\nlosses, allowing decision-makers to balance suboptimality and feasibility. We\nexperimentally demonstrate that adjusting this parameter provides a\ndecision-maker the control over the trade-off between the two. Moreover, across\nseveral COP instances, we find that for a single value of the tunable\nparameter, our method matches the performance of the existing baselines on\nsuboptimality and feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u53c2\u6570\u7684\u51b3\u7b56\u5bfc\u5411\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u63a8\u5bfc\u51fa\u4e24\u4e2a\u635f\u5931\u51fd\u6570\u6765\u5e73\u8861\u53ef\u884c\u6027\u548c\u51b3\u7b56\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u53ef\u8c03\u53c2\u6570\u6765\u63a7\u5236\u4e24\u8005\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u53c2\u6570\u4e0d\u786e\u5b9a\u65f6\uff0c\u9884\u6d4b\u7136\u540e\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u540c\u65f6\u7ba1\u7406\u53ef\u884c\u6027\u548c\u51b3\u7b56\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4f18\u5316\u95ee\u9898\u662f\u7ebf\u6027\u89c4\u5212\u6216\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u4f46\u5b9e\u9645\u4e2d\u5f88\u591a\u95ee\u9898\u4e0d\u6ee1\u8db3\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u51b3\u7b56\u5bfc\u5411\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u63a8\u5bfc\u4e86\u4e24\u4e2a\u635f\u5931\u51fd\u6570\uff1a\u4e00\u4e2a\u60e9\u7f5a\u4e0d\u53ef\u884c\u6027\uff0c\u53e6\u4e00\u4e2a\u60e9\u7f5a\u6b21\u4f18\u51b3\u7b56\u3002\u5f15\u5165\u53ef\u8c03\u53c2\u6570\u5f62\u6210\u4e24\u4e2a\u635f\u5931\u7684\u52a0\u6743\u5e73\u5747\uff0c\u8ba9\u51b3\u7b56\u8005\u80fd\u591f\u5e73\u8861\u6b21\u4f18\u6027\u548c\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8c03\u6574\u53c2\u6570\u80fd\u591f\u6709\u6548\u63a7\u5236\u6b21\u4f18\u6027\u548c\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5728\u591a\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u5b9e\u4f8b\u4e2d\uff0c\u5bf9\u4e8e\u5355\u4e2a\u53c2\u6570\u503c\uff0c\u8be5\u65b9\u6cd5\u5728\u6b21\u4f18\u6027\u548c\u53ef\u884c\u6027\u65b9\u9762\u4e0e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9884\u6d4b\u7ea6\u675f\u53c2\u6570\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u5141\u8bb8\u51b3\u7b56\u8005\u6839\u636e\u5177\u4f53\u9700\u6c42\u5728\u53ef\u884c\u6027\u548c\u51b3\u7b56\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u4f18\u5316\u95ee\u9898\u7c7b\u578b\u5047\u8bbe\u3002"}}
{"id": "2510.04974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04974", "abs": "https://arxiv.org/abs/2510.04974", "authors": ["Allen Daniel Sunny"], "title": "StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R", "comment": "8 pages, 4 figures. Part of the R package StructuralDecompose\n  (https://cran.r-project.org/web/packages/StructuralDecompose/index.html)", "summary": "We present StructuralDecompose, an R package for modular and interpretable\ntime series decomposition. Unlike existing approaches that treat decomposition\nas a monolithic process, StructuralDecompose separates the analysis into\ndistinct components: changepoint detection, anomaly detection, smoothing, and\ndecomposition. This design provides flexibility and robust- ness, allowing\nusers to tailor methods to specific time series characteristics. We demonstrate\nthe package on simulated and real-world datasets, benchmark its performance\nagainst state-of-the- art tools such as Rbeast and autostsm, and discuss its\nrole in interpretable machine learning workflows.", "AI": {"tldr": "StructuralDecompose\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u7684R\u5305\uff0c\u901a\u8fc7\u5206\u79bb\u53d8\u5316\u70b9\u68c0\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u5e73\u6ed1\u548c\u5206\u89e3\u7b49\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u89c6\u4e3a\u5355\u4e00\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002StructuralDecompose\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5206\u79bb\u4e3a\u56db\u4e2a\u72ec\u7acb\u7ec4\u4ef6\uff1a\u53d8\u5316\u70b9\u68c0\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u5e73\u6ed1\u548c\u5206\u89e3\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u65f6\u95f4\u5e8f\u5217\u7279\u6027\u5b9a\u5236\u65b9\u6cd5\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5305\u7684\u6027\u80fd\uff0c\u5e76\u4e0eRbeast\u3001autostsm\u7b49\u5148\u8fdb\u5de5\u5177\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u5305\u5728\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7a33\u5065\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04979", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.04979", "abs": "https://arxiv.org/abs/2510.04979", "authors": ["Xuefeng Xu", "Graham Cormode"], "title": "Federated Computation of ROC and PR Curves", "comment": "23 pages", "summary": "Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are\nfundamental tools for evaluating machine learning classifiers, offering\ndetailed insights into the trade-offs between true positive rate vs. false\npositive rate (ROC) or precision vs. recall (PR). However, in Federated\nLearning (FL) scenarios, where data is distributed across multiple clients,\ncomputing these curves is challenging due to privacy and communication\nconstraints. Specifically, the server cannot access raw prediction scores and\nclass labels, which are used to compute the ROC and PR curves in a centralized\nsetting. In this paper, we propose a novel method for approximating ROC and PR\ncurves in a federated setting by estimating quantiles of the prediction score\ndistribution under distributed differential privacy. We provide theoretical\nbounds on the Area Error (AE) between the true and estimated curves,\ndemonstrating the trade-offs between approximation accuracy, privacy, and\ncommunication cost. Empirical results on real-world datasets demonstrate that\nour method achieves high approximation accuracy with minimal communication and\nstrong privacy guarantees, making it practical for privacy-preserving model\nevaluation in federated systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u901a\u8fc7\u5206\u5e03\u5f0f\u5dee\u5206\u9690\u79c1\u4f30\u8ba1\u9884\u6d4b\u5206\u6570\u5206\u5e03\u5206\u4f4d\u6570\u6765\u8fd1\u4f3cROC\u548cPR\u66f2\u7ebf\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u8bc4\u4f30\u96be\u9898\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u7531\u4e8e\u6570\u636e\u5206\u5e03\u5728\u591a\u4e2a\u5ba2\u6237\u7aef\u4e14\u5b58\u5728\u9690\u79c1\u548c\u901a\u4fe1\u9650\u5236\uff0c\u670d\u52a1\u5668\u65e0\u6cd5\u8bbf\u95ee\u539f\u59cb\u9884\u6d4b\u5206\u6570\u548c\u7c7b\u522b\u6807\u7b7e\uff0c\u5bfc\u81f4\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97ROC\u548cPR\u66f2\u7ebf\uff0c\u9700\u8981\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u5dee\u5206\u9690\u79c1\u6280\u672f\u4f30\u8ba1\u9884\u6d4b\u5206\u6570\u5206\u5e03\u7684\u5206\u4f4d\u6570\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6210\u672c\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u73b0\u8054\u90a6\u73af\u5883\u4e0b\u7684ROC\u548cPR\u66f2\u7ebf\u8fd1\u4f3c\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u771f\u5b9e\u66f2\u7ebf\u4e0e\u4f30\u8ba1\u66f2\u7ebf\u4e4b\u95f4\u9762\u79ef\u8bef\u5dee\u7684\u754c\u9650\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u4ee5\u6700\u5c0f\u901a\u4fe1\u6210\u672c\u548c\u5f3a\u9690\u79c1\u4fdd\u8bc1\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8fd1\u4f3c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\u8bc4\u4f30\u65b9\u6848\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u51c6\u786e\u7684ROC\u548cPR\u66f2\u7ebf\u8fd1\u4f3c\u3002"}}
{"id": "2510.04988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04988", "abs": "https://arxiv.org/abs/2510.04988", "authors": ["Kristi Topollai", "Anna Choromanska"], "title": "Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization", "comment": null, "summary": "The vast majority of modern deep learning models are trained with\nmomentum-based first-order optimizers. The momentum term governs the\noptimizer's memory by determining how much each past gradient contributes to\nthe current convergence direction. Fundamental momentum methods, such as\nNesterov Accelerated Gradient and the Heavy Ball method, as well as more recent\noptimizers such as AdamW and Lion, all rely on the momentum coefficient that is\ncustomarily set to $\\beta = 0.9$ and kept constant during model training, a\nstrategy widely used by practitioners, yet suboptimal. In this paper, we\nintroduce an \\textit{adaptive memory} mechanism that replaces constant momentum\nwith a dynamic momentum coefficient that is adjusted online during\noptimization. We derive our method by approximating the objective function\nusing two planes: one derived from the gradient at the current iterate and the\nother obtained from the accumulated memory of the past gradients. To the best\nof our knowledge, such a proximal framework was never used for momentum-based\noptimization. Our proposed approach is novel, extremely simple to use, and does\nnot rely on extra assumptions or hyperparameter tuning. We implement adaptive\nmemory variants of both SGD and AdamW across a wide range of learning tasks,\nfrom simple convex problems to large-scale deep learning scenarios,\ndemonstrating that our approach can outperform standard SGD and Adam with\nhand-tuned momentum coefficients. Finally, our work opens doors for new ways of\ninducing adaptivity in optimization.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u8bb0\u5fc6\u673a\u5236\uff0c\u7528\u52a8\u6001\u52a8\u91cf\u7cfb\u6570\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u52a8\u91cf\u7cfb\u6570\uff0c\u901a\u8fc7\u8fd1\u4f3c\u76ee\u6807\u51fd\u6570\u7684\u53cc\u5e73\u9762\u6846\u67b6\u5b9e\u73b0\uff0c\u5728\u591a\u79cd\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6\u4f18\u5316\u5668\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u4f7f\u7528\u56fa\u5b9a\u52a8\u91cf\u7cfb\u6570\uff08\u901a\u5e38\u8bbe\u4e3a0.9\uff09\uff0c\u8fd9\u79cd\u505a\u6cd5\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\u4f46\u5e76\u975e\u6700\u4f18\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u52a8\u6001\u52a8\u91cf\u8c03\u6574\u673a\u5236\u3002", "method": "\u5f15\u5165\u81ea\u9002\u5e94\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u7528\u53cc\u5e73\u9762\u8fd1\u4f3c\u76ee\u6807\u51fd\u6570\uff1a\u4e00\u4e2a\u6765\u81ea\u5f53\u524d\u8fed\u4ee3\u7684\u68af\u5ea6\uff0c\u53e6\u4e00\u4e2a\u6765\u81ea\u8fc7\u53bb\u68af\u5ea6\u7684\u7d2f\u79ef\u8bb0\u5fc6\uff0c\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u52a8\u91cf\u7cfb\u6570\u3002", "result": "\u5728\u4ece\u7b80\u5355\u51f8\u95ee\u9898\u5230\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u573a\u666f\u7684\u5e7f\u6cdb\u4efb\u52a1\u4e2d\uff0c\u81ea\u9002\u5e94\u8bb0\u5fc6\u7248\u672c\u7684SGD\u548cAdamW\u5747\u4f18\u4e8e\u624b\u52a8\u8c03\u53c2\u7684\u6807\u51c6\u7248\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65b0\u9896\u3001\u7b80\u5355\u6613\u7528\u4e14\u65e0\u9700\u989d\u5916\u5047\u8bbe\u6216\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u4e3a\u4f18\u5316\u4e2d\u7684\u81ea\u9002\u5e94\u673a\u5236\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.04995", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.04995", "abs": "https://arxiv.org/abs/2510.04995", "authors": ["Xuefeng Xu", "Graham Cormode"], "title": "Power Transform Revisited: Numerically Stable, and Federated", "comment": "25 pages", "summary": "Power transforms are popular parametric techniques for making data more\nGaussian-like, and are widely used as preprocessing steps in statistical\nanalysis and machine learning. However, we find that direct implementations of\npower transforms suffer from severe numerical instabilities, which can lead to\nincorrect results or even crashes. In this paper, we provide a comprehensive\nanalysis of the sources of these instabilities and propose effective remedies.\nWe further extend power transforms to the federated learning setting,\naddressing both numerical and distributional challenges that arise in this\ncontext. Experiments on real-world datasets demonstrate that our methods are\nboth effective and robust, substantially improving stability compared to\nexisting approaches.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5e42\u53d8\u6362\u5728\u6570\u503c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u3002", "motivation": "\u5e42\u53d8\u6362\u662f\u4f7f\u6570\u636e\u66f4\u63a5\u8fd1\u9ad8\u65af\u5206\u5e03\u7684\u91cd\u8981\u9884\u5904\u7406\u6280\u672f\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u5b58\u5728\u4e25\u91cd\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7ed3\u679c\u751a\u81f3\u7cfb\u7edf\u5d29\u6e83\u3002", "method": "\u5168\u9762\u5206\u6790\u5e42\u53d8\u6362\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u7684\u6765\u6e90\uff0c\u63d0\u51fa\u6709\u6548\u7684\u8865\u6551\u63aa\u65bd\uff0c\u5e76\u5c06\u5e42\u53d8\u6362\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u73af\u5883\uff0c\u89e3\u51b3\u8be5\u573a\u666f\u4e0b\u7684\u6570\u503c\u548c\u5206\u5e03\u6311\u6218\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u65e2\u6709\u6548\u53c8\u7a33\u5065\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5e42\u53d8\u6362\u7684\u6570\u503c\u7a33\u5b9a\u6027\u95ee\u9898\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u6210\u529f\u63d0\u5347\u4e86\u8be5\u6280\u672f\u5728\u7edf\u8ba1\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.04996", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04996", "abs": "https://arxiv.org/abs/2510.04996", "authors": ["Wei Xiong", "Chenlu Ye", "Baohao Liao", "Hanze Dong", "Xinxing Xu", "Christof Monz", "Jiang Bian", "Nan Jiang", "Tong Zhang"], "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training", "comment": "16 pages, 6 figures", "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.", "AI": {"tldr": "\u63d0\u51faReinforce-Ada\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8eLLM\u7684\u5728\u7ebfRL\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u5206\u914d\u91c7\u6837\u52aa\u529b\u5230\u5177\u6709\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u6216\u5b66\u4e60\u6f5c\u529b\u7684\u63d0\u793a\uff0c\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9LLM\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u65f6\uff0c\u7531\u4e8e\u8de8\u63d0\u793a\u7684\u56fa\u5b9a\u5747\u5300\u91c7\u6837\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u4e0d\u7a33\u5b9a\uff0c\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u8fde\u7eed\u6d88\u9664\u8fc7\u7a0b\uff0c\u4ea4\u7ec7\u4f30\u8ba1\u548c\u91c7\u6837\uff0c\u4e00\u65e6\u6536\u96c6\u5230\u8db3\u591f\u4fe1\u53f7\u5c31\u81ea\u52a8\u505c\u6b62\u5bf9\u63d0\u793a\u7684\u91c7\u6837\u3002\u901a\u8fc7\u5f62\u6210\u5177\u6709\u5f3a\u5236\u5956\u52b1\u591a\u6837\u6027\u7684\u56fa\u5b9a\u5927\u5c0f\u7ec4\uff0c\u5e76\u4f7f\u7528\u5728\u81ea\u9002\u5e94\u91c7\u6837\u9636\u6bb5\u805a\u5408\u7684\u5168\u5c40\u7edf\u8ba1\u8ba1\u7b97\u4f18\u52bf\u57fa\u7ebf\u6765\u7a33\u5b9a\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReinforce-Ada\u76f8\u6bd4GRPO\u52a0\u901f\u4e86\u6536\u655b\u5e76\u63d0\u9ad8\u4e86\u6700\u7ec8\u6027\u80fd\uff0c\u7279\u522b\u662f\u4f7f\u7528\u5e73\u8861\u91c7\u6837\u53d8\u4f53\u65f6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u65b9\u5dee\u611f\u77e5\u3001\u81ea\u9002\u5e94\u6570\u636e\u6574\u7406\u5728\u5b9e\u73b0\u63a8\u7406\u80fd\u529bLLM\u7684\u9ad8\u6548\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2510.05023", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05023", "abs": "https://arxiv.org/abs/2510.05023", "authors": ["Weixin Wang", "Haoyang Zheng", "Guang Lin", "Wei Deng", "Pan Xu"], "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective", "comment": "39 pages, 3 figures, 2 tables", "summary": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed\nbandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in\neach round to sample from the posterior, relaxing the need for conjugacy\nassumptions between priors and reward distributions in vanilla TS. However,\nthey often require approximating a different posterior distribution in\ndifferent round of the bandit problem. This requires tricky, round-specific\ntuning of hyperparameters such as dynamic learning rates, causing challenges in\nboth theoretical analysis and practical implementation. To alleviate this\nnon-stationarity, we introduce TS-SA, which incorporates stochastic\napproximation (SA) within the TS framework. In each round, TS-SA constructs a\nposterior approximation only using the most recent reward(s), performs a\nLangevin Monte Carlo (LMC) update, and applies an SA step to average noisy\nproposals over time. This can be interpreted as approximating a stationary\nposterior target throughout the entire algorithm, which further yields a fixed\nstep-size, a unified convergence analysis framework, and improved posterior\nestimates through temporal averaging. We establish near-optimal regret bounds\nfor TS-SA, with a simplified and more intuitive theoretical analysis enabled by\ninterpreting the entire algorithm as a simulation of a stationary SGLD process.\nOur empirical results demonstrate that even a single-step Langevin update with\ncertain warm-up outperforms existing methods substantially on bandit tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86TS-SA\u7b97\u6cd5\uff0c\u5c06\u968f\u673a\u903c\u8fd1(SA)\u96c6\u6210\u5230Thompson\u91c7\u6837\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u8fd1\u4f3c\u4e00\u4e2a\u56fa\u5b9a\u7684\u540e\u9a8c\u5206\u5e03\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9700\u8981\u6bcf\u8f6e\u8c03\u6574\u8d85\u53c2\u6570\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8fd1\u4f3cThompson\u91c7\u6837\u7b97\u6cd5\u9700\u8981\u5728\u4e0d\u540c\u8f6e\u6b21\u4e2d\u8fd1\u4f3c\u4e0d\u540c\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u8fd9\u5bfc\u81f4\u9700\u8981\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570\uff0c\u7ed9\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5b9e\u73b0\u5e26\u6765\u6311\u6218\u3002", "method": "\u5728\u6bcf\u8f6e\u4e2d\uff0cTS-SA\u4ec5\u4f7f\u7528\u6700\u8fd1\u7684\u5956\u52b1\u6784\u5efa\u540e\u9a8c\u8fd1\u4f3c\uff0c\u6267\u884cLangevin Monte Carlo\u66f4\u65b0\uff0c\u5e76\u5e94\u7528SA\u6b65\u9aa4\u5bf9\u65f6\u95f4\u4e0a\u7684\u566a\u58f0\u63d0\u8bae\u8fdb\u884c\u5e73\u5747\uff0c\u4ece\u800c\u8fd1\u4f3c\u4e00\u4e2a\u56fa\u5b9a\u7684\u540e\u9a8c\u76ee\u6807\u3002", "result": "\u5efa\u7acb\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u754c\uff0c\u7406\u8bba\u5206\u6790\u66f4\u7b80\u5316\u76f4\u89c2\uff0c\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u5355\u6b65Langevin\u66f4\u65b0\u5728\u67d0\u4e9b\u9884\u70ed\u6761\u4ef6\u4e0b\u4e5f\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TS-SA\u901a\u8fc7\u5f15\u5165\u968f\u673a\u903c\u8fd1\u5b9e\u73b0\u4e86\u56fa\u5b9a\u7684\u6b65\u957f\u3001\u7edf\u4e00\u7684\u6536\u655b\u5206\u6790\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5e73\u5747\u6539\u8fdb\u4e86\u540e\u9a8c\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002"}}
{"id": "2510.05024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05024", "abs": "https://arxiv.org/abs/2510.05024", "authors": ["Nevan Wichers", "Aram Ebtekar", "Ariana Azarbal", "Victor Gillioz", "Christine Ye", "Emil Ryd", "Neil Rathi", "Henry Sleight", "Alex Mallen", "Fabien Roger", "Samuel Marks"], "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment", "comment": null, "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u63a5\u79cd\u63d0\u793a\"\u7684\u7b80\u5355\u6280\u672f\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u63d0\u793a\u4e2d\u660e\u786e\u8981\u6c42\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u884c\u4e3a\uff0c\u6765\u9632\u6b62\u6a21\u578b\u5b66\u4e60\u8fd9\u4e9b\u4e0d\u826f\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u4f1a\u5728\u4e0d\u5b8c\u7f8e\u7684\u76d1\u7763\u4fe1\u53f7\u4e0b\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5956\u52b1\u653b\u51fb\u548c\u5949\u627f\u7b49\u4e0d\u826f\u884c\u4e3a\u3002\u6539\u8fdb\u76d1\u7763\u8d28\u91cf\u53ef\u80fd\u6602\u8d35\u6216\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u5728\u8bad\u7ec3\u4fe1\u53f7\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u5b66\u4e60\u884c\u4e3a\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u63a5\u79cd\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u4fee\u6539\u8bad\u7ec3\u63d0\u793a\u6765\u660e\u786e\u8981\u6c42\u4e0d\u5e0c\u671b\u51fa\u73b0\u7684\u884c\u4e3a\u3002\u4f8b\u5982\uff0c\u4e3a\u4e86\u9884\u9632\u5956\u52b1\u653b\u51fb\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u4fee\u6539\u63d0\u793a\uff0c\u8981\u6c42\u751f\u6210\u4ec5\u5728\u63d0\u4f9b\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u6709\u6548\u4f46\u5728\u5176\u4ed6\u8f93\u5165\u4e0a\u5931\u8d25\u7684\u4ee3\u7801\u3002", "result": "\u5728\u56db\u4e2a\u8bbe\u7f6e\u4e2d\u53d1\u73b0\uff0c\u63a5\u79cd\u63d0\u793a\u51cf\u5c11\u4e86\u4e0d\u826f\u884c\u4e3a\u7684\u5b66\u4e60\uff0c\u540c\u65f6\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u671f\u671b\u80fd\u529b\u7684\u5b66\u4e60\u3002\u8fd8\u53d1\u73b0\u90a3\u4e9b\u5728\u5fae\u8c03\u524d\u66f4\u80fd\u5f15\u53d1\u4e0d\u826f\u884c\u4e3a\u7684\u63d0\u793a\uff0c\u5728\u8bad\u7ec3\u4e2d\u80fd\u66f4\u6709\u6548\u5730\u9884\u9632\u8be5\u884c\u4e3a\u3002", "conclusion": "\u63a5\u79cd\u63d0\u793a\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u4ece\u5fae\u8c03\u4e2d\u7684\u6cdb\u5316\u65b9\u5f0f\uff0c\u9632\u6b62\u5b66\u4e60\u4e0d\u826f\u884c\u4e3a\uff0c\u540c\u65f6\u4e0d\u4f1a\u663e\u8457\u5e72\u6270\u671f\u671b\u80fd\u529b\u3002"}}
{"id": "2510.05036", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05036", "abs": "https://arxiv.org/abs/2510.05036", "authors": ["Sergio Rozada", "Vimal K. B.", "Andrea Cavallo", "Antonio G. Marques", "Hadi Jamali-Rad", "Elvin Isufi"], "title": "Graph-Aware Diffusion for Signal Generation", "comment": null, "summary": "We study the problem of generating graph signals from unknown distributions\ndefined over given graphs, relevant to domains such as recommender systems or\nsensor networks. Our approach builds on generative diffusion models, which are\nwell established in vision and graph generation but remain underexplored for\ngraph signals. Existing methods lack generality, either ignoring the graph\nstructure in the forward process or designing graph-aware mechanisms tailored\nto specific domains. We adopt a forward process that incorporates the graph\nthrough the heat equation. Rather than relying on the standard formulation, we\nconsider a time-warped coefficient to mitigate the exponential decay of the\ndrift term, yielding a graph-aware generative diffusion model (GAD). We analyze\nits forward dynamics, proving convergence to a Gaussian Markov random field\nwith covariance parametrized by the graph Laplacian, and interpret the backward\ndynamics as a sequence of graph-signal denoising problems. Finally, we\ndemonstrate the advantages of GAD on synthetic data, real traffic speed\nmeasurements, and a temperature sensor network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u611f\u77e5\u751f\u6210\u6269\u6563\u6a21\u578b(GAD)\uff0c\u7528\u4e8e\u5728\u56fe\u7ed3\u6784\u4e0a\u751f\u6210\u4fe1\u53f7\uff0c\u901a\u8fc7\u7ed3\u5408\u70ed\u65b9\u7a0b\u548c\u65f6\u95f4\u626d\u66f2\u7cfb\u6570\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u56fe\u7ed3\u6784\u6216\u9886\u57df\u7279\u5b9a\u8bbe\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u56fe\u7ed3\u6784\u4e0a\u751f\u6210\u672a\u77e5\u5206\u5e03\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u56fe\u7ed3\u6784\uff0c\u8981\u4e48\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u70ed\u65b9\u7a0b\u7684\u524d\u5411\u8fc7\u7a0b\uff0c\u5f15\u5165\u65f6\u95f4\u626d\u66f2\u7cfb\u6570\u6765\u7f13\u89e3\u6f02\u79fb\u9879\u7684\u6307\u6570\u8870\u51cf\uff0c\u6784\u5efa\u56fe\u611f\u77e5\u751f\u6210\u6269\u6563\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u4e86\u524d\u5411\u52a8\u6001\u6536\u655b\u5230\u5177\u6709\u56fe\u62c9\u666e\u62c9\u65af\u53c2\u6570\u5316\u534f\u65b9\u5dee\u7684\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u3001\u4ea4\u901a\u901f\u5ea6\u6d4b\u91cf\u548c\u6e29\u5ea6\u4f20\u611f\u5668\u7f51\u7edc\u4e0a\u5c55\u793a\u4e86\u4f18\u52bf\u3002", "conclusion": "GAD\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u56fe\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u56fe\u4fe1\u53f7\u751f\u6210\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05040", "abs": "https://arxiv.org/abs/2510.05040", "authors": ["Jihoon Lee", "Hoyeon Moon", "Kevin Zhai", "Arun Kumar Chithanar", "Anit Kumar Sahu", "Soummya Kar", "Chul Lee", "Souradip Chakraborty", "Amrit Singh Bedi"], "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts", "comment": null, "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u9690\u542b\u5b66\u4e60\u6df7\u5408\u534a\u81ea\u56de\u5f52\u4e13\u5bb6\uff0c\u4e0d\u540c\u751f\u6210\u987a\u5e8f\u5c55\u73b0\u4e0d\u540c\u4e13\u4e1a\u884c\u4e3a\u3002HEX\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u5f02\u6784\u5757\u8c03\u5ea6\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u56fa\u5b9a\u8c03\u5ea6\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528dLLMs\u4e2d\u9690\u542b\u7684\u6df7\u5408\u4e13\u5bb6\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faHEX\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u96c6\u6210\u4e0d\u540c\u5757\u5927\u5c0f\u7684\u751f\u6210\u8def\u5f84\uff0c\u907f\u514d\u5355\u4e00\u56fa\u5b9a\u8c03\u5ea6\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728GSM8K\u4e0a\u51c6\u786e\u7387\u4ece24.72%\u63d0\u5347\u523088.10%(3.56\u500d)\uff0c\u5728MATH\u3001ARC-C\u548cTruthfulQA\u7b49\u57fa\u51c6\u4e0a\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HEX\u4e3adLLMs\u5efa\u7acb\u4e86\u65b0\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\uff0c\u8bc1\u660e\u63a9\u7801\u6267\u884c\u987a\u5e8f\u5728\u63a8\u7406\u6027\u80fd\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.05049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05049", "abs": "https://arxiv.org/abs/2510.05049", "authors": ["Ahmed Elhussein", "Paul Meddeb", "Abigail Newbury", "Jeanne Mirone", "Martin Stoll", "Gamze Gursoy"], "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings", "comment": null, "summary": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments.", "AI": {"tldr": "KEEP\u662f\u4e00\u4e2a\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u4e34\u5e8a\u6570\u636e\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u533b\u7597\u4ee3\u7801\u8868\u793a\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4fdd\u7559\u672c\u4f53\u5173\u7cfb\u548c\u5b66\u4e60\u5b9e\u8bc1\u6a21\u5f0f\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u4ee3\u7801\u8868\u793a\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u80fd\u6355\u6349\u6b63\u5f0f\u5173\u7cfb\u4f46\u5ffd\u7565\u73b0\u5b9e\u6a21\u5f0f\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5b66\u4e60\u5b9e\u8bc1\u5173\u8054\u4f46\u5ffd\u89c6\u533b\u5b66\u672f\u8bed\u4e2d\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u3002", "method": "KEEP\u9996\u5148\u4ece\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u5d4c\u5165\uff0c\u7136\u540e\u5728\u60a3\u8005\u8bb0\u5f55\u4e0a\u8fdb\u884c\u6b63\u5219\u5316\u8bad\u7ec3\uff0c\u81ea\u9002\u5e94\u6574\u5408\u5b9e\u8bc1\u6a21\u5f0f\u540c\u65f6\u4fdd\u7559\u672c\u4f53\u5173\u7cfb\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u8f85\u52a9\u6216\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728UK Biobank\u548cMIMIC IV\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cKEEP\u5728\u6355\u6349\u8bed\u4e49\u5173\u7cfb\u548c\u9884\u6d4b\u4e34\u5e8a\u7ed3\u679c\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "conclusion": "KEEP\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u4ee3\u7801\u8868\u793a\u7684\u6743\u8861\u95ee\u9898\uff0c\u8ba1\u7b97\u9700\u6c42\u4f4e\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2510.05054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05054", "abs": "https://arxiv.org/abs/2510.05054", "authors": ["Peter Van Katwyk", "Karianne J. Bergen"], "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model", "comment": "Reviewed and published in TMLR at\n  https://openreview.net/forum?id=xRiEdSyVjY", "summary": "Uncertainty quantification is critical for ensuring robustness in high-stakes\nmachine learning applications. We introduce HybridFlow, a modular hybrid\narchitecture that unifies the modeling of aleatoric and epistemic uncertainty\nby combining a Conditional Masked Autoregressive normalizing flow for\nestimating aleatoric uncertainty with a flexible probabilistic predictor for\nepistemic uncertainty. The framework supports integration with any\nprobabilistic model class, allowing users to easily adapt HybridFlow to\nexisting architectures without sacrificing predictive performance. HybridFlow\nimproves upon previous uncertainty quantification frameworks across a range of\nregression tasks, such as depth estimation, a collection of regression\nbenchmarks, and a scientific case study of ice sheet emulation. We also provide\nempirical results of the quantified uncertainty, showing that the uncertainty\nquantified by HybridFlow is calibrated and better aligns with model error than\nexisting methods for quantifying aleatoric and epistemic uncertainty.\nHybridFlow addresses a key challenge in Bayesian deep learning, unifying\naleatoric and epistemic uncertainty modeling in a single robust framework.", "AI": {"tldr": "HybridFlow\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u6761\u4ef6\u63a9\u7801\u81ea\u56de\u5f52\u5f52\u4e00\u5316\u6d41\u548c\u7075\u6d3b\u7684\u6982\u7387\u9884\u6d4b\u5668\uff0c\u7edf\u4e00\u5efa\u6a21\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u56de\u5f52\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u3002", "motivation": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u9ad8\u98ce\u9669\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7edf\u4e00\u5efa\u6a21\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faHybridFlow\u6df7\u5408\u67b6\u6784\uff0c\u4f7f\u7528\u6761\u4ef6\u63a9\u7801\u81ea\u56de\u5f52\u5f52\u4e00\u5316\u6d41\u4f30\u8ba1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u7075\u6d3b\u7684\u6982\u7387\u9884\u6d4b\u5668\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u4e0e\u4efb\u4f55\u6982\u7387\u6a21\u578b\u7c7b\u96c6\u6210\u3002", "result": "\u5728\u6df1\u5ea6\u4f30\u8ba1\u3001\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u548c\u51b0\u76d6\u6a21\u62df\u7b49\u4efb\u52a1\u4e2d\uff0cHybridFlow\u4f18\u4e8e\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5176\u91cf\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u7ecf\u8fc7\u6821\u51c6\u4e14\u4e0e\u6a21\u578b\u8bef\u5dee\u66f4\u4e00\u81f4\u3002", "conclusion": "HybridFlow\u89e3\u51b3\u4e86\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5355\u4e00\u9c81\u68d2\u6846\u67b6\u4e2d\u7edf\u4e00\u4e86\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002"}}
{"id": "2510.05056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05056", "abs": "https://arxiv.org/abs/2510.05056", "authors": ["Alexis Ross", "Megha Srivastava", "Jeremiah Blanchard", "Jacob Andreas"], "title": "Modeling Student Learning with 3.8 Million Program Traces", "comment": null, "summary": "As programmers write code, they often edit and retry multiple times, creating\nrich \"interaction traces\" that reveal how they approach coding tasks and\nprovide clues about their level of skill development. For novice programmers in\nparticular, these traces reflect the diverse reasoning processes they employ to\ncode, such as exploratory behavior to understand how a programming concept\nworks, re-strategizing in response to bugs, and personalizing stylistic\nchoices. In this work, we explore what can be learned from training language\nmodels on such reasoning traces: not just about code, but about coders, and\nparticularly students learning to program. We introduce a dataset of over 3.8\nmillion programming reasoning traces from users of Pencil Code, a free online\neducational platform used by students to learn simple programming concepts.\nCompared to models trained only on final programs or synthetically-generated\ntraces, we find that models trained on real traces are stronger at modeling\ndiverse student behavior. Through both behavioral and probing analyses, we also\nfind that many properties of code traces, such as goal backtracking or number\nof comments, can be predicted from learned representations of the students who\nwrite them. Building on this result, we show that we can help students recover\nfrom mistakes by steering code generation models to identify a sequence of\nedits that will results in more correct code while remaining close to the\noriginal student's style. Together, our results suggest that many properties of\ncode are properties of individual students and that training on edit traces can\nlead to models that are more steerable, more predictive of student behavior\nwhile programming, and better at generating programs in their final states.\nCode and data is available at https://github.com/meghabyte/pencilcode-public", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u5b66\u751f\u5728\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u7684\u4ea4\u4e92\u8f68\u8ff9\uff08\u7f16\u8f91\u548c\u91cd\u8bd5\u8bb0\u5f55\uff09\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b66\u751f\u7684\u7f16\u7a0b\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u7b26\u5408\u5b66\u751f\u4e2a\u4eba\u98ce\u683c\u7684\u4ee3\u7801\u4fee\u6b63\u5efa\u8bae\u3002", "motivation": "\u5b66\u751f\u7f16\u7a0b\u65f6\u7684\u4ea4\u4e92\u8f68\u8ff9\u53cd\u6620\u4e86\u4ed6\u4eec\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u6280\u80fd\u53d1\u5c55\u6c34\u5e73\uff0c\u8fd9\u4e9b\u4fe1\u606f\u6bd4\u6700\u7ec8\u4ee3\u7801\u672c\u8eab\u66f4\u80fd\u63ed\u793a\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u6536\u96c6\u4e86Pencil Code\u5e73\u53f0\u4e0a380\u4e07\u6761\u7f16\u7a0b\u4ea4\u4e92\u8f68\u8ff9\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5206\u6790\u8fd9\u4e9b\u771f\u5b9e\u8f68\u8ff9\uff0c\u5e76\u4e0e\u4ec5\u57fa\u4e8e\u6700\u7ec8\u4ee3\u7801\u6216\u5408\u6210\u8f68\u8ff9\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u5b66\u751f\u884c\u4e3a\u591a\u6837\u6027\uff0c\u5e76\u80fd\u9884\u6d4b\u5b66\u751f\u7f16\u7a0b\u7279\u5f81\uff08\u5982\u76ee\u6807\u56de\u6eaf\u3001\u6ce8\u91ca\u6570\u91cf\u7b49\uff09\uff0c\u8fd8\u80fd\u751f\u6210\u4fdd\u6301\u5b66\u751f\u4e2a\u4eba\u98ce\u683c\u7684\u4ee3\u7801\u4fee\u6b63\u5efa\u8bae\u3002", "conclusion": "\u4ee3\u7801\u7684\u8bb8\u591a\u7279\u6027\u5b9e\u9645\u4e0a\u53cd\u6620\u4e86\u5b66\u751f\u7684\u4e2a\u4eba\u7279\u5f81\uff0c\u57fa\u4e8e\u7f16\u8f91\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u66f4\u5177\u53ef\u5f15\u5bfc\u6027\uff0c\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u5b66\u751f\u884c\u4e3a\u5e76\u751f\u6210\u6700\u7ec8\u7a0b\u5e8f\u3002"}}
{"id": "2510.05060", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.05060", "abs": "https://arxiv.org/abs/2510.05060", "authors": ["Roberto Neglia", "Andrea Cini", "Michael M. Bronstein", "Filippo Maria Bianchi"], "title": "ResCP: Reservoir Conformal Prediction for Time Series Forecasting", "comment": null, "summary": "Conformal prediction offers a powerful framework for building\ndistribution-free prediction intervals for exchangeable data. Existing methods\nthat extend conformal prediction to sequential data rely on fitting a\nrelatively complex model to capture temporal dependencies. However, these\nmethods can fail if the sample size is small and often require expensive\nretraining when the underlying data distribution changes. To overcome these\nlimitations, we propose Reservoir Conformal Prediction (ResCP), a novel\ntraining-free conformal prediction method for time series. Our approach\nleverages the efficiency and representation learning capabilities of reservoir\ncomputing to dynamically reweight conformity scores. In particular, we compute\nsimilarity scores among reservoir states and use them to adaptively reweight\nthe observed residuals at each step. With this approach, ResCP enables us to\naccount for local temporal dynamics when modeling the error distribution\nwithout compromising computational scalability. We prove that, under reasonable\nassumptions, ResCP achieves asymptotic conditional coverage, and we empirically\ndemonstrate its effectiveness across diverse forecasting tasks.", "AI": {"tldr": "\u63d0\u51faReservoir Conformal Prediction (ResCP)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u50a8\u5c42\u8ba1\u7b97\u52a8\u6001\u91cd\u52a0\u6743\u4fdd\u5f62\u5206\u6570\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\u8003\u8651\u5c40\u90e8\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u5c06\u4fdd\u5f62\u9884\u6d4b\u6269\u5c55\u5230\u5e8f\u5217\u6570\u636e\u7684\u65b9\u6cd5\u9700\u8981\u62df\u5408\u590d\u6742\u6a21\u578b\u6765\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4f46\u5728\u6837\u672c\u91cf\u5c0f\u6216\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u50a8\u5c42\u8ba1\u7b97\u7684\u6548\u7387\u548c\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u8ba1\u7b97\u50a8\u5c42\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u5206\u6570\uff0c\u5e76\u7528\u4e8e\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u6bcf\u4e2a\u6b65\u9aa4\u7684\u89c2\u6d4b\u6b8b\u5dee\u3002", "result": "\u5728\u5408\u7406\u5047\u8bbe\u4e0b\u8bc1\u660eResCP\u5b9e\u73b0\u6e10\u8fd1\u6761\u4ef6\u8986\u76d6\uff0c\u5e76\u5728\u591a\u79cd\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "ResCP\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u8ba1\u7b97\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5e8f\u5217\u4fdd\u5f62\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05064", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05064", "abs": "https://arxiv.org/abs/2510.05064", "authors": ["Sara Kangaslahti", "Nihal V. Nayak", "Jonathan Geuter", "Marco Fumero", "Francesco Locatello", "David Alvarez-Melis"], "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation", "comment": "10 pages, 7 figures in main text", "summary": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u56de\u65cb\u84b8\u998f\"\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5927\u6a21\u578b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\uff0c\u7136\u540e\u91cd\u65b0\u6574\u5408\u6559\u5e08\u5c42\u5757\u6765\u751f\u6210\u4e2d\u95f4\u5c3a\u5bf8\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u6027\u80fd\u5e73\u6ed1\u6269\u5c55\u7684\u6a21\u578b\u5bb6\u65cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u72ec\u7acb\u8bad\u7ec3\u6bcf\u4e2a\u5c3a\u5bf8\u7684\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u53ea\u80fd\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u5c3a\u5bf8\u9009\u62e9\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u7ea6\u675f\u9700\u6c42\u3002", "method": "\u4ece\u5927\u578b\u6559\u5e08\u6a21\u578b\u84b8\u998f\u5230\u5c0f\u578b\u5b66\u751f\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u91cd\u65b0\u6574\u5408\u6559\u5e08\u5c42\u5757\u6765\u9010\u6b65\u91cd\u5efa\u4e2d\u95f4\u5c3a\u5bf8\u6a21\u578b\uff0c\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u751f\u6210\u7684\u96f6\u6837\u672c\u63d2\u503c\u6a21\u578b\u6027\u80fd\u5728\u5e08\u751f\u6a21\u578b\u95f4\u5e73\u6ed1\u6269\u5c55\uff0c\u901a\u5e38\u5339\u914d\u6216\u8d85\u8d8a\u540c\u5c3a\u5bf8\u7684\u9884\u8bad\u7ec3\u6216\u84b8\u998f\u6a21\u578b\u3002", "conclusion": "\u56de\u65cb\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u7ec6\u7c92\u5ea6\u6a21\u578b\u5bb6\u65cf\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u652f\u6301\u8de8\u90e8\u7f72\u73af\u5883\u7684\u7075\u6d3b\u9002\u914d\u3002"}}
{"id": "2510.05080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05080", "abs": "https://arxiv.org/abs/2510.05080", "authors": ["Yangyang Wang", "Tayo Fabusuyi"], "title": "MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis", "comment": null, "summary": "This study presents a novel small-area estimation framework to enhance urban\ntransportation planning through detailed characterization of travel behavior.\nOur approach improves on the four-step travel model by employing publicly\navailable microdata files and machine learning methods to predict travel\nbehavior for a representative, synthetic population at small geographic areas.\nThis approach enables high-resolution estimation of trip generation, trip\ndistribution, mode choice, and route assignment. Validation using ACS/PUMS\nwork-commute datasets demonstrates that our framework achieves higher accuracy\ncompared to conventional approaches. The resulting granular insights enable the\ntailoring of interventions to address localized situations and support a range\nof policy applications and targeted interventions, including the optimal\nplacement of micro-fulfillment centers, effective curb-space management, and\nthe design of more inclusive transportation solutions particularly for\nvulnerable communities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5c0f\u533a\u57df\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u5408\u6210\u4eba\u53e3\u7684\u51fa\u884c\u884c\u4e3a\uff0c\u6539\u8fdb\u4f20\u7edf\u56db\u6b65\u51fa\u884c\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u51fa\u884c\u751f\u6210\u3001\u5206\u5e03\u3001\u65b9\u5f0f\u9009\u62e9\u548c\u8def\u5f84\u5206\u914d\u4f30\u8ba1\u3002", "motivation": "\u6539\u8fdb\u4f20\u7edf\u56db\u6b65\u51fa\u884c\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u51fa\u884c\u884c\u4e3a\u7279\u5f81\uff0c\u652f\u6301\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u4e2d\u7684\u7cbe\u7ec6\u5316\u51b3\u7b56\u548c\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684\u5fae\u89c2\u6570\u636e\u6587\u4ef6\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u5c0f\u5730\u7406\u533a\u57df\u7684\u4ee3\u8868\u6027\u5408\u6210\u4eba\u53e3\u9884\u6d4b\u51fa\u884c\u884c\u4e3a\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\u8be5\u6846\u67b6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u7cbe\u7ec6\u5316\u6d1e\u5bdf\uff0c\u652f\u6301\u5fae\u5c65\u884c\u4e2d\u5fc3\u9009\u5740\u3001\u8def\u7f18\u7a7a\u95f4\u7ba1\u7406\u548c\u5305\u5bb9\u6027\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u7b49\u653f\u7b56\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u57ce\u5e02\u4ea4\u901a\u89c4\u5212\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u7684\u5c0f\u533a\u57df\u51fa\u884c\u884c\u4e3a\u4f30\u8ba1\uff0c\u652f\u6301\u9488\u5bf9\u6027\u7684\u653f\u7b56\u5e72\u9884\u548c\u66f4\u5305\u5bb9\u7684\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u8bbe\u8ba1\u3002"}}
{"id": "2510.05092", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05092", "abs": "https://arxiv.org/abs/2510.05092", "authors": ["Avichal Goel", "Yoon Kim", "Nir Shavit", "Tony T. Wang"], "title": "Learning to Interpret Weight Differences in Language Models", "comment": "The weight diffs and DIT adapters trained in the paper can be found\n  at https://huggingface.co/diff-interpretation-tuning/loras", "summary": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Diff Interpretation Tuning (DIT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u63cf\u8ff0\u81ea\u8eab\u5fae\u8c03\u5bfc\u81f4\u7684\u6743\u91cd\u53d8\u5316\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5176\u5185\u90e8\u77e5\u8bc6\u66f4\u65b0\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u540e\u7684\u6743\u91cd\u53d8\u5316\u96be\u4ee5\u89e3\u91ca\uff0c\u4e14\u5fae\u8c03\u6570\u636e\u96c6\u5f80\u5f80\u4e0d\u516c\u5f00\u6216\u8fc7\u4e8e\u5e9e\u5927\uff0c\u65e0\u6cd5\u76f4\u63a5\u5206\u6790\u6a21\u578b\u5982\u4f55\u88ab\u6539\u53d8\u3002", "method": "\u4f7f\u7528\u5408\u6210\u7684\u5e26\u6807\u7b7e\u6743\u91cd\u5dee\u5f02\u6570\u636e\u8bad\u7ec3DIT\u9002\u914d\u5668\uff0c\u8be5\u9002\u914d\u5668\u53ef\u5e94\u7528\u4e8e\u517c\u5bb9\u7684\u5fae\u8c03\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u63cf\u8ff0\u81ea\u8eab\u7684\u53d8\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u573a\u666f\u4e2d\uff08\u62a5\u544a\u9690\u85cf\u884c\u4e3a\u548c\u603b\u7ed3\u5fae\u8c03\u77e5\u8bc6\uff09\uff0c\u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u4f7f\u7528\u51c6\u786e\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5176\u5fae\u8c03\u5f15\u8d77\u7684\u4fee\u6539\u3002", "conclusion": "DIT\u65b9\u6cd5\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u9014\u5f84\u3002"}}
{"id": "2510.05095", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05095", "abs": "https://arxiv.org/abs/2510.05095", "authors": ["Mingkang Zhu", "Xi Chen", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.", "AI": {"tldr": "BVPO\u901a\u8fc7\u6df7\u5408\u9ad8\u65b9\u5dee\u7684\u8f68\u8ff9\u68af\u5ea6\u548c\u4f4e\u65b9\u5dee\u7684\u7a7a\u8f68\u8ff9\u68af\u5ea6\u6765\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u51cf\u5c11\u8f68\u8ff9\u91c7\u6837\u65b9\u5dee\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u504f\u597d\u5bf9\u9f50\u65f6\uff0c\u9700\u8981\u8fb9\u7f18\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002\u5e38\u7528\u65b9\u6cd5\u4f18\u5316\u5355\u4e00\u6837\u672c\u8f68\u8ff9\uff0c\u5bfc\u81f4\u68af\u5ea6\u65b9\u5dee\u5927\u3002", "method": "\u63d0\u51faBVPO\u65b9\u6cd5\uff0c\u6df7\u5408\u8f68\u8ff9\u68af\u5ea6\u4f30\u8ba1\u5668\u548c\u7a7a\u8f68\u8ff9\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u627e\u5230\u6700\u4f18\u6df7\u5408\u6743\u91cd\uff0c\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u5728AlpacaEval~2\u548cArena-Hard\u4e0a\u5206\u522b\u63d0\u53477.8\u548c6.8\u5206\uff1b\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53474.0\u5206\u3002", "conclusion": "\u8f68\u8ff9\u91c7\u6837\u65b9\u5dee\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4f18\u5316\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u80fd\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u5f3a\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2510.05102", "categories": ["cs.LG", "cs.AI", "cs.CG", "math.AT", "stat.ML", "55N31, 68T05, 62R40, 05C, 68R05", "I.2.6; G.2.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.05102", "abs": "https://arxiv.org/abs/2510.05102", "authors": ["Cheng Xin", "Fan Xu", "Xin Ding", "Jie Gao", "Jiaxin Ding"], "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration", "comment": "submitted to ICML 2025", "summary": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.", "AI": {"tldr": "TopInG\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u5b66\u7684\u53ef\u89e3\u91ca\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6301\u4e45\u540c\u8c03\u8bc6\u522b\u6301\u4e45\u6027\u7406\u6027\u5b50\u56fe\uff0c\u901a\u8fc7\u7406\u6027\u8fc7\u6ee4\u5b66\u4e60\u548c\u62d3\u6251\u5dee\u5f02\u7ea6\u675f\u6765\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91caGNN\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u7406\u6027\u5b50\u56fe\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9650\u5236\u4e86GNN\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6301\u4e45\u540c\u8c03\u8bc6\u522b\u6301\u4e45\u7406\u6027\u5b50\u56fe\uff0c\u4f7f\u7528\u7406\u6027\u8fc7\u6ee4\u5b66\u4e60\u5efa\u6a21\u7406\u6027\u5b50\u56fe\u7684\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u81ea\u8c03\u6574\u7684\u62d3\u6251\u5dee\u5f02\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTopInG\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406\u53d8\u5f62\u5f0f\u7406\u6027\u5b50\u56fe\u3001\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3001\u7f13\u89e3\u4f2a\u76f8\u5173\u3002", "conclusion": "TopInG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4fdd\u8bc1\u7684\u62d3\u6251\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5173\u952e\u51b3\u7b56\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
