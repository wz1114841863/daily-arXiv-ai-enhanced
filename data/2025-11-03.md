<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh是一个多租户服务架构，将AI工作流作为共享服务执行和优化，通过细粒度算子分解、去重和批处理实现成本降低和效率提升。


<details>
  <summary>Details</summary>
Motivation: AI部署正从单一LLM任务转向数据转换、微调和智能体交互的流水线模式，需要新的系统架构来优化这些工作负载的执行效率。

Method: 将工作流分解为细粒度算子并记录血缘关系，通过全局控制平面管理算子池，使用单一效用函数选择批次和工作器，数据平面采用无状态工作器和内容寻址存储。

Result: 相比基线方案，FlowMesh实现了最高3.8倍的成本降低和2.0倍的能耗降低，提供相似或更好的延迟性能，在动态和易故障环境下保持高效。

Conclusion: FlowMesh通过共享服务架构有效解决了现代AI工作流管道的执行优化问题，在成本、能耗和性能方面都取得了显著改进。

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [2] [A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration](https://arxiv.org/abs/2510.27039)
*Zhuo Zheng,Lingran Meng,Ziyu Lin*

Main category: cs.DC

TL;DR: 提出基于云的混合模型，结合时空图神经网络和Transformer架构进行交通流预测，在云平台上实现可扩展性和实时适应性


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素影响下

Method: 集成时空图神经网络(ST-GNN)和Transformer架构，利用GNN建模空间相关性，Transformer捕捉长期时间依赖，并通过特征融合整合外部上下文特征

Result: 在数据集上的实验评估显示，模型优于基线方法(LSTM、TCN、GCN、纯Transformer)，RMSE仅为17.92，MAE仅为10.53

Conclusion: 混合GNN-Transformer方法为基于云的智能交通系统应用提供了有效且可扩展的解决方案，为交通流预测提供了方法论进步，并对缓解拥堵具有实际意义

Abstract: Accurate traffic flow forecasting is essential for the development of
intelligent transportation systems (ITS), supporting tasks such as traffic
signal optimization, congestion management, and route planning. Traditional
models often fail to effectively capture complex spatial-temporal dependencies
in large-scale road networks, especially under the influence of external
factors such as weather, holidays, and traffic accidents. To address this
challenge, this paper proposes a cloud-based hybrid model that integrates
Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture
for traffic flow prediction. The model leverages the strengths of GNNs in
modeling spatial correlations across road networks and the Transformers'
ability to capture long-term temporal dependencies. External contextual
features are incorporated via feature fusion to enhance predictive accuracy.
The proposed model is deployed on a cloud computing platform to achieve
scalability and real-time adaptability. Experimental evaluation of the dataset
shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure
Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings
suggest that the hybrid GNN-Transformer approach provides an effective and
scalable solution for cloud-based ITS applications, offering methodological
advancements for traffic flow forecasting and practical implications for
congestion mitigation.

</details>


### [3] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: 提出了一种协同的张量并行和流水线并行调度方法，通过将前向和后向传播解耦为细粒度计算单元并交织编排，同时减少两种并行方式中的通信开销和流水线气泡。


<details>
  <summary>Details</summary>
Motivation: 现有的混合并行训练方法中，张量并行带来显著的集体通信开销，而流水线并行存在同步效率低下的问题。现有工作往往只从单一角度解决这些问题，缺乏协同优化。

Method: 将流水线并行的前向和后向传播解耦为细粒度计算单元，然后交织编排形成复合计算序列，从而几乎完全消除张量并行相关的气泡，并在此基础上设计流水线并行调度以最小化流水线气泡。

Result: 实验结果显示，相比现有调度方法，该方法在LLM训练中吞吐量提升最高达12%，在MLLM训练中提升最高达16%。

Conclusion: 提出的协同张量和流水线并行调度方法能有效同时减少两种并行方式中的性能瓶颈，显著提升大规模模型训练效率。

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [4] [A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination](https://arxiv.org/abs/2510.27289)
*Zhengchang Hua,Panagiotis Oikonomou,Karim Djemame,Nikos Tziritas,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出DT-MADDPG算法，将多智能体强化学习与协作数字孪生网络结合，在保护隐私的同时实现去中心化系统的协调控制


<details>
  <summary>Details</summary>
Motivation: 解决大规模去中心化系统（如V2G网络中的电动汽车车队）协调控制问题，在保护个体智能体隐私的同时实现全局最优控制策略

Method: 开发数字孪生辅助多智能体深度确定性策略梯度算法，通过协作构建的预测全局模型增强集中式评论家，无需收集敏感原始数据

Result: 在模拟V2G环境中，DT-MADDPG实现了与标准MADDPG相当的协调性能，同时在数据隐私和架构去中心化方面具有显著优势

Conclusion: 为复杂现实世界信息物理系统部署智能、基于学习的协调提供了一个实用且鲁棒的框架

Abstract: The coordination of large-scale, decentralised systems, such as a fleet of
Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a
significant challenge for modern control systems. While collaborative Digital
Twins have been proposed as a solution to manage such systems without
compromising the privacy of individual agents, deriving globally optimal
control policies from the high-level information they share remains an open
problem. This paper introduces Digital Twin Assisted Multi-Agent Deep
Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid
architecture that integrates a multi-agent reinforcement learning framework
with a collaborative DT network. Our core contribution is a simulation-assisted
learning algorithm where the centralised critic is enhanced by a predictive
global model that is collaboratively built from the privacy-preserving data
shared by individual DTs. This approach removes the need for collecting
sensitive raw data at a centralised entity, a requirement of traditional
multi-agent learning algorithms. Experimental results in a simulated V2G
environment demonstrate that DT-MADDPG can achieve coordination performance
comparable to the standard MADDPG algorithm while offering significant
advantages in terms of data privacy and architectural decentralisation. This
work presents a practical and robust framework for deploying intelligent,
learning-based coordination in complex, real-world cyber-physical systems.

</details>


### [5] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出了一种在线策略，用于完全依赖能量收集的MEC系统，通过动态调度计算任务和控制能耗来平衡间歇性能源供应与动态用户需求。


<details>
  <summary>Details</summary>
Motivation: MEC系统与可再生能源收集技术结合以提升可持续性，但间歇性能源供应与动态用户需求之间的平衡是一个重大资源分配挑战。

Method: 动态调度具有依赖关系的计算任务，通过实时决策服务器频率调节和服务模块迁移来控制能耗。

Result: 使用真实世界数据集的实验表明，该算法在有效利用收集能量的同时保持低服务延迟。

Conclusion: 所提出的在线策略能够有效解决能量收集MEC系统中的资源分配问题，实现能源高效利用和低延迟服务的平衡。

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [6] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的启发式方法，用于寻找并行分区算法CUDA实现中的最优子系统规模，并通过kNN分类方法构建预测模型。


<details>
  <summary>Details</summary>
Motivation: 为了优化并行分区算法在CUDA实现中的性能，需要找到最优的子系统规模，但传统方法效率低下，因此采用机器学习方法来自动确定最优参数。

Method: 使用k-近邻(kNN)分类方法构建预测模型，通过计算实验确定不同线性代数方程组规模下的最优子系统规模，并将方法扩展到递归并行分区算法。

Result: 通过比较预测值与实际数据，算法表现良好，能够准确预测最优子系统规模，并成功构建了预测递归步数的kNN模型。

Conclusion: 基于机器学习的启发式方法能够有效确定并行分区算法的最优参数，为CUDA实现提供了实用的优化工具。

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


### [7] [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)
*Nandor Licker,Kevin Hu,Vladimir Zaytsev,Lequn Chen*

Main category: cs.DC

TL;DR: TransferEngine是一个统一的网络接口抽象层，为LLM系统提供便携的点对点通信，支持多种NIC硬件，峰值吞吐达400Gbps，应用于分布式推理、RL权重更新和MoE路由等场景。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统模式（如分布式推理、MoE路由、异步RL微调）需要灵活的点对点通信，但现有实现被锁定在特定NIC硬件上，缺乏跨硬件供应商的可移植性。

Method: 设计TransferEngine桥接常见NIC功能，暴露统一的WriteImm操作和ImmCounter原语进行完成通知，透明管理每个GPU的多个NIC，不依赖网络传输的排序假设。

Result: 在NVIDIA ConnectX-7和AWS EFA上实现400Gbps峰值吞吐；支持KvCache传输实现动态扩展的分布式推理；RL权重更新在万亿参数模型上达到1.3秒；MoE调度/组合在ConnectX-7上超越DeepEP解码延迟，在EFA上实现首个可行延迟。

Conclusion: TransferEngine提供便携的点对点通信，补充集体通信功能，同时避免硬件锁定问题。

Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning](https://arxiv.org/abs/2510.26829)
*Svetlana Churina,Niranjan Chebrolu,Kokil Jaidka*

Main category: cs.LG

TL;DR: 论文研究了持续预训练中LLMs对错误信息的脆弱性，发现即使少量接触错误信息也会导致模型内部表征的持久漂移，类似于人类的幻觉真相效应。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在持续预训练过程中是否像人类一样容易受到重复错误信息的影响，即是否存在类似幻觉真相效应的脆弱性。

Method: 提出了Layer of Truth框架，通过注入受控的污染数据，在不同检查点、模型规模和问题类型下探测中间表征，量化事实信念的变化。

Result: 研究发现即使最小程度的暴露也能在已确立的事实中诱导持久的表征漂移，且不同层和模型规模的易感性存在差异。

Conclusion: 持续更新的LLMs存在被忽视的脆弱性：它们能够类似人类一样内化错误信息，强调在模型更新过程中需要加强对事实完整性的监控。

Abstract: Large language models (LLMs) continually evolve through pre-training on
ever-expanding web data, but this adaptive process also exposes them to subtle
forms of misinformation. While prior work has explored data poisoning during
static pre-training, the effects of such manipulations under continual
pre-training remain largely unexplored. Drawing inspiration from the illusory
truth effect in human cognition - where repeated exposure to falsehoods
increases belief in their accuracy - we ask whether LLMs exhibit a similar
vulnerability. We investigate whether repeated exposure to false but
confidently stated facts can shift a model's internal representation away from
the truth.
  We introduce Layer of Truth, a framework and dataset for probing belief
dynamics in continually trained LLMs. By injecting controlled amounts of
poisoned data and probing intermediate representations across checkpoints,
model scales, and question types, we quantify when and how factual beliefs
shift. Our findings reveal that even minimal exposure can induce persistent
representational drift in well-established facts, with susceptibility varying
across layers and model sizes. These results highlight an overlooked
vulnerability of continually updated LLMs: their capacity to internalize
misinformation analogously to humans, underscoring the need for robust
monitoring of factual integrity during model updates.

</details>


### [9] [MLPerf Automotive](https://arxiv.org/abs/2510.27065)
*Radoyeh Shojaei,Predrag Djurdjevic,Mostafa El-Khamy,James Goel,Kasper Mecklenburg,John Owens,Pınar Muyan-Özçelik,Tom St. John,Jinho Suh,Arjun Suresh*

Main category: cs.LG

TL;DR: MLPerf Automotive是首个用于评估汽车系统中AI加速器性能的标准化公开基准测试，专注于汽车感知任务，提供延迟和精度指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试套件无法满足汽车机器学习系统的独特需求，包括安全性和实时处理等约束条件，因此需要专门针对汽车领域的标准化性能评估方法。

Method: 通过与MLCommons和自动驾驶车辆计算联盟合作开发，基准测试框架包含2D目标检测、2D语义分割和3D目标检测等汽车感知任务，提供参考模型和提交规则。

Result: 建立了首个汽车机器学习系统标准化基准测试，提供了可重复的性能比较框架，并发布了基准测试代码。

Conclusion: MLPerf Automotive填补了汽车AI加速器性能评估的空白，为行业提供了标准化的测试方法，有助于推动汽车机器学习系统的发展。

Abstract: We present MLPerf Automotive, the first standardized public benchmark for
evaluating Machine Learning systems that are deployed for AI acceleration in
automotive systems. Developed through a collaborative partnership between
MLCommons and the Autonomous Vehicle Computing Consortium, this benchmark
addresses the need for standardized performance evaluation methodologies in
automotive machine learning systems. Existing benchmark suites cannot be
utilized for these systems since automotive workloads have unique constraints
including safety and real-time processing that distinguish them from the
domains that previously introduced benchmarks target. Our benchmarking
framework provides latency and accuracy metrics along with evaluation protocols
that enable consistent and reproducible performance comparisons across
different hardware platforms and software implementations. The first iteration
of the benchmark consists of automotive perception tasks in 2D object
detection, 2D semantic segmentation, and 3D object detection. We describe the
methodology behind the benchmark design including the task selection, reference
models, and submission rules. We also discuss the first round of benchmark
submissions and the challenges involved in acquiring the datasets and the
engineering efforts to develop the reference implementations. Our benchmark
code is available at https://github.com/mlcommons/mlperf_automotive.

</details>


### [10] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: 本文提出了SmoothGuard防御框架，通过随机噪声注入和聚类预测聚合来增强多模态大语言模型的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然性能优异，但对对抗性攻击高度脆弱，存在安全和可靠性问题。

Method: 在HuggingFace生态系统中生成对抗图像，然后使用SmoothGuard框架：对连续模态添加高斯噪声，生成多个候选输出，通过嵌入聚类过滤受对抗性影响的预测，从多数簇中选择最终答案。

Result: 在POPE、LLaVA-Bench和MM-SafetyBench上的实验表明，SmoothGuard显著提高了对抗攻击的鲁棒性，同时保持了竞争力。消融研究发现最佳噪声范围为0.1-0.2。

Conclusion: SmoothGuard是一种轻量级、模型无关的防御框架，能有效增强多模态大语言模型的鲁棒性，在对抗攻击下提供稳定响应。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>


### [11] [Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility](https://arxiv.org/abs/2510.26841)
*Kangkang Sun,Jun Wu,Minyi Guo,Jianhua Li,Jianwei Huang*

Main category: cs.LG

TL;DR: 提出FedPF算法，通过零和博弈框架解决联邦学习中隐私保护与公平性的内在冲突，证明隐私与公平存在不可避免的权衡关系


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中同时确保跨人口群体公平性和保护敏感客户端数据的根本挑战

Method: 将多目标优化转化为零和博弈，引入差分隐私的公平联邦学习算法FedPF

Result: 在三个数据集上实现高达42.9%的歧视减少，同时保持竞争力准确率，但发现隐私-公平紧张关系不可避免

Conclusion: 同时实现隐私和公平目标需要精心平衡的妥协，而非单独优化任一目标

Abstract: Federated Learning (FL) enables collaborative model training without data
sharing, yet participants face a fundamental challenge, e.g., simultaneously
ensuring fairness across demographic groups while protecting sensitive client
data. We introduce a differentially private fair FL algorithm (\textit{FedPF})
that transforms this multi-objective optimization into a zero-sum game where
fairness and privacy constraints compete against model utility. Our theoretical
analysis reveals a surprising inverse relationship, i.e., stricter privacy
protection fundamentally limits the system's ability to detect and correct
demographic biases, creating an inherent tension between privacy and fairness.
Counterintuitively, we prove that moderate fairness constraints initially
improve model generalization before causing performance degradation, where a
non-monotonic relationship that challenges conventional wisdom about
fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %
discrimination reduction across three datasets while maintaining competitive
accuracy, but more importantly, reveals that the privacy-fairness tension is
unavoidable, i.e., achieving both objectives simultaneously requires carefully
balanced compromises rather than optimization of either in isolation. The
source code for our proposed algorithm is publicly accessible at
https://github.com/szpsunkk/FedPF.

</details>


### [12] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出CAS-Spec方法，通过动态可切换推理加速策略构建推测性草稿模型，结合动态树级联算法实现自适应路由和草稿长度分配，在无需专门训练的情况下实现显著推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有的自推测解码方法虽然集成方便但速度提升有限，而依赖专门训练的方法虽然加速效果好但成本高。级联草稿模型方法有潜力但训练多个模型的成本限制了实际应用。

Method: 使用动态可切换推理加速策略（包括层稀疏化和激活量化）构建推测性草稿模型，提出动态树级联算法自适应路由多级草稿模型并分配草稿长度。

Result: 相比现有自推测解码方法达到最先进的加速效果，在各种LLM和数据集上比自回归解码平均加速1.1倍到2.3倍。动态树级联算法相比级联基线和树基线算法分别提升47%和48%的平均加速。

Conclusion: CAS-Spec方法无需专门训练即可实现显著推理加速，易于集成到现有LLM中，随着自推测解码技术的发展具有进一步加速的潜力。

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [13] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: SERFLOW通过动态卸载机器学习模型分区到FaaS和IaaS服务，考虑虚拟机冷启动和长尾服务时间等现实因素，使用阶段特定资源供应和自适应负载均衡，降低云成本超过23%。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略虚拟机冷启动、长尾服务时间分布等现实因素，无法有效处理输入依赖的退出率变化，导致单一资源配置无法适应所有查询分布。

Method: 将ML查询建模为遍历无环阶段序列，每个阶段包含稀疏模型参数块，使用FaaS无服务器函数和阶段特定资源供应，结合基于请求摄入的自适应负载均衡。

Result: SERFLOW能够有效适应动态工作负载，将云成本降低超过23%。

Conclusion: 通过考虑现实因素并采用阶段特定资源供应与自适应负载均衡，SERFLOW在动态工作负载下实现了显著的成本节约和效率提升。

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [14] [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)
*Mahsa Valizadeh,Rui Tuo,James Caverlee*

Main category: cs.LG

TL;DR: BI-DCGAN是一种贝叶斯扩展的DCGAN，通过引入模型不确定性来解决GAN中的模式崩溃问题，提高生成样本的多样性，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: GAN在生成合成数据方面表现出色，但存在模式崩溃问题，生成器只产生有限范围的输出，无法捕捉完整的数据分布。这在需要多样性和不确定性感知的实际应用中尤为关键。

Method: BI-DCGAN通过Bayes by Backprop学习网络权重的分布，并使用平均场变分推断在GAN训练中高效近似后验分布。这是首个基于协方差矩阵分析的理论证明，表明贝叶斯建模能增强GAN的样本多样性。

Result: 在标准生成基准测试上的广泛实验验证了理论结果，BI-DCGAN比传统DCGAN产生更多样化和鲁棒的输出，同时保持训练效率。

Conclusion: BI-DCGAN为需要多样性和不确定性的应用提供了一个可扩展且及时的解决方案，特别是在现代替代方法如扩散模型仍然过于资源密集的情况下。

Abstract: Generative Adversarial Networks (GANs) are proficient at generating synthetic
data but continue to suffer from mode collapse, where the generator produces a
narrow range of outputs that fool the discriminator but fail to capture the
full data distribution. This limitation is particularly problematic, as
generative models are increasingly deployed in real-world applications that
demand both diversity and uncertainty awareness. In response, we introduce
BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty
into the generative process while maintaining computational efficiency.
BI-DCGAN integrates Bayes by Backprop to learn a distribution over network
weights and employs mean-field variational inference to efficiently approximate
the posterior distribution during GAN training. We establishes the first
theoretical proof, based on covariance matrix analysis, that Bayesian modeling
enhances sample diversity in GANs. We validate this theoretical result through
extensive experiments on standard generative benchmarks, demonstrating that
BI-DCGAN produces more diverse and robust outputs than conventional DCGANs,
while maintaining training efficiency. These findings position BI-DCGAN as a
scalable and timely solution for applications where both diversity and
uncertainty are critical, and where modern alternatives like diffusion models
remain too resource-intensive.

</details>


### [15] [Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering](https://arxiv.org/abs/2510.26898)
*Crystal Su,Kuai Yu,Jingrui Zhang,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 提出了一种结合本体论与大型语言模型的化学工程框架，通过结构化领域知识和生成推理的统一，为过程控制和安全分析等关键工程应用提供透明可审计的方法。


<details>
  <summary>Details</summary>
Motivation: 将符号化结构与神经生成相结合，为化学工程中的过程控制、安全分析等关键应用提供透明且可审计的LLM应用方法，解决传统LLM在专业领域缺乏事实基础和可解释性的问题。

Method: 开发了本体集成LLM框架，通过数据获取、语义预处理、信息提取和本体映射步骤，生成模板化问答对指导微调，采用控制聚焦解码和引用门机制约束输出到本体链接术语。

Result: 构建了能够量化语言质量和本体准确性的评估指标，通过语义检索和迭代验证增强了系统的可解释性和可靠性。

Conclusion: 本体集成LLM框架成功地将结构化领域知识与生成推理相结合，为化学工程关键应用提供了透明、可审计的解决方案，展示了符号结构与神经生成融合的潜力。

Abstract: This work presents an ontology-integrated large language model (LLM)
framework for chemical engineering that unites structured domain knowledge with
generative reasoning. The proposed pipeline aligns model training and inference
with the COPE ontology through a sequence of data acquisition, semantic
preprocessing, information extraction, and ontology mapping steps, producing
templated question-answer pairs that guide fine-tuning. A control-focused
decoding stage and citation gate enforce syntactic and factual grounding by
constraining outputs to ontology-linked terms, while evaluation metrics
quantify both linguistic quality and ontological accuracy. Feedback and future
extensions, including semantic retrieval and iterative validation, further
enhance the system's interpretability and reliability. This integration of
symbolic structure and neural generation provides a transparent, auditable
approach for applying LLMs to process control, safety analysis, and other
critical engineering contexts.

</details>


### [16] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: 提出一个结合聚类和少样本预测的框架，利用大规模充电需求数据集发现站点原型，以提升电动汽车充电需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于小规模数据集、简单的时间依赖建模方法，以及对运营历史有限站点的弱泛化能力，需要更准确的方法来支持电动汽车基础设施规划。

Method: 集成聚类与少样本预测的框架，使用大规模充电需求数据集识别站点原型，并针对不同原型训练专家模型。

Result: 原型特定的专家模型在未见过站点的需求预测中优于全局基线模型。

Conclusion: 通过将预测性能作为基础设施分割的基础，为运营商提供了降低成本、优化能源和定价策略、支持电网弹性的可行见解，这对实现气候目标至关重要。

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [17] [MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2510.26937)
*Zimeng Huang,Jinxin Ke,Xiaoxuan Fan,Yufeng Yang,Yang Liu,Liu Zhonghan,Zedi Wang,Junteng Dai,Haoyi Jiang,Yuyu Zhou,Keze Wang,Ziliang Chen*

Main category: cs.LG

TL;DR: 提出了MM-OPERA基准测试，包含11,497个实例，用于评估大视觉语言模型在开放关联推理方面的能力，包括远程项目关联和上下文关联两个任务。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型在关联推理方面存在不足，而现有基准测试主要关注封闭式任务，无法评估开放式的关联推理能力，这是人类认知和创造性思维的关键。

Method: 设计了MM-OPERA基准测试，包含远程项目关联和上下文关联两个开放任务，采用LLM-as-a-Judge策略评估自由形式响应，应用过程奖励知情判断来精确分析推理路径。

Result: 通过对最先进LVLMs的广泛实证研究，包括任务实例敏感性分析、LLM-as-a-Judge策略有效性分析和跨能力、领域、语言、文化等的多样性分析，全面揭示了当前LVLMs在关联推理方面的局限性。

Conclusion: 该工作为开发更类人、更通用的人工智能铺平了道路，提供了对当前LVLMs关联推理能力的全面细致理解。

Abstract: Large Vision-Language Models (LVLMs) have exhibited remarkable progress.
However, deficiencies remain compared to human intelligence, such as
hallucination and shallow pattern matching. In this work, we aim to evaluate a
fundamental yet underexplored intelligence: association, a cornerstone of human
cognition for creative thinking and knowledge integration. Current benchmarks,
often limited to closed-ended tasks, fail to capture the complexity of
open-ended association reasoning vital for real-world applications. To address
this, we present MM-OPERA, a systematic benchmark with 11,497 instances across
two open-ended tasks: Remote-Item Association (RIA) and In-Context Association
(ICA), aligning association intelligence evaluation with human psychometric
principles. It challenges LVLMs to resemble the spirit of divergent thinking
and convergent associative reasoning through free-form responses and explicit
reasoning paths. We deploy tailored LLM-as-a-Judge strategies to evaluate
open-ended outputs, applying process-reward-informed judgment to dissect
reasoning with precision. Extensive empirical studies on state-of-the-art
LVLMs, including sensitivity analysis of task instances, validity analysis of
LLM-as-a-Judge strategies, and diversity analysis across abilities, domains,
languages, cultures, etc., provide a comprehensive and nuanced understanding of
the limitations of current LVLMs in associative reasoning, paving the way for
more human-like and general-purpose AI. The dataset and code are available at
https://github.com/MM-OPERA-Bench/MM-OPERA.

</details>


### [18] [Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction](https://arxiv.org/abs/2510.26940)
*Ashwin Kumar,Hanyu Zhang,David A. Schweidel,William Yeoh*

Main category: cs.LG

TL;DR: 该论文审计了最先进的移动性预测模型，发现基于用户人口统计的隐藏差异，并提出Fairness-Guided Incremental Sampling (FGIS)方法，通过增量数据收集减少人口统计性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索移动性预测模型的社会影响，揭示基于用户人口统计的隐藏差异，并解决预测性能在种族和民族群体之间的系统性差异问题。

Method: 提出FGIS（公平引导增量采样）策略，使用Size-Aware K-Means (SAKM)聚类方法在潜在移动空间中划分用户，并基于预期性能增益和当前群体表示优先选择用户。

Result: 该方法将群体间的总差异减少了高达40%，同时保持整体准确性，在早期采样阶段改进最为显著。

Conclusion: 研究揭示了移动性预测管道中的结构性不平等，并展示了轻量级、以数据为中心的干预措施如何在低数据应用中以最小复杂性改善公平性。

Abstract: Next location prediction underpins a growing number of mobility, retail, and
public-health applications, yet its societal impacts remain largely unexplored.
In this paper, we audit state-of-the-art mobility prediction models trained on
a large-scale dataset, highlighting hidden disparities based on user
demographics. Drawing from aggregate census data, we compute the difference in
predictive performance on racial and ethnic user groups and show a systematic
disparity resulting from the underlying dataset, resulting in large differences
in accuracy based on location and user groups. To address this, we propose
Fairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy
designed for incremental data collection settings. Because individual-level
demographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a
clustering method that partitions users in latent mobility space while
enforcing census-derived group proportions. This yields proxy racial labels for
the four largest groups in the state: Asian, Black, Hispanic, and White. Built
on these labels, our sampling algorithm prioritizes users based on expected
performance gains and current group representation. This method incrementally
constructs training datasets that reduce demographic performance gaps while
preserving overall accuracy. Our method reduces total disparity between groups
by up to 40\% with minimal accuracy trade-offs, as evaluated on a state-of-art
MetaPath2Vec model and a transformer-encoder model. Improvements are most
significant in early sampling stages, highlighting the potential for
fairness-aware strategies to deliver meaningful gains even in low-resource
settings. Our findings expose structural inequities in mobility prediction
pipelines and demonstrate how lightweight, data-centric interventions can
improve fairness with little added complexity, especially for low-data
applications.

</details>


### [19] [Can machines think efficiently?](https://arxiv.org/abs/2510.26954)
*Adam Winchell*

Main category: cs.LG

TL;DR: 提出一个新的图灵测试版本，在原有模仿游戏基础上加入能量消耗约束，从效率角度评估智能，将抽象思维问题与有限资源现实联系起来。


<details>
  <summary>Details</summary>
Motivation: 原有图灵测试已不足以区分人类与机器智能，先进AI系统已能通过原测试，并引发严重的伦理和环境问题，迫切需要更新测试标准。

Method: 在原有模仿游戏中增加能量消耗因素，通过能量约束来评估智能效率，将思维抽象问题与有限资源现实连接。

Result: 新测试为智能评估提供了可测量的实际终点线，这是原测试所缺乏的，迫使社会权衡使用AI的时间节省与其总资源成本。

Conclusion: 通过引入能量约束，新图灵测试能够更全面地评估智能，考虑效率因素，为AI发展提供更可持续的评估框架。

Abstract: The Turing Test is no longer adequate for distinguishing human and machine
intelligence. With advanced artificial intelligence systems already passing the
original Turing Test and contributing to serious ethical and environmental
concerns, we urgently need to update the test. This work expands upon the
original imitation game by accounting for an additional factor: the energy
spent answering the questions. By adding the constraint of energy, the new test
forces us to evaluate intelligence through the lens of efficiency, connecting
the abstract problem of thinking to the concrete reality of finite resources.
Further, this proposed new test ensures the evaluation of intelligence has a
measurable, practical finish line that the original test lacks. This additional
constraint compels society to weigh the time savings of using artificial
intelligence against its total resource cost.

</details>


### [20] [Predicting Household Water Consumption Using Satellite and Street View Images in Two Indian Cities](https://arxiv.org/abs/2510.26957)
*Qiao Wang,Joseph George*

Main category: cs.LG

TL;DR: 利用公开可用的卫星图像、Google街景分割和简单地理空间数据预测印度城市家庭用水量，接近传统调查方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统家庭用水监测方法成本高、耗时，需要寻找更高效的替代方案来支持快速城市化地区的用水管理。

Method: 比较四种方法：调查特征（基准）、CNN嵌入（卫星、街景、组合）、街景语义地图加辅助数据，采用序数分类框架。

Result: 街景分割加遥感协变量达到0.55准确率，接近调查模型的0.59准确率，在用水量分布极端值处精度高，但中间类别存在混淆。

Conclusion: 开放获取图像结合最小化地理空间数据，为城市分析中获取可靠家庭用水估计提供了有前景的替代方案。

Abstract: Monitoring household water use in rapidly urbanizing regions is hampered by
costly, time-intensive enumeration methods and surveys. We investigate whether
publicly available imagery-satellite tiles, Google Street View (GSV)
segmentation-and simple geospatial covariates (nightlight intensity, population
density) can be utilized to predict household water consumption in
Hubballi-Dharwad, India. We compare four approaches: survey features
(benchmark), CNN embeddings (satellite, GSV, combined), and GSV semantic maps
with auxiliary data. Under an ordinal classification framework, GSV
segmentation plus remote-sensing covariates achieves 0.55 accuracy for water
use, approaching survey-based models (0.59 accuracy). Error analysis shows high
precision at extremes of the household water consumption distribution, but
confusion among middle classes is due to overlapping visual proxies. We also
compare and contrast our estimates for household water consumption to that of
household subjective income. Our findings demonstrate that open-access imagery,
coupled with minimal geospatial data, offers a promising alternative to
obtaining reliable household water consumption estimates using surveys in urban
analytics.

</details>


### [21] [Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget](https://arxiv.org/abs/2510.26981)
*Zhichao Hou,Weizhi Gao,Xiaorui Liu*

Main category: cs.LG

TL;DR: 提出了一种在有限计算预算下最大化对抗攻击效果的方法，通过精细控制层激活重计算来优化攻击效能。


<details>
  <summary>Details</summary>
Motivation: 解决AI安全研究中计算资源有限时的关键挑战：如何在固定计算预算下最大化迭代对抗攻击的效果。简单地减少攻击迭代次数会降低成本但显著削弱攻击效果。

Method: 提出细粒度控制机制，在迭代和层级别上选择性重计算层激活，以在约束预算内实现可达到的攻击效能。

Result: 大量实验表明，该方法在同等成本下始终优于现有基线方法。当集成到对抗训练中时，仅使用原始预算的30%就能达到相当的性能。

Conclusion: 该方法有效解决了有限计算预算下的对抗攻击优化问题，显著提升了攻击效率，为AI安全研究提供了实用的解决方案。

Abstract: This work tackles a critical challenge in AI safety research under limited
compute: given a fixed computation budget, how can one maximize the strength of
iterative adversarial attacks? Coarsely reducing the number of attack
iterations lowers cost but substantially weakens effectiveness. To fulfill the
attainable attack efficacy within a constrained budget, we propose a
fine-grained control mechanism that selectively recomputes layer activations
across both iteration-wise and layer-wise levels. Extensive experiments show
that our method consistently outperforms existing baselines at equal cost.
Moreover, when integrated into adversarial training, it attains comparable
performance with only 30% of the original budget.

</details>


### [22] [HADSF: Aspect Aware Semantic Control for Explainable Recommendation](https://arxiv.org/abs/2510.26994)
*Zheng Nie,Peijie Sun*

Main category: cs.LG

TL;DR: 提出了HADSF框架，通过双阶段语义处理解决LLM在推荐系统中信息提取的冗余、幻觉和成本效益问题，引入新指标评估提取质量，实验证明能有效降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的推荐系统存在三个问题：(i)无范围控制的自由形式评论挖掘产生冗余和噪声表示，(ii)缺乏将LLM幻觉与下游效果关联的原则性指标，(iii)未探索不同模型规模下的成本质量权衡。

Method: HADSF双阶段方法：第一阶段通过自适应选择生成紧凑的语料级方面词汇表，第二阶段在词汇表指导下进行显式约束的结构化方面-观点三元组提取。引入ADR和OFR指标评估表示保真度。

Result: 在约300万条评论上测试1.5B-70B参数的LLM，集成到标准评分预测器后，HADSF能持续降低预测误差，并使小模型在代表性部署场景中达到竞争性性能。

Conclusion: HADSF框架有效解决了LLM增强型可解释推荐中的关键挑战，揭示了幻觉严重程度与评分预测误差之间的非单调关系，为可复现研究提供了完整工具链。

Abstract: Recent advances in large language models (LLMs) promise more effective
information extraction for review-based recommender systems, yet current
methods still (i) mine free-form reviews without scope control, producing
redundant and noisy representations, (ii) lack principled metrics that link LLM
hallucination to downstream effectiveness, and (iii) leave the cost-quality
trade-off across model scales largely unexplored. We address these gaps with
the Hyper-Adaptive Dual-Stage Semantic Framework (HADSF), a two-stage approach
that first induces a compact, corpus-level aspect vocabulary via adaptive
selection and then performs vocabulary-guided, explicitly constrained
extraction of structured aspect-opinion triples. To assess the fidelity of the
resulting representations, we introduce Aspect Drift Rate (ADR) and Opinion
Fidelity Rate (OFR) and empirically uncover a nonmonotonic relationship between
hallucination severity and rating prediction error. Experiments on
approximately 3 million reviews across LLMs spanning 1.5B-70B parameters show
that, when integrated into standard rating predictors, HADSF yields consistent
reductions in prediction error and enables smaller models to achieve
competitive performance in representative deployment scenarios. We release
code, data pipelines, and metric implementations to support reproducible
research on hallucination-aware, LLM-enhanced explainable recommendation. Code
is available at https://github.com/niez233/HADSF

</details>


### [23] [Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules](https://arxiv.org/abs/2510.26997)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，将学习规则视为在（部分可观测）损失景观中导航的策略，并将最优规则识别为相关最优控制问题的解。


<details>
  <summary>Details</summary>
Motivation: 传统上学习规则通常是假设而非推导的，需要理解为什么某些学习规则比其他规则更好，以及在什么假设下可以认为给定规则是最优的。

Method: 将学习规则建模为在部分可观测损失景观中导航的策略，将其形式化为最优控制问题，在不同假设下推导各种学习规则。

Result: 在该框架下自然涌现出多种已知规则：梯度下降（短视优化）、动量（长视规划）、自然梯度（参数空间几何）、非梯度规则（部分可控性）、Adam等自适应优化器（在线贝叶斯推断）。

Conclusion: 该框架通过统一目标澄清了学习的计算结构，为设计自适应算法提供了原则性基础，并将持续学习策略理解为对任务不确定性的最优响应。

Abstract: Learning rules -- prescriptions for updating model parameters to improve
performance -- are typically assumed rather than derived. Why do some learning
rules work better than others, and under what assumptions can a given rule be
considered optimal? We propose a theoretical framework that casts learning
rules as policies for navigating (partially observable) loss landscapes, and
identifies optimal rules as solutions to an associated optimal control problem.
A range of well-known rules emerge naturally within this framework under
different assumptions: gradient descent from short-horizon optimization,
momentum from longer-horizon planning, natural gradients from accounting for
parameter space geometry, non-gradient rules from partial controllability, and
adaptive optimizers like Adam from online Bayesian inference of loss landscape
shape. We further show that continual learning strategies like weight resetting
can be understood as optimal responses to task uncertainty. By unifying these
phenomena under a single objective, our framework clarifies the computational
structure of learning and offers a principled foundation for designing adaptive
algorithms.

</details>


### [24] [A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms](https://arxiv.org/abs/2510.27001)
*Elise Wolf*

Main category: cs.LG

TL;DR: 提出了一个可复现的多臂老虎机算法评估框架，系统比较了8种经典和方差感知算法，发现在高不确定性环境中方差感知算法表现更优


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法评估缺乏标准化条件和可复现性，特别是方差感知扩展算法的性能高度依赖环境条件，需要可靠的比较方法

Method: 开发了Bandit Playground代码库，包含明确定义的实验设置、多种性能指标（奖励、遗憾、奖励分布、风险价值、动作最优性）和交互式评估界面

Result: 方差感知算法在高不确定性且臂间奖励差异微妙的环境中表现更好，而经典算法在更可分离的场景或经过精细调优时表现相当或更优

Conclusion: 贡献包括：(1) 系统评估MAB算法的框架，(2) 明确了方差感知方法优于经典方法的具体条件

Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for
more complex reinforcement learning algorithms. However, evaluating and
comparing MAB algorithms remains challenging due to the lack of standardized
conditions and replicability. This is particularly problematic for
variance-aware extensions of classical methods like UCB, whose performance can
heavily depend on the underlying environment. In this study, we address how
performance differences between bandit algorithms can be reliably observed, and
under what conditions variance-aware algorithms outperform classical ones. We
present a reproducible evaluation designed to systematically compare eight
classical and variance-aware MAB algorithms. The evaluation framework,
implemented in our Bandit Playground codebase, features clearly defined
experimental setups, multiple performance metrics (reward, regret, reward
distribution, value-at-risk, and action optimality), and an interactive
evaluation interface that supports consistent and transparent analysis. We show
that variance-aware algorithms can offer advantages in settings with high
uncertainty where the difficulty arises from subtle differences between arm
rewards. In contrast, classical algorithms often perform equally well or better
in more separable scenarios or if fine-tuned extensively. Our contributions are
twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2)
insights into the conditions under which variance-aware approaches outperform
their classical counterparts.

</details>


### [25] [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)
*Mihir Mahajan,Alfred Nguyen,Franz Srambical,Stefan Bauer*

Main category: cs.LG

TL;DR: Jasmine是一个基于JAX的高性能世界建模代码库，支持从单主机到数百个加速器的扩展，相比现有开源实现能实现数量级更快的CoinRun案例研究复现。


<details>
  <summary>Details</summary>
Motivation: 世界建模作为解决机器人等领域数据稀缺问题的途径，但开放训练基础设施仍不成熟，需要高性能、可扩展的代码库。

Method: 开发Jasmine代码库，通过数据加载、训练和检查点的性能优化，保证完全可复现训练，支持多样化分片配置。

Result: Jasmine实现了比现有开源实现快一个数量级的CoinRun案例研究复现速度，并与大规模数据集配对建立了严格的基准测试管道。

Conclusion: Jasmine为世界建模提供了高性能、可扩展的基础设施，支持跨模型家族和架构消融的严格基准测试。

Abstract: While world models are increasingly positioned as a pathway to overcoming
data scarcity in domains such as robotics, open training infrastructure for
world modeling remains nascent. We introduce Jasmine, a performant JAX-based
world modeling codebase that scales from single hosts to hundreds of
accelerators with minimal code changes. Jasmine achieves an order-of-magnitude
faster reproduction of the CoinRun case study compared to prior open
implementations, enabled by performance optimizations across data loading,
training and checkpointing. The codebase guarantees fully reproducible training
and supports diverse sharding configurations. By pairing Jasmine with curated
large-scale datasets, we establish infrastructure for rigorous benchmarking
pipelines across model families and architectural ablations.

</details>


### [26] [Mixture-of-Transformers Learn Faster: A Theoretical Study on Classification Problems](https://arxiv.org/abs/2510.27004)
*Hongbo Li,Qinhang Wu,Sen Lin,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: 本文提出了Mixture-of-Transformers (MoT)理论框架，通过连续训练门控网络实现transformer块的专业化，证明了专家专业化能减少梯度冲突、使子任务强凸，并在O(log(ε⁻¹))迭代步数内将预测损失降至接近零，显著优于单transformer的O(ε⁻¹)收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Experts (MoE)模型虽然提高了transformer效率，但缺乏统一的理论解释，特别是在允许前馈层和注意力层都专业化的情况下。

Method: 提出MoT理论框架，每个transformer块作为由连续训练门控网络控制的专家，开发三阶段训练算法，研究专家专业化和注意力对齐的核心学习动态。

Result: transformer专家专业化于不同任务类别，门控网络准确路由数据样本，专家专业化减少梯度冲突并使子任务强凸，预测损失在O(log(ε⁻¹))步数内降至接近零。

Conclusion: 为transformer级专业化和学习动态提供了首个统一理论解释，为设计高效大规模模型提供实践指导。

Abstract: Mixture-of-Experts (MoE) models improve transformer efficiency but lack a
unified theoretical explanation, especially when both feed-forward and
attention layers are allowed to specialize. To this end, we study the
Mixture-of-Transformers (MoT), a tractable theoretical framework in which each
transformer block acts as an expert governed by a continuously trained gating
network. This design allows us to isolate and study the core learning dynamics
of expert specialization and attention alignment. In particular, we develop a
three-stage training algorithm with continuous training of the gating network,
and show that each transformer expert specializes in a distinct class of tasks
and that the gating network accurately routes data samples to the correct
expert. Our analysis shows how expert specialization reduces gradient conflicts
and makes each subtask strongly convex. We prove that the training drives the
expected prediction loss to near zero in $O(\log(\epsilon^{-1}))$ iteration
steps, significantly improving over the $O(\epsilon^{-1})$ rate for a single
transformer. We further validate our theoretical findings through extensive
real-data experiments, demonstrating the practical effectiveness of MoT.
Together, these results offer the first unified theoretical account of
transformer-level specialization and learning dynamics, providing practical
guidance for designing efficient large-scale models.

</details>


### [27] [Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion](https://arxiv.org/abs/2510.27014)
*Sean Patten,Pin-Yu Chen,Christina Schweikert,D. Frank Hsu*

Main category: cs.LG

TL;DR: 提出了一种使用组合融合分析(CFA)集成多种机器学习模型的新方法，在IMDB情感分析数据集上达到了97.072%的最新准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常通过扩大单个模型规模来提升性能，但计算资源消耗大。CFA利用认知多样性概念，通过量化模型间的差异性来战略性地组合预测结果，计算效率更高。

Method: 应用组合融合分析(CFA)，使用秩-分数特征函数量化模型间的相异性，将基于RoBERTa架构的transformer模型与传统机器学习模型（随机森林、SVM、XGBoost）进行集成。

Result: 在IMDB情感分析数据集上达到97.072%的准确率，超越了传统集成方法，实验结果表明CFA能有效计算和利用模型多样性。

Conclusion: 组合融合分析通过利用模型间的认知多样性，提供了一种计算效率高的替代方案，相比单纯扩大模型规模的方法，在保持高性能的同时显著降低了计算资源需求。

Abstract: This paper presents a novel approach to sentiment classification using the
application of Combinatorial Fusion Analysis (CFA) to integrate an ensemble of
diverse machine learning models, achieving state-of-the-art accuracy on the
IMDB sentiment analysis dataset of 97.072\%. CFA leverages the concept of
cognitive diversity, which utilizes rank-score characteristic functions to
quantify the dissimilarity between models and strategically combine their
predictions. This is in contrast to the common process of scaling the size of
individual models, and thus is comparatively efficient in computing resource
use. Experimental results also indicate that CFA outperforms traditional
ensemble methods by effectively computing and employing model diversity. The
approach in this paper implements the combination of a transformer-based model
of the RoBERTa architecture with traditional machine learning models, including
Random Forest, SVM, and XGBoost.

</details>


### [28] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [29] [Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning](https://arxiv.org/abs/2510.27044)
*Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: RLVR方法在数学推理任务中虽然能提升评估指标，但往往是通过强化表面启发式而非获得真正的推理策略，揭示了RLVR泛化能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在数学推理任务中的真实能力，验证其是否能促进真正的推理过程而非仅仅利用捷径。

Method: 在Activity Scheduling和Longest Increasing Subsequence两个组合问题上使用RLVR，采用精心设计的数据集和多种奖励设计。

Result: RLVR改善了评估指标，但通常是通过强化表面启发式而非获得新的推理策略，显示出有限的泛化能力。

Conclusion: 需要能够区分真正数学推理和捷径利用的基准测试，以提供对进展的忠实衡量。

Abstract: Mathematical reasoning is a central challenge for large language models
(LLMs), requiring not only correct answers but also faithful reasoning
processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as
a promising approach for enhancing such capabilities; however, its ability to
foster genuine reasoning remains unclear. We investigate RLVR on two
combinatorial problems with fully verifiable solutions: \emph{Activity
Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully
curated datasets with unique optima. Across multiple reward designs, we find
that RLVR improves evaluation metrics but often by reinforcing superficial
heuristics rather than acquiring new reasoning strategies. These findings
highlight the limits of RLVR generalization, emphasizing the importance of
benchmarks that disentangle genuine mathematical reasoning from shortcut
exploitation and provide faithful measures of progress. Code available at
https://github.com/xashru/rlvr-seq-generalization.

</details>


### [30] [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)
*Alex Irpan,Alexander Matt Turner,Mark Kurzeja,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 本文提出一致性训练方法，通过使模型对提示中的无关线索保持响应不变性，来减少LLM的附和性和越狱攻击。


<details>
  <summary>Details</summary>
Motivation: LLM的事实性和拒绝能力容易被提示中的简单变化所破坏，模型经常表现出附和用户信念或满足不当请求的问题。

Method: 采用两种一致性训练方法：基于外部输出的BCT和基于内部激活的ACT，通过数据增强使模型在不同提示变体下保持相同行为。

Result: 两种方法都有效减少了Gemini 2.5 Flash对无关线索的敏感性，BCT在减少越狱方面表现更好。

Conclusion: 一致性训练可以避免过时训练数据的问题，简化训练流程，某些对齐问题更适合被视为一致性问题而非最优响应问题。

Abstract: An LLM's factuality and refusal training can be compromised by simple changes
to a prompt. Models often adopt user beliefs (sycophancy) or satisfy
inappropriate requests which are wrapped within special text (jailbreaking). We
explore \emph{consistency training}, a self-supervised paradigm that teaches a
model to be invariant to certain irrelevant cues in the prompt. Instead of
teaching the model what exact response to give on a particular prompt, we aim
to teach the model to behave identically across prompt data augmentations (like
adding leading questions or jailbreak text). We try enforcing this invariance
in two ways: over the model's external outputs (\emph{Bias-augmented
Consistency Training} (BCT) from Chua et al. [2025]) and over its internal
activations (\emph{Activation Consistency Training} (ACT), a method we
introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant
cues. Because consistency training uses responses from the model itself as
training data, it avoids issues that arise from stale training data, such as
degrading model capabilities or enforcing outdated response guidelines. While
BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak
reduction. We think that BCT can simplify training pipelines by removing
reliance on static datasets. We argue that some alignment problems are better
viewed not in terms of optimal responses, but rather as consistency issues.

</details>


### [31] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 提出了EMOC框架来评估算法相似性，通过嵌入算法实现到特征空间来支持聚类、分类和多样性量化等任务。


<details>
  <summary>Details</summary>
Motivation: 在克隆检测和程序合成等应用中，需要实用且一致的算法相似性度量方法，但现有方法存在概念混淆和不可计算性问题。

Method: 引入EMOC（评估-内存-操作-复杂度）框架，将算法实现嵌入到特征空间，并编译了PACD数据集进行验证。

Result: EMOC特征支持算法类型聚类分类、近重复检测和LLM生成程序多样性量化，代码和数据已开源。

Conclusion: EMOC框架为算法相似性评估提供了实用的解决方案，有助于促进该领域的可复现性和未来研究。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [32] [Towards Understanding Self-play for LLM Reasoning](https://arxiv.org/abs/2510.27072)
*Justin Yang Chae,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 本文分析了自博弈训练在大型语言模型数学推理中的训练动态，通过与RLVR和SFT的比较，研究了参数更新稀疏性、熵动态和奖励函数，揭示了自博弈的机制和局限性。


<details>
  <summary>Details</summary>
Motivation: 自博弈训练在LLM推理中显示出强大的领域内外性能提升，但其改进机制仍然缺乏深入理解，需要系统分析其训练动态。

Method: 通过分析绝对零推理器的训练动态，比较自博弈与RLVR和SFT方法，研究参数更新稀疏性、标记分布的熵动态以及替代提议者奖励函数。

Result: 研究阐明了自博弈与其他后训练策略的差异，揭示了其内在局限性，并通过pass@k评估将这些动态与推理性能联系起来。

Conclusion: 研究结果澄清了自博弈的训练机制，指出了改进LLM数学推理的未来方向，为优化自博弈训练提供了理论基础。

Abstract: Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.

</details>


### [33] [Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions](https://arxiv.org/abs/2510.27090)
*Sina Javadzadeh,Rahil Soroushmojdehi,S. Alireza Seyyed Mousavi,Mehrnaz Asadi,Sumiko Abe,Terence D. Sanger*

Main category: cs.LG

TL;DR: 提出了一种用于颅内记录数据的可扩展表示学习框架，通过功能嵌入和Transformer建模实现跨受试者的神经数据聚合。


<details>
  <summary>Details</summary>
Motivation: 解决跨受试者颅内记录数据聚合的挑战，因为电极数量、位置和覆盖区域差异很大，传统的空间归一化方法无法准确捕捉功能相似性。

Method: 使用孪生编码器和对比目标学习电极的功能身份嵌入，然后通过Transformer对功能标记建模跨区域关系，支持可变通道数。

Result: 学习到的功能空间支持准确的受试者内区分，形成清晰的区域一致性聚类，能够零样本迁移到未见通道，Transformer能够重建掩码通道。

Conclusion: 该框架为在缺乏严格任务结构和统一传感器布局的情况下，实现大规模跨受试者颅内神经数据聚合和预训练提供了可行路径。

Abstract: Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.

</details>


### [34] [QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.27091)
*Yiwei Chen,Kehuan Yan,Yu Pan,Daoyi Dong*

Main category: cs.LG

TL;DR: 提出了一种量子启发的神经网络（QiNN-QJ），通过量子跳跃算子实现多模态纠缠建模，解决了传统量子启发融合模型训练不稳定和泛化能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有量子启发融合模型主要依赖幺正变换生成量子纠缠，但存在训练不稳定和泛化能力有限的问题。需要一种更稳定可控的纠缠建模方法。

Method: 将每个模态编码为量子纯态，通过可微分的量子跳跃算子将可分积态转换为纠缠表示。联合学习哈密顿量和Lindblad算子，通过耗散动力学生成可控的跨模态纠缠。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS等基准数据集上优于最先进模型，并通过冯·诺依曼纠缠熵增强了后验可解释性。

Conclusion: 为纠缠多模态融合建立了原则性框架，为量子启发方法在复杂跨模态相关性建模中开辟了新途径。

Abstract: Quantum theory provides non-classical principles, such as superposition and
entanglement, that inspires promising paradigms in machine learning. However,
most existing quantum-inspired fusion models rely solely on unitary or
unitary-like transformations to generate quantum entanglement. While
theoretically expressive, such approaches often suffer from training
instability and limited generalizability. In this work, we propose a
Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal
entanglement modelling. Each modality is firstly encoded as a quantum pure
state, after which a differentiable module simulating the QJ operator
transforms the separable product state into the entangled representation. By
jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates
controllable cross-modal entanglement among modalities with dissipative
dynamics, where structured stochasticity and steady-state attractor properties
serve to stabilize training and constrain entanglement shaping. The resulting
entangled states are projected onto trainable measurement vectors to produce
predictions. In addition to achieving superior performance over the
state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI,
and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through
von-Neumann entanglement entropy. This work establishes a principled framework
for entangled multimodal fusion and paves the way for quantum-inspired
approaches in modelling complex cross-modal correlations.

</details>


### [35] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 提出了一种概率层次贝叶斯模型，用于从异质样本的bulk RNA-seq数据中解卷积细胞类型表达谱和比例，利用高分辨率单细胞参考数据，应用于人类子宫内膜组织月经周期研究。


<details>
  <summary>Details</summary>
Motivation: bulk组织RNA测序会掩盖细胞类型特异性动态，需要开发能够解析异质样本中细胞组成和表达变化的方法。

Method: 概率层次贝叶斯模型，利用高分辨率单细胞参考数据，通过详细描述模型结构、先验分布和推断策略来解卷积bulk RNA-seq数据。

Result: 揭示了月经周期不同阶段上皮细胞、基质细胞和免疫细胞比例的动态变化，识别了与子宫内膜功能相关的细胞类型特异性差异基因表达（如分泌期基质细胞中的蜕膜化标记物）。

Conclusion: 该贝叶斯方法对参考数据不匹配和噪声具有鲁棒性，在生育和子宫内膜疾病方面具有潜在临床意义，未来可整合空间转录组学。

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [36] [Group-Sensitive Offline Contextual Bandits](https://arxiv.org/abs/2510.27123)
*Yihong Guo,Junjie Luo,Guodong Gao,Ritu Agarwal,Anqi Liu*

Main category: cs.LG

TL;DR: 提出了一种离线上下文赌博机中的群体敏感公平约束方法，通过引入群体间奖励差异约束来减少策略学习过程中可能出现的奖励差异，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 离线策略优化在最大化整体期望奖励时可能无意中放大不同群体间的奖励差异，导致某些群体比其他群体受益更多，这在资源有限的情况下引发公平性担忧。

Method: 提出了一个约束离线策略优化框架，将群体间奖励差异约束引入基于离策略梯度的优化过程，使用双重稳健估计器来改进训练期间群体奖励差异的估计。

Result: 在合成和真实世界数据集上的实证结果表明，该方法在保持竞争力的整体性能的同时，有效减少了奖励差异。

Conclusion: 该方法成功解决了离线上下文赌博机中的公平性问题，通过约束优化框架平衡了整体性能和群体间公平性。

Abstract: Offline contextual bandits allow one to learn policies from
historical/offline data without requiring online interaction. However, offline
policy optimization that maximizes overall expected rewards can unintentionally
amplify the reward disparities across groups. As a result, some groups might
benefit more than others from the learned policy, raising concerns about
fairness, especially when the resources are limited. In this paper, we study a
group-sensitive fairness constraint in offline contextual bandits, reducing
group-wise reward disparities that may arise during policy learning. We tackle
the following common-parity requirements: the reward disparity is constrained
within some user-defined threshold or the reward disparity should be minimized
during policy optimization. We propose a constrained offline policy
optimization framework by introducing group-wise reward disparity constraints
into an off-policy gradient-based optimization procedure. To improve the
estimation of the group-wise reward disparity during training, we employ a
doubly robust estimator and further provide a convergence guarantee for policy
optimization. Empirical results in synthetic and real-world datasets
demonstrate that our method effectively reduces reward disparities while
maintaining competitive overall performance.

</details>


### [37] [AI Agents in Drug Discovery](https://arxiv.org/abs/2510.27130)
*Srijit Seal,Dinh Long Huynh,Moudather Chelbi,Sara Khosravi,Ankur Kumar,Mattson Thieme,Isaac Wilks,Mark Davies,Jessica Mustali,Yannick Sun,Nick Edwards,Daniil Boiko,Andrei Tyrin,Douglas W. Selinger,Ayaan Parikh,Rahul Vijayan,Shoman Kasbekar,Dylan Reid,Andreas Bender,Ola Spjuth*

Main category: cs.LG

TL;DR: 本文综述了基于大语言模型的AI代理在药物发现中的应用，展示了这些系统如何通过自主推理、执行实验和迭代优化来加速药物研发流程。


<details>
  <summary>Details</summary>
Motivation: AI代理作为变革性工具，能够整合多样化的生物医学数据，通过机器人平台执行实验，并在闭环中迭代优化假设，从而解决药物发现中的复杂研究流程问题。

Method: 构建基于大语言模型的AI代理架构，包括ReAct、Reflection、Supervisor和Swarm系统，结合感知、计算、行动和记忆工具，实现端到端的决策制定。

Result: 早期实施显示在速度、可重复性和可扩展性方面取得显著进展，将原本需要数月的工作流程压缩到数小时，同时保持科学可追溯性。

Conclusion: 尽管面临数据异质性、系统可靠性、隐私和基准测试等挑战，但AI代理系统在药物发现领域展现出巨大潜力，为科学研究和转化应用提供了技术支持方向。

Abstract: Artificial intelligence (AI) agents are emerging as transformative tools in
drug discovery, with the ability to autonomously reason, act, and learn through
complicated research workflows. Building on large language models (LLMs)
coupled with perception, computation, action, and memory tools, these agentic
AI systems could integrate diverse biomedical data, execute tasks, carry out
experiments via robotic platforms, and iteratively refine hypotheses in closed
loops. We provide a conceptual and technical overview of agentic AI
architectures, ranging from ReAct and Reflection to Supervisor and Swarm
systems, and illustrate their applications across key stages of drug discovery,
including literature synthesis, toxicity prediction, automated protocol
generation, small-molecule synthesis, drug repurposing, and end-to-end
decision-making. To our knowledge, this represents the first comprehensive work
to present real-world implementations and quantifiable impacts of agentic AI
systems deployed in operational drug discovery settings. Early implementations
demonstrate substantial gains in speed, reproducibility, and scalability,
compressing workflows that once took months into hours while maintaining
scientific traceability. We discuss the current challenges related to data
heterogeneity, system reliability, privacy, and benchmarking, and outline
future directions towards technology in support of science and translation.

</details>


### [38] [Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring](https://arxiv.org/abs/2510.27131)
*Hong Jiao,Hanna Choi,Haowei Hua*

Main category: cs.LG

TL;DR: 该研究比较了GPT-4.1和GPT-5生成的rationale在自动评分中的效用，发现基于文章的评分通常优于基于rationale的评分，但后者在分数0的类别上表现更好。集成建模进一步提升了评分准确性。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-4.1和GPT-5生成的rationale在自动评分系统中的实际效用，比较基于文章和基于rationale的评分方法的性能差异。

Method: 使用2012年Kaggle ASAP数据中的Prompt 6文章，比较基于文章的评分和基于rationale的评分方法，并进行多种集成建模实验。

Result: 基于文章的评分QWK更高，但基于rationale的评分在分数0类别上F1分数更高。集成建模显著提升了评分准确性，最佳集成模型QWK达到0.870，优于文献报道的0.848。

Conclusion: 基于文章的评分总体上更优，但基于rationale的评分在特定类别上有优势，两者集成可获得最佳评分性能。

Abstract: This study explored the utilities of rationales generated by GPT-4.1 and
GPT-5 in automated scoring using Prompt 6 essays from the 2012 Kaggle ASAP
data. Essay-based scoring was compared with rationale-based scoring. The study
found in general essay-based scoring performed better than rationale-based
scoring with higher Quadratic Weighted Kappa (QWK). However, rationale-based
scoring led to higher scoring accuracy in terms of F1 scores for score 0 which
had less representation due to class imbalance issues. The ensemble modeling of
essay-based scoring models increased the scoring accuracy at both specific
score levels and across all score levels. The ensemble modeling of essay-based
scoring and each of the rationale-based scoring performed about the same.
Further ensemble of essay-based scoring and both rationale-based scoring
yielded the best scoring accuracy with QWK of 0.870 compared with 0.848
reported in literature.

</details>


### [39] [FairAD: Computationally Efficient Fair Graph Clustering via Algebraic Distance](https://arxiv.org/abs/2510.27136)
*Minh Phu Vuong,Young-Ju Lee,Iván Ojeda-Ruiz,Chul-Ho Lee*

Main category: cs.LG

TL;DR: FairAD是一种高效的公平图聚类方法，通过代数距离构建亲和矩阵并施加公平约束，然后进行图粗化找到代表节点，最后求解约束最小化问题实现公平聚类。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型对某些人口统计群体存在不当行为的担忧日益增长，公平性概念受到关注，促使研究图聚类中的公平性。公平图聚类旨在将图中的节点划分为k个不相交的簇，使得每个簇中受保护群体的比例与整个数据集中该群体的比例一致。

Method: 首先基于代数距离概念构建新的亲和矩阵以施加公平约束，然后对该亲和矩阵进行图粗化过程找到对应k个簇的代表节点，最后求解约束最小化问题获得公平聚类解。

Result: 在修改的随机块模型和六个公共数据集上的实验结果表明，FairAD能够实现公平聚类，同时比最先进的公平图聚类算法快达40倍。

Conclusion: FairAD是一种计算高效的公平图聚类方法，能够有效解决在大型图中融入公平约束的计算挑战。

Abstract: Due to the growing concern about unsavory behaviors of machine learning
models toward certain demographic groups, the notion of 'fairness' has recently
drawn much attention from the community, thereby motivating the study of
fairness in graph clustering. Fair graph clustering aims to partition the set
of nodes in a graph into $k$ disjoint clusters such that the proportion of each
protected group within each cluster is consistent with the proportion of that
group in the entire dataset. It is, however, computationally challenging to
incorporate fairness constraints into existing graph clustering algorithms,
particularly for large graphs. To address this problem, we propose FairAD, a
computationally efficient fair graph clustering method. It first constructs a
new affinity matrix based on the notion of algebraic distance such that
fairness constraints are imposed. A graph coarsening process is then performed
on this affinity matrix to find representative nodes that correspond to $k$
clusters. Finally, a constrained minimization problem is solved to obtain the
solution of fair clustering. Experiment results on the modified stochastic
block model and six public datasets show that FairAD can achieve fair
clustering while being up to 40 times faster compared to state-of-the-art fair
graph clustering algorithms.

</details>


### [40] [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)
*Sein Kwon,Seulgi Baek,Hyunseo Yang,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: RelTune是一个新颖的数据库参数调优框架，通过构建参数依赖关系图和使用GNN学习性能相关语义嵌入，结合混合评分引导的贝叶斯优化，显著提升了调优效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据库参数自动调优方法存在三个主要局限：忽略参数间的依赖关系、仅优化少数参数而忽略其他重要参数、贝叶斯优化依赖代理模型导致预测不稳定和探索效率低。

Method: 提出RelTune框架：1）构建参数依赖关系图；2）使用GNN学习性能相关语义的潜在嵌入；3）引入混合评分引导的贝叶斯优化（HBO），结合代理模型预测和亲和度评分。

Result: 在多个DBMS和工作负载上的实验表明，RelTune相比传统基于贝叶斯优化的方法收敛更快、优化效率更高，在所有评估场景中都达到了最先进的性能。

Conclusion: RelTune通过建模参数依赖关系和引入混合评分机制，有效解决了现有自动调优方法的局限性，为数据库性能优化提供了更高效的解决方案。

Abstract: Database Management Systems (DBMSs) are fundamental for managing large-scale
and heterogeneous data, and their performance is critically influenced by
configuration parameters. Effective tuning of these parameters is essential for
adapting to diverse workloads and maximizing throughput while minimizing
latency. Recent research has focused on automated configuration optimization
using machine learning; however, existing approaches still exhibit several key
limitations. Most tuning frameworks disregard the dependencies among
parameters, assuming that each operates independently. This simplification
prevents optimizers from leveraging relational effects across parameters,
limiting their capacity to capture performancesensitive interactions. Moreover,
to reduce the complexity of the high-dimensional search space, prior work often
selects only the top few parameters for optimization, overlooking others that
contribute meaningfully to performance. Bayesian Optimization (BO), the most
common method for automatic tuning, is also constrained by its reliance on
surrogate models, which can lead to unstable predictions and inefficient
exploration. To overcome these limitations, we propose RelTune, a novel
framework that represents parameter dependencies as a Relational Graph and
learns GNN-based latent embeddings that encode performancerelevant semantics.
RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),
which combines surrogate predictions with an Affinity Score measuring proximity
to previously high-performing configurations. Experimental results on multiple
DBMSs and workloads demonstrate that RelTune achieves faster convergence and
higher optimization efficiency than conventional BO-based methods, achieving
state-of-the-art performance across all evaluated scenarios.

</details>


### [41] [Exploring Landscapes for Better Minima along Valleys](https://arxiv.org/abs/2510.27153)
*Tong Zhao,Jiacheng Li,Yuanchang Zhou,Guangming Tan,Weile Jia*

Main category: cs.LG

TL;DR: 提出了一种名为"E"的适配器，用于梯度优化器，使其在达到局部最小值后继续探索损失景观的谷底区域，以寻找更好的局部最小值，从而提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的优化器在达到局部最小值后就停止搜索参数空间，但复杂的损失景观几何特性使得无法保证该点是最低点或具有最佳泛化能力。

Method: 设计了一个适配器"E"，修改梯度优化器的行为，使其在达到局部最小值后继续沿着损失景观的谷底区域进行探索。

Result: 在大型批次训练场景中测试，改进后的Lamb优化器ALTO将当前最优优化器的测试准确率平均提高了2.5%。

Conclusion: 这种方法为优化算法设计开辟了新的研究方向，能够更有效地寻找更低、更平坦的局部最小值，从而获得更好的泛化性能。

Abstract: Finding lower and better-generalizing minima is crucial for deep learning.
However, most existing optimizers stop searching the parameter space once they
reach a local minimum. Given the complex geometric properties of the loss
landscape, it is difficult to guarantee that such a point is the lowest or
provides the best generalization. To address this, we propose an adaptor "E"
for gradient-based optimizers. The adapted optimizer tends to continue
exploring along landscape valleys (areas with low and nearly identical losses)
in order to search for potentially better local minima even after reaching a
local minimum. This approach increases the likelihood of finding a lower and
flatter local minimum, which is often associated with better generalization. We
also provide a proof of convergence for the adapted optimizers in both convex
and non-convex scenarios for completeness. Finally, we demonstrate their
effectiveness in an important but notoriously difficult training scenario,
large-batch training, where Lamb is the benchmark optimizer. Our testing
results show that the adapted Lamb, ALTO, increases the test accuracy
(generalization) of the current state-of-the-art optimizer by an average of
2.5% across a variety of large-batch training tasks. This work potentially
opens a new research direction in the design of optimization algorithms.

</details>


### [42] [Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](https://arxiv.org/abs/2510.27172)
*Zixuan Hu,Li Shen,Zhenyi Wang,Yongxian Wei,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出贝叶斯数据调度器（BDS），一种无需攻击模拟的自适应微调阶段防御策略，通过贝叶斯推理学习数据点安全属性的后验分布，从而减轻有害数据的影响。


<details>
  <summary>Details</summary>
Motivation: 现有防御策略通过攻击模拟预先建立鲁棒性，但存在根本限制：无法扩展到有界威胁模型之外，以及难以适应变化的攻击设置。有害微调对大型语言模型的微调即服务构成关键安全风险。

Method: 将有害微调防御建模为贝叶斯推理问题，学习每个数据点安全属性的后验分布。通过从后验分布中采样安全属性来加权数据，从而约束微调过程。引入基于摊销贝叶斯学习的神经调度器，实现高效迁移。

Result: 在各种攻击和防御设置下的综合结果表明，该方法达到了最先进的性能。

Conclusion: BDS通过贝叶斯推理实现了自适应防御，无需攻击模拟，能够针对特定数据集定制防御策略，并在不同设置下表现出色。

Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service
for large language models. Existing defense strategies preemptively build
robustness via attack simulation but suffer from fundamental limitations: (i)
the infeasibility of extending attack simulations beyond bounded threat models
due to the inherent difficulty of anticipating unknown attacks, and (ii)
limited adaptability to varying attack settings, as simulation fails to capture
their variability and complexity. To address these challenges, we propose
Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with
no need for attack simulation. BDS formulates harmful fine-tuning defense as a
Bayesian inference problem, learning the posterior distribution of each data
point's safety attribute, conditioned on the fine-tuning and alignment
datasets. The fine-tuning process is then constrained by weighting data with
their safety attributes sampled from the posterior, thus mitigating the
influence of harmful data. By leveraging the post hoc nature of Bayesian
inference, the posterior is conditioned on the fine-tuning dataset, enabling
BDS to tailor its defense to the specific dataset, thereby achieving adaptive
defense. Furthermore, we introduce a neural scheduler based on amortized
Bayesian learning, enabling efficient transfer to new data without retraining.
Comprehensive results across diverse attack and defense settings demonstrate
the state-of-the-art performance of our approach. Code is available at
https://github.com/Egg-Hu/Bayesian-Data-Scheduler.

</details>


### [43] [A Polynomial-time Algorithm for Online Sparse Linear Regression with Improved Regret Bound under Weaker Conditions](https://arxiv.org/abs/2510.27177)
*Junfan Li,Shizhong Liao,Zenglin Xu,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新的多项式时间算法，用于在线稀疏线性回归问题，在兼容性条件下显著改进了之前的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 研究在线稀疏线性回归问题，其中算法每实例只能访问k个属性进行预测，该问题已被证明是NP难问题。之前的工作需要数据矩阵满足线性独立性、兼容性条件或受限等距性质等假设。

Method: 利用Dantzig选择器，结合算法相关的协方差矩阵采样方案、自适应参数调整方案，以及带仔细初始化的批处理在线牛顿步法。

Result: 在比之前假设更弱的兼容性条件下，显著改进了之前的遗憾边界，并扩展到具有额外观测的OSLR问题。

Conclusion: 提出的算法在兼容性条件下实现了更好的遗憾边界，改进来自于对估计器ℓ1范数误差的更紧收敛率分析。

Abstract: In this paper, we study the problem of online sparse linear regression (OSLR)
where the algorithms are restricted to accessing only $k$ out of $d$ attributes
per instance for prediction, which was proved to be NP-hard. Previous work gave
polynomial-time algorithms assuming the data matrix satisfies the linear
independence of features, the compatibility condition, or the restricted
isometry property. We introduce a new polynomial-time algorithm, which
significantly improves previous regret bounds (Ito et al., 2017) under the
compatibility condition that is weaker than the other two assumptions. The
improvements benefit from a tighter convergence rate of the $\ell_1$-norm error
of our estimators. Our algorithm leverages the well-studied Dantzig Selector,
but importantly with several novel techniques, including an algorithm-dependent
sampling scheme for estimating the covariance matrix, an adaptive parameter
tuning scheme, and a batching online Newton step with careful initializations.
We also give novel and non-trivial analyses, including an induction method for
analyzing the $\ell_1$-norm error, careful analyses on the covariance of
non-independent random variables, and a decomposition on the regret. We further
extend our algorithm to OSLR with additional observations where the algorithms
can observe additional $k_0$ attributes after each prediction, and improve
previous regret bounds (Kale et al., 2017; Ito et al., 2017).

</details>


### [44] [MDAS-GNN: Multi-Dimensional Spatiotemporal GNN with Spatial Diffusion for Urban Traffic Risk Forecasting](https://arxiv.org/abs/2510.27197)
*Ziyuan Gao*

Main category: cs.LG

TL;DR: MDAS-GNN是一种基于多维注意力的空间扩散图神经网络，用于交通事故预测，通过整合交通安全、基础设施和环境风险三个核心维度，在多个英国城市数据集上表现出优于基准方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统事故预测模型将路段视为独立单元，无法捕捉城市交通网络中复杂的空间关系和时间依赖性，而交通事故每年造成全球135万人死亡，是严重的公共卫生挑战。

Method: 开发MDAS-GNN框架，采用特征特定的空间扩散机制和多头时间注意力，捕捉不同时间跨度的依赖性，整合三个风险维度：交通安全、基础设施和环境风险。

Result: 在英国交通部三个城市数据集上的评估显示，MDAS-GNN相比基准方法具有优越性能，在短、中、长期预测中保持低误差，长期预测表现尤其突出。消融研究证实多维特征集成比单特征方法减少预测误差达40%。

Conclusion: 该框架为土木工程师和城市规划者提供了先进的交通基础设施设计预测能力，支持数据驱动的道路网络优化、基础设施资源改进和城市发展项目中的战略安全干预。

Abstract: Traffic accidents represent a critical public health challenge, claiming over
1.35 million lives annually worldwide. Traditional accident prediction models
treat road segments independently, failing to capture complex spatial
relationships and temporal dependencies in urban transportation networks. This
study develops MDAS-GNN, a Multi-Dimensional Attention-based Spatial-diffusion
Graph Neural Network integrating three core risk dimensions: traffic safety,
infrastructure, and environmental risk. The framework employs feature-specific
spatial diffusion mechanisms and multi-head temporal attention to capture
dependencies across different time horizons. Evaluated on UK Department for
Transport accident data across Central London, South Manchester, and SE
Birmingham, MDASGNN achieves superior performance compared to established
baseline methods. The model maintains consistently low prediction errors across
short, medium, and long-term periods, with particular strength in long-term
forecasting. Ablation studies confirm that integrated multi-dimensional
features outperform singlefeature approaches, reducing prediction errors by up
to 40%. This framework provides civil engineers and urban planners with
advanced predictive capabilities for transportation infrastructure design,
enabling data-driven decisions for road network optimization, infrastructure
resource improvements, and strategic safety interventions in urban development
projects.

</details>


### [45] [Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models](https://arxiv.org/abs/2510.27207)
*Hamed Najafi,Dongsheng Luo,Jason Liu*

Main category: cs.LG

TL;DR: 提出了Feature-Function Curvature Analysis (FFCA)框架，通过分析模型学习函数的几何特性，为每个特征生成4维签名，并扩展到动态原型分析来追踪训练过程中的特征演化。


<details>
  <summary>Details</summary>
Motivation: 主流可解释AI方法通常提供不完整的静态视图，将特征作用简化为单一分数，无法处理非线性和交互效应。

Method: FFCA框架量化特征的4个维度：影响、波动性、非线性和交互性，并通过动态原型分析追踪训练过程中的特征签名演化。

Result: 首次直接实证证明了分层学习现象，模型先学习简单线性效应再学习复杂交互；动态分析可识别模型容量不足并预测过拟合发生。

Conclusion: FFCA通过静态和动态组件提供必要的几何上下文，将模型解释从简单量化转变为对整个学习过程的细致可信分析。

Abstract: Explainable AI (XAI) is critical for building trust in complex machine
learning models, yet mainstream attribution methods often provide an
incomplete, static picture of a model's final state. By collapsing a feature's
role into a single score, they are confounded by non-linearity and
interactions. To address this, we introduce Feature-Function Curvature Analysis
(FFCA), a novel framework that analyzes the geometry of a model's learned
function. FFCA produces a 4-dimensional signature for each feature, quantifying
its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction.
Crucially, we extend this framework into Dynamic Archetype Analysis, which
tracks the evolution of these signatures throughout the training process. This
temporal view moves beyond explaining what a model learned to revealing how it
learns. We provide the first direct, empirical evidence of hierarchical
learning, showing that models consistently learn simple linear effects before
complex interactions. Furthermore, this dynamic analysis provides novel,
practical diagnostics for identifying insufficient model capacity and
predicting the onset of overfitting. Our comprehensive experiments demonstrate
that FFCA, through its static and dynamic components, provides the essential
geometric context that transforms model explanation from simple quantification
to a nuanced, trustworthy analysis of the entire learning process.

</details>


### [46] [Soft Task-Aware Routing of Experts for Equivariant Representation Learning](https://arxiv.org/abs/2510.27222)
*Jaebyeong Jeon,Hyeonseo Jang,Jy-yong Sohn,Kibok Lee*

Main category: cs.LG

TL;DR: 提出了Soft Task-Aware Routing (STAR)方法，通过将投影头建模为专家来减少不变和等变表示学习中的冗余特征学习，提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用分离的投影头来联合学习不变和等变表示，但忽略了它们之间的共享信息，导致冗余特征学习和模型容量利用不足。

Method: 引入STAR路由策略，将投影头建模为专家，使专家专门捕获共享或任务特定信息，通过降低不变和等变嵌入之间的典型相关性来减少冗余。

Result: 实验结果显示在各种迁移学习任务中都有持续改进，验证了STAR方法的有效性。

Conclusion: STAR方法通过建模投影头为专家来减少冗余特征学习，提高了不变和等变表示学习的效率，在迁移学习中表现优异。

Abstract: Equivariant representation learning aims to capture variations induced by
input transformations in the representation space, whereas invariant
representation learning encodes semantic information by disregarding such
transformations. Recent studies have shown that jointly learning both types of
representations is often beneficial for downstream tasks, typically by
employing separate projection heads. However, this design overlooks information
shared between invariant and equivariant learning, which leads to redundant
feature learning and inefficient use of model capacity. To address this, we
introduce Soft Task-Aware Routing (STAR), a routing strategy for projection
heads that models them as experts. STAR induces the experts to specialize in
capturing either shared or task-specific information, thereby reducing
redundant feature learning. We validate this effect by observing lower
canonical correlations between invariant and equivariant embeddings.
Experimental results show consistent improvements across diverse transfer
learning tasks. The code is available at https://github.com/YonseiML/star.

</details>


### [47] [FedSM: Robust Semantics-Guided Feature Mixup for Bias Reduction in Federated Learning with Long-Tail Data](https://arxiv.org/abs/2510.27240)
*Jingrui Zhang,Yimeng Xu,Shujie Li,Feng Liang,Haihan Duan,Yanjie Dong,Victor C. M. Leung,Xiping Hu*

Main category: cs.LG

TL;DR: FedSM是一个客户端中心的联邦学习框架，通过语义引导的特征混合和轻量级分类器重训练来解决非IID和长尾数据分布导致的模型偏差问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID和长尾数据分布下会产生有偏的全局模型，需要解决这种数据不平衡带来的偏差问题。

Method: 使用预训练的图文对齐模型计算类别级语义相关性，指导本地特征与全局原型进行混合，生成类别一致的伪特征，并通过概率类别选择增强特征多样性。

Result: 在多个长尾数据集上的实验表明，FedSM在准确性上持续优于最先进方法，对领域偏移具有高鲁棒性且计算效率高。

Conclusion: FedSM通过语义引导的特征混合有效缓解了联邦学习中的数据偏差问题，所有计算都在本地进行，服务器开销最小。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing private data. However, FL suffers from
biased global models due to non-IID and long-tail data distributions. We
propose \textbf{FedSM}, a novel client-centric framework that mitigates this
bias through semantics-guided feature mixup and lightweight classifier
retraining. FedSM uses a pretrained image-text-aligned model to compute
category-level semantic relevance, guiding the category selection of local
features to mix-up with global prototypes to generate class-consistent
pseudo-features. These features correct classifier bias, especially when data
are heavily skewed. To address the concern of potential domain shift between
the pretrained model and the data, we propose probabilistic category selection,
enhancing feature diversity to effectively mitigate biases. All computations
are performed locally, requiring minimal server overhead. Extensive experiments
on long-tail datasets with various imbalanced levels demonstrate that FedSM
consistently outperforms state-of-the-art methods in accuracy, with high
robustness to domain shift and computational efficiency.

</details>


### [48] [Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation](https://arxiv.org/abs/2510.27253)
*Qiyan Deng,Changqian Zheng,Lianpeng Qiao,Yuping Wang,Chengliang Chai,Lei Cao*

Main category: cs.LG

TL;DR: 提出IWD框架，利用影响函数在数据集蒸馏中考虑数据质量，通过自适应权重优先处理有益数据，提升蒸馏数据集质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法假设所有实例贡献相等，但实际数据集中包含冗余甚至有害实例，直接蒸馏会降低模型性能。

Method: IWD框架利用影响函数评估每个实例对蒸馏目标的影响，分配自适应权重，优先处理有益数据，可无缝集成到各种蒸馏框架中。

Result: 集成IWD能提高蒸馏数据集质量，增强模型性能，准确率提升最高达7.8%。

Conclusion: IWD通过考虑数据质量改进数据集蒸馏，提供模块化设计，能有效提升蒸馏效果和模型性能。

Abstract: Dataset distillation condenses large datasets into synthetic subsets,
achieving performance comparable to training on the full dataset while
substantially reducing storage and computation costs. Most existing dataset
distillation methods assume that all real instances contribute equally to the
process. In practice, real-world datasets contain both informative and
redundant or even harmful instances, and directly distilling the full dataset
without considering data quality can degrade model performance. In this work,
we present Influence-Weighted Distillation IWD, a principled framework that
leverages influence functions to explicitly account for data quality in the
distillation process. IWD assigns adaptive weights to each instance based on
its estimated impact on the distillation objective, prioritizing beneficial
data while downweighting less useful or harmful ones. Owing to its modular
design, IWD can be seamlessly integrated into diverse dataset distillation
frameworks. Our empirical results suggest that integrating IWD tends to improve
the quality of distilled datasets and enhance model performance, with accuracy
gains of up to 7.8%.

</details>


### [49] [ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models](https://arxiv.org/abs/2510.27256)
*Xin Tang,Youfang Han,Fangfei Gou,Wei Zhao,Xin Meng,Yang Yu,Jinguo Zhang,Yuanchun Shi,Yuntao Wang,Tengxiang Zhang*

Main category: cs.LG

TL;DR: 提出了ECVL-ROUTER，首个面向视觉语言模型的场景感知路由框架，通过动态选择合适模型来平衡响应速度、输出质量和能耗。


<details>
  <summary>Details</summary>
Motivation: 用户需求在不同场景下差异很大（快速响应、高质量输出、低能耗），仅依赖云端大模型会导致高延迟和高能耗，而边缘小模型能处理简单任务但能力有限。

Method: 引入新的路由策略和评估指标，根据用户需求动态选择模型，并构建了专门用于路由器训练的多模态响应质量数据集。

Result: 成功将80%以上的查询路由到小模型，同时问题解决概率下降不到10%。

Conclusion: 该框架有效平衡了大模型和小模型的优势，在保持性能的同时显著降低了延迟和能耗。

Abstract: Vision-Language Models (VLMs) excel in diverse multimodal tasks. However,
user requirements vary across scenarios, which can be categorized into fast
response, high-quality output, and low energy consumption. Relying solely on
large models deployed in the cloud for all queries often leads to high latency
and energy cost, while small models deployed on edge devices are capable of
handling simpler tasks with low latency and energy cost. To fully leverage the
strengths of both large and small models, we propose ECVL-ROUTER, the first
scenario-aware routing framework for VLMs. Our approach introduces a new
routing strategy and evaluation metrics that dynamically select the appropriate
model for each query based on user requirements, maximizing overall utility. We
also construct a multimodal response-quality dataset tailored for router
training and validate the approach through extensive experiments. Results show
that our approach successfully routes over 80\% of queries to the small model
while incurring less than 10\% drop in problem solving probability.

</details>


### [50] [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)
*Yifan Zhang,Zhen Qin,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了高阶线性注意力（HLA），一种因果流式机制，通过紧凑前缀充分统计实现高阶交互，在保持恒定大小状态的同时实现线性时间计算，无需构建n×n矩阵。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型扩展到长上下文时缩放点积注意力的二次成本问题，现有线性时间注意力和状态空间模型通常限于一阶或基于核的近似，限制了表达能力。

Method: 使用紧凑前缀充分统计实现高阶交互，提供闭式流式恒等式、严格因果掩码变体和基于关联扫描的块并行训练方案，精确重现串行递归的激活。

Result: HLA在保持恒定状态大小的情况下实现线性时间计算，结合了注意力样的数据依赖混合与现代循环架构的效率。

Conclusion: HLA作为一个原则性、可扩展的构建块，将注意力样的数据依赖混合与现代循环架构的效率相结合。

Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.

</details>


### [51] [ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction](https://arxiv.org/abs/2510.27263)
*Han Yu,Kehan Li,Dongbai Li,Yue He,Xingxuan Zhang,Peng Cui*

Main category: cs.LG

TL;DR: 提出了ODP-Bench基准，用于统一评估OOD性能预测算法，包含常用OOD数据集和现有算法，提供训练好的模型作为测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有OOD性能预测研究评估协议不一致，覆盖的数据集和分布偏移类型有限，需要公平比较框架。

Method: 构建综合基准ODP-Bench，整合常用OOD数据集和现有性能预测算法，提供预训练模型作为测试平台。

Result: 建立了统一的评估框架，避免了重复训练模型的负担，保证了比较的一致性。

Conclusion: ODP-Bench为OOD性能预测研究提供了便利和公平的比较平台，并通过深入实验分析揭示了算法能力边界。

Abstract: Recently, there has been gradually more attention paid to Out-of-Distribution
(OOD) performance prediction, whose goal is to predict the performance of
trained models on unlabeled OOD test datasets, so that we could better leverage
and deploy off-the-shelf trained models in risk-sensitive scenarios. Although
progress has been made in this area, evaluation protocols in previous
literature are inconsistent, and most works cover only a limited number of
real-world OOD datasets and types of distribution shifts. To provide convenient
and fair comparisons for various algorithms, we propose Out-of-Distribution
Performance Prediction Benchmark (ODP-Bench), a comprehensive benchmark that
includes most commonly used OOD datasets and existing practical performance
prediction algorithms. We provide our trained models as a testbench for future
researchers, thus guaranteeing the consistency of comparison and avoiding the
burden of repeating the model training process. Furthermore, we also conduct
in-depth experimental analyses to better understand their capability boundary.

</details>


### [52] [HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction](https://arxiv.org/abs/2510.27281)
*Minghui Li,Yuanhang Wang,Peijin Guo,Wei Wan,Shengshan Hu,Shengqing Hu*

Main category: cs.LG

TL;DR: HiF-DTA是一个用于药物-靶点亲和力预测的分层网络，通过双路径策略提取药物和蛋白质的全局序列语义和局部拓扑特征，并采用多尺度方法融合原子、亚结构和分子级表示。


<details>
  <summary>Details</summary>
Motivation: 现有序列深度学习方法忽略了同时建模全局序列语义特征和局部拓扑结构特征，且将药物表示为扁平序列而缺乏多尺度特征表示。

Method: 采用双路径策略提取药物和蛋白质的全局序列语义和局部拓扑特征，通过多尺度方法学习原子、亚结构和分子级表示，并使用多尺度双线性注意力模块进行融合。

Result: 在Davis、KIBA和Metz数据集上的实验表明，HiF-DTA优于现有最先进基线方法，消融实验证实了全局-局部特征提取和多尺度融合的重要性。

Conclusion: HiF-DTA通过分层网络结构和多尺度特征融合，有效提升了药物-靶点亲和力预测的准确性。

Abstract: Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing
experimental costs and accelerating early screening in computational drug
discovery. While sequence-based deep learning methods avoid reliance on costly
3D structures, they still overlook simultaneous modeling of global sequence
semantic features and local topological structural features within drugs and
proteins, and represent drugs as flat sequences without atomic-level,
substructural-level, and molecular-level multi-scale features. We propose
HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract
both global sequence semantic and local topological features from drug and
protein sequences, and models drugs multi-scale to learn atomic, substructural,
and molecular representations fused via a multi-scale bilinear attention
module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms
state-of-the-art baselines, with ablations confirming the importance of
global-local extraction and multi-scale fusion.

</details>


### [53] [Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments](https://arxiv.org/abs/2510.27287)
*Harsh Vishwakarma,Ankush Agarwal,Ojas Patil,Chaitanya Devaguptapu,Mahesh Chandran*

Main category: cs.LG

TL;DR: EnterpriseBench是一个模拟企业环境的综合基准测试，包含500个跨领域任务，用于评估LLM在企业系统中的表现，结果显示最先进模型的任务完成率仅为41.8%。


<details>
  <summary>Details</summary>
Motivation: 企业系统对提升员工和客户的生产力与决策能力至关重要，但将LLM集成到企业系统中面临数据分散和访问控制复杂等挑战，需要专门的评估基准。

Method: 开发了EnterpriseBench基准测试，模拟企业环境特征，包括数据源碎片化、访问控制层次和跨职能工作流，并提供从组织元数据生成内部一致任务的数据生成管道。

Result: 使用最先进的LLM代理进行实验，结果显示即使最强大的模型也只能完成41.8%的任务，表明企业AI系统仍有很大改进空间。

Conclusion: EnterpriseBench填补了企业环境评估基准的空白，揭示了当前LLM在企业任务处理中的局限性，为开发更有效的企业AI系统提供了重要参考。

Abstract: Enterprise systems are crucial for enhancing productivity and decision-making
among employees and customers. Integrating LLM based systems into enterprise
systems enables intelligent automation, personalized experiences, and efficient
information retrieval, driving operational efficiency and strategic growth.
However, developing and evaluating such systems is challenging due to the
inherent complexity of enterprise environments, where data is fragmented across
multiple sources and governed by sophisticated access controls. We present
EnterpriseBench, a comprehensive benchmark that simulates enterprise settings,
featuring 500 diverse tasks across software engineering, HR, finance, and
administrative domains. Our benchmark uniquely captures key enterprise
characteristics including data source fragmentation, access control
hierarchies, and cross-functional workflows. Additionally, we provide a novel
data generation pipeline that creates internally consistent enterprise tasks
from organizational metadata. Experiments with state-of-the-art LLM agents
demonstrate that even the most capable models achieve only 41.8% task
completion, highlighting significant opportunities for improvement in
enterprise-focused AI systems.

</details>


### [54] [Temporal Cardiovascular Dynamics for Improved PPG-Based Heart Rate Estimation](https://arxiv.org/abs/2510.27297)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 提出一种基于互信息分析心率非线性混沌行为的新方法，结合深度学习显著提升心率估计精度，在真实场景数据集上相比传统方法提升达40%。


<details>
  <summary>Details</summary>
Motivation: 心率振荡具有复杂的非线性混沌特性，这给日常生活中的心血管健康监测带来挑战，需要从数学角度解释和处理这种非线性时间复杂性。

Method: 通过互信息研究心率的非线性混沌行为，提出一种能够增强真实条件下心率估计的新方法，并与深度学习解决方案相结合。

Result: 在四个真实场景数据集上的验证表明，该方法相比传统方法和现有机器学习技术，心率估计精度提升高达40%，减少了对多模态传感的依赖并消除了后处理需求。

Conclusion: 提出的方法不仅从数学角度解释了心率的非线性时间复杂性，还显著提升了深度学习解决方案的性能，为日常心血管健康监测提供了更有效的工具。

Abstract: The oscillations of the human heart rate are inherently complex and
non-linear -- they are best described by mathematical chaos, and they present a
challenge when applied to the practical domain of cardiovascular health
monitoring in everyday life. In this work, we study the non-linear chaotic
behavior of heart rate through mutual information and introduce a novel
approach for enhancing heart rate estimation in real-life conditions. Our
proposed approach not only explains and handles the non-linear temporal
complexity from a mathematical perspective but also improves the deep learning
solutions when combined with them. We validate our proposed method on four
established datasets from real-life scenarios and compare its performance with
existing algorithms thoroughly with extensive ablation experiments. Our results
demonstrate a substantial improvement, up to 40\%, of the proposed approach in
estimating heart rate compared to traditional methods and existing
machine-learning techniques while reducing the reliance on multiple sensing
modalities and eliminating the need for post-processing steps.

</details>


### [55] [Binary Anomaly Detection in Streaming IoT Traffic under Concept Drift](https://arxiv.org/abs/2510.27304)
*Rodrigo Matos Carnier,Laura Lahesoo,Kensuke Fukuda*

Main category: cs.LG

TL;DR: 该研究比较了批处理和流式学习在物联网异常检测中的表现，发现流式学习方法能更好地处理概念漂移问题，其中自适应随机森林在保持高准确率的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着物联网流量增长，传统批学习模型面临维护成本高、对概念漂移适应性差的问题，需要研究流式学习方法在物联网异常检测中的应用。

Method: 将物联网流量异常检测作为二分类问题，通过混合现有数据集模拟异构网络数据流，逐个样本流式处理，比较树基和非树基机器学习算法在批处理和流式学习中的表现。

Result: 自适应随机森林达到F1分数0.990±0.006，计算成本仅为批处理的三分之一；Hoeffding自适应树达到F1分数0.910±0.007，计算成本降低四倍，但稳定性略有牺牲。

Conclusion: 流式学习在物联网异常检测中优于批处理，能有效处理概念漂移，树基算法表现优异，但当前数据集在暴露模型局限性方面仍有不足。

Abstract: With the growing volume of Internet of Things (IoT) network traffic, machine
learning (ML)-based anomaly detection is more relevant than ever. Traditional
batch learning models face challenges such as high maintenance and poor
adaptability to rapid anomaly changes, known as concept drift. In contrast,
streaming learning integrates online and incremental learning, enabling
seamless updates and concept drift detection to improve robustness. This study
investigates anomaly detection in streaming IoT traffic as binary
classification, comparing batch and streaming learning approaches while
assessing the limitations of current IoT traffic datasets. We simulated
heterogeneous network data streams by carefully mixing existing datasets and
streaming the samples one by one. Our results highlight the failure of batch
models to handle concept drift, but also reveal persisting limitations of
current datasets to expose model limitations due to low traffic heterogeneity.
We also investigated the competitiveness of tree-based ML algorithms,
well-known in batch anomaly detection, and compared it to non-tree-based ones,
confirming the advantages of the former. Adaptive Random Forest achieved
F1-score of 0.990 $\pm$ 0.006 at one-third the computational cost of its batch
counterpart. Hoeffding Adaptive Tree reached F1-score of 0.910 $\pm$ 0.007,
reducing computational cost by four times, making it a viable choice for online
applications despite a slight trade-off in stability.

</details>


### [56] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 本文提出了一种新的语义新颖性评估方法，通过不可归因性来衡量模型输出是否在预训练语料中没有语义相似的上下文。


<details>
  <summary>Details</summary>
Motivation: 传统训练数据归因方法关注哪些训练样本影响了特定输出，而本文反其道而行之，研究哪些输出无法归因到任何预训练样本，以此评估模型生成的语义新颖性。

Method: 使用两阶段检索流程：先用轻量级GIST嵌入索引语料库并检索前n个候选，然后用ColBERTv2重新排序。如果最近的语料项归因度低于人类生成的文本参考，则认为模型输出具有新颖性。

Result: 在SmolLM和SmolLM2上的评估发现：(1)模型利用预训练数据的跨度比之前报道的要长；(2)某些领域系统性地促进或抑制新颖性；(3)指令微调不仅改变风格，还增加新颖性。

Conclusion: 围绕不可归因性重新定义新颖性评估，能够在预训练规模上进行高效分析。作者发布了约20TB的语料块和索引工件以支持复现和大规模扩展分析。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [57] [MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data](https://arxiv.org/abs/2510.27321)
*Yu-Chen Kuo,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: MedM2T是一个时间感知的多模态框架，用于处理医疗数据的多模态性和异质性时间结构，在心血管疾病预测、院内死亡率预测和ICU住院时间回归任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗数据具有固有的多模态性和异质性时间结构，这给建模带来了重大挑战，需要开发能够处理这些复杂性的框架。

Method: MedM2T集成了稀疏时间序列编码器处理不规则稀疏时间序列，分层时间感知融合捕获微观和宏观时间模式，双模态注意力提取跨模态交互，并使用模态特定的预训练编码器和共享编码器对齐特征。

Result: 在MIMIC-IV和MIMIC-IV-ECG数据集上，MedM2T在三个任务上表现优异：CVD预测AUROC 0.947、AUPRC 0.706；死亡率预测AUROC 0.901、AUPRC 0.558；ICU住院时间回归MAE 2.31。

Conclusion: MedM2T展现了在临床预测中的鲁棒性和广泛适用性，是一个有前景的工具。

Abstract: The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.

</details>


### [58] [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
*Arun Jose*

Main category: cs.LG

TL;DR: 基于结果强化学习的语言模型在链式思维推理方面表现出色，但研究发现强化学习会导致推理过程变得难以理解，而最终答案仍然可读，这可能会削弱监控方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究链式思维推理的可读性和忠实性，以了解模型意图并检测潜在恶意行为，因为有效的监控需要推理过程清晰可读。

Method: 分析了14个推理模型的链式思维可读性，比较了强化学习训练前后的变化，并通过强制使用可读部分来测试推理质量。

Result: 强化学习导致推理过程变得难以理解（除Claude外），模型使用不可读的推理得出正确答案，可读性在更难问题上进一步下降，但可读性与性能之间没有直接相关性。

Conclusion: 如果没有明确优化可读性，基于结果的强化学习自然会产生推理过程不透明的模型，这可能破坏监控方法的有效性。

Abstract: Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model's CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.

</details>


### [59] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 该论文提出了一个监控性评分框架，结合忠实性和详尽性来评估思维链的质量，发现模型可能看似忠实但难以监控，且不同模型家族的监控性差异显著。


<details>
  <summary>Details</summary>
Motivation: 思维链输出提供了模型逐步推理的可见性，但只有当思维链对其内部推理保持透明时，这种可见性才能帮助发现不安全或未对齐的行为。当前衡量忠实性的方法存在局限性。

Method: 引入详尽性概念（思维链是否列出解决任务所需的每个因素），并将忠实性和详尽性结合成一个单一的监控性评分，评估思维链作为模型外部'工作记忆'的效果。在BBH、GPQA和MMLU数据集上评估指令调优和推理模型。

Result: 模型可能看似忠实但难以监控，当它们遗漏关键因素时；不同模型家族的监控性差异显著。

Conclusion: 监控性评分框架能够更全面地评估思维链作为安全监控工具的有效性，不同模型在监控性方面表现差异明显，为基于思维链监控的安全方案提供了重要参考。

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [60] [FedMuon: Accelerating Federated Learning with Matrix Orthogonalization](https://arxiv.org/abs/2510.27403)
*Junkang Liu,Fanhua Shang,Junchao Zhou,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了FedMuon优化器来解决联邦学习中的通信瓶颈问题，通过矩阵正交化优化和客户端漂移抑制技术，显著减少通信轮次并提高收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的核心瓶颈在于通信轮次，现有方法使用元素级优化器(Adam/SGD)忽略了权重矩阵的几何结构，导致病态方向放大和收敛缓慢。

Method: 引入Muon优化器进行矩阵正交化，并提出FedMuon包含两个关键技术：动量聚合和局部-全局对齐，以减少客户端漂移。

Result: 在IID设置下显著加速收敛并减少通信轮次；在非IID设置下，FedMuon实现了线性加速收敛率，并在语言和视觉模型上验证了有效性。

Conclusion: FedMuon通过矩阵结构优化和客户端漂移抑制，有效解决了联邦学习的通信瓶颈问题，在IID和非IID设置下都表现出优越性能。

Abstract: The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.

</details>


### [61] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment框架通过将未知的潜在空间与标记化的概念图谱对齐，实现跨语言模型的解释性转移，无需训练特定模型的稀疏自编码器或手动标记组件。


<details>
  <summary>Details</summary>
Motivation: 现有解释性方法成本高、难以扩展，需要为每个新模型训练稀疏自编码器、手动标记组件并进行验证，限制了可解释AI的广泛应用。

Method: 使用共享输入和轻量级表示对齐技术，将未知潜在空间与预构建的概念图谱（标记化的人类可解释潜在空间）进行对齐。

Result: 对齐后能够在先前不透明的模型中实现语义特征搜索检索和沿人类可解释概念的可控生成，无需标记概念数据。

Conclusion: 该框架通过投资一个高质量概念图谱，以最小边际成本使多个新模型变得透明可控，分摊了可解释AI和机制解释性的成本。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [62] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 提出MVeLMA多模态端到端机器学习管道，用于预测野火后县级植被损失，通过多模态特征集成和堆叠集成架构提高预测性能，并生成置信度地图辅助恢复工作。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分探索影响植被损失的所有因素及其相互作用，且预测模型缺乏可解释性，限制了实际应用价值。

Method: 使用多模态特征集成管道和堆叠集成架构，结合概率建模进行不确定性估计。

Result: 模型在预测野火后植被损失方面优于多个最先进和基线模型，并能生成植被损失置信度地图识别高风险县。

Conclusion: 该研究成果可为未来灾害救援规划、生态政策制定和野生动物恢复管理提供重要参考。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [63] [Spectral Neural Graph Sparsification](https://arxiv.org/abs/2510.27474)
*Angelica Liguori,Ettore Ritacco,Pietro Sabatino,Annalisa Socievole*

Main category: cs.LG

TL;DR: 提出Spectral Preservation Network（SPN）框架，通过生成保留原始图谱特性的简化图来进行图表示学习，降低计算成本的同时支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络依赖固定结构且易出现过平滑问题，需要一种能够自适应演化图结构和节点特征的方法。

Method: 引入联合图演化层和谱一致性损失，前者联合变换图拓扑和节点特征矩阵，后者通过强制图谱特性和节点特征向量的一致性来正则化变换过程。

Result: 在节点级稀疏化任务上，通过分析成熟指标并与最先进方法对比，实验结果显示SPN具有优越性能和明显优势。

Conclusion: SPN框架通过谱保持机制有效解决了传统图神经网络的局限性，在保持图关键特性的同时显著降低了计算复杂度。

Abstract: Graphs are central to modeling complex systems in domains such as social
networks, molecular chemistry, and neuroscience. While Graph Neural Networks,
particularly Graph Convolutional Networks, have become standard tools for graph
learning, they remain constrained by reliance on fixed structures and
susceptibility to over-smoothing. We propose the Spectral Preservation Network,
a new framework for graph representation learning that generates reduced graphs
serving as faithful proxies of the original, enabling downstream tasks such as
community detection, influence propagation, and information diffusion at a
reduced computational cost. The Spectral Preservation Network introduces two
key components: the Joint Graph Evolution layer and the Spectral Concordance
loss. The former jointly transforms both the graph topology and the node
feature matrix, allowing the structure and attributes to evolve adaptively
across layers and overcoming the rigidity of static neighborhood aggregation.
The latter regularizes these transformations by enforcing consistency in both
the spectral properties of the graph and the feature vectors of the nodes. We
evaluate the effectiveness of Spectral Preservation Network on node-level
sparsification by analyzing well-established metrics and benchmarking against
state-of-the-art methods. The experimental results demonstrate the superior
performance and clear advantages of our approach.

</details>


### [64] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出了一种在单纯形上学习和采样概率分布的方法，通过平滑双射将开单纯形映射到欧几里得空间，利用Aitchison几何定义映射，并支持通过Dirichlet插值对分类数据进行建模。


<details>
  <summary>Details</summary>
Motivation: 现有的在单纯形上操作的方法使用黎曼几何或自定义噪声过程，而本方法在欧几里得空间中工作同时尊重Aitchison几何，旨在提供更有效的建模方式。

Method: 使用平滑双射将开单纯形映射到欧几里得空间，利用Aitchison几何定义映射，通过Dirichlet插值将离散观测去量化成连续观测，从而在欧几里得空间中进行密度建模。

Result: 在合成和真实世界数据集上实现了竞争性性能，能够精确恢复原始离散分布。

Conclusion: 该方法在欧几里得空间中有效建模单纯形上的概率分布，同时保持对Aitchison几何的尊重，性能优于现有方法。

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [65] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 通过重采样研究推理模型的因果影响，发现单条思维链分析不足，需要从分布层面理解模型推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多只分析单条思维链，但模型实际上定义了多种可能推理路径的分布，需要更全面的因果分析方法。

Method: 使用重采样方法研究模型决策：测量特定句子的因果影响、比较策略内/外干预效果、引入韧性指标评估推理步骤重要性、应用因果中介分析。

Result: 自保陈述因果影响小；策略外干预效果弱且不稳定；关键规划陈述难以移除但移除后影响大；提示即使被移除仍对推理有持续影响。

Conclusion: 通过重采样研究分布能够实现可靠的因果分析，更清晰地描述模型推理过程，并为思维链干预提供原则性方法。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [66] [FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models](https://arxiv.org/abs/2510.27486)
*Junkang Liu,Fanhua Shang,Kewen Zhu,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了FedAdamW算法，这是第一个专门为联邦学习设计的AdamW优化器，通过局部校正机制和分离权重衰减来解决数据异构性、客户端漂移和收敛速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中直接应用AdamW存在三个主要挑战：数据异构导致二阶矩估计方差高、局部过拟合导致客户端漂移、每轮重新初始化矩估计减慢收敛速度。

Method: FedAdamW使用局部校正机制和分离权重衰减来对齐局部更新与全局更新，高效聚合二阶矩估计的均值以减少方差，并进行重新初始化。

Result: 理论证明FedAdamW实现了线性加速收敛率，无需异构性假设。在语言和视觉Transformer模型上的实验表明，相比基线方法显著减少了通信轮数并提高了测试准确率。

Conclusion: FedAdamW是第一个专门为联邦学习设计的AdamW优化器，能有效解决数据异构环境下的训练挑战，在通信效率和模型性能方面都有显著提升。

Abstract: AdamW has become one of the most effective optimizers for training
large-scale models. We have also observed its effectiveness in the context of
federated learning (FL). However, directly applying AdamW in federated learning
settings poses significant challenges: (1) due to data heterogeneity, AdamW
often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)
the local overfitting of AdamW may cause client drift; and (3) Reinitializing
moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down
convergence. To address these challenges, we propose the first
\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},
for training and fine-tuning various large models. \texttt{FedAdamW} aligns
local updates with the global update using both a \textbf{local correction
mechanism} and decoupled weight decay to mitigate local overfitting.
\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment
estimates to reduce their variance and reinitialize them. Theoretically, we
prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of
$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$
without \textbf{heterogeneity assumption}, where $S$ is the number of
participating clients per round, $K$ is the number of local iterations, and $R$
is the total number of communication rounds. We also employ PAC-Bayesian
generalization analysis to explain the effectiveness of decoupled weight decay
in local training. Empirically, we validate the effectiveness of
\texttt{FedAdamW} on language and vision Transformer models. Compared to
several baselines, \texttt{FedAdamW} significantly reduces communication rounds
and improves test accuracy. The code is available in
https://github.com/junkangLiu0/FedAdamW.

</details>


### [67] [InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames](https://arxiv.org/abs/2510.27497)
*Haorui Li,Weitao Du,Yuqiang Li,Hongyu Guo,Shengchao Liu*

Main category: cs.LG

TL;DR: InertialAR是一个基于Transformer的自回归模型，通过惯性框架对齐和几何感知注意力机制解决3D分子生成的SE(3)不变性和排列不变性问题，在无条件分子生成和可控生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: Transformer自回归模型在文本和图像模态上取得了成功，但在3D分子生成领域仍未被充分探索，主要面临两个挑战：(1)如何将分子规范化为对SE(3)变换和原子排列不变的1D序列；(2)如何设计能够建模混合原子令牌（包含离散原子类型和连续3D坐标）的架构。

Method: 提出InertialAR方法：1）通过惯性框架对齐和原子重排序实现规范化的令牌化；2）使用几何旋转位置编码(GeoRoPE)为注意力机制注入几何感知能力；3）采用分层自回归范式，先预测原子类型再通过扩散损失预测3D坐标。

Result: 在QM9、GEOM-Drugs和B3LYP数据集的无条件分子生成任务中，在10个评估指标中的7个达到最先进性能。在目标化学功能性的可控生成任务中，在所有5个指标上显著优于强基线模型，获得最先进结果。

Conclusion: InertialAR成功将Transformer自回归模型扩展到3D分子生成领域，通过创新的规范化令牌化和几何感知架构设计，解决了该领域的关键挑战，在多个基准测试中展现了卓越性能。

Abstract: Transformer-based autoregressive models have emerged as a unifying paradigm
across modalities such as text and images, but their extension to 3D molecule
generation remains underexplored. The gap stems from two fundamental
challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens
that is invariant to both SE(3) transformations and atom index permutations,
and (2) designing an architecture capable of modeling hybrid atom-based tokens
that couple discrete atom types with continuous 3D coordinates. To address
these challenges, we introduce InertialAR. InertialAR devises a canonical
tokenization that aligns molecules to their inertial frames and reorders atoms
to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the
attention mechanism with geometric awareness via geometric rotary positional
encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive
paradigm to predict the next atom-based token, predicting the atom type first
and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR
achieves state-of-the-art performance on 7 of the 10 evaluation metrics for
unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover,
it significantly outperforms strong baselines in controllable generation for
targeted chemical functionality, attaining state-of-the-art results across all
5 metrics.

</details>


### [68] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 提出DP-FedPGN算法，通过引入全局梯度范数惩罚来寻找全局平坦最小值，解决客户端差分隐私联邦学习中模型泛化性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的客户端差分隐私联邦学习方法会导致更尖锐的损失景观，降低模型泛化能力。虽然使用SAM可以找到局部平坦最小值，但在CL-DPFL中局部平坦性不能反映全局平坦性。

Method: 提出DP-FedPGN算法，在局部损失中引入全局梯度范数惩罚来寻找全局平坦最小值，同时减少局部更新范数和梯度裁剪误差。使用Rényi DP提供严格隐私保证。

Result: 在ResNet和Transformer模型上进行了有效性测试，在六个视觉和自然语言处理任务中相比现有最先进算法取得了显著改进。

Conclusion: DP-FedPGN能够缓解DP导致的性能下降，消除数据异构性的影响，实现快速收敛，并提供严格的隐私保证。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [69] [Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs](https://arxiv.org/abs/2510.27517)
*Zherui Yang,Zhehao Li,Kangbo Lyu,Yixuan Li,Tao Du,Ligang Liu*

Main category: cs.LG

TL;DR: 提出了一种基于GNN的学习方法来构建GPU友好的稀疏近似逆(SPAI)预处理器，避免了传统方法中的三角求解问题，在GPU上实现了40%-53%的求解时间减少。


<details>
  <summary>Details</summary>
Motivation: 传统预处理器依赖预设算法，无法充分利用数据优化；现有基于学习的方法使用GNN但依赖不完全分解，导致三角求解阻碍GPU并行化，且引入难以建模的长距离依赖。

Method: 使用GNN构建SPAI预处理器，避免三角求解，每个CG步骤仅需两次矩阵向量乘积；引入基于统计的尺度不变损失函数，匹配CG收敛特性。

Result: 在三个PDE衍生数据集和一个合成数据集上的评估显示，该方法优于标准预处理器和先前基于学习的预处理器，在GPU上减少40%-53%求解时间，条件数更好且泛化性能优越。

Conclusion: 提出的学习型SPAI预处理器方法有效解决了GPU并行化问题，显著提升了CG求解器的性能，具有更好的泛化能力和实际应用价值。

Abstract: The conjugate gradient solver (CG) is a prevalent method for solving
symmetric and positive definite linear systems Ax=b, where effective
preconditioners are crucial for fast convergence. Traditional preconditioners
rely on prescribed algorithms to offer rigorous theoretical guarantees, while
limiting their ability to exploit optimization from data. Existing
learning-based methods often utilize Graph Neural Networks (GNNs) to improve
the performance and speed up the construction. However, their reliance on
incomplete factorization leads to significant challenges: the associated
triangular solve hinders GPU parallelization in practice, and introduces
long-range dependencies which are difficult for GNNs to model. To address these
issues, we propose a learning-based method to generate GPU-friendly
preconditioners, particularly using GNNs to construct Sparse Approximate
Inverse (SPAI) preconditioners, which avoids triangular solves and requires
only two matrix-vector products at each CG step. The locality of matrix-vector
product is compatible with the local propagation mechanism of GNNs. The
flexibility of GNNs also allows our approach to be applied in a wide range of
scenarios. Furthermore, we introduce a statistics-based scale-invariant loss
function. Its design matches CG's property that the convergence rate depends on
the condition number, rather than the absolute scale of A, leading to improved
performance of the learned preconditioner. Evaluations on three PDE-derived
datasets and one synthetic dataset demonstrate that our method outperforms
standard preconditioners (Diagonal, IC, and traditional SPAI) and previous
learning-based preconditioners on GPUs. We reduce solution time on GPUs by
40%-53% (68%-113% faster), along with better condition numbers and superior
generalization performance. Source code available at
https://github.com/Adversarr/LearningSparsePreconditioner4GPU

</details>


### [70] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: 通用时间序列基础模型在EEG任务中表现优异，即使使用非神经来源数据或合成数据进行预训练，也能有效迁移到脑电信号分析。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在特定生物医学信号（如脑电图EEG）中的应用潜力，目前这方面的研究相对较少。

Method: 测试两种预训练策略：(a) 在多领域真实世界时间序列数据上预训练，(b) 在纯合成数据上预训练，并在脑电任务（运动想象分类和睡眠阶段预测）上评估性能。

Result: 两种预训练变体都表现出色，持续优于广泛使用的卷积基线EEGNet和最新的EEG专用基础模型CBraMod。

Conclusion: 跨领域预训练的通用时间序列基础模型可以有效应用于脑电信号分析，EEG领域可以从更广泛的时间序列文献进展中受益。

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [71] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: 提出了一种结合迁移学习和主动学习的贝叶斯框架，用于结构健康监测中的领域自适应，旨在减少标签数据需求并提高分类模型的数据效率。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测数据获取成本高且困难，特别是带标签数据。传统方法在处理来自不同结构的数据分布差异时泛化能力差，需要解决标签稀缺场景下的模型学习问题。

Method: 采用贝叶斯框架进行领域自适应，结合主动采样策略选择最具信息量的观测进行标注，在无监督领域自适应基础上利用有限的目标域标签数据改进映射。

Result: 在实验桥梁数据集上的评估表明，结合迁移学习和主动学习能够显著提高标签稀缺场景下分类模型学习的数据效率。

Conclusion: 该方法可减少结构运营期间的检查次数，从而降低运营成本，对数据驱动的结构运维具有重要意义。

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [72] [TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control](https://arxiv.org/abs/2510.27527)
*Yuxiang Chen,Xiaoming Xu,Pengle Zhang,Michael Beyer,Martin Rapp,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: TetraJet-v2是一种端到端的4位全量化训练方法，使用NVFP4格式处理所有线性层中的激活、权重和梯度，解决了权重振荡和异常值问题，显著缩小了与全精度训练的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练成本极高，促使对低精度全量化训练的研究兴趣。虽然4位格式如NVFP4能带来显著的效率提升，但在如此低精度下实现近乎无损的训练仍然具有挑战性。

Method: 提出TetraJet-v2方法，包括：1）针对NVFP4线性层的无偏双块量化方法；2）抑制权重振荡的OsciReset算法；3）保持异常值精度的OutControl算法。

Result: TetraJet-v2在预训练LLM中始终优于先前的FP4训练方法，模型规模达370M，数据规模达200B tokens，将性能差距平均缩小了51.3%。

Conclusion: TetraJet-v2成功实现了高效的4位全量化训练，显著降低了与全精度训练的性能差距，为低精度LLM训练提供了可行的解决方案。

Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.

</details>


### [73] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: AstuteRAG-FQA是一个专为金融问答设计的自适应RAG框架，通过任务感知提示工程解决金融领域RAG应用中的挑战，包括数据访问限制、检索准确性、监管约束和敏感数据解释。


<details>
  <summary>Details</summary>
Motivation: 将RAG应用于金融领域面临关键挑战：专有数据集访问受限、检索准确性有限、监管约束和敏感数据解释问题，需要专门解决方案。

Method: 采用混合检索策略整合开源和专有金融数据，使用动态提示框架实时适应查询复杂度，提出四层任务分类，并集成多层安全机制和实时合规监控。

Result: 框架评估了三种数据集成技术（上下文嵌入、小模型增强、目标微调），分析了它们在各种金融环境中的效率和可行性。

Conclusion: AstuteRAG-FQA为金融问答提供了一个安全、合规且高效的自适应RAG解决方案，能够处理多样化的金融查询类型。

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [74] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: ORGEval是一个基于图论的评估框架，用于评估大型语言模型在线性和混合整数线性规划建模中的能力。它通过将优化模型表示为图，将等价性检测转化为图同构测试，解决了现有求解器方法的不一致性、不可行性和高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 工业应用中制定优化问题需要大量人工工作和领域专业知识。虽然大型语言模型在自动化这一过程中显示出潜力，但由于缺乏稳健的评估指标，评估其性能仍然困难。现有的基于求解器的方法经常面临不一致性、不可行性和高计算成本的问题。

Method: ORGEval将优化模型表示为图，将等价性检测简化为图同构测试。作者识别并证明了一个充分条件：当测试的图是对称可分解（SD）时，Weisfeiler-Lehman（WL）测试能够正确检测同构。ORGEval集成了定制化的WL测试变体和SD检测算法来评估模型等价性。

Result: 实验结果表明，该方法能够成功检测模型等价性，并在随机参数配置下产生100%一致的结果，同时在运行时间上显著优于基于求解器的方法，特别是在困难问题上。利用ORGEval构建了Bench4Opt数据集，并基准测试了最先进的大型语言模型在优化建模中的表现。

Conclusion: 尽管优化建模对所有大型语言模型仍然具有挑战性，但DeepSeek-V3和Claude-Opus-4在直接提示下达到了最高准确率，甚至超过了领先的推理模型。ORGEval提供了一个稳健、高效的框架来评估LLMs在优化问题制定中的能力。

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


### [75] [Panprediction: Optimal Predictions for Any Downstream Task and Loss](https://arxiv.org/abs/2510.27638)
*Sivaraman Balakrishnan,Nika Haghtalab,Daniel Hsu,Brian Lee,Eric Zhao*

Main category: cs.LG

TL;DR: 本文提出了panprediction框架，将模型训练视为从数据中提取足够信息，使模型能够最小化下游多个任务的多个损失函数。该框架统一了omniprediction和多组学习，并证明了在温和假设下，同时最小化无限多任务和损失函数的统计复杂度与单一任务单一损失相当。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习关注在固定分布上最小化固定损失函数，而新兴范式认为训练应提取足够信息使模型能处理下游多个任务的多个损失。本文旨在形式化这一范式，研究其统计复杂度。

Method: 设计了确定性panpredictor（样本复杂度~O(1/ε³)）和随机化panpredictor（样本复杂度~O(1/ε²)）的学习算法。关键技术是将panprediction约简到统计高效的步校准概念。

Result: 证明了在温和假设下，同时最小化无限多任务和无限多损失函数的统计复杂度与单一任务单一损失相当。改进了确定性omniprediction的最佳样本复杂度1/ε倍，并匹配了omniprediction和多组学习的其他已知样本复杂度保证。

Conclusion: panprediction框架统一了omniprediction和多组学习，展示了同时处理多任务多损失的可行性，且统计复杂度与单任务单损失相当，为更通用的学习范式提供了理论基础。

Abstract: Supervised learning is classically formulated as training a model to minimize
a fixed loss function over a fixed distribution, or task. However, an emerging
paradigm instead views model training as extracting enough information from
data so that the model can be used to minimize many losses on many downstream
tasks. We formalize a mathematical framework for this paradigm, which we call
panprediction, and study its statistical complexity. Formally, panprediction
generalizes omniprediction and sits upstream from multi-group learning, which
respectively focus on predictions that generalize to many downstream losses or
many downstream tasks, but not both. Concretely, we design algorithms that
learn deterministic and randomized panpredictors with
$\tilde{O}(1/\varepsilon^3)$ and $\tilde{O}(1/\varepsilon^2)$ samples,
respectively. Our results demonstrate that under mild assumptions,
simultaneously minimizing infinitely many losses on infinitely many tasks can
be as statistically easy as minimizing one loss on one task. Along the way, we
improve the best known sample complexity guarantee of deterministic
omniprediction by a factor of $1/\varepsilon$, and match all other known sample
complexity guarantees of omniprediction and multi-group learning. Our key
technical ingredient is a nearly lossless reduction from panprediction to a
statistically efficient notion of calibration, called step calibration.

</details>


### [76] [Imbalanced Classification through the Lens of Spurious Correlations](https://arxiv.org/abs/2510.27650)
*Jakob Hackstein,Sidney Bender*

Main category: cs.LG

TL;DR: 该论文提出了一种基于可解释AI的方法，通过反事实解释来识别和消除类别不平衡导致的Clever Hans效应，从而提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习中的基本挑战，传统方法主要关注数据或损失重加权方案，但作者认为不平衡会通过少数类别的未充分指定而放大Clever Hans效应。

Method: 采用基于反事实解释的可解释AI方法，联合识别和消除在类别不平衡下出现的Clever Hans效应。

Result: 在三个数据集上实现了具有竞争力的分类性能，并展示了类别不平衡下Clever Hans效应的产生机制。

Conclusion: 该方法为类别不平衡问题提供了新的视角，揭示了现有方法普遍忽视的Clever Hans效应问题。

Abstract: Class imbalance poses a fundamental challenge in machine learning, frequently
leading to unreliable classification performance. While prior methods focus on
data- or loss-reweighting schemes, we view imbalance as a data condition that
amplifies Clever Hans (CH) effects by underspecification of minority classes.
In a counterfactual explanations-based approach, we propose to leverage
Explainable AI to jointly identify and eliminate CH effects emerging under
imbalance. Our method achieves competitive classification performance on three
datasets and demonstrates how CH effects emerge under imbalance, a perspective
largely overlooked by existing approaches.

</details>


### [77] [Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition](https://arxiv.org/abs/2510.27651)
*Shuyan Lyu,Zhanzimo Wu,Junliang Du*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈理论的逐层训练方法，使用确定性信息瓶颈和Rényi熵函数，在CIFAR数据集上性能优于现有逐层训练方法，接近SGD性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统端到端训练的生物学不合理性（需要存储权重和反向传播），以及逐层训练在大数据集和复杂架构上的局限性。

Method: 基于信息瓶颈原理，每层与辅助分类器联合训练，学习最小充分的任务相关表示，使用确定性信息瓶颈和矩阵Rényi熵。

Result: 在CIFAR-10、CIFAR-100和交通标志识别任务上验证，性能优于现有逐层训练方法，接近SGD训练效果。

Conclusion: 提出的逐层训练方法具有生物学合理性，减少内存使用，避免梯度问题，在大规模数据集上表现良好。

Abstract: Modern deep neural networks (DNNs) are typically trained with a global
cross-entropy loss in a supervised end-to-end manner: neurons need to store
their outgoing weights; training alternates between a forward pass
(computation) and a top-down backward pass (learning) which is biologically
implausible. Alternatively, greedy layer-wise training eliminates the need for
cross-entropy loss and backpropagation. By avoiding the computation of
intermediate gradients and the storage of intermediate outputs, it reduces
memory usage and helps mitigate issues such as vanishing or exploding
gradients. However, most existing layer-wise training approaches have been
evaluated only on relatively small datasets with simple deep architectures. In
this paper, we first systematically analyze the training dynamics of popular
convolutional neural networks (CNNs) trained by stochastic gradient descent
(SGD) through an information-theoretic lens. Our findings reveal that networks
converge layer-by-layer from bottom to top and that the flow of information
adheres to a Markov information bottleneck principle. Building on these
observations, we propose a novel layer-wise training approach based on the
recently developed deterministic information bottleneck (DIB) and the
matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each
layer is trained jointly with an auxiliary classifier that connects directly to
the output layer, enabling the learning of minimal sufficient task-relevant
representations. We empirically validate the effectiveness of our training
procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further
demonstrate its applicability to a practical task involving traffic sign
recognition. Our approach not only outperforms existing layer-wise training
baselines but also achieves performance comparable to SGD.

</details>


### [78] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: 该论文分析了多智能体强化学习中的开放性与信用分配问题的关系，指出传统信用分配方法在开放系统中存在不足，并通过概念分析和实证研究证明了开放性会导致信用错误归因。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体强化学习的发展，理解开放系统的动态特性变得至关重要。传统信用分配方法假设静态智能体群体、固定任务和稳定类型，无法适应开放环境中智能体进出、任务变化和类型演变的动态特性。

Method: 首先进行概念分析，引入新的开放性子类别来详细说明智能体流动、任务取消等事件如何打破环境平稳性和固定团队组成的假设；然后使用代表性的时间和结构算法在开放环境中进行实证研究。

Result: 实证结果表明，开放性直接导致信用错误归因，表现为损失函数不稳定和性能显著下降。

Conclusion: 开放性是多智能体强化学习中的一个关键挑战，会严重影响信用分配的有效性，需要开发新的方法来应对开放系统中的动态变化。

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [79] [AMD MI300X GPU Performance Analysis](https://arxiv.org/abs/2510.27583)
*Chandrish Ambati,Trung Diep*

Main category: cs.PF

TL;DR: 对AMD MI300X GPU在LLM推理关键性能领域的综合评估


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，需要能够高效服务数百亿参数模型的高性能、可扩展GPU硬件。虽然NVIDIA GPU因成熟的CUDA软件栈和先进加速器而主导LLM部署，但AMD最新的MI300X GPU提供了有竞争力的替代方案，具有高HBM容量、矩阵核心和专有互连技术。

Method: 对AMD MI300X GPU在LLM推理关键性能领域进行综合评估，包括计算吞吐量、内存带宽和互连通信性能。

Result: 论文未提供具体评估结果，但表明对MI300X在LLM推理关键指标上的性能进行了全面分析。

Conclusion: AMD MI300X GPU作为NVIDIA GPU的替代方案，在HBM容量、矩阵核心和互连技术方面具有竞争力，适合大规模LLM部署。

Abstract: The rapid growth of large language models (LLMs) has driven the need for
high-performance, scalable GPU hardware capable of efficiently serving models
with hundreds of billions of parameters. While NVIDIA GPUs have traditionally
dominated LLM deployments due to their mature CUDA software stack and state-of
the-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,
featuring high HBM capacity, matrix cores, and their proprietary interconnect.
In this paper, we present a comprehensive evaluation of the AMD MI300X GPUs
across key performance domains critical to LLM inference including compute
throughput, memory bandwidth, and interconnect communication.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [80] [Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies](https://arxiv.org/abs/2510.26944)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Choreographer是一个用于评估细粒度加速器的仿真框架，能够全面捕捉硬件和软件开销，通过详细的缓存模型提供准确性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法全面捕捉核心-加速器和缓存-加速器交互中的所有硬件和软件开销，需要开发一个能够进行系统级评估的仿真框架。

Method: 开发了基于gem5的硬件栈，包含AMBA CHI网状网络和完整的Linux软件栈，提供C++ API和模块化配置选项，并建立了详细的缓存模型。

Result: 案例研究表明：数据感知预取器在图形分析工作负载中实现1.08x-1.88x加速，快速排序加速器实现超过2x加速且地址转换开销最小。

Conclusion: Choreographer能够有效建模复杂的硬件-软件交互，在小任务卸载场景中优化性能，为细粒度加速器设计提供准确的系统级评估。

Abstract: In this paper, we introduce Choreographer, a simulation framework that
enables a holistic system-level evaluation of fine-grained accelerators
designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer
captures all hardware and software overheads in core-accelerator and
cache-accelerator interactions, integrating a detailed gem5-based hardware
stack featuring an AMBA coherent hub interface (CHI) mesh network and a
complete Linux-based software stack. To facilitate rapid prototyping, it offers
a C++ application programming interface and modular configuration options. Our
detailed cache model provides accurate insights into performance variations
caused by cache configurations, which are not captured by other frameworks. The
framework is demonstrated through two case studies: a data-aware prefetcher for
graph analytics workloads, and a quicksort accelerator. Our evaluation shows
that the prefetcher achieves speedups between 1.08x and 1.88x by reducing
memory access latency, while the quicksort accelerator delivers more than 2x
speedup with minimal address translation overhead. These findings underscore
the ability of Choreographer to model complex hardware-software interactions
and optimize performance in small task offloading scenarios.

</details>


### [81] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: 本文深入分析了FPGA和ASIC中的时序收敛挑战与约束，通过案例研究比较了Xilinx Kintex UltraScale+ FPGA和7nm ASIC的时序性能，结果显示ASIC在时序方面表现更优，但现代FPGA仍具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究FPGA和ASIC的时序收敛挑战，分析两种技术在不同应用场景下的时序性能差异，为高性能设计提供指导。

Method: 采用深度分析方法，研究核心时序原理、架构差异和设计方法学，并通过XCKU040 FPGA与7nm ASIC的案例研究进行实际时序分析和性能权衡比较。

Result: 实验结果显示ASIC实现45ps建立时间和35ps保持时间的优异时序性能，而现代FPGA达到180ps建立时间和120ps保持时间，证明FPGA在高性能设计中仍具有适用性。

Conclusion: ASIC在时序性能方面优于FPGA，但现代FPGA的时序表现已足够满足高性能设计需求，验证了FPGA在高性能应用中的适用性。

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [82] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: 该论文对基于描述符的对象感知内存系统进行了全面调查，这种架构范式旨在弥合硬件/软件接口的语义鸿沟，通过将描述符提升为一级架构抽象，使硬件能够动态获取和执行软件定义对象的丰富语义。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统的安全性和效率因缺乏原生架构机制来传播高级程序语义（如对象身份、边界和生命周期）而受到根本性损害。

Method: 建立内存对象和描述符的基础概念，引入描述符寻址模式的新分类法，提供结构化框架来分析比较不同实现，并通过CentroID模型作为案例研究。

Result: 统一分析揭示了该范式如何全面解决内存保护、管理和处理的相互关联挑战，展示了混合标记指针编码和描述符处理机制如何体现实用高效的对象感知设计路径。

Conclusion: 明确的对象语义跨层通信为下一代缓存层次结构、统一虚拟内存甚至128位架构提供了基础研究方向。

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>


### [83] [A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable Medical LLMs-Agents](https://arxiv.org/abs/2510.27107)
*Zhipeng Liao,Kunming Shao,Jiangnan Yu,Liang Zhao,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Jie Yang,Mohamad Sawan*

Main category: cs.AR

TL;DR: 提出了一种用于边缘RAG的分层检索架构，通过两阶段检索方案结合近似检索和高精度检索，显著降低能耗和内存访问，同时保持检索精度。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署医疗AI代理时，RAG实现会带来大量内存访问和能耗问题，需要解决这些部署挑战。

Method: 采用分层检索架构，结合近似检索生成候选集，然后在预选文档嵌入上进行高精度检索的两阶段方案。

Result: 在TSMC 28nm技术下，相比纯INT8检索，整体内存访问减少近50%，计算量减少75%，1MB数据检索的总能耗为177.76μJ/查询。

Conclusion: 该分层检索架构能有效降低边缘RAG的能耗和内存访问，同时保持检索精度，适用于医疗AI代理部署。

Abstract: With powerful and integrative large language models (LLMs), medical AI agents
have demonstrated unique advantages in providing personalized medical
consultations, continuous health monitoring, and precise treatment plans.
Retrieval-Augmented Generation (RAG) integrates personal medical documents into
LLMs by an external retrievable database to address the costly retraining or
fine-tuning issues in deploying customized agents. While deploying medical
agents in edge devices ensures privacy protection, RAG implementations impose
substantial memory access and energy consumption during the retrieval stage.
This paper presents a hierarchical retrieval architecture for edge RAG,
leveraging a two-stage retrieval scheme that combines approximate retrieval for
candidate set generation, followed by high-precision retrieval on pre-selected
document embeddings. The proposed architecture significantly reduces energy
consumption and external memory access while maintaining retrieval accuracy.
Simulation results show that, under TSMC 28nm technology, the proposed
hierarchical retrieval architecture has reduced the overall memory access by
nearly 50% and the computation by 75% compared to pure INT8 retrieval, and the
total energy consumption for 1 MB data retrieval is 177.76 {\mu}J/query.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [84] [Lorentzian Switching Dynamics in HZO-based FeMEMS Synapses for Neuromorphic Weight Storage](https://arxiv.org/abs/2510.27095)
*Shubham Jadhav,Kaustav Roy,Luis Amaro,Thejas Basavarajappa,Madhav Ramesh,Debdeep Jena,Huili,Xing,Amit Lal*

Main category: cs.ET

TL;DR: 提出了一种基于铁电MEMS的突触器件，通过机械驱动非破坏性读取模拟权重，避免了传统铁电突触的破坏性电读取问题，实现了7位以上的编程精度。


<details>
  <summary>Details</summary>
Motivation: 传统铁电突触使用剩余极化状态存储权重，需要破坏性电读取，这会限制器件的耐久性和可靠性。需要开发非破坏性读取的突触元件。

Method: 使用释放的Hf0.5Zr0.5O2 MEMS单晶片，将模拟权重存储在压电系数d31,eff中。通过部分切换铁电畴来调制d31,eff，使用低振幅机械驱动非破坏性地读取权重。

Result: 实现了超过7位的编程水平，机械切换分布函数遵循洛伦兹分布，与成核限制切换一致。提取的中值阈值服从Merz型场-时间定律，建立了机械权重与电切换动力学之间的定量联系。

Conclusion: 这种机械读取突触避免了去极化和电荷注入效应，提供双极权重，直接显示部分畴群，为高比特神经形态硬件提供了稳健、节能的途径。

Abstract: Neuromorphic computing demands synaptic elements that can store and update
weights with high precision while being read non-destructively. Conventional
ferroelectric synapses store weights in remnant polarization states and might
require destructive electrical readout, limiting endurance and reliability. We
demonstrate a ferroelectric MEMS (FeMEMS) based synapse in which analog weights
are stored in the piezoelectric coefficient $d_{31,eff}$ of a released
Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) MEMS unimorph. Partial switching of
ferroelectric domains modulates $d_{31,eff}$, and a low-amplitude mechanical
drive reads out the weight without read-disturb in the device yielding more
than 7-bit of programming levels. The mechanical switching distribution
function follows a Lorentzian distribution as a logarithmic function of partial
poling voltage ($V_p$) consistent with nucleation-limited switching (NLS), and
the median threshold extracted from electromechanical data obeys a Merz-type
field-time law with a dimensionless exponent $\alpha = 3.62$. These
relationships establish a quantitative link between mechanical weights and
electrical switching kinetics. This mechanically read synapse avoids
depolarization and charge-injection effects, provides bipolar weights (well
suited for excitatory and inhibitory synapses), directly reveals partial domain
populations, and offers a robust, energy-efficient route toward high-bit
neuromorphic hardware.

</details>
