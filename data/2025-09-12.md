<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: HD-MoE是一个针对MoE架构LLM的自动并行计算优化系统，通过离线混合并行映射算法和在线动态调度策略，在NMP加速器上实现高效推理


<details>
  <summary>Details</summary>
Motivation: MoE架构的LLM虽然性能优越且计算成本低，但内存容量和带宽要求高。现有的并行映射策略（TP和EP）存在通信成本高或计算利用率不平衡的问题，MoE的动态路由机制进一步加剧了效率挑战

Method: 提出HD-MoE系统，包含离线自动混合并行映射算法和在线动态调度策略，旨在减少通信成本的同时最大化计算利用率

Result: 实验结果显示，HD-MoE相比TP实现1.1-1.8倍加速，相比EP实现1.1-1.5倍加速，相比基线混合TP-EP实现1.0-1.4倍加速

Conclusion: HD-MoE通过自动优化MoE并行计算，在NMP加速器上显著提升了LLM推理效率，解决了现有并行策略的局限性

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出基于信噪比的直观不确定性估计框架，使用方差门控置信因子缩放预测，讨论委员会机器多样性崩溃问题


<details>
  <summary>Details</summary>
Motivation: 评估神经网络的样本级不确定性量化对高风险应用决策至关重要，但传统的不确定性加性分解方法受到质疑

Method: 基于不同模型预测中类别概率分布的信噪比构建不确定性估计框架，引入方差门控置信因子对预测进行缩放

Result: 提出了新的不确定性分解框架，能够有效量化模型相关和数据相关的不确定性成分

Conclusion: 该框架为神经网络不确定性估计提供了更直观和有效的方法，并揭示了委员会机器多样性崩溃现象

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [3] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种改进的矩阵乘性权重更新算法，在矩阵版本的学习专家建议问题中实现了实例最优的遗憾界，计算复杂度与原算法相同。


<details>
  <summary>Details</summary>
Motivation: 现有的MMWU算法在矩阵LEA问题上只能达到极小极大最优遗憾界，但无法实现实例最优性能。本文旨在开发一个既能保持相同计算复杂度又能获得更好遗憾界的算法。

Method: 开发了一个基于势函数的通用框架，利用拉普拉斯变换技术构建新的单边Jensen迹不等式，并通过虚误差函数导出了最优势函数。

Result: 新算法实现了O(√(T·S(X||d⁻¹I_d)))的实例最优遗憾界，在量子态学习等应用中超越了现有技术，并能预测非线性量子特性。

Conclusion: 该算法在保持计算效率的同时显著改进了遗憾界，为量子学习理论提供了强大的新工具，在多个量子学习任务中表现出优越性能。

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [4] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 该论文提出了一种对抗性腐败环境下鲁棒的Q-learning算法，能够在部分奖励信号被恶意扰动时仍保持有效性能，并提供了有限时间收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，奖励信号可能受到极端噪声、传感器故障或恶意攻击的对抗性腐败，这会严重降低传统算法（如Q-learning）的性能，因此需要开发鲁棒的算法来应对这种挑战。

Method: 提出了一种新的可证明鲁棒的Q-learning变体算法，在异步采样模型和时间相关数据下运行。还提出了一个无需先验了解真实奖励分布统计信息的算法变体，利用了改进的Azuma-Hoeffding不等式技术。

Result: 算法在对抗性腐败下的有限时间收敛率与非对抗情况下的现有结果相匹配，仅增加一个与腐败样本比例成正比的附加项。信息论下界证明了这个附加项是不可避免的。

Conclusion: 这项工作为异步Q-learning提供了首个有限时间鲁棒性保证，填补了鲁棒强化学习领域的重要空白，所提出的技术工具可能具有独立的研究价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [5] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出基于Wasserstein分布鲁棒优化的新框架，解决多源异构数据中群体分布不确定性下的最差群体性能优化问题


<details>
  <summary>Details</summary>
Motivation: 传统方法假设能准确估计各群体数据分布，但在噪声、非平稳和动态环境中这一假设常被违反，导致模型性能下降

Method: 使用Wasserstein分布鲁棒优化(DRO)方法处理群体内的分布不确定性，同时保持优化最差群体性能的目标，开发了梯度下降-上升算法

Result: 在真实世界数据上验证了方法的有效性

Conclusion: 该框架能有效处理群体分布不确定性，提升模型在最差群体上的性能表现

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [6] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出ProDiGy算法，通过联合双评分系统评估客户端梯度，在数据非IID的联邦学习中有效防御拜占庭攻击


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构环境下容易受到对抗攻击，现有防御机制在非IID数据分布下效果不佳

Method: 使用基于梯度接近性和差异性的联合双评分系统来评估客户端梯度，促进诚实客户端的自然相似性，同时检测可疑的一致性作为攻击指标

Result: 在多种场景下优于现有防御方法，特别是在非IID数据分布时，其他防御机制失效时仍能保持强大的防御能力和模型精度

Conclusion: 双视角方法在联邦学习中有效，能够同时促进诚实客户端的自然相似性并检测潜在攻击

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [7] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: FoundationalECGNet是一个用于自动ECG分类的基础框架，通过多技术融合实现99%的F1分数，能区分正常/异常ECG并分类5种心脏疾病。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，现有ECG分析方法面临噪声、类别不平衡和数据集异质性等挑战，需要更准确和可扩展的诊断系统。

Method: 整合Morlet和Daubechies小波变换的双阶段去噪、卷积块注意力模块(CBAM)、图注意力网络(GAT)和时间序列变换器(TST)，共同捕捉多通道ECG信号的空间和时间依赖性。

Result: 在多个数据集上，正常vs异常分类达到99% F1分数，传导障碍和肥大症99% F1分数，心律失常98.9% F1分数，并提供风险等级评估。

Conclusion: FoundationalECGNet是一个可扩展、可解释和通用的自动ECG分析解决方案，有望提高医疗环境中的诊断精度和患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [8] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 该论文通过将LRP型归因方法表示为修正梯度矩阵的乘积，建立了与雅可比矩阵乘法的类比，推导了奇异值上界和归因值的分量界限，并获得了控制经验均值收敛的乘性常数，发现LRP-beta的常数与权重范数无关。


<details>
  <summary>Details</summary>
Motivation: 分析LRP型归因方法的数值特性，特别是研究归因值的分布规律和收敛性质，为多数据增强场景和Smoothgrad型方法提供理论支撑。

Method: 将LRP归因方法表示为修正梯度矩阵的乘积，建立与雅可比矩阵链式法则的类比，推导奇异值上界和分量界限，获得控制经验均值收敛的乘性常数。

Result: 获得了控制归因图期望值收敛的乘性常数，发现LRP-beta方法的常数与权重范数无关，这与基于梯度的方法和LRP-epsilon形成显著区别。

Conclusion: 该分析为多非几何数据增强场景和Smoothgrad型归因方法提供了重要理论依据，LRP-beta的权重范数无关特性使其在实践应用中具有独特优势。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [9] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 该论文提出了一种碳感知的联邦学习调度方法，通过利用碳强度的时间变化和适度延长训练时间来减少碳排放，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习训练产生大量碳排放，联邦学习通过分布式计算可以利用不同地区和时间的碳强度差异来减少碳排放。

Method: 采用碳感知的客户端选择和训练调度策略，包括利用空闲时间延迟训练到低碳时段、α-公平碳分配机制和全局微调阶段。

Result: 在真实碳强度数据上的实验表明，该方法在多种碳预算下都优于不考虑碳强度的基线方法，特别是在严格碳约束下表现更佳。

Conclusion: 碳感知调度能有效减少联邦学习的碳排放，同时保持模型准确性，为可持续机器学习提供了可行方案。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [10] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 提出一个结合主动帕累托前沿学习算法(PyePAL)、可视化技术和可解释AI的框架，用于优化聚合物薄膜旋涂工艺参数，实现硬度和弹性的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜以获得特定机械性能本质上是一个多目标优化问题，需要同时优化硬度、弹性等多个性能指标，传统方法难以高效处理这种复杂的多目标优化挑战。

Method: 使用PyePAL算法结合高斯过程模型预测设计变量(转速、稀释度、聚合物混合物)与目标值(硬度、弹性)的关系，自适应选择样本；采用UMAP进行二维可视化，并引入模糊语言总结将参数与性能关系转化为语言描述。

Result: 实验结果表明该方法能有效识别有前景的聚合物设计，同时视觉和语言解释促进了专家驱动的分析和知识发现。

Conclusion: 该集成框架不仅实现了高效的多目标优化，还通过可视化和语言解释增强了结果的可解释性，为材料设计提供了新的分析工具。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [11] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: ANNA注意力机制在保持Transformer表达能力的同时实现亚二次时间复杂度，能够模拟MPC算法并解决关键推理任务


<details>
  <summary>Details</summary>
Motivation: 标准Transformer虽然具有模拟大规模并行计算(MPC)算法的表示能力，但其二次时间复杂性严重限制了可扩展性，需要开发更高效的注意力机制

Method: 提出近似最近邻注意力(ANNA)机制，具有亚二次时间复杂性，证明ANNA-transformers能够保持标准注意力的表达能力，并能解决Match2和k-hop等关键推理任务

Result: ANNA-transformers能够匹配MPC算法的能力，以接近最优的深度解决关键推理任务，且常数深度的ANNA-transformers可以模拟常数深度的低秩transformers

Conclusion: ANNA提供了一种统一的方法来推理广泛的注意力近似机制，在保持表达能力的同时显著提高了计算效率

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [12] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个密集Transformer模型家族，在多个参数规模(0.13B-1.7B)和token规模(最高1T)上训练，使用8个开源参考数据集，为研究提供基准参考点。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供标准化的训练基准，帮助评估不同训练方法的质量和合理性，促进训练过程的比较和复现。

Method: 在8个开源参考数据集上训练密集Transformer模型，涵盖不同参数和token规模，提供中间检查点、日志和代码。

Result: NemoTron-CC HQ数据集表现最佳，其次是DCLM-baseline和FineWeb-Edu；建立了可比较的基准参考点。

Conclusion: 该工作提供了标准化的研究基线，简化了复现过程，促进了训练方法的比较和未来研究的发展。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [13] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 提出了一种针对表格数据的上下文条件异常检测框架，通过自动识别上下文特征和使用深度自编码器建模条件数据分布，在多个基准数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据通常包含异构上下文（如不同用户），使得全局罕见事件在特定上下文中可能是正常的。依赖单一全局分布会忽略这些上下文差异，降低检测性能

Method: 自动识别上下文特征，使用简单的深度自编码器建模条件数据分布

Result: 在多个表格基准数据集上的广泛实验表明，该方法优于最先进的异常检测方法

Conclusion: 上下文在准确区分异常和正常实例中具有重要作用，提出的上下文条件框架能有效提升表格数据异常检测性能

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [14] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 通过混合专家模型(MoWE)方法，使用视觉Transformer间控网络动态结合现有天气预测模型输出，以更少计算资源获得更优的预测精度


<details>
  <summary>Details</summary>
Motivation: 数据驱动天气模型进展平台化，需要新方法突破现有限制，而非重新建模型

Method: 使用Vision Transformer基础的间控网络，根据预测时长动态学习多个专家模型在每个格点的权重组合

Result: 在2天预测范围内，RMSE比最佳AI天气模型降低10%，显著超过单独模型和简单平均结果

Conclusion: 提供了计算效率高、可扩展的策略，通过最大化利用现有高质量预测模型来推动数据驱动天气预测的发展

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [15] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇范围综述分析了100多篇文献，系统评估了机器学习在电力系统保护和扰动管理中的应用现状、性能表现及适用性，指出了当前研究的局限性并提出了标准化建议。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式能源的集成改变了现代电力系统，挑战了传统保护方案，需要评估机器学习在电力系统保护中的研究现状和应用潜力。

Method: 采用PRISMA范围综述框架，对100多篇文献进行系统分析，评估机器学习在保护任务中的研究范围、性能表现和适用方法，并建立了机器学习导向的分类法。

Result: 机器学习模型在模拟数据集上表现出高精度，但在真实条件下的性能验证不足；现有文献存在方法严谨性、数据集质量和评估指标不一致的问题；缺乏标准化阻碍了结果的可比性和发现的普适性。

Conclusion: 提出了标准化报告实践指南，包括数据集文档、方法透明度和一致评估协议；未来研究应优先考虑公共基准数据集、现实验证方法和先进机器学习架构，以推动机器学习保护从理论承诺走向实际部署。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [16] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE是一个可扩展的XAI框架，通过RKHS中的正交函数分解避免特征子集枚举，提供功能组件而非单一标量归因，在表格数据上实现3倍中值加速并保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 解决现有XAI框架的两个主要限制：特征子集枚举的指数级计算成本和将效应总结为单一标量值的表达能力不足问题。

Method: 基于再生核希尔伯特空间(RKHS)的正交函数分解方法，通过递归核中心化程序的分析投影方案计算功能组件f_S(x_S)，避免显式子集枚举。

Result: 在10个数据集上实现中值约3倍的加速(0.6-9.7倍范围)，保持高保真度(R^2在0.81-0.999之间)，并在大多数数据集上具有显著的排名一致性。

Conclusion: STRIDE通过提供结构化功能视角补充了标量归因方法，支持新颖的诊断功能如'组件手术'来量化特定交互作用的影响。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [17] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 提出Rashomon Ensemble方法，通过策略性地从多个性能相近的高性能模型中选择模型来提升泛化能力，基于性能和解释对模型分组，构建既保持预测准确性又最大化多样性的集成模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习中选择泛化能力好的模型仍然具有挑战性，Rashomon效应指多个模型在给定学习问题上表现相似的情况，这在制造过程、医疗诊断等现实场景中常见，数据中的多样化模式导致存在多个高性能解决方案。

Method: 提出Rashomon Ensemble方法，基于模型性能和解释对模型进行分组，选择覆盖解决方案空间不同区域的模型构建集成，确保最大化多样性同时保持预测准确性。

Result: 在开放和专有的协作现实世界数据集上验证，在Rashomon比率较大的场景中实现了高达0.20+ AUROC的改进，并在各种现实应用中展示了实际业务效益。

Conclusion: 该方法展示了在处理Rashomon效应时的鲁棒性、实用性和有效性，能够构建对分布偏移和未见数据变化更加稳健的集成模型。

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [18] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 该论文研究了深度线性网络的黎曼几何，作为学习过程热力学描述的基础，主要使用群作用分析过参数化，并通过黎曼淹没从参数空间到可观测量空间建立几何联系。


<details>
  <summary>Details</summary>
Motivation: 为深度学习过程提供热力学描述的理论基础，通过黎曼几何工具分析深度线性网络的参数空间结构，特别是过参数化现象。

Method: 使用群作用分析过参数化，通过黎曼淹没将参数空间映射到可观测量空间，利用雅可比矩阵理论构造平衡流形切空间的正交基，并基于群轨道叶状结构定义玻尔兹曼熵。

Result: 成功建立了深度线性网络参数空间的黎曼几何框架，证明了可观测量空间上的黎曼几何可以通过平衡流形的黎曼淹没获得，并给出了玻尔兹曼熵的计算方法。

Conclusion: 该研究为理解深度学习中的几何结构和热力学性质提供了新的理论工具，特别是在处理过参数化网络方面具有重要理论价值。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [19] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 基于敏感性的动态秩分配方法Sensitivity-LoRA，解决了传统LoRA方法在调试大语言模型时的均匀秩分配问题，通过测量权重矩阵的全局和局部敏感性来优化计算效率


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA方法在大语言模型的参数高效微调方面表现突出，但其均匀秩分配方式导致计算效率低下，而现有的秩分配技术又存在计算复杂、不稳定等问题，限制了实际应用

Method: 提出Sensitivity-LoRA方法，利用损失函数的二阶导数（Hessian矩阵）来测量权重矩阵的敏感性，根据全局和局部敏感性动态分配秩，以最小的计算开销实现最优秩分配

Result: 实验结果表明Sensitivity-LoRA在多种任务和测试集上都展现出稳健的有效性、高效率和稳定性

Conclusion: Sensitivity-LoRA为大语言模型的高效微调提供了一种更优雅的解决方案，通过动态秩分配机制显著提升了调试效率和实用性

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [20] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 基于因果关系的深度学习框架，通过MVGC和PCMCI+算法进行因果特征选择，在北极海冰预测中实现了更高的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决传统相关性学习模型无法区分真实因果关系与偏相关关系的问题，提高模型的稳健性、可解释性和通用性

Method: 整合多元格兰杰因果性(MVGC)和PCMCI+算法进行因果特征选择，构建混合神经网络架构，利用43年北极海冰范围(SIE)数据和海洋-大气变量

Result: 统计显示统计因果输入特征能够提高预测准确性和可解释性，减少不必要特征，提高计算效率

Conclusion: 该框架不仅在北极海冰预测中表现优异，还可应用于其他动态高维预测领域，为因果感知预测模型提供了可扩展的理论基础和实践性能

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [21] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 通过物理启发神经网络(PINN)和价值梯度迭代(VGI)模块，提出了一种可扩展到复杂多机器人动态系统的连续时间多机器强化学习框架


<details>
  <summary>Details</summary>
Motivation: 解决现有连续时间RL方法在多机器设置中遇到的两大挑战：汉密尔-雅可比-贝尔曼(HJB)方程的维数灾难问题，以及在多机器环境中准确近似集中式价值函数的困难

Method: 使用PINN来近似HJB基于价值函数，并通过价值梯度迭代(VGI)模块迭代精炼沿轨迹的价值梯度，确保价值学习与价值梯度学习的一致性

Result: 在连续时间版本的多机器粒子环境(MPE)和多机器MuJoCo标准测试中，该方法一贯表现超过现有连续时间RL基准方法，并能扩展到复杂多机器动态系统

Conclusion: 该研究成功将连续时间RL扩展到多机器领域，通过PINN和VGI模块有效解决了HJB方程的维数灾难和价值函数近似问题，为复杂动态系统的高频或不规则时间间隔交互提供了有效解决方案

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [22] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 使用机器学习模型预测ISP间的对等关系，XGBoost模型达到98%准确率，可自动化对等伙伴选择过程


<details>
  <summary>Details</summary>
Motivation: 对等关系能够提供更好的ISP特定优化，但目前的对等过程复杂耗时，需要自动化对等伙伴选择来提高效率

Method: 从PeeringDB、CAIDA等公开数据库收集ISP数据，测试三类机器学习模型（树基础、神经网络、Transformer）在预测对等关系方面的性能

Result: XGBoost树基模型表现最佳，达到98%的预测准确率，并且对时间、空间和数据缺失具有良好的强锐性

Conclusion: 该方法可以帮助ISP完全自动化对等伙伴选择过程，推动互联网生态系统向更高效和优化的方向发展

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [23] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: HISPASpoof是首个大规模西班牙语合成语音检测数据集，包含真实语音和6种零样本TTS系统生成的合成语音，解决了西班牙语在语音取证领域的代表性不足问题。


<details>
  <summary>Details</summary>
Motivation: 西班牙语作为全球6亿多人使用的语言，在语音取证领域代表性不足，而现有的英语和中文检测器无法有效泛化到西班牙语，需要专门的西班牙语数据集来推进可靠的语音取证技术。

Method: 构建HISPASpoof数据集，包含来自公开语料库的六种口音的真实语音，以及使用六种零样本TTS系统生成的合成语音。评估了五种代表性检测方法在西班牙语上的性能。

Result: 研究表明，在英语上训练的检测器无法有效泛化到西班牙语，而在HISPASpoof上训练能显著提升检测性能。同时评估了合成语音溯源（识别生成方法）的性能。

Conclusion: HISPASpoof为推进西班牙语可靠和包容性语音取证提供了关键基准，填补了西班牙语在合成语音检测领域的数据空白。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [24] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种无需训练的适应性token合并框架，通过多目标优化和贝叶斯优化来减少视觉Transformer模型的计算开销和传输资源消耗，同时保持竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型在语义通信系统中展现出强大能力，但其计算需求是在资源受限的6G网络中实际部署的主要障碍。

Method: 使用高斯过程基于贝叶斯优化来构建希尔顿最优配置式的希尔顿前沿，将每层合并比例选择形式化为多目标优化问题。

Result: 实验结果显示该方法在广泛的信器器比条件下都能较其他基线方法更优，实现了浮点运算的显著减少，同时保持竞争力的准确性。

Conclusion: 这些发现为在未来边缘智能系统中部署基于Transformer的语义通信提供了一种可扩展和高效的方法。

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [25] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 量子启发的神经网络与深度强化学习结合，在USD/TWD交易中实现了11.87%的5年收益率，最大回撤仅0.92%，优于多个货币ETF


<details>
  <summary>Details</summary>
Motivation: 量子计算与深度强化学习的融合为金融交易提供了新的可能性，特别是在外汇交易这种需要快速决策和风险控制的领域

Method: 采用量子长短期记忆网络(QLSTM)进行短期趋势预测，结合量子异步优势行动者-评论家算法(QA3C)，使用2000-2025年数据进行训练(80%训练集，20%测试集)

Result: 多头策略在约5年内获得11.87%的回报率，最大回撤仅为0.92%，表现优于多个货币ETF，展示了混合模型在外汇交易中的竞争力

Conclusion: QLSTM在小利润交易和严格风险控制方面表现有效，未来可进一步优化。主要限制是经典量子模拟和简化策略，需要进一步研究真实量子硬件应用

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [26] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过重要性采样权重空间的长度公平裁剪来解决PPO/GRPO方法在序列长度上的不公平问题，提出基于高斯理论的裁剪策略，在多个数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法（如PPO/GRPO）在序列裁剪时存在长度不公平问题：固定裁剪范围会系统性地对短响应和长响应进行不同权重重分配，扭曲有效目标。

Method: 提出FSPO方法，在重要性采样权重空间实施长度公平裁剪。引入高斯理论启发的解决方案：用KL校正漂移项和√L缩放因子对序列log-IS比率进行带裁剪。

Result: FSPO能够平坦化不同长度区间的裁剪率，稳定训练过程，在多个评估数据集上均优于所有基线方法。

Conclusion: FSPO通过理论驱动的长度公平裁剪机制，有效解决了序列级RL中的长度偏差问题，为LLM的序列级优化提供了更公平有效的解决方案。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [27] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 深度学习天气预测模型的当前评估指标存在"统计相似性陷阱"，奖励模糊预测而漏掉稀有高影响事件。DART框架通过双解码器结构和物理驱动方法，专门优化极端对流检测，显著提升了预测精度和操作灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测评估指标存在显著缺陷，导致模型过度关注统计相似性而忽视重要的极端天气事件。需要一种专门优化极端对流检测的方法来提高对高影响天气的预测能力。

Method: 提出DART框架，采用双解码器结构，进行明确的背景/极端分解，结合物理驱动的过采样技术和任务特定损失函数。还发现移除集成水气运输(IVT)可显著提升极端对流检测性能。

Result: 实验验证了统计相似性陷阱的存在（精细基准线达到97.9%相关性但极端检测CSI为0.00）。DART实现了CSI=0.273（基准模型为6.72），极端对流检测提升270%，训练时间仅需10分钟。通过2023年8月吉大港洪涛案例进行了实际验证。

Conclusion: DART框架成功解决了深度学习天气模型的评估偏差问题，为极端天气预测提供了一种高效、准确的解决方案。该方法具有较强的操作灵活性和实用性，为可信任AI在极端天气预防中的应用探索了新路径。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [28] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚机制解决约束强化学习在约束边界附近的不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 在连续控制设置中，约束强化学习需要在奖励最大化和约束满足之间取得平衡，但现有方法在约束边界附近表现不稳定，导致训练性能不佳

Method: 引入自适应激励机制，在接近约束边界前保持约束范围内；提出IP3O算法，通过渐进增加的惩罚来稳定训练动态

Result: 在基准环境上的实证评估显示，IP3O相比最先进的安全RL算法表现出更好的性能

Conclusion: IP3O算法有效解决了约束强化学习的稳定性问题，同时提供了理论保证，推导了算法最优性的最坏情况误差界限

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [29] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 本研究通过机器学习方法识别了农业旅游发展的关键指标，使用LASSO算法结合多种分类器获得了超过98%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为促进农村发展的重要经济模式，需要系统研究其发展策略和关键因素，以支持农民收入多样化和传统文化保护。

Method: 研究分为两个阶段：通过文献综述识别关键指标，然后使用LASSO特征选择方法结合Logistic回归、决策树、随机森林、XGBoost等机器学习分类器进行分析。

Result: 在70-30%训练-测试数据下，Logistic回归模型达到最高98%的分类准确率，随机森林为95%。在80-20%数据分配下，Logistic回归续继99%准确率，决策树和XGBoost为97%。

Conclusion: 研究证明了机器学习方法在识别农业旅游发展关键因素中的有效性，特别是LASSO算法结合Logistic回归模型能够提供极高的预测准确性，为农业旅游发展策略提供了量化支撑。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [30] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde框架结合数据抽象、图神经网络和强化学习，为具有结构化状态的决策问题生成归纳策略函数，在未见过的测试实例上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决具有丰富结构化状态（如对象类和关系）的决策问题，需要能够处理不同规模和结构问题的归纳策略函数。

Method: 使用数据抽象将MDP状态表示为实体事实数据库，转换为二分图并通过神经消息传递映射到潜在状态，结合监督学习和强化学习训练策略。

Result: 在8个RDDL问题域的测试中，Vejde策略在未见实例上平均泛化良好，得分接近实例特定的MLP代理。

Conclusion: Vejde框架能够有效生成可泛化的归纳策略，在结构化决策问题上表现优异，接近专门训练的实例特定代理的性能。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [31] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过知识蒸馏将LLM的领域知识和推理能力转移到传统序列模型中，在保持快速推理的同时提升问答模拟器的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有问答模拟器中LLM-free方法性能不佳而LLM-based方法推理速度慢、资源消耗高的问题，寻求性能与效率的平衡

Method: 采用知识蒸馏技术，从大型语言模型(LLM)中提取领域知识和推理能力，用于辅助传统序列模型的预测，提升模拟性能

Result: 在问答模拟和知识追踪任务上都取得了优异的结果，实现了性能提升的同时保持了较快的推理速度

Conclusion: LDSim方法成功地将LLM的优势通过知识蒸馏转移到轻量级模型中，为教育推荐系统提供了高效且准确的问答模拟解决方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [32] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出MMT-FD方法解决旋转机械故障诊断中标注数据稀缺和模型泛化能力不足的问题，通过多注意力元变换器和对比学习实现少样本无监督诊断


<details>
  <summary>Details</summary>
Motivation: 旋转机械故障诊断需要大量标注数据，但实际工业应用中数据获取困难且成本高，不同设备需要单独训练模型，缺乏通用性

Method: 集成时频域编码器和元学习泛化模型，使用时频域随机增强生成状态表示，通过元学习网络进行分类和泛化训练，最后用少量标注数据微调

Result: 在轴承故障数据集和转子试验台数据上验证，仅用1%标注样本数据就达到99%的故障诊断准确率，展现强大泛化能力

Conclusion: MMT-FD框架能有效解决实际工程中故障样本有限和预测模型泛化能力不足的问题，适用于多种机械设备的故障诊断

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [33] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 提出了熵调制策略梯度（EMPG）框架，解决LLM智能体在长时任务中因稀疏奖励导致的信用分配问题，通过基于不确定性和任务结果重新校准学习信号，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 长时任务中基于大语言模型的智能体面临稀疏奖励难以分配信用的问题，传统方法主要关注创建密集奖励信号，但LLM学习动态中存在策略梯度幅度与熵耦合的根本问题。

Method: 提出EMPG框架，基于步骤不确定性和最终任务结果重新校准学习信号：放大自信正确动作的更新，惩罚自信错误，衰减不确定步骤的更新以稳定探索，并引入未来清晰度奖励项。

Result: 在WebShop、ALFWorld和DeepSearch三个挑战性智能体任务上的综合实验表明，EMPG实现了显著的性能提升，明显优于强策略梯度基线方法。

Conclusion: EMPG通过解耦策略梯度幅度与熵的关系，有效解决了长时任务中的信用分配问题，为LLM智能体的强化学习提供了更稳定和高效的训练框架。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [34] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 通过数据驱动的DRSALife模型学习软人工生命规则集，无需物理先验知识即可识别反应-正分布系统的出现动力学，准确率达74%，并具有强壁面性。


<details>
  <summary>Details</summary>
Motivation: 解决在没有物理先验知识的情况下，从观测数据中反正应-正分布系统的系统识别问题。这些系统在神经科学、生态学、传染病学等领域广泛存在，具有重要研究价值。

Method: 提出DRSALife概念框架，通过学习软人工生命模型（如组员基模型和细胞自动机）来表征反应-正分布系统的出现动力学。该方法在Elementary CA Rule 30、生命游戏和Vicsek群集问题中已经验证。

Result: 学习到的模型能够准确预测出现动力学（准确率74%），并在高斯噪声和时间稀疏性条件下依然表现稳定。成功识别了基础偏微分方程的结构和参数。

Conclusion: DRSALife框架为无先验知识的反应-正分布系统识别提供了有效方法，展示了在噪声和稀疏数据条件下的健壁性，为软人工生命规则集学习领域做出了重要贡献。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [35] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: MoSE框架通过匿名游走提取子图，动态路由到专家模型，提升GNN的结构表达能力，在多种图任务中表现优异且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖局部消息传递，难以捕捉复杂高阶子图模式，现有方法主要针对图级任务且配置固定，限制了灵活性和应用范围。

Method: 提出MoSE框架：使用匿名游走提取信息子图，基于结构语义动态路由到专门专家模型，理论证明在SWL测试中表达能力更强。

Result: 大量实验表明MoSE优于现有基线方法，可视化显示模型能够学习到可解释的结构模式。

Conclusion: MoSE提供了灵活且表达力强的子图表示学习框架，在多种图任务中表现优异，同时具有良好的可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [36] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出了一种基于多项式核的HGR相关系数计算方法，相比现有方法具有更好的鲁棒性和确定性，适用于实际机器学习应用


<details>
  <summary>Details</summary>
Motivation: HGR相关系数能够捕捉非线性相关性，在算法公平性、科学分析和因果发现中有重要应用，但现有计算方法存在偏差-方差权衡问题，影响实际应用的稳健性

Method: 采用用户可配置的多项式核来计算HGR相关系数，提供更快的计算速度和几乎同等有效的约束条件

Result: 新方法在鲁棒性和确定性方面具有显著优势，实验验证了其在约束机器学习框架中的适用性，计算得到的次梯度可作为损失正则化器

Conclusion: 提出的多项式核方法为HGR相关系数计算提供了更可靠的选择，特别适合现实世界应用场景

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [37] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习中模型和超参数选择需要大量专业知识和计算资源，现有LLM方法依赖试错和昂贵API，缺乏可解释性和泛化性。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理，采用LLM-as-judge评估控制输出格式、准确性和完整性。

Result: 在8个医学影像数据集上，MetaLLMiX性能优于传统HPO方法，计算成本大幅降低，本地部署在5/8任务上达到最优结果，响应时间减少99.6-99.9%，训练速度提升2.4-15.7倍。

Conclusion: MetaLLMiX提供了一种高效、可解释的超参数优化解决方案，在保持精度的同时显著降低了计算成本，优于现有的API驱动方法。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [38] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的反事实解释虽然有效但不够简洁，在追求简洁时又容易失效，存在有效性-简洁性权衡问题，表明自生成反事实解释作为可解释性工具效果有限且可能产生误导


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自生成反事实解释来向人类解释其决策过程，以促进人机有效协作

Method: 评估LLMs生成的反事实解释的有效性和简洁性，分析其在多种模型、数据集和评估设置下的表现

Result: LLMs生成的反事实解释通常有效但不够简洁，追求简洁时又容易失效，存在明显的有效性-简洁性权衡问题

Conclusion: 自生成反事实解释作为可解释性工具效果有限且可能产生误导，在关键应用场景部署LLMs时需要考虑不可靠自解释对下游决策的影响

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [39] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习与地统计学的混合框架KpR，通过空间滞后特征增强ML模型，在土壤属性预测中显著提升精度和不确定性估计


<details>
  <summary>Details</summary>
Motivation: 机器学习和地统计学在土壤属性预测中各具优势但方法不同，需要结合两者优势来提升数字土壤制图的预测性能

Method: 提出kriging prior regression (KpR)方法，通过普通克里金法构建空间滞后特征来丰富机器学习模型的空间上下文信息，使用TabPFN模型在六个田间尺度数据集上进行评估

Result: KpR与TabPFN组合相比其他空间技术和非空间机器学习算法（如随机森林）具有更可靠的uncertainty估计和更准确的预测，平均R2提升约30%

Conclusion: KpR与TabPFN是一个强大且通用的数字土壤制图建模框架，特别适用于样本量小的精准农业场景，能够补偿近端土壤传感数据有限时的弱关系问题

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [40] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 本研究提出了fuser算法和SAC验证框架，用于分析微生物群落在不同环境下的动态关联网络，解决了传统方法在跨环境预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统微生物共现网络推断算法通常只分析单一环境下的静态关联，无法捕捉微生物群落在不同生态条件下的动态适应过程，需要开发能够处理跨环境预测的新方法。

Method: 提出了Same-All Cross-validation (SAC)验证框架，并开发了fuser算法。该算法在训练时保留子样本特异性信号，同时跨环境共享相关信息，生成环境特异性的预测网络。

Result: fuser算法在同质环境(Same)下与glmnet等现有算法性能相当，在跨环境(All)场景中显著降低了测试误差。

Conclusion: fuser算法能够有效捕捉微生物群落在不同环境下的动态关联模式，为微生物生态学研究提供了更准确的跨环境预测工具。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [41] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD是首个通过concrete scores将score matching扩展到离散图的可控分子图生成模型，通过Composable Guidance和Probability Calibration技术，在四个分子数据集上实现了15.3%的平均可控性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散模型在多条件设置下效果有限，主要依赖于联合条件或连续松弛方法，这会损害生成保真度。需要开发能够灵活控制多个属性约束的分子生成方法。

Method: 提出CSGD模型，通过concrete scores将score matching扩展到离散图；引入Composable Guidance实现细粒度条件控制；使用Probability Calibration调整转移概率来缓解训练-测试不匹配问题。

Result: 在四个分子数据集上，CSGD实现了最先进的性能，平均可控性比先前方法提高15.3%，同时保持高有效性和分布保真度。

Conclusion: 基于score的建模方法在离散图生成中具有实际优势，能够实现灵活的多属性分子设计，为材料和药物发现提供了有效的可控分子生成解决方案。

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [42] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: AquaCast是一个多输入多输出的深度学习模型，用于城市水动力预测，通过融合内源变量和外源因素，在真实和合成数据集上都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统城市水动力预测方法无法同时捕捉变量间和时间依赖关系的问题，特别是如何有效利用外源因素（如降水历史和预报）来提升预测精度。

Method: 开发AquaCast模型，使用嵌入层融合外源输入，无需预测外源变量，专注于内源变量的预测。模型能够同时处理变量间和时间上的依赖关系。

Result: 在LausanneCity数据集上仅使用内源变量就达到state-of-the-art性能，加入外源变量后性能进一步提升。在三个大规模合成数据集上的测试也证实了模型的泛化能力和可扩展性。

Conclusion: AquaCast模型能够有效融合内外源信息，在城市水动力预测任务中表现出色，具有良好的泛化能力和鲁棒性，为城市水资源管理提供了可靠的技术支持。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [43] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 一种全自动化AI系统，使用专门设计的Agent-E代理来识别特定地理区域的论文，并通过RPA完成预定操作，在5个会议768篇论文中实现了100%的可用性和99.4%的准确性


<details>
  <summary>Details</summary>
Motivation: 应对学术文献快速增长带来的挑战，减少研究人员、资助机构和学术社会在学术发现方面的手动劳动强度

Method: 开发了一个从数据发现到直接行动的全自动化系统，使用专门设计的AI代理'Agent-E'识别特定地理区域的论文，然后通过机器人过程自动化(RPA)完成预定操作

Result: 在5个不同会议的586篇论文上验证，系统成功识别了所有目标论文，可用性达到100%，准确性达到99.4%

Conclusion: 这个系统展示了任务导向型AI代理的潜力，不仅能过滤信息，还能积极参与并加速学术社区的工作流程

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [44] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 通过学习四种简单类型的时间规则，该方法在时间知识图预测任务中达到了与八个最先进模型相当或更优的性能，同时提供完全可解释的预测结果。


<details>
  <summary>Details</summary>
Motivation: 受最近工作中使用重复事实的强基线的启发，开发一种完全可解释的旹间知识图预测方法。

Method: 学习四种简单类型的时间规则，使用考虑到最近性和频率的信心函数。

Result: 在9个数据集上评估，方法的性能匹配或超越了8个最先进模型和2个基线模型。

Conclusion: 该方法不仅在时间知识图预测任务中表现优异，而且能够提供完全可解释的预测结果，展示了规则基方法在该领域的潜力。

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [45] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出了D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保护隐私的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能，且随着模型参数指数增长，随机优化器的学习效率面临挑战

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影SGD，动态调整效用与隐私的权衡

Result: 在不同目标函数上表现出可证明的次线性收敛率，实验显示在保持隐私的同时显著提高准确性

Conclusion: DDP以隐私为代价带来更好效用，随机投影实现更高效的模型学习，D2P2-SGD在隐私保护与性能间取得更好平衡

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [46] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的实验集合，包含9,408个管道在300个数据集上的完整实验结果


<details>
  <summary>Details</summary>
Motivation: 解决机器学习算法选择问题中评估算法性能的高计算成本，以及OpenML等在线存储库中实验记录的局限性（缺乏管道多样性、预处理步骤代表性不足、样本不平衡）

Method: 创建PIPES实验集合，设计代表所有技术组合的多样化管道，在300个数据集上执行9,408个管道的实验，记录详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息

Result: 建立了包含详细实验结果的综合性数据库，支持研究人员进行多样化和代表性管道的分析，提供了扩展潜力

Conclusion: PIPES通过提供全面且多样化的实验集合，克服了现有数据源的局限性，为元学习社区提供了有价值的资源，并支持进一步扩展

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [47] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 本文研究使用少样本学习技术在呼吸声音分类中的效果，采用原型网络和谱图表征进行COVID-19、流感和健康状态的咳喷检测，在每类仅15个支持示例下达到74.87%的多分类准确率，证明少样本学习在医疗诊断中的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗诊断中标签数据有限的挑战，探索少样本学习是否能够在使用较少训练样本的情况下达到传统深度学习的性能水平，并比较多分类与二元分类模型的表现差异。

Method: 采用原型网络(Prototypical Networks)算法，使用咳喷声音的谱图表征进行特征提取和分类，在少样本设置下进行多分类和二元分类对比实验。

Result: 少样本学习模型可达到竞争力准确率：多分类分类在每类15个支持示例下获得74.87%准确率，二元分类在所有类别对中均超70%以上准确率。统计测验显示多分类与二元分类模型性能无显著差异。

Conclusion: 少样本学习在呼吸声音分类中具有强大潜力，特别是在标签数据有限的医疗诊断场景中。多分类分类模型与二元分类模型表现相似，支持在这种设置下使用多分类分类的可行性。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [48] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的无监督图对齐框架，通过双通道编码器和几何一致性映射来同时提升节点特异性和实现跨图潜在空间对齐，在多个数据集上都显示出更优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督图对齐方法的两个关键问题：GNN嵌入中因过度平滑导致的节点特异性适适，以及结构噪声、特征异质性和训练不稳定导致的潜在空间错位问题。

Method: 使用双通道编码器结合低通和高通谱滤波器生成既具备结构感知又高医别度的嵌入，并通过几何感知功能映射模块学习双射和等距变换来保证跨图嵌入空间的几何一致性。

Result: 在图谱标准数据集上一贵超过现有无监督对齐基线方法，显示出对结构不一致性和具有挑战性的对齐场景的优异稳健性。在视觉-语言数据集上也能有效应用。

Conclusion: 该框架通过同时提升节点特异性和维护潜在空间几何一致性，有效解决了图对齐问题，并具有良好的扩展性。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [49] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出了一种针对随机和混沌时空系统的深度学习模拟器，能够根据PDE参数值进行条件化模拟，通过预训练和微调实现参数泛化，并支持不同域大小和分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分方法计算成本高，难以高效探索参数空间。需要开发能够快速模拟随机和混沌系统、支持参数泛化并提供不确定性量化的计算工具。

Method: 采用预训练+微调策略，在单一参数域预训练后，使用多样化小数据集微调以实现参数泛化。引入局部注意力机制处理不同域大小和分辨率，支持概率变体进行不确定性量化。

Result: 在混沌Kuramoto-Sivashinsky方程和随机强迫beta-plane湍流上验证，模型能够捕捉插值参数下的现象，相比传统数值积分获得显著计算加速。

Conclusion: 该模拟器为参数空间探索提供了高效工具，概率变体支持不确定性量化和罕见事件统计研究，在计算流体动力学等领域具有重要应用价值。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [50] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO是一种新颖的数据稀疏算子学习算法，通过贪婪算法自适应构建网络结构，实现严格的离散化不变性，在泛化性能上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决多输入PDE组的算子学习问题，旨在消除泛化差距并实现严格的离散化不变性

Method: 结合降基方法和生成预训练物理信息神经网络，使用贪婪算法离线构建网络结构，通过任务特定激活函数进行知识蒸馏

Result: ReBaNO在分布内和分布外测试中显著优于PCA-Net、DeepONet、FNO和CNO等先进算法，是唯一实现严格离散化不变性的算子学习算法

Conclusion: ReBaNO通过数学严谨的贪婪算法构建紧凑网络架构，在最小计算成本下嵌入物理约束，为数据稀疏环境下的PDE求解提供了高效解决方案

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [51] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 提出基于群体反事实解释的时间演化分析方法来解释概念漂移，通过三层框架（数据层、模型层、解释层）全面诊断漂移原因


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在动态环境中面临概念漂移问题，虽然漂移检测已有研究，但解释模型决策逻辑如何变化仍是一个挑战

Method: 分析群体反事实解释(GCEs)的时间演化，追踪漂移前后GCEs聚类质心和反事实动作向量的变化，构建三层诊断框架

Result: 该方法能够揭示模型决策边界和底层逻辑的结构性变化，区分不同漂移根源（如空间数据漂移vs概念重标记）

Conclusion: 通过群体反事实解释的时间演化分析，提供了更全面的概念漂移诊断方法，能够解释模型决策逻辑的变化原因

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [52] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出了基于功能基团的分子表示框架FGR，结合传统化学功能基团和从大数据挖掘的新功能基团，实现了高性能且可解释的分子性质预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分子性质预测模型缺乏可解释性，阻碍了化学家的采用。需要开发既保持高性能又具有化学可解释性的分子表示方法。

Method: 使用功能基团概念构建分子表示框架FGR，整合传统化学功能基团(FG)和通过序列模式挖掘从大数据中发现的新功能基团(MFG)，并在大规模未标记分子数据集上进行预训练。

Result: 在33个涵盖物理化学、生物物理、量子力学、生物活性和药代动力学的基准数据集上达到最先进性能，同时保持化学可解释性。

Conclusion: FGR框架是开发高性能、化学可解释深度学习模型的重要进展，使化学家能够直接将预测性质与特定功能基团联系起来，促进对结构-性质关系的新见解。

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [53] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准各群体安全阈值来减少伤害并在受保护子群体间均衡公平性目标（覆盖率或伤害）。


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习中的安全性和公平性问题，特别是在医疗保健等敏感领域，需要一种能够在减少伤害的同时确保跨受保护子群体公平性的方法。

Method: 使用可行性引导的公平自适应强化学习（FG-FARL），校准每个群体的安全阈值，并与行为克隆（BC）和HACO（混合自适应符合离线RL）基线进行比较。基于医疗补助人群健康管理项目的去标识化纵向轨迹数据进行评估。

Result: FG-FARL实现了与基线相当的价值，同时改善了公平性指标，通过自助法95%置信区间的离策略价值估计和子群体差异分析（带p值）验证了效果。

Conclusion: FG-FARL提供了一条实现更安全和更公平决策支持的实际路径，在保持性能的同时显著提升了公平性。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [54] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant是一种新的2位量化方法，使用可学习的蝴蝶变换替代固定Hadamard变换，通过层自适应旋转有效抑制激活值异常值，在LLaMA-2-7B上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有旋转方法使用固定变换无法适应不同Transformer层的特定异常值模式，需要层自适应的旋转方法。

Method: 提出ButterflyQuant，使用可学习的蝴蝶变换（参数化为连续Givens旋转角），保证正交性同时支持梯度学习，并引入均匀性正则化。

Result: 在LLaMA-2-7B的2位量化中，困惑度从QuaRot的22.1降至15.4，仅需128个校准样本和单GPU几分钟训练。

Conclusion: ButterflyQuant通过可学习的层自适应旋转有效解决了极端量化中的异常值问题，显著提升性能且计算高效。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 本文通过理论计算和实验测试对比分布式标识符方案，证明ULIDs在网络开销、生成速度和冲突风险方面显著优于UUIDv4和UUIDv7，是高性能分布式系统的最佳选择


<details>
  <summary>Details</summary>
Motivation: 分布式系统需要突破传统自增键的限制，找到能够确保数据唯一性和高效索引的标识符生成方案

Method: 结合数学冲突概率计算与实验室测量，在模拟分布式环境中测试生成速度和网络传输开销，对比分析传统自增键、UUIDv4、UUIDv7和ULIDs

Result: ULIDs网络开销减少83.7%，生成速度提高97.32%，冲突风险比UUIDv7低98.42%，高生成速率下仍保持可忽略的冲突概率

Conclusion: ULIDs是高性能分布式系统的最佳选择，提供高效、时间序列化和字典排序的标识符，适用于可扩展应用

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [56] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 通过机器学习预测变异检测流水线执行时间，采用灵活作业店调度算法生成最优执行计划，在GPU环境中实现2倍速度提升


<details>
  <summary>Details</summary>
Motivation: 人类基因组变异检测需要大量计算资源，云环境下需要最大化执行效率以优化成本

Method: 使用ML模型预测变异检测流水线各阶段执行时间，基于基因序列特征，然后采用灵活作业店调度算法生成最优执行计划

Result: 在公开基因组数据集上验证，ML预测准确度高，相比贪心算法获得2倍速度提升，相比动态调度获得1.6倍速度提升

Conclusion: 结合ML预测和优化调度的方法能够有效提升基因组变异检测在云环境中的执行效率

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [57] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个缓存一致性感知的任务图建模框架，通过解耦一致性影响、量化其权重并推断任务间依赖关系，为动态工作负载构建统一的任务图，解决了现有方法忽略缓存一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 随着多核系统扩展，缓存一致性成为系统性能关键因素，但现有任务图建模方法要么依赖预定义图，要么忽略一致性交互，导致设计假设与实际运行时行为存在差距。

Method: CoTAM框架通过分析一致性影响（将其从整体执行中解耦）、通过学习权重方案量化其影响，并推断任务间依赖关系来生成一致性感知的任务图。

Result: 大量实验表明CoTAM优于隐式方法，弥合了动态工作负载行为与现有设计之间的差距。

Conclusion: 将缓存一致性纳入任务图建模对于准确和可推广的系统级分析至关重要，CoTAM框架为此提供了有效解决方案。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [58] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 本文比较了WebAssembly和unikernel基于MicroVM在边缘计算无服务器环境中的性能，结果显示WebAssembly在轻量函数上冷启动更快，但复杂工作负载性能较差，而Firecracker提供更稳定的性能特利是I/O密集型任务。


<details>
  <summary>Details</summary>
Motivation: 边缘无服务器计算需要轻量执行环境来最小化冷启动延迟，特利是在紧急边缘计算(UEC)场景下。本文的动机是对比两种主流的轻量化技术方案的性能特性。

Method: 研究介绍了Limes，一个基于Wasmtime构建的WebAssembly运行时，并将其与基于Firecracker的SPARE环境进行性能对比测试。测试包括了不同类型的无服务器工作负载。

Result: 结果显示WebAssembly在轻量函数上拥有更低的冷启动时间，但在复杂工作负载下性能不佳。Firecracker提供了更高但更稳定的冷启动时间，并在执行性能方面表现更好，特利是在I/O密集型任务中。

Conclusion: 边缘无服务器计算的技术选择应基于具体工作负载特性。WebAssembly适用于轻量函数的快速启动场景，而Firecracker更适合需要稳定性和高性能的复杂工作负载，特利是I/O密集型任务。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [59] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 基于重心有理插值的近似编码分布式计算方案，解决现有CDC方案的固定阈值限制和数值不稳定问题，实现任意结果数量的解码能力和更高的近似精度。


<details>
  <summary>Details</summary>
Motivation: 现有编码分布式计算(CDC)方案存在两个关键问题：固定的恢复阈值限制了灵活性，以及编码/解码函数的极点导致解码不准确和数值不稳定。

Method: 提出一种基于重心有理插值(BRI)的近似CDC方案，该方案能够利用任意工作节点返回的结果进行解码，支持有限域和实数域计算，并保证数值稳定性。

Result: 实验结果显示，该方案在等待时间和近似精度方面都优于现有CDC方案，同时提供了灵活的精度调节能力。

Conclusion: 重心有理插值基于的CDC方案有效解决了现有方案的限制，提供了更灵活、稳定且精确的分布式计算方案，在协作移动边缘计算中具有重要价值。

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [60] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 在不对称信任分布式系统中，本文提出了一种新方法来定义不对称问题，并在更弱假设下实现了可靠广播和共识算法。


<details>
  <summary>Details</summary>
Motivation: 现有不对称信任模型中的解决方案需要过于严格的假设，这会失去不对称信任的优势，因此需要强化假设来解决基础问题。

Method: 提出了一种新的方法来定义不对称问题，并基于此设计了可靠广播和共识算法，这些算法需要比之前更弱的假设条件。

Result: 新算法在更弱的假设下实现了可靠广播和共识，保持了不对称信任的优势，而且方法具有普遍性，可扩展到其他核心问题。

Conclusion: 该研究为不对称信任分布式系统提供了更实用的解决方案，通过减弱假设条件来保持系统的灵活性和可用性。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [61] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv是一个专为LLM代理优化的无服务器平台，通过可重用的沙盒和内存模板等技术，显著降低了启动延迟和内存使用，在容器和VM环境中都比现有系统表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有无服务器计算基础设施对LLM代理等新兴工作负载存在瓶颈，运行成本可能高达LLM API调用成本的70%，需要更高效的高密度无服务器平台。

Method: 设计了支持容器和VM环境的无服务器平台TrEnv，采用可重用的沙盒、内存模板、浏览器共享和页面缓存绕过机制来优化执行环境的重用和恢复。

Result: 在容器环境中P99延迟降低7倍，内存使用减少48%；在VM环境中P99延迟降低58%，内存节省61%，相比E2B等先进系统有显著提升。

Conclusion: TrEnv通过协同设计有效解决了LLM代理在无服务器平台上的性能瓶颈，为高密度、低延迟的代理部署提供了可行解决方案。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [62] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: 本文概述了Wallace树乘法器的设计、进展和方法论，包括在gpdk45技术上使用Cadence Virtuoso设计的8位Wallace树乘法器原理图和版图，以及最终实现的16位组合乘法累加(MAC)单元。


<details>
  <summary>Details</summary>
Motivation: 设计并行数字乘法器架构，通过全加器和半加器电路在每级减少尽可能多的部分积，实现相对于输入大小的最坏情况时间复杂度最小化(O(log(n)))。

Method: 使用Cadence Virtuoso在gpdk45技术上设计Wallace树8位乘法器的原理图和版图，并实现16位组合乘法累加(MAC)单元。

Result: 成功设计了Wallace树8位乘法器，实现了O(log(n))的时间复杂度优化，并完成了16位MAC单元的设计。

Conclusion: Wallace树乘法器架构能够有效减少电路深度的时间复杂度，项目成功实现了8位乘法器和16位MAC单元的设计，验证了该架构在数字乘法中的高效性。

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [63] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA是一个硬件软件协同设计的系统，针对长上下文LLM推理中的内存带宽和容量瓶颈问题，通过非对称量化方案、扁平化脉动阵列架构和完整软件栈优化，实现了比现有加速器最高8.5倍的利用率提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体应用（如网页DOM处理、复杂工具调用）需要处理超长上下文，导致推理阶段产生大量片外内存访问，受到内存带宽和容量两个内存墙的限制，使得计算单元利用率低下。

Method: 采用硬件软件协同设计：1）支持非对称量化方案的高效硬件实现；2）具有FlashAttention原生支持的扁平化脉动阵列架构；3）完整的软件栈包括自定义ISA、编译器、周期模拟器和自动化设计空间探索流程。

Result: 模拟结果显示，PLENA在相同乘法器数量和内存配置下，比现有加速器利用率最高提升8.5倍，吞吐量比A100 GPU高2.24倍，比TPU v6e高3.85倍。

Conclusion: PLENA系统有效解决了长上下文LLM推理中的内存墙问题，显著提升了硬件利用率和推理性能，该系统将开源发布。

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>
