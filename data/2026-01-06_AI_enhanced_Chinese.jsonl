{"id": "2601.01186", "categories": ["cs.ET", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.01186", "abs": "https://arxiv.org/abs/2601.01186", "authors": ["Alexandre Baigol", "Nikhil Garg", "Matteo Mazza", "Yanming Zhang", "Elisa Zaccaria", "Wooseok Choi", "Bert Jan Offrein", "Laura B\u00e9gon-Lours"], "title": "Analog Weight Update Rule in Ferroelectric Hafnia, using pico-Joule Programming Pulses", "comment": "10 pages, 5 figures. Submitted to Advanced Electronic Materials (Wiley), under review", "summary": "In an effort to compete with the brain's efficiency at processing information, neuromorphic hardware combines artificial synapses and neurons using mixed-signal circuits and emerging memories. In ferroelectric resistive weights, the strength of the synaptic connection between two neurons is stored in the device conductance. During learning, programming pulses are applied to the synaptic weight, which reconfigures the ferroelectric domains and adjusts the conductance. One strategy to lower the energy cost during the training phase is to lower the duration of the programming pulses. However, the latter cannot be shorter than the self-loading time of the resistive weights, limited by intrinsic parasitics in the circuits. In this work, ferroelectric resistive weights are fabricated using a process compatible with CMOS Back-End-Of-Line integration, based on hafnia/zirconia nanolaminates. By laterally scaling the device area under 100 $\u03bc$m$^2$, the self-loading time becomes sufficiently short to enable 20 ns programming, which corresponds to a maximum of 3 picoJoules per pulse. Further, in this work, the weight update rule with 20 ns pulses is experimentally measured not only for different amplitudes but also for different initial conductance states. We find that the final weight is determined by the pulse amplitude, independent of the initial weight value.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a2a\u5411\u7f29\u5c0f\u94c1\u7535\u7535\u963b\u5668\u4ef6\u9762\u79ef\u81f3100\u03bcm\u00b2\u4ee5\u4e0b\uff0c\u5b9e\u73b0\u4e8620\u7eb3\u79d2\u7f16\u7a0b\u8109\u51b2\uff0c\u5c06\u6bcf\u4e2a\u8109\u51b2\u80fd\u8017\u964d\u81f33\u76ae\u7126\u8033\uff0c\u5e76\u53d1\u73b0\u6700\u7ec8\u6743\u91cd\u4ec5\u7531\u8109\u51b2\u5e45\u5ea6\u51b3\u5b9a\uff0c\u4e0e\u521d\u59cb\u72b6\u6001\u65e0\u5173\u3002", "motivation": "\u4e3a\u4e86\u4e0e\u5927\u8111\u5904\u7406\u4fe1\u606f\u7684\u9ad8\u6548\u6027\u7ade\u4e89\uff0c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u9700\u8981\u964d\u4f4e\u8bad\u7ec3\u9636\u6bb5\u7684\u80fd\u8017\u3002\u7f29\u77ed\u7f16\u7a0b\u8109\u51b2\u6301\u7eed\u65f6\u95f4\u662f\u964d\u4f4e\u80fd\u8017\u7684\u7b56\u7565\u4e4b\u4e00\uff0c\u4f46\u53d7\u5230\u7535\u963b\u5668\u4ef6\u81ea\u52a0\u8f7d\u65f6\u95f4\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e0eCMOS\u540e\u7aef\u5de5\u827a\u517c\u5bb9\u7684\u94ea/\u9506\u6c27\u5316\u7269\u7eb3\u7c73\u5c42\u53e0\u7ed3\u6784\u5236\u9020\u94c1\u7535\u7535\u963b\u5668\u4ef6\uff0c\u901a\u8fc7\u6a2a\u5411\u7f29\u5c0f\u5668\u4ef6\u9762\u79ef\u81f3100\u03bcm\u00b2\u4ee5\u4e0b\u6765\u51cf\u5c11\u81ea\u52a0\u8f7d\u65f6\u95f4\uff0c\u5b9e\u73b020\u7eb3\u79d2\u7f16\u7a0b\u8109\u51b2\uff0c\u5e76\u5b9e\u9a8c\u6d4b\u91cf\u4e0d\u540c\u5e45\u5ea6\u548c\u521d\u59cb\u7535\u5bfc\u72b6\u6001\u4e0b\u7684\u6743\u91cd\u66f4\u65b0\u89c4\u5219\u3002", "result": "\u5668\u4ef6\u9762\u79ef\u7f29\u5c0f\u540e\u81ea\u52a0\u8f7d\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff0c\u5b9e\u73b0\u4e8620\u7eb3\u79d2\u7f16\u7a0b\u8109\u51b2\uff0c\u6bcf\u4e2a\u8109\u51b2\u6700\u5927\u80fd\u8017\u4ec5\u4e3a3\u76ae\u7126\u8033\u3002\u5b9e\u9a8c\u53d1\u73b0\u6700\u7ec8\u6743\u91cd\u503c\u4ec5\u53d6\u51b3\u4e8e\u7f16\u7a0b\u8109\u51b2\u7684\u5e45\u5ea6\uff0c\u4e0e\u521d\u59cb\u6743\u91cd\u72b6\u6001\u65e0\u5173\u3002", "conclusion": "\u901a\u8fc7\u6a2a\u5411\u7f29\u5c0f\u94c1\u7535\u7535\u963b\u5668\u4ef6\u9762\u79ef\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u81ea\u52a0\u8f7d\u65f6\u95f4\uff0c\u5b9e\u73b0\u8d85\u4f4e\u80fd\u8017\u7684\u5feb\u901f\u7f16\u7a0b\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u91cd\u8981\u9014\u5f84\u3002"}}
{"id": "2601.01234", "categories": ["cs.ET", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01234", "abs": "https://arxiv.org/abs/2601.01234", "authors": ["Ka-Yan Fung", "Yuxing Tao", "Tze-Leung", "Rick Lui", "Kuen-Fung Sin"], "title": "Bridging Language Gaps: Utilizing Interactive Robots to Teach Cantonese in Real-Life Contexts for Newly-Arrived Children", "comment": null, "summary": "Hong Kong's education system is notably multicultural, including local, non-Chinese-speaking, and newly arrived students (NAS) (Mandarine Chinese-speaking). NAS can guess the meaning of vocabulary but cannot speak out, presenting unique challenges for them, particularly language barriers and cultural differences. These challenges hinder their academic success and social integration, leading to feelings of isolation and demotivation. Current resources often fail to address the emotional well-being of these students and predominantly focus on English language acquisition, leaving a gap in support for learning Cantonese and navigating the local cultural landscape. This study explores the effectiveness of an interactive robot, Boon Boon, in teaching Cantonese through real-life contexts to enhance NAS children learning engagement and motivation. The research questions are: (1) How does interactive robot-empowered scenario learning influence the learning engagement and motivation of NAS in learning Cantonese? and (2) What is the impact of a robot-empowered scenario learning system on the Cantonese language proficiency of NAS? Fourteen children are invited to participate in a four-day learning program with Boon Boon. The preliminary result indicated that Boon Boon drove students' attention to learning and academic achievement. Future research will focus on long-term assessments of robot-empowered learning's effectiveness and explore the scalability of this approach across diverse educational settings and cultural backgrounds.", "AI": {"tldr": "\u9999\u6e2f\u591a\u5143\u6587\u5316\u6559\u80b2\u4e2d\uff0c\u65b0\u4f86\u6e2f\u5b78\u751f\u9762\u81e8\u8a9e\u8a00\u548c\u6587\u5316\u969c\u7919\uff0c\u7814\u7a76\u63a2\u7d22\u4e92\u52d5\u6a5f\u5668\u4ebaBoon Boon\u901a\u904e\u60c5\u5883\u6559\u5b78\u63d0\u5347\u5b78\u751f\u7cb5\u8a9e\u5b78\u7fd2\u53c3\u8207\u5ea6\u548c\u52d5\u6a5f\u7684\u6548\u679c\u3002", "motivation": "\u9999\u6e2f\u6559\u80b2\u7cfb\u7d71\u5177\u6709\u591a\u5143\u6587\u5316\u7279\u9ede\uff0c\u65b0\u4f86\u6e2f\u5b78\u751f\uff08\u666e\u901a\u8a71\u4f7f\u7528\u8005\uff09\u9762\u81e8\u7cb5\u8a9e\u8a9e\u8a00\u969c\u7919\u548c\u6587\u5316\u5dee\u7570\uff0c\u73fe\u6709\u8cc7\u6e90\u4e3b\u8981\u95dc\u6ce8\u82f1\u8a9e\u5b78\u7fd2\u800c\u5ffd\u7565\u7cb5\u8a9e\u548c\u6587\u5316\u9069\u61c9\uff0c\u5c0e\u81f4\u5b78\u751f\u5b78\u696d\u56f0\u96e3\u3001\u793e\u6703\u878d\u5165\u554f\u984c\u548c\u60c5\u611f\u56f0\u64fe\u3002", "method": "\u7814\u7a76\u9080\u8acb14\u540d\u5152\u7ae5\u53c3\u8207\u70ba\u671f\u56db\u5929\u7684\u5b78\u7fd2\u9805\u76ee\uff0c\u4f7f\u7528\u4e92\u52d5\u6a5f\u5668\u4ebaBoon Boon\u901a\u904e\u771f\u5be6\u751f\u6d3b\u60c5\u5883\u9032\u884c\u7cb5\u8a9e\u6559\u5b78\uff0c\u63a2\u8a0e\u6a5f\u5668\u4eba\u8ce6\u80fd\u7684\u60c5\u5883\u5b78\u7fd2\u5c0d\u5b78\u7fd2\u53c3\u8207\u5ea6\u3001\u52d5\u6a5f\u548c\u8a9e\u8a00\u80fd\u529b\u7684\u5f71\u97ff\u3002", "result": "\u521d\u6b65\u7d50\u679c\u986f\u793aBoon Boon\u80fd\u5920\u5438\u5f15\u5b78\u751f\u7684\u5b78\u7fd2\u6ce8\u610f\u529b\u4e26\u4fc3\u9032\u5b78\u696d\u6210\u5c31\uff0c\u8868\u660e\u4e92\u52d5\u6a5f\u5668\u4eba\u5728\u63d0\u5347\u65b0\u4f86\u6e2f\u5b78\u751f\u7cb5\u8a9e\u5b78\u7fd2\u53c3\u8207\u5ea6\u548c\u52d5\u6a5f\u65b9\u9762\u5177\u6709\u7a4d\u6975\u6548\u679c\u3002", "conclusion": "\u4e92\u52d5\u6a5f\u5668\u4eba\u60c5\u5883\u5b78\u7fd2\u5c0d\u65b0\u4f86\u6e2f\u5b78\u751f\u7684\u7cb5\u8a9e\u5b78\u7fd2\u5177\u6709\u6f5b\u5728\u7a4d\u6975\u5f71\u97ff\uff0c\u672a\u4f86\u7814\u7a76\u5c07\u95dc\u6ce8\u9577\u671f\u6548\u679c\u8a55\u4f30\u4ee5\u53ca\u5728\u4e0d\u540c\u6559\u80b2\u74b0\u5883\u548c\u6587\u5316\u80cc\u666f\u4e2d\u7684\u53ef\u64f4\u5c55\u6027\u3002"}}
{"id": "2601.01578", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.01578", "abs": "https://arxiv.org/abs/2601.01578", "authors": ["Psyche T. Malabo", "Bobby D. Gerardo"], "title": "Adaptive Tuning of the Unscented Kalman Filter using Particle Swarm Optimization for Inertial-GPS Sensor Fusion Systems", "comment": null, "summary": "Accurate vehicle positioning requires effective IMU-GPS fusion, yet prior methods-EKF, UKF, ML, GA, and DE-suffer from nonlinearity, instability, or high computational cost. This study introduces a PSO-based adaptive tuning framework for optimizing UKF parameters (\u03b1, \\b{eta}, \\k{appa}, Q, R), evaluated in CARLA 0.9.14 using a Tesla Model 3 under diverse maneuvers and environmental conditions. Within defined parameter bounds, convergence stabilized within 15 generations, achieving an 82.14% accuracy improvement over manual tuning and reducing IMU drift by up to 21,606.59m. Multi-trial statistical validation confirmed consistent gains with low confidence intervals. With update times remaining below the 10 ms real-time threshold, the PSO-tuned UKF demonstrates practical localization performance for dynamic, GPS-challenged conditions.", "AI": {"tldr": "PSO\u4f18\u5316UKF\u53c2\u6570\uff0c\u5728CARLA\u4eff\u771f\u4e2d\u63d0\u5347\u8f66\u8f86\u5b9a\u4f4d\u7cbe\u5ea682.14%\uff0c\u51cf\u5c11IMU\u6f02\u79fb\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42", "motivation": "\u73b0\u6709IMU-GPS\u878d\u5408\u65b9\u6cd5\uff08EKF\u3001UKF\u3001ML\u3001GA\u3001DE\uff09\u5b58\u5728\u975e\u7ebf\u6027\u3001\u4e0d\u7a33\u5b9a\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u53c2\u6570\u4f18\u5316\u65b9\u6cd5", "method": "\u63d0\u51fa\u57fa\u4e8ePSO\u7684\u81ea\u9002\u5e94\u8c03\u4f18\u6846\u67b6\uff0c\u4f18\u5316UKF\u53c2\u6570\uff08\u03b1\u3001\u03b2\u3001\u03ba\u3001Q\u3001R\uff09\uff0c\u5728CARLA 0.9.14\u4e2d\u4f7f\u7528Tesla Model 3\u8fdb\u884c\u591a\u573a\u666f\u6d4b\u8bd5", "result": "\u5728\u53c2\u6570\u8fb9\u754c\u518515\u4ee3\u6536\u655b\uff0c\u76f8\u6bd4\u624b\u52a8\u8c03\u4f18\u7cbe\u5ea6\u63d0\u534782.14%\uff0cIMU\u6f02\u79fb\u51cf\u5c11\u9ad8\u8fbe21,606.59\u7c73\uff0c\u66f4\u65b0\u65f6\u95f4\u4f4e\u4e8e10ms\u5b9e\u65f6\u9608\u503c", "conclusion": "PSO\u8c03\u4f18\u7684UKF\u5728\u52a8\u6001\u3001GPS\u53d7\u9650\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5b9e\u7528\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u901a\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u786e\u8ba4\u4e86\u7a33\u5b9a\u589e\u76ca"}}
{"id": "2601.02007", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.02007", "abs": "https://arxiv.org/abs/2601.02007", "authors": ["Kunyu Wu", "Qiushi Zhao", "Jingyi Zhou", "Junqiao Wang", "Hao Qin", "Xinyue Zhang", "Xingqi Zhang"], "title": "Physics-Informed Deep Recurrent Back-Projection Network for Tunnel Propagation Modeling", "comment": null, "summary": "Accurate and efficient modeling of radio wave propagation in railway tunnels is is critical for ensuring reliable communication-based train control (CBTC) systems. Fine-grid parabolic wave equation (PWE) solvers provide high-fidelity field predictions but are computationally expensive for large-scale tunnels, whereas coarse-grid models lose essential modal and geometric details. To address this challenge, we propose a physics-informed recurrent back-projection propagation network (PRBPN) that reconstructs fine-resolution received-signal-strength (RSS) fields from coarse PWE slices. The network integrates multi-slice temporal fusion with an iterative projection/back-projection mechanism that enforces physical consistency and avoids any pre-upsampling stage, resulting in strong data efficiency and improved generalization. Simulations across four tunnel cross-section geometries and four frequencies show that the proposed PRBPN closely tracks fine-mesh PWE references. Engineering-level validation on the Massif Central tunnel in France further confirms robustness in data-scarce scenarios, trained with only a few paired coarse/fine RSS. These results indicate that the proposed PRBPN can substantially reduce reliance on computationally intensive fine-grid solvers while maintaining high-fidelity tunnel propagation predictions.", "AI": {"tldr": "\u63d0\u51faPRBPN\u7f51\u7edc\uff0c\u4ece\u7c97\u7f51\u683cPWE\u5207\u7247\u91cd\u5efa\u7cbe\u7ec6RSS\u573a\uff0c\u51cf\u5c11\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u7cbe\u7ec6\u7f51\u683c\u6c42\u89e3\u5668\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u96a7\u9053\u4f20\u64ad\u9884\u6d4b", "motivation": "\u94c1\u8def\u96a7\u9053\u4e2d\u65e0\u7ebf\u7535\u6ce2\u4f20\u64ad\u7684\u7cbe\u786e\u9ad8\u6548\u5efa\u6a21\u5bf9CBTC\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u7cbe\u7ec6\u7f51\u683cPWE\u6c42\u89e3\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7c97\u7f51\u683c\u6a21\u578b\u4f1a\u4e22\u5931\u91cd\u8981\u6a21\u6001\u548c\u51e0\u4f55\u7ec6\u8282\uff0c\u9700\u8981\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u5faa\u73af\u53cd\u6295\u5f71\u4f20\u64ad\u7f51\u7edc(PRBPN)\uff0c\u96c6\u6210\u591a\u5207\u7247\u65f6\u95f4\u878d\u5408\u4e0e\u8fed\u4ee3\u6295\u5f71/\u53cd\u6295\u5f71\u673a\u5236\uff0c\u5f3a\u5236\u7269\u7406\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u9884\u4e0a\u91c7\u6837\u9636\u6bb5", "result": "\u5728\u56db\u79cd\u96a7\u9053\u622a\u9762\u51e0\u4f55\u548c\u56db\u79cd\u9891\u7387\u4e0b\u7684\u4eff\u771f\u663e\u793a\uff0cPRBPN\u80fd\u7d27\u5bc6\u8ddf\u8e2a\u7cbe\u7ec6\u7f51\u683cPWE\u53c2\u8003\u3002\u5728\u6cd5\u56fdMassif Central\u96a7\u9053\u7684\u5de5\u7a0b\u7ea7\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027", "conclusion": "PRBPN\u80fd\u663e\u8457\u51cf\u5c11\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u7cbe\u7ec6\u7f51\u683c\u6c42\u89e3\u5668\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u96a7\u9053\u4f20\u64ad\u9884\u6d4b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.01031", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.01031", "abs": "https://arxiv.org/abs/2601.01031", "authors": ["Bharadwaj Veeravalli"], "title": "A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations", "comment": null, "summary": "We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\u7684\u591a\u7aef\u53e3\u5e76\u53d1\u901a\u4fe1\u53ef\u5206\u8d1f\u8f7d\u7406\u8bba\uff08MPCC-DLT\uff09\u6846\u67b6\uff0c\u91cf\u5316\u8ba1\u7b97\u901f\u5ea6\u3001\u94fe\u8def\u5e26\u5bbd\u548c\u7ed3\u679c\u5927\u5c0f\u5f00\u9500\u5bf9\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7684\u8054\u5408\u5f71\u54cd\uff0c\u5e76\u6269\u5c55\u4e86\u5b9e\u65f6\u51c6\u5165\u63a7\u5236\u673a\u5236\u3002", "motivation": "\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\uff08DSS\uff09\u9700\u8981\u5904\u7406\u5e76\u53d1\u6570\u636e\u5206\u53d1\u3001\u5e76\u884c\u8ba1\u7b97\u548c\u7ed3\u679c\u8fd4\u56de\u7684\u590d\u6742\u4efb\u52a1\uff0c\u73b0\u6709\u7406\u8bba\u7f3a\u4e4f\u5bf9\u5f02\u6784\u673a\u8f7d\u5904\u7406\u548c\u661f\u95f4\u94fe\u8def\u6761\u4ef6\u7684\u8054\u5408\u5206\u6790\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86MPCC-DLT\u6846\u67b6\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u8d1f\u8f7d\u5206\u914d\u548c\u5b8c\u6210\u65f6\u95f4\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5efa\u7acb\u4e86\u622a\u6b62\u65f6\u95f4\u53ef\u884c\u6027\u6761\u4ef6\uff0c\u5e76\u6269\u5c55\u4e86\u5904\u7406\u968f\u673a\u4efb\u52a1\u5230\u8fbe\u548c\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u7684\u5b9e\u65f6\u51c6\u5165\u63a7\u5236\u673a\u5236\u3002", "result": "\u9ad8\u5ea6\u53ef\u5206\u4efb\u52a1\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u901a\u4fe1\u5bc6\u96c6\u578b\u4efb\u52a1\u56e0\u7ed3\u679c\u4f20\u8f93\u5f00\u9500\u800c\u6536\u76ca\u9012\u51cf\uff1b\u5b9e\u65f6\u6a21\u62df\u663e\u793a\u4efb\u52a1\u7ed3\u6784\u548c\u7cfb\u7edf\u53c2\u6570\u5171\u540c\u51b3\u5b9a\u622a\u6b62\u65f6\u95f4\u6ee1\u8db3\u60c5\u51b5\u548c\u8fd0\u884c\u72b6\u6001\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u5206\u6790\u53ef\u5904\u7406\u7684MPCC-DLT\u6a21\u578b\uff0c\u4e3a\u5e94\u7528\u611f\u77e5\u8c03\u5ea6\u548c\u672a\u6765\u536b\u661f\u661f\u5ea7\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.00831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00831", "abs": "https://arxiv.org/abs/2601.00831", "authors": ["Uday Kumar Nidadala", "Venkata Bhumika Guthi"], "title": "Horizon Reduction as Information Loss in Offline Reinforcement Learning", "comment": "13 pages, 3 figures", "summary": "Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0chorizon reduction\uff08\u89c6\u91ce\u7f29\u51cf\uff09\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u4f7f\u6700\u4f18\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u4e0e\u6b21\u4f18\u7b56\u7565\u533a\u5206\uff0c\u5373\u4f7f\u6709\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u89c6\u91ce\u7f29\u51cf\u80fd\u6539\u5584\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u5c55\u6027\uff0c\u4f46\u5176\u7406\u8bba\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u89c6\u91ce\u7f29\u51cf\u53ef\u80fd\u5bfc\u81f4\u7684\u57fa\u672c\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u5c06\u89c6\u91ce\u7f29\u51cf\u5f62\u5f0f\u5316\u4e3a\u4ece\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u7247\u6bb5\u4e2d\u5b66\u4e60\uff0c\u5e76\u8bc1\u660e\u5728\u8fd9\u79cd\u8303\u5f0f\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u53ef\u80fd\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u4e0e\u6b21\u4f18\u7b56\u7565\u533a\u5206\u3002\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u53cd\u4f8b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u4e09\u79cd\u7ed3\u6784\u5931\u6548\u6a21\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u89c6\u91ce\u7f29\u51cf\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u5931\u6548\u6a21\u5f0f\uff1a\u524d\u7f00\u4e0d\u53ef\u533a\u5206\u6027\u5bfc\u81f4\u53ef\u8bc6\u522b\u6027\u5931\u8d25\u3001\u622a\u65ad\u56de\u62a5\u5f15\u8d77\u7684\u76ee\u6807\u9519\u8bef\u6307\u5b9a\u3001\u79bb\u7ebf\u6570\u636e\u96c6\u652f\u6301\u548c\u8868\u793a\u6df7\u53e0\u3002", "conclusion": "\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u4ec5\u901a\u8fc7\u7b97\u6cd5\u6539\u8fdb\u514b\u670d\u3002\u7814\u7a76\u4e3a\u89c6\u91ce\u7f29\u51cf\u7684\u5b89\u5168\u4f7f\u7528\u5efa\u7acb\u4e86\u5fc5\u8981\u6761\u4ef6\uff0c\u8865\u5145\u4e86\u5173\u4e8e\u4fdd\u5b88\u76ee\u6807\u548c\u5206\u5e03\u504f\u79fb\u7684\u7b97\u6cd5\u5de5\u4f5c\u3002"}}
{"id": "2601.01158", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.01158", "abs": "https://arxiv.org/abs/2601.01158", "authors": ["Yilun Zhao", "Yu Chen", "Kaiyan Chang", "He Li", "Bing Li", "Yinhe Han", "Ying Wang"], "title": "A System Architecture for Low Latency Multiprogramming Quantum Computing", "comment": null, "summary": "As quantum systems scale, Multiprogramming Quantum Computing (MPQC) becomes essential to improve device utilization and throughput. However, current MPQC pipelines rely on expensive online compilation to co-optimize concurrently running programs, because quantum executables are device-dependent, non-portable across qubit regions, and highly susceptible to noise and crosstalk. This online step dominates runtime and impedes low-latency deployments for practical, real-world workloads in the future, such as repeatedly invoked Quantum Neural Network (QNN) services.\n  We present FLAMENCO, a fidelity-aware multi-version compilation system that enables independent offline compilation and low-latency, high-fidelity multiprogramming at runtime. At the architecture level, FLAMENCO abstracts devices into compute units to drastically shrink the search space of region allocation. At compile time, it generates diverse executable versions for each program -- each bound to a distinct qubit region -- allowing dynamic region selection at runtime and overcoming non-portability. At runtime, FLAMENCO employs a streamlined orchestrator that leverages post-compilation fidelity metrics to avoid conflicts and mitigate crosstalk, achieving reliable co-execution without online co-optimization. Comprehensive evaluations against state-of-the-art MPQC baselines show that FLAMENCO removes online compilation overhead, achieves over 5$\\times$ runtime speedup, improves execution fidelity, and maintains high utilization as concurrency increases.", "AI": {"tldr": "FLAMENCO\u662f\u4e00\u4e2a\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u591a\u7248\u672c\u7f16\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u52a8\u6001\u533a\u57df\u9009\u62e9\uff0c\u6d88\u9664\u4e86\u91cf\u5b50\u591a\u7a0b\u5e8f\u8ba1\u7b97\u4e2d\u7684\u5728\u7ebf\u7f16\u8bd1\u5f00\u9500\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u7a0b\u5e8f\u6267\u884c\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u591a\u7a0b\u5e8f\u91cf\u5b50\u8ba1\u7b97\uff08MPQC\uff09\u5bf9\u63d0\u9ad8\u8bbe\u5907\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684MPQC\u6d41\u6c34\u7ebf\u4f9d\u8d56\u6602\u8d35\u7684\u5728\u7ebf\u7f16\u8bd1\u6765\u534f\u540c\u4f18\u5316\u5e76\u53d1\u8fd0\u884c\u7684\u7a0b\u5e8f\uff0c\u56e0\u4e3a\u91cf\u5b50\u53ef\u6267\u884c\u6587\u4ef6\u662f\u8bbe\u5907\u76f8\u5173\u7684\u3001\u8de8\u91cf\u5b50\u6bd4\u7279\u533a\u57df\u4e0d\u53ef\u79fb\u690d\u7684\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u548c\u4e32\u6270\u9ad8\u5ea6\u654f\u611f\u3002\u8fd9\u79cd\u5728\u7ebf\u6b65\u9aa4\u4e3b\u5bfc\u4e86\u8fd0\u884c\u65f6\uff0c\u963b\u788d\u4e86\u672a\u6765\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u91cd\u590d\u8c03\u7528\u7684\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u670d\u52a1\uff09\u7684\u4f4e\u5ef6\u8fdf\u90e8\u7f72\u3002", "method": "1. \u67b6\u6784\u5c42\u9762\uff1a\u5c06\u8bbe\u5907\u62bd\u8c61\u4e3a\u8ba1\u7b97\u5355\u5143\uff0c\u5927\u5e45\u7f29\u5c0f\u533a\u57df\u5206\u914d\u7684\u641c\u7d22\u7a7a\u95f4\n2. \u7f16\u8bd1\u65f6\uff1a\u4e3a\u6bcf\u4e2a\u7a0b\u5e8f\u751f\u6210\u7ed1\u5b9a\u5230\u4e0d\u540c\u91cf\u5b50\u6bd4\u7279\u533a\u57df\u7684\u591a\u6837\u5316\u53ef\u6267\u884c\u7248\u672c\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u52a8\u6001\u533a\u57df\u9009\u62e9\n3. \u8fd0\u884c\u65f6\uff1a\u91c7\u7528\u7b80\u5316\u7684\u7f16\u6392\u5668\uff0c\u5229\u7528\u7f16\u8bd1\u540e\u4fdd\u771f\u5ea6\u6307\u6807\u907f\u514d\u51b2\u7a81\u548c\u51cf\u8f7b\u4e32\u6270\uff0c\u65e0\u9700\u5728\u7ebf\u534f\u540c\u4f18\u5316", "result": "\u4e0e\u6700\u5148\u8fdb\u7684MPQC\u57fa\u7ebf\u76f8\u6bd4\uff0cFLAMENCO\u6d88\u9664\u4e86\u5728\u7ebf\u7f16\u8bd1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc75\u500d\u7684\u8fd0\u884c\u65f6\u52a0\u901f\uff0c\u63d0\u9ad8\u4e86\u6267\u884c\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u5e76\u53d1\u589e\u52a0\u65f6\u4fdd\u6301\u9ad8\u5229\u7528\u7387\u3002", "conclusion": "FLAMENCO\u901a\u8fc7\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u591a\u7248\u672c\u7f16\u8bd1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u72ec\u7acb\u7684\u79bb\u7ebf\u7f16\u8bd1\u548c\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u7a0b\u5e8f\u8fd0\u884c\u65f6\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u591a\u7a0b\u5e8f\u8ba1\u7b97\u4e2d\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.02210", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.02210", "abs": "https://arxiv.org/abs/2601.02210", "authors": ["Vejaykarthy Srithar", "Syeda Amna Rizvi", "Amani Abusafia", "Athman Bouguettaya", "Balsam Alkouz"], "title": "Impact of Spatial Proximity on Drone Services", "comment": null, "summary": "We demonstrate the peer-to-peer impact of drones flying in close proximity. Understanding these impacts is crucial for planning efficient drone delivery services. In this regard, we conducted a set of experiments using drones at varying positions in a 3D space under different wind conditions. We collected data on drone energy consumption traveling in a skyway segment. We developed a Graphical User Interface (GUI) that plots drone trajectories within a segment. The GUI facilitates analyzing the peer-to-peer influence of drones on their energy consumption. The analysis includes drones' positions, distance of separation, and wind impact.", "AI": {"tldr": "\u7814\u7a76\u65e0\u4eba\u673a\u5728\u8fd1\u8ddd\u79bb\u98de\u884c\u65f6\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4f4d\u7f6e\u3001\u95f4\u8ddd\u548c\u98ce\u51b5\u5bf9\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1GUI\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177", "motivation": "\u7406\u89e3\u65e0\u4eba\u673a\u5728\u8fd1\u8ddd\u79bb\u98de\u884c\u65f6\u7684\u76f8\u4e92\u5f71\u54cd\u5bf9\u4e8e\u89c4\u5212\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u914d\u9001\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7814\u7a76\u4f4d\u7f6e\u3001\u95f4\u8ddd\u548c\u98ce\u51b5\u7b49\u56e0\u7d20\u5bf9\u80fd\u8017\u7684\u5f71\u54cd", "method": "\u5728\u4e0d\u540c\u98ce\u51b5\u4e0b\uff0c\u8ba9\u65e0\u4eba\u673a\u57283D\u7a7a\u95f4\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6536\u96c6\u5728\u822a\u6bb5\u98de\u884c\u7684\u80fd\u8017\u6570\u636e\uff0c\u5f00\u53d1GUI\u53ef\u89c6\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u5e76\u5206\u6790\u76f8\u4e92\u5f71\u54cd", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u7ed8\u5236\u65e0\u4eba\u673a\u8f68\u8ff9\u7684GUI\u5de5\u5177\uff0c\u4fbf\u4e8e\u5206\u6790\u65e0\u4eba\u673a\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u5305\u62ec\u4f4d\u7f6e\u3001\u95f4\u8ddd\u548c\u98ce\u51b5\u5bf9\u80fd\u8017\u7684\u5f71\u54cd", "conclusion": "\u65e0\u4eba\u673a\u5728\u8fd1\u8ddd\u79bb\u98de\u884c\u65f6\u5b58\u5728\u76f8\u4e92\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u5de5\u5177\u53ef\u4ee5\u5206\u6790\u8fd9\u4e9b\u5f71\u54cd\uff0c\u4e3a\u4f18\u5316\u65e0\u4eba\u673a\u914d\u9001\u670d\u52a1\u63d0\u4f9b\u4f9d\u636e"}}
{"id": "2601.01125", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01125", "abs": "https://arxiv.org/abs/2601.01125", "authors": ["Mohammad Goudarzi", "Arash Shaghaghi", "Zhiyu Wang", "Rajkumar Buyya"], "title": "Performance and Security Aware Distributed Service Placement in Fog Computing", "comment": null, "summary": "The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.", "AI": {"tldr": "\u63d0\u51faSPA-DDRL\u6846\u67b6\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u96fe\u8ba1\u7b97\u4e2d\u7684\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u548c\u5b89\u5168\u5408\u89c4\u6027\uff0c\u901a\u8fc7\u4e09\u5c42\u5b89\u5168\u8bc4\u5206\u4f53\u7cfb\u548c\u5206\u5e03\u5f0f\u67b6\u6784\u5b9e\u73b0\u6027\u80fd\u4e0e\u5b89\u5168\u5e73\u8861\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u96fe\u8ba1\u7b97\u4e2d\u670d\u52a1\u653e\u7f6e\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5f02\u6784\u8d44\u6e90\u3001\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u591a\u6837\u5316\u5b89\u5168\u9700\u6c42\u4f7f\u5f97\u6700\u4f18\u670d\u52a1\u653e\u7f6e\u6781\u5177\u6311\u6218\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5927\u591a\u53ea\u5173\u6ce8\u6027\u80fd\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u90e8\u7f72\u51b3\u7b56\u7684\u5b89\u5168\u5f71\u54cd\u3002", "method": "\u63d0\u51faSPA-DDRL\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u52a0\u6743\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u6700\u5c0f\u5316\u5ef6\u8fdf\u540c\u65f6\u6700\u5927\u5316\u57fa\u4e8e\u96fe\u8282\u70b9\u5b89\u5168\u80fd\u529b\u7684\u5b89\u5168\u8bc4\u5206\u3002\u91c7\u7528\u4e09\u5c42\u5b89\u5168\u8bc4\u5206\u4f53\u7cfb\uff08\u914d\u7f6e\u7ea7\u68c0\u67e5\u3001\u80fd\u529b\u7ea7\u8bc4\u4f30\u3001\u63a7\u5236\u7ea7\u8bc4\u4f30\uff09\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u4ee3\u7406-\u5b66\u4e60\u8005\u67b6\u6784\uff0c\u96c6\u6210LSTM\u7f51\u7edc\u3001\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u79bb\u7b56\u7565\u6821\u6b63\u673a\u5236\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u7269\u8054\u7f51\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSPA-DDRL\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u548c\u653e\u7f6e\u5b89\u5168\u6027\uff0c\u54cd\u5e94\u65f6\u95f4\u63d0\u534716.3%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb33%\uff0c\u5728\u6240\u6709\u7cfb\u7edf\u89c4\u6a21\u4e0b\u90fd\u80fd\u4fdd\u6301\u4e00\u81f4\u3001\u53ef\u884c\u3001\u5b89\u5168\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SPA-DDRL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u96fe\u8ba1\u7b97\u4e2d\u670d\u52a1\u653e\u7f6e\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u4e09\u5c42\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u5408\u89c4\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u4f18\u5316\uff0c\u4e3a\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u670d\u52a1\u653e\u7f6e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01265", "categories": ["cs.AR", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.01265", "abs": "https://arxiv.org/abs/2601.01265", "authors": ["Nick Lindsay", "Caroline Trippel", "Anurag Khandelwal", "Abhishek Bhattacharjee"], "title": "CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)", "comment": "This is an extended version of a paper which has been accepted to the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems conference (ASPLOS, March 2026). 20 pages, 20 figures, 8 tables", "summary": "Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.\n  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $\u03bc$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.\n  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.", "AI": {"tldr": "CounterPoint\u662f\u4e00\u4e2a\u6d4b\u8bd5\u5fae\u67b6\u6784\u6a21\u578b\u4e0e\u6027\u80fd\u8ba1\u6570\u5668\u6570\u636e\u4e00\u81f4\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u03bc\u8def\u5f84\u51b3\u7b56\u56fe\u8868\u8fbe\u6a21\u578b\uff0c\u5229\u7528\u591a\u7ef4\u8ba1\u6570\u5668\u7f6e\u4fe1\u533a\u57df\u51cf\u5c11\u590d\u7528\u566a\u58f0\uff0c\u5e2e\u52a9\u4e13\u5bb6\u53d1\u73b0\u672a\u6587\u6863\u5316\u7684\u786c\u4ef6\u7279\u6027\u3002", "motivation": "\u786c\u4ef6\u4e8b\u4ef6\u8ba1\u6570\u5668\u867d\u7136\u80fd\u63ed\u793a\u5fae\u67b6\u6784\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u89c4\u683c\u6a21\u7cca\u3001\u8bbe\u8ba1\u4e0d\u900f\u660e\u548c\u590d\u7528\u566a\u58f0\uff0c\u5bfc\u81f4\u6570\u636e\u96be\u4ee5\u89e3\u91ca\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u9a8c\u8bc1\u5fae\u67b6\u6784\u6a21\u578b\u4e0e\u8ba1\u6570\u5668\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faCounterPoint\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u03bc\u8def\u5f84\u51b3\u7b56\u56fe\u8868\u8fbe\u7528\u6237\u6307\u5b9a\u7684\u5fae\u67b6\u6784\u6a21\u578b\uff1b2\uff09\u6d4b\u8bd5\u6a21\u578b\u4e0e\u6027\u80fd\u8ba1\u6570\u5668\u6570\u636e\u7684\u4e00\u81f4\u6027\uff1b3\uff09\u5f53\u51fa\u73b0\u4e0d\u5339\u914d\u65f6\uff0c\u7cbe\u786e\u5b9a\u4f4d\u53ef\u80fd\u7684\u5fae\u67b6\u6784\u7279\u5f81\uff1b4\uff09\u4f7f\u7528\u591a\u7ef4\u8ba1\u6570\u5668\u7f6e\u4fe1\u533a\u57df\u6765\u51cf\u8f7b\u590d\u7528\u566a\u58f0\u7684\u5f71\u54cd\u3002", "result": "\u5c06CounterPoint\u5e94\u7528\u4e8eHaswell\u5185\u5b58\u7ba1\u7406\u5355\u5143\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u591a\u4e2a\u672a\u6587\u6863\u5316\u548c\u6587\u6863\u4e0d\u5b8c\u6574\u7684\u5fae\u67b6\u6784\u884c\u4e3a\uff0c\u5305\u62ec\uff1a\u8d1f\u8f7d\u5b58\u50a8\u961f\u5217\u4fa7TLB\u9884\u53d6\u5668\u3001\u5408\u5e76\u9875\u8868\u904d\u5386\u5668\u3001\u53ef\u4e2d\u6b62\u9875\u8868\u904d\u5386\u7b49\u3002", "conclusion": "CounterPoint\u5e2e\u52a9\u4e13\u5bb6\u5c06\u566a\u58f0\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\u6d4b\u91cf\u4e0e\u5fae\u67b6\u6784\u5fc3\u7406\u6a21\u578b\u76f8\u534f\u8c03\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u5148\u524d\u9690\u85cf\u7684\u5fae\u5999\u786c\u4ef6\u7279\u6027\uff0c\u4e3a\u5fae\u67b6\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2601.00832", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00832", "abs": "https://arxiv.org/abs/2601.00832", "authors": ["Israk Hasan Jone", "D. M. Rafiun Bin Masud", "Promit Sarker", "Sayed Fuad Al Labib", "Nazmul Islam", "Farhad Billah"], "title": "ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI", "comment": "8 Page, fugure 11", "summary": "Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u867e\u75c5\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f7f\u7528\u516d\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5305\u542b1,149\u5f20\u56fe\u50cf\u7684\u56db\u4e2a\u75be\u75c5\u7c7b\u522b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2dConvNeXt-Tiny\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8696.88%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u867e\u662f\u5168\u7403\u6d88\u8d39\u6700\u5e7f\u6cdb\u7684\u6c34\u4ea7\u54c1\u79cd\u4e4b\u4e00\uff0c\u5177\u6709\u91cd\u8981\u7684\u8425\u517b\u548c\u7ecf\u6d4e\u4ef7\u503c\u3002\u867e\u517b\u6b96\u662f\u8bb8\u591a\u5730\u533a\u7684\u91cd\u8981\u6536\u5165\u6765\u6e90\uff0c\u4f46\u4e0e\u5176\u4ed6\u5f62\u5f0f\u7684\u6c34\u4ea7\u517b\u6b96\u4e00\u6837\uff0c\u867e\u517b\u6b96\u53d7\u5230\u75be\u75c5\u7206\u53d1\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u8fd9\u4e9b\u75be\u75c5\u5bf9\u53ef\u6301\u7eed\u867e\u751f\u4ea7\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u81ea\u52a8\u5316\u75be\u75c5\u5206\u7c7b\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u7684\u68c0\u6d4b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5206\u7c7b\u867e\u75c5\u3002\u4f7f\u7528\u5305\u542b1,149\u5f20\u56fe\u50cf\u3001\u8986\u76d6\u56db\u4e2a\u75be\u75c5\u7c7b\u522b\u7684\u6570\u636e\u96c6\u3002\u90e8\u7f72\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aResNet50\u3001EfficientNet\u3001DenseNet201\u3001MobileNet\u3001ConvNeXt-Tiny\u548cXception\u3002\u8fdb\u884c\u4e86\u80cc\u666f\u79fb\u9664\u548cKeras\u56fe\u50cf\u7ba1\u9053\u7684\u6807\u51c6\u5316\u9884\u5904\u7406\u3002\u4f7f\u7528\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u65b9\u6cd5\uff08FGSM\uff09\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u4ee5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002\u5b9e\u65bd\u4e86CutMix\u548cMixUp\u7b49\u9ad8\u7ea7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u5e94\u7528\u4e86Grad-CAM\u3001Grad-CAM++\u548cXGrad-CAM\u7b49\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u6765\u652f\u6301\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89c6\u5316\u6a21\u578b\u5173\u6ce8\u533a\u57df\u3002", "result": "\u63a2\u7d22\u6027\u7ed3\u679c\u8868\u660e\uff0cConvNeXt-Tiny\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696.88%\u7684\u51c6\u786e\u7387\u3002\u7ecf\u8fc71000\u6b21\u8fed\u4ee3\u540e\uff0c\u6a21\u578b\u768499%\u7f6e\u4fe1\u533a\u95f4\u4e3a[0.953,0.971]\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u867e\u75c5\u81ea\u52a8\u5206\u7c7b\u7cfb\u7edf\uff0cConvNeXt-Tiny\u6a21\u578b\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0c\u4e3a\u867e\u517b\u6b96\u4e2d\u7684\u75be\u75c5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u867e\u517b\u6b96\u7684\u53ef\u6301\u7eed\u6027\u548c\u751f\u4ea7\u529b\u3002"}}
{"id": "2601.02270", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2601.02270", "abs": "https://arxiv.org/abs/2601.02270", "authors": ["Gabriel Timothy", "Syeda Amna Rizvi", "Muhammad Umair", "Athman Bouguettaya", "Balsam Alkouz"], "title": "Modeling Inter-drone Interference as a Service in Skyway Networks", "comment": null, "summary": "We present a novel investigation into the impact of inter-drone interference on delivery efficiencies within multi-drone skyway networks. We conduct controlled experiments to analyze the behavior of drones in an indoor testbed environment. Our study compares performance between solo flights and concurrent multi-drone operations along predefined routes. This analysis captures interference occurring during both flight and at charging stations, providing a comprehensive evaluation of its effects on overall network performance. We conduct a comprehensive series of experiments across diverse scenarios to systematically understand and model the dynamics of inter-drone interference. Key metrics, such as power consumption and delivery times, are considered. This generates a comprehensive dataset for in-depth analysis of interference at both the node and segment levels. These findings are then formalized into a predictive model. The results validate the effectiveness of the developed model, demonstrating its potential to accurately forecast inter-drone interferences.", "AI": {"tldr": "\u7814\u7a76\u65e0\u4eba\u673a\u7fa4\u5728\u5929\u7a7a\u7f51\u7edc\u4e2d\u7684\u76f8\u4e92\u5e72\u6270\u5bf9\u914d\u9001\u6548\u7387\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5e72\u6270\u673a\u5236\u5e76\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u591a\u65e0\u4eba\u673a\u914d\u9001\u7f51\u7edc\u7684\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u4e4b\u95f4\u7684\u76f8\u4e92\u5e72\u6270\u6210\u4e3a\u5f71\u54cd\u914d\u9001\u6548\u7387\u7684\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u5f71\u54cd\u673a\u5236\u3002", "method": "\u5728\u5ba4\u5185\u6d4b\u8bd5\u73af\u5883\u4e2d\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5355\u673a\u98de\u884c\u4e0e\u591a\u673a\u5e76\u53d1\u98de\u884c\u7684\u6027\u80fd\uff0c\u5206\u6790\u98de\u884c\u4e2d\u548c\u5145\u7535\u7ad9\u7684\u5e72\u6270\uff0c\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u751f\u6210\u4e86\u5168\u9762\u7684\u5e72\u6270\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u9884\u6d4b\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u65e0\u4eba\u673a\u95f4\u5e72\u6270\uff0c\u4e3a\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5efa\u7acb\u4e86\u65e0\u4eba\u673a\u5e72\u6270\u9884\u6d4b\u6a21\u578b\uff0c\u4e3a\u591a\u65e0\u4eba\u673a\u5929\u7a7a\u7f51\u7edc\u7684\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.01209", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01209", "abs": "https://arxiv.org/abs/2601.01209", "authors": ["Xin Tan", "Yicheng Feng", "Yu Zhou", "Yimin Jiang", "Yibo Zhu", "Hong Xu"], "title": "OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL", "comment": null, "summary": "Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.", "AI": {"tldr": "OrchestrRL\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u8026\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0e\u751f\u6210\u9636\u6bb5\u7684\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8ba1\u7b97\u8c03\u5ea6\u548c\u53ef\u91cd\u6784\u6df7\u5408\u5149\u7535\u7f51\u6765\u89e3\u51b3\u52a8\u6001\u8d1f\u8f7d\u548c\u7f51\u7edc\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b01.40\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5c06\u5f3a\u5316\u5b66\u4e60\u7684\u751f\u6210\u548c\u8bad\u7ec3\u9636\u6bb5\u89e3\u8026\u4e3a\u5e76\u884c\u5f02\u6b65\u6d41\u6c34\u7ebf\u867d\u80fd\u63d0\u9ad8\u6269\u5c55\u6027\u548c\u541e\u5410\u91cf\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u751f\u6210\u9636\u6bb5\u56e0\u52a8\u6001\u8d1f\u8f7d\u53d8\u5316\u548c\u6267\u884c\u4e0d\u5747\u8861\u6210\u4e3a\u74f6\u9888\uff1b\u89e3\u8026\u9636\u6bb5\u4ea7\u751f\u591a\u6837\u52a8\u6001\u7f51\u7edc\u6d41\u91cf\u6a21\u5f0f\u538b\u57ae\u4f20\u7edf\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u63d0\u51faOrchestrRL\u7f16\u6392\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u81ea\u9002\u5e94\u8ba1\u7b97\u8c03\u5ea6\u5668\uff0c\u52a8\u6001\u8c03\u6574\u5e76\u884c\u5ea6\u4ee5\u5339\u914d\u8d1f\u8f7d\u7279\u5f81\uff0c\u52a0\u901f\u6267\u884c\u5e76\u91cd\u65b0\u5e73\u8861\u8bf7\u6c42\uff1b2) RFabric\u53ef\u91cd\u6784\u6df7\u5408\u5149\u7535\u7f51\uff0c\u5229\u7528\u5149\u7535\u8def\u5f00\u5173\u5b9e\u65f6\u91cd\u6784\u62d3\u6251\uff0c\u652f\u6301\u4e0d\u540c\u5e76\u884c\u914d\u7f6e\u4e0b\u7684\u751f\u6210\u3001\u8bad\u7ec3\u8fed\u4ee3\u4e2d\u7684\u5c42\u95f4\u96c6\u4f53\u901a\u4fe1\u548c\u5468\u671f\u6027\u6743\u91cd\u540c\u6b65\u3002", "result": "\u572848\u4e2aH800 GPU\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cOrchestrRL\u5b9e\u73b0\u6700\u9ad81.40\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002\u901a\u8fc7RLSim\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u8bc4\u4f30RFabric\uff0c\u663e\u793a\u76f8\u6bd4\u9759\u6001Fat-Tree\u7f51\u7edc\u5177\u6709\u66f4\u4f18\u7684\u6027\u80fd\u6210\u672c\u6548\u7387\u3002", "conclusion": "OrchestrRL\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u8ba1\u7b97\u548c\u7f51\u7edc\u8282\u594f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89e3\u8026\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0cRFabric\u6df7\u5408\u5149\u7535\u7f51\u4e3a\u5927\u89c4\u6a21RL\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00834", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00834", "abs": "https://arxiv.org/abs/2601.00834", "authors": ["Julian Evan Chrisnanto", "Salsabila Rahma Alia", "Nurfauzi Fadillah", "Yulison Herry Chrisnanto"], "title": "Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds", "comment": "19 pages, 7 figures", "summary": "Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a \"Stochastic Cloth\" manifold with extreme Gaussian curvature fluctuations ($K \\in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the \"splitting spot\" and \"labyrinthine\" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\\mathcal{E}_{mass} \\approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.", "AI": {"tldr": "IM-PINN\uff1a\u4e00\u79cd\u65e0\u7f51\u683c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\u5d4c\u5165\u81ea\u52a8\u5fae\u5206\u56fe\uff0c\u5728\u8fde\u7eed\u53c2\u6570\u57df\u4e2d\u76f4\u63a5\u6c42\u89e3\u590d\u6742\u6d41\u5f62\u4e0a\u7684\u975e\u7ebf\u6027\u53cd\u5e94\u6269\u6563\u65b9\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u4fdd\u771f\u7f51\u683c\u751f\u6210\u6210\u672c\u548c\u8f9b\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742\u975e\u6b27\u51e0\u91cc\u5f97\u6d41\u5f62\u4e0a\u6a21\u62df\u975e\u7ebf\u6027\u53cd\u5e94\u6269\u6563\u52a8\u529b\u5b66\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9ad8\u4fdd\u771f\u7f51\u683c\u751f\u6210\u7684\u9ad8\u6602\u6210\u672c\uff0c\u4ee5\u53ca\u79bb\u6563\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\u4e2d\u7684\u8f9b\u6f02\u79fb\u95ee\u9898\u3002\u4f20\u7edf\u81ea\u9002\u5e94\u7ec6\u5316\u65b9\u6cd5\u5728\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\u4e0b\u65e0\u6cd5\u89e3\u6790\u5404\u5411\u5f02\u6027\u7684\u56fe\u7075\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u5185\u5728\u5ea6\u91cf\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08IM-PINN\uff09\uff0c\u5c06\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\u5d4c\u5165\u81ea\u52a8\u5fae\u5206\u56fe\uff0c\u89e3\u6790\u91cd\u6784\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\uff0c\u4f7f\u89e3\u590d\u6742\u5ea6\u4e0e\u51e0\u4f55\u79bb\u6563\u5316\u89e3\u8026\u3002\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u6765\u7f13\u89e3\u8c31\u504f\u5dee\uff0c\u5728\u5177\u6709\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\u7684\"\u968f\u673a\u5e03\u6599\"\u6d41\u5f62\u4e0a\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "IM-PINN\u6210\u529f\u6062\u590d\u4e86Gray-Scott\u6a21\u578b\u7684\"\u5206\u88c2\u6591\u70b9\"\u548c\"\u8ff7\u5bab\"\u72b6\u6001\u3002\u4e0e\u8868\u9762\u6709\u9650\u5143\u6cd5\uff08SFEM\uff09\u76f8\u6bd4\uff0cIM-PINN\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7269\u7406\u4e25\u8c28\u6027\uff1a\u5168\u5c40\u8d28\u91cf\u5b88\u6052\u8bef\u5dee\u4e3a0.157\uff08SFEM\u4e3a0.258\uff09\uff0c\u4f5c\u4e3a\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u5168\u5c40\u6c42\u89e3\u5668\u6d88\u9664\u4e86\u534a\u9690\u5f0f\u79ef\u5206\u56fa\u6709\u7684\u8d28\u91cf\u6f02\u79fb\u3002", "conclusion": "IM-PINN\u4e3a\u5728\u6f14\u5316\u8868\u9762\u4e0a\u6a21\u62df\u751f\u7269\u6a21\u5f0f\u5f62\u6210\u63d0\u4f9b\u4e86\u5185\u5b58\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u65e0\u5173\u7684\u8303\u5f0f\uff0c\u6865\u63a5\u4e86\u5fae\u5206\u51e0\u4f55\u548c\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u6d41\u5f62\u4e0a\u7684\u8ba1\u7b97\u6311\u6218\u3002"}}
{"id": "2601.02053", "categories": ["cs.AR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.02053", "abs": "https://arxiv.org/abs/2601.02053", "authors": ["Leandro Lanzieri", "Jiri Kral", "Goerschwin Fey", "Holger Schlarb", "Thomas C. Schmidt"], "title": "Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows", "comment": null, "summary": "Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 \u00b0C temperature increase.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u81ea\u6d4b\u8bd5\u7684\u5fae\u63a7\u5236\u5668\u786c\u4ef6\u8001\u5316\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u957f\u5ea6\u65f6\u95f4\u7a97\u53e3\u786e\u5b9a\u8bbe\u5907\u6700\u5927\u5de5\u4f5c\u9891\u7387\uff0c\u53ef\u73b0\u573a\u90e8\u7f72\u68c0\u6d4b\u6e29\u5ea6\u5f15\u8d77\u7684\u6027\u80fd\u9000\u5316", "motivation": "\u5fae\u63a7\u5236\u5668\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u53ef\u9760\u6027\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u786c\u4ef6\u8001\u5316\u5bfc\u81f4\u7684\u6545\u969c\u53ef\u80fd\u4ea7\u751f\u4e25\u91cd\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u53ef\u90e8\u7f72\u7684\u8001\u5316\u76d1\u63a7\u6280\u672f\uff0c\u901a\u5e38\u91c7\u7528\u9759\u6001\u4fdd\u62a4\u5e26\u9632\u6b62\u65f6\u5e8f\u9519\u8bef\uff0c\u4f46\u8fd9\u4f1a\u9650\u5236\u6027\u80fd\u5e76\u5728\u8bbe\u5907\u8001\u5316\u65f6\u5bfc\u81f4\u7a81\u7136\u6545\u969c", "method": "\u91c7\u7528\u57fa\u4e8e\u8f6f\u4ef6\u7684\u81ea\u6d4b\u8bd5\u65b9\u6cd5\u8bbe\u8ba1\u5fae\u63a7\u5236\u5668\u786c\u4ef6\u9000\u5316\u76d1\u63a7\u6280\u672f\uff0c\u5229\u7528\u53ef\u53d8\u957f\u5ea6\u65f6\u95f4\u7a97\u53e3\u786e\u5b9a\u8bbe\u5907\u7684\u6700\u5927\u5de5\u4f5c\u9891\u7387\uff0c\u53ef\u5728\u73b0\u573a\u90e8\u7f72", "result": "\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e00\u81f4\u68c0\u6d4b\u5230\u6e29\u5ea6\u5f15\u8d77\u7684\u6700\u5927\u5de5\u4f5c\u9891\u7387\u9000\u5316\uff0c\u5728\u6e29\u5ea6\u5347\u9ad860\u00b0C\u65f6\uff0c\u4e0d\u540c\u8bbe\u5907\u7684\u9891\u7387\u9000\u5316\u6700\u9ad8\u53ef\u8fbe13.79%", "conclusion": "\u63d0\u51fa\u7684\u8f6f\u4ef6\u81ea\u6d4b\u8bd5\u65b9\u6cd5\u4e3a\u5fae\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u90e8\u7f72\u7684\u786c\u4ef6\u8001\u5316\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u6e29\u5ea6\u5f15\u8d77\u7684\u6027\u80fd\u9000\u5316\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u9759\u6001\u4fdd\u62a4\u5e26\u65b9\u6cd5"}}
{"id": "2601.01310", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01310", "abs": "https://arxiv.org/abs/2601.01310", "authors": ["Songyu Zhang", "Aaron Tam", "Myungjin Lee", "Shixiong Qi", "K. K. Ramakrishnan"], "title": "Making MoE based LLM inference resilient with Tarragon", "comment": null, "summary": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.", "AI": {"tldr": "Tarragon\u662f\u4e00\u4e2a\u5177\u6709\u6545\u969c\u6062\u590d\u80fd\u529b\u7684MoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u6570\u636e\u8def\u5f84\u548c\u81ea\u6108\u673a\u5236\uff0c\u5c06\u6545\u969c\u5f71\u54cd\u9650\u5236\u5728\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u6545\u969c\u5bfc\u81f4\u7684\u505c\u987f\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\u6545\u969c\u6062\u590d\u80fd\u529b\u5dee\uff0c\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9\u6545\u969c\u5c31\u4f1a\u89e6\u53d1\u6574\u4e2a\u670d\u52a1\u7684\u7c97\u7c92\u5ea6\u91cd\u542f\uff0c\u4e22\u5f03\u5df2\u7d2f\u79ef\u7684\u8fdb\u5ea6\u5e76\u6682\u505c\u6574\u4e2a\u63a8\u7406\u6d41\u6c34\u7ebf\uff0c\u8fd9\u4e0d\u9002\u5408\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684LLM\u670d\u52a1\u3002", "method": "1. \u5229\u7528MoE\u4e2d\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u4e13\u5bb6\u8ba1\u7b97\u7684\u81ea\u7136\u5206\u79bb\uff0c\u5c06\u6ce8\u610f\u529b\u5de5\u4f5c\u8282\u70b9\u548c\u4e13\u5bb6\u5de5\u4f5c\u8282\u70b9\u89c6\u4e3a\u72ec\u7acb\u7684\u6545\u969c\u57df\uff1b2. \u5f15\u5165\u53ef\u91cd\u6784\u6570\u636e\u8def\u5f84\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u8bf7\u6c42\u5230\u5065\u5eb7\u8282\u70b9\u6765\u5c4f\u853d\u6545\u969c\uff1b3. \u5b9e\u73b0\u81ea\u6108\u673a\u5236\uff0c\u653e\u677e\u73b0\u6709MoE\u6846\u67b6\u7684\u7d27\u5bc6\u540c\u6b65\u6267\u884c\uff1b4. \u5bf9\u6709\u72b6\u6001\u7684\u6ce8\u610f\u529b\u5de5\u4f5c\u8282\u70b9\u91c7\u7528\u5f02\u6b65\u589e\u91cfKV\u7f13\u5b58\u68c0\u67e5\u70b9\u4e0e\u6309\u8bf7\u6c42\u6062\u590d\uff1b5. \u5bf9\u65e0\u72b6\u6001\u7684\u4e13\u5bb6\u5de5\u4f5c\u8282\u70b9\u5229\u7528\u5269\u4f59GPU\u5185\u5b58\u90e8\u7f72\u5f71\u5b50\u4e13\u5bb6\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684MegaScale-Infer\u76f8\u6bd4\uff0cTarragon\u5c06\u6545\u969c\u5bfc\u81f4\u7684\u505c\u987f\u65f6\u95f4\u51cf\u5c11\u4e86160-213\u500d\uff08\u4ece\u7ea664\u79d2\u964d\u81f30.3-0.4\u79d2\uff09\uff0c\u540c\u65f6\u5728\u65e0\u6545\u969c\u53d1\u751f\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u3002", "conclusion": "Tarragon\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u6545\u969c\u9694\u79bb\u548c\u9ad8\u6548\u7684\u81ea\u6108\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u670d\u52a1\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.00841", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00841", "abs": "https://arxiv.org/abs/2601.00841", "authors": ["Bharath Nunepalli"], "title": "SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes", "comment": null, "summary": "Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u7ea7\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\u6765\u6ee1\u8db3\u670d\u52a1\u7ea7\u522b\u76ee\u6807\uff0c\u53d1\u73b0\u56fa\u5b9a\u57fa\u7ebf\u7b56\u7565\u8868\u73b0\u826f\u597d\uff0c\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u5728\u8d28\u91cf\u5bfc\u5411SLO\u4e0b\u63d0\u4f9b\u989d\u5916\u6210\u672c\u8282\u7701", "motivation": "RAG\u7cfb\u7edf\u9762\u4e34\u5b9e\u9645\u63a7\u5236\u95ee\u9898\uff1a\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u884c\u4e3a\uff0c\u4ee5\u6ee1\u8db3\u6210\u672c\u3001\u62d2\u7edd\u7387\u548c\u5e7b\u89c9\u98ce\u9669\u7b49\u670d\u52a1\u7ea7\u522b\u76ee\u6807", "method": "\u5c06\u67e5\u8be2\u7ea7\u63a7\u5236\u5efa\u6a21\u4e3a\u79bb\u6563\u52a8\u4f5c\u9009\u62e9\uff08\u68c0\u7d22\u6df1\u5ea6\u3001\u751f\u6210\u6a21\u5f0f\u6216\u62d2\u7edd\uff09\uff0c\u4f7f\u7528SQuAD 2.0\u6784\u5efa\u79bb\u7ebf\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e24\u79cd\u7b80\u5355\u7b56\u7565\u5b66\u4e60\u76ee\u6807\uff1a\u6700\u4f73\u52a8\u4f5c\u76d1\u7763\u5206\u7c7b\u548c\u5956\u52b1\u52a0\u6743\u53d8\u4f53", "result": "\u56fa\u5b9a\u57fa\u7ebf\u7b56\u7565\uff08\u4f4ek\u503c\u3001\u9632\u62a4\u63d0\u793a\uff09\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff1b\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u5728\u8d28\u91cf\u5bfc\u5411SLO\u4e0b\u63d0\u4f9b\u989d\u5916\u6210\u672c\u8282\u7701\uff0c\u5728\u5ec9\u4ef7SLO\u4e0b\u5f53\u62d2\u7edd\u88ab\u9ad8\u5ea6\u5956\u52b1\u65f6\u53ef\u80fd\u51fa\u73b0\u62d2\u7edd\u5d29\u6e83", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86RAG\u7ba1\u9053SLO\u611f\u77e5\u63a7\u5236\u7684\u53ef\u91cd\u590d\u6848\u4f8b\u7814\u7a76\uff0c\u5f3a\u8c03\u5931\u8d25\u6a21\u5f0f\u548c\u62a5\u544a\u89c4\u8303\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u7684\u68c0\u7d22\u5668\u6216\u8bed\u8a00\u6a21\u578b"}}
{"id": "2601.02135", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.02135", "abs": "https://arxiv.org/abs/2601.02135", "authors": ["Liu Shijie", "Zeng Zhenghao", "Jiao Han", "Huang Yihua"], "title": "HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV", "comment": null, "summary": "RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\\times$ and an energy efficiency improvement of 139.17$\\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\\times$ and an energy efficiency improvement of 171.36$\\times$.", "AI": {"tldr": "HFRWKV\uff1a\u9488\u5bf9RWKV\u6a21\u578b\u7684FPGA\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u3001\u53ef\u590d\u7528\u67b6\u6784\u548c\u5168\u7247\u4e0a\u8ba1\u7b97\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u80fd\u6548", "motivation": "RWKV\u4f5c\u4e3a\u73b0\u4ee3RNN\u67b6\u6784\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u5177\u6709\u7ebf\u6027\u5185\u5b58\u4f18\u52bf\uff0c\u4f46\u5176\u987a\u5e8f\u8ba1\u7b97\u6a21\u5f0f\u96be\u4ee5\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u6027\uff0c\u4e14\u9891\u7e41\u7684\u7247\u5916\u6743\u91cd\u8bbf\u95ee\u9020\u6210\u5185\u5b58\u74f6\u9888", "method": "1. \u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7b56\u7565\uff1b2. \u590d\u6742\u8fd0\u7b97\uff08\u6307\u6570\u3001\u9664\u6cd5\uff09\u91c7\u7528\u53ef\u590d\u7528\u67b6\u6784\u7ed3\u5408\u67e5\u627e\u8868\u6216\u5206\u6bb5\u7ebf\u6027\u903c\u8fd1\uff1b3. \u5168\u7247\u4e0a\u8ba1\u7b97\u7cfb\u7edf\u96c6\u6210\u5e76\u884c\u77e9\u9635\u5411\u91cf\u5904\u7406\u9635\u5217\u548c\u9ad8\u6548\u6d41\u6c34\u7ebf\u67b6\u6784\uff1b4. \u8ba1\u7b97\u91cd\u6392\u5e8f\u548c\u5206\u5757\u53cc\u7f13\u51b2\u6280\u672f\u6d88\u9664\u6570\u636e\u4f20\u8f93\u74f6\u9888", "result": "\u5728Alveo U50\u548cU280\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4CPU\u5b9e\u73b063.48\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c139.17\u500d\u80fd\u6548\u63d0\u5347\uff1b\u76f8\u6bd4GPU\u5b9e\u73b032.33\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c171.36\u500d\u80fd\u6548\u63d0\u5347", "conclusion": "HFRWKV\u901a\u8fc7\u5b9a\u5236\u5316\u786c\u4ef6\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86RWKV\u5728GPU\u4e0a\u7684\u5e76\u884c\u6027\u4e0d\u8db3\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e3aRNN\u7c7b\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01500", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01500", "abs": "https://arxiv.org/abs/2601.01500", "authors": ["Jinxiao Zhang", "Yunpu Xu", "Xiyong Wu", "Runmin Dong", "Shenggan Cheng", "Yi Zhao", "Mengxuan Chen", "Qinrui Zheng", "Jianting Liu", "Haohuan Fu"], "title": "DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster", "comment": null, "summary": "Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.", "AI": {"tldr": "DiT-HC\uff1a\u9996\u4e2a\u5728\u4e0b\u4e00\u4ee3HPC CPU\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u548c\u6269\u5c55\u751f\u6210\u6a21\u578bDiT\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u901a\u4fe1\u81ea\u7531\u5f20\u91cf\u5e76\u884c\u3001\u4f18\u5316\u7b97\u5b50\u5185\u6838\u548c\u81ea\u5b9a\u4e49MPI\u540e\u7aef\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740\u751f\u6210\u57fa\u7840\u6a21\u578b\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u6570\u636e\u91cd\u5efa\u548c\u6a21\u62df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u4ee5\u53ca\u65b0\u786c\u4ef6\u7279\u6027\uff08\u5982\u77e9\u9635\u52a0\u901f\u5355\u5143\u548c\u9ad8\u5e26\u5bbd\u5185\u5b58\uff09\u7684\u53d1\u5c55\uff0cCPU\u96c6\u7fa4\u4e3a\u52a0\u901f\u548c\u6269\u5c55\u6b64\u7c7b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u673a\u4f1a\uff0c\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u4e0e\u79d1\u5b66\u8ba1\u7b97\u7684\u878d\u5408\u3002", "method": "DiT-HC\u7cfb\u7edf\u5305\u542b\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a1) \u901a\u4fe1\u81ea\u7531\u5f20\u91cf\u5e76\u884c(CFTP)\u4e0eAutoMem\u5b9e\u73b0\u81ea\u52a8\u5185\u5b58\u611f\u77e5\u6570\u636e\u6d41\uff1b2) HCOps\u4f18\u5316\u7b97\u5b50\u5957\u4ef6\uff0c\u5229\u7528\u5411\u91cf\u548c\u77e9\u9635\u52a0\u901f\u5355\u5143\u4f18\u5316GEMM\u548c\u7b97\u5b50\u5185\u6838\uff1b3) \u81ea\u5b9a\u4e49MPI\u540e\u7aef\uff0c\u91cd\u53e0\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u5185\u5b58\u79fb\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6bd4\u539f\u751f\u6216\u516c\u5171CPU\u5e93\u52a0\u901f8.2\u523087.7\u500d\uff0c\u5728256\u4e2a\u8282\u70b9\u4e0a\u8fbe\u523090.6%\u7684\u5f31\u6269\u5c55\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5728CPU\u96c6\u7fa4\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002", "conclusion": "DiT-HC\u5c55\u793a\u4e86\u5728CPU\u96c6\u7fa4\u4e0a\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765HPC-AI\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u4fc3\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u4e0e\u79d1\u5b66\u8ba1\u7b97\u7684\u7edf\u4e00\u3002"}}
{"id": "2601.00844", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00844", "abs": "https://arxiv.org/abs/2601.00844", "authors": ["Matthieu Destrade", "Oumayma Bounou", "Quentin Le Lidec", "Jean Ponce", "Yann LeCun"], "title": "Value-guided action planning with JEPA world models", "comment": "Presented as a poster at the World Modeling Workshop 2026, Mila", "summary": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u89c4\u5212\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5851\u9020\u8868\u793a\u7a7a\u95f4\u4f7f\u72b6\u6001\u5d4c\u5165\u95f4\u7684\u8ddd\u79bb\u8fd1\u4f3c\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u89c4\u5212\u6027\u80fd", "motivation": "JEPA\u6846\u67b6\u80fd\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u6d4b\u76ee\u6807\u5b66\u4e60\u73af\u5883\u52a8\u6001\u8868\u793a\uff0c\u4f46\u5176\u652f\u6301\u6709\u6548\u884c\u52a8\u89c4\u5212\u7684\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u589e\u5f3a\u89c4\u5212\u80fd\u529b", "method": "\u5851\u9020JEPA\u8868\u793a\u7a7a\u95f4\uff0c\u4f7f\u72b6\u6001\u5d4c\u5165\u95f4\u7684\u8ddd\u79bb\uff08\u6216\u51c6\u8ddd\u79bb\uff09\u8fd1\u4f3c\u7ed9\u5b9a\u73af\u5883\u4e2d\u5230\u8fbe\u6210\u672c\u7684\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u6b64\u7ea6\u675f", "result": "\u76f8\u6bd4\u6807\u51c6JEPA\u6a21\u578b\uff0c\u5728\u7b80\u5355\u63a7\u5236\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u8868\u793a\u7a7a\u95f4\u4f7f\u8ddd\u79bb\u8fd1\u4f3c\u503c\u51fd\u6570\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b"}}
{"id": "2601.01298", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01298", "abs": "https://arxiv.org/abs/2601.01298", "authors": ["Jorge L. Ruiz Williams"], "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware", "comment": null, "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.", "AI": {"tldr": "Warp Cortex\u662f\u4e00\u4e2a\u5f02\u6b65\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u667a\u80fd\u4f53\u903b\u8f91\u4e0e\u7269\u7406\u5185\u5b58\uff0c\u5b9e\u73b0\u767e\u4e07\u7ea7\u667a\u80fd\u4f53\u8ba4\u77e5\u6269\u5c55\uff0c\u5c06\u5185\u5b58\u590d\u6742\u5ea6\u4eceO(N*L)\u964d\u81f3O(1)\u6743\u91cd\u548cO(N*k)\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u5b58\u5728\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u95ee\u9898\uff0c\u4f7f\u5f97\"\u7cfb\u7edf2\"\u5e76\u884c\u63a8\u7406\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u67b6\u6784\u3001\u5355\u4f8b\u6743\u91cd\u5171\u4eab\u548c\u62d3\u6251\u7a81\u89e6\u6280\u672f\uff0c\u5c06KV\u7f13\u5b58\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u5e94\u7528\u89c1\u8bc1\u590d\u6742\u7a00\u758f\u5316\u6280\u672f\uff0c\u5e76\u5f15\u5165\u5f15\u7528\u6ce8\u5165\u673a\u5236\u5b9e\u73b0\u975e\u4fb5\u5165\u5f0fKV\u7f13\u5b58\u66f4\u65b0\u3002", "result": "\u5728\u5355\u5f20NVIDIA RTX 4090\u4e0a\u5b9e\u73b0100\u4e2a\u5e76\u53d1\u667a\u80fd\u4f53\u4ec5\u97002.2GB\u663e\u5b58\uff0c\u7406\u8bba\u5bb9\u91cf\u8d85\u8fc71000\u4e2a\u667a\u80fd\u4f53\uff0c\u8ba1\u7b97\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "Warp Cortex\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u4f18\u5316\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5e76\u884c\u63a8\u7406\u7cfb\u7edf\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.01596", "categories": ["cs.DC", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.01596", "abs": "https://arxiv.org/abs/2601.01596", "authors": ["Congrong Ren", "Robert Underwood", "Sheng Di", "Emrecan Kutay", "Zarija Lukic", "Aylin Yener", "Franck Cappello", "Hanqi Guo"], "title": "FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data", "comment": null, "summary": "This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\u7684\u8c31\u7279\u5f81\u4fdd\u6301\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u5728\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u540c\u65f6\u7ea6\u675f\u8bef\u5dee\u6765\u4fdd\u62a4\u79d1\u5b66\u6570\u636e\u7684\u5173\u952e\u9891\u57df\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4ec5\u4fdd\u8bc1\u7a7a\u95f4\u57df\u7cbe\u5ea6\uff0c\u5ffd\u89c6\u4e86\u9891\u7387\u57df\u7279\u5f81\uff0c\u800c\u8bb8\u591a\u79d1\u5b66\u5e94\u7528\uff08\u5982\u5b87\u5b99\u5b66\u3001\u71c3\u70e7\u6e4d\u6d41\u3001X\u5c04\u7ebf\u884d\u5c04\uff09\u9700\u8981\u540c\u65f6\u4fdd\u7559\u7a7a\u95f4\u548c\u9891\u7387\u8868\u793a\u4ee5\u83b7\u5f97\u4e92\u8865\u7684\u79d1\u5b66\u6d1e\u5bdf\u3002", "method": "\u63d0\u51fa\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\uff0c\u5c06\u9891\u7387\u57df\u8bef\u5dee\u8868\u793a\u4e3a\u7a7a\u95f4\u57df\u8bef\u5dee\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u63a8\u5bfc\u51fa\u540c\u65f6\u7ea6\u675f\u4e24\u4e2a\u57df\u8bef\u5dee\u7684\u533a\u57df\u3002\u901a\u8fc7\u8fed\u4ee3\u6295\u5f71\u5c06\u57fa\u7840\u538b\u7f29\u5668\u7684\u7a7a\u95f4\u8bef\u5dee\u5411\u91cf\u6295\u5f71\u5230\u7a7a\u95f4\u548c\u9891\u7387\u7ea6\u675f\u533a\u57df\u7684\u4ea4\u96c6\u5185\uff0c\u5e76\u4f7f\u7528GPU\u5e76\u884c\u52a0\u901f\u3002", "result": "\u5728\u5b87\u5b99\u5b66\u6a21\u62df\u3001X\u5c04\u7ebf\u884d\u5c04\u3001\u71c3\u70e7\u6a21\u62df\u548c\u8111\u7535\u56fe\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u4fdd\u7559\u4e86\u7a7a\u95f4\u548c\u9891\u7387\u57df\u7684\u5173\u952e\u79d1\u5b66\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u538b\u7f29\u6280\u672f\u5ffd\u89c6\u9891\u7387\u57df\u7279\u5f81\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u7ea6\u675f\u7a7a\u95f4\u548c\u9891\u7387\u8bef\u5dee\uff0c\u4e3a\u9700\u8981\u540c\u65f6\u5206\u6790\u4e24\u4e2a\u57df\u7684\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00847", "abs": "https://arxiv.org/abs/2601.00847", "authors": ["Ryan Shamim"], "title": "You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference", "comment": "24 pages, 5 figures. Deterministic evaluation protocol. Includes theoretical analysis and empirical validation on GPT-2 and Gemma 2 9B", "summary": "Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.", "AI": {"tldr": "MFEE\u6846\u67b6\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u63a7\u5236\u5e73\u9762\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u9009\u62e9\u6027\u6267\u884ctransformer\uff0c\u5b9e\u73b078.1%\u7684\u6267\u884c\u51cf\u5c11\u540c\u65f6\u4fdd\u6301100%\u51c6\u786e\u7387", "motivation": "\u5f53\u524dAI\u63a8\u7406\u7cfb\u7edf\u5c06transformer\u6267\u884c\u89c6\u4e3a\u5f3a\u5236\u6027\u7684\uff0c\u6df7\u6dc6\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u6267\u884c\u5fc5\u8981\u6027\u3002\u9700\u8981\u533a\u5206\u4f55\u65f6\u5fc5\u987b\u6267\u884ctransformer\uff0c\u4f55\u65f6\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u8def\u5f84\u4fdd\u6301\u6b63\u786e\u6027", "method": "\u63d0\u51faMeaning-First Execution (MFEE)\u63a7\u5236\u5e73\u9762\u67b6\u6784\uff0c\u4f5c\u4e3a\u73b0\u6709\u5806\u6808\u4e4b\u4e0a\u7684\u95e8\u63a7\u5c42\uff0c\u4e0d\u4fee\u6539\u6a21\u578b\u3001\u6743\u91cd\u6216\u53c2\u6570\u3002\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u9009\u62e9\u6027\u8c03\u7528transformer\u63a8\u7406", "result": "\u57281000\u4e2a\u591a\u6837\u5316\u63d0\u793a\u4e0b\uff0cMFEE\u5b9e\u73b078.1%\u6267\u884c\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301100%\u7cbe\u786e\u5339\u914d\u7b49\u4ef7\u6027\u3002\u76f8\u6bd4\u6a21\u5f0f\u8def\u7531\u5668\u6700\u591a53.3%\u907f\u514d\u7387\u4e14\u6709\u6b63\u786e\u6027\u5931\u8d25\uff0cMFEE\u8fbe\u5230100%\u907f\u514d\u7387\u4e14\u96f6\u5931\u8d25", "conclusion": "\u901a\u8fc7\u5b9a\u74061\u8bc1\u660e\u4ec5\u57fa\u4e8e\u6709\u9650\u7279\u5f81\u6620\u5c04\u7684\u8def\u7531\u5668\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u96f6\u5047\u8df3\u8fc7\u548c\u6b63\u907f\u514d\u7387\u3002\u6267\u884c\u6cbb\u7406\u5e94\u6210\u4e3aML\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u7684\u57fa\u7840\u5c42\uff0c\u4e0e\u6a21\u578b\u7ea7\u4f18\u5316\u6280\u672f\u6b63\u4ea4"}}
{"id": "2601.02253", "categories": ["cs.LG", "cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02253", "abs": "https://arxiv.org/abs/2601.02253", "authors": ["Emrah Mete", "Emin Erkan Korkmaz"], "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission", "comment": "9 pages, 4 figures", "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.", "AI": {"tldr": "\u63d0\u51faNeuro-Channel Networks (NCN)\uff0c\u4e00\u79cd\u65e0\u9700\u4e58\u6cd5\u8fd0\u7b97\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7684\u79bb\u5b50\u901a\u9053\u673a\u5236\uff0c\u7528\u901a\u9053\u5bbd\u5ea6\u548c\u795e\u7ecf\u9012\u8d28\u53c2\u6570\u66ff\u4ee3\u4f20\u7edf\u6743\u91cd\uff0c\u4ec5\u4f7f\u7528\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u4f4d\u8fd0\u7b97\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e25\u91cd\u4f9d\u8d56\u6602\u8d35\u3001\u9ad8\u80fd\u8017\u4e14\u4f9b\u5e94\u7d27\u5f20\u7684GPU\u786c\u4ef6\uff0c\u9650\u5236\u4e86AI\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u666e\u53ca\u3002\u4f20\u7edf\u4eba\u5de5\u611f\u77e5\u5668\u4f9d\u8d56\u5bc6\u96c6\u77e9\u9635\u4e58\u6cd5\uff0c\u800c\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u901a\u8fc7\u7269\u7406\u79bb\u5b50\u901a\u9053\u9650\u5236\u548c\u5316\u5b66\u795e\u7ecf\u9012\u8d28\u8c03\u8282\u4fe1\u53f7\u4f20\u8f93\uff0c\u65e0\u9700\u7b97\u672f\u4e58\u6cd5\uff0c\u6548\u7387\u66f4\u9ad8\u3002", "method": "\u63d0\u51faNeuro-Channel Networks (NCN)\uff1a1) \u7528\u901a\u9053\u5bbd\u5ea6(Channel Widths)\u7269\u7406\u9650\u5236\u4fe1\u53f7\u5e45\u5ea6\uff1b2) \u7528\u795e\u7ecf\u9012\u8d28(Neurotransmitter)\u53c2\u6570\u57fa\u4e8e\u7b26\u53f7\u903b\u8f91\u8c03\u8282\u4fe1\u53f7\u4f20\u8f93\uff1b3) \u524d\u5411\u4f20\u64ad\u4ec5\u4f7f\u7528\u52a0\u6cd5\u3001\u51cf\u6cd5\u548c\u4f4d\u8fd0\u7b97(\u6700\u5c0f\u503c\u3001\u7b26\u53f7)\uff0c\u5b8c\u5168\u6d88\u9664\u6d6e\u70b9\u4e58\u6cd5\uff1b4) \u4f7f\u7528\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "NCN\u80fd\u591f\u4ee5100%\u51c6\u786e\u7387\u89e3\u51b3\u975e\u7ebf\u6027\u53ef\u5206\u95ee\u9898(XOR\u548c\u591a\u6570\u51fd\u6570)\uff0c\u8bc1\u660e\u5176\u65e0\u9700\u4e58\u6cd5\u6743\u91cd\u5373\u53ef\u5f62\u6210\u590d\u6742\u51b3\u7b56\u8fb9\u754c\u7684\u80fd\u529b\u3002", "conclusion": "NCN\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u590d\u6742\u6a21\u578b\u80fd\u591f\u5728\u5546\u7528CPU\u6216\u8d85\u4f4e\u529f\u8017\u82af\u7247\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684GPU\u96c6\u7fa4\uff0c\u6709\u671b\u5b9e\u73b0AI\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5e7f\u6cdb\u90e8\u7f72\u3002"}}
{"id": "2601.01712", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01712", "abs": "https://arxiv.org/abs/2601.01712", "authors": ["Jiarui Wang", "Huichao Chai", "Yuanhang Zhang", "Zongjin Zhou", "Wei Guo", "Xingkun Yang", "Qiang Tang", "Bo Pan", "Jiawei Zhu", "Ke Cheng", "Yuting Yan", "Shulan Wang", "Yingjie Zhu", "Zhengfan Yuan", "Jiaqi Huang", "Yuhan Zhang", "Xiaosong Sun", "Zhinan Zhang", "Hong Zhu", "Yongsheng Zhang", "Tiantian Dong", "Zhong Xiao", "Deliang Liu", "Chengzhou Lu", "Yuan Sun", "Zhiyuan Chen", "Xinming Han", "Zaizhu Liu", "Yaoyuan Wang", "Ziyang Zhang", "Yong Liu", "Jinxin Xu", "Yajing Sun", "Zhoujun Yu", "Wenting Zhou", "Qidong Zhang", "Zhengyong Zhang", "Zhonghai Gu", "Yibo Jin", "Yongxiang Feng", "Pengfei Zuo"], "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference", "comment": null, "summary": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.", "AI": {"tldr": "RelayGR\u7cfb\u7edf\u901a\u8fc7\u9009\u62e9\u6027\u9884\u63a8\u65ad\u7528\u6237\u884c\u4e3a\u524d\u7f00\u3001\u4fdd\u6301KV\u7f13\u5b58\u5728HBM\u4e2d\uff0c\u5e76\u786e\u4fdd\u540e\u7eed\u6392\u5e8f\u80fd\u76f4\u63a5\u4f7f\u7528\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u5728\u4e25\u683c\u5c3e\u90e8\u5ef6\u8fdfSLO\u4e0b\uff0c\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5728\u7ebf\u5e8f\u5217\u957f\u5ea6\u53d7\u5230\u9650\u5236\u3002\u5927\u591a\u6570GR token\u7f16\u7801\u7684\u7528\u6237\u884c\u4e3a\u4e0e\u5019\u9009\u7269\u54c1\u65e0\u5173\uff0c\u8fd9\u4e3a\u9884\u63a8\u65ad\u548c\u590d\u7528\u7528\u6237\u884c\u4e3a\u524d\u7f00\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "RelayGR\u91c7\u7528\u4e09\u79cd\u6280\u672f\uff1a1) \u5e8f\u5217\u611f\u77e5\u89e6\u53d1\u5668\uff0c\u5728\u6709\u9650\u7f13\u5b58\u5360\u7528\u548c\u9884\u63a8\u65ad\u8d1f\u8f7d\u4e0b\u4ec5\u5904\u7406\u6709\u98ce\u9669\u7684\u8bf7\u6c42\uff1b2) \u4eb2\u548c\u611f\u77e5\u8def\u7531\u5668\uff0c\u5c06\u7f13\u5b58\u751f\u4ea7\u548c\u6d88\u8d39\u8def\u7531\u5230\u540c\u4e00\u5b9e\u4f8b\uff1b3) \u5185\u5b58\u611f\u77e5\u6269\u5c55\u5668\uff0c\u5229\u7528\u670d\u52a1\u5668\u672c\u5730DRAM\u6355\u83b7\u77ed\u671f\u8de8\u8bf7\u6c42\u590d\u7528\u3002", "result": "\u5728\u56fa\u5b9aP99 SLO\u4e0b\uff0cRelayGR\u652f\u6301\u957f\u8fbe1.5\u500d\u7684\u5e8f\u5217\u957f\u5ea6\uff0c\u5e76\u5c06SLO\u517c\u5bb9\u541e\u5410\u91cf\u63d0\u9ad8\u81f33.6\u500d\u3002", "conclusion": "RelayGR\u901a\u8fc7\u521b\u65b0\u7684\u4e2d\u7ee7\u5f0f\u63a8\u7406\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5de5\u4e1a\u89c4\u6a21\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.00850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00850", "abs": "https://arxiv.org/abs/2601.00850", "authors": ["Aayush Kumar"], "title": "EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference", "comment": "24 pages,3 Figures, Submitting to IEEE Access", "summary": "Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.", "AI": {"tldr": "EdgeJury\uff1a\u8f7b\u91cf\u7ea7\u96c6\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\uff083B-8B\uff09\u901a\u8fc7\u5e76\u884c\u89d2\u8272\u751f\u6210\u3001\u533f\u540d\u4ea4\u53c9\u8bc4\u5ba1\u3001\u4e3b\u5e2d\u5408\u6210\u548c\u4e00\u81f4\u6027\u6807\u6ce8\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u771f\u5b9e\u6027\uff0c\u5728\u8fb9\u7f18\u90e8\u7f72\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u53ef\u9760\u95ee\u7b54\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u524d\u6cbf\u89c4\u6a21\u6a21\u578b\u6216\u68c0\u7d22\u7ba1\u9053\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u4ec5\u4f7f\u7528\u5c0f\u578b\u6a21\u578b\u5c31\u80fd\u63d0\u5347\u771f\u5b9e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "EdgeJury\u91c7\u7528\u56db\u9636\u6bb5\u96c6\u6210\u6846\u67b6\uff1a1\uff09\u5e76\u884c\u89d2\u8272\u4e13\u4e1a\u5316\u751f\u6210\uff1b2\uff09\u533f\u540d\u4ea4\u53c9\u8bc4\u5ba1\uff0c\u5305\u542b\u7ed3\u6784\u5316\u6279\u8bc4\u548c\u6392\u540d\uff1b3\uff09\u4e3b\u5e2d\u5408\u6210\uff0c\u6574\u5408\u6700\u5f3a\u5185\u5bb9\u5e76\u89e3\u51b3\u6807\u8bb0\u95ee\u9898\uff1b4\uff09\u57fa\u4e8e\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u7684\u58f0\u660e\u7ea7\u4e00\u81f4\u6027\u6807\u6ce8\u3002", "result": "\u5728TruthfulQA\uff08MC1\uff09\u4e0a\u8fbe\u523076.2%\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5355\u4e2a8B\u57fa\u7ebf\uff0862.8%\uff09\u63d0\u534721.4%\uff1b\u5728200\u4e2a\u5bf9\u6297\u6027EdgeCases\u95ee\u9898\u4e0a\u83b7\u5f9748.2%\u76f8\u5bf9\u589e\u76ca\uff1b\u4eba\u5de5\u5206\u6790\u663e\u793a\u4e8b\u5b9e\u6027\u5e7b\u89c9\u9519\u8bef\u51cf\u5c11\u7ea655%\uff1b\u5728Cloudflare Workers AI\u4e0a\u5b9e\u73b08.4\u79d2\u4e2d\u4f4d\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u534f\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u96c6\u6210\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u4e13\u6709\u5927\u6a21\u578bAPI\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u5728\u8bef\u89e3\u5bc6\u96c6\u578b\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u771f\u5b9e\u6027\uff0c\u9002\u5408\u8fb9\u7f18\u670d\u52a1\u5668\u63a8\u7406\u90e8\u7f72\u3002"}}
{"id": "2601.01787", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01787", "abs": "https://arxiv.org/abs/2601.01787", "authors": ["Yuxiao Li", "Mingze Xia", "Xin Liang", "Bei Wang", "Robert Underwood", "Sheng Di", "Hemant Sharma", "Dishant Beniwal", "Franck Cappello", "Hanqi Guo"], "title": "pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression", "comment": null, "summary": "Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u4fee\u6b63\u538b\u7f29\u540e\u6570\u636e\u7684\u62d3\u6251\u7279\u5f81\uff08PLMSS\uff09\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387", "motivation": "\u6709\u635f\u538b\u7f29\u4f1a\u626d\u66f2\u6570\u636e\u7684\u5173\u952e\u7279\u5f81\uff0c\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u548c\u79d1\u5b66\u7ed3\u8bba\u3002\u73b0\u6709\u5355GPU\u7b97\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u6781\u7aef\u89c4\u6a21\u6570\u636e\uff0c\u5e76\u884c\u8ba1\u7b97\u79ef\u5206\u8def\u5f84\u662f\u4e3b\u8981\u74f6\u9888", "method": "\u7b80\u5316MSz\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u6240\u6709\u4f4d\u7f6e\u4fdd\u7559\u6700\u9661\u4e0a\u5347\u548c\u4e0b\u964d\u65b9\u5411\u6765\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u548c\u4fee\u6b63\u79ef\u5206\u8def\u5f84\uff0c\u51cf\u5c11\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u989d\u5916\u5b58\u50a8\u5f00\u9500", "result": "\u5728Perlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86PLMSS\u4fee\u6b63\u7684\u6269\u5c55\u74f6\u9888\uff0c\u80fd\u591f\u5728\u6781\u7aef\u89c4\u6a21\u6570\u636e\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u4e3a\u79d1\u5b66\u6570\u636e\u538b\u7f29\u540e\u7684\u62d3\u6251\u7279\u5f81\u4fee\u6b63\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00853", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00853", "abs": "https://arxiv.org/abs/2601.00853", "authors": ["Sameer Rahil", "Zain Abdullah Ahmad", "Talha Asif"], "title": "FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments", "comment": "13 pages, 27 figures", "summary": "Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \\textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.", "AI": {"tldr": "FedSCAM\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u805a\u5408\u6743\u91cd\uff0c\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u7684\u6536\u655b\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\uff08\u7279\u522b\u662f\u975eIID\u6807\u7b7e\u5206\u5e03\uff09\u5bf9\u6536\u655b\u548c\u6cdb\u5316\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709SAM\u65b9\u6cd5\u5bf9\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684\u6270\u52a8\u534a\u5f84\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51faFedSCAM\u7b97\u6cd5\uff1a1) \u8ba1\u7b97\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u5f02\u8d28\u6027\u6307\u6807\uff1b2) \u6839\u636e\u5f02\u8d28\u6027\u5206\u6570\u53cd\u5411\u8c03\u5236SAM\u6270\u52a8\u534a\u5f84\uff0c\u9632\u6b62\u9ad8\u65b9\u5dee\u5ba2\u6237\u7aef\u7834\u574f\u5168\u5c40\u6a21\u578b\uff1b3) \u5f15\u5165\u5f02\u8d28\u6027\u611f\u77e5\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\uff0c\u4f18\u5148\u8003\u8651\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e0d\u540c\u7a0b\u5ea6\u7684\u72c4\u5229\u514b\u96f7\u6807\u7b7e\u504f\u659c\u8fdb\u884c\u5b9e\u9a8c\uff0cFedSCAM\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6d4b\u8bd5\u51c6\u786e\u7387\u65b9\u9762\u4e0eFedSAM\u3001FedLESAM\u7b49\u5148\u8fdb\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "FedSCAM\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6270\u52a8\u534a\u5f84\u548c\u805a\u5408\u6743\u91cd\u6765\u9002\u5e94\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.01980", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01980", "abs": "https://arxiv.org/abs/2601.01980", "authors": ["Manuel Parra-Roy\u00f3n", "\u00c1lvaro Rodr\u00edguez-Gallardo", "Susana S\u00e1nchez-Exp\u00f3sito", "Laura Darriba-Pol", "Jes\u00fas S\u00e1nchez-Casta\u00f1eda", "M. \u00c1ngeles Mendoza", "Juli\u00e1n Garrido", "Javier Mold\u00f3n", "Lourdes Verdes-Montenegro"], "title": "Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet", "comment": "8 pages", "summary": "The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.\n  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.\n  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728SKA\u5929\u6587\u53f0\u533a\u57df\u4e2d\u5fc3\u7f51\u7edc\u4e2d\u91c7\u7528\u5206\u5e03\u5f0f\u548c\u539f\u4f4d\u8ba1\u7b97\uff0c\u7ed3\u5408FaaS\u548c\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u636e\u96c6\u4e2d\u5904\u7406\u9762\u4e34\u7684\u7f51\u7edc\u548c\u5b58\u50a8\u74f6\u9888\u95ee\u9898\u3002", "motivation": "SKA\u5929\u6587\u53f0\u5c06\u4ea7\u751f\u524d\u6240\u672a\u6709\u7684\u6d77\u91cf\u6570\u636e\uff0c\u4f20\u7edf\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u8ba1\u7b97\u6a21\u578b\u7531\u4e8e\u7f51\u7edc\u548c\u5b58\u50a8\u74f6\u9888\u5df2\u4e0d\u53ef\u884c\u3002\u533a\u57df\u4e2d\u5fc3\u7f51\u7edc\u9700\u8981\u5728\u8fd1exascale\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u6765\u89e3\u51b3\u6570\u636e\u79fb\u52a8\u5e26\u6765\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5411\u5206\u5e03\u5f0f\u548c\u539f\u4f4d\u8ba1\u7b97\u8f6c\u53d8\uff0c\u5c06\u8ba1\u7b97\u79fb\u52a8\u5230\u6570\u636e\u9644\u8fd1\u3002\u96c6\u6210\u51fd\u6570\u5373\u670d\u52a1\uff08FaaS\uff09\u4e0e\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u667a\u80fd\u51b3\u7b56\u5b9e\u4f53\uff0cFaaS\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6a21\u5757\u5316\u51fd\u6570\u6267\u884c\uff0c\u8fdb\u5316\u7b97\u6cd5\u7528\u4e8e\u591a\u76ee\u6807\u4f18\u5316\uff08\u6267\u884c\u65f6\u95f4\u548c\u80fd\u8017\uff09\uff0c\u8003\u8651\u6570\u636e\u4f4d\u7f6e\u548c\u4f20\u8f93\u6210\u672c\u7ea6\u675f\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8eSRCNet\u67b6\u6784\u4e2d\u9ad8\u6548\u3001\u6210\u672c\u611f\u77e5\u7684\u8ba1\u7b97\u5230\u6570\u636e\u7b56\u7565\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7MOEAs\u63a2\u7d22\u8003\u8651\u6267\u884c\u65f6\u95f4\u3001\u80fd\u8017\u3001\u6570\u636e\u4f4d\u7f6e\u548c\u4f20\u8f93\u6210\u672c\u7684\u8fd1\u6700\u4f18\u6267\u884c\u8ba1\u5212\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aSKA\u533a\u57df\u4e2d\u5fc3\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7FaaS\u548c\u8fdb\u5316\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u5230\u6570\u636e\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u5929\u6587\u6570\u636e\u5904\u7406\u6311\u6218\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.00857", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00857", "abs": "https://arxiv.org/abs/2601.00857", "authors": ["Yuchi Ma", "Yawen Shen", "Anu Swatantran", "David B. Lobell"], "title": "Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks", "comment": null, "summary": "Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.", "AI": {"tldr": "\u8bc4\u4f30AlphaEarth Foundation\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u519c\u4e1a\u76d1\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u548c\u8015\u4f5c\u5236\u56fe\u65b9\u9762\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5b58\u5728\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u65f6\u95f4\u654f\u611f\u6027\u7b49\u9650\u5236", "motivation": "\u5c3d\u7ba1AlphaEarth Foundation\u7b49\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u519c\u4e1a\u76d1\u6d4b\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u6df1\u5165\u8bc4\u4f30\uff0c\u4e14\u4e0e\u4f20\u7edf\u9065\u611f\u6a21\u578b\u7f3a\u4e4f\u5168\u9762\u6bd4\u8f83", "method": "\u5728\u7f8e\u56fd\u7684\u4e09\u4e2a\u519c\u4e1a\u4e0b\u6e38\u4efb\u52a1\uff08\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3001\u8015\u4f5c\u5236\u56fe\u3001\u8986\u76d6\u4f5c\u7269\u5236\u56fe\uff09\u4e2d\u8bc4\u4f30AEF\u5d4c\u5165\uff0c\u4f7f\u7528\u516c\u5f00\u548c\u79c1\u6709\u6570\u636e\u6e90\uff0c\u8bad\u7ec3\u4f20\u7edf\u9065\u611f\u6a21\u578b\u4f5c\u4e3a\u5bf9\u6bd4\u57fa\u51c6", "result": "AEF\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u4ea7\u91cf\u9884\u6d4b\u548c\u53bf\u7ea7\u8015\u4f5c\u5236\u56fe\u65b9\u9762\u4e0e\u4e13\u95e8\u6784\u5efa\u7684\u9065\u611f\u6a21\u578b\u7ade\u4e89\u529b\u76f8\u5f53\uff0c\u4f46\u5b58\u5728\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u3001\u53ef\u89e3\u91ca\u6027\u4f4e\u3001\u65f6\u95f4\u654f\u611f\u6027\u4e0d\u8db3\u7b49\u9650\u5236", "conclusion": "AEF\u5d4c\u5165\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u654f\u611f\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u5f53\u524d\u7248\u672c\u5b58\u5728\u660e\u663e\u9650\u5236"}}
{"id": "2601.02092", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02092", "abs": "https://arxiv.org/abs/2601.02092", "authors": ["Abdullah Al Asif", "Sixing Yu", "Juan Pablo Munoz", "Arya Mazaheri", "Ali Jannesari"], "title": "SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks", "comment": null, "summary": "SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \\textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\\times$ lower total communication cost and $13\\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.", "AI": {"tldr": "SuperSFL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5272\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u8d44\u6e90\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u5b50\u7f51\u7edc\uff0c\u6709\u6548\u5e94\u5bf9\u8bbe\u5907\u5f02\u6784\u6027\u6311\u6218\uff0c\u5e76\u5f15\u5165\u4e09\u9636\u6bb5\u68af\u5ea6\u878d\u5408\u4f18\u5316\u673a\u5236\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u4f20\u7edfSplitFed Learning\u5728\u5f02\u6784\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u5305\u62ec\u8bbe\u5907\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\u5dee\u5f02\u5927\u3001\u6536\u655b\u901f\u5ea6\u6162\u3001\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u8bbe\u5907\u8d44\u6e90\u5dee\u5f02\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u4f7f\u7528\u6743\u91cd\u5171\u4eab\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u8d44\u6e90\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u5b50\u7f51\u7edc\uff1b2) \u5f15\u5165\u4e09\u9636\u6bb5\u68af\u5ea6\u878d\u5408(TPGF)\u4f18\u5316\u673a\u5236\u534f\u8c03\u672c\u5730\u66f4\u65b0\u3001\u670d\u52a1\u5668\u7aef\u8ba1\u7b97\u548c\u68af\u5ea6\u878d\u5408\uff1b3) \u91c7\u7528\u5bb9\u9519\u5ba2\u6237\u7aef\u5206\u7c7b\u5668\u548c\u534f\u4f5c\u5f0f\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u805a\u5408\u673a\u5236\u5e94\u5bf9\u901a\u4fe1\u4e2d\u65ad\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e100\u4e2a\u5f02\u6784\u5ba2\u6237\u7aef\u6d4b\u8bd5\u663e\u793a\uff1a\u901a\u4fe1\u8f6e\u6570\u6536\u655b\u901f\u5ea6\u63d0\u53472-5\u500d\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u603b\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe20\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed13\u500d\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u66f4\u597d\u7684\u80fd\u6548\u3002", "conclusion": "SuperSFL\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u611f\u77e5\u5b50\u7f51\u7edc\u751f\u6210\u548c\u4f18\u5316\u7684\u68af\u5ea6\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u4f4e\u7684\u901a\u4fe1\u6210\u672c\u548c\u66f4\u9ad8\u7684\u80fd\u6548\uff0c\u662f\u5b9e\u7528\u7684\u5f02\u6784\u8fb9\u7f18\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00860", "categories": ["cs.LG", "cs.AI", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00860", "abs": "https://arxiv.org/abs/2601.00860", "authors": ["Xidi Wang"], "title": "Path Integral Solution for Dissipative Generative Dynamics", "comment": "6 pages, 2 figures, 2 tables, along with 2 supplementary materials", "summary": "Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u7eaf\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u548c\u975e\u5c40\u57df\u4e0a\u4e0b\u6587\u805a\u5408\u53ef\u4ee5\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u800c\u5b88\u6052\u5b9a\u5f8b\u4f1a\u5bfc\u81f4\u6839\u672c\u6027\u5931\u8d25\u3002\u8bed\u8a00\u751f\u6210\u88ab\u786e\u7acb\u4e3a\u8017\u6563\u91cf\u5b50\u573a\u8bba\u3002", "motivation": "\u63a2\u7d22\u7eaf\u673a\u68b0\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u7814\u7a76\u91cf\u5b50\u52a8\u529b\u5b66\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u8017\u6563\u4e0e\u975e\u5c40\u57df\u6027\u5bf9\u667a\u80fd\u6d8c\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5177\u6709\u5c01\u95ed\u5f62\u5f0f\u8def\u5f84\u79ef\u5206\u4f20\u64ad\u5b50\u7684Koopman\u7b97\u5b50\uff0c\u5206\u6790\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u975e\u5c40\u57df\u4e0a\u4e0b\u6587\u805a\u5408\u3002\u901a\u8fc7\u8c31\u5206\u6790\u63ed\u793a\u7279\u5f81\u503c\u7ed3\u6784\uff0c\u5305\u62ec\u8870\u51cf\u6a21\u5f0f\uff08\u9057\u5fd8\uff09\u3001\u589e\u957f\u6a21\u5f0f\uff08\u653e\u5927\uff09\u548c\u4e2d\u6027\u6a21\u5f0f\uff08\u4fdd\u6301\uff09\u3002", "result": "\u8bc1\u660e\u4e0d\u53ef\u9006\u8ba1\u7b97\u9700\u8981\u53d7\u63a7\u4fe1\u606f\u8017\u6563\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\u805a\u5408\u3002\u54c8\u5bc6\u987f\u7ea6\u675f\u4f1a\u6d88\u9664\u8017\u6563\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5c3d\u7ba1\u6a21\u578b\u5bb9\u91cf\u4e0d\u53d8\u3002\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u7684\u6587\u672c\u751f\u6210\u3002", "conclusion": "\u8bed\u8a00\u751f\u6210\u662f\u8017\u6563\u91cf\u5b50\u573a\u8bba\uff0c\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u548c\u975e\u5c40\u57df\u6027\u7684\u7ed3\u5408\u83b7\u5f97\u667a\u80fd\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u5b88\u6052\u5b9a\u5f8b\u3002\u8fd9\u4e3a\u7406\u89e3\u667a\u80fd\u6d8c\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.02286", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02286", "abs": "https://arxiv.org/abs/2601.02286", "authors": ["Rahul Sengupta", "Nooshin Yousefzadeh", "Manav Sanghvi", "Yash Ranjan", "Anand Rangarajan", "Sanjay Ranka", "Yashaswi Karnati", "Jeremy Dilmore", "Tushar Patel", "Ryan Casburn"], "title": "BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation", "comment": "6 pages, 10 figures", "summary": "With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO\", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.", "AI": {"tldr": "BigSUMO\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u4ea4\u901a\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u901a\u6570\u636e\u5206\u6790\u3001\u4e2d\u65ad\u68c0\u6d4b\u548c\u5e76\u884c\u4eff\u771f\uff0c\u5e2e\u52a9\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u5168\u7403\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6548\u7ba1\u7406\u5bf9\u4ea4\u901a\u673a\u6784\u548c\u57ce\u5e02\u89c4\u5212\u8005\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u80fd\u591f\u5206\u6790\u5927\u91cf\u4ea4\u901a\u6570\u636e\u5e76\u5236\u5b9a\u6709\u6548\u5e72\u9884\u63aa\u65bd\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86BigSUMO\u6846\u67b6\uff0c\u8be5\u7cfb\u7edf\u6444\u5165\u9ad8\u5206\u8fa8\u7387\u73af\u5f62\u68c0\u6d4b\u5668\u548c\u4fe1\u53f7\u72b6\u6001\u6570\u636e\uff0c\u4ee5\u53ca\u7a00\u758f\u7684\u8f68\u8ff9\u6570\u636e\u3002\u9996\u5148\u8fdb\u884c\u63cf\u8ff0\u6027\u5206\u6790\u548c\u4e2d\u65ad\u68c0\u6d4b\uff0c\u7136\u540e\u4f7f\u7528SUMO\u5fae\u89c2\u4eff\u771f\u5668\u8fdb\u884c\u9884\u6d4b\u6027\u5206\u6790\uff0c\u6d4b\u8bd5\u6570\u767e\u79cd\u5047\u8bbe\u573a\u666f\u4ee5\u4f18\u5316\u4ea4\u901a\u6027\u80fd\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u96c6\u6210\u4e0d\u540c\u7684\u6570\u636e\u5904\u7406\u548c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u3002", "result": "BigSUMO\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f00\u6e90\u8f6f\u4ef6\u548c\u5e93\u6784\u5efa\u7684\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u7ba1\u9053\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002", "conclusion": "BigSUMO\u6709\u671b\u6210\u4e3a\u5f00\u53d1\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u7684\u6709\u4ef7\u503c\u5de5\u5177\uff0c\u5e2e\u52a9\u4ea4\u901a\u673a\u6784\u548c\u57ce\u5e02\u89c4\u5212\u8005\u66f4\u597d\u5730\u7ba1\u7406\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2601.00862", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00862", "abs": "https://arxiv.org/abs/2601.00862", "authors": ["Joey Chan", "Huan Wang", "Haoyu Pan", "Wei Wu", "Zirong Wang", "Zhen Chen", "Ershun Pan", "Min Xie", "Lifeng Xi"], "title": "Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions", "comment": "Due to space limitations, the open-source method for supporting materials is currently under discussion", "summary": "Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\\,^{\\circ}\\mathrm{C}$ to $45\\,^{\\circ}\\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u7535\u6c60\u5bb9\u91cf\u8870\u51cf\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u5305\u542b1704\u4e2a\u7535\u6c60\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8de8\u5316\u5b66\u4f53\u7cfb\u3001\u5bb9\u91cf\u5c3a\u5ea6\u548c\u5de5\u51b5\u7684\u7a33\u5b9a\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7535\u6c60\u5bb9\u91cf\u8870\u51cf\u9884\u6d4b\u5bf9\u50a8\u80fd\u7cfb\u7edf\u7684\u5b89\u5168\u3001\u53ef\u9760\u548c\u957f\u671f\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u5316\u5b66\u4f53\u7cfb\u3001\u5f62\u6001\u548c\u5de5\u51b5\u7684\u5f3a\u5f02\u8d28\u6027\u4f7f\u5f97\u5355\u4e00\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u57df\u4e4b\u5916\u3002", "method": "\u6536\u96c620\u4e2a\u516c\u5f00\u8001\u5316\u6570\u636e\u96c6\u6784\u5efa\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff081704\u4e2a\u7535\u6c60\uff0c396\u4e07\u5145\u653e\u7535\u5faa\u73af\u6bb5\uff09\uff0c\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u7269\u7406\u5f15\u5bfc\u7684\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u6765\u6355\u6349\u5171\u4eab\u7684\u9000\u5316\u6a21\u5f0f\u3002", "result": "\u5728\u5df2\u89c1\u548c\u523b\u610f\u4fdd\u7559\u7684\u672a\u89c1\u6570\u636e\u96c6\u4e0a\uff0c\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u76f8\u6bd4\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u51c6\u786e\u5ea6\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u7684\u5316\u5b66\u4f53\u7cfb\u3001\u5bb9\u91cf\u5c3a\u5ea6\u548c\u5de5\u51b5\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eTSFM\u7684\u67b6\u6784\u5c55\u793a\u4e86\u4f5c\u4e3a\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u5bb9\u91cf\u8870\u51cf\u9884\u6d4b\u7684\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u8de8\u591a\u6837\u7535\u6c60\u7c7b\u578b\u548c\u5de5\u51b5\u7684\u7a33\u5065\u9884\u6d4b\u3002"}}
{"id": "2601.02311", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02311", "abs": "https://arxiv.org/abs/2601.02311", "authors": ["Deep Pankajbhai Mehta"], "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies", "comment": "8 pages, 3 tables", "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.", "AI": {"tldr": "\u63d0\u51faplacement semantics\u6846\u67b6\uff0c\u4ec5\u901a\u8fc7\u56db\u79cd\u8bad\u7ec3\u72b6\u6001\u5728\u8bbe\u5907\u4e0a\u7684\u4e94\u79cd\u653e\u7f6e\u6a21\u5f0f\uff0c\u5c31\u80fd\u63a8\u5bfc\u51fa\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u91cf\uff0c\u7edf\u4e00\u4e86\u5404\u79cd\u5e76\u884c\u7b56\u7565\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5728\u591a\u4e2a\u52a0\u901f\u5668\u4e0a\u5206\u914d\u8ba1\u7b97\uff0c\u4f46\u5b9e\u8df5\u8005\u901a\u8fc7\u8bd5\u9519\u9009\u62e9\u5e76\u884c\u7b56\u7565\uff08\u6570\u636e\u3001\u5f20\u91cf\u3001\u6d41\u6c34\u7ebf\u3001ZeRO\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7cfb\u7edf\u6846\u67b6\u6765\u9884\u6d4b\u5b83\u4eec\u7684\u884c\u4e3a\u3002", "method": "\u5f15\u5165placement semantics\uff1a\u6bcf\u4e2a\u7b56\u7565\u901a\u8fc7\u56db\u79cd\u8bad\u7ec3\u72b6\u6001\uff08\u53c2\u6570\u3001\u4f18\u5316\u5668\u3001\u68af\u5ea6\u3001\u6fc0\u6d3b\uff09\u5728\u8bbe\u5907\u4e0a\u4f7f\u7528\u4e94\u79cd\u6a21\u5f0f\uff08\u590d\u5236\u3001\u5206\u7247\u3001\u5206\u7247-\u805a\u96c6\u3001\u7269\u5316\u3001\u5378\u8f7d\uff09\u6765\u6307\u5b9a\u3002\u4ec5\u4ece\u653e\u7f6e\u65b9\u5f0f\u5c31\u80fd\u63a8\u5bfc\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u91cf\u3002", "result": "\u9884\u6d4b\u7ed3\u679c\u4e0e\u5df2\u53d1\u8868\u7ed3\u679c\u5b8c\u5168\u5339\u914d\uff1aZeRO-3\u6bd4\u6570\u636e\u5e76\u884c\u5c11\u75288\u500d\u5185\u5b58\uff0c\u901a\u4fe1\u6210\u672c\u589e\u52a01.5\u500d\u3002\u8bc1\u660e\u4e86\u68af\u5ea6\u5b8c\u6574\u6027\u548c\u72b6\u6001\u4e00\u81f4\u6027\u662f\u5206\u5e03\u5f0f\u8bad\u7ec3\u5339\u914d\u5355\u8bbe\u5907\u7ed3\u679c\u7684\u5fc5\u8981\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b89\u5168\u7ec4\u5408\u7b56\u7565\u7684\u89c4\u5219\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86ZeRO Stages 1-3\u3001FSDP\u3001\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\uff0c\u5c06\u5b83\u4eec\u89c6\u4e3a\u5177\u6709\u4e0d\u540c\u653e\u7f6e\u9009\u62e9\u7684\u5b9e\u4f8b\uff0c\u4e3a\u7cfb\u7edf\u5316\u9009\u62e9\u5e76\u884c\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.00863", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.00863", "abs": "https://arxiv.org/abs/2601.00863", "authors": ["Markus J. Buehler"], "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery", "comment": null, "summary": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51famateriomusic\u751f\u6210\u6846\u67b6\uff0c\u5c06\u7269\u8d28\u5c42\u6b21\u7ed3\u6784\u4e0e\u97f3\u4e50\u521b\u4f5c\u903b\u8f91\u8fde\u63a5\uff0c\u901a\u8fc7\u53ef\u9006\u6620\u5c04\u4f7f\u58f0\u97f3\u6210\u4e3a\u79d1\u5b66\u63a2\u9488\uff0c\u63ed\u793a\u79d1\u5b66\u4e0e\u827a\u672f\u5728\u7ea6\u675f\u4e0b\u7684\u521b\u9020\u6027\u672c\u8d28\u3002", "motivation": "\u63a2\u7d22\u7269\u8d28\u7ed3\u6784\u4e0e\u97f3\u4e50\u521b\u4f5c\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5efa\u7acb\u58f0\u97f3\u4f5c\u4e3a\u79d1\u5b66\u63a2\u9488\u7684\u65b9\u6cd5\u8bba\uff0c\u63ed\u793a\u79d1\u5b66\u4e0e\u827a\u672f\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u521b\u9020\u6027\u8fc7\u7a0b\u7684\u5171\u540c\u672c\u8d28\u3002", "method": "\u4f7f\u7528\u53ef\u9006\u6620\u5c04\u65b9\u6cd5\uff1a1) \u5206\u5b50\u5149\u8c31\u6620\u5c04\u5230\u97f3\u8c03\uff1b2) \u4e09\u7ef4\u7f51\u7edc\u6620\u5c04\u5230\u53ef\u6f14\u594f\u4e50\u5668\uff1b3) \u679a\u4e3e\u6240\u67092^12\u97f3\u4e50\u97f3\u9636\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff1b4) \u57fa\u4e8e\u7fa4\u4f53\u7684AI\u6a21\u578b\u8fdb\u884c\u97f3\u4e50\u521b\u4f5c\u3002", "result": "\u53d1\u73b0\u6587\u5316\u91cd\u8981\u97f3\u4e50\u7cfb\u7edf\u805a\u96c6\u5728\u4e2d\u71b5\u3001\u4e2d\u7f3a\u9677\u8d70\u5eca\uff0c\u4e0e\u6750\u6599\u79d1\u5b66\u4e2d\u7684Hall-Petch\u6700\u4f18\u7f3a\u9677\u5bc6\u5ea6\u76f4\u63a5\u5bf9\u5e94\uff1bAI\u521b\u4f5c\u97f3\u4e50\u5c55\u73b0\u4eba\u7c7b\u7ed3\u6784\u7279\u5f81\uff1b\u632f\u52a8\u6210\u4e3a\u8de8\u5c3a\u5ea6\u7ed3\u6784\u7ec4\u7ec7\u7684\u5171\u4eab\u8bed\u6cd5\u3002", "conclusion": "\u79d1\u5b66\u4e0e\u827a\u672f\u90fd\u662f\u5728\u7ea6\u675f\u4e0b\u7684\u751f\u6210\u6027\u4e16\u754c\u6784\u5efa\u884c\u4e3a\uff0c\u632f\u52a8\u662f\u8de8\u5c3a\u5ea6\u7ed3\u6784\u7ec4\u7ec7\u7684\u5171\u4eab\u8bed\u6cd5\uff0c\u9009\u62e9\u6027\u4e0d\u5b8c\u7f8e\u662f\u6062\u590d\u8fde\u8d2f\u6027\u4e0e\u9002\u5e94\u6027\u5e73\u8861\u7684\u673a\u5236\uff0c\u521b\u9020\u6027\u6e90\u4e8e\u7ea6\u675f\u4e0e\u81ea\u7531\u5ea6\u6269\u5c55\u7684\u78b0\u649e\u3002"}}
{"id": "2601.00864", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00864", "abs": "https://arxiv.org/abs/2601.00864", "authors": ["Clemens Damke", "Eyke H\u00fcllermeier"], "title": "Distribution Matching for Graph Quantification Under Structural Covariate Shift", "comment": "17 pages, presented at ECML-PKDD 2025", "summary": "Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.", "AI": {"tldr": "\u5c06KDEy\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230\u56fe\u6570\u636e\uff0c\u901a\u8fc7\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u5904\u7406\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u7ed3\u6784\u504f\u79fb\u95ee\u9898", "motivation": "\u5728\u56fe\u6570\u636e\u4e2d\uff0c\u4f20\u7edf\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5148\u9a8c\u6982\u7387\u504f\u79fb\u5047\u8bbe\u5728\u7ed3\u6784\u504f\u79fb\u60c5\u51b5\u4e0b\u4e0d\u6210\u7acb\uff0c\u9700\u8981\u4e13\u95e8\u5904\u7406\u56fe\u7ed3\u6784\u53d8\u5316\u5e26\u6765\u7684\u5206\u5e03\u53d8\u5316", "method": "\u5c06\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u601d\u60f3\u6269\u5c55\u5230KDEy\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6743\u91cd\u6765\u9002\u5e94\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u7ed3\u6784\u504f\u79fb\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6807\u51c6\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5c06\u7ed3\u6784\u91cd\u8981\u6027\u91c7\u6837\u4e0eKDEy\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u7ed3\u6784\u504f\u79fb\u4e0b\u7684\u91cf\u5316\u5b66\u4e60\u95ee\u9898"}}
{"id": "2601.01649", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01649", "abs": "https://arxiv.org/abs/2601.01649", "authors": ["Umesh Vangapally", "Wenhan Wu", "Chen Chen", "Zhishuai Guo"], "title": "Communication-Efficient Federated AUC Maximization with Cyclic Client Participation", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-\u0141ojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/\u03b5^3)$ and an iteration complexity of $O(1/\u03b5^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u8054\u90a6AUC\u6700\u5927\u5316\u7b97\u6cd5\uff0c\u5728\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u4e0b\u8fbe\u5230$\\widetilde{O}(1/\u03b5^{1/2})$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$\\widetilde{O}(1/\u03b5)$\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5728\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u8fbe\u5230$O(1/\u03b5^3)$\u901a\u4fe1\u590d\u6742\u5ea6\uff0c\u5728PL\u6761\u4ef6\u4e0b\u53ef\u63d0\u5347\u81f3$\\widetilde{O}(1/\u03b5^{1/2})$\u3002", "motivation": "\u73b0\u6709\u8054\u90a6AUC\u6700\u5927\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5ba2\u6237\u7aef\u5b8c\u5168\u53ef\u7528\uff0c\u4f46\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5ba2\u6237\u7aef\u901a\u5e38\u4ee5\u5faa\u73af\u65b9\u5f0f\u53c2\u4e0e\u8bad\u7ec3\uff0c\u8fd9\u7ed9\u4e0d\u53ef\u5206\u89e3\u7684AUC\u76ee\u6807\u5e26\u6765\u4e86\u72ec\u7279\u7684\u4f18\u5316\u6311\u6218\u3002", "method": "\u9488\u5bf9\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u7684\u8054\u90a6AUC\u6700\u5927\u5316\u95ee\u9898\uff0c\u7814\u7a76\u4e24\u79cd\u8bbe\u7f6e\uff1a1\uff09\u4f7f\u7528\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\uff0c\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u975e\u51f8-\u5f3a\u51f9\u6781\u5c0f\u6781\u5927\u4f18\u5316\uff0c\u5229\u7528Polyak-\u0141ojasiewicz\u6761\u4ef6\uff1b2\uff09\u8003\u8651\u4e00\u822c\u6210\u5bf9AUC\u635f\u5931\uff0c\u5efa\u7acb\u901a\u4fe1\u548c\u8fed\u4ee3\u590d\u6742\u5ea6\u754c\u9650\u3002", "result": "\u5728\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u4e0b\u8fbe\u5230$\\widetilde{O}(1/\u03b5^{1/2})$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$\\widetilde{O}(1/\u03b5)$\u8fed\u4ee3\u590d\u6742\u5ea6\uff1b\u5728\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u8fbe\u5230$O(1/\u03b5^3)$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$O(1/\u03b5^4)$\u8fed\u4ee3\u590d\u6742\u5ea6\uff0cPL\u6761\u4ef6\u4e0b\u53ef\u63d0\u5347\u81f3$\\widetilde{O}(1/\u03b5^{1/2})$\u901a\u4fe1\u590d\u6742\u5ea6\u548c$\\widetilde{O}(1/\u03b5)$\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u533b\u5b66\u6210\u50cf\u548c\u6b3a\u8bc8\u68c0\u6d4b\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\u7684\u8054\u90a6AUC\u6700\u5927\u5316\u95ee\u9898\u3002"}}
{"id": "2601.00866", "categories": ["cs.LG", "cs.AI", "math.DS"], "pdf": "https://arxiv.org/pdf/2601.00866", "abs": "https://arxiv.org/abs/2601.00866", "authors": ["Shivani Saini", "Ramesh Kumar Vats", "Arup Kumar Sahoo"], "title": "A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam", "comment": "31 pages", "summary": "Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8f85\u52a9\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(A-PINN)\u6846\u67b6\uff0c\u7ed3\u5408\u5e73\u8861\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u7ed3\u6784\u632f\u52a8\u5206\u6790\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u81f3\u5c1140%\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u53ca\u5176\u53d8\u4f53\u5728\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6b63\u53cd\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u4ee5\u51c6\u786e\u6355\u6349\u7ed3\u6784\u632f\u52a8\u73b0\u8c61\u5e76\u786e\u4fdd\u53ef\u9760\u7684\u9884\u6d4b\u5206\u6790\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u8f85\u52a9\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(A-PINN)\u6846\u67b6\uff0c\u91c7\u7528\u5e73\u8861\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8fd1\u4f3c\u6b27\u62c9-\u4f2f\u52aa\u5229\u6881\u65b9\u7a0b\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6570\u503c\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u589e\u5f3a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u81f3\u5c1140%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5165\u7406\u89e3\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u89e3\u51b3\u632f\u52a8\u95ee\u9898\u65b9\u9762\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6539\u8fdb\u7684A-PINN\u6846\u67b6\u5728\u7ed3\u6784\u632f\u52a8\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.01840", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01840", "abs": "https://arxiv.org/abs/2601.01840", "authors": ["Qiantao Yang", "Liquan Chen", "Mingfu Xue", "Songze Li"], "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack", "comment": "Accepted in AAAI 2026", "summary": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.", "AI": {"tldr": "FedCSPACK\uff1a\u4e00\u79cd\u57fa\u4e8e\u4f59\u5f26\u7a00\u758f\u5316\u53c2\u6570\u6253\u5305\u548c\u53cc\u6743\u91cd\u805a\u5408\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u5ba2\u6237\u7aef\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u6a21\u578b\u5206\u5272\u548c\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u6a21\u578b\u517c\u5bb9\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u901a\u4fe1\u5e26\u5bbd\u548c\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672a\u80fd\u6709\u6548\u5e73\u8861\u6570\u636e\u5f02\u6784\u6027\u5904\u7406\u548c\u6709\u9650\u5ba2\u6237\u7aef\u8d44\u6e90\u4e4b\u95f4\u7684\u77db\u76fe", "method": "\u63d0\u51faFedCSPACK\u65b9\u6cd5\uff1a1\uff09\u5ba2\u6237\u7aef\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u6027\u6253\u5305\u6a21\u578b\u53c2\u6570\u5e76\u9009\u62e9\u8d21\u732e\u6700\u5927\u7684\u53c2\u6570\u5305\u8fdb\u884c\u5171\u4eab\uff0c\u51cf\u5c11\u5e26\u5bbd\u9700\u6c42\uff1b2\uff09\u5ba2\u6237\u7aef\u751f\u6210\u57fa\u4e8e\u5171\u4eab\u53c2\u6570\u5305\u7684\u63a9\u7801\u77e9\u9635\uff0c\u63d0\u9ad8\u670d\u52a1\u5668\u4e0a\u7a00\u758f\u66f4\u65b0\u7684\u5bf9\u9f50\u548c\u805a\u5408\u6548\u7387\uff1b3\uff09\u5728\u63a9\u7801\u4e2d\u5d4c\u5165\u65b9\u5411\u548c\u5206\u5e03\u8ddd\u79bb\u6743\u91cd\uff0c\u5b9e\u73b0\u52a0\u6743\u5f15\u5bfc\u805a\u5408\u673a\u5236", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5341\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedCSPACK\u5728\u4fdd\u6301\u9ad8\u6a21\u578b\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387", "conclusion": "FedCSPACK\u901a\u8fc7\u4f59\u5f26\u7a00\u758f\u5316\u53c2\u6570\u6253\u5305\u548c\u53cc\u6743\u91cd\u805a\u5408\u673a\u5236\uff0c\u6709\u6548\u5229\u7528\u6709\u9650\u7684\u5ba2\u6237\u7aef\u8d44\u6e90\uff0c\u51cf\u5c11\u6570\u636e\u5f02\u6784\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5728\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861"}}
{"id": "2601.00868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00868", "abs": "https://arxiv.org/abs/2601.00868", "authors": ["Aditya Sreevatsa K", "Arun Kumar Raveendran", "Jesrael K Mani", "Prakash G Shigli", "Rajkumar Rangadore", "Narayana Darapaneni", "Anwesh Reddy Paduri"], "title": "SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation", "comment": null, "summary": "SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.", "AI": {"tldr": "SmartFlow\u662f\u4e00\u4e2a\u591a\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548cAgentic AI\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6218\u7565DQN\u5b66\u4e60\u3001\u6218\u672f\u4f18\u5316\u548c\u57fa\u4e8eLLM\u7684\u901a\u4fe1\u5c42\uff0c\u663e\u8457\u51cf\u5c11\u7f51\u7edc\u4e0d\u5e73\u8861\u5e76\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u670d\u52a1\u4e2d\u7684\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u51cf\u5c11\u7a7a\u95f2\u65f6\u95f4\u3001\u63d0\u9ad8\u5355\u8f66\u53ef\u7528\u6027\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5e76\u5f25\u5408\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u64cd\u4f5c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u591a\u5c42\u67b6\u6784\uff1a\u6218\u7565\u5c42\u4f7f\u7528DQN\u5728\u7ebd\u7ea6Citi Bike\u7f51\u7edc\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u5b66\u4e60\u518d\u5e73\u8861\u7b56\u7565\uff1b\u6218\u672f\u5c42\u4f18\u5316\u591a\u6bb5\u884c\u7a0b\u548c\u8c03\u5ea6\uff1b\u901a\u4fe1\u5c42\u4f7f\u7528\u57fa\u4e8eLLM\u7684Agentic AI\u5c06\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6307\u4ee4\u3002", "result": "\u5728\u591a\u6b21\u79cd\u5b50\u8fd0\u884c\u8bc4\u4f30\u4e2d\uff0cSmartFlow\u5c06\u7f51\u7edc\u4e0d\u5e73\u8861\u51cf\u5c11\u8d85\u8fc795%\uff0c\u540c\u65f6\u9700\u8981\u6700\u5c0f\u7684\u884c\u9a76\u8ddd\u79bb\uff0c\u5e76\u5b9e\u73b0\u5f3a\u5927\u7684\u5361\u8f66\u5229\u7528\u7387\u3002", "conclusion": "SmartFlow\u4e3a\u590d\u6742\u57ce\u5e02\u79fb\u52a8\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684AI\u9a71\u52a8\u7269\u6d41\u89e3\u51b3\u65b9\u6848\u84dd\u56fe\uff0c\u6210\u529f\u6574\u5408\u4e86\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u64cd\u4f5c\u3002"}}
{"id": "2601.00873", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00873", "abs": "https://arxiv.org/abs/2601.00873", "authors": ["Osasumwen Cedric Ogiesoba-Eguakun", "Suman Rath"], "title": "Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems", "comment": "10 pages", "summary": "Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7528\u4e8e\u68c0\u6d4b\u5fae\u7535\u7f51\u4e2d\u7684\u534f\u8c03\u9690\u853d\u653b\u51fb\uff0c\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\u5728\u4f4e\u7ef4\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u7ecf\u5178SVM\u57fa\u7ebf\u6709\u9002\u5ea6\u63d0\u5347\u3002", "motivation": "\u534f\u8c03\u9690\u853d\u653b\u51fb\u662f\u5206\u5e03\u5f0f\u53d1\u7535\u7cfb\u7edf\u7684\u4e25\u91cd\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u5b83\u4eec\u4fee\u6539\u63a7\u5236\u548c\u6d4b\u91cf\u4fe1\u53f7\u4f46\u4fdd\u6301\u63a5\u8fd1\u6b63\u5e38\u884c\u4e3a\uff0c\u4f7f\u5f97\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u3002\u9700\u8981\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9ad8\u8d28\u91cf\u6a21\u62df\u6d4b\u91cf\u6570\u636e\u521b\u5efa\u5e73\u8861\u7684\u4e8c\u5143\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u7279\u5f81\uff1aDG1\u7684\u65e0\u529f\u529f\u7387\u3001\u76f8\u5bf9\u4e8e\u6807\u79f0\u503c\u7684\u9891\u7387\u504f\u5dee\u548c\u7aef\u7535\u538b\u5e45\u503c\u3002\u8bc4\u4f30\u4e86\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u3001\u5b8c\u5168\u91cf\u5b50\u53d8\u5206\u5206\u7c7b\u5668\u548c\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\u3002", "result": "\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u6a21\u578b\uff08\u91cf\u5b50\u7279\u5f81\u5d4c\u5165\u4e0e\u7ecf\u5178RBF\u652f\u6301\u5411\u91cf\u673a\u7ed3\u5408\uff09\u5728\u4f4e\u7ef4\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u5927\u7684\u7ecf\u5178SVM\u57fa\u7ebf\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u9002\u5ea6\u63d0\u5347\u3002\u5b8c\u5168\u91cf\u5b50\u6a21\u578b\u7531\u4e8e\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u5f53\u524dNISQ\u786c\u4ef6\u7684\u9650\u5236\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u8bad\u7ec3\u66f4\u53ef\u9760\uff0c\u8868\u660e\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u5373\u4f7f\u5728\u5b8c\u5168\u91cf\u5b50\u5b66\u4e60\u5c1a\u4e0d\u5b9e\u7528\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u589e\u5f3a\u5165\u4fb5\u68c0\u6d4b\u80fd\u529b\u3002\u8fd9\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2601.00874", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.00874", "abs": "https://arxiv.org/abs/2601.00874", "authors": ["M. Rizki Oktavian"], "title": "LLMize: A Framework for Large Language Model-Based Numerical Optimization", "comment": null, "summary": "Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.", "AI": {"tldr": "LLMize\u662f\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u5019\u9009\u89e3\u3001\u5916\u90e8\u8bc4\u4f30\u548c\u53cd\u9988\u6539\u8fdb\u7684\u9ed1\u76d2\u8fc7\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u8d85\u8d8a\u4f20\u7edf\u8bed\u8a00\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u542f\u53d1\u4e86\u5c06\u5176\u5e94\u7528\u4e8e\u6570\u503c\u4f18\u5316\u7684\u53ef\u80fd\u6027\u3002\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u6570\u5b66\u7f16\u7a0b\u6216\u5143\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cLLMize\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8ba9\u4ece\u4e1a\u8005\u80fd\u591f\u5b9a\u4e49\u590d\u6742\u4f18\u5316\u95ee\u9898\u3002", "method": "LLMize\u5c06\u4f18\u5316\u6784\u5efa\u4e3a\u9ed1\u76d2\u8fc7\u7a0b\uff1a\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u751f\u6210\u5019\u9009\u89e3\uff0c\u901a\u8fc7\u5916\u90e8\u76ee\u6807\u51fd\u6570\u8bc4\u4f30\uff0c\u5229\u7528\u89e3-\u5206\u6570\u53cd\u9988\u5728\u8fde\u7eed\u8fed\u4ee3\u4e2d\u6539\u8fdb\u3002\u652f\u6301\u591a\u79cd\u4f18\u5316\u7b56\u7565\uff0c\u5305\u62ec\u4f18\u5316\u63d0\u793a\uff08OPRO\uff09\u548c\u53d7\u8fdb\u5316\u7b97\u6cd5\u3001\u6a21\u62df\u9000\u706b\u542f\u53d1\u7684\u6df7\u5408LLM\u65b9\u6cd5\u3002", "result": "\u5728\u51f8\u4f18\u5316\u3001\u7ebf\u6027\u89c4\u5212\u3001\u65c5\u884c\u5546\u95ee\u9898\u3001\u795e\u7ecf\u7f51\u7edc\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6838\u71c3\u6599\u6676\u683c\u4f18\u5316\u7b49\u4efb\u52a1\u4e0a\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u7b80\u5355\u95ee\u9898\uff0cLLM\u4f18\u5316\u4e0d\u5982\u7ecf\u5178\u6c42\u89e3\u5668\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5bf9\u4e8e\u7ea6\u675f\u548c\u542f\u53d1\u5f0f\u96be\u4ee5\u5f62\u5f0f\u5316\u7684\u590d\u6742\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6613\u7528\u7684\u65b9\u6cd5\u3002", "conclusion": "LLMize\u4e3a\u590d\u6742\u3001\u9886\u57df\u7279\u5b9a\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6613\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6ce8\u5165\u7ea6\u675f\u3001\u89c4\u5219\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u4f7f\u4ece\u4e1a\u8005\u65e0\u9700\u6570\u5b66\u7f16\u7a0b\u6216\u5143\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e13\u4e1a\u77e5\u8bc6\u5373\u53ef\u5b9a\u4e49\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2601.00877", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00877", "abs": "https://arxiv.org/abs/2601.00877", "authors": ["Thomas Andrews", "Mark Law", "Sara Ahmadi-Abhari", "Alessandra Russo"], "title": "LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification", "comment": "NeurIPS 2025, Data on the Brain & Mind Workshop", "summary": "We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.", "AI": {"tldr": "LearnAD\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u8111\u90e8MRI\u6570\u636e\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5b66\u4e60\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u7edf\u8ba1\u6a21\u578b/\u51b3\u7b56\u6811/\u968f\u673a\u68ee\u6797/GNN\u8bc6\u522b\u76f8\u5173\u8111\u8fde\u63a5\uff0c\u7136\u540e\u4f7f\u7528FastLAS\u5b66\u4e60\u5168\u5c40\u89c4\u5219\uff0c\u5728\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e0eSVM\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2d\uff0c\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982GNN\uff09\u5728\u8111\u90e8\u75be\u75c5\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u53c8\u80fd\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e2e\u52a9\u533b\u751f\u7406\u89e3\u75be\u75c5\u673a\u5236\u548c\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u3002", "method": "LearnAD\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u3001\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u6216GNN\u8bc6\u522b\u76f8\u5173\u7684\u8111\u8fde\u63a5\u7279\u5f81\uff0c\u7136\u540e\u5e94\u7528FastLAS\uff08\u4e00\u79cd\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7cfb\u7edf\uff09\u4ece\u8fd9\u4e9b\u7279\u5f81\u4e2d\u5b66\u4e60\u5168\u5c40\u53ef\u89e3\u91ca\u89c4\u5219\u3002\u8fd9\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6700\u4f73\u5b9e\u4f8b\u6027\u80fd\u4f18\u4e8e\u51b3\u7b56\u6811\uff0c\u4e0e\u652f\u6301\u5411\u91cf\u673a\u51c6\u786e\u7387\u76f8\u5f53\uff0c\u4ec5\u7565\u4f4e\u4e8e\u4f7f\u7528\u6240\u6709\u7279\u5f81\u7684\u968f\u673a\u68ee\u6797\u548cGNN\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "LearnAD\u5c55\u793a\u4e86\u7b26\u53f7\u5b66\u4e60\u5982\u4f55\u80fd\u591f\u6df1\u5316\u6211\u4eec\u5bf9GNN\u5728\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u4e2d\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u9ad8\u6027\u80fd\u9ed1\u76d2\u6a21\u578b\u4e0e\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.00883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00883", "abs": "https://arxiv.org/abs/2601.00883", "authors": ["Zhongyang Shen"], "title": "Outlier Detection Using Vector Cosine Similarity by Adding a Dimension", "comment": "This is an updated version of the paper originally published in ICAIIC 2024 (DOI: 10.1109/ICAIIC60209.2024.10463442). Changes include minor typographical and grammatical corrections, as well as an added description of an optimized open-source Python implementation (MDOD) available on PyPI at https://pypi.org/project/mdod/", "summary": "We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5411\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u5229\u7528\u89c2\u6d4b\u70b9\u4e0e\u6d4b\u91cf\u70b9\u4e4b\u95f4\u7684\u5411\u91cf\u76f8\u4f3c\u6027\u8bc6\u522b\u5f02\u5e38\u6570\u636e", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u591a\u7ef4\u6570\u636e\u4e2d\u53ef\u80fd\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u76f8\u4f3c\u5ea6\u7684\u65b0\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u8bc6\u522b\u5f02\u5e38\u70b9", "method": "1. \u5411\u539f\u59cb\u6570\u636e\u6dfb\u52a0\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff1b2. \u9009\u62e9\u6d4b\u91cf\u70b9\u5e76\u521b\u5efa\u89c2\u6d4b\u70b9\uff08\u539f\u70b9\uff0c\u4ec5\u5728\u65b0\u7ef4\u5ea6\u6709\u975e\u96f6\u503c\uff09\uff1b3. \u6784\u5efa\u4ece\u89c2\u6d4b\u70b9\u5230\u6d4b\u91cf\u70b9\u53ca\u5176\u4ed6\u70b9\u7684\u5411\u91cf\uff1b4. \u6bd4\u8f83\u8fd9\u4e9b\u5411\u91cf\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8bc6\u522b\u5f02\u5e38", "result": "\u5f00\u53d1\u4e86\u4f18\u5316\u5b9e\u73b0MDOD\u5e76\u53d1\u5e03\u5728PyPI\u4e0a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5411\u91cf\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u673a\u5236\uff0c\u4e3a\u591a\u7ef4\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00889", "abs": "https://arxiv.org/abs/2601.00889", "authors": ["Nalin Dhiman"], "title": "FANoS: Friction-Adaptive Nos\u00e9--Hoover Symplectic Momentum for Stiff Objectives", "comment": "13 pages, 5 figures, 4 tables", "summary": "We study a physics-inspired optimizer, \\emph{FANoS} (Friction-Adaptive Nos\u00e9--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nos\u00e9--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.\n  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\\times 10^{-3}$, and L-BFGS reaches $\\approx 4.4\\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.\n  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.", "AI": {"tldr": "FANoS\u662f\u4e00\u79cd\u53d7\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u3001Nos\u00e9-Hoover\u6052\u6e29\u5668\u548c\u534a\u9690\u5f0f\u79ef\u5206\u5668\uff0c\u5728Rosenbrock\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eAdamW\u548cSGD+momentum\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u4e0d\u7a33\u5b9a\uff0c\u4e0d\u662f\u901a\u7528\u4f18\u5316\u5668\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u53d7\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7ed3\u6784\u4fdd\u6301\u79ef\u5206\u548c\u6052\u6e29\u5668\u601d\u60f3\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u5668\uff0c\u65e8\u5728\u5904\u7406\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u521a\u6027\u8c37\u5730\u95ee\u9898\u3002", "method": "FANoS\u7ed3\u5408\u4e86\uff1a(1) \u4f5c\u4e3a\u79bb\u6563\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u7684\u52a8\u91cf\u66f4\u65b0\uff0c(2) \u4f7f\u7528\u52a8\u80fd\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574\u6807\u91cf\u6469\u64e6\u7cfb\u6570\u7684Nos\u00e9-Hoover\u578b\u6052\u6e29\u5668\u53d8\u91cf\uff0c(3) \u534a\u9690\u5f0f\uff08\u8f9b\u6b27\u62c9\uff09\u79ef\u5206\u5668\uff0c\u53ef\u9009\u914d\u5bf9\u89d2RMS\u9884\u5904\u7406\u5668\u3002", "result": "\u5728Rosenbrock-100D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFANoS-RMS\u8fbe\u52301.74\u00d710\u207b\u00b2\uff0c\u4f18\u4e8e\u672a\u88c1\u526a\u7684AdamW(48.50)\u548cSGD+momentum(90.76)\uff0c\u4f46\u4e0d\u5982\u68af\u5ea6\u88c1\u526a\u7684AdamW(1.87\u00d710\u207b\u00b3)\u548cL-BFGS(\u22484.4\u00d710\u207b\u00b9\u2070)\u3002\u5728\u75c5\u6001\u51f8\u4e8c\u6b21\u95ee\u9898\u548c\u5c0f\u578bPINN\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "conclusion": "FANoS\u662f\u73b0\u6709\u601d\u60f3\u7684\u53ef\u89e3\u91ca\u6027\u7efc\u5408\uff0c\u5728\u67d0\u4e9b\u521a\u6027\u975e\u51f8\u8c37\u5730\u4e2d\u6709\u5e2e\u52a9\uff0c\u4f46\u4e0d\u662f\u73b0\u4ee3\u57fa\u7ebf\u7684\u901a\u7528\u66ff\u4ee3\u54c1\uff0c\u5176\u884c\u4e3a\u5bf9\u6e29\u5ea6\u8c03\u5ea6\u548c\u8d85\u53c2\u6570\u9009\u62e9\u654f\u611f\u3002"}}
{"id": "2601.00892", "categories": ["cs.LG", "cs.CV", "physics.data-an", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00892", "abs": "https://arxiv.org/abs/2601.00892", "authors": ["Ana Carpio", "Gema Duro"], "title": "Hierarchical topological clustering", "comment": "not peer reviewed, reviewed version to appear in Soft Computing", "summary": "Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c42\u6b21\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u53ef\u4f7f\u7528\u4efb\u610f\u8ddd\u79bb\u5ea6\u91cf\uff0c\u901a\u8fc7\u6301\u4e45\u6027\u5206\u6790\u8bc6\u522b\u4efb\u610f\u5f62\u72b6\u7684\u805a\u7c7b\u548c\u5f02\u5e38\u503c", "motivation": "\u62d3\u6251\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u5047\u8bbe\u6570\u636e\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u63a2\u7d22\u6570\u636e\u4e91\uff0c\u4f46\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u7684\u901a\u7528\u805a\u7c7b\u7b97\u6cd5", "method": "\u63d0\u51fa\u5c42\u6b21\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u8ddd\u79bb\u9009\u62e9\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\u63a8\u65ad\u5f02\u5e38\u503c\u548c\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u7684\u6301\u4e45\u6027", "result": "\u5728\u56fe\u50cf\u3001\u533b\u7597\u548c\u7ecf\u6d4e\u6570\u636e\u7b49\u5305\u542b\u76f8\u5173\u5f02\u5e38\u503c\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7b97\u6cd5\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u5176\u4ed6\u6280\u672f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u805a\u7c7b", "conclusion": "\u8be5\u5c42\u6b21\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\u4e3a\u5904\u7406\u590d\u6742\u6570\u636e\u7ed3\u6784\u548c\u5f02\u5e38\u503c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027"}}
{"id": "2601.00894", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00894", "abs": "https://arxiv.org/abs/2601.00894", "authors": ["Gihyeon Sim"], "title": "When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training", "comment": "14 pages, 1 figure, 14 tables, code available at https://github.com/deveworld/ponderTTT", "summary": "Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).", "AI": {"tldr": "PonderTTT\uff1a\u57fa\u4e8e\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u7684\u8bad\u7ec3\u65e0\u5173\u95e8\u63a7\u7b56\u7565\uff0c\u9009\u62e9\u6027\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\uff0c\u5728\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6240\u6709\u8f93\u5165\u91c7\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u4e0d\u8003\u8651\u96be\u5ea6\u5dee\u5f02\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6839\u636e\u8f93\u5165\u96be\u5ea6\u9009\u62e9\u6027\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "method": "\u63d0\u51faPonderTTT\u95e8\u63a7\u7b56\u7565\uff0c\u4f7f\u7528TTT\u5c42\u7684\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u4f5c\u4e3a\u95e8\u63a7\u4fe1\u53f7\u3002\u95e8\u63a7\u51b3\u7b56\u5b8c\u5168\u8bad\u7ec3\u65e0\u5173\uff0c\u65e0\u9700\u5b66\u4e60\u5206\u7c7b\u5668\u6216\u8f85\u52a9\u7f51\u7edc\uff0c\u4ec5\u9700\u5728\u672a\u6807\u8bb0\u6570\u636e\u4e0a\u6821\u51c6\u5355\u4e2a\u6807\u91cf\u9608\u503c\uff0c\u5e76\u901a\u8fc7EMA\u6301\u7eed\u8c03\u6574\u4ee5\u7ef4\u6301\u76ee\u6807\u66f4\u65b0\u7387", "result": "\u5728GPT-2\u6a21\u578b\uff08124M\u52301.5B\u53c2\u6570\uff09\u7684\u4ee3\u7801\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728OOD\u8bed\u8a00\u4e0a\u6bd4\u968f\u673a\u8df3\u8fc7\u57fa\u7ebf\u964d\u4f4e16%\u635f\u5931\uff0c\u8fbe\u523082-89%\u7684Oracle\u6062\u590d\u7387\uff0c\u4e14\u5b8c\u5168\u8bad\u7ec3\u65e0\u5173", "conclusion": "\u81ea\u76d1\u7763\u91cd\u5efa\u635f\u5931\u662f\u63a8\u7406\u517c\u5bb9\u7684\u6709\u6548\u95e8\u63a7\u4fe1\u53f7\uff0cPonderTTT\u80fd\u591f\u9ad8\u6548\u9009\u62e9\u6027\u5730\u89e6\u53d1\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6027\u80fd"}}
{"id": "2601.00898", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00898", "abs": "https://arxiv.org/abs/2601.00898", "authors": ["Ruiming Liang", "Yinan Zheng", "Kexin Zheng", "Tianyi Tan", "Jianxiong Li", "Liyuan Mao", "Zhihao Wang", "Guang Chen", "Hangjun Ye", "Jingjing Liu", "Jinqiao Wang", "Xianyuan Zhan"], "title": "Dichotomous Diffusion Policy Optimization", "comment": null, "summary": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.", "AI": {"tldr": "DIPOLE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u7a33\u5b9a\u5b66\u4e60\u7684\u4e8c\u5206\u7b56\u7565\uff08\u4e00\u4e2a\u6700\u5927\u5316\u5956\u52b1\uff0c\u4e00\u4e2a\u6700\u5c0f\u5316\u5956\u52b1\uff09\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u63a7\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u76f4\u63a5\u6700\u5927\u5316\u4ef7\u503c\u76ee\u6807\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff1b2\uff09\u4f9d\u8d56\u7c97\u7cd9\u7684\u9ad8\u65af\u4f3c\u7136\u8fd1\u4f3c\u9700\u8981\u5927\u91cf\u5c0f\u6b65\u53bb\u566a\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u7a33\u5b9a\u4e14\u53ef\u63a7\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDIPOLE\u7b97\u6cd5\uff1a\u91cd\u65b0\u5ba1\u89c6RL\u4e2d\u7684KL\u6b63\u5219\u5316\u76ee\u6807\uff0c\u8bbe\u8ba1\u8d2a\u5a6a\u5316\u7b56\u7565\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u4e8c\u5206\u7b56\u7565\uff08\u5956\u52b1\u6700\u5927\u5316\u548c\u6700\u5c0f\u5316\u7b56\u7565\uff09\u3002\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u8fd9\u4e24\u4e2a\u7b56\u7565\u7684\u5206\u6570\u6765\u751f\u6210\u4f18\u5316\u52a8\u4f5c\uff0c\u4ece\u800c\u7075\u6d3b\u63a7\u5236\u8d2a\u5a6a\u7a0b\u5ea6\u3002", "result": "\u5728ExORL\u548cOGBench\u7684\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebfRL\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8fd8\u6210\u529f\u8bad\u7ec3\u4e86\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cAD\u57fa\u51c6NAVSIM\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "DIPOLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u53ef\u63a7\u7684\u6269\u6563\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u5927\u578b\u6269\u6563\u7b56\u7565\uff0c\u5e76\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2601.00908", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00908", "abs": "https://arxiv.org/abs/2601.00908", "authors": ["Chorok Lee"], "title": "Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment", "comment": null, "summary": "Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5728COVID-19\u5f15\u53d1\u7684\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u8986\u76d6\u7387\u4e0b\u964d\u7a0b\u5ea6\u5dee\u5f02\u5de8\u5927\uff080%-86.7%\uff09\uff0c\u5355\u7279\u5f81\u4f9d\u8d56\u662f\u707e\u96be\u6027\u5931\u6548\u7684\u5173\u952e\u56e0\u7d20\u3002\u901a\u8fc7SHAP\u5206\u6790\u53d1\u73b0\uff0c\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5ea6\u4e0e\u5931\u6548\u7a0b\u5ea6\u9ad8\u5ea6\u76f8\u5173\uff0c\u5b63\u5ea6\u91cd\u8bad\u7ec3\u53ef\u90e8\u5206\u4fee\u590d\u707e\u96be\u6027\u4efb\u52a1\uff0c\u4f46\u7a33\u5065\u4efb\u52a1\u65e0\u9700\u91cd\u8bad\u7ec3\u3002", "motivation": "\u4fdd\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\u4f1a\u4e0b\u964d\uff0c\u4f46\u4e0d\u540c\u4efb\u52a1\u4e0b\u964d\u7a0b\u5ea6\u5dee\u5f02\u5de8\u5927\u3002\u672c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u7684\u539f\u56e0\uff0c\u5e76\u5f00\u53d1\u5e94\u5bf9\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728COVID-19\u5f15\u53d1\u7684\u4f9b\u5e94\u94fe\u4efb\u52a1\u5206\u5e03\u504f\u79fb\u80cc\u666f\u4e0b\u3002", "method": "\u4f7f\u7528COVID-19\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u5206\u67908\u4e2a\u4f9b\u5e94\u94fe\u4efb\u52a1\u5728\u4e25\u91cd\u7279\u5f81\u53d8\u52a8\uff08Jaccard\u7ea60\uff09\u4e0b\u7684\u8868\u73b0\u3002\u901a\u8fc7SHAP\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u5206\u5e03\uff0c\u8ba1\u7b97\u7279\u5f81\u96c6\u4e2d\u5ea6\u4e0e\u8986\u76d6\u7387\u4e0b\u964d\u7684\u76f8\u5173\u6027\u3002\u6d4b\u8bd5\u5b63\u5ea6\u91cd\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u5e76\u63a2\u7d224\u4e2a\u4e2d\u7b49\u7279\u5f81\u7a33\u5b9a\u6027\u4efb\u52a1\u4ee5\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u8986\u76d6\u7387\u4e0b\u964d\u4ece0%\u523086.7%\u4e0d\u7b49\uff0c\u5dee\u5f02\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\u707e\u96be\u6027\u5931\u6548\u4e0e\u5355\u7279\u5f81\u4f9d\u8d56\u9ad8\u5ea6\u76f8\u5173\uff08rho=0.714, p=0.047\uff09\u3002\u707e\u96be\u6027\u4efb\u52a1\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5728\u5355\u4e00\u7279\u5f81\uff08\u589e\u52a04.5\u500d\uff09\uff0c\u7a33\u5065\u4efb\u52a1\u5219\u5206\u5e03\u5728\u591a\u4e2a\u7279\u5f81\uff0810-20\u500d\uff09\u3002\u5b63\u5ea6\u91cd\u8bad\u7ec3\u5c06\u707e\u96be\u6027\u4efb\u52a1\u8986\u76d6\u7387\u4ece22%\u63d0\u5347\u81f341%\uff08+19pp, p=0.04\uff09\uff0c\u4f46\u5bf9\u7a33\u5065\u4efb\u52a1\u65e0\u76ca\uff0899.8%\u8986\u76d6\u7387\uff09\u3002", "conclusion": "\u7279\u5f81\u7a33\u5b9a\u6027\u800c\u975e\u96c6\u4e2d\u5ea6\u51b3\u5b9a\u6a21\u578b\u7a33\u5065\u6027\uff0c\u4f46\u96c6\u4e2d\u5ea6\u6548\u5e94\u5728\u4e25\u91cd\u504f\u79fb\u4e0b\u7279\u522b\u91cd\u8981\u3002\u63d0\u51fa\u51b3\u7b56\u6846\u67b6\uff1a\u90e8\u7f72\u524d\u76d1\u63a7SHAP\u96c6\u4e2d\u5ea6\uff1b\u82e5\u96c6\u4e2d\u5ea6>40%\u5219\u5b63\u5ea6\u91cd\u8bad\u7ec3\uff1b\u82e5\u7a33\u5065\u5219\u65e0\u9700\u91cd\u8bad\u7ec3\u3002\u8fd9\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4fdd\u5f62\u9884\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2601.00915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00915", "abs": "https://arxiv.org/abs/2601.00915", "authors": ["Jacquelyn Shelton", "Przemyslaw Polewski", "Alexander Robel", "Matthew Hoffman", "Stephen Price"], "title": "Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles", "comment": "draft / preliminary", "summary": "Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.", "AI": {"tldr": "\u63d0\u51faLC-CVAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u6f5c\u5728\u7a7a\u95f4\u5728\u951a\u70b9\u4f4d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u6709\u9650\u6c14\u5019\u6a21\u578b\u8fd0\u884c\u4e2d\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u65b0\u5b9e\u73b0\uff0c\u89e3\u51b3\u4f20\u7edfCVAE\u5728\u8de8\u5b9e\u73b0\u6cdb\u5316\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u6c14\u5019\u6a21\u578b\u96c6\u5408\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u8bb8\u591a\u4e0b\u6e38\u5206\u6790\u9700\u8981\u989d\u5916\u7684\u7edf\u8ba1\u4e00\u81f4\u7684\u6c14\u5019\u53d8\u91cf\u5b9e\u73b0\u3002\u9700\u8981\u4ece\u6709\u9650\u8fd0\u884c\u4e2d\u751f\u6210\u65b0\u5b9e\u73b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6f5c\u5728\u7ea6\u675f\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(LC-CVAE)\uff0c\u5728\u5171\u4eab\u5730\u7406\"\u951a\u70b9\"\u4f4d\u7f6e\u5f3a\u5236\u8de8\u5b9e\u73b0\u6f5c\u5728\u5d4c\u5165\u7684\u540c\u8d28\u6027\uff0c\u7136\u540e\u4f7f\u7528\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u9884\u6d4b\u65b0\u5b9e\u73b0\u4e2d\u672a\u91c7\u6837\u4f4d\u7f6e\u7684\u6f5c\u5728\u5750\u6807\uff0c\u6700\u540e\u89e3\u7801\u751f\u6210\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u573a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a(1)\u5355\u5b9e\u73b0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff1b(2)\u7eb3\u5165\u7ea65\u4e2a\u5b9e\u73b0\u540e\u6536\u76ca\u9012\u51cf\uff1b(3)\u7a7a\u95f4\u8986\u76d6\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u8fd9\u4e0e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5e73\u5747\u90bb\u8fd1\u8ddd\u79bb\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "LC-CVAE\u65b9\u6cd5\u80fd\u6709\u6548\u4ece\u6709\u9650\u6c14\u5019\u6a21\u578b\u8fd0\u884c\u4e2d\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u65b0\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCVAE\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u6c14\u5019\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.00919", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00919", "abs": "https://arxiv.org/abs/2601.00919", "authors": ["Zichuan Fu", "Wentao Song", "Guojing Li", "Yejing Wang", "Xian Wu", "Yimin Deng", "Hanyu Yan", "Yefeng Zheng", "Xiangyu Zhao"], "title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation", "comment": "ICLR 2026 conference", "summary": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLazy Attention\u673a\u5236\uff0c\u901a\u8fc7\u4f4d\u7f6e\u533a\u5206\u548c\u5f39\u6027Softmax\u89e3\u51b3\u6ce8\u610f\u529b\u8fc7\u8f7d\u548c\u6b20\u8f7d\u95ee\u9898\uff0c\u7f13\u89e3\u8868\u793a\u5d29\u6e83\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u548c\u9ad8\u8fbe59.58%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "motivation": "Transformer\u67b6\u6784\u4e2d\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u8868\u793a\u5d29\u6e83\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\u4e24\u4e2a\u5df2\u77e5\u95ee\u9898\uff0c\u5148\u524d\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u5b83\u4eec\u6df1\u5c42\u8054\u7cfb\u7684\u7edf\u4e00\u7406\u89e3\u3002\u672c\u6587\u8ba4\u4e3a\u8fd9\u4e24\u4e2a\u95ee\u9898\u90fd\u6e90\u4e8e\u4e0d\u6070\u5f53\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002", "method": "\u63d0\u51faLazy Attention\u673a\u5236\uff1a1) \u9488\u5bf9\u6ce8\u610f\u529b\u8fc7\u8f7d\u95ee\u9898\uff0c\u91c7\u7528\u8de8\u5934\u548c\u8de8\u7ef4\u5ea6\u7684\u4f4d\u7f6e\u533a\u5206\u6765\u589e\u5f3atoken\u533a\u5206\u5ea6\uff1b2) \u9488\u5bf9\u6ce8\u610f\u529b\u6b20\u8f7d\u95ee\u9898\uff0c\u5f15\u5165\u5f39\u6027Softmax\u5f52\u4e00\u5316\u51fd\u6570\uff0c\u653e\u677e\u6807\u51c6softmax\u7ea6\u675f\u4ee5\u6291\u5236\u5bf9\u65e0\u5173token\u7684\u5173\u6ce8\u3002", "result": "\u5728FineWeb-Edu\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLazy Attention\u6210\u529f\u7f13\u89e3\u4e86\u6ce8\u610f\u529b\u6c89\u6ca1\u95ee\u9898\uff0c\u5728\u4e5d\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6807\u51c6\u6ce8\u610f\u529b\u548c\u73b0\u4ee3\u67b6\u6784\u76f8\u6bd4\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe59.58%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "conclusion": "Lazy Attention\u901a\u8fc7\u7edf\u4e00\u7684\u89c6\u89d2\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u8fc7\u8f7d\u548c\u6b20\u8f7d\u95ee\u9898\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u96c6\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8868\u793a\u5d29\u6e83\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.00920", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00920", "abs": "https://arxiv.org/abs/2601.00920", "authors": ["Xingsheng Chen", "Regina Zhang", "Bo Gao", "Xingwei He", "Xiaofeng Liu", "Pietro Lio", "Kwok-Yan Lam", "Siu-Ming Yiu"], "title": "MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs", "comment": "12 pages, 6 tables", "summary": "Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.", "AI": {"tldr": "MODE\uff1a\u4e00\u4e2a\u7ed3\u5408\u4f4e\u79e9\u795e\u7ecfODE\u4e0e\u589e\u5f3aMamba\u67b6\u6784\u7684\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u52a8\u6001\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8868\u8fbe\u529b\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u6570\u636e\u65f6\u5b58\u5728\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faMODE\u6846\u67b6\uff1a1\uff09\u7ebf\u6027\u6807\u8bb0\u5316\u5c42\u5904\u7406\u8f93\u5165\u5e8f\u5217\uff1b2\uff09\u591a\u4e2a\u589e\u5f3aMamba\u7f16\u7801\u5668\u5757\uff0c\u6bcf\u4e2a\u5305\u542b\u56e0\u679c\u5377\u79ef\u3001SiLU\u6fc0\u6d3b\u548c\u4f4e\u79e9\u795e\u7ecfODE\u589e\u5f3a\uff1b3\uff09\u57fa\u4e8e\u4f2aODE\u52a8\u6001\u7684\u5206\u6bb5\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\uff0c\u81ea\u9002\u5e94\u805a\u7126\u91cd\u8981\u5b50\u5e8f\u5217\uff1b4\uff09\u4f4e\u79e9\u516c\u5f0f\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMODE\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MODE\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u3001Mamba\u9009\u62e9\u6027\u626b\u63cf\u4e0e\u4f4e\u79e9\u795e\u7ecfODE\u7684\u96c6\u6210\u3001\u4ee5\u53ca\u4f4e\u79e9\u8fd1\u4f3c\u548c\u52a8\u6001\u9009\u62e9\u6027\u626b\u63cf\u5e26\u6765\u7684\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00921", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00921", "abs": "https://arxiv.org/abs/2601.00921", "authors": ["Azadeh Alavi", "Hamidreza Khalili", "Stanley H. Chan", "Fatemeh Kouchmeshki", "Ross Vlahos"], "title": "Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease", "comment": "24 pages, 4 figures", "summary": "Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u91cf\u5b50\u6838\u65b9\u6cd5\u548c\u51e0\u4f55\u611f\u77e5SPD\u63cf\u8ff0\u7b26\uff0c\u4ece\u8840\u6db2\u548c\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u6db2\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4bCOPD\u5c0f\u9f20\u6a21\u578b\u7684\u808c\u8089\u529f\u80fd\u6307\u6807\uff0c\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6162\u6027\u963b\u585e\u6027\u80ba\u75c5\uff08COPD\uff09\u5e38\u4f34\u968f\u9aa8\u9abc\u808c\u529f\u80fd\u969c\u788d\uff0c\u4e0e\u5168\u8eab\u548c\u6c14\u9053\u708e\u75c7\u5bc6\u5207\u76f8\u5173\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5fae\u521b\u751f\u7269\u6807\u5fd7\u7269\u7eb5\u5411\u9884\u6d4b\u808c\u8089\u529f\u80fd\u6307\u6807\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u9884\u6d4b\u5de5\u5177\u3002", "method": "\u4f7f\u7528213\u53ea\u52a8\u7269\uff08Sham\u7ec4\u4e0e\u9999\u70df\u70df\u96fe\u66b4\u9732\u7ec4\uff09\u7684\u4e34\u5e8a\u524d\u6570\u636e\u96c6\uff0c\u5305\u542b\u8840\u6db2\u548c\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u6db2\u6d4b\u91cf\u503c\u3002\u6bd4\u8f83\u4e86\u8c03\u4f18\u7684\u7ecf\u5178\u57fa\u7ebf\u65b9\u6cd5\u3001\u51e0\u4f55\u611f\u77e5\u5bf9\u79f0\u6b63\u5b9a\uff08SPD\uff09\u63cf\u8ff0\u7b26\uff08\u4f7f\u7528Stein\u6563\u5ea6\uff09\u4ee5\u53ca\u4e3a\u4f4e\u7ef4\u8868\u683c\u6570\u636e\u8bbe\u8ba1\u7684\u91cf\u5b50\u6838\u6a21\u578b\u3002", "result": "\u5728\u808c\u8089\u91cd\u91cf\u9884\u6d4b\u4e2d\uff0c\u4f7f\u7528\u56db\u4e2a\u53ef\u89e3\u91ca\u8f93\u5165\uff08\u8840\u6db2C\u53cd\u5e94\u86cb\u767d\u3001\u4e2d\u6027\u7c92\u7ec6\u80de\u8ba1\u6570\u3001\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u7ec6\u80de\u6570\u548c\u6761\u4ef6\uff09\u7684\u91cf\u5b50\u6838\u5cad\u56de\u5f52\u83b7\u5f97\u6d4b\u8bd5RMSE\u4e3a4.41mg\uff0cR\u00b2\u4e3a0.605\uff0c\u4f18\u4e8e\u76f8\u540c\u7279\u5f81\u96c6\u7684\u5cad\u56de\u5f52\u57fa\u7ebf\uff084.70mg\uff0c0.553\uff09\u3002\u51e0\u4f55\u611f\u77e5Stein\u6563\u5ea6\u539f\u578b\u8ddd\u79bb\u5728\u4ec5\u4f7f\u7528\u751f\u7269\u6807\u5fd7\u7269\u7684\u8bbe\u7f6e\u4e2d\u4e5f\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb\uff084.55mg vs 4.79mg\uff09\u3002\u7b5b\u67e5\u5f0f\u8bc4\u4f30\uff08\u4ee5\u8bad\u7ec3\u7ec4Sham\u5747\u503c\u76840.8\u500d\u4e3a\u9608\u503c\uff09\u68c0\u6d4b\u4f4e\u808c\u8089\u91cd\u91cf\u7684ROC-AUC\u6700\u9ad8\u8fbe0.90\u3002", "conclusion": "\u51e0\u4f55\u548c\u91cf\u5b50\u6838\u63d0\u5347\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u7684\u751f\u7269\u533b\u5b66\u9884\u6d4b\u95ee\u9898\u4e2d\u80fd\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u7684\u6a21\u578b\u9009\u62e9\uff0c\u4e3aCOPD\u808c\u8089\u529f\u80fd\u969c\u788d\u7684\u9884\u6d4b\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.00924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00924", "abs": "https://arxiv.org/abs/2601.00924", "authors": ["Rares Folea", "Radu Iacob", "Emil Slusanschi", "Traian Rebedea"], "title": "Complexity-based code embeddings", "comment": null, "summary": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u5d4c\u5165\uff0c\u4f7f\u7528XGBoost\u5728Codeforces\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u591a\u6807\u7b7e\u5206\u7c7b", "motivation": "\u9700\u8981\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u793a\uff08\u5d4c\u5165\uff09\uff0c\u4ee5\u4fbf\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u5206\u6790\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7f16\u7a0b\u7ade\u8d5b\u4e2d\u7684\u4ee3\u7801\u7247\u6bb5\u5206\u7c7b\u95ee\u9898", "method": "\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u4ee3\u7801\u5d4c\u5165\uff0c\u4f7f\u7528XGBoost\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b", "result": "\u5728\u5305\u542b11\u4e2a\u7c7b\u522b\u7684\u591a\u6807\u7b7e\u6570\u636e\u96c6\uff08\u57fa\u4e8eCodeforces\u5e73\u53f0\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e86\u5e73\u5747F1\u5206\u6570\uff08\u5177\u4f53\u5206\u6570\u672a\u5728\u6458\u8981\u4e2d\u7ed9\u51fa\uff09", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u7801\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u793a\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4ee3\u7801\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5e94\u7528"}}
{"id": "2601.00932", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00932", "abs": "https://arxiv.org/abs/2601.00932", "authors": ["Andrea Thomas Nava", "Lijo Johny", "Fabio Azzalini", "Johannes Schneider", "Arianna Casanova"], "title": "Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation", "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026)", "summary": "Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \\emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4ea7\u54c1\u5f00\u53d1\u6846\u67b6\uff0c\u4f7f\u7528\u8054\u5408\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u5c5e\u6027\uff0c\u5e76\u5f15\u5165ConfMC\u65b9\u6cd5\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u8986\u76d6\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u4ea7\u54c1\u5f00\u53d1\u9700\u8981\u5927\u91cf\u5b9e\u9a8c\u6765\u63a2\u7d22\u8bbe\u8ba1\u53c2\u6570\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u800c\u591a\u5c5e\u6027\u4f18\u5316\u4e2d\u5c5e\u6027\u95f4\u7684\u76f8\u5173\u6027\u589e\u52a0\u4e86\u590d\u6742\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u5c5e\u6027\u5e76\u63d0\u4f9b\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5\u3002", "method": "1) \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8bbe\u8ba1\u89c4\u683c\u4e0e\u4ea7\u54c1\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b2) \u5e94\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u5bfb\u627e\u6700\u4f18\u8bbe\u8ba1\u7279\u5f81\uff1b3) \u4f7f\u7528\u8054\u5408\u795e\u7ecf\u7f51\u7edc\u6355\u83b7\u591a\u76ee\u6807\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff1b4) \u63d0\u51faConfMC\u65b9\u6cd5\uff08\u7ed3\u5408\u5d4c\u5957\u5171\u5f62\u9884\u6d4b\u548c\u8499\u7279\u5361\u6d1bdropout\uff09\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u3001\u975e\u5747\u5300\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u4e14\u5728\u8c03\u6574\u8986\u76d6\u6c34\u5e73\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4ea7\u54c1\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u5c5e\u6027\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7ConfMC\u6280\u672f\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00933", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.00933", "abs": "https://arxiv.org/abs/2601.00933", "authors": ["Jinyu Xu", "Abhishek K. Umrawal"], "title": "LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection", "comment": "14 pages and 6 figures", "summary": "We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\\unicode{x2014}$called the seed set$\\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.", "AI": {"tldr": "\u63d0\u51faLOFA\u7b97\u6cd5\u7528\u4e8e\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\uff0c\u5728\u5b8c\u5168bandit\u53cd\u9988\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7ecf\u9a8c\u9057\u61be", "motivation": "\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\u4e2d\uff0c\u73b0\u6709\u7b97\u6cd5\u867d\u7136\u5229\u7528\u5b50\u6a21\u6027\u5b9e\u73b0\u4f4e\u9057\u61be\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u964d\u4f4e\u7ecf\u9a8c\u9057\u61be", "method": "\u63d0\u51faLazy Online Forward Algorithm (LOFA)\uff0c\u8fdb\u4e00\u6b65\u5229\u7528\u5f71\u54cd\u529b\u51fd\u6570\u7684\u5b50\u6a21\u6027\u8d28\uff0c\u5728\u5b8c\u5168bandit\u53cd\u9988\u6a21\u578b\u4e0b\u5de5\u4f5c", "result": "\u5728\u771f\u5b9e\u793e\u4ea4\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLOFA\u5728\u7d2f\u79ef\u9057\u61be\u548c\u77ac\u65f6\u5956\u52b1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709bandit\u7b97\u6cd5", "conclusion": "LOFA\u7b97\u6cd5\u901a\u8fc7\u66f4\u5145\u5206\u5730\u5229\u7528\u5b50\u6a21\u6027\u8d28\uff0c\u5728\u5728\u7ebf\u5f71\u54cd\u529b\u6700\u5927\u5316\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2601.00942", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00942", "abs": "https://arxiv.org/abs/2601.00942", "authors": ["Kabir Grover"], "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures", "comment": null, "summary": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.", "AI": {"tldr": "\u7a00\u758fMoE\u67b6\u6784\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u7814\u7a76\uff1a\u6307\u4ee4\u5fae\u8c03\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20", "motivation": "\u968f\u7740\u7a00\u758fMoE\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u3002\u867d\u7136\u6761\u4ef6\u8ba1\u7b97\u5e26\u6765\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\uff0c\u4f46\u7a00\u758f\u8def\u7531\u4e0e\u57fa\u4e8e\u6e29\u5ea6\u7684\u91c7\u6837\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u5426\u4f1a\u635f\u5bb3\u8f93\u51fa\u7a33\u5b9a\u6027\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u8bc4\u4f30\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff1aOLMoE-7B\uff08\u7a00\u758f\u57fa\u7840\u6a21\u578b\uff09\u3001Mixtral-8x7B\uff08\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u548cQwen2.5-3B\uff08\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u3002\u5728\u5177\u6709\u5ba2\u89c2\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u786e\u5b9a\u6027\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6db5\u76d6\u56db\u79cd\u89e3\u7801\u914d\u7f6e\uff08\u4ece\u8d2a\u5a6a\u89e3\u7801\u5230T=1.0\uff09\uff0c\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u683c\u5f0f\u5408\u89c4\u6027\u3001\u91cd\u590d\u751f\u6210\u8f93\u51fa\u4e00\u81f4\u6027\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u603b\u8ba19,360\u6b21\u6a21\u578b\u751f\u6210\u3002", "result": "\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u6240\u6709\u89e3\u7801\u6e29\u5ea6\u4e0b\u8868\u73b0\u51fa\u4e0e\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u76f8\u5f53\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u7a00\u758f\u57fa\u7840\u6a21\u578b\u968f\u7740\u6e29\u5ea6\u5347\u9ad8\u51fa\u73b0\u7cfb\u7edf\u6027\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u8868\u660e\u6307\u4ee4\u5fae\u8c03\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u5bf9\u89e3\u7801\u968f\u673a\u6027\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u662f\u786e\u4fdd\u7a00\u758f\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u6027\u5173\u952e\u5e94\u7528\u4e2d\u7a33\u5b9a\u6027\u7684\u5173\u952e\uff0c\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u7a00\u758f\u67b6\u6784\u53ef\u4ee5\u5b89\u5168\u91c7\u7528\u800c\u4e0d\u727a\u7272\u8f93\u51fa\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.00965", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00965", "abs": "https://arxiv.org/abs/2601.00965", "authors": ["Tianshuo Yang", "Ryan Rabinowitz", "Terrance E. Boult", "Jugal Kalita"], "title": "Adapting Feature Attenuation to NLP", "comment": null, "summary": "Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u7279\u5f81\u8870\u51cf\u5047\u8bbe\u79fb\u690d\u5230\u6587\u672c\u5904\u7406\uff0c\u8bc4\u4f30\u4e86COSTARR\u7b49\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5728\u9ad8\u7c7b\u522b\u6570\u8bbe\u7f6e\u4e0b\u6548\u679c\u6709\u9650\u3002", "motivation": "Transformer\u5206\u7c7b\u5668\uff08\u5982BERT\uff09\u5728\u5c01\u95ed\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9762\u5bf9\u672a\u89c1\u7c7b\u522b\u8f93\u5165\u65f6\u8106\u5f31\uff0c\u8fd9\u662f\u90e8\u7f72NLP\u7cfb\u7edf\u5e38\u89c1\u573a\u666f\u3002\u9700\u8981\u7814\u7a76\u6587\u672c\u7684\u5f00\u653e\u96c6\u8bc6\u522b\u95ee\u9898\uff0c\u63a2\u7d22\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684OSR\u65b9\u6cd5\u5728NLP\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684COSTARR\u6846\u67b6\u9002\u914d\u5230\u4e24\u4e2a\u8bed\u8a00\u6a21\u578b\uff08BERT base\u548cGPT-2\uff09\uff0c\u8bad\u7ec3\u5b83\u4eec\u6807\u6ce8176\u4e2aarXiv\u5b66\u79d1\u9886\u57df\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u6700\u5927softmax\u6982\u7387\uff08MSP\uff09\u3001MaxLogit\u548c\u6e29\u5ea6\u7f29\u653e\u81ea\u7531\u80fd\u5206\u6570\uff0c\u4f7f\u7528OOSA\u548cAUOSCR\u6307\u6807\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "COSTARR\u53ef\u4ee5\u6269\u5c55\u5230NLP\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f46\u5728\u7edf\u8ba1\u4e0a\u6ca1\u6709\u663e\u8457\u4f18\u4e8eMaxLogit\u6216MSP\uff1b\u81ea\u7531\u80fd\u5206\u6570\u5728\u9ad8\u7c7b\u522b\u6570\u8bbe\u7f6e\u4e0b\u843d\u540e\u4e8e\u6240\u6709\u5176\u4ed6\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\u4e86\u5c06\u89c6\u89c9\u4e2d\u5fc3OSR\u601d\u60f3\u79fb\u690d\u5230\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\uff0c\u6307\u51fa\u9700\u8981\u66f4\u5927\u7684\u9aa8\u5e72\u7f51\u7edc\u548c\u4efb\u52a1\u5b9a\u5236\u7684\u8870\u51cf\u7b56\u7565\u3002"}}
{"id": "2601.00968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00968", "abs": "https://arxiv.org/abs/2601.00968", "authors": ["Longwei Wang", "Mohammad Navid Nayyem", "Abdullah Al Rakin", "KC Santosh", "Chaowei Zhang", "Yang Zhou"], "title": "Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks", "comment": "8pages,4 figures", "summary": "The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLIME\u89e3\u91ca\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6291\u5236\u865a\u5047\u7279\u5f81\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u9700\u8981\u65e2\u5bf9\u6297\u6270\u52a8\u9c81\u68d2\u53c8\u51b3\u7b56\u900f\u660e\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u7684\u865a\u5047\u3001\u4e0d\u7a33\u5b9a\u6216\u8bed\u4e49\u65e0\u5173\u7279\u5f81\u4f1a\u663e\u8457\u589e\u52a0\u5bf9\u6297\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5c5e\u6027\u5f15\u5bfc\u7684\u7cbe\u70bc\u6846\u67b6\uff0c\u5c06LIME\u4ece\u88ab\u52a8\u8bca\u65ad\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8bad\u7ec3\u4fe1\u53f7\u3002\u901a\u8fc7\u7279\u5f81\u63a9\u7801\u3001\u654f\u611f\u611f\u77e5\u6b63\u5219\u5316\u548c\u5bf9\u6297\u589e\u5f3a\u6784\u5efa\u95ed\u73af\u7cbe\u70bc\u6d41\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u96c6\u6216\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-10-C\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u89e3\u91ca\u5bf9\u9f50\u4f5c\u4e3a\u4e3b\u52a8\u8bad\u7ec3\u4fe1\u53f7\u7684\u6709\u6548\u6846\u67b6\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u900f\u660e\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.00970", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00970", "abs": "https://arxiv.org/abs/2601.00970", "authors": ["Boris N. Oreshkin", "Mayank Jauhari", "Ravi Kiran Selvam", "Malcolm Wolff", "Wenhao Pan", "Shankar Ramasubramanian", "Kin G. Olivares", "Tatiana Konstantinova", "Andres Potapczynski", "Mengfei Cao", "Dmitry Efimov", "Michael W. Mahoney", "Andrew G. Wilson"], "title": "Zero-shot Forecasting by Simulation Alone", "comment": null, "summary": "Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a \"student-beats-teacher\" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.", "AI": {"tldr": "SarSim0\uff1a\u9996\u4e2a\u5b9e\u7528\u7684\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6a21\u62df\u5668\uff0c\u57fa\u4e8eSARIMA\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u6b65\u6cd5\u751f\u6210\u7a33\u5b9a\u3001\u591a\u5b63\u8282\u6027\u3001\u95f4\u6b47\u6027\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u652f\u6301\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5728M-Series\u548cGiftEval\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u6570\u636e\u6709\u9650\u3001\u8bc4\u4f30\u6613\u6cc4\u9732\u3001\u9690\u79c1\u548c\u8bb8\u53ef\u9650\u5236\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5feb\u901f\u751f\u6210\u6570\u636e\u53c8\u80fd\u5b9e\u73b0\u826f\u597d\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\u7684\u6a21\u62df\u5668\u3002", "method": "\u57fa\u4e8eSARIMA\u6a21\u578b\u7684\u4e09\u6b65\u6a21\u62df\u6d41\u7a0b\uff1a1) \u4ece\u7279\u5f81\u591a\u9879\u5f0f\u7a33\u5b9a\u533a\u57df\u91c7\u6837\u7a33\u5b9a\u8f68\u8ff9\uff1b2) \u901a\u8fc7\u53e0\u52a0\u65b9\u6848\u7ec4\u5408\u591a\u4e2a\u8def\u5f84\u751f\u6210\u591a\u5b63\u8282\u6027\u5e8f\u5217\uff1b3) \u6dfb\u52a0\u57fa\u4e8e\u901f\u7387\u7684\u91cd\u5c3e\u566a\u58f0\u6a21\u578b\u6355\u6349\u7a81\u53d1\u6027\u548c\u95f4\u6b47\u6027\u3002", "result": "SarSim0\u6bd4\u57fa\u4e8e\u6838\u7684\u751f\u6210\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u53ef\u8bad\u7ec3\u7ea610\u4ebf\u4e2a\u6a21\u62df\u5e8f\u5217\uff1b\u795e\u7ecf\u7f51\u7edc\u9aa8\u5e72\u5728\u96f6\u6837\u672c\u534f\u8bae\u4e0b\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u7edf\u8ba1\u9884\u6d4b\u5668\u548c\u8fd1\u671f\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\uff0c\u5728GiftEval\u4e0a\u751a\u81f3\u8d85\u8fc7\u751f\u6210\u8fc7\u7a0bAutoARIMA\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SarSim0\u662f\u9996\u4e2a\u5b9e\u7528\u7684\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u9884\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01003", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01003", "abs": "https://arxiv.org/abs/2601.01003", "authors": ["Amin Abyaneh", "Charlotte Morissette", "Mohamad H. Danesh", "Anas El Houssaini", "David Meger", "Gregory Dudek", "Hsiu-Chin Lin"], "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations", "comment": "Under review at ICLR 2026", "summary": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u901a\u8fc7\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u5f15\u5165\u6536\u7f29\u6027\u6765\u589e\u5f3a\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u867d\u7136\u4f5c\u4e3a\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u7684\u5f3a\u5927\u751f\u6210\u6a21\u578b\uff0c\u4f46\u5176\u57fa\u4e8e\u5206\u6570\u7684SDE\u5efa\u6a21\u4f1a\u5f15\u5165\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u3001\u9700\u8981\u5927\u91cf\u6570\u636e\u3001\u4e14\u52a8\u4f5c\u751f\u6210\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u4e0d\u592a\u5173\u952e\u7684\u95ee\u9898\u5728\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u4f1a\u7d2f\u79ef\u5e76\u5bfc\u81f4\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u8bf1\u5bfc\u6536\u7f29\u884c\u4e3a\u3002\u6536\u7f29\u6027\u5c06\u9644\u8fd1\u7684\u6d41\u62c9\u8fd1\uff0c\u589e\u5f3a\u5bf9\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u52a8\u4f5c\u65b9\u5dee\u3002\u5f00\u53d1\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\u548c\u5b9e\u9645\u5b9e\u73b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6700\u5c0f\u5316\u4fee\u6539\u548c\u8ba1\u7b97\u6210\u672c\u5730\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u7b56\u7565\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDPs\u901a\u5e38\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "\u6536\u7f29\u6269\u6563\u7b56\u7565\u901a\u8fc7\u5f15\u5165\u6536\u7f29\u6027\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u4e3a\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01009", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.01009", "abs": "https://arxiv.org/abs/2601.01009", "authors": ["Mojtaba Aliasghar-Mamaghani", "Mohammadreza Khalafi"], "title": "Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms", "comment": null, "summary": "This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.", "AI": {"tldr": "\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u6df7\u51dd\u571f\u914d\u5408\u6bd4\u5bf9\u6c2f\u79bb\u5b50\u6e17\u900f\u65f6\u95f4\u6f14\u5316\u7684\u5f71\u54cd\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u5bff\u547d\u8bc4\u4f30\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u53d7\u4fb5\u8680\u73af\u5883\u4e2d\u6df7\u51dd\u571f\u7ed3\u6784\u7684\u670d\u5f79\u5bff\u547d\uff0c\u9700\u8981\u7406\u89e3\u6c2f\u79bb\u5b50\u6e17\u900f\u7684\u65f6\u95f4\u6f14\u5316\u89c4\u5f8b\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6df7\u51dd\u571f\u914d\u5408\u6bd4\u4e0e\u6c2f\u79bb\u5b50\u6e17\u900f\u4e4b\u95f4\u7684\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u7b80\u5355\u548c\u590d\u6742\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1a\u7ebf\u6027\u56de\u5f52\u3001KNN\u56de\u5f52\u3001\u6838\u5cad\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u56de\u5f52\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u3001\u591a\u5c42\u611f\u77e5\u673a\u548c\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff0c\u57fa\u4e8e\u7efc\u5408\u6570\u636e\u96c6\u9884\u6d4b\u6c2f\u79bb\u5b50\u6e17\u900f\u3002", "result": "\u6838\u5cad\u56de\u5f52\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u591a\u5c42\u611f\u77e5\u673a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0cGRU\u56e0\u6570\u636e\u591a\u6837\u6027\u672a\u80fd\u51c6\u786e\u9884\u6d4b\u3002\u591a\u6570\u914d\u5408\u6bd4\u7ec4\u5206\u4e0e\u6c2f\u79bb\u5b50\u542b\u91cf\u5448\u8d1f\u76f8\u5173\uff0c\u5c11\u6570\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u6df7\u51dd\u571f\u914d\u5408\u6bd4\u4e0e\u6c2f\u79bb\u5b50\u6e17\u900f\u7684\u6f5c\u5728\u5173\u8054\uff0c\u4e3a\u63cf\u8ff0\u6c2f\u79bb\u5b50\u6e17\u900f\u7269\u7406\u8fc7\u7a0b\u63d0\u4f9b\u66ff\u4ee3\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5ef6\u957f\u57fa\u7840\u8bbe\u65bd\u670d\u5f79\u5bff\u547d\u3002"}}
{"id": "2601.01014", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01014", "abs": "https://arxiv.org/abs/2601.01014", "authors": ["Haoran Su", "Chenyu You"], "title": "Geometric and Dynamic Scaling in Deep Transformers", "comment": "Research Proposal Only", "summary": "Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.", "AI": {"tldr": "\u6df1\u5ea6Transformer\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u672c\u8d28\u4e0a\u662f\u51e0\u4f55\u95ee\u9898\uff0c\u800c\u975e\u4f18\u5316\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u548c\u6df1\u5ea6\u589e\u91cf\u5b66\u4e60\u6765\u907f\u514d\u8868\u793a\u9000\u5316", "motivation": "\u73b0\u6709\u89e3\u91ca\u5c06Transformer\u6df1\u5ea6\u6269\u5c55\u65f6\u7684\u8868\u793a\u5d29\u6e83\u5f52\u56e0\u4e8e\u4f18\u5316\u4e0d\u7a33\u5b9a\u6216\u68af\u5ea6\u6d88\u5931\uff0c\u4f46\u8fd9\u65e0\u6cd5\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728\u73b0\u4ee3\u5f52\u4e00\u5316\u548c\u521d\u59cb\u5316\u65b9\u6848\u4e0b\u5d29\u6e83\u4ecd\u7136\u5b58\u5728\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u4e00\u4e2a\u6839\u672c\u7684\u51e0\u4f55\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff1a1) \u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u9650\u5236\u6b8b\u5dee\u66f4\u65b0\u5230\u6709\u6548\u7684\u5c40\u90e8\u5207\u5411\u65b9\u5411\uff0c\u9632\u6b62\u4e0d\u53d7\u63a7\u5236\u7684\u6d41\u5f62\u6f02\u79fb\uff1b2) \u6df1\u5ea6\u589e\u91cf\u5b66\u4e60\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u975e\u5355\u8c03\u66f4\u65b0\uff0c\u80fd\u591f\u53cd\u5c04\u548c\u64e6\u9664\u5197\u4f59\u7279\u5f81\u800c\u975e\u65e0\u6761\u4ef6\u7d2f\u79ef\u3002", "result": "\u8fd9\u4e9b\u673a\u5236\u89e3\u8026\u4e86\u7279\u5f81\u66f4\u65b0\u7684\u65b9\u5411\u548c\u7b26\u53f7\uff0c\u5b9e\u73b0\u4e86\u8de8\u6df1\u5ea6\u7684\u7a33\u5b9a\u51e0\u4f55\u6f14\u5316\u3002\u7531\u6b64\u4ea7\u751f\u7684\u67b6\u6784\u79f0\u4e3a\u6d41\u5f62\u51e0\u4f55Transformer(MGT)\u3002", "conclusion": "\u5f3a\u5236\u51e0\u4f55\u6709\u6548\u6027\u540c\u65f6\u5141\u8bb8\u52a8\u6001\u64e6\u9664\u5bf9\u4e8e\u907f\u514d\u8d85\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u79e9\u5d29\u6e83\u81f3\u5173\u91cd\u8981\u3002\u51e0\u4f55\u800c\u975e\u6df1\u5ea6\u672c\u8eab\u662f\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002"}}
{"id": "2601.01016", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01016", "abs": "https://arxiv.org/abs/2601.01016", "authors": ["Ata Akbari Asanjan", "Milad Memarzadeh", "Bryan Matthews", "Nikunj Oza"], "title": "Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study", "comment": null, "summary": "In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e2d\u5e94\u7528\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u548c\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u9891\u7387\u539f\u7406\u5206\u6790\u53d1\u73b0RFT\u6a21\u578b\u80fd\u540c\u65f6\u5b66\u4e60\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff0c\u800c\u4f20\u7edfDNN\u53ea\u80fd\u4ece\u4f4e\u9891\u5f00\u59cb\u9010\u6b65\u5b66\u4e60\u9ad8\u9891\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\u5982\u4f55\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e2d\u8fdb\u884c\u57fa\u4e8e\u91cd\u6784\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\u548c\u53ef\u8bad\u7ec3\u7684\u5085\u91cc\u53f6\u53d8\u6362\u53d8\u4f53\uff0c\u901a\u8fc7\u9891\u7387\u539f\u7406\u5206\u6790\u6a21\u578b\u8bad\u7ec3\u884c\u4e3a\uff0c\u4f7f\u7528\u4e24\u4e2a\u4f4e\u7ef4\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u6570\u636e\u8868\u793a\u5206\u6790\uff0c\u4ee5\u53ca\u822a\u7a7a\u5b89\u5168\u6570\u636e\u96c6\u8fdb\u884c\u9ad8\u7ef4\u91cd\u6784\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5085\u91cc\u53f6\u53d8\u6362\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f46\u53ef\u8bad\u7ec3\u5085\u91cc\u53f6\u53d8\u6362\u76f8\u5bf9\u4e8e\u968f\u673a\u53d8\u4f53\u7684\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\u3002RFT\u6a21\u578b\u80fd\u540c\u65f6\u5b66\u4e60\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff0c\u800c\u4f20\u7edfDNN\u53ea\u80fd\u4ece\u4f4e\u9891\u5f00\u59cb\u9010\u6b65\u5b66\u4e60\u3002", "conclusion": "\u5085\u91cc\u53f6\u53d8\u6362\u80fd\u663e\u8457\u6539\u8fdb\u81ea\u7f16\u7801\u5668\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u3002\u867d\u7136\u53ef\u8bad\u7ec3\u5085\u91cc\u53f6\u53d8\u6362\u7684\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\uff0c\u4f46\u968f\u673a\u5085\u91cc\u53f6\u53d8\u6362\u5df2\u663e\u793a\u51fa\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2601.01021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01021", "abs": "https://arxiv.org/abs/2601.01021", "authors": ["Dai Shi", "Lequan Lin", "Andi Han", "Luke Thompson", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Zhiyong Wang", "Junbin Gao"], "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations", "comment": null, "summary": "Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.", "AI": {"tldr": "\u57fa\u4e8eWiener\u6df7\u6c8c\u5c55\u5f00\u7684\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60SDE/SPDE\u7684\u89e3\u7b97\u5b50\uff0c\u901a\u8fc7\u6b63\u4ea4Hermite\u7279\u5f81\u6295\u5f71\u566a\u58f0\u8def\u5f84\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u5b9e\u73b0\u4ece\u566a\u58f0\u5230\u5b8c\u6574\u89e3\u8f68\u8ff9\u7684\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u3002", "motivation": "SDE\u548cSPDE\u662f\u5efa\u6a21\u968f\u673a\u52a8\u529b\u5b66\u7684\u57fa\u672c\u5de5\u5177\uff0c\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u903c\u8fd1\u5176\u89e3\u7b97\u5b50\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u5feb\u901f\u5b9e\u7528\u7684\u6c42\u89e3\u5668\uff0c\u8fd8\u80fd\u4e3a\u7ecf\u5178\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u968f\u673a\u7cfb\u7edf\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u6216\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178Wiener\u6df7\u6c8c\u5c55\u5f00(WCE)\uff0c\u5c06\u9a71\u52a8\u566a\u58f0\u8def\u5f84\u6295\u5f71\u5230\u6b63\u4ea4Wick Hermite\u7279\u5f81\u4e0a\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u5f97\u5230\u7684\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u4ece\u800c\u53ef\u4ee5\u4ece\u566a\u58f0\u76f4\u63a5\u91cd\u6784\u5b8c\u6574\u89e3\u8f68\u8ff9\u3002\u7406\u8bba\u65b9\u9762\uff0c\u4e3a\u591a\u7ef4SDE\u548c\u534a\u7ebf\u6027SPDE\u660e\u786e\u5199\u51fa\u6df7\u6c8c\u7cfb\u6570\u7684\u8026\u5408ODE/PDE\u7cfb\u7edf\uff0c\u4f7f\u968f\u673a\u5f3a\u8feb\u4e0e\u786e\u5b9a\u6027\u52a8\u529b\u5b66\u5206\u79bb\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u9898\u4e0a\u9a8c\u8bc1\u6a21\u578b\uff1a\u7ecf\u5178SPDE\u57fa\u51c6\u6d4b\u8bd5\u3001\u56fe\u50cf\u4e0a\u7684\u6269\u6563\u5355\u6b65\u91c7\u6837\u3001\u56fe\u4e0a\u7684\u62d3\u6251\u63d2\u503c\u3001\u91d1\u878d\u5916\u63a8\u3001\u53c2\u6570\u4f30\u8ba1\u4ee5\u53ca\u6d2a\u6c34\u9884\u6d4b\u7684\u6d41\u5f62SDE\uff0c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7cbe\u5ea6\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "WCE\u57fa\u795e\u7ecf\u7b97\u5b50\u4e3a\u5b66\u4e60SDE/SPDE\u89e3\u7b97\u5b50\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\uff0c\u8868\u660e\u8be5\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.01023", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01023", "abs": "https://arxiv.org/abs/2601.01023", "authors": ["Jo\u00e3o Morais", "Sadjad Alikhani", "Akshay Malhotra", "Shahab Hamidi-Rad", "Ahmed Alkhateeb"], "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning", "comment": "resources available in: https://www.wi-lab.net/research/dataset-similarity", "summary": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u7684\u65e0\u7ebf\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u8de8\u6570\u636e\u96c6\u53ef\u8fc1\u79fb\u6027\uff0c\u5728CSI\u538b\u7f29\u548c\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u4e2d\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u96c6\u7684\u76f8\u4f3c\u6027\u6765\u652f\u6301\u6570\u636e\u96c6\u9009\u62e9/\u589e\u5f3a\u3001\u4eff\u771f\u5230\u771f\u5b9e\u6570\u636e\u6bd4\u8f83\u3001\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u6570\u636e\u751f\u6210\u4ee5\u53ca\u6a21\u578b\u8bad\u7ec3/\u9002\u5e94\u51b3\u7b56\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eUMAP\u5d4c\u5165\u7684\u5ea6\u91cf\u7ed3\u5408Wasserstein\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u5bf9\u4e8e\u76d1\u7763\u4efb\u52a1\u8fd8\u96c6\u6210\u4e86\u76d1\u7763UMAP\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u60e9\u7f5a\u3002", "result": "\u5728\u65e0\u76d1\u7763CSI\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u96c6\u8ddd\u79bb\u4e0e\u8bad\u7ec3-\u6d4b\u8bd5\u6027\u80fd\u7684Pearson\u76f8\u5173\u7cfb\u6570\u8d85\u8fc70.85\uff1b\u5728\u76d1\u7763\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u6807\u7b7e\u611f\u77e5\u8ddd\u79bb\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5ea6\u91cf\u65e0\u7ebf\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u8de8\u6570\u636e\u96c6\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u652f\u6301\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u96c6\u6bd4\u8f83\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u6570\u636e\u96c6\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.01045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01045", "abs": "https://arxiv.org/abs/2601.01045", "authors": ["Tatsuaki Tsuruyama"], "title": "Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI", "comment": null, "summary": "Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.\n  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570\u7684\u6295\u5f71\u53cd\u5411\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u751f\u6210\u6a21\u578b\u4e2d\u63a7\u5236\u7c97\u7c92\u5ea6\u91cf\uff08\u5982\u56fe\u50cf\u5757\u5f3a\u5ea6\uff09\u7684\u6f14\u5316", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u7406\u8bba\u63cf\u8ff0\u7c97\u7c92\u5ea6\u91cf\uff08\u5982\u56fe\u50cf\u5757\u5f3a\u5ea6\uff09\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u6f14\u5316\u89c4\u5f8b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u663e\u5f0f\u63a7\u5236\u8fd9\u4e9b\u91cf\u7684\u65b9\u6cd5", "method": "\u5c06\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570V\u79fb\u690d\u5230\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51faV-delta\u6295\u5f71\u53cd\u5411\u6269\u6563\u65b9\u6848\uff0c\u6269\u5c55\u5355\u8c03\u6027\u5230\u975e\u9f50\u6b21\u5757\u4fdd\u6301\u9a6c\u5c14\u53ef\u592b\u6838", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5c06\u5757\u8d28\u91cf\u8bef\u5dee\u548c\u6cc4\u6f0f\u5bb9\u5fcd\u52bf\u4fdd\u6301\u5728\u9884\u8bbe\u5bb9\u5dee\u5185\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u975e\u6295\u5f71\u52a8\u6001\u76f8\u5f53\u7684\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf", "conclusion": "\u5c06\u751f\u6210\u91c7\u6837\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ece\u566a\u58f0\u5230\u6570\u636e\u7684\u4fe1\u606f\u52bf\u51cf\u5c11\u8fc7\u7a0b\uff0c\u4e3a\u5177\u6709\u663e\u5f0f\u7c97\u7c92\u5ea6\u91cf\u63a7\u5236\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219"}}
{"id": "2601.01061", "categories": ["cs.LG", "cs.AI", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01061", "abs": "https://arxiv.org/abs/2601.01061", "authors": ["Yajing Liu", "Erkao Bao", "Linqi Song"], "title": "A UCB Bandit Algorithm for General ML-Based Estimators", "comment": "15 pages, 4 figures, 1 table, Multi-Arm bandit, psi-UCB, generalized machine learning models", "summary": "We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB", "AI": {"tldr": "\u63d0\u51faML-UCB\u7b97\u6cd5\uff0c\u5c06\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u5230\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u5efa\u6a21\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u5904\u7406\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u90e8\u7f72\u590d\u6742ML\u6a21\u578b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u53ef\u5904\u7406\u7684\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u6765\u8fdb\u884c\u539f\u5219\u6027\u63a2\u7d22\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u7279\u5b9a\u7406\u8bba\u5206\u6790\uff0c\u9650\u5236\u4e86\u5148\u8fdbML\u6a21\u578b\u5728\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5047\u8bbe\u5747\u65b9\u8bef\u5dee\u968f\u8bad\u7ec3\u6837\u672c\u6570\u5448\u5e42\u5f8b\u4e0b\u964d\uff0c\u63a8\u5bfc\u51fa\u5e7f\u4e49\u6d53\u5ea6\u4e0d\u7b49\u5f0f\uff0c\u63d0\u51faML-UCB\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u76f4\u63a5\u5efa\u6a21\u5e95\u5c42\u4f30\u8ba1\u5668\u7684\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\uff0c\u65e0\u9700\u6a21\u578b\u7279\u5b9a\u7406\u8bba\u5206\u6790\uff0c\u53ea\u8981\u5b66\u4e60\u66f2\u7ebf\u53ef\u4ee5\u7ecf\u9a8c\u6027\u8868\u5f81\u5373\u53ef\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660eML-UCB\u80fd\u591f\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\uff0c\u5728\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\u4e0a\u4f7f\u7528\u5728\u7ebf\u77e9\u9635\u5206\u89e3\uff08\u6a21\u62df\u7b80\u5316\u53cc\u5854\u6a21\u578b\uff09\u8fdb\u884c\u6d4b\u8bd5\uff0c\u76f8\u6bd4LinUCB\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ML-UCB\u4e3a\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u96c6\u6210\u5230\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\u514b\u670d\u4e86\u4f20\u7edf\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u5e8f\u5217\u51b3\u7b56\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2601.01062", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01062", "abs": "https://arxiv.org/abs/2601.01062", "authors": ["Yunlin Zeng"], "title": "SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models", "comment": "14 pages, 3 figures. Accepted to WVAQ 2026, WACV 2026", "summary": "Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\\% win rate) and narrative depth (+50\\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff0c\u4f7f\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u81ea\u7136\u6027\u548c\u53d9\u4e8b\u6df1\u5ea6\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u957f\u7bc7\u53d9\u4e8b\uff08\u7279\u522b\u662f\u591a\u8bf4\u8bdd\u8005\u64ad\u5ba2\u5bf9\u8bdd\uff09\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4e14\u96be\u4ee5\u8bc4\u4f30\u3002\u73b0\u6709\u6807\u51c6\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5bf9\u8bdd\u81ea\u7136\u6027\u3001\u4e2a\u6027\u548c\u53d9\u4e8b\u6d41\u7a0b\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5f80\u5f80\u5956\u52b1\u5b89\u5168\u3001\u91cd\u590d\u7684\u8f93\u51fa\u800c\u975e\u5f15\u4eba\u5165\u80dc\u7684\u53d9\u4e8b\u3002", "method": "1. \u63d0\u51fa\u7aef\u5230\u7aef\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7ba1\u9053\uff1b2. \u57284000\u4e2a\u56fe\u50cf-\u5bf9\u8bdd\u5bf9\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff1b3. \u91c7\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u5728\u7ed3\u6784\u5316\u64ad\u5ba2\u7814\u7a76\u8bed\u6599\u5e93\u7684\u9ad8\u8d28\u91cf\u64ad\u5ba2\u5bf9\u8bdd\u4e0e\u5408\u6210\u751f\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u89c6\u89c9\u53d9\u4e8b\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u7167\u7247\u5e8f\u5217\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b4. \u63d0\u51fa\u8d85\u8d8a\u6587\u672c\u91cd\u53e0\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528AI\u4f5c\u4e3a\u8bc4\u5224\u548c\u65b0\u578b\u98ce\u683c\u6307\u6807\u3002", "result": "\u5fae\u8c03\u768432B\u6a21\u578b\u5728\u5bf9\u8bdd\u81ea\u7136\u6027\u4e0a\u663e\u8457\u4f18\u4e8e235B\u57fa\u7840\u6a21\u578b\uff08\u80dc\u7387>80%\uff09\uff0c\u53d9\u4e8b\u6df1\u5ea6\u63d0\u534750%\uff08\u5e73\u5747\u5bf9\u8bdd\u8f6e\u6b21\u957f\u5ea6\u589e\u52a0\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\uff08CLIPScore: 20.39\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5927\u578b\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u521b\u9020\u6027\u53d9\u4e8b\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.01065", "categories": ["cs.LG", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01065", "abs": "https://arxiv.org/abs/2601.01065", "authors": ["Achraf Hsain", "Yahya Zaki", "Othman Abaakil", "Hibat-allah Bekkar", "Yousra Chtouki"], "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco", "comment": "Published in IEEE GCAIoT 2024", "summary": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u96c6\u6210\u5230\u6c34\u4ea7\u517b\u6b96\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u63a7\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u6c34\u4ea7\u517b\u6b96\u4e1a\u9762\u4e34\u6c34\u8d28\u6ce2\u52a8\u3001\u75be\u75c5\u7206\u53d1\u548c\u9972\u6599\u7ba1\u7406\u6548\u7387\u4f4e\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u4eba\u5de5\u76d1\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u53ef\u80fd\u5bfc\u81f4\u95ee\u9898\u5904\u7406\u5ef6\u8fdf\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u5b9e\u65f6\u6536\u96c6pH\u503c\u3001\u6e29\u5ea6\u3001\u6eb6\u89e3\u6c27\u3001\u6c28\u6c2e\u6c34\u5e73\u7b49\u53c2\u6570\u6570\u636e\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u76d1\u6d4b\u3001\u5f02\u5e38\u62a5\u8b66\u548c\u63a7\u5236\u529f\u80fd\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u5b9e\u65f6\u6c34\u8d28\u76d1\u6d4b\u6570\u636e\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u62a5\u8b66\uff0c\u6536\u96c6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u4f18\u5316\u6c34\u5904\u7406\u8fc7\u7a0b\u3001\u9972\u6599\u5206\u914d\u548c\u6a21\u5f0f\u5206\u6790\uff0c\u63d0\u9ad8\u9972\u6599\u6548\u7387\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "\u7814\u7a76\u8868\u660eTinyML\u5728\u6c34\u4ea7\u517b\u6b96\u76d1\u6d4b\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8003\u8651\u4f20\u611f\u5668\u9009\u62e9\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u786c\u4ef6\u7ea6\u675f\u548c\u4f26\u7406\u56e0\u7d20\uff0c\u53ef\u4e3a\u5f00\u53d1\u66f4\u53ef\u6301\u7eed\u3001\u9ad8\u6548\u7684\u517b\u6b96\u5b9e\u8df5\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2601.01069", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01069", "abs": "https://arxiv.org/abs/2601.01069", "authors": ["Jing Wang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs", "comment": "accepted by IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2303.02691", "summary": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \\emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_\u03bc$ and $c_\u03bc$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u6790\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\u52a0\u6743\u7b56\u7565\u7684\u7406\u8bba\u5206\u6790\uff0c\u4ea7\u751f\u4e86\u66f4\u7b80\u5355\u7684\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5e76\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u548c\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u9057\u61be\u754c\uff0c\u540c\u65f6\u8fd8\u5c06\u6846\u67b6\u6269\u5c55\u5230\u5177\u6709\u51fd\u6570\u8fd1\u4f3c\u7684\u975e\u5e73\u7a33\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\uff0c\u52a0\u6743\u7b56\u7565\u5728\u5904\u7406\u6e10\u53d8\u6f02\u79fb\u6a21\u5f0f\u65f6\u5e38\u7528\uff0c\u4f46\u5148\u524d\u7406\u8bba\u5206\u6790\u590d\u6742\uff0c\u5bfc\u81f4\u7b97\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u6216\u7edf\u8ba1\u6b21\u4f18\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u52a0\u6743\u7b56\u7565\u5206\u6790\u590d\u6742\u3001\u7b97\u6cd5\u8bbe\u8ba1\u7e41\u7410\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u70bc\u7684\u5206\u6790\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u52a0\u6743\u7b56\u7565\u7684\u63a8\u5bfc\u8fc7\u7a0b\u3002\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u8be5\u6846\u67b6\u4ea7\u751f\u4e86\u66f4\u7b80\u5355\u7684\u57fa\u4e8e\u6743\u91cd\u7684\u7b97\u6cd5\u3002\u6846\u67b6\u8fd8\u53ef\u6269\u5c55\u5230\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u81ea\u534f\u8c03\u8d4c\u535a\u673a\uff0c\u4ee5\u53ca\u5177\u6709\u51fd\u6570\u8fd1\u4f3c\u7684\u975e\u5e73\u7a33\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08\u7ebf\u6027\u6df7\u5408MDP\u548c\u591a\u9879Logit\u6df7\u5408MDP\uff09\u3002", "result": "\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u65b0\u7b97\u6cd5\u4e0e\u7a97\u53e3/\u91cd\u542f\u7b97\u6cd5\u540c\u6837\u9ad8\u6548\uff0c\u4e14\u4fdd\u6301\u76f8\u540c\u9057\u61be\u754c\u3002\u5728\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u83b7\u5f97\u4e86 $\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ \u7684\u9057\u61be\u754c\uff0c\u4f18\u4e8e\u5148\u524d\u7684 $\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$\u3002\u6846\u67b6\u8fd8\u6210\u529f\u6269\u5c55\u5230\u975e\u5e73\u7a33MDPs\uff0c\u4e3a\u7ebf\u6027\u6df7\u5408MDP\u548c\u591a\u9879Logit\u6df7\u5408MDP\u5efa\u7acb\u4e86\u52a8\u6001\u9057\u61be\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7cbe\u70bc\u5206\u6790\u6846\u67b6\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\u52a0\u6743\u7b56\u7565\u5206\u6790\u590d\u6742\u7684\u95ee\u9898\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86\u7406\u8bba\u6027\u80fd\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\uff0c\u4e3a\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2601.01075", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01075", "abs": "https://arxiv.org/abs/2601.01075", "authors": ["Hansen Jin Lillemark", "Benhao Huang", "Fangneng Zhan", "Yilun Du", "Thomas Anderson Keller"], "title": "Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments", "comment": "11 main text pages, 10 figures", "summary": "Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.", "AI": {"tldr": "\u63d0\u51faFlow Equivariant World Models\u6846\u67b6\uff0c\u5c06\u81ea\u8fd0\u52a8\u548c\u5916\u90e8\u7269\u4f53\u8fd0\u52a8\u7edf\u4e00\u4e3a\u5355\u53c2\u6570\u674e\u7fa4\"\u6d41\"\uff0c\u5229\u7528\u7fa4\u7b49\u53d8\u6027\u5b9e\u73b0\u7a33\u5b9a\u6f5c\u5728\u4e16\u754c\u8868\u793a\uff0c\u57282D/3D\u90e8\u5206\u89c2\u6d4b\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5177\u8eab\u7cfb\u7edf\u4f53\u9a8c\u4e16\u754c\u4e3a\"\u6d41\u52a8\u7684\u4ea4\u54cd\u66f2\"\uff1a\u591a\u4e2a\u8fde\u7eed\u611f\u5b98\u8f93\u5165\u6d41\u4e0e\u81ea\u8fd0\u52a8\u8026\u5408\uff0c\u4ea4\u7ec7\u7740\u5916\u90e8\u7269\u4f53\u52a8\u6001\u3002\u8fd9\u4e9b\u6d41\u9075\u5faa\u5e73\u6ed1\u7684\u65f6\u95f4\u53c2\u6570\u5bf9\u79f0\u6027\uff0c\u4f46\u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u4e16\u754c\u6a21\u578b\u5ffd\u7565\u8fd9\u79cd\u7ed3\u6784\uff0c\u53cd\u590d\u4ece\u6570\u636e\u4e2d\u91cd\u65b0\u5b66\u4e60\u76f8\u540c\u53d8\u6362\u3002", "method": "\u5f15\u5165Flow Equivariant World Models\u6846\u67b6\uff0c\u5c06\u81ea\u8fd0\u52a8\u548c\u5916\u90e8\u7269\u4f53\u8fd0\u52a8\u7edf\u4e00\u4e3a\u5355\u53c2\u6570\u674e\u7fa4\"\u6d41\"\uff0c\u5229\u7528\u8fd9\u79cd\u7edf\u4e00\u5b9e\u73b0\u5bf9\u53d8\u6362\u7684\u7fa4\u7b49\u53d8\u6027\uff0c\u63d0\u4f9b\u6570\u767e\u65f6\u95f4\u6b65\u7684\u7a33\u5b9a\u6f5c\u5728\u4e16\u754c\u8868\u793a\u3002", "result": "\u57282D\u548c3D\u90e8\u5206\u89c2\u6d4b\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u57fa\u51c6\u4e0a\uff0cFlow Equivariant World Models\u663e\u8457\u4f18\u4e8e\u53ef\u6bd4\u8f83\u7684\u6700\u5148\u8fdb\u6269\u6563\u57fa\u548c\u8bb0\u5fc6\u589e\u5f3a\u4e16\u754c\u5efa\u6a21\u67b6\u6784\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u5f53\u524d\u89c6\u91ce\u5916\u5b58\u5728\u53ef\u9884\u6d4b\u4e16\u754c\u52a8\u6001\u65f6\u3002\u6d41\u7b49\u53d8\u6027\u5bf9\u957f\u5e8f\u5217\u5916\u63a8\u7279\u522b\u6709\u76ca\uff0c\u80fd\u6cdb\u5316\u8fdc\u8d85\u8bad\u7ec3\u8303\u56f4\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e16\u754c\u6a21\u578b\u8868\u793a\u7ed3\u6784\u4e0e\u5185\u90e8\u548c\u5916\u90e8\u8fd0\u52a8\u5bf9\u9f50\uff0c\u6d41\u7b49\u53d8\u6027\u4e3a\u6570\u636e\u9ad8\u6548\u3001\u5bf9\u79f0\u5f15\u5bfc\u7684\u5177\u8eab\u667a\u80fd\u5f00\u8f9f\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2601.01082", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01082", "abs": "https://arxiv.org/abs/2601.01082", "authors": ["Bryon Tjanaka", "Henry Chen", "Matthew C. Fontaine", "Stefanos Nikolaidis"], "title": "Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces", "comment": "Source code available at https://github.com/icaros-usc/discount-models", "summary": "Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.", "AI": {"tldr": "DMS\u7b97\u6cd5\u901a\u8fc7\u4f7f\u7528\u8fde\u7eed\u6298\u6263\u6a21\u578b\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2dQD\u4f18\u5316\u7684\u5931\u771f\u95ee\u9898\uff0c\u4f18\u4e8eCMA-MAE\u7b49\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709QD\u7b97\u6cd5\u5728\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u5b58\u5728\u5931\u771f\u95ee\u9898\uff0c\u5373\u8bb8\u591a\u89e3\u6620\u5c04\u5230\u76f8\u4f3c\u7684\u5ea6\u91cf\u503c\uff0c\u5bfc\u81f4\u63a2\u7d22\u505c\u6ede\u3002CMA-MAE\u4f7f\u7528\u76f4\u65b9\u56fe\u8bb0\u5f55\u6298\u6263\u503c\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u5ea6\u91cf\u7684\u89e3\u843d\u5165\u540c\u4e00\u5355\u5143\u683c\uff0c\u83b7\u5f97\u76f8\u540c\u6298\u6263\u503c\u800c\u65e0\u6cd5\u7ee7\u7eed\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u6298\u6263\u6a21\u578b\u641c\u7d22(DMS)\uff0c\u4f7f\u7528\u63d0\u4f9b\u5e73\u6ed1\u8fde\u7eed\u6298\u6263\u503c\u8868\u793a\u7684\u6a21\u578b\u6765\u6307\u5bfc\u63a2\u7d22\u3002\u8be5\u6a21\u578b\u80fd\u591f\u533a\u5206\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u5ea6\u91cf\u7684\u89e3\uff0c\u4ece\u800c\u6301\u7eed\u63a2\u7d22\u3002", "result": "DMS\u5728\u9ad8\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u65b0\u5e94\u7528\u9886\u57df\uff08\u5ea6\u91cf\u7a7a\u95f4\u4e3a\u56fe\u50cf\u9ad8\u7ef4\u7a7a\u95f4\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eCMA-MAE\u548c\u5176\u4ed6\u9ed1\u76d2QD\u7b97\u6cd5\u3002\u65b0\u5e94\u7528\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u63d0\u4f9b\u56fe\u50cf\u6570\u636e\u96c6\u800c\u975e\u624b\u52a8\u8bbe\u8ba1\u5ea6\u91cf\u51fd\u6570\u6765\u6307\u5b9a\u6240\u9700\u5ea6\u91cf\u3002", "conclusion": "DMS\u901a\u8fc7\u8fde\u7eed\u6298\u6263\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u5931\u771f\u95ee\u9898\uff0c\u63a8\u52a8\u4e86QD\u5728\u56fe\u50cf\u7b49\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u5e94\u7528\uff0c\u4e3aQD\u7b97\u6cd5\u5728\u9ad8\u7ef4\u573a\u666f\u4e2d\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.01089", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.01089", "abs": "https://arxiv.org/abs/2601.01089", "authors": ["Nobuyuki Ota"], "title": "Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding", "comment": null, "summary": "Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.", "AI": {"tldr": "CDT\u662f\u4e00\u4e2a\u6574\u5408DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u4e2d\u5fc3\u6cd5\u5219\u903b\u8f91\u5b9e\u73b0\u591a\u6a21\u6001\u6574\u5408\uff0c\u5728CRISPRi\u589e\u5f3a\u5b50\u6270\u52a8\u6570\u636e\u4e0a\u53d6\u5f97\u826f\u597d\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u673a\u5236\u89e3\u91ca\u6027\u3002", "motivation": "\u867d\u7136DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u7684\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u5404\u81ea\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5b64\u7acb\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u6574\u5408\u7ec6\u80de\u8fc7\u7a0b\u7684\u5efa\u6a21\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9075\u5faa\u4e2d\u5fc3\u6cd5\u5219\u4fe1\u606f\u6d41\u65b9\u5411\u7684\u6574\u5408\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e2d\u5fc3\u6cd5\u5219\u53d8\u6362\u5668(CDT)\uff0c\u6574\u5408DNA\u3001RNA\u548c\u86cb\u767d\u8d28\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u65b9\u5411\u6027\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff1aDNA-to-RNA\u6ce8\u610f\u529b\u5efa\u6a21\u8f6c\u5f55\u8c03\u63a7\uff0cRNA-to-Protein\u6ce8\u610f\u529b\u5efa\u6a21\u7ffb\u8bd1\u5173\u7cfb\uff0c\u4ea7\u751f\u7edf\u4e00\u865a\u62df\u7ec6\u80de\u5d4c\u5165\u3002", "result": "\u5728K562\u7ec6\u80de\u7684CRISPRi\u589e\u5f3a\u5b50\u6270\u52a8\u6570\u636e\u4e0a\uff0cCDT v1\u5b9e\u73b0Pearson\u76f8\u5173\u7cfb\u65700.503\uff0c\u8fbe\u5230\u4ea4\u53c9\u5b9e\u9a8c\u53d8\u5f02\u6027\u7406\u8bba\u4e0a\u9650(r=0.797)\u768463%\u3002\u6ce8\u610f\u529b\u548c\u68af\u5ea6\u5206\u6790\u63d0\u4f9b\u4e92\u8865\u7684\u89e3\u91ca\u7a97\u53e3\uff0c\u68af\u5ea6\u5206\u6790\u8bc6\u522b\u51faCTCF\u7ed3\u5408\u4f4d\u70b9\u3002", "conclusion": "\u4e0e\u751f\u7269\u4fe1\u606f\u6d41\u5bf9\u9f50\u7684AI\u67b6\u6784\u65e2\u80fd\u5b9e\u73b0\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53c8\u80fd\u63d0\u4f9b\u673a\u5236\u89e3\u91ca\u6027\uff0c\u4e3a\u6574\u5408\u591a\u6a21\u6001\u7ec6\u80de\u751f\u7269\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.01119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01119", "abs": "https://arxiv.org/abs/2601.01119", "authors": ["Muhammad Ashad Kabir", "Sirajam Munira", "Dewan Tasnia Azad", "Saleh Mohammed Ikram", "Mohammad Habibur Rahman Sarker", "Syed Manzoor Ahmed Hanifi"], "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings", "comment": "27 pages", "summary": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u65e9\u671f\u6162\u6027\u80be\u75c5\u793e\u533a\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709CKD\u7b5b\u67e5\u5de5\u5177\u4e3b\u8981\u57fa\u4e8e\u9ad8\u6536\u5165\u56fd\u5bb6\u4eba\u7fa4\u5f00\u53d1\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u5730\u533a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5927\u591a\u4f9d\u8d56\u7b80\u5355\u7684\u52a0\u6027\u8bc4\u5206\u51fd\u6570\uff0c\u65e0\u6cd5\u6355\u6349\u98ce\u9669\u56e0\u7d20\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u5bf9\u65e9\u671fCKD\u9884\u6d4b\u80fd\u529b\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5b5f\u52a0\u62c9\u56fd\u793e\u533a\u6570\u636e\u96c6\uff08\u5357\u4e9a\u9996\u4e2a\u6b64\u7c7b\u6570\u636e\u96c6\uff09\uff0c\u8bc4\u4f3012\u79cdML\u5206\u7c7b\u5668\uff0c\u5e94\u752810\u79cd\u4e92\u8865\u7279\u5f81\u9009\u62e9\u6280\u672f\u8bc6\u522b\u7a33\u5065\u9884\u6d4b\u56e0\u5b50\uff0c\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e76\u5728\u5370\u5ea6\u3001\u963f\u8054\u914b\u548c\u5b5f\u52a0\u62c9\u56fd\u7684\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\uff0c\u4f7f\u7528SHAP\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "RFECV\u9009\u62e9\u7279\u5f81\u5b50\u96c6\u7684ML\u6a21\u578b\u5e73\u8861\u51c6\u786e\u7387\u8fbe90.40%\uff0c\u6700\u5c0f\u975e\u75c5\u7406\u6d4b\u8bd5\u7279\u5f81\u96c6\u5e73\u8861\u51c6\u786e\u7387\u8fbe89.23%\uff0c\u901a\u5e38\u4f18\u4e8e\u66f4\u5927\u6216\u5b8c\u6574\u7279\u5f81\u96c6\u3002\u76f8\u6bd4\u73b0\u6709\u7b5b\u67e5\u5de5\u5177\uff0c\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u663e\u8457\u63d0\u9ad8\u4e14\u8f93\u5165\u66f4\u5c11\u3002\u5916\u90e8\u9a8c\u8bc1\u663e\u793a78%\u81f398%\u7684\u654f\u611f\u6027\uff0cSHAP\u8bc6\u522b\u51fa\u4e0e\u5df2\u77e5CKD\u98ce\u9669\u56e0\u7d20\u4e00\u81f4\u7684\u4e34\u5e8a\u610f\u4e49\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u53ef\u89e3\u91caML\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8fdb\u884c\u65e9\u671fCKD\u793e\u533a\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b5b\u67e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01123", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01123", "abs": "https://arxiv.org/abs/2601.01123", "authors": ["Yaniv Galron", "Hadar Sinai", "Haggai Maron", "Moshe Eliasof"], "title": "Learning from Historical Activations in Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.", "AI": {"tldr": "HISTOGRAPH\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u4e24\u9636\u6bb5\u56fe\u6c60\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5386\u53f2\u6765\u6539\u8fdb\u56fe\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6df1\u5c42GNN\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u6c60\u5316\u65b9\u6cd5\u901a\u5e38\u53ea\u4f7f\u7528\u6700\u540e\u4e00\u5c42GNN\u7279\u5f81\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\uff08\u5386\u53f2\u56fe\u6fc0\u6d3b\uff09\u3002\u8fd9\u5728\u8282\u70b9\u8868\u793a\u53ef\u80fd\u968f\u5c42\u6570\u663e\u8457\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u5c24\u5176\u660e\u663e\uff0c\u4e14\u56fe\u7279\u6709\u7684\u6311\u6218\uff08\u5982\u6df1\u5c42\u67b6\u6784\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff09\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HISTOGRAPH\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6700\u7ec8\u805a\u5408\u5c42\uff1a\u9996\u5148\u5728\u4e2d\u95f4\u6fc0\u6d3b\u4e0a\u5e94\u7528\u7edf\u4e00\u7684\u5c42\u95f4\u6ce8\u610f\u529b\uff0c\u7136\u540e\u5e94\u7528\u8282\u70b9\u95f4\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u5efa\u6a21\u8282\u70b9\u8868\u793a\u5728\u5c42\u95f4\u7684\u6f14\u5316\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8282\u70b9\u7684\u6fc0\u6d3b\u5386\u53f2\u548c\u56fe\u7ed3\u6784\u6765\u7cbe\u70bc\u7528\u4e8e\u6700\u7ec8\u9884\u6d4b\u7684\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cHISTOGRAPH\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u6301\u7eed\u6539\u8fdb\u4e86\u4f20\u7edf\u6280\u672f\uff0c\u5728\u6df1\u5c42GNN\u4e2d\u8868\u73b0\u51fa\u7279\u522b\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5386\u53f2\u56fe\u6fc0\u6d3b\uff0cHISTOGRAPH\u80fd\u591f\u66f4\u6709\u6548\u5730\u805a\u5408\u56fe\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u6df1\u5c42\u67b6\u6784\u4e2d\uff0c\u4e3a\u89e3\u51b3\u56fe\u6c60\u5316\u4e2d\u7684\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.01127", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01127", "abs": "https://arxiv.org/abs/2601.01127", "authors": ["Golbahar Amanpour", "Benyamin Ghojogh"], "title": "Wittgenstein's Family Resemblance Clustering Algorithm", "comment": null, "summary": "This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ef4\u7279\u6839\u65af\u5766\u5bb6\u65cf\u76f8\u4f3c\u6027\u6982\u5ff5\u7684\u805a\u7c7b\u7b97\u6cd5WFR\u53ca\u5176\u6838\u53d8\u4f53\uff0c\u65e0\u9700\u9884\u8bbe\u805a\u7c7b\u6570\u91cf\u6216\u5f62\u72b6\u5047\u8bbe", "motivation": "\u5c06\u7ef4\u7279\u6839\u65af\u5766\u7684\u5bb6\u65cf\u76f8\u4f3c\u6027\u54f2\u5b66\u6982\u5ff5\u5e94\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u805a\u7c7b\u95ee\u9898\uff0c\u89e3\u51b3\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u9700\u8981\u9884\u8bbe\u805a\u7c7b\u6570\u91cf\u548c\u5f62\u72b6\u5047\u8bbe\u7684\u9650\u5236", "method": "\u63d0\u51faWFR\u7b97\u6cd5\uff1a\u8ba1\u7b97\u76f8\u90bb\u6570\u636e\u5b9e\u4f8b\u95f4\u7684\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u901a\u8fc7\u9608\u503c\u5904\u7406\u540e\u6784\u5efa\u76f8\u4f3c\u56fe\uff0c\u56fe\u7684\u8fde\u901a\u5206\u91cf\u5373\u4e3a\u805a\u7c7b\u7ed3\u679c\uff1b\u8fd8\u5f00\u53d1\u4e86\u6838\u53d8\u4f53kernel WFR", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cWFR\u662f\u4e00\u79cd\u6709\u6548\u7684\u975e\u7ebf\u6027\u805a\u7c7b\u7b97\u6cd5\uff0c\u4e0d\u9700\u8981\u9884\u5148\u77e5\u9053\u805a\u7c7b\u6570\u91cf\u6216\u5bf9\u5176\u5f62\u72b6\u505a\u51fa\u5047\u8bbe", "conclusion": "\u6210\u529f\u5c06\u54f2\u5b66\u6982\u5ff5\u8f6c\u5316\u4e3a\u5b9e\u7528\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e3a\u805a\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u4ef7\u503c"}}
{"id": "2601.01146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01146", "abs": "https://arxiv.org/abs/2601.01146", "authors": ["Anusree M", "Akhila Henry", "Pramod P Nair"], "title": "Self-Training the Neurochaos Learning Algorithm", "comment": null, "summary": "In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u4e0e\u81ea\u8bad\u7ec3\u7684\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u800c\u65e0\u6807\u7b7e\u6570\u636e\u5374\u5bb9\u6613\u83b7\u5f97\u3002\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u7b7e\u6570\u636e\u7a00\u5c11\u6216\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u67b6\u6784\uff0c\u5c06\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u4e0e\u57fa\u4e8e\u9608\u503c\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u5c06\u8f93\u5165\u7279\u5f81\u8f6c\u6362\u4e3a\u6df7\u6c8c\u53d1\u653e\u7387\u8868\u793a\uff0c\u6355\u6349\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff1b\u81ea\u8bad\u7ec3\u5219\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4f2a\u6807\u7b7e\u6837\u672c\u9010\u6b65\u6269\u5c55\u6807\u7b7e\u96c6\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c5\u4e2a\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u752885%\u65e0\u6807\u7b7e\u548c15%\u6709\u6807\u7b7e\u6570\u636e\u3002\u63d0\u51fa\u7684NL+ST\u67b6\u6784\u76f8\u6bd4\u72ec\u7acb\u81ea\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728Iris(188.66%)\u3001Wine(158.58%)\u548cGlass Identification(110.48%)\u7b49\u6709\u9650\u3001\u975e\u7ebf\u6027\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "\u6df7\u6c8c\u7279\u5f81\u63d0\u53d6\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.01150", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01150", "abs": "https://arxiv.org/abs/2601.01150", "authors": ["Wenbin Pei", "Ruohao Dai", "Bing Xue", "Mengjie Zhang", "Qiang Zhang", "Yiu-Ming Cheung"], "title": "Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification", "comment": null, "summary": "Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.", "AI": {"tldr": "Evo-TFS\uff1a\u4e00\u79cd\u7ed3\u5408\u65f6\u9891\u57df\u7279\u5f81\u7684\u8fdb\u5316\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u901a\u8fc7\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5e73\u8861\uff0c\u5728\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5bb9\u6613\u5ffd\u7565\u5c11\u6570\u7c7b\uff1b\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u63d2\u503c\uff0c\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u7279\u6027\u548c\u751f\u6210\u591a\u6837\u5316\u6837\u672c\u3002", "method": "\u63d0\u51faEvo-TFS\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u540c\u65f6\u8003\u8651\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u5305\u542b\u65f6\u9891\u57df\u7279\u5f81\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6307\u5bfc\u8fdb\u5316\u8fc7\u7a0b\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvo-TFS\u4f18\u4e8e\u73b0\u6709\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u57df\u548c\u9891\u57df\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "Evo-TFS\u901a\u8fc7\u7ed3\u5408\u65f6\u9891\u57df\u7279\u5f81\u7684\u8fdb\u5316\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u751f\u6210\u4e86\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u5c11\u6570\u7c7b\u6837\u672c\u3002"}}
{"id": "2601.01162", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01162", "abs": "https://arxiv.org/abs/2601.01162", "authors": ["Zihua Yang", "Xin Liao", "Yiqun Zhang", "Yiu-ming Cheung"], "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE", "AI": {"tldr": "ARISE\u5229\u7528LLM\u83b7\u53d6\u5206\u7c7b\u6570\u636e\u7684\u8bed\u4e49\u5d4c\u5165\uff0c\u7ed3\u5408\u539f\u59cb\u6570\u636e\u63d0\u5347\u805a\u7c7b\u8d28\u91cf\uff0c\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd47\u4e2a\u5bf9\u6bd4\u65b9\u6cd5\u63d0\u534719-27%", "motivation": "\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u9762\u4e34\u76f8\u4f3c\u6027\u5ea6\u91cf\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u65e0\u5e8f\u5c5e\u6027\u503c\u89c6\u4e3a\u7b49\u8ddd\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002\u73b0\u6709\u57fa\u4e8e\u5171\u73b0\u6a21\u5f0f\u7684\u65b9\u6cd5\u5728\u6837\u672c\u6709\u9650\u65f6\u4e0d\u53ef\u9760\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faARISE\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u5c5e\u6027\u503c\u7684\u8bed\u4e49\u63cf\u8ff0\uff0c\u6784\u5efa\u8bed\u4e49\u611f\u77e5\u8868\u793a\uff0c\u5c06LLM\u589e\u5f3a\u7684\u5d4c\u5165\u4e0e\u539f\u59cb\u6570\u636e\u7ed3\u5408\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u52a0\u6743\u673a\u5236\u63a2\u7d22\u8bed\u4e49\u663e\u8457\u7684\u805a\u7c7b\u7ed3\u6784\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARISE\u76f8\u6bd47\u4e2a\u4ee3\u8868\u6027\u5bf9\u6bd4\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u6027\u80fd\u63d0\u534719-27%\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "ARISE\u901a\u8fc7\u6574\u5408\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u6709\u6548\u5f25\u8865\u4e86\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0cLLM\u63d0\u4f9b\u7684\u8bed\u4e49\u5d4c\u5165\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2601.01206", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01206", "abs": "https://arxiv.org/abs/2601.01206", "authors": ["Soroush Elyasi", "Arya VarastehNezhad", "Fattaneh Taghiyareh"], "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches", "comment": null, "summary": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.", "AI": {"tldr": "\u4f7f\u7528\u591a\u7c7b\u578b\u4e25\u8083\u6e38\u620f\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u5c97\u4f4d\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u804c\u4e1a\u8bc4\u4f30\u4e2d\u7684\u81ea\u6211\u62a5\u544a\u95ee\u5377\u5b58\u5728\u53cd\u5e94\u504f\u5dee\u3001\u75b2\u52b3\u548c\u6545\u610f\u626d\u66f2\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u504f\u89c1\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u5f00\u53d1\u8005\u76f8\u5173\u7279\u8d28\uff0c\u8bbe\u8ba1\u5b9a\u5236\u79fb\u52a8\u6e38\u620f\u6355\u6349\u95ee\u9898\u89e3\u51b3\u3001\u89c4\u5212\u3001\u9002\u5e94\u6027\u7b49\u884c\u4e3a\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5efa\u6a21\u7b56\u7565\u4ece\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u9884\u6d4b\u9002\u5e94\u6027", "result": "\u6a21\u578b\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387\uff0c\u5408\u9002\u5019\u9009\u4eba\u5c55\u73b0\u51fa\u72ec\u7279\u7684\u6e38\u620f\u884c\u4e3a\u6a21\u5f0f\uff08\u66f4\u591a\u89e3\u8c1c\u80dc\u5229\u3001\u66f4\u591a\u4fa7\u6311\u6218\u3001\u66f4\u9891\u7e41\u83dc\u5355\u5bfc\u822a\u3001\u8f83\u5c11\u6682\u505c/\u91cd\u8bd5/\u653e\u5f03\uff09", "conclusion": "\u6e38\u620f\u8fc7\u7a0b\u4e2d\u6355\u83b7\u7684\u9690\u5f0f\u884c\u4e3a\u75d5\u8ff9\u80fd\u6709\u6548\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u9002\u5e94\u6027\uff0c\u652f\u6301\u4e25\u8083\u6e38\u620f\u4f5c\u4e3a\u804c\u4e1a\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u3001\u5438\u5f15\u4eba\u4e14\u504f\u89c1\u8f83\u5c11\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2601.01207", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01207", "abs": "https://arxiv.org/abs/2601.01207", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chanran Kim", "Yumin Lee", "Hawon Shin", "Yeowon Jeon", "Minjeong Kim", "Jiwoo Kang"], "title": "Sparse Bayesian Message Passing under Structural Uncertainty", "comment": null, "summary": "Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u7a00\u758f\u7b26\u53f7\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5efa\u6a21\u5e26\u7b26\u53f7\u90bb\u63a5\u77e9\u9635\u7684\u540e\u9a8c\u5206\u5e03\u6765\u5904\u7406\u5f02\u8d28\u6027\u548c\u8fb9\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u4e2d\u7684\u534a\u76d1\u7763\u5b66\u4e60\u5e38\u9762\u4e34\u5f02\u8d28\u6027\u6311\u6218\uff0c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u90bb\u63a5\u7ed3\u6784\uff0c\u8981\u4e48\u901a\u8fc7\u6b63\u5219\u5316\u5904\u7406\u7ed3\u6784\u566a\u58f0\uff0c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u5efa\u6a21\u5e26\u7b26\u53f7\u90bb\u63a5\u77e9\u9635\u7684\u540e\u9a8c\u5206\u5e03\uff08\u8fb9\u53ef\u4e3a\u6b63\u3001\u8d1f\u6216\u7f3a\u5931\uff09\uff0c\u63d0\u51fa\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u4f20\u9012\u7f51\u7edc\uff0c\u7ed3\u5408\u540e\u9a8c\u8fb9\u7f18\u5316\u548c\u7a00\u758f\u7b26\u53f7\u6d88\u606f\u805a\u5408\uff0c\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7ed3\u6784\u566a\u58f0\u4e0b\u7684\u5f02\u8d28\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u8fb9\u566a\u58f0\u548c\u5f02\u8d28\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5728\u5f02\u8d28\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.01223", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01223", "abs": "https://arxiv.org/abs/2601.01223", "authors": ["Marzieh Amiri Shahbazi", "Ali Baheri", "Nasibeh Azadeh-Fard"], "title": "Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data", "comment": null, "summary": "Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u8d1d\u53f6\u65af-\u4fdd\u5f62\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u548c\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\u548c\u98ce\u9669\u81ea\u9002\u5e94\u7cbe\u5ea6\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u65e2\u8981\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\uff0c\u53c8\u8981\u98ce\u9669\u81ea\u9002\u5e94\u7cbe\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u8981\u6c42\u3002", "method": "\u96c6\u6210\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u4e0e\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u5229\u7528\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u5bf9\u4fdd\u5f62\u5206\u6570\u8fdb\u884c\u52a0\u6743\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u8986\u76d6\u6709\u6548\u6027\u3002", "result": "\u572861,538\u4f8b\u5165\u9662\u60a3\u8005\u30013,793\u5bb6\u7f8e\u56fd\u533b\u9662\u548c4\u4e2a\u5730\u533a\u7684\u8bc4\u4f30\u4e2d\uff0c\u65b9\u6cd5\u8fbe\u5230\u76ee\u6807\u8986\u76d6\uff0894.3% vs 95%\u76ee\u6807\uff09\uff0c\u4f4e\u4e0d\u786e\u5b9a\u6027\u75c5\u4f8b\u533a\u95f4\u5bbd\u5ea6\u51cf\u5c1121%\uff0c\u9ad8\u98ce\u9669\u9884\u6d4b\u9002\u5f53\u52a0\u5bbd\u3002\u7eaf\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u4e25\u91cd\u6b20\u8986\u76d6\uff0814.1%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u98ce\u9669\u5206\u5c42\u4e34\u5e8a\u534f\u8bae\u3001\u9ad8\u6548\u8d44\u6e90\u89c4\u5212\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\uff0c\u4e3a\u4e0d\u786e\u5b9a\u75c5\u4f8b\u63d0\u4f9b\u589e\u5f3a\u76d1\u7763\u7684\u4fdd\u5b88\u5206\u914d\uff0c\u4e3a\u591a\u6837\u5316\u533b\u7597\u73af\u5883\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2601.01231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01231", "abs": "https://arxiv.org/abs/2601.01231", "authors": ["Md Muhtasim Munif Fahim", "Humyra Ankona", "Md Monimul Huq", "Md Rezaul Karim"], "title": "The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context", "comment": "Conference Paper", "summary": "Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.\n  Purpose: This study introduces the \"Dependency Divide\", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.\n  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.\n  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.\n  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\"\u4f9d\u8d56\u9e3f\u6c9f\"\u6846\u67b6\uff0c\u53d1\u73b0\u5728\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u73af\u5883\u4e2d\uff0c\u9ad8\u5ea6\u6295\u5165\u7684\u5b66\u751f\u53cd\u800c\u5bf9\u7f51\u7edc\u6545\u969c\u66f4\u654f\u611f\uff0c\u6311\u6218\u4e86\"\u6295\u5165\u8d8a\u591a\u8d8a\u597d\"\u7684\u4f20\u7edf\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u9e3f\u6c9f\u6846\u67b6\u65e0\u6cd5\u89e3\u91ca\u5728\u76f8\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u5b66\u751f\u6ee1\u610f\u5ea6\u5dee\u5f02\u7684\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u6570\u5b57\u5b66\u4e60\u53c2\u4e0e\u5ea6\u4e0e\u57fa\u7840\u8bbe\u65bd\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u5bf9\u5b5f\u52a0\u62c9\u56fd396\u540d\u5927\u5b66\u751f\u8fdb\u884c\u6a2a\u65ad\u9762\u7814\u7a76\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5206\u6790\u65b9\u6cd5\uff1aK-prototypes\u805a\u7c7b\u8bc6\u522b\u5b66\u751f\u7c7b\u578b\u3001\u968f\u673a\u68ee\u6797\u6a21\u578b\u5206\u6790\u6ee1\u610f\u5ea6\u9a71\u52a8\u56e0\u7d20\u3001\u503e\u5411\u5f97\u5206\u5339\u914d\u68c0\u9a8c\u4f9d\u8d56\u9e3f\u6c9f\u5047\u8bbe\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u7c7b\u5b66\u751f\uff1a\u5076\u5c14\u53c2\u4e0e(58%)\u3001\u9ad8\u6548\u5b66\u4e60\u8005(35%)\u3001\u9ad8\u5ea6\u6295\u5165(7%)\u3002\u53d1\u73b0\u6559\u80b2\u8bbe\u5907\u4f7f\u7528\u65f6\u95f4\u4e0e\u7f51\u7edc\u53ef\u9760\u6027\u7684\u663e\u8457\u4ea4\u4e92\u4f5c\u7528\uff0c\u8bc1\u5b9e\u4f9d\u8d56\u9e3f\u6c9f\u5b58\u5728\u3002\u9ad8\u5ea6\u6295\u5165\u5b66\u751f\u5bf9\u57fa\u7840\u8bbe\u65bd\u6545\u969c\u6700\u8106\u5f31\u3002", "conclusion": "\u5728\u8106\u5f31\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u4e2d\uff0c\u6570\u5b57\u80fd\u529b\u53ef\u80fd\u6210\u4e3a\u8d1f\u62c5\u3002\u653f\u7b56\u5e94\u4f18\u5148\u4fdd\u969c\u9ad8\u4f9d\u8d56\u7528\u6237\u7684\u53ef\u9760\u6027\uff0c\u5efa\u7acb\u5e94\u6025\u7cfb\u7edf\uff0c\u5e76\u6559\u80b2\u5b66\u751f\u8ba4\u8bc6\u4f9d\u8d56\u98ce\u9669\uff0c\u800c\u975e\u4e00\u5473\u9f13\u52b1\u53c2\u4e0e\u3002"}}
{"id": "2601.01237", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01237", "abs": "https://arxiv.org/abs/2601.01237", "authors": ["Abidemi Koledoye", "Chinemerem Unachukwu", "Gold Nwobu", "Hasin Rana"], "title": "Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions", "comment": "14 pages", "summary": "State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9Mamba SSM\u548cLLaMA Transformer\u5728\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u4e0a\u8fdb\u884c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u6cbb\u7597\u4f1a\u8bdd\u4f5c\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u6bd4\u8f83\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u6548\u7387\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4f55\u65f6\u9009\u62e9SSM\u7684\u6307\u5bfc\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSMs)\u4f5c\u4e3aTransformer\u7684\u66ff\u4ee3\u65b9\u6848\u5728\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5177\u6709\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6(O(N))\uff0c\u800cTransformer\u662f\u4e8c\u6b21\u590d\u6742\u5ea6(O(N^2))\u3002\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e24\u8005\u5728\u5b9e\u9645\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6cbb\u7597\u4f1a\u8bdd\u4f5c\u4e3a\u4ee3\u8868\u6027\u6d4b\u8bd5\u6848\u4f8b\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30Mamba SSM\u548cLLaMA Transformer\uff1a1)\u8ba1\u7b97\u6548\u7387(\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u6d4b\u8bd5512\u52308,192\u4e2atoken)\uff1b2)\u8868\u793a\u6548\u7387(\u5206\u6790\u9690\u85cf\u72b6\u6001\u52a8\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f)\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u660e\u786e\u4e86SSM\u76f8\u5bf9\u4e8eTransformer\u5177\u6709\u4f18\u52bf\u7684\u5177\u4f53\u6761\u4ef6\u3002", "conclusion": "\u8be5\u57fa\u51c6\u7814\u7a76\u4e3a\u9009\u62e9\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u6839\u636e\u5177\u4f53\u6761\u4ef6\u5728SSM\u548cTransformer\u4e4b\u95f4\u505a\u51fa\u660e\u667a\u9009\u62e9\u3002"}}
{"id": "2601.01268", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01268", "abs": "https://arxiv.org/abs/2601.01268", "authors": ["Maayan Gelboim", "Amir Adler", "Mauricio Araya-Polo"], "title": "Accelerated Full Waveform Inversion by Deep Compressed Learning", "comment": null, "summary": "We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u6ce2\u5f62\u53cd\u6f14\u6570\u636e\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u9009\u62e9\u5173\u952e\u5730\u9707\u6570\u636e\uff0c\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u8868\u793a\u5b66\u4e60\u548cK-means\u805a\u7c7b\uff0c\u5b9e\u73b0\u5206\u5c42\u6570\u636e\u9009\u62e9\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5de5\u4e1a\u7ea7\u5168\u6ce2\u5f62\u53cd\u6f14\u9700\u8981\u592a\u5b57\u8282\u7ea7\u522b\u6570\u636e\u5b58\u50a8\uff0c\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u9650\u5236\u4e86\u590d\u6742\u5730\u4e0b\u60c5\u51b5\u5206\u6790\u548c\u591a\u573a\u666f\u63a2\u7d22\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u964d\u7ef4\u65b9\u6cd5\u6765\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "1. \u4f7f\u7528\u5e26\u4e8c\u503c\u5316\u611f\u77e5\u5c42\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u4ece\u5927\u91cf\u5730\u4e0b\u6a21\u578b\u4e2d\u5b66\u4e60\u7b80\u6d01\u4f46\u5173\u952e\u7684\u5730\u9707\u91c7\u96c6\u5e03\u5c40\uff1b2. \u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u4ece\u5927\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u8f83\u5c0f\u5b50\u96c6\uff1b3. \u901a\u8fc7\u81ea\u7f16\u7801\u5668\u8ba1\u7b97\u6570\u636e\u7684\u6f5c\u5728\u8868\u793a\uff1b4. \u5bf9\u6f5c\u5728\u8868\u793a\u8fdb\u884cK-means\u805a\u7c7b\uff0c\u8fdb\u4e00\u6b65\u9009\u62e9\u6700\u76f8\u5173\u7684FWI\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u752810%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u57282D FWI\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u968f\u673a\u6570\u636e\u91c7\u6837\uff0c\u4e3a\u5927\u89c4\u6a213D\u53cd\u6f14\u52a0\u901fFWI\u94fa\u5e73\u4e86\u9053\u8def\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u9009\u62e9\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11FWI\u8f93\u5165\u7ef4\u5ea6\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5904\u7406\u5927\u89c4\u6a21\u5730\u9707\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01290", "abs": "https://arxiv.org/abs/2601.01290", "authors": ["Harshita Narnoli", "Mihai Surdeanu"], "title": "The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification", "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025", "summary": "In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5f53\u6f14\u793a\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLM\u884c\u4e3a\u7c7b\u4f3c\u4e8ekNN\u5206\u7c7b\u5668\uff1b\u5f53\u76f8\u5173\u6027\u4f4e\u65f6\uff0cLLM\u80fd\u5229\u7528\u53c2\u6570\u8bb0\u5fc6\u83b7\u5f97\u66f4\u597d\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5b9e\u8df5\u4e2d\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5176\u5de5\u4f5c\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76LLM\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u884c\u4e3a\u673a\u5236\uff0c\u7279\u522b\u662f\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5206\u7c7b\u4f5c\u4e3a\u7528\u4f8b\uff0c\u57286\u4e2a\u6570\u636e\u96c6\u548c3\u4e2aLLM\u4e0a\uff0c\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u57fa\u4e8e\u76f8\u540c\u6f14\u793a\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u5668\uff08\u68af\u5ea6\u4e0b\u964d\u548ck\u8fd1\u90bb\uff09\u8fdb\u884c\u884c\u4e3a\u6bd4\u8f83\u3002", "result": "\u5f53\u6f14\u793a\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLM\u884c\u4e3a\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u76f8\u4f3c\uff0c\u4e14\u66f4\u63a5\u8fd1kNN\u800c\u975e\u903b\u8f91\u56de\u5f52\uff1b\u5f53\u6f14\u793a\u76f8\u5173\u6027\u4f4e\u65f6\uff0cLLM\u8868\u73b0\u4f18\u4e8e\u8fd9\u4e9b\u5206\u7c7b\u5668\uff0c\u56e0\u5176\u80fd\u5229\u7528\u53c2\u6570\u8bb0\u5fc6\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u5728\u6f14\u793a\u76f8\u5173\u65f6\u7c7b\u4f3c\u4e8ekNN\u5206\u7c7b\uff0c\u5728\u6f14\u793a\u4e0d\u76f8\u5173\u65f6\u80fd\u5229\u7528LLM\u7684\u53c2\u6570\u8bb0\u5fc6\u4f18\u52bf\uff0c\u8fd9\u4e3a\u7406\u89e3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2601.01295", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01295", "abs": "https://arxiv.org/abs/2601.01295", "authors": ["Changhoon Song", "Seungchan Ko", "Youngjoon Hong"], "title": "Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space", "comment": null, "summary": "Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \\le 1/2$. In this paper, we introduce a log-weighted Barron space $\\mathscr{B}^{\\log}$, which requires a strictly weaker assumption than $\\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\\mathscr{B}^{\\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\\mathscr{B}^{s,\\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\uff0c\u8bc1\u660e\u6df1\u5ea6ReLU\u7f51\u7edc\u5728\u8be5\u7a7a\u95f4\u4e2d\u5177\u6709\u66f4\u4f4e\u7684\u89c4\u5f8b\u6027\u8981\u6c42\uff0c\u4e3a\u6df1\u5ea6\u67b6\u6784\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u6210\u529f\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u7ecf\u5178Barron\u7a7a\u95f4\u7406\u8bba\u867d\u7136\u89e3\u91ca\u4e86\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u80fd\u529b\uff0c\u4f46\u5bf9\u76ee\u6807\u51fd\u6570\u7684\u89c4\u5f8b\u6027\u8981\u6c42\u4ecd\u7136\u8fc7\u9ad8\uff08\u6bd4Sobolev\u7a7a\u95f4\u66f4\u5f3a\uff09\uff0c\u4e14\u73b0\u6709\u6df1\u5ea6\u654f\u611f\u7ed3\u679c\u901a\u5e38\u5047\u8bbe\u7ea6\u675f\u6761\u4ef6\uff08\u5982sL \u2264 1/2\uff09\uff0c\u9650\u5236\u4e86\u7406\u8bba\u5bf9\u5b9e\u9645\u6df1\u5ea6\u6a21\u578b\u6210\u529f\u7684\u89e3\u91ca\u529b\u3002", "method": "1. \u5f15\u5165\u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\u212c^log\uff0c\u5176\u5047\u8bbe\u6761\u4ef6\u6bd4\u4efb\u4f55s>0\u7684\u212c^s\u7a7a\u95f4\u90fd\u5f31\uff1b2. \u7814\u7a76\u8be5\u7a7a\u95f4\u7684\u5d4c\u5165\u6027\u8d28\u5e76\u901a\u8fc7Rademacher\u590d\u6742\u5ea6\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff1b3. \u8bc1\u660e\u212c^log\u4e2d\u51fd\u6570\u53ef\u7531\u6df1\u5ea6ReLU\u7f51\u7edc\u8fd1\u4f3c\uff0c\u5e76\u7ed9\u51fa\u663e\u5f0f\u7684\u6df1\u5ea6\u4f9d\u8d56\u5173\u7cfb\uff1b4. \u5b9a\u4e49\u212c^{s,log}\u7a7a\u95f4\u65cf\uff0c\u5efa\u7acbH^1\u8303\u6570\u4e0b\u7684\u8fd1\u4f3c\u754c\uff0c\u5e76\u786e\u5b9a\u4fdd\u6301\u8fd9\u4e9b\u901f\u7387\u7684\u6700\u5927\u6df1\u5ea6\u5c3a\u5ea6\u3002", "result": "1. \u5bf9\u6570\u52a0\u6743Barron\u7a7a\u95f4\u212c^log\u63d0\u4f9b\u4e86\u6bd4\u7ecf\u5178Barron\u7a7a\u95f4\u66f4\u5f31\u7684\u89c4\u5f8b\u6027\u8981\u6c42\uff1b2. \u6df1\u5ea6ReLU\u7f51\u7edc\u5728\u8be5\u7a7a\u95f4\u4e2d\u5177\u6709\u660e\u786e\u7684\u6df1\u5ea6\u4f9d\u8d56\u8fd1\u4f3c\u80fd\u529b\uff1b3. \u5efa\u7acb\u4e86H^1\u8303\u6570\u4e0b\u7684\u8fd1\u4f3c\u754c\uff0c\u5e76\u786e\u5b9a\u4e86\u4fdd\u6301\u901f\u7387\u7684\u6700\u5927\u6df1\u5ea6\u5c3a\u5ea6\u3002", "conclusion": "\u6df1\u5ea6\u51cf\u5c11\u4e86\u9ad8\u6548\u8868\u793a\u6240\u9700\u7684\u89c4\u5f8b\u6027\u8981\u6c42\uff0c\u4e3a\u6df1\u5ea6\u67b6\u6784\u8d85\u8d8a\u7ecf\u5178Barron\u8bbe\u7f6e\u7684\u5b9e\u9645\u6027\u80fd\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u89e3\u91ca\u4e86\u5176\u5728\u5f53\u4eca\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7a33\u5b9a\u4f7f\u7528\u7684\u539f\u56e0\u3002"}}
{"id": "2601.01297", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01297", "abs": "https://arxiv.org/abs/2601.01297", "authors": ["Anantha Sharma"], "title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System", "comment": "26 pages", "summary": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.\n  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.\n  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.", "AI": {"tldr": "Argus\u6846\u67b6\u5c06\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u6570\u636e\u6d41\u5f62\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\u4e0a\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\uff0c\u901a\u8fc7Voronoi\u5212\u5206\u5b9e\u73b0\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u7684\u6f02\u79fb\u5ea6\u91cf\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u5e76\u80fd\u5b9a\u4f4d\u6f02\u79fb\u7a7a\u95f4\u4f4d\u7f6e\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5168\u5c40\u6bd4\u8f83\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u3001\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u4e22\u5931\u51e0\u4f55\u7ed3\u6784\u3001\u91cd\u65b0\u805a\u7c7b\u65b9\u6cd5\u5b58\u5728\u8eab\u4efd\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7ef4\u7ed3\u6784\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "Argus\u6846\u67b6\u5c06\u6f02\u79fb\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u6570\u636e\u6d41\u5f62\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\u4e0a\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\u3002\u4f7f\u7528Voronoi\u5212\u5206\u5728\u89c4\u8303\u6b63\u4ea4\u57fa\u4e0a\u521b\u5efa\u7a7a\u95f4\u5206\u533a\uff0c\u8bc1\u660e\u8fd9\u79cd\u5212\u5206\u4ea7\u751f\u7684\u6f02\u79fb\u5ea6\u91cf\u5bf9\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u3002\u5f15\u5165\u56fe\u8bba\u65b9\u6cd5\u533a\u5206\u8fde\u8d2f\u5206\u5e03\u6f02\u79fb\u4e0e\u5b64\u7acb\u6270\u52a8\uff0c\u4f7f\u7528\u4e58\u79ef\u91cf\u5316\u5212\u5206\u6269\u5c55\u5230\u8d85\u9ad8\u7ef4\u5ea6\u3002", "result": "\u6846\u67b6\u5b9e\u73b0O(N)\u590d\u6742\u5ea6\uff0c\u63d0\u4f9b\u5355\u5143\u7ea7\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u80fd\u6b63\u786e\u8bc6\u522b\u5750\u6807\u65cb\u8f6c\u4e0b\u7684\u6f02\u79fb\u800c\u73b0\u6709\u65b9\u6cd5\u4f1a\u4ea7\u751f\u8bef\u62a5\u3002\u56fe\u8bba\u65b9\u6cd5\u80fd\u6709\u6548\u533a\u5206\u6f02\u79fb\u4f20\u64ad\u6a21\u5f0f\uff0c\u4e58\u79ef\u91cf\u5316\u5212\u5206\u53ef\u6269\u5c55\u5230500+\u7ef4\u5ea6\u3002", "conclusion": "Argus\u4e3a\u5206\u5e03\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u5728\u4fdd\u6301\u9ad8\u7ef4\u7ed3\u6784\u7684\u540c\u65f6\u907f\u514d\u4e86\u6210\u5bf9\u6bd4\u8f83\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u901a\u8fc7\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u6f02\u79fb\u68c0\u6d4b\u3002"}}
{"id": "2601.01306", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01306", "abs": "https://arxiv.org/abs/2601.01306", "authors": ["John Zhao"], "title": "Towards a Principled Muon under $\u03bc\\mathsf{P}$: Ensuring Spectral Conditions throughout Training", "comment": "21 pages, 0 figures", "summary": "The $\u03bc$-parameterization ($\u03bc$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $\u03bc$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $\u03bc$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $\u03bc$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $\u03bc$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $\u03bc$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMuon++\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u77e9\u9635\u4f18\u5316\u5668\uff0c\u80fd\u591f\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u9760\u5730\u6ee1\u8db3\u03bcP\u7684\u8c31\u6761\u4ef6\uff0c\u65e0\u9700\u5bf9\u6743\u91cd\u8fdb\u884c\u663e\u5f0f\u8c31\u5f52\u4e00\u5316\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u03bcP\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u77e9\u9635\u4f18\u5316\u5668\uff08\u5982Muon\uff09\u5728\u03bcP\u6846\u67b6\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u65e0\u6cd5\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u8bc1\u8c31\u6761\u4ef6\uff0c\u8981\u4e48\u9700\u8981\u9891\u7e41\u7684\u8c31\u5f52\u4e00\u5316\u64cd\u4f5c\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51faMuon++\uff0c\u6838\u5fc3\u6d1e\u5bdf\u662f\u5bf9\u4e8e\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff0c\u4ec5\u9700\u5728\u4f18\u5316\u5668\u66f4\u65b0\u5c42\u9762\u7ef4\u6301\u8c31\u63a7\u5236\u5373\u53ef\u4fdd\u6301\u03bcP\u517c\u5bb9\u7684\u7f29\u653e\u7279\u6027\uff0c\u65e0\u9700\u5bf9\u6743\u91cd\u8fdb\u884c\u663e\u5f0f\u8c31\u5f52\u4e00\u5316\u3002\u540c\u65f6\u9996\u6b21\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\u3002", "result": "Muon++\u80fd\u591f\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u9760\u5730\u6ee1\u8db3\u03bcP\u7684\u8c31\u6761\u4ef6\uff0c\u586b\u8865\u4e86\u03bcP\u7406\u8bba\u4e0e\u77e9\u9635\u4f18\u5316\u5668\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u66f4\u9002\u5408\u957f\u65f6\u7a0bLLM\u8bad\u7ec3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Muon++\u65b9\u6cd5\u901a\u8fc7\u4ec5\u63a7\u5236\u4f18\u5316\u5668\u66f4\u65b0\u7684\u8c31\u7279\u6027\uff0c\u53ef\u9760\u5730\u4fdd\u8bc1\u4e86\u03bcP\u7684\u8c31\u6761\u4ef6\uff0c\u4e3a\u77e9\u9635\u4f18\u5316\u5668\u5728\u03bcP\u6846\u67b6\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\u6982\u5ff5\u3002"}}
{"id": "2601.01313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01313", "abs": "https://arxiv.org/abs/2601.01313", "authors": ["Vladimer Khasia"], "title": "Spectral-Window Hybrid (SWH)", "comment": null, "summary": "Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \\textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \\textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\\mathcal{O}(T \\log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH", "AI": {"tldr": "SWH\u662f\u4e00\u79cd\u5e76\u884c\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u5206\u652f\uff08\u57fa\u4e8e\u5377\u79ef\u5b9a\u7406\uff09\u548c\u5c40\u90e8\u5206\u652f\uff08\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff09\u89e3\u8026\u5e8f\u5217\u5efa\u6a21\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u81f3\u957f\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u7cbe\u5ea6\u3002", "motivation": "Transformer\u7684\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faSpectral-Window Hybrid (SWH)\u67b6\u6784\uff0c\u5c06\u5e8f\u5217\u5efa\u6a21\u89e3\u8026\u4e3a\u4e24\u4e2a\u5e76\u884c\u6d41\uff1a\u5168\u5c40\u5206\u652f\u5229\u7528\u5377\u79ef\u5b9a\u7406\u5efa\u6a21\u957f\u7a0b\u8870\u51cf\u52a8\u6001\uff08O(T log T)\u65f6\u95f4\uff09\uff0c\u5c40\u90e8\u5206\u652f\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5904\u7406\u6709\u754c\u4e0a\u4e0b\u6587\u5185\u7684token\u4ea4\u4e92\u3002", "result": "SWH\u5728\u77ed\u4e0a\u4e0b\u6587\u4e0a\u8fbe\u5230\u6807\u51c6Transformer\u7684\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u80fd\u591f\u9ad8\u6548\u7ebf\u6027\u6269\u5c55\u5230\u957f\u5e8f\u5217\u3002", "conclusion": "SWH\u901a\u8fc7\u5e76\u884c\u53cc\u6d41\u8bbe\u8ba1\u907f\u514d\u4e86\u5168\u5c40\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5c40\u90e8\u7cbe\u5ea6\uff0c\u4e3a\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01347", "abs": "https://arxiv.org/abs/2601.01347", "authors": ["Yuyan Pi", "Min Jin", "Wentao Xie", "Xinhua Liu"], "title": "From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion", "comment": "34 pages,5 figures", "summary": "Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.", "AI": {"tldr": "GM-MLG\uff1a\u57fa\u4e8e\u56fe-\u57fa\u5e8f\u7279\u5f81\u878d\u5408\u548c\u591a\u6807\u7b7e\u751f\u6210\u7684\u5f00\u653e\u5f0f\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u5c06ADR\u9884\u6d4b\u4ece\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u5316\u4e3aTransformer\u89e3\u7801\u5668\u9a71\u52a8\u7684\u591a\u6807\u7b7e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u5e76\u6269\u5c55\u9884\u6d4b\u7a7a\u95f4", "motivation": "\u5f53\u524d\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b9\u6cd5\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u836f\u7269\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3001\u5c01\u95ed\u6807\u7b7e\u96c6\u9650\u5236\u3001\u4ee5\u53ca\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u8ba1\u7b97\u751f\u7269\u5b66\u5728\u964d\u4f4e\u65b0\u836f\u5f00\u53d1\u6210\u672c\u548c\u65f6\u95f4\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faGM-MLG\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u539f\u5b50\u7ea7\u3001\u5c40\u90e8\u5206\u5b50\u7ea7\uff08\u901a\u8fc7BRICS\u7b97\u6cd5\u52a8\u6001\u63d0\u53d6\u7ec6\u7c92\u5ea6\u57fa\u5e8f\uff09\u548c\u5168\u5c40\u5206\u5b50\u7ea7\u7684\u53cc\u56fe\u8868\u793a\u67b6\u6784\uff1b2\uff09\u5c06ADR\u9884\u6d4b\u4ece\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u5316\u4e3a\u57fa\u4e8eTransformer\u89e3\u7801\u5668\u7684\u591a\u6807\u7b7e\u751f\u6210\uff0c\u5c06ADR\u6807\u7b7e\u89c6\u4e3a\u79bb\u6563\u6807\u8bb0\u5e8f\u5217\uff1b3\uff09\u4f7f\u7528\u4f4d\u7f6e\u5d4c\u5165\u663e\u5f0f\u6355\u83b7\u5927\u89c4\u6a21\u6807\u7b7e\u7a7a\u95f4\u4e2d\u7684\u4f9d\u8d56\u548c\u5171\u73b0\u5173\u7cfb\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u52a8\u6001\u6269\u5c55\u9884\u6d4b\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGM-MLG\u5b9e\u73b0\u4e86\u6700\u9ad838%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u589e\u76ca\u8fbe20%\uff0c\u5c06\u9884\u6d4b\u7a7a\u95f4\u4ece200\u79cd\u6269\u5c55\u5230\u8d85\u8fc710,000\u79cd\u3002\u901a\u8fc7\u9006\u5408\u6210\u57fa\u5e8f\u5206\u6790\u9610\u660e\u4e86ADR\u4e0e\u57fa\u5e8f\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u6784\u6548\u5173\u7cfb\u3002", "conclusion": "GM-MLG\u5f00\u521b\u4e86\u5f00\u653e\u5f0fADR\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u3001\u6807\u7b7e\u96c6\u5c01\u95ed\u548c\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u836f\u7269\u5b89\u5168\u7cfb\u7edf\u6027\u98ce\u9669\u964d\u4f4e\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u521b\u65b0\u652f\u6301\u3002"}}
{"id": "2601.01357", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.01357", "abs": "https://arxiv.org/abs/2601.01357", "authors": ["Ke Xiao", "Haoze Zhang", "Runze Mao", "Han Li", "Zhi X. Chen"], "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows", "comment": null, "summary": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.", "AI": {"tldr": "FlamePilot\u662f\u4e00\u4e2aLLM\u667a\u80fd\u4f53\uff0c\u4e13\u95e8\u7528\u4e8e\u71c3\u70e7\u5efa\u6a21\u7814\u7a76\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u81ea\u6821\u6b63\u7684CFD\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u79d1\u5b66\u6587\u732e\u77e5\u8bc6\u4e0eCFD\u5de5\u5177\u6267\u884c\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u7684\u6210\u529f\u7387\u548c\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u590d\u6742\u79d1\u5b66\u9886\u57df\uff08\u5982\u71c3\u70e7\u5efa\u6a21\uff09\u5b58\u5728\u5173\u952e\u7f3a\u53e3\uff0c\u9700\u8981\u5c06\u9886\u57df\u6587\u732e\u77e5\u8bc6\u4e0e\u4e13\u4e1a\u5de5\u5177\uff08\u5982CFD\u4ee3\u7801\uff09\u7684\u6267\u884c\u80fd\u529b\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u7528\u7684AI\u8f85\u52a9\u7814\u7a76\u3002", "method": "FlamePilot\u91c7\u7528\u57fa\u4e8e\u539f\u5b50\u5de5\u5177\u7684\u67b6\u6784\uff0c\u786e\u4fdd\u5728OpenFOAM\u548cDeepFlame\u7b49\u6846\u67b6\u4e2d\u7a33\u5065\u8bbe\u7f6e\u548c\u6267\u884c\u590d\u6742\u6a21\u62df\uff1b\u7cfb\u7edf\u80fd\u591f\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u5b66\u4e60\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u6307\u5bfc\u4ece\u521d\u59cb\u8bbe\u7f6e\u5230\u4f18\u5316\u7ed3\u679c\u7684\u6574\u4e2a\u6a21\u62df\u8fc7\u7a0b\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlamePilot\u83b7\u5f97\u4e86\u5b8c\u7f8e\u76841.0\u53ef\u6267\u884c\u6027\u5206\u6570\u548c0.438\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u5148\u524d\u6700\u4f73\u4ee3\u7406\u76840.625\u548c0.250\u5206\u6570\uff1b\u5728MILD\u71c3\u70e7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u914d\u7f6e\u6a21\u62df\u3001\u6267\u884c\u6a21\u62df\u3001\u540e\u5904\u7406\u7ed3\u679c\u3001\u63d0\u51fa\u57fa\u4e8e\u8bc1\u636e\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5e76\u5728\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u4e0b\u7ba1\u7406\u591a\u6b65\u53c2\u6570\u7814\u7a76\u76f4\u81f3\u6536\u655b\u3002", "conclusion": "FlamePilot\u901a\u8fc7\u900f\u660e\u53ef\u89e3\u91ca\u7684\u8303\u5f0f\uff0c\u4e3aAI\u8d4b\u80fd\u7684\u71c3\u70e7\u5efa\u6a21\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u7814\u7a76\u4eba\u5458\u4e0e\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u667a\u80fd\u4f53\u8d1f\u8d23\u5de5\u4f5c\u6d41\u7a0b\u7f16\u6392\uff0c\u7814\u7a76\u4eba\u5458\u5219\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6b21\u5206\u6790\u3002"}}
{"id": "2601.01368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01368", "abs": "https://arxiv.org/abs/2601.01368", "authors": ["Mujin Zhou", "Junzhe Zhang"], "title": "Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach", "comment": null, "summary": "Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ef-GAN\u6846\u67b6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u4ece\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u4e8c\u5143\u56e0\u679c\u7ed3\u6784\uff0c\u5c06\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u91cf\uff0c\u5e76\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e0e\u6a21\u578b\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684f-\u6563\u5ea6\u3002", "motivation": "\u4ece\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u72ec\u7acb\u4e8e\u7279\u5b9a\u6743\u91cd\u503c\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8ef-GAN\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u91cf\uff0c\u5e76\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316f-\u6563\u5ea6\u3002\u901a\u8fc7f-GAN\u6846\u67b6\u5c06\u76ee\u6807\u8f6c\u5316\u4e3amin-max\u5bf9\u6297\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528Gumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u68af\u5ea6\u641c\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u4e8c\u5143\u56e0\u679c\u7ed3\u6784\uff0c\u901a\u8fc7\u5bf9\u6297\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u6709\u6548\u7684\u7ed3\u6784\u5b66\u4e60\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8ef-GAN\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u4f18\u5316\u548c\u79bb\u6563\u7a7a\u95f4\u68af\u5ea6\u641c\u7d22\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u56e0\u679c\u53d1\u73b0\u3002"}}
{"id": "2601.01383", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01383", "abs": "https://arxiv.org/abs/2601.01383", "authors": ["Yen-Chia Chen", "Hsing-Kuo Pao", "Hanjuan Huang"], "title": "Data Complexity-aware Deep Model Performance Forecasting", "comment": "12 pages, 12 figures", "summary": "Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u80fd\u5728\u8bad\u7ec3\u524d\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u548c\u6a21\u578b\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u5b9e\u9645\u8bad\u7ec3\u6216\u590d\u6742\u6a21\u62df\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u67b6\u6784\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u8bd5\u9519\u6cd5\uff0c\u8017\u65f6\u8017\u8d44\u6e90\u4e14\u96be\u4ee5\u81ea\u52a8\u5316\u3002\u73b0\u6709\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u90e8\u5206\u8bad\u7ec3\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u6570\u636e\u96c6\u7684\u53ef\u6d4b\u91cf\u5c5e\u6027\u9884\u6d4b\u57fa\u7ebf\u6027\u80fd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u6a21\u578b\u67b6\u6784\u548c\u8d85\u53c2\u6570\u7ec6\u8282\u8c03\u6574\u4f30\u8ba1\u3002\u6846\u67b6\u53ef\u8de8\u6570\u636e\u96c6\u548c\u6a21\u578b\u7c7b\u578b\u6cdb\u5316\u3002", "result": "\u6846\u67b6\u4e0d\u4ec5\u80fd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u6307\u5bfc\u67b6\u6784\u9009\u62e9\u3001\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u5728\u8bad\u7ec3\u524d\u68c0\u6d4b\u6f5c\u5728\u95ee\u9898\u6570\u636e\u96c6\u3002\u53d1\u73b0\u6570\u636e\u96c6\u65b9\u5dee\u7b49\u7279\u5f81\u53ef\u4f5c\u4e3a\u6570\u636e\u8d28\u91cf\u7684\u65e9\u671f\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u6a21\u578b\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\uff0c\u51cf\u5c11\u8bd5\u9519\u6210\u672c\uff0c\u540c\u65f6\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u6570\u636e\u5904\u7406\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2601.01387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01387", "abs": "https://arxiv.org/abs/2601.01387", "authors": ["Yongzhe Li", "Lin Guan", "Zihan Cai", "Zuxian Lin", "Jiyu Huang", "Liukai Chen"], "title": "Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning", "comment": null, "summary": "Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.", "AI": {"tldr": "\u63d0\u51faSaMPFA\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u548c\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u5347\u7535\u529b\u7cfb\u7edf\u6f6e\u6d41\u5206\u6790\u5728\u4e0d\u540c\u7cfb\u7edf\u89c4\u6a21\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u5bf9\u62d3\u6251\u53d8\u5316\u5177\u6709\u5f3a\u9002\u5e94\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6f6e\u6d41\u5206\u6790\u5177\u6709\u91cd\u8981\u5b9e\u9645\u610f\u4e49\uff0c\u9700\u8981\u589e\u5f3a\u6a21\u578b\u5728\u53ef\u53d8\u7cfb\u7edf\u89c4\u6a21\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u652f\u8def\u529f\u7387\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faSaMPFA\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u6280\u672f\uff0c\u4ece\u5b8c\u6574\u7535\u7f51\u4e2d\u63d0\u53d6\u4e0d\u540c\u89c4\u6a21\u7684\u5b50\u56fe\u4ee5\u589e\u5f3a\u8de8\u5c3a\u5ea6\u5b66\u4e60\u80fd\u529b\uff1b2) \u65e0\u53c2\u8003\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4b\u6bcd\u7ebf\u7535\u538b\u548c\u652f\u8def\u529f\u7387\u800c\u975e\u76f8\u89d2\uff0c\u907f\u514d\u8bef\u5dee\u653e\u5927\u5e76\u5b66\u4e60\u76f8\u89d2\u5dee\u7684\u7269\u7406\u5173\u7cfb\uff1b3) \u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u989d\u5916\u9879\uff0c\u9f13\u52b1\u6a21\u578b\u6355\u6349\u89d2\u5dee\u548c\u529f\u7387\u4f20\u8f93\u7684\u7269\u7406\u6a21\u5f0f\u3002", "result": "\u5728IEEE 39\u8282\u70b9\u7cfb\u7edf\u548c\u5b9e\u9645\u7701\u7ea7\u7535\u7f51\u4e0a\u7684\u4eff\u771f\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u53ef\u53d8\u7cfb\u7edf\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e864.47%\u548c36.82%\u3002", "conclusion": "SaMPFA\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7535\u529b\u7cfb\u7edf\u6f6e\u6d41\u5206\u6790\u4e2d\u7684\u8de8\u5c3a\u5ea6\u9002\u5e94\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4e3a\u62d3\u6251\u53d8\u5316\u7684\u6f6e\u6d41\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01403", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01403", "abs": "https://arxiv.org/abs/2601.01403", "authors": ["Zewei Yu", "Jianqiu Xu", "Caimin Li"], "title": "A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble", "comment": "8 pages", "summary": "With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.", "AI": {"tldr": "GDME\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u6c60\u548c\u56fe\u7ed3\u6784\u8fdb\u884c\u6a21\u578b\u96c6\u6210\uff0c\u80fd\u6709\u6548\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\u5e76\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u3002", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u6d41\u6570\u636e\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u8bbe\u8ba1\u6216\u96be\u4ee5\u6709\u6548\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\uff0c\u4e14\u6570\u636e\u6a21\u5f0f\u591a\u6837\u4e14\u5feb\u901f\u6f14\u53d8\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faGDME\u6846\u67b6\uff1a\u7ef4\u62a4\u52a8\u6001\u6a21\u578b\u6c60\uff08\u6301\u7eed\u4fee\u526a\u6027\u80fd\u4e0d\u4f73\u6a21\u578b\u5e76\u5f15\u5165\u65b0\u6a21\u578b\uff09\uff1b\u4f7f\u7528\u52a8\u6001\u56fe\u7ed3\u6784\u8868\u793a\u6a21\u578b\u95f4\u5173\u7cfb\uff1b\u901a\u8fc7\u56fe\u4e0a\u7684\u793e\u533a\u68c0\u6d4b\u9009\u62e9\u9002\u5f53\u7684\u96c6\u6210\u5b50\u96c6\uff1b\u5229\u7528\u56fe\u7ed3\u6784\u53d8\u5316\u76d1\u6d4b\u6982\u5ff5\u6f02\u79fb\u3002", "result": "\u5728\u4e03\u4e2a\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGDME\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347\u9ad8\u8fbe24%\uff1b\u5176\u96c6\u6210\u7b56\u7565\u76f8\u6bd4\u5355\u4e2a\u6a21\u578b\u548c\u5e73\u5747\u96c6\u6210\u5177\u6709\u66f4\u4f18\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "GDME\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u6c60\u548c\u56fe\u7ed3\u6784\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\u548c\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\u7684\u6311\u6218\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.01417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01417", "abs": "https://arxiv.org/abs/2601.01417", "authors": ["Itay Safran"], "title": "A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory", "comment": null, "summary": "We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $\u03a9\\big(d^{1+\\frac{1}{2^{k-2}-1}}\\big)$ is necessary to represent the maximum for any depth $3\\le k\\le \\log_2(\\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Tur\u00e1n's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86ReLU\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97d\u4e2a\u5b9e\u6570\u6700\u5927\u503c\u51fd\u6570\u9700\u8981\u8d85\u7ebf\u6027\u5bbd\u5ea6\uff0c\u5f53\u6df1\u5ea63\u2264k\u2264log\u2082(log\u2082(d))\u65f6\uff0c\u5bbd\u5ea6\u81f3\u5c11\u4e3a\u03a9(d^{1+1/(2^{k-2}-1)})\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6k\u22653\u7684\u65e0\u6761\u4ef6\u8d85\u7ebf\u6027\u4e0b\u754c\u3002", "motivation": "\u6700\u5927\u503c\u51fd\u6570\u662f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u57fa\u672c\u7b97\u5b50\uff0c\u4f46\u5bf9\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6d45\u5c42\u7f51\u7edc\uff0c\u5bf9\u4e8e\u6df1\u5c42\u7f51\u7edc\u8ba1\u7b97\u6700\u5927\u503c\u6240\u9700\u7684\u6700\u5c0f\u5bbd\u5ea6\u548c\u6df1\u5ea6\u5173\u7cfb\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63ed\u793a\u6700\u5927\u503c\u51fd\u6570\u7684\u5185\u5728\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u8bba\u8bc1\u65b9\u6cd5\uff0c\u5c06\u6700\u5927\u503c\u51fd\u6570\u7684\u4e0d\u53ef\u5fae\u5206\u810a\u7ebf\u4e0e\u8ba1\u7b97\u7f51\u7edc\u7b2c\u4e00\u9690\u85cf\u5c42\u8bf1\u5bfc\u7684\u56fe\u4e2d\u7684\u56e2\u76f8\u5173\u8054\u3002\u5229\u7528\u6781\u503c\u56fe\u8bba\u4e2d\u7684Tur\u00e1n\u5b9a\u7406\uff0c\u8bc1\u660e\u8db3\u591f\u7a84\u7684\u7f51\u7edc\u65e0\u6cd5\u6355\u6349\u6700\u5927\u503c\u51fd\u6570\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6df1\u5ea6\u5c42\u6b21\u5b9a\u7406\uff1a\u5bf9\u4e8e\u4efb\u610f\u6df1\u5ea63\u2264k\u2264log\u2082(log\u2082(d))\uff0c\u8868\u793a\u6700\u5927\u503c\u51fd\u6570\u9700\u8981\u5bbd\u5ea6\u03a9(d^{1+1/(2^{k-2}-1)})\u3002\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6k\u22653\u7684\u65e0\u6761\u4ef6\u8d85\u7ebf\u6027\u4e0b\u754c\uff0c\u5373\u4f7f\u6df1\u5ea6\u968fd\u53d8\u5316\u4e5f\u6210\u7acb\u3002", "conclusion": "\u5c3d\u7ba1\u6700\u5927\u503c\u51fd\u6570\u770b\u4f3c\u7b80\u5355\uff0c\u4f46\u5176\u4e0d\u53ef\u5fae\u5206\u8d85\u5e73\u9762\u7684\u51e0\u4f55\u7ed3\u6784\u8d4b\u4e88\u4e86\u56fa\u6709\u7684\u590d\u6742\u6027\u3002\u8be5\u8bc1\u660e\u6280\u672f\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0b\u754c\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u4e0e\u5bbd\u5ea6\u5728\u8868\u793a\u80fd\u529b\u4e0a\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2601.01424", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.01424", "abs": "https://arxiv.org/abs/2601.01424", "authors": ["Akshay Sasi", "Malavika Pradeep", "Nusaibah Farrukh", "Rahul Venugopal", "Elizabeth Sherly"], "title": "Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance", "comment": "6 pages, 6 figures. Code available at https://github.com/AkshaySasi/Unveiling-the-Heart-Brain-Connection-An-Analysis-of-ECG-in-Cognitive-Performance. Presented at AIHC (not published)", "summary": "Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528ECG\u4fe1\u53f7\u66ff\u4ee3EEG\u8fdb\u884c\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001XGBoost\u6846\u67b6\u5c06ECG\u7279\u5f81\u6620\u5c04\u5230EEG\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ec5\u7528ECG\u5373\u53ef\u51c6\u786e\u5206\u7c7b\u8ba4\u77e5\u72b6\u6001\u3002", "motivation": "\u867d\u7136EEG\u662f\u8bc4\u4f30\u5fc3\u7406\u5de5\u4f5c\u8d1f\u8377\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u4fbf\u643a\u6027\u6709\u9650\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u5e7f\u6cdb\u53ef\u7528\u7684\u53ef\u7a7f\u6234\u8bbe\u5907ECG\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9700\u8981\u9a8c\u8bc1ECG\u662f\u5426\u80fd\u53ef\u9760\u53cd\u6620\u8ba4\u77e5\u8d1f\u8377\u5e76\u4f5c\u4e3aEEG\u6307\u6807\u7684\u4ee3\u7406\u3002", "method": "\u6536\u96c6\u5de5\u4f5c\u8bb0\u5fc6\u548c\u88ab\u52a8\u542c\u529b\u4efb\u52a1\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u53d6ECG\u65f6\u57dfHRV\u6307\u6807\u548cCatch22\u63cf\u8ff0\u7b26\uff0c\u5bf9\u5e94EEG\u9891\u8c31\u548cCatch22\u7279\u5f81\u3002\u63d0\u51fa\u8de8\u6a21\u6001XGBoost\u6846\u67b6\uff0c\u5c06ECG\u7279\u5f81\u6295\u5f71\u5230EEG\u4ee3\u8868\u7684\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ec5\u7528ECG\u8fdb\u884c\u5de5\u4f5c\u8d1f\u8377\u63a8\u65ad\u3002", "result": "ECG\u884d\u751f\u7684\u6295\u5f71\u80fd\u663e\u8457\u6355\u6349\u8ba4\u77e5\u72b6\u6001\u7684\u53d8\u5316\uff0c\u4e3a\u51c6\u786e\u5206\u7c7b\u63d0\u4f9b\u826f\u597d\u652f\u6301\u3002ECG\u7279\u5f81\u80fd\u6709\u6548\u6620\u5c04\u5230EEG\u8ba4\u77e5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\u3002", "conclusion": "ECG\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u3001\u5b9e\u65f6\u3001\u53ef\u7a7f\u6234\u7684\u65e5\u5e38\u8ba4\u77e5\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u751f\u7406\u8ba1\u7b97\u63d0\u4f9b\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.01452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01452", "abs": "https://arxiv.org/abs/2601.01452", "authors": ["Jian Feng", "Zhihong Huang"], "title": "Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models", "comment": "19 pages, 1 figures, 4 tables", "summary": "Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/\u03b3$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\\times$--1.08$\\times$ of MeZO).", "AI": {"tldr": "BSZO\u662f\u4e00\u79cd\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u7684\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edfZO\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5728\u4fdd\u6301\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u5355\u6b65\u968f\u673a\u6270\u52a8\u7684\u68af\u5ea6\u4f30\u8ba1\uff0c\u4fe1\u606f\u5229\u7528\u6548\u7387\u4f4e\uff0c\u6536\u655b\u901f\u5ea6\u6162\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u4fe1\u606f\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316(BSZO)\uff0c\u5c06\u6bcf\u4e2a\u6709\u9650\u5dee\u5206\u6d4b\u91cf\u89c6\u4e3a\u566a\u58f0\u89c2\u6d4b\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u6784\u5efa\u6295\u5f71\u68af\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u673a\u5236\u8c03\u6574\u6270\u52a8\u5c3a\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aBSZO\u5c06\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e86k/\u03b3\u500d\u3002\u5728RoBERTa\u3001Mistral\u548cOPT\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBSZO\u4f18\u4e8eMeZO\u3001MeZO-Adam\u548cHiZOO\uff0c\u5728OPT-13B\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad86.67%\u7684\u7edd\u5bf9\u5e73\u5747\u6539\u8fdb\uff0c\u540c\u65f6\u5185\u5b58\u4f7f\u7528\u4ec5\u4e3a\u57fa\u7840\u63a8\u7406\u76841.00-1.08\u500d\u3002", "conclusion": "BSZO\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u6709\u6548\u6574\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u7684\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u4f18\u52bf\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.01465", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01465", "abs": "https://arxiv.org/abs/2601.01465", "authors": ["Ze Peng", "Jian Zhang", "Yisen Wang", "Lei Qi", "Yinghuan Shi", "Yang Gao"], "title": "Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD", "comment": "Published as a conference paper at ICLR 2025", "summary": "Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called \"omniscient trajectory\". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $\u03a9(1)$ rates to $O(1/\\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\uff0c\u80fd\u66f4\u597d\u5730\u5229\u7528SGD\u7684\u5e73\u5766\u6027\u504f\u597d\uff0c\u5728\u6570\u503c\u4e0a\u66f4\u7d27\u4e14\u80fd\u6b63\u786e\u53cd\u6620\u5e73\u5766\u6027\u6539\u5584\u65f6\u7684\u6cdb\u5316\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\u867d\u7136\u7406\u8bba\u4e0a\u6570\u636e\u4f9d\u8d56\u548c\u7b97\u6cd5\u4f9d\u8d56\uff0c\u4f46\u672a\u80fd\u6709\u6548\u6355\u6349SGD\u5e73\u5766\u6027\u504f\u597d\u5bf9\u6cdb\u5316\u7684\u6539\u5584\uff0c\u4e14\u5728\u6570\u503c\u4e0a\u8f83\u677e\u3002\u9700\u8981\u5f00\u53d1\u80fd\u66f4\u597d\u5229\u7528\u5e73\u5766\u6027\u504f\u597d\u7684\u6cdb\u5316\u754c\u3002", "method": "\u63d0\u51fa\"\u5168\u77e5\u8f68\u8ff9\"\u6280\u672f\uff0c\u63a8\u5bfc\u51fa\u66f4\u5145\u5206\u5229\u7528\u5e73\u5766\u6027\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\u3002\u8be5\u754c\u8868\u660e\u5f53\u6700\u7ec8\u6743\u91cd\u534f\u65b9\u5dee\u7684\u5927\u65b9\u5dee\u65b9\u5411\u5728\u635f\u5931\u666f\u89c2\u4e2d\u5177\u6709\u5c0f\u5c40\u90e8\u66f2\u7387\u65f6\uff0c\u6a21\u578b\u6cdb\u5316\u66f4\u597d\u3002", "result": "\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u754c\u4e0d\u4ec5\u6b63\u786e\u53cd\u6620\u4e86\u5e73\u5766\u6027\u6539\u5584\u65f6\u7684\u6cdb\u5316\u63d0\u5347\uff0c\u4e14\u6570\u503c\u4e0a\u66f4\u7d27\u3002\u5e94\u7528\u4e8e\u51f8-Lipschitz-\u6709\u754c\u95ee\u9898\u7684\u68af\u5ea6\u4e0b\u964d\u6781\u5c0f\u5316\u8d85\u989d\u98ce\u9669\u65f6\uff0c\u5c06\u4ee3\u8868\u6027\u4fe1\u606f\u8bba\u754c\u7684\u03a9(1)\u7387\u6539\u8fdb\u4e3aO(1/\u221an)\u3002", "conclusion": "\u901a\u8fc7\"\u5168\u77e5\u8f68\u8ff9\"\u6280\u672f\uff0c\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u66f4\u597d\u5229\u7528SGD\u5e73\u5766\u6027\u504f\u597d\u7684\u4fe1\u606f\u8bba\u6cdb\u5316\u754c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u754c\u672a\u80fd\u6355\u6349\u5e73\u5766\u6027\u6539\u5584\u7684\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.01473", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.01473", "abs": "https://arxiv.org/abs/2601.01473", "authors": ["Myung-Hwan Jang", "Jeong-Min Park", "Yunyong Ko", "Sang-Wook Kim"], "title": "Accelerating Storage-Based Training for Graph Neural Networks", "comment": "10 pages, 12 figures, 2 tables, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2026", "summary": "Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \\textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \\textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \\textsf{AGNES}, that employs a method of \\textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \\textsf{AGNES} employs a simple yet effective strategy, \\textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \\textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.", "AI": {"tldr": "AGNES\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b58\u50a8\u7684GNN\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u72b6\u5b58\u50a8I/O\u5904\u7406\u548c\u8d85\u6279\u6b21\u5904\u7406\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684I/O\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b58\u50a8\u7684GNN\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u65f6\u9762\u4e34\u4e25\u91cd\u7684\u6570\u636e\u51c6\u5907\u74f6\u9888\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u91cf\u5c0f\u578b\u5b58\u50a8I/O\u64cd\u4f5c\uff0c\u8fd9\u9650\u5236\u4e86\u9ad8\u6027\u80fd\u5b58\u50a8\u8bbe\u5907\u7684\u5e26\u5bbd\u5229\u7528\u7387\u3002", "method": "\u63d0\u51faAGNES\u6846\u67b6\uff0c\u91c7\u7528\u5757\u72b6\u5b58\u50a8I/O\u5904\u7406\u65b9\u6cd5\u6765\u5145\u5206\u5229\u7528\u9ad8\u6027\u80fd\u5b58\u50a8\u8bbe\u5907\u7684I/O\u5e26\u5bbd\uff0c\u5e76\u7ed3\u5408\u8d85\u6279\u6b21\u5904\u7406\u7b56\u7565\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u56fe\u7684\u7279\u6027\u8fdb\u4e00\u6b65\u4f18\u5316\u6bcf\u4e2a\u5b58\u50a8I/O\u7684\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAGNES\u59cb\u7ec8\u4f18\u4e8e\u56db\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6bd4\u6700\u4f73\u7ade\u4e89\u5bf9\u624b\u5feb\u8fbe4.1\u500d\u3002", "conclusion": "AGNES\u901a\u8fc7\u521b\u65b0\u7684\u5b58\u50a8I/O\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21GNN\u8bad\u7ec3\u4e2d\u7684\u5b58\u50a8\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5904\u7406web\u89c4\u6a21\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5355\u673a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01475", "abs": "https://arxiv.org/abs/2601.01475", "authors": ["Ruofeng Yang", "Yongcan Li", "Bo Jiang", "Cheng Chen", "Shuai Li"], "title": "Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts", "comment": null, "summary": "Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \\times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\\sqrt{\u03a3_{k=1}^Kn_k}\\sqrt{\u03a3_{k=1}^Kn_kd_k}/\\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoLR-MoG\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u5efa\u6a21\u4e3a\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u7684\u5b50\u7a7a\u95f4\u8054\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u6f5c\u5728\u6d41\u5f62\u591a\u6a21\u6001\u7279\u6027\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ef4\u5ea6\u8bc5\u5492\u7684\u7a81\u7834\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4f30\u8ba1\u8bef\u5dee\u53d7\u7ef4\u5ea6\u8bc5\u5492\u5f71\u54cd\uff08n^{-1/D}\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u6570\u636e\u5efa\u6a21\u4e3a\u9ad8\u65af\u6f5c\u5728\u53d8\u91cf\u7684\u7ebf\u6027\u5b50\u7a7a\u95f4\u8054\u5408\uff0c\u867d\u7136\u53cd\u6620\u4e86\u591a\u6d41\u5f62\u7279\u6027\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u6f5c\u5728\u6d41\u5f62\u7684\u591a\u6a21\u6001\u7279\u6027\u3002", "method": "\u63d0\u51faMoLR-MoG\uff08\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u7684\u5b50\u7a7a\u95f4\u6df7\u5408\uff09\u5efa\u6a21\u65b9\u6cd5\uff1a\u5c06\u76ee\u6807\u6570\u636e\u5efa\u6a21\u4e3aK\u4e2a\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u8054\u5408\uff0c\u6bcf\u4e2a\u5b50\u7a7a\u95f4\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6f5c\u5728\u53d8\u91cf\uff08n_k\u4e2a\u6a21\u6001\uff0c\u7ef4\u5ea6d_k\uff09\u3002\u5bf9\u5e94\u7684\u5f97\u5206\u51fd\u6570\u5177\u6709\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\uff0c\u80fd\u6355\u6349\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u5305\u542b\u975e\u7ebf\u6027\u7279\u6027\u3002", "result": "1. \u5b9e\u9a8c\u8868\u660eMoE-latent MoG NN\u7684\u751f\u6210\u7ed3\u679c\u8fdc\u4f18\u4e8eMoE-latent Gaussian score\uff1b2. MoE-latent MoG NN\u4e0e\u53c2\u6570\u591a10\u500d\u7684MoE-latent Unet\u6027\u80fd\u76f8\u5f53\uff1b3. \u7406\u8bba\u5206\u6790\u5f97\u5230R^4\u221a(\u2211n_k)\u221a(\u2211n_kd_k)/\u221an\u7684\u4f30\u8ba1\u8bef\u5dee\uff0c\u7a81\u7834\u4e86\u7ef4\u5ea6\u8bc5\u5492\uff1b4. \u8bc1\u660e\u4e86MoLR-MoG\u5efa\u6a21\u4e0b\u7684\u4f18\u5316\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "MoLR-MoG\u5efa\u6a21\u65b9\u6cd5\u5408\u7406\u4e14\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u6269\u6563\u6a21\u578b\u53ea\u9700\u5c0f\u8bad\u7ec3\u6837\u672c\u548c\u5feb\u901f\u4f18\u5316\u8fc7\u7a0b\u5c31\u80fd\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6570\u636e\u7ed3\u6784\u7a81\u7834\u4e86\u7ef4\u5ea6\u8bc5\u5492\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.01484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01484", "abs": "https://arxiv.org/abs/2601.01484", "authors": ["Itai Morad", "Nir Shlezinger", "Yonina C. Eldar"], "title": "SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines", "comment": null, "summary": "Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.", "AI": {"tldr": "\u672c\u6587\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u6790\u77e5\u8bc6\u84b8\u998f\uff0c\u8bc1\u660e\u4ece\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u5b66\u4e60\u80fd\u964d\u4f4e\u65b9\u5dee\u3001\u63d0\u5347\u6536\u655b\u7a33\u5b9a\u6027\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u6559\u5e08\u6a21\u578b\u80fd\u8ba9\u5b66\u751f\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u66f4\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u867d\u7136\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u4e25\u683c\u5206\u6790KD\u7684\u6536\u655b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5f53\u6559\u5e08\u63d0\u4f9b\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\uff08BCPs\uff09\u65f6\uff0c\u4e0e\u4f20\u7edf\u7684one-hot\u76d1\u7763\u76f8\u6bd4\u6709\u4f55\u7406\u8bba\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u6790\u77e5\u8bc6\u84b8\u998f\uff0c\u7814\u7a76\u4e24\u79cd\u76d1\u7763\u65b9\u5f0f\uff1a1\uff09\u6559\u5e08\u63d0\u4f9b\u7cbe\u786e\u7684\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\uff08BCPs\uff09\uff1b2\uff09\u6559\u5e08\u63d0\u4f9b\u5e26\u566a\u58f0\u7684BCPs\u8fd1\u4f3c\u3002\u7406\u8bba\u5206\u6790\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7684\u6536\u655b\u884c\u4e3a\u5206\u6790\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u7684\u6548\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u4eceBCPs\u5b66\u4e60\u80fd\u5b9e\u73b0\u65b9\u5dee\u964d\u4f4e\uff0c\u5e76\u5728\u6536\u655b\u8fb9\u754c\u4e2d\u53bb\u9664\u90bb\u57df\u9879\u3002\u566a\u58f0\u6c34\u5e73\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u4ece\u8d1d\u53f6\u65af\u6559\u5e08\u84b8\u998f\u7684\u5b66\u751f\u4e0d\u4ec5\u51c6\u786e\u7387\u66f4\u9ad8\uff08\u6700\u9ad8\u63d0\u53474.27%\uff09\uff0c\u800c\u4e14\u6536\u655b\u66f4\u7a33\u5b9a\uff08\u566a\u58f0\u51cf\u5c11\u8fbe30%\uff09\u3002", "conclusion": "\u8d1d\u53f6\u65af\u89c6\u89d2\u4e3a\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u8bc1\u660e\u8d1d\u53f6\u65af\u5206\u7c7b\u6982\u7387\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u80fd\u5e26\u6765\u65b9\u5dee\u964d\u4f4e\u548c\u6536\u655b\u7a33\u5b9a\u6027\u63d0\u5347\u3002\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u80fd\u63d0\u4f9b\u66f4\u597d\u7684BCPs\u4f30\u8ba1\uff0c\u4ece\u800c\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002"}}
{"id": "2601.01493", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01493", "abs": "https://arxiv.org/abs/2601.01493", "authors": ["Yijie Zhou", "Shi Pu"], "title": "Accelerating Decentralized Optimization via Overlapping Local Steps", "comment": null, "summary": "Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.", "AI": {"tldr": "\u63d0\u51faOLDSGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u52a0\u901f\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\uff0c\u51cf\u5c11\u7f51\u7edc\u7a7a\u95f2\u65f6\u95f4\uff0c\u4fdd\u6301\u4e0eLocal SGD\u76f8\u540c\u7684\u5e73\u5747\u66f4\u65b0\u4f46\u907f\u514d\u901a\u4fe1\u963b\u585e", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u74f6\u9888\uff0c\u9891\u7e41\u7684\u8282\u70b9\u95f4\u540c\u6b65\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u51cf\u5c11\u7f51\u7edc\u7a7a\u95f2\u65f6\u95f4\u7684\u52a0\u901f\u65b9\u6cd5", "method": "\u8bbe\u8ba1\u91cd\u53e0\u5c40\u90e8\u53bb\u4e2d\u5fc3\u5316SGD\uff08OLDSGD\uff09\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u66f4\u65b0\u673a\u5236\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684\u91cd\u53e0\uff0c\u4fdd\u6301\u4e0eLocal SGD\u76f8\u540c\u7684\u5e73\u5747\u66f4\u65b0\u4f46\u907f\u514d\u901a\u4fe1\u5f15\u8d77\u7684\u505c\u987f", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u5149\u6ed1\u975e\u51f8\u76ee\u6807\u7684\u975e\u6e10\u8fd1\u6536\u655b\u7387\uff0cOLDSGD\u4fdd\u6301\u4e0e\u6807\u51c6Local Decentralized SGD\u76f8\u540c\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u4f46\u6539\u5584\u6bcf\u6b21\u8fed\u4ee3\u8fd0\u884c\u65f6\u95f4\uff1b\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5728\u4e0d\u540c\u901a\u4fe1\u5ef6\u8fdf\u4e0b\u90fd\u80fd\u6539\u5584\u5b9e\u9645\u65f6\u95f4\u6536\u655b", "conclusion": "OLDSGD\u901a\u8fc7\u5bf9\u73b0\u6709\u6846\u67b6\u7684\u6700\u5c0f\u4fee\u6539\uff0c\u63d0\u4f9b\u4e86\u4e0d\u727a\u7272\u7406\u8bba\u4fdd\u8bc1\u7684\u66f4\u5feb\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01501", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01501", "abs": "https://arxiv.org/abs/2601.01501", "authors": ["Fan Xu", "Wei Gong", "Hao Wu", "Lilan Peng", "Nan Wang", "Qingsong Wen", "Xian Wu", "Kun Wang", "Xibin Zhao"], "title": "Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE", "comment": null, "summary": "Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.", "AI": {"tldr": "HiGO\uff1a\u4e00\u79cd\u7528\u4e8e\u5168\u7403\u91ce\u706b\u9884\u6d4b\u7684\u591a\u5c3a\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edcODE\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u56fe\u7ed3\u6784\u548c\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u6027\u80fd", "motivation": "\u91ce\u706b\u4f5c\u4e3a\u5730\u7403\u7cfb\u7edf\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u53d7\u5230\u5927\u6c14\u3001\u6d77\u6d0b\u548c\u9646\u5730\u8fc7\u7a0b\u5728\u5e7f\u6cdb\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u5168\u7403\u5929\u6c14\u9884\u62a5\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u5c42\u6b21\u5316\u56feODE\uff08HiGO\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u5730\u7403\u7cfb\u7edf\u8868\u793a\u4e3a\u591a\u5c42\u56fe\u5c42\u6b21\u7ed3\u6784\uff1b2\uff09\u8bbe\u8ba1\u81ea\u9002\u5e94\u6ee4\u6ce2\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u7528\u4e8e\u5c42\u5185\u548c\u5c42\u95f4\u4fe1\u606f\u6d41\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\uff1b3\uff09\u5728\u591a\u4e2a\u5c42\u6b21\u4e0a\u96c6\u6210GNN\u53c2\u6570\u5316\u7684\u795e\u7ecfODE\u6a21\u5757\uff0c\u663e\u5f0f\u5b66\u4e60\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8fde\u7eed\u52a8\u6001\u3002", "result": "\u5728SeasFire Cube\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHiGO\u5728\u957f\u671f\u91ce\u706b\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5176\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u89c2\u6d4b\u4e00\u81f4\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "HiGO\u6846\u67b6\u6210\u529f\u5730\u5c06\u591a\u5c3a\u5ea6\u5efa\u6a21\u548c\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4e3a\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01558", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01558", "abs": "https://arxiv.org/abs/2601.01558", "authors": ["Pengfei Qu", "Wenyu Ouyang", "Chi Zhang", "Yikai Chai", "Shuolong Xu", "Lei Ye", "Yongri Piao", "Miao Zhang", "Huchuan Lu"], "title": "Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings", "comment": "12 pages, 11 figures", "summary": "Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.", "AI": {"tldr": "\u536b\u661f\u56fe\u50cf\u5d4c\u5165\u6bd4\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u66f4\u80fd\u6709\u6548\u9884\u6d4b\u65e0\u89c2\u6d4b\u8bb0\u5f55\u6cb3\u6d41\u7684\u6d41\u91cf\uff0c\u901a\u8fc7\u9009\u62e9\u76f8\u4f3c\u6d41\u57df\u53ef\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u65e0\u6cd5\u5b8c\u5168\u63cf\u8ff0\u81ea\u7136\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u65e0\u6d41\u91cf\u8bb0\u5f55\u6cb3\u6d41\u7684\u5f84\u6d41", "method": "\u4f7f\u7528AlphaEarth Foundation\u5d4c\u5165\uff08\u4ece\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u8868\u5f81\uff09\u4ee3\u66ff\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u9009\u62e9\u76f8\u4f3c\u6d41\u57df\u4f5c\u4e3a\"\u4f9b\u4f53\u6d41\u57df\"\u6765\u6539\u8fdb\u9884\u6d4b", "result": "\u57fa\u4e8e\u536b\u661f\u5d4c\u5165\u7684\u6a21\u578b\u5728\u672a\u53c2\u4e0e\u8bad\u7ec3\u7684\u6d41\u57df\u4e0a\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u9009\u62e9\u76f8\u4f3c\u6d41\u57df\u53ef\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u6dfb\u52a0\u4e0d\u76f8\u4f3c\u6d41\u57df\u4f1a\u964d\u4f4e\u7cbe\u5ea6", "conclusion": "\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u8868\u5f81\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u6d41\u57df\u7269\u7406\u5dee\u5f02\uff0c\u589e\u5f3a\u6c34\u6587\u9884\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u5f00\u53d1\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u6c34\u6587\u6a21\u578b"}}
{"id": "2601.01580", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01580", "abs": "https://arxiv.org/abs/2601.01580", "authors": ["Zibo Zhao", "Yuanting Zha", "Haipeng Zhang", "Xingcheng Xu"], "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs", "comment": null, "summary": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($\u03c0_{sample}$) for generation and decision ($\u03c0_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $\u03c0_{sample}$ while leaving $\u03c0_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($\u03c0_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u548c\u4e24\u9636\u6bb5\u51b3\u7b56-\u91c7\u6837\u5047\u8bbe\uff0c\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86RL\u540e\u8bad\u7ec3\u4e3a\u4f55\u80fd\u4ea7\u751f\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86RL\u7684\u6cdb\u5316\u4f18\u52bf\u4e3b\u8981\u6765\u81ea\u51b3\u7b56\u80fd\u529b\u7684\u63d0\u5347\u800c\u975e\u91c7\u6837\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728RL\u540e\u8bad\u7ec3\u540e\u51fa\u73b0\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4f46\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\u5982\u4f55\u4ea7\u751f\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u4f55\u65f6\u4fee\u8ba2\u8fd9\u4e24\u79cd\u4e0d\u540c\u529f\u80fd\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u4ece\u673a\u5236\u4e0a\u89e3\u91caRL\u4e3a\u4f55\u6bd4SFT\u66f4\u6210\u529f\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u6765\u8868\u5f81\u5956\u52b1\u68af\u5ea6\u5728\u7b56\u7565\u7ec4\u4ef6\u4e2d\u7684\u5206\u5e03\uff0c\u5f62\u5f0f\u5316\u4e3a\u4e24\u9636\u6bb5\u51b3\u7b56-\u91c7\u6837\u5047\u8bbe\uff0c\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u7528\u4e8e\u751f\u6210\u7684\u91c7\u6837\u7b56\u7565\u548c\u7528\u4e8e\u9a8c\u8bc1\u7684\u51b3\u7b56\u7b56\u7565\u3002\u7406\u8bba\u5206\u6790\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u7684\u68af\u5ea6\u5f52\u56e0\u7279\u6027\uff0c\u5e76\u5728\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u4ee3\u7406\u5956\u52b1\u8868\u73b0\u51fa\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u800cSFT\u548cKL\u60e9\u7f5a\u8868\u73b0\u51fa\u4e0d\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u957f\u5ea6\u52a0\u6743\u521b\u5efa\u4e86\u4e0d\u5bf9\u79f0\u6b63\u5219\u5316\uff0c\u7ea6\u675f\u91c7\u6837\u7b56\u7565\u800c\u8ba9\u51b3\u7b56\u7b56\u7565\u4f18\u5316\u4e0d\u8db3\u3002\u5b9e\u8bc1\u9a8c\u8bc1\uff1aRL\u7684\u6cdb\u5316\u4f18\u52bf\u4e3b\u8981\u6765\u81ea\u51b3\u7b56\u80fd\u529b\u7684\u63d0\u5347\u800c\u975e\u91c7\u6837\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u673a\u5236\u4e0a\u89e3\u91ca\u4e86\u601d\u7ef4\u6a21\u578b\u4e2d\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u4e3aRL\u5728\u4ea7\u751f\u81ea\u6211\u53cd\u601d\u80fd\u529b\u65b9\u9762\u7684\u6210\u529f\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u51b3\u7b56\u80fd\u529b\u5728\u6cdb\u5316\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2601.01605", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01605", "abs": "https://arxiv.org/abs/2601.01605", "authors": ["Xin Di", "Xinglin Piao", "Fei Wang", "Guodong Jing", "Yong Zhang"], "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training", "comment": null, "summary": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.", "AI": {"tldr": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u63d0\u5347\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u9ad8\u8d28\u91cf\u672c\u5730\u8bad\u7ec3\u6570\u636e\u548c\u9759\u6001\u53c2\u6570\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u65b9\u6cd5\u5728\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u9ad8\u8d28\u91cf\u672c\u5730\u8bad\u7ec3\u6570\u636e\u548c\u9759\u6001\u6a21\u578b\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u533a\u57df\u548c\u6781\u7aef\u4e8b\u4ef6\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\uff0c\u6838\u5fc3\u662f\u8bbe\u8ba1\u7684\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5757\uff0c\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u6807\u51c6\u7ebf\u6027\u6295\u5f71\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u975e\u5e73\u7a33\u6c14\u8c61\u5206\u5e03\uff0c\u589e\u5f3a\u964d\u6c34\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u8de8\u533a\u57df\u6781\u7aef\u964d\u6c34\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cREE-TTT\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "REE-TTT\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01616", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01616", "abs": "https://arxiv.org/abs/2601.01616", "authors": ["Md Istiauk Hossain Rifat", "Moin Khan", "Mohammad Zunaed"], "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry", "comment": "9 pages, 9 figures", "summary": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u884c\u4e1a\u5f00\u53d1\u4e86\u57fa\u4e8e\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b(NILM)\u7684\u5b9e\u65f6\u80fd\u8017\u76d1\u63a7\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u7eba\u7ec7\u5207\u5272\u673a\u7b49\u76f8\u540c\u7535\u673a\u8d1f\u8f7d\uff0c\u901a\u8fc7\u786c\u4ef6\u91c7\u96c6\u548c\u4e91\u7aef\u5904\u7406\u5b9e\u73b0\u8fdc\u7a0b\u76d1\u63a7\uff0c\u4f46\u76f8\u540c\u8bbe\u5907\u540c\u65f6\u8fd0\u884c\u65f6\u8d1f\u8f7d\u5206\u89e3\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u884c\u4e1a\u4f5c\u4e3a\u9ad8\u80fd\u8017\u4ea7\u4e1a\uff0c\u5176\u80fd\u8017\u76d1\u63a7\u65b9\u6cd5\u843d\u540e\uff0c\u5bfc\u81f4\u80fd\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u8fd0\u8425\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u5f00\u53d1\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u7684\u5b9e\u65f6\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u7535\u538b\u7535\u6d41\u4f20\u611f\u5668\u3001Arduino Mega\u548cESP8266\u7684\u786c\u4ef6\u7cfb\u7edf\uff0c\u91c7\u96c6\u603b\u8d1f\u8f7d\u548c\u5355\u4e2a\u8d1f\u8f7d\u6570\u636e\u5e76\u4e0a\u4f20\u81f3\u4e91\u7aef\u5904\u7406\u3002\u521b\u5efa\u4e86\u5305\u542b\u4e09\u4e2a\u76f8\u540c\u611f\u5e94\u7535\u673a\u548c\u8f85\u52a9\u8d1f\u8f7d\u7684\u65b0\u6570\u636e\u96c6\uff08\u8d85\u8fc718\u4e07\u4e2a\u6837\u672c\uff09\uff0c\u4f7f\u7528\u5148\u8fdb\u7684MATNILM\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5de5\u4e1a\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u603b\u4f53\u80fd\u8017\u4f30\u8ba1\u76f8\u5bf9\u51c6\u786e\uff0c\u4f46\u5355\u4e2a\u8bbe\u5907\u7684\u8d1f\u8f7d\u5206\u89e3\u9762\u4e34\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u591a\u4e2a\u76f8\u540c\u673a\u5668\u540c\u65f6\u8fd0\u884c\u65f6\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u96c6\u6210\u7cfb\u7edf\u901a\u8fc7Blynk\u5e94\u7528\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u8fdc\u7a0b\u76d1\u63a7\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86NILM\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5305\u62ec\u66f4\u9ad8\u9891\u7387\u7684\u6570\u636e\u91c7\u96c6\u3001\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u5904\u7406\u76f8\u540c\u8d1f\u8f7d\u7684\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.01653", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.01653", "abs": "https://arxiv.org/abs/2601.01653", "authors": ["Hao Xiang Li", "Yash Shah", "Lorenzo Giusti"], "title": "Learning Resilient Elections with Adversarial GNNs", "comment": null, "summary": "In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u6295\u7968\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9009\u4e3e\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\u6765\u63d0\u5347\u6295\u7968\u89c4\u5219\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6297\u7b56\u7565\u6027\u6295\u7968\u80fd\u529b\u3002", "motivation": "\u9009\u4e3e\u5728\u73b0\u4ee3\u6c11\u4e3b\u3001\u5e02\u573a\u8c03\u8282\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bbe\u8ba1\u4e00\u4e2a\u6ee1\u8db3\u6240\u6709\u573a\u666f\u7684\u901a\u7528\u6295\u7968\u89c4\u5219\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u96c6\u5408\u4e0d\u53d8\u67b6\u6784\u7684\u81ea\u52a8\u5316\u673a\u5236\u8bbe\u8ba1\u65b9\u6cd5\u867d\u7136\u9002\u5408\u5efa\u6a21\u9009\u4e3e\u7cfb\u7edf\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6297\u7b56\u7565\u6027\u6295\u7968\u7b49\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u5c06\u9009\u4e3e\u8868\u793a\u4e3a\u4e8c\u5206\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6295\u7968\u89c4\u5219\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u6765\u63d0\u5347\u6295\u7968\u89c4\u5219\u5728\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u540c\u65f6\u5bf9\u7b56\u7565\u6027\u6295\u7968\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u5148\u524d\u5de5\u4f5c\u5728\u5b66\u4e60\u6295\u7968\u89c4\u5219\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u63d0\u5347\u4e86\u6295\u7968\u89c4\u5219\u7684\u8868\u8fbe\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u9009\u4e3e\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u5b66\u4e60\u51fa\u66f4\u5177\u8868\u8fbe\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7684\u6295\u7968\u89c4\u5219\u3002"}}
{"id": "2601.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01663", "abs": "https://arxiv.org/abs/2601.01663", "authors": ["He Sun", "Jiwoong Shin", "Ravi Dhar"], "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths", "comment": null, "summary": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.", "AI": {"tldr": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837(LAS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u8f68\u8ff9\u957f\u5ea6\u5206\u7ec4\u91c7\u6837\u6765\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff0c\u6539\u5584\u8f68\u8ff9\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u5339\u914d\u6548\u679c", "motivation": "\u5728\u53ef\u53d8\u957f\u5ea6\u8f68\u8ff9\u751f\u6210\u5efa\u6a21\u4e2d\uff0c\u6807\u51c6\u5c0f\u6279\u91cf\u8bad\u7ec3\u5728\u8f68\u8ff9\u957f\u5ea6\u9ad8\u5ea6\u5f02\u8d28\u65f6\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u4f1a\u964d\u4f4e\u8f68\u8ff9\u6d3e\u751f\u7edf\u8ba1\u91cf\u7684\u5206\u5e03\u5339\u914d\u8d28\u91cf", "method": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837(LAS)\uff1a\u6309\u8f68\u8ff9\u957f\u5ea6\u5206\u7ec4\uff0c\u4ece\u5355\u4e00\u957f\u5ea6\u6876\u4e2d\u91c7\u6837\u6279\u6b21\uff0c\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff1b\u96c6\u6210\u5230\u5e26\u8f85\u52a9\u65f6\u95f4\u5bf9\u9f50\u635f\u5931\u7684\u8f68\u8ff9GAN\u4e2d", "result": "\u7406\u8bba\u4e0a\u63d0\u4f9b\u5206\u5e03\u7ea7\u4fdd\u8bc1\u548cIPM/Wasserstein\u673a\u5236\u89e3\u91ca\uff1b\u5b9e\u8bc1\u4e0a\u5728\u591a\u5546\u573a\u8d2d\u7269\u8f68\u8ff9\u548c\u591a\u79cd\u516c\u5171\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u6539\u5584\u6d3e\u751f\u53d8\u91cf\u5206\u5e03\u5339\u914d", "conclusion": "LAS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6279\u5904\u7406\u7b56\u7565\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u5c31\u80fd\u6539\u5584\u8f68\u8ff9\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u5339\u914d\u6027\u80fd\uff0c\u5728\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u91c7\u6837"}}
{"id": "2601.01664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01664", "abs": "https://arxiv.org/abs/2601.01664", "authors": ["Amichai Painsky"], "title": "Who is the Winning Algorithm? Rank Aggregation for Comparative Studies", "comment": null, "summary": "Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u4e0d\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff09\u6765\u4f30\u8ba1\u5404\u7b97\u6cd5\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6982\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u7edf\u8ba1\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6b21\u6570\u6765\u8bc4\u4f30\u6027\u80fd\uff0c\u4f46\u5ffd\u7565\u4e86\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u5982\u7b2c\u4e8c\u3001\u7b2c\u4e09\u540d\u7b49\uff09\uff0c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u5305\u542b\u66f4\u591a\u5173\u4e8e\u7b97\u6cd5\u76f8\u5bf9\u6027\u80fd\u7684\u6709\u4ef7\u503c\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u65b0\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u4e0d\u4ec5\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff09\uff0c\u901a\u8fc7\u66f4\u5168\u9762\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6982\u7387\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u5df2\u77e5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u5229\u7528\u7b97\u6cd5\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\u800c\u975e\u4ec5\u83b7\u80dc\u6b21\u6570\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u4e3a\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f9d\u636e\u3002"}}
{"id": "2601.01665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01665", "abs": "https://arxiv.org/abs/2601.01665", "authors": ["Wei Liu", "Yaoxin Wu", "Yingqian Zhang", "Thomas B\u00e4ck", "Yingjie Fan"], "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives", "comment": null, "summary": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u9c81\u68d2\u6027\u7684\u504f\u597d\u6761\u4ef6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u5668\u6846\u67b6\uff0c\u5305\u542b\u504f\u597d\u5bf9\u6297\u653b\u51fb\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff0c\u4ee5\u53ca\u901a\u8fc7\u786c\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u7684\u9632\u5fa1\u7b56\u7565\u589e\u5f3a\u6c42\u89e3\u5668\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u5728\u591a\u6837\u590d\u6742\u95ee\u9898\u5206\u5e03\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u589e\u5f3a\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u9c81\u68d2\u6027\u6846\u67b6\uff1a1) \u504f\u597d\u5bf9\u6297\u653b\u51fb\u751f\u6210\u66b4\u9732\u6c42\u89e3\u5668\u5f31\u70b9\u7684\u56f0\u96be\u5b9e\u4f8b\uff1b2) \u901a\u8fc7\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf\u9000\u5316\u91cf\u5316\u653b\u51fb\u5f71\u54cd\uff1b3) \u9632\u5fa1\u7b56\u7565\u6574\u5408\u786c\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u5230\u5bf9\u6297\u8bad\u7ec3\u4e2d\uff0c\u51cf\u5c11\u5bf9\u53d7\u9650\u504f\u597d\u533a\u57df\u7684\u8fc7\u62df\u5408\u3002", "result": "\u5728\u591a\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\u3001\u591a\u76ee\u6807\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u548c\u591a\u76ee\u6807\u80cc\u5305\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff1a\u653b\u51fb\u65b9\u6cd5\u6210\u529f\u4e3a\u4e0d\u540c\u6c42\u89e3\u5668\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff1b\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u795e\u7ecf\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u56f0\u96be\u6216\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u9c81\u68d2\u6027\u6846\u67b6\u6709\u6548\u8bc4\u4f30\u548c\u589e\u5f3a\u504f\u597d\u6761\u4ef6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u5668\uff0c\u653b\u51fb\u65b9\u6cd5\u80fd\u66b4\u9732\u5f31\u70b9\uff0c\u9632\u5fa1\u7b56\u7565\u80fd\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u5b66\u4e60\u578b\u6c42\u89e3\u5668\u63d0\u4f9b\u9c81\u68d2\u6027\u4fdd\u969c\u3002"}}
{"id": "2601.01678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01678", "abs": "https://arxiv.org/abs/2601.01678", "authors": ["Siba Smarak Panigrahi", "Jovana Videnovi\u0107", "Maria Brbi\u0107"], "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist", "comment": "33 pages, 5 figures, 7 tables. Code available at https://github.com/mlbio-epfl/HeurekaBench", "summary": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.", "AI": {"tldr": "HeurekaBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u548c\u4ee3\u7801\u5e93\u4e2d\u521b\u5efa\u5f00\u653e\u5f0f\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u79d1\u5b66\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u73b0\u5b9e\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u573a\u666f\uff0c\u9700\u8981\u80fd\u591f\u6574\u5408\u6570\u636e\u5206\u6790\u3001\u89e3\u91ca\u548c\u4ece\u5b9e\u9a8c\u6570\u636e\u751f\u6210\u65b0\u89c1\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u534a\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u591a\u4e2aLLM\u4ece\u79d1\u5b66\u7814\u7a76\u548c\u4ee3\u7801\u5e93\u4e2d\u63d0\u53d6\u89c1\u89e3\u5e76\u751f\u6210\u5019\u9009\u5de5\u4f5c\u6d41\uff0c\u7136\u540e\u4e0e\u62a5\u544a\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\uff0c\u521b\u5efa\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u7684\u5f00\u653e\u5f0f\u7814\u7a76\u95ee\u9898\u57fa\u51c6\u3002", "result": "\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u57fa\u51c6\uff0c\u53d1\u73b0\u6279\u8bc4\u6a21\u5757\u53ef\u5c06\u5f00\u6e90LLM\u667a\u80fd\u4f53\u7684\u4e0d\u826f\u54cd\u5e94\u6539\u5584\u8fbe22%\uff0c\u7f29\u5c0f\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "HeurekaBench\u4e3a\u79d1\u5b66\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u8def\u5f84\uff0c\u5c06\u57fa\u51c6\u6784\u5efa\u624e\u6839\u4e8e\u771f\u5b9e\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2601.01688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01688", "abs": "https://arxiv.org/abs/2601.01688", "authors": ["Yash Thesia", "Meera Suthar"], "title": "DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors", "comment": "8 pages, 3 figures, 4 tables", "summary": "Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the \"Cold Start\" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique \"optimization trajectory\" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.", "AI": {"tldr": "DiMEx\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u65e0\u6570\u636e\u6a21\u578b\u7a83\u53d6\uff0cHSE\u9632\u5fa1\u901a\u8fc7\u68c0\u6d4b\u4f18\u5316\u8f68\u8ff9\u6765\u5bf9\u6297\u6b64\u7c7b\u653b\u51fb", "motivation": "\u73b0\u6709\u65e0\u6570\u636e\u6a21\u578b\u63d0\u53d6(DFME)\u65b9\u6cd5\u5b58\u5728\"\u51b7\u542f\u52a8\"\u95ee\u9898\uff0cGAN\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u67e5\u8be2\u4ece\u968f\u673a\u566a\u58f0\u6536\u655b\u5230\u6709\u610f\u4e49\u6570\u636e\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u7a83\u53d6\u65b9\u6cd5\uff0c\u540c\u65f6\u4e5f\u9700\u8981\u76f8\u5e94\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "DiMEx\uff1a\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u5728\u751f\u6210\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u968f\u673a\u5d4c\u5165\u8d1d\u53f6\u65af\u4f18\u5316(REMBO)\uff0c\u7acb\u5373\u5408\u6210\u9ad8\u8d28\u91cf\u67e5\u8be2\u3002HSE\u9632\u5fa1\uff1a\u6df7\u5408\u72b6\u6001\u96c6\u6210\u9632\u5fa1\uff0c\u901a\u8fc7\u68c0\u6d4b\u6f5c\u5728\u7a7a\u95f4\u653b\u51fb\u7684\u72ec\u7279\"\u4f18\u5316\u8f68\u8ff9\"\u6765\u8bc6\u522b\u653b\u51fb\u3002", "result": "DiMEx\u5728SVHN\u6570\u636e\u96c6\u4e0a\u4ec5\u75282000\u6b21\u67e5\u8be2\u5c31\u8fbe\u523052.1%\u7684\u534f\u8bae\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u7684GAN\u57fa\u7ebf\u9ad8\u51fa16%\u4ee5\u4e0a\u3002HSE\u9632\u5fa1\u80fd\u5c06\u653b\u51fb\u6210\u529f\u7387\u6291\u5236\u523021.6%\uff0c\u4e14\u5ef6\u8fdf\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "DiMEx\u5c55\u793a\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u7a83\u53d6\u7684\u53ef\u884c\u6027\uff0c\u800cHSE\u9632\u5fa1\u5219\u63d0\u4f9b\u4e86\u5bf9\u6297\u6b64\u7c7b\u8bed\u4e49\u653b\u51fb\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u68c0\u6d4b\u653b\u51fb\u4f18\u5316\u8f68\u8ff9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.01692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01692", "abs": "https://arxiv.org/abs/2601.01692", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "title": "Enhanced Multi-model Online Conformal Prediction", "comment": null, "summary": "Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e8c\u5206\u56fe\u9009\u62e9\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u9884\u6d4b\u6548\u7387", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u4f9d\u8d56\u5355\u4e00\u56fa\u5b9a\u6a21\u578b\uff0c\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u7a33\u5b9a\uff1b\u73b0\u6709\u591a\u6a21\u578b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u53d7\u6027\u80fd\u5dee\u7684\u6a21\u578b\u5f71\u54cd", "method": "\u5f00\u53d1\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u4e8c\u5206\u56fe\u8bc6\u522b\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u4ece\u4e2d\u9009\u62e9\u6a21\u578b\u6784\u5efa\u9884\u6d4b\u96c6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u578b\u5171\u5f62\u9884\u6d4b\u6280\u672f", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u578b\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u8ba1\u7b97\u548c\u6027\u80fd\u95ee\u9898"}}
{"id": "2601.01701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01701", "abs": "https://arxiv.org/abs/2601.01701", "authors": ["Mohammed Ayalew Belay", "Adil Rasheed", "Pierluigi Salvo Rossi"], "title": "Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT", "comment": null, "summary": "Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u96c6\u6210\u8054\u90a6\u5b66\u4e60(DTFL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u79cd\u65b0\u9896\u65b9\u6cd5\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u9690\u79c1\u3001\u901a\u4fe1\u6548\u7387\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u4f9d\u8d56\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u3001\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u9ad8\u8bef\u62a5\u7387\u548c\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\uff0c\u9700\u8981\u65e2\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u53c8\u80fd\u9ad8\u6548\u901a\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e94\u79cd\u6570\u5b57\u5b6a\u751f\u96c6\u6210\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff1aDTML\uff08\u5143\u5b66\u4e60\uff09\u3001FPF\uff08\u53c2\u6570\u878d\u5408\uff09\u3001LPE\uff08\u5206\u5c42\u53c2\u6570\u4ea4\u6362\uff09\u3001CWA\uff08\u5faa\u73af\u6743\u91cd\u9002\u5e94\uff09\u548cDTKD\uff08\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u7ed3\u5408\u5408\u6210\u4e0e\u771f\u5b9e\u77e5\u8bc6\uff0c\u5e73\u8861\u6cdb\u5316\u4e0e\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u572880%\u76ee\u6807\u51c6\u786e\u7387\u4e0b\uff0cCWA\u4ec5\u970033\u8f6e\uff0cFPF\u970041\u8f6e\uff0cLPE\u970048\u8f6e\uff0cDTML\u970087\u8f6e\uff0c\u800c\u6807\u51c6FedAvg\u548cDTKD\u5728100\u8f6e\u5185\u672a\u8fbe\u6807\u3002CWA\u6bd4DTML\u51cf\u5c1162%\u8f6e\u6b21\uff0c\u6bd4LPE\u51cf\u5c1131%\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u77e5\u8bc6\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\u80fd\u663e\u8457\u52a0\u901f\u6536\u655b\uff0c\u63d0\u5347\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u5de5\u4e1a\u7269\u8054\u7f51\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01714", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01714", "abs": "https://arxiv.org/abs/2601.01714", "authors": ["Kareem Ahmed", "Sameer Singh"], "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning", "comment": null, "summary": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.", "AI": {"tldr": "EPIC\u662f\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u7eb3\u5165\u8bed\u8a00\u6a21\u578b\u89e3\u7801\uff0c\u5728\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\u663e\u5f0f\u8c03\u8282\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u91c7\u6837\u5206\u5e03\u7684\u71b5\u4e0e\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u7b97\u6cd5\u4f9d\u8d56\u8d2a\u5a6a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u751f\u6210\u6587\u672c\u540c\u8d28\u5316\u3001\u91cd\u590d\u4e14\u4e0d\u8fde\u8d2f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9650\u5236\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u5230\u9ad8\u6982\u7387\u5ef6\u7eed\u65f6\u5f15\u5165\u4e86\u77ed\u89c6\u626d\u66f2\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u7801\u7b56\u7565\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "EPIC\u901a\u8fc7\u71b5\u611f\u77e5\u61d2\u60f0Gumbel-Max\u91c7\u6837\uff0c\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u7eb3\u5165\u89e3\u7801\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u663e\u5f0f\u8c03\u8282\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\u8868\u8fbe\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u91c7\u6837\u5206\u5e03\u7684\u71b5\u4e0e\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\u3002\u7b97\u6cd5\u662f\u7cbe\u786e\u7684\u4e14\u9ad8\u6548\uff0c\u6bcf\u6b65\u4ec5\u9700\u4e9a\u7ebf\u6027\u6b21\u6570\u7684\u71b5\u8bc4\u4f30\u3002", "result": "\u5728\u521b\u610f\u5199\u4f5c\u548c\u6458\u8981\u4efb\u52a1\u4e2d\uff0cEPIC\u5728LM-as-judge\u504f\u597d\u80dc\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u89e3\u7801\u7b56\u7565\u3002\u81ea\u52a8\u6307\u6807\u663e\u793aEPIC\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u751f\u6210\u548c\u66f4\u5fe0\u5b9e\u7684\u6458\u8981\u3002\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cEPIC\u4e5f\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EPIC\u901a\u8fc7\u5c06\u71b5\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u89e3\u7801\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u5fe0\u5b9e\u5ea6\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d85\u53c2\u6570\u81ea\u7531\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01754", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.01754", "abs": "https://arxiv.org/abs/2601.01754", "authors": ["Selim Jerad", "Anej Svete", "Sophie Hao", "Ryan Cotterell", "William Merrill"], "title": "Context-Free Recognition with Transformers", "comment": null, "summary": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.", "AI": {"tldr": "\u5faa\u73afTransformer\u901a\u8fc7O(log n)\u5faa\u73af\u5c42\u548cO(n\u2076)\u586b\u5145token\u53ef\u4ee5\u8bc6\u522b\u6240\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ef\u80fd\u4e0d\u5b9e\u7528\uff1b\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\uff0c\u4ec5\u9700O(n\u00b3)\u586b\u5145token\uff0c\u66f4\u9ad8\u6548\u3002", "motivation": "Transformer\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7b49\u8bed\u6cd5\u7ed3\u6784\u826f\u597d\u7684\u8f93\u5165\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7406\u8bba\u4e0a\u6807\u51c6Transformer\u65e0\u6cd5\u8bc6\u522b\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff08CFL\uff09\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8bc1\u660e\u5faa\u73af\u5c42\u80fd\u5e2e\u52a9\u8bc6\u522b\u6b63\u5219\u8bed\u8a00\uff0c\u4f46CFL\u8bc6\u522b\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5faa\u73afTransformer\u67b6\u6784\uff0c\u4f7f\u7528O(log n)\u5faa\u73af\u5c42\u548cO(n\u2076)\u586b\u5145token\u6765\u5b9e\u73b0CFL\u8bc6\u522b\u3002\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\u5b50\u7c7b\uff0c\u5c06\u586b\u5145token\u9700\u6c42\u964d\u4f4e\u5230O(n\u00b3)\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5faa\u73af\u673a\u5236\u5728\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u5faa\u73afTransformer\u80fd\u8bc6\u522b\u6240\u6709CFL\uff0c\u4f46\u9700\u8981\u5927\u91cf\u586b\u5145token\u53ef\u80fd\u4e0d\u5b9e\u7528\u3002\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\uff0c\u8bc6\u522b\u95ee\u9898\u53d8\u5f97\u53ef\u5904\u7406\uff0c\u4ec5\u9700O(n\u00b3)\u586b\u5145\u3002\u5b9e\u9a8c\u8868\u660e\u5faa\u73af\u673a\u5236\u786e\u5b9e\u6709\u52a9\u4e8e\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u8bc6\u522b\u3002", "conclusion": "CFL\u8bc6\u522b\u5bf9Transformer\u5177\u6709\u590d\u6742\u6027\uff1a\u901a\u7528\u8bc6\u522b\u53ef\u80fd\u9700\u8981\u5927\u91cf\u586b\u5145token\u800c\u4e0d\u5b9e\u7528\uff0c\u4f46\u81ea\u7136\u7ea6\u675f\uff08\u5982\u65e0\u6b67\u4e49\u6027\uff09\u80fd\u4ea7\u751f\u9ad8\u6548\u7684\u8bc6\u522b\u7b97\u6cd5\u3002\u5faa\u73af\u673a\u5236\u4e3aTransformer\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2601.01786", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.01786", "abs": "https://arxiv.org/abs/2601.01786", "authors": ["Intae Jeon", "Yujeong Kwon", "Hyungjoon Koo"], "title": "UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk", "comment": "11 pages, 7 Tables, 6 Figures To appear in the Software Engineering in Practice (SEIP) track of ICSE", "summary": "The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.", "AI": {"tldr": "UnPII\uff1a\u9996\u4e2a\u57fa\u4e8ePII\u98ce\u9669\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7PII\u98ce\u9669\u6307\u6570\uff08PRI\uff09\u91cf\u5316\u4e0d\u540c\u654f\u611f\u5c5e\u6027\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5b9e\u73b0\u5dee\u5f02\u5316\u9057\u5fd8\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u7387\u3001\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u653f\u5e9c\u7b49\u5173\u952e\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\u3002GDPR\u7b49\u6cd5\u89c4\u8981\u6c42\u5e94\u8bf7\u6c42\u5220\u9664PII\uff0c\u9700\u8981\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6570\u636e\u5220\u9664\u65b9\u6848\u3002\u73b0\u6709\u9057\u5fd8\u6280\u672f\u91c7\u7528\u7edf\u4e00\u7b56\u7565\uff0c\u672a\u8003\u8651\u4e0d\u540cPII\u5c5e\u6027\u7684\u9690\u79c1\u98ce\u9669\u548c\u4e1a\u52a1\u98ce\u9669\u5dee\u5f02\u3002", "method": "\u63d0\u51faUnPII\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165PII\u98ce\u9669\u6307\u6570\uff08PRI\uff09\uff0c\u7efc\u5408\u8003\u8651\u53ef\u8bc6\u522b\u6027\u3001\u654f\u611f\u6027\u3001\u53ef\u7528\u6027\u3001\u53ef\u94fe\u63a5\u6027\u3001\u6301\u4e45\u6027\u3001\u53ef\u66b4\u9732\u6027\u548c\u5408\u89c4\u6027\u4e03\u4e2a\u98ce\u9669\u7ef4\u5ea6\uff1b2\uff09\u6784\u5efa\u5408\u6210PII\u6570\u636e\u96c6\uff081700\u4e2a\u5b9e\u4f8b\uff09\u6a21\u62df\u771f\u5b9e\u66b4\u9732\u573a\u666f\uff1b3\uff09\u4e0e\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\uff08\u68af\u5ea6\u4e0a\u5347\u3001\u8d1f\u504f\u597d\u4f18\u5316\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539\u7b97\u6cd5\u539f\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cUnPII\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff1a\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad811.8%\uff0c\u5b9e\u7528\u6027\u63d0\u5347\u6700\u9ad86.3%\uff0c\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u6700\u9ad812.4%\u3002\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u5e73\u5747\u4ea7\u751f27.5%\u7684\u5fae\u8c03\u5f00\u9500\u3002", "conclusion": "UnPII\u662f\u9996\u4e2a\u57fa\u4e8ePII\u98ce\u9669\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7PRI\u5b9e\u73b0\u5dee\u5f02\u5316\u9057\u5fd8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u7b97\u6cd5\u7684\u517c\u5bb9\u6027\uff0c\u4e3a\u5408\u89c4\u6027\u6570\u636e\u5220\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01792", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.01792", "abs": "https://arxiv.org/abs/2601.01792", "authors": ["NAVER Cloud HyperCLOVA X Team"], "title": "HyperCLOVA X 8B Omni", "comment": "Technical Report", "summary": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.", "AI": {"tldr": "HyperCLOVA X 8B Omni\u662f\u9996\u4e2a\u652f\u6301\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u4efb\u610f\u8f93\u5165\u8f93\u51fa\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5e8f\u5217\u5904\u7406\u5b9e\u73b0\u7406\u89e3\u548c\u751f\u6210\uff0c\u5728\u97e9\u8bed\u548c\u82f1\u8bed\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u4efb\u610f\u5230\u4efb\u610f\u5168\u6a21\u6001\u52a9\u624b\uff0c\u907f\u514d\u4f20\u7edf\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u5206\u79bb\u7684\u6a21\u6001\u7279\u5b9a\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u66f4\u5b9e\u7528\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u63a5\u53e3\u7edf\u4e00\u5904\u7406\u4ea4\u9519\u7684\u591a\u6a21\u6001\u5e8f\u5217\uff0c\u4f7f\u7528\u89c6\u89c9\u548c\u97f3\u9891\u7f16\u7801\u5668\u6ce8\u5165\u8fde\u7eed\u5d4c\u5165\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7406\u89e3\u548c\u57fa\u7840\u3002", "result": "\u5728\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u7684\u591a\u79cd\u8f93\u5165\u8f93\u51fa\u7ec4\u5408\u4e0a\uff0c\u4e0e\u7c7b\u4f3c\u89c4\u6a21\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u652f\u6301\u97e9\u8bed\u548c\u82f1\u8bed\u3002", "conclusion": "HyperCLOVA X 8B Omni\u4f5c\u4e3a8B\u89c4\u6a21\u7684\u5168\u6a21\u6001\u8def\u5f84\u70b9\uff0c\u5176\u5f00\u6e90\u6743\u91cd\u5c06\u652f\u6301\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2601.01793", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01793", "abs": "https://arxiv.org/abs/2601.01793", "authors": ["Shamik Bhattacharyya", "Rachel Kalpana Kalaimani"], "title": "Distributed Federated Learning by Alternating Periods of Training", "comment": null, "summary": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u670d\u52a1\u5668\u67b6\u6784\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4f9d\u8d56\u5355\u4e00\u4e2d\u592e\u670d\u52a1\u5668\uff0c\u5728\u5927\u89c4\u6a21\u5ba2\u6237\u7aef\u573a\u666f\u4e0b\u9762\u4e34\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u9700\u8981\u66f4\u5065\u58ee\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848", "method": "\u8bbe\u8ba1\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e2a\u5177\u6709\u670d\u52a1\u5668\u95f4\u901a\u4fe1\u80fd\u529b\u7684\u670d\u52a1\u5668\uff0c\u6bcf\u4e2a\u670d\u52a1\u5668\u5173\u8054\u4e00\u7ec4\u4e0d\u76f8\u4ea4\u7684\u5ba2\u6237\u7aef\uff1b\u63d0\u51faDFL\u7b97\u6cd5\uff0c\u4ea4\u66ff\u8fdb\u884c\u5ba2\u6237\u7aef\u672c\u5730\u8bad\u7ec3\u548c\u670d\u52a1\u5668\u95f4\u5168\u5c40\u8bad\u7ec3", "result": "\u5728\u9002\u5f53\u53c2\u6570\u9009\u62e9\u4e0b\uff0cDFL\u7b97\u6cd5\u786e\u4fdd\u6240\u6709\u670d\u52a1\u5668\u6536\u655b\u5230\u7406\u60f3\u6a21\u578b\u7684\u5fae\u5c0f\u5bb9\u5fcd\u8303\u56f4\u5185\uff0c\u6709\u6548\u6574\u5408\u672c\u5730\u548c\u5168\u5c40\u8bad\u7ec3\u6a21\u578b\uff1b\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u4e3b\u5f20", "conclusion": "\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u9650\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5065\u58ee\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01800", "abs": "https://arxiv.org/abs/2601.01800", "authors": ["Qi Wei", "Junchao Fan", "Zhao Yang", "Jianhua Wang", "Jingkai Mao", "Xiaolin Chang"], "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.", "AI": {"tldr": "CARRL\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7a00\u758f\u5b89\u5168\u98ce\u9669\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u98ce\u9669\u66b4\u9732\u5bf9\u624b\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\u7684\u535a\u5f08\uff0c\u5728\u7ea6\u675f\u9884\u7b97\u4e0b\u8bc6\u522b\u5173\u952e\u5b89\u5168\u65f6\u523b\uff0c\u51cf\u5c11\u78b0\u649e\u7387\u81f3\u5c1122.66%\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u5c06\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u4e0e\u5bf9\u624b\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u96f6\u548c\u535a\u5f08\uff0c\u5ffd\u7565\u4e86\u53cc\u65b9\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u4e14\u672a\u80fd\u53cd\u6620\u5b89\u5168\u5173\u952e\u98ce\u9669\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "CARRL\u5305\u542b\u4e24\u4e2a\u4ea4\u4e92\u7ec4\u4ef6\uff1a\u98ce\u9669\u66b4\u9732\u5bf9\u624b\uff08REA\uff09\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\uff08RTRA\uff09\u3002\u5c06\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u822c\u548c\u535a\u5f08\uff0cREA\u91c7\u7528\u89e3\u8026\u4f18\u5316\u673a\u5236\u5728\u7ea6\u675f\u9884\u7b97\u4e0b\u8bc6\u522b\u7a00\u758f\u5b89\u5168\u5173\u952e\u65f6\u523b\uff0cRTRA\u901a\u8fc7\u53cc\u56de\u653e\u7f13\u51b2\u6c60\u8054\u5408\u5229\u7528\u6b63\u5e38\u548c\u5bf9\u6297\u7ecf\u9a8c\uff0c\u5e76\u5f3a\u5236\u6270\u52a8\u4e0b\u7684\u7b56\u7565\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u81f3\u5c11\u51cf\u5c1122.66%\u7684\u78b0\u649e\u7387\u3002", "conclusion": "CARRL\u901a\u8fc7\u5efa\u6a21\u4e0d\u5bf9\u79f0\u4ea4\u4e92\u548c\u7a00\u758f\u5b89\u5168\u98ce\u9669\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.01803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01803", "abs": "https://arxiv.org/abs/2601.01803", "authors": ["Dennis Jabs", "Aditya Mohan", "Marius Lindauer"], "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions", "comment": "Workshop paper at RLDM'25", "summary": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(\u03b8)$, obtained by repeatedly sampling minibatches, updating $\u03b8$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(\u03b8)$ can improve stability, directly estimating $R(\u03b8)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(\u03b8)$. In such cases, our moment-based correction narrows $R(\u03b8)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u8bc4\u8bba\u5bb6\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u7684PPO\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\u6765\u51cf\u5c11\u7b56\u7565\u66f4\u65b0\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728Walker2D\u73af\u5883\u4e2d\u5c06\u7a33\u5b9a\u6027\u63d0\u5347\u8fbe75%\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5e38\u5b66\u4e60\u5230\u76f8\u540c\u7d2f\u79ef\u56de\u62a5\u4f46\u884c\u4e3a\u5dee\u5f02\u5f88\u5927\u7684\u7b56\u7565\uff0c\u8fd9\u662f\u7531\u4e8e\u73af\u5883\u548c\u7b97\u6cd5\u56e0\u7d20\u5bfc\u81f4\u7684\u3002\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u5c0f\u7684\u53c2\u6570\u53d8\u5316\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u6b65\u6001\uff0c\u8fd9\u65e2\u5f71\u54cd\u7b97\u6cd5\u6bd4\u8f83\u4e5f\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u3002\u867d\u7136\u7ea6\u675f\u7b56\u7565\u4fdd\u6301\u7a84\u7684\u56de\u62a5\u5206\u5e03\u53ef\u4ee5\u6539\u5584\u7a33\u5b9a\u6027\uff0c\u4f46\u76f4\u63a5\u4f30\u8ba1\u8be5\u5206\u5e03\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u72b6\u6001-\u52a8\u4f5c\u56de\u62a5\u5206\u5e03\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u5206\u5e03\u7684\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u6765\u504f\u7f6ePPO\u7684\u4f18\u52bf\u51fd\u6570\u3002\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\uff0c\u8be5\u65b9\u6cd5\u963b\u6b62\u7b56\u7565\u8fdb\u5165\u5bb9\u6613\u4e0d\u7a33\u5b9a\u7684\u53c2\u6570\u533a\u57df\u3002", "result": "\u5728Walker2D\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u7a33\u5b9a\u6027\u63d0\u5347\u8fbe75%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u56de\u62a5\u3002\u5f53\u66f4\u65b0\u540e\u8bc4\u8bba\u5bb6\u503c\u4e0e\u66f4\u65b0\u540e\u56de\u62a5\u5bf9\u9f50\u4e0d\u4f73\u65f6\uff0c\u6807\u51c6PPO\u96be\u4ee5\u4ea7\u751f\u7a84\u7684\u56de\u62a5\u5206\u5e03\uff0c\u800c\u57fa\u4e8e\u77e9\u7684\u4fee\u6b63\u80fd\u6709\u6548\u7f29\u5c0f\u8be5\u5206\u5e03\u3002", "conclusion": "\u5229\u7528\u73af\u5883\u968f\u673a\u6027\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u7684\u9ad8\u9636\u77e9\u6765\u504f\u7f6e\u4f18\u52bf\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u7b56\u7565\u66f4\u65b0\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u6539\u5584\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2601.01829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01829", "abs": "https://arxiv.org/abs/2601.01829", "authors": ["Peiyan Hu", "Haodong Feng", "Hongyuan Liu", "Tongtong Yan", "Wenhao Deng", "Tianrun Gao", "Rong Zheng", "Haoren Zheng", "Chenglei Yu", "Chuanrui Wang", "Kaiwen Li", "Zhi-Ming Ma", "Dezhi Zhou", "Xingcai Lu", "Dixia Fan", "Tailin Wu"], "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data", "comment": "46 pages, 21 figures", "summary": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.", "AI": {"tldr": "RealPDEBench\uff1a\u9996\u4e2a\u7ed3\u5408\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u4e0e\u914d\u5bf9\u6570\u503c\u6a21\u62df\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u6570\u636e\u96c6\u30013\u4e2a\u4efb\u52a1\u30018\u4e2a\u6307\u6807\u548c10\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3sim-to-real\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u53d1\u5c55\u3001\u8bc4\u4f30\u4ee5\u53casim-to-real\u8fc1\u79fb\u7814\u7a76\u3002\u9700\u8981\u5efa\u7acb\u8fde\u63a5\u771f\u5b9e\u6d4b\u91cf\u4e0e\u6a21\u62df\u6570\u636e\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u79d1\u5b66ML\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u6784\u5efa\u5305\u542b5\u4e2a\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\u6570\u636e\u96c6\u53ca\u5176\u914d\u5bf9\u6570\u503c\u6a21\u62df\u7684\u57fa\u51c6\uff1b\u5b9a\u4e493\u4e2a\u4efb\u52a1\uff08\u771f\u5b9e\u6570\u636e\u9884\u6d4b\u3001\u6a21\u62df\u6570\u636e\u9884\u6d4b\u3001sim-to-real\u8fc1\u79fb\uff09\uff1b\u8bbe\u8ba18\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u6570\u636e\u5bfc\u5411\u548c\u7269\u7406\u5bfc\u5411\uff09\uff1b\u8bc4\u4f3010\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u6a21\u578b\uff08SOTA\u6a21\u578b\u3001\u9884\u8bad\u7ec3PDE\u57fa\u7840\u6a21\u578b\u3001\u4f20\u7edf\u65b9\u6cd5\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u62df\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u80fd\u6301\u7eed\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u51c6\u786e\u6027\u548c\u6536\u655b\u6027\u3002\u57fa\u51c6\u63ed\u793a\u4e86sim-to-real\u9e3f\u6c9f\u7684\u5177\u4f53\u8868\u73b0\u3002", "conclusion": "RealPDEBench\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u8fde\u63a5\u771f\u5b9e\u4e0e\u6a21\u62df\u6570\u636e\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86sim-to-real\u5dee\u8ddd\uff0c\u5e76\u8bc1\u660e\u6a21\u62df\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u79d1\u5b66ML\u5411\u5b9e\u9645\u90e8\u7f72\u53d1\u5c55\u3002"}}
{"id": "2601.01833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01833", "abs": "https://arxiv.org/abs/2601.01833", "authors": ["Chenyu Hu", "Qiming Hu", "Sinan Chen", "Nianyu Li", "Mingyue Zhang", "Jialong Li"], "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks", "comment": null, "summary": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.", "AI": {"tldr": "FAROS\uff1a\u4e00\u4e2a\u589e\u5f3a\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\u6765\u9632\u5fa1\u540e\u95e8\u653b\u51fb\uff0c\u76f8\u6bd4\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u540e\u95e8\u653b\u51fb\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u6570\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u653b\u51fb\u7b56\u7565\u3002", "method": "\u63d0\u51faFAROS\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\uff08ADS\uff09\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\uff08RCC\uff09\u3002ADS\u6839\u636e\u5ba2\u6237\u7aef\u4e0a\u4f20\u68af\u5ea6\u7684\u79bb\u6563\u5ea6\u52a8\u6001\u8c03\u6574\u9632\u5fa1\u654f\u611f\u5ea6\uff1bRCC\u901a\u8fc7\u8ba1\u7b97\u9ad8\u7f6e\u4fe1\u5ea6\u5ba2\u6237\u7aef\u6838\u5fc3\u96c6\u7684\u8d28\u5fc3\u6765\u964d\u4f4e\u5355\u70b9\u6545\u969c\u98ce\u9669\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "FAROS\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u9632\u5fa1\u673a\u5236\u6709\u6548\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u56fa\u6709\u9650\u5236\u3002"}}
{"id": "2601.01860", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.01860", "abs": "https://arxiv.org/abs/2601.01860", "authors": ["Shuta Kikuchi", "Shu Tanaka"], "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation", "comment": "6 pages, 2 figures", "summary": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u673a\u548c\u4e8c\u6b21\u4f18\u5316\u9000\u706b(FMQA)\u7684\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c06\u4e0a\u4f4d\u6027\u68c0\u6d4b\u5efa\u6a21\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528MDR\u8ba1\u7b97\u7684\u5206\u7c7b\u9519\u8bef\u7387\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u5185\u6709\u6548\u8bc6\u522b\u9ad8\u9636\u4e0a\u4f4d\u6027\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u5728\u9057\u4f20\u5173\u8054\u7814\u7a76\u4e2d\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u7684\u8ba1\u7b97\u6311\u6218\u3002\u867d\u7136\u591a\u56e0\u5b50\u964d\u7ef4(MDR)\u662f\u5e38\u7528\u7684\u4e0a\u4f4d\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u968f\u7740\u4f4d\u70b9\u6570\u91cf\u6216\u76f8\u4e92\u4f5c\u7528\u9636\u6570\u589e\u52a0\uff0c\u57fa\u4e8eMDR\u7684\u7a77\u4e3e\u641c\u7d22\u53d8\u5f97\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "\u5c06\u4e0a\u4f4d\u6027\u68c0\u6d4b\u95ee\u9898\u5b9a\u4e49\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u56e0\u5b50\u5206\u89e3\u673a\u7ed3\u5408\u4e8c\u6b21\u4f18\u5316\u9000\u706b(FMQA)\u6c42\u89e3\u3002\u63d0\u51fa\u57fa\u4e8eFMQA\u7684\u9ad8\u6548\u4e0a\u4f4d\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5176\u4e2dMDR\u8ba1\u7b97\u7684\u5206\u7c7b\u9519\u8bef\u7387(CER)\u4f5c\u4e3a\u9ed1\u76d2\u76ee\u6807\u51fd\u6570\u3002", "result": "\u4f7f\u7528\u5177\u6709\u9884\u5b9a\u4e49\u9ad8\u9636\u4e0a\u4f4d\u6027\u7684\u6a21\u62df\u75c5\u4f8b\u5bf9\u7167\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5404\u79cd\u76f8\u4e92\u4f5c\u7528\u9636\u6570\u548c\u9057\u4f20\u4f4d\u70b9\u6570\u91cf\u4e0b\uff0c\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u5185\u6210\u529f\u8bc6\u522b\u51fa\u771f\u5b9e\u7684\u4e0a\u4f4d\u6027\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5bf9\u4e8e\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u662f\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\uff0c\u80fd\u591f\u89e3\u51b3\u4f20\u7edfMDR\u65b9\u6cd5\u9762\u4e34\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002"}}
{"id": "2601.01887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01887", "abs": "https://arxiv.org/abs/2601.01887", "authors": ["Jiawen Zhang", "Lipeng He", "Kejia Chen", "Jian Lou", "Jian Liu", "Xiaohu Yang", "Ruoxi Jia"], "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance", "comment": null, "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.", "AI": {"tldr": "\u4ec5\u9700\u4e00\u4e2a\u5b89\u5168\u6837\u672c\u5373\u53ef\u5b8c\u5168\u6062\u590d\u5b89\u5168\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u727a\u7272\u6027\u80fd\u4e14\u6210\u672c\u6781\u4f4e", "motivation": "\u5fae\u8c03\u5b89\u5168\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u663e\u8457\u635f\u5bb3\u5176\u5b89\u5168\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5b89\u5168\u6837\u672c\u6216\u6821\u51c6\u96c6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6a21\u578b\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51fa\u4ec5\u9700\u5355\u4e2a\u5b89\u5168\u793a\u4f8b\u5373\u53ef\u6062\u590d\u5b89\u5168\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b89\u5168\u68af\u5ea6\u5177\u6709\u4f4e\u79e9\u7ed3\u6784\uff0c\u89e3\u91ca\u4e86\u8fd9\u79cd\u9ad8\u6548\u4fee\u6b63\u7684\u53ef\u80fd\u6027", "result": "\u8be5\u65b9\u6cd5\u57285\u4e2a\u5b89\u5168\u5bf9\u9f50LLM\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u65e0\u8bba\u6709\u5bb3\u793a\u4f8b\u6570\u91cf\u6216\u6a21\u578b\u5927\u5c0f\u5982\u4f55\uff0c\u90fd\u80fd\u5728\u51e0\u4e2aepoch\u5185\u6536\u655b", "conclusion": "\u5b89\u5168\u5bf9\u9f50\u53ef\u4ee5\u6781\u4f4e\u6210\u672c\u9ad8\u6548\u6062\u590d\uff0c\u65e0\u9700\u727a\u7272\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLLM\u5b89\u5168\u4fee\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.01901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01901", "abs": "https://arxiv.org/abs/2601.01901", "authors": ["Yuexuan Xia", "Yinghao Zhang", "Yalin Liu", "Hong-Ning Dai", "Yong Xia"], "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data", "comment": null, "summary": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.", "AI": {"tldr": "FedBiCross\uff1a\u4e00\u79cd\u4e2a\u6027\u5316\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u3001\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\u548c\u4e2a\u6027\u5316\u84b8\u998f\u89e3\u51b3\u975eIID\u6570\u636e\u4e0b\u9884\u6d4b\u51b2\u7a81\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u975eIID\u6570\u636e\u4e0b\uff0c\u6240\u6709\u5ba2\u6237\u7aef\u7684\u9884\u6d4b\u5728\u5e73\u5747\u65f6\u4f1a\u76f8\u4e92\u62b5\u6d88\uff0c\u4ea7\u751f\u63a5\u8fd1\u5747\u5300\u7684\u8f6f\u6807\u7b7e\uff0c\u4e3a\u84b8\u998f\u63d0\u4f9b\u5f31\u76d1\u7763\u3002\u8fd9\u9650\u5236\u4e86\u5728\u9690\u79c1\u654f\u611f\u7684\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faFedBiCross\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u6839\u636e\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\u6027\u5bf9\u5ba2\u6237\u7aef\u8fdb\u884c\u805a\u7c7b\uff0c\u5f62\u6210\u4e00\u81f4\u7684\u5b50\u96c6\u6210\uff1b2\uff09\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u9009\u62e9\u6027\u5730\u5229\u7528\u6709\u76ca\u7684\u8de8\u96c6\u7fa4\u77e5\u8bc6\uff0c\u540c\u65f6\u6291\u5236\u8d1f\u8fc1\u79fb\uff1b3\uff09\u4e2a\u6027\u5316\u84b8\u998f\uff0c\u8fdb\u884c\u5ba2\u6237\u7aef\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedBiCross\u5728\u4e0d\u540c\u975eIID\u7a0b\u5ea6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedBiCross\u901a\u8fc7\u805a\u7c7b\u548c\u9009\u62e9\u6027\u77e5\u8bc6\u8f6c\u79fb\u6709\u6548\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u4e0b\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9884\u6d4b\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u533b\u7597\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01903", "abs": "https://arxiv.org/abs/2601.01903", "authors": ["Ungsik Kim", "Suwon Lee"], "title": "TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train", "comment": null, "summary": "The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\\ell \\cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \\mapsto \\text{FSI}(v)$ admits an MPO representation with TT-rank $O(\\ell d)$, enabling an efficient sweep algorithm with $O(\\ell^2 d^3 \\cdot 2^d)$ time and $O(\\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\\times$ speedup over baseline, 85$\\times$ over SHAP-IQ, and 290$\\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.", "AI": {"tldr": "TT-FSI\uff1a\u5229\u7528\u77e9\u9635\u4e58\u79ef\u7b97\u5b50\uff08MPO\uff09\u9ad8\u6548\u8ba1\u7b97\u5fe0\u5b9eShapley\u4ea4\u4e92\u6307\u6570\uff0c\u5c06\u5185\u5b58\u9700\u6c42\u4eceO(4^d)\u964d\u81f3O(\u2113d\u00b2)\uff0c\u65f6\u95f4\u4eceO(d^\u2113\u00b72^d)\u964d\u81f3O(\u2113\u00b2d\u00b3\u00b72^d)\uff0c\u5728d=20\u65f6\u5b9e\u73b0280\u500d\u52a0\u901f\u548c290\u500d\u5185\u5b58\u51cf\u5c11\u3002", "motivation": "\u5fe0\u5b9eShapley\u4ea4\u4e92\u6307\u6570\uff08FSI\uff09\u662f\u552f\u4e00\u6ee1\u8db3\u5fe0\u5b9e\u6027\u516c\u7406\u7684Shapley\u4ea4\u4e92\u6307\u6570\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u9700\u8981O(d^\u2113\u00b72^d)\u65f6\u95f4\u548cO(4^d)\u5185\u5b58\uff0c\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faTT-FSI\u65b9\u6cd5\uff0c\u5229\u7528FSI\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u8bc1\u660e\u7ebf\u6027\u7b97\u5b50v\u21a6FSI(v)\u5177\u6709TT\u79e9\u4e3aO(\u2113d)\u7684\u77e9\u9635\u4e58\u79ef\u7b97\u5b50\u8868\u793a\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u9ad8\u6548\u7684\u626b\u63cf\u7b97\u6cd5\uff0c\u4ec5\u9700O(\u2113\u00b2d\u00b3\u00b72^d)\u65f6\u95f4\u548cO(\u2113d\u00b2)\u6838\u5fc3\u5b58\u50a8\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\uff08d=8\u5230d=20\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u6700\u9ad8280\u500d\u52a0\u901f\uff0c\u76f8\u6bd4SHAP-IQ\u6700\u9ad885\u500d\u52a0\u901f\uff0c\u5185\u5b58\u51cf\u5c11290\u500d\u3002TT-FSI\u53ef\u6269\u5c55\u5230d=20\uff08100\u4e07\u4e2a\u8054\u76df\uff09\uff0c\u800c\u6240\u6709\u7ade\u4e89\u65b9\u6cd5\u5747\u5931\u8d25\u3002", "conclusion": "TT-FSI\u901a\u8fc7\u5f20\u91cf\u7f51\u7edc\u8868\u793a\u663e\u8457\u964d\u4f4e\u4e86FSI\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u7684\u5185\u5b58\u548c\u65f6\u95f4\u6539\u8fdb\uff0c\u4f7f\u5fe0\u5b9eShapley\u4ea4\u4e92\u6307\u6570\u80fd\u591f\u5e94\u7528\u4e8e\u5b9e\u9645\u9ad8\u7ef4\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u7279\u5f81\u4ea4\u4e92\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2601.01904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01904", "abs": "https://arxiv.org/abs/2601.01904", "authors": ["Yuxuan Li", "Harshith Reddy Kethireddy", "Srijita Das"], "title": "Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning", "comment": null, "summary": "Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.\n  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.\n  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u504f\u597d\u5b66\u4e60\uff08PbRL\uff09\uff0c\u91cd\u70b9\u5173\u6ce8\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u5728\u67d0\u4e9b\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u666e\u901aPbRL\u65b9\u6cd5\u53cd\u800c\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u504f\u597d\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u504f\u597d\u6570\u636e\u5e38\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u5747\u5300\u5206\u5e03\u7684\u566a\u58f0\uff0c\u800c\u5ffd\u7565\u4e86\u4e0e\u89c2\u6d4b\u7279\u5f81\u76f8\u5173\u7684\u566a\u58f0\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u4e3a\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u7684\u5f62\u5f0f\u5316\u6982\u5ff5\uff0c\u5305\u62ec\u8f68\u8ff9\u7279\u5f81\u566a\u58f0\u3001\u8f68\u8ff9\u76f8\u4f3c\u6027\u566a\u58f0\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u566a\u58f0\u548c\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u7b49\u591a\u79cd\u53d8\u4f53\u3002\u5728DMControl\u548cMeta-world\u7684\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u67d0\u4e9b\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u8bbe\u7f6e\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u566a\u58f0\u9c81\u68d2PbRL\u65b9\u6cd5\u5b66\u4e60\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u6ca1\u6709\u663e\u5f0f\u53bb\u566a\u7684PbRL\u65b9\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u53cd\u800c\u8868\u73b0\u66f4\u597d\u3002\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u8868\u73b0\u51fa\u4e0e\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u76f8\u4f3c\u7684\u7279\u5f81\u3002", "conclusion": "\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u5bf9\u73b0\u6709\u566a\u58f0\u9c81\u68d2PbRL\u65b9\u6cd5\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u9c81\u68d2\u5730\u5b66\u4e60\u5904\u7406\u8fd9\u7c7b\u566a\u58f0\u3002\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u6a21\u62df\u4e86\u771f\u5b9e\u4eba\u7c7b\u7684\u566a\u58f0\u7279\u5f81\uff0c\u503c\u5f97\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2601.01917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01917", "abs": "https://arxiv.org/abs/2601.01917", "authors": ["Ryo Iwaki", "Takayuki Osogami"], "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning", "comment": "The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at https://link.springer.com/chapter/10.1007/978-981-95-4091-4_35", "summary": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u4f4d\u6570\u626d\u66f2\u7684\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6839\u636e\u6570\u636e\u53ef\u7528\u6027\u8c03\u6574\u4fdd\u5b88\u7a0b\u5ea6\u6765\u514b\u670d\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u5bfc\u81f4\u7684\u8fc7\u5ea6\u4fdd\u5b88\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\uff0c\u5373\u5bf9\u6240\u6709\u5206\u4f4d\u6570\u8fdb\u884c\u5747\u5300\u4f4e\u4f30\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u4fdd\u5b88\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u4ece\u800c\u9650\u5236\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u5206\u4f4d\u6570\u626d\u66f2\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u6839\u636e\u652f\u6301\u6570\u636e\u7684\u53ef\u7528\u6027\u8c03\u6574\u4fdd\u5b88\u7a0b\u5ea6\uff0c\u5b9e\u73b0\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7406\u8bba\u5206\u6790\uff0c\u80fd\u591f\u9488\u5bf9\u4e0d\u540c\u5206\u4f4d\u6570\u5e94\u7528\u4e0d\u540c\u7a0b\u5ea6\u7684\u60b2\u89c2\u4e3b\u4e49\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u8bc1\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u5728\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5206\u4f4d\u6570\u626d\u66f2\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u514b\u670d\u4e86\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u548c\u6027\u80fd\u3002"}}
{"id": "2601.01927", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01927", "abs": "https://arxiv.org/abs/2601.01927", "authors": ["Firuz Kamalov", "Hana Sulieman", "Witold Pedrycz"], "title": "Theoretical Convergence of SMOTE-Generated Samples", "comment": null, "summary": "Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.", "AI": {"tldr": "\u672c\u6587\u5bf9SMOTE\u8fc7\u91c7\u6837\u65b9\u6cd5\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5408\u6210\u53d8\u91cf\u5728\u6982\u7387\u4e0a\u6536\u655b\u4e8e\u539f\u59cb\u53d8\u91cf\uff0c\u5728\u7d27\u81f4\u6761\u4ef6\u4e0b\u6709\u66f4\u5f3a\u7684\u5747\u503c\u6536\u655b\uff0c\u5e76\u53d1\u73b0\u8f83\u5c0f\u7684\u6700\u8fd1\u90bb\u79e9\u80fd\u5e26\u6765\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u4e0d\u5e73\u8861\u6570\u636e\u5e7f\u6cdb\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0cSMOTE\u4f5c\u4e3a\u6700\u6d41\u884c\u7684\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\uff0c\u9700\u8981\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u4e24\u65b9\u9762\u8fdb\u884c\u9a8c\u8bc1\u3002\u5f53\u524d\u7f3a\u4e4f\u5bf9SMOTE\u6536\u655b\u6027\u8d28\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002", "method": "\u5bf9SMOTE\u65b9\u6cd5\u8fdb\u884c\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u5408\u6210\u968f\u673a\u53d8\u91cfZ\u5728\u6982\u7387\u4e0a\u6536\u655b\u4e8e\u57fa\u7840\u968f\u673a\u53d8\u91cfX\uff0c\u5728X\u7d27\u81f4\u6761\u4ef6\u4e0b\u8bc1\u660e\u66f4\u5f3a\u7684\u5747\u503c\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u8bc1\u660e\u4e86SMOTE\u5408\u6210\u53d8\u91cfZ\u4ee5\u6982\u7387\u6536\u655b\u4e8eX\uff0c\u5728\u7d27\u81f4\u6761\u4ef6\u4e0b\u6709\u5747\u503c\u6536\u655b\uff0c\u53d1\u73b0\u8f83\u5c0f\u7684\u6700\u8fd1\u90bb\u79e9\u80fd\u5e26\u6765\u66f4\u5feb\u6536\u655b\uff0c\u6570\u503c\u5b9e\u9a8c\u652f\u6301\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u4e3aSMOTE\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u7406\u89e3\uff0c\u8d85\u8d8a\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u573a\u666f\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2601.01931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01931", "abs": "https://arxiv.org/abs/2601.01931", "authors": ["Willem R\u00f6pke", "Samuel Coward", "Andrei Lupu", "Thomas Foster", "Tim Rockt\u00e4schel", "Jakob Foerster"], "title": "D\u00e9j\u00e0Q: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems", "comment": null, "summary": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce D\u00e9j\u00e0Q, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of D\u00e9j\u00e0Q, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.", "AI": {"tldr": "D\u00e9j\u00e0Q\u662f\u4e00\u4e2a\u901a\u8fc7\u8054\u5408\u6f14\u5316\u5408\u6210\u6570\u5b66\u95ee\u9898\u4e0e\u6a21\u578b\u8bad\u7ec3\u6765\u589e\u5f3a\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7a81\u53d8\u7b56\u7565\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6570\u636e", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u3002\u9700\u8981\u52a8\u6001\u9002\u5e94\u6a21\u578b\u80fd\u529b\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faD\u00e9j\u00e0Q\u6846\u67b6\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8054\u5408\u6f14\u5316\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u5b66\u95ee\u9898\u3002\u4f7f\u7528\u4e24\u79cdLLM\u9a71\u52a8\u7684\u7a81\u53d8\u7b56\u7565\uff1a1\uff09\u6539\u53d8\u4e0a\u4e0b\u6587\u7ec6\u8282\uff1b2\uff09\u76f4\u63a5\u4fee\u6539\u95ee\u9898\u7ed3\u6784\u3002\u6a21\u578b\u81ea\u8eab\u53c2\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u7a81\u53d8\u751f\u6210", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u65b0\u9896\u4e14\u6709\u610f\u4e49\u7684\u6570\u5b66\u95ee\u9898\uff0cLLM\u9a71\u52a8\u7684\u7a81\u53d8\u7b56\u7565\u6539\u5584\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u679c\u3002\u5206\u6790\u4e86\u751f\u6210\u95ee\u9898\u7684\u6709\u6548\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u6f14\u5316\u8bad\u7ec3\u6570\u636e\u7684\u6f5c\u529b", "conclusion": "\u52a8\u6001\u6f14\u5316\u8bad\u7ec3\u6570\u636e\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u4f5c\u8005\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2601.01943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01943", "abs": "https://arxiv.org/abs/2601.01943", "authors": ["Tieu-Long Phan", "Nhu-Ngoc Nguyen Song", "Peter F. Stadler"], "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling", "comment": "31 pages (including references), 3 figures, 7 tables", "summary": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.", "AI": {"tldr": "SynRXN\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u7684\u7edf\u4e00\u57fa\u51c6\u6846\u67b6\u548c\u5f00\u653e\u6570\u636e\u8d44\u6e90\uff0c\u5c06\u7aef\u5230\u7aef\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u7cfb\u5217\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5b58\u5728\u6570\u636e\u96c6\u5f02\u8d28\u6027\u3001\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\u3001\u6570\u636e\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u7cfb\u5217\uff1a\u53cd\u5e94\u5e73\u8861\u3001\u539f\u5b50\u6620\u5c04\u3001\u53cd\u5e94\u5206\u7c7b\u3001\u53cd\u5e94\u6027\u8d28\u9884\u6d4b\u548c\u5408\u6210\u8def\u7ebf\u8bbe\u8ba1\uff1b\u4ece\u5f02\u6784\u516c\u5171\u6e90\u6536\u96c6\u53cd\u5e94\u6570\u636e\uff0c\u8fdb\u884c\u7edf\u4e00\u8868\u793a\u548c\u7248\u672c\u63a7\u5236\uff1b\u63d0\u4f9b\u900f\u660e\u5206\u5272\u51fd\u6570\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u548c\u6307\u6807\u5957\u4ef6\uff1b\u91c7\u7528\u9632\u6cc4\u6f0f\u7684\u6570\u636e\u5212\u5206\u7b56\u7565\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7248\u672c\u5316\u6570\u636e\u96c6\u3001\u673a\u5668\u53ef\u8bfb\u6e05\u5355\u3001\u6821\u9a8c\u548c\u3001\u884c\u6570\u7edf\u8ba1\u7684\u5f00\u653e\u8d44\u6e90\uff1b\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u751f\u6210\u7684\u811a\u672c\u5316\u6784\u5efa\u65b9\u6848\uff1b\u652f\u6301\u654f\u611f\u4efb\u52a1\u7684\u72ec\u7acb\u8bc4\u4f30\u96c6\uff0c\u9632\u6b62\u6570\u636e\u6c61\u67d3\u3002", "conclusion": "SynRXN\u901a\u8fc7\u6d88\u9664\u6570\u636e\u96c6\u5f02\u8d28\u6027\u5e76\u63d0\u4f9b\u900f\u660e\u53ef\u590d\u7528\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86CASP\u65b9\u6cd5\u7684\u516c\u5e73\u7eb5\u5411\u6bd4\u8f83\uff0c\u652f\u6301\u5168\u53cd\u5e94\u4fe1\u606f\u5b66\u7ba1\u9053\u7684\u4e25\u683c\u6d88\u878d\u548c\u538b\u529b\u6d4b\u8bd5\uff0c\u964d\u4f4e\u4e86\u5b9e\u9645\u5408\u6210\u89c4\u5212\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8bc4\u4f30\u95e8\u69db\u3002"}}
{"id": "2601.01966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01966", "abs": "https://arxiv.org/abs/2601.01966", "authors": ["Bo Yin", "Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior", "comment": null, "summary": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRefinement Provenance Inference (RPI)\u4efb\u52a1\uff0c\u5f00\u53d1RePro\u6846\u67b6\u901a\u8fc7\u6559\u5e08\u5f3a\u5236token\u5206\u5e03\u548clogit\u6392\u5e8f\u4fe1\u53f7\u6765\u63a8\u65ad\u8bad\u7ec3\u6570\u636e\u4e2dprompt\u662f\u5426\u7ecf\u8fc7LLM\u7cbe\u70bc", "motivation": "\u6307\u4ee4\u8c03\u4f18\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56LLM-based prompt refinement\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5b9e\u4f8b\u7ea7\u5ba1\u8ba1\u95ee\u9898\uff1a\u5bf9\u4e8e\u5fae\u8c03\u6a21\u578b\u548c\u8bad\u7ec3prompt-response\u5bf9\uff0c\u80fd\u5426\u63a8\u65ad\u6a21\u578b\u662f\u5728\u539f\u59cbprompt\u8fd8\u662f\u5176LLM\u7cbe\u70bc\u7248\u672c\u4e0a\u8bad\u7ec3\u7684\uff1f\u8fd9\u5bf9\u6570\u636e\u96c6\u6cbb\u7406\u548c\u8bad\u7ec3\u6570\u636e\u4e89\u8bae\u89e3\u51b3\u5f88\u91cd\u8981\u3002", "method": "\u63d0\u51faRePro\u6846\u67b6\uff0c\u5229\u7528prompt refinement\u5bfc\u81f4\u7684\u6559\u5e08\u5f3a\u5236token\u5206\u5e03\u7a33\u5b9a\u53ef\u68c0\u6d4b\u53d8\u5316\uff0c\u878d\u5408\u6559\u5e08\u5f3a\u5236\u4f3c\u7136\u7279\u5f81\u548clogit\u6392\u5e8f\u4fe1\u53f7\u3002\u901a\u8fc7shadow fine-tuning\u5b66\u4e60\u53ef\u8fc1\u79fb\u8868\u793a\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u5728\u672a\u89c1\u8fc7\u7684\u53d7\u5bb3\u8005\u6a21\u578b\u4e0a\u63a8\u65ad\u6765\u6e90\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u8bbf\u95ee\u3002", "result": "RePro\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u5f3a\u5927\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u8de8\u4e0d\u540crefiner\u8fc1\u79fb\uff0c\u8868\u660e\u5b83\u5229\u7528\u4e86refiner\u65e0\u5173\u7684\u5206\u5e03\u53d8\u5316\u800c\u975e\u91cd\u5199\u98ce\u683c\u4f2a\u5f71\u3002", "conclusion": "\u8bba\u6587\u5f62\u5f0f\u5316\u4e86Refinement Provenance Inference (RPI)\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86prompt refinement\u4f1a\u4ea7\u751f\u53ef\u68c0\u6d4b\u7684\u5206\u5e03\u53d8\u5316\uff0c\u63d0\u51fa\u7684RePro\u6846\u67b6\u80fd\u6709\u6548\u63a8\u65ad\u8bad\u7ec3\u6570\u636e\u7684\u6765\u6e90\uff0c\u4e3a\u6570\u636e\u96c6\u6cbb\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.01979", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01979", "abs": "https://arxiv.org/abs/2601.01979", "authors": ["Julie Keisler", "Anastase Alexandre Charantonis", "Yannig Goude", "Boutheina Oueslati", "Claire Monteleoni"], "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition", "comment": null, "summary": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.", "AI": {"tldr": "SerpentFlow\uff1a\u4e00\u79cd\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u5c06\u6570\u636e\u5206\u4e3a\u5171\u4eab\u7ed3\u6784\u548c\u57df\u7279\u5b9a\u6210\u5206\uff0c\u5229\u7528\u5408\u6210\u8bad\u7ec3\u5bf9\u5b9e\u73b0\u6761\u4ef6\u751f\u6210", "motivation": "\u89e3\u51b3\u65e0\u914d\u5bf9\u89c2\u6d4b\u60c5\u51b5\u4e0b\u7684\u57df\u5bf9\u9f50\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u57df\u95f4\u5171\u4eab\u5e95\u5c42\u7ed3\u6784\u6a21\u5f0f\u4f46\u5177\u4f53\u5b9e\u73b0\u4e0d\u540c\u7684\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u8de8\u57df\u76f4\u63a5\u76d1\u7763\u800c\u9762\u4e34\u6311\u6218", "method": "\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u5171\u4eab\u6210\u5206\u548c\u57df\u7279\u5b9a\u6210\u5206\uff0c\u901a\u8fc7\u9694\u79bb\u5171\u4eab\u7ed3\u6784\u5e76\u7528\u968f\u673a\u566a\u58f0\u66ff\u6362\u57df\u7279\u5b9a\u6210\u5206\uff0c\u6784\u5efa\u5171\u4eab\u8868\u793a\u4e0e\u76ee\u6807\u57df\u6837\u672c\u4e4b\u95f4\u7684\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u4ece\u800c\u652f\u6301\u6761\u4ef6\u751f\u6210\u6a21\u578b", "result": "\u5728\u5408\u6210\u56fe\u50cf\u3001\u7269\u7406\u8fc7\u7a0b\u6a21\u62df\u548c\u6c14\u5019\u964d\u5c3a\u5ea6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u91cd\u5efa\u4e0e\u5e95\u5c42\u4f4e\u9891\u6a21\u5f0f\u4e00\u81f4\u7684\u9ad8\u9891\u7ed3\u6784\uff0c\u652f\u6301\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u4f5c\u4e3a\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u7684\u6709\u6548\u7b56\u7565", "conclusion": "SerpentFlow\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u65e0\u76d1\u7763\u8de8\u57df\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1"}}
{"id": "2601.02022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02022", "abs": "https://arxiv.org/abs/2601.02022", "authors": ["Yifan Zhu", "John C. Duchi", "Benjamin Van Roy"], "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit", "comment": null, "summary": "We prove that Thompson sampling exhibits $\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(\u03bc_0, \u03a3_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $\u03c3^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$ decouples additively from the minimax (long run) regret $\u03c3d \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u8d4c\u535a\u673a\u4e2d\u5177\u6709$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\u4e0a\u754c\uff0c\u5176\u4e2d\u5148\u9a8c\u4f9d\u8d56\u7684\"\u9884\u70ed\"\u9879\u4e0e\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879\u5448\u52a0\u6027\u800c\u975e\u4e58\u6027\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u9057\u61be\u754c\u901a\u5e38\u5305\u542b\u5148\u9a8c\u5206\u5e03\u53c2\u6570\u4e0e\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879\u7684\u4e58\u79ef\u5173\u7cfb\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7406\u8bba\u5206\u6790\u8fc7\u4e8e\u60b2\u89c2\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e24\u4e2a\u9879\u5b9e\u9645\u4e0a\u53ef\u4ee5\u89e3\u8026\u4e3a\u52a0\u6027\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u9057\u61be\u5206\u6790\u3002", "method": "\u901a\u8fc7\u65b0\u7684\"\u692d\u5706\u52bf\u80fd\"\u5f15\u7406\u6765\u5206\u6790Thompson\u91c7\u6837\u7684\u6027\u80fd\uff0c\u8be5\u5f15\u7406\u80fd\u591f\u66f4\u7cbe\u7ec6\u5730\u5904\u7406\u5148\u9a8c\u534f\u65b9\u5dee\u77e9\u9635\u7684\u5f71\u54cd\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0b\u754c\u8bc1\u660e\u6765\u8868\u660e\u9884\u70ed\u9879\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002", "result": "\u8bc1\u660e\u4e86Thompson\u91c7\u6837\u5177\u6709$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\u4e0a\u754c\uff0c\u5176\u4e2d$\u03c3d \\sqrt{T}$\u662f\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879\uff0c$d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$\u662f\u5148\u9a8c\u4f9d\u8d56\u7684\u9884\u70ed\u9879\uff0c\u4e24\u8005\u5448\u52a0\u6027\u5173\u7cfb\u3002\u4e0b\u754c\u5206\u6790\u8868\u660e\u9884\u70ed\u9879\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u9057\u61be\u53ef\u4ee5\u5206\u89e3\u4e3a\u52a0\u6027\u7684\u6781\u5c0f\u6781\u5927\u9879\u548c\u5148\u9a8c\u4f9d\u8d56\u9879\uff0c\u8fd9\u6bd4\u73b0\u6709\u7684\u4e58\u79ef\u5173\u7cfb\u66f4\u7cbe\u786e\u5730\u63cf\u8ff0\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u65b0\u7684\u692d\u5706\u52bf\u80fd\u5f15\u7406\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u5de5\u5177\u3002"}}
{"id": "2601.02031", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02031", "abs": "https://arxiv.org/abs/2601.02031", "authors": ["Felix Stollenwerk", "Anna Lokrantz", "Niclas Hertzberg"], "title": "Output Embedding Centering for Stable LLM Pretraining", "comment": "11 pages, 5 figures", "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called \u03bc-centering, or a regularization method called \u03bc-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that \u03bc-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.", "AI": {"tldr": "\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316(OEC)\u65b9\u6cd5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u8f93\u51falogit\u53d1\u6563\u95ee\u9898\uff0c\u5305\u62ec\u786e\u5b9a\u6027\u03bc-centering\u548c\u6b63\u5219\u5316\u03bc-loss\u4e24\u79cd\u5b9e\u73b0\uff0c\u76f8\u6bd4\u73b0\u6709z-loss\u65b9\u6cd5\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u7387\u654f\u611f\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e0d\u4ec5\u6602\u8d35\uff0c\u800c\u4e14\u5bb9\u6613\u51fa\u73b0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u5728\u8bad\u7ec3\u540e\u671f\u4f7f\u7528\u5927\u5b66\u4e60\u7387\u65f6\u7ecf\u5e38\u51fa\u73b0\u7684\u8f93\u51falogit\u53d1\u6563\u95ee\u9898\uff0c\u73b0\u6709\u6700\u5e38\u7528\u7684\u7f13\u89e3\u7b56\u7565z-loss\u53ea\u662f\u6cbb\u6807\u4e0d\u6cbb\u672c\uff0c\u6ca1\u6709\u89e3\u51b3\u6839\u672c\u539f\u56e0\u3002", "method": "\u4ece\u8f93\u51fa\u5d4c\u5165\u51e0\u4f55\u7684\u89d2\u5ea6\u5206\u6790\u4e0d\u7a33\u5b9a\u6027\uff0c\u8bc6\u522b\u5176\u6839\u672c\u539f\u56e0\u3002\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316(OEC)\u4f5c\u4e3a\u65b0\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u8bc1\u660e\u5176\u80fd\u6291\u5236\u8f93\u51falogit\u53d1\u6563\u3002OEC\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a\u786e\u5b9a\u6027\u64cd\u4f5c\u03bc-centering\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u03bc-loss\u3002", "result": "\u4e24\u79cdOEC\u53d8\u4f53\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u7387\u654f\u611f\u6027\u65b9\u9762\u90fd\u4f18\u4e8ez-loss\u3002\u7279\u522b\u662f\uff0c\u5373\u4f7f\u5728z-loss\u5931\u8d25\u7684\u5927\u5b66\u4e60\u7387\u60c5\u51b5\u4e0b\uff0cOEC\u4e5f\u80fd\u786e\u4fdd\u8bad\u7ec3\u6536\u655b\u3002\u6b64\u5916\uff0c\u03bc-loss\u5bf9\u6b63\u5219\u5316\u8d85\u53c2\u6570\u8c03\u6574\u7684\u654f\u611f\u6027\u663e\u8457\u4f4e\u4e8ez-loss\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u8f93\u51fa\u5d4c\u5165\u51e0\u4f55\u8bc6\u522b\u4e86\u8f93\u51falogit\u53d1\u6563\u7684\u6839\u672c\u539f\u56e0\uff0c\u63d0\u51fa\u7684OEC\u65b9\u6cd5\uff08\u03bc-centering\u548c\u03bc-loss\uff09\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709z-loss\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.02036", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02036", "abs": "https://arxiv.org/abs/2601.02036", "authors": ["Yiyang Wang", "Xi Chen", "Xiaogang Xu", "Yu Liu", "Hengshuang Zhao"], "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models", "comment": null, "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.", "AI": {"tldr": "\u63d0\u51faGDRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7fa4\u4f53\u7ea7\u79bb\u7ebf\u4f18\u5316\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u9677\u9631\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5bf9\u9f50\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u7b49\u95ee\u9898\uff0c\u9700\u8981\u9488\u5bf9\u6574\u6d41\u6d41\u6a21\u578b\u7684\u7279\u6027\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7fa4\u4f53\u7ea7\u76f4\u63a5\u5956\u52b1\u4f18\u5316(GDRO)\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u6574\u6d41\u6d41\u6a21\u578b\u7279\u6027\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u652f\u6301\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u65e0\u9700\u56fe\u50cf\u91c7\u6837\uff0c\u4e14\u72ec\u7acb\u4e8e\u6269\u6563\u91c7\u6837\u5668\uff0c\u907f\u514d\u4e86ODE\u5230SDE\u7684\u8fd1\u4f3c\u9700\u6c42\u3002", "result": "GDRO\u5728OCR\u548cGenEval\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5206\u6570\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "GDRO\u4e3a\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2601.02037", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.02037", "abs": "https://arxiv.org/abs/2601.02037", "authors": ["Wei Hu", "Zewei Yu", "Jianqiu Xu"], "title": "Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling", "comment": null, "summary": "Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.", "AI": {"tldr": "DMPEAD\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u52a8\u6001\u6a21\u578b\u6c60\u4e0e\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u6a21\u578b\u6c60\u3001\u52a8\u6001\u66f4\u65b0\u548c\u96c6\u6210\u6392\u540d\u9760\u524d\u6a21\u578b\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u4e14\u5bf9\u7b56\u7565\u654f\u611f\uff1b\u96c6\u6210\u65b9\u6cd5\u8981\u4e48\u7ec4\u5408\u6240\u6709\u6a21\u578b\u8981\u4e48\u4ec5\u9650\u4e8e\u5355\u53d8\u91cf\u6570\u636e\uff1b\u5927\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6570\u636e\u7ef4\u5ea6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDMPEAD\u6846\u67b6\uff1a1) \u901a\u8fc7\u53c2\u6570\u8fc1\u79fb\u548c\u591a\u6837\u6027\u5ea6\u91cf\u6784\u5efa\u591a\u6837\u5316\u6a21\u578b\u6c60\uff1b2) \u4f7f\u7528\u5143\u6a21\u578b\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7b56\u7565\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u6c60\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6269\u5c55\u3001\u5b50\u96c6\u9009\u62e9\u548c\u6c60\u5408\u5e76\uff1b3) \u901a\u8fc7\u4ee3\u7406\u5ea6\u91cf\u6392\u540d\u548ctop-k\u805a\u5408\u5728\u9009\u5b9a\u5b50\u96c6\u4e2d\u96c6\u6210\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DMPEAD\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u6c60\u6784\u5efa\u548c\u96c6\u6210\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.02050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02050", "abs": "https://arxiv.org/abs/2601.02050", "authors": ["Yanhai Gan", "Yipeng Chen", "Ning Li", "Xingguo Liu", "Junyu Dong", "Xianyao Chen"], "title": "Explore the Ideology of Deep Learning in ENSO Forecasts", "comment": "5 figures. Code available at https://github.com/liuxingguo9349/pptv-enso-env", "summary": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u51fd\u6570\u9971\u548c\u533a\"\u62ef\u6551\"\u6b7b\u4ea1\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u63ed\u793a\u4e86ENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u7684\u6210\u56e0\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u4e86ENSO\u9884\u6d4b\u6280\u80fd\uff0c\u4f46\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u79d1\u5b66\u4fe1\u4efb\u548c\u4e1a\u52a1\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u6570\u5b66\u57fa\u7840\u624e\u5b9e\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u6765\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6fc0\u6d3b\u51fd\u6570\u9971\u548c\u533a\"\u62ef\u6551\"\u6b7b\u4ea1\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "result": "\u5206\u6790\u663e\u793aENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5370\u5ea6\u6d0b\u548c\u5927\u897f\u6d0b\u4e5f\u6709\u8d21\u732e\uff0c\u8fd9\u4e0e\u7269\u7406\u7406\u89e3\u4e00\u81f4\u3002\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1\u6625\u5b63\u654f\u611f\u6027\u6269\u5927\uff0c\u4f46\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u53d8\u91cf\u9009\u62e9\u4e0d\u4f18\u9020\u6210\u7684\u3002", "conclusion": "\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u53ef\u80fd\u6e90\u4e8e\u6b21\u4f18\u7684\u53d8\u91cf\u9009\u62e9\uff0c\u5efa\u8bae\u7eb3\u5165\u66f4\u591a\u6d77\u6d0b-\u5927\u6c14\u53d8\u91cf\u53ef\u80fd\u6709\u52a9\u4e8e\u7a81\u7834SPB\u9650\u5236\uff0c\u63a8\u8fdb\u957f\u671fENSO\u9884\u6d4b\u3002"}}
{"id": "2601.02080", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02080", "abs": "https://arxiv.org/abs/2601.02080", "authors": ["Yizhi Liu"], "title": "The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks", "comment": null, "summary": "Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value \u03c3_2 and filtering out high-frequency feature components. We derive a spectral bound linking \u03c3_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u53cc\u968f\u673a\u77e9\u9635\u7ea6\u675f\u5728\u6df1\u5ea6\u67b6\u6784\u4e2d\u7684\u8c31\u9000\u5316\u73b0\u8c61\uff08\u540c\u8d28\u6027\u9677\u9631\uff09\uff0c\u8868\u660e\u9ad8\u71b5\u7ea6\u675f\u4f1a\u6291\u5236\u6b21\u4e3b\u5bfc\u5947\u5f02\u503c\uff0c\u9650\u5236\u7279\u5f81\u53d8\u6362\u7684\u6709\u6548\u6df1\u5ea6\uff0c\u5bfc\u81f4\u51e0\u4f55\u7ed3\u6784\u5728\u566a\u58f0\u4e3b\u5bfc\u65f6\u4e0d\u53ef\u9006\u4e22\u5931\u3002", "motivation": "\u53cc\u968f\u673a\u77e9\u9635\u5728\u7ed3\u6784\u4fdd\u6301\u7684\u6df1\u5ea6\u67b6\u6784\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u7684\u8c31\u9000\u5316\u95ee\u9898\u3002\u7814\u7a76\u8005\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u7f51\u7edc\u7684\u8c31\u7279\u6027\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u71b5\u7ea6\u675f\u4e0b\u53ef\u80fd\u5bfc\u81f4\u7684\u7279\u5f81\u9000\u5316\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53cc\u968f\u673a\u77e9\u9635\u7684\u8c31\u7279\u6027\uff0c\u63a8\u5bfc\u4e86\u6b21\u4e3b\u5bfc\u5947\u5f02\u503c\u03c3_2\u4e0e\u7f51\u7edc\u6709\u6548\u6df1\u5ea6\u7684\u8c31\u754c\uff0c\u5206\u6790\u4e86\u6700\u5927\u71b5\u504f\u7f6e\u5982\u4f55\u9a71\u52a8\u6df7\u5408\u7b97\u5b50\u8d8b\u5411\u5747\u5300\u91cd\u5fc3\uff0c\u5e76\u5f62\u5f0f\u5316\u8bc1\u660e\u4e86\u5c42\u5f52\u4e00\u5316\u5728\u566a\u58f0\u4e3b\u5bfc\u673a\u5236\u4e0b\u7684\u5931\u6548\u6761\u4ef6\u3002", "result": "\u53d1\u73b0\u9ad8\u71b5\u7ea6\u675f\u4f1a\u6291\u5236\u6b21\u4e3b\u5bfc\u5947\u5f02\u503c\uff0c\u9650\u5236\u7279\u5f81\u53d8\u6362\u7684\u6709\u6548\u611f\u53d7\u91ce\uff1b\u5f53\u4fe1\u566a\u6bd4\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\uff0c\u51e0\u4f55\u7ed3\u6784\u4f1a\u4e0d\u53ef\u9006\u5730\u4e22\u5931\u5230\u566a\u58f0\u8bf1\u5bfc\u7684\u6b63\u4ea4\u5d29\u6e83\u4e2d\uff1b\u5c42\u5f52\u4e00\u5316\u65e0\u6cd5\u7f13\u89e3\u8fd9\u79cd\u5d29\u6e83\u3002", "conclusion": "\u53cc\u968f\u673a\u77e9\u9635\u7ea6\u675f\u7f51\u7edc\u5b58\u5728\u71b5\u7a33\u5b9a\u6027\u4e0e\u8c31\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u9ad8\u71b5\u7ea6\u675f\u867d\u7136\u63d0\u4f9b\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6982\u7387\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4f1a\u727a\u7272\u7f51\u7edc\u7684\u8c31\u8868\u8fbe\u80fd\u529b\u548c\u6df1\u5c42\u7279\u5f81\u53d8\u6362\u80fd\u529b\u3002"}}
{"id": "2601.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02081", "abs": "https://arxiv.org/abs/2601.02081", "authors": ["Jiacheng Lyu", "Bihua Bao"], "title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling", "comment": "14 pages", "summary": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u8f6f\u9009\u62e9\u4e0b\u91c7\u6837\u6846\u67b6\uff0c\u5c06\u6570\u636e\u7f29\u51cf\u91cd\u6784\u4e3a\u53ef\u5fae\u5206\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u5668\u7f51\u7edc\u548c\u4efb\u52a1\u7f51\u7edc\u7684\u5bf9\u6297\u535a\u5f08\u5b66\u4e60\u6837\u672c\u91cd\u8981\u6027\u6743\u91cd\uff0c\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684\u667a\u80fd\u6570\u636e\u4e0b\u91c7\u6837\u3002", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8bad\u7ec3\u5e26\u6765\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u6570\u636e\u4e0b\u91c7\u6837\u65b9\u6cd5\u4f5c\u4e3a\u9759\u6001\u3001\u4efb\u52a1\u65e0\u5173\u7684\u9884\u5904\u7406\u6b65\u9aa4\u901a\u5e38\u4f1a\u4e22\u5f03\u5bf9\u4e0b\u6e38\u9884\u6d4b\u5173\u952e\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u8f6f\u9009\u62e9\u4e0b\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u5668\u7f51\u7edc\u548c\u4efb\u52a1\u7f51\u7edc\u7684\u5bf9\u6297\u535a\u5f08\uff0c\u9009\u62e9\u5668\u7f51\u7edc\u5b66\u4e60\u4e3a\u6837\u672c\u5206\u914d\u8fde\u7eed\u91cd\u8981\u6027\u6743\u91cd\uff0c\u4f7f\u7528Gumbel-Softmax\u677e\u5f1b\u5b9e\u73b0\u76f4\u63a5\u4f18\u5316\uff0c\u5728\u5e73\u8861\u9884\u6d4b\u4fdd\u771f\u5ea6\u548c\u7a00\u758f\u6027\u7684\u635f\u5931\u51fd\u6570\u6307\u5bfc\u4e0b\u8bc6\u522b\u548c\u4fdd\u7559\u5bf9\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u3002", "result": "\u5728\u56db\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cASSS\u59cb\u7ec8\u4f18\u4e8e\u805a\u7c7b\u548c\u6700\u8fd1\u90bb\u7ec6\u5316\u7b49\u542f\u53d1\u5f0f\u4e0b\u91c7\u6837\u57fa\u7ebf\uff0c\u4e0d\u4ec5\u80fd\u5339\u914d\u6709\u65f6\u751a\u81f3\u8d85\u8fc7\u4f7f\u7528\u6574\u4e2a\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u53bb\u566a\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u4efb\u52a1\u611f\u77e5\u6570\u636e\u4e0b\u91c7\u6837\u786e\u7acb\u4e3a\u53ef\u5b66\u4e60\u7ec4\u4ef6\uff0c\u4e3a\u6709\u6548\u7684\u5927\u89c4\u6a21\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02094", "categories": ["cs.LG", "math.FA"], "pdf": "https://arxiv.org/pdf/2601.02094", "abs": "https://arxiv.org/abs/2601.02094", "authors": ["Hans Krupakar", "V A Kandappan"], "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting", "comment": null, "summary": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHAM\uff08Horizon Activation Mapping\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\uff0c\u901a\u8fc7\u68af\u5ea6\u8303\u6570\u5e73\u5747\u5206\u6790\u4e0d\u540c\u65f6\u95f4\u5b50\u5e8f\u5217\u7684\u91cd\u8981\u6027\uff0c\u9002\u7528\u4e8e\u8de8\u6a21\u578b\u5bb6\u65cf\u7684\u6a21\u578b\u9009\u62e9\u548c\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u4f9d\u8d56\u8bef\u5dee\u6307\u6807\u548c\u7279\u5b9a\u67b6\u6784\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u8de8\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\u6765\u7406\u89e3\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5b50\u5e8f\u5217\u4e0a\u7684\u5173\u6ce8\u6a21\u5f0f\u3002", "method": "\u63d0\u51faHAM\u6280\u672f\uff0c\u53d7grad-CAM\u542f\u53d1\uff0c\u4f7f\u7528\u68af\u5ea6\u8303\u6570\u5e73\u5747\u6765\u7814\u7a76\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4e0d\u540c\u65f6\u95f4\u5b50\u5e8f\u5217\u7684\u91cd\u8981\u6027\u3002\u5f15\u5165\u56e0\u679c\u548c\u53cd\u56e0\u679c\u6a21\u5f0f\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u68af\u5ea6\u66f4\u65b0\u8303\u6570\u5e73\u5747\uff0c\u4ee5\u53ca\u8868\u793a\u8303\u6570\u5e73\u5747\u5747\u5300\u5206\u5e03\u7684\u6bd4\u4f8b\u7ebf\u3002\u5728ETTm2\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ecMLP-based\u3001\u81ea\u6ce8\u610f\u529b\u3001SSM\u548c\u6269\u6563\u6a21\u578b\u3002", "result": "HAM\u80fd\u591f\u53ef\u89c6\u5316\u4e0d\u540c\u6a21\u578b\u5728\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002\u53d1\u73b0\u6279\u6b21\u5927\u5c0f\u5dee\u5f02\u7684\u6d3b\u52a8\u53ef\u80fd\u8868\u660e\u5b58\u5728\u6307\u6570\u8fd1\u4f3c\u5173\u7cfb\u3002NHITS\u7684\u795e\u7ecf\u8fd1\u4f3c\u5b9a\u7406\u548cSpaceTime\u7684\u6307\u6570\u81ea\u56de\u5f52\u6d3b\u52a8\u5728HAM\u56fe\u4e2d\u5f97\u5230\u4f53\u73b0\u3002HAM\u53ef\u7528\u4e8e\u7ec6\u7c92\u5ea6\u6a21\u578b\u9009\u62e9\u3001\u9a8c\u8bc1\u96c6\u9009\u62e9\u548c\u8de8\u6a21\u578b\u5bb6\u65cf\u6bd4\u8f83\u3002", "conclusion": "HAM\u662f\u4e00\u79cd\u6709\u6548\u7684\u8de8\u6a21\u578b\u5bb6\u65cf\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\uff0c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7406\u89e3\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u3001\u9a8c\u8bc1\u96c6\u8bbe\u8ba1\u548c\u8de8\u6a21\u578b\u6bd4\u8f83\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2601.02105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02105", "abs": "https://arxiv.org/abs/2601.02105", "authors": ["Hyunjun Kim"], "title": "LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training", "comment": null, "summary": "Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.\n  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.\n  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.\n  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.", "AI": {"tldr": "\u63d0\u51faLION-DG\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u9488\u5bf9\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u7684\u8f85\u52a9\u5206\u7c7b\u5668\u8fdb\u884c\u5c42\u611f\u77e5\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u68af\u5ea6\u5524\u9192\u673a\u5236\u5b9e\u73b0\u65e0\u8d85\u53c2\u6570\u9690\u5f0f\u9884\u70ed\uff0c\u52a0\u901f\u6536\u655b", "motivation": "\u73b0\u6709\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\u5927\u591a\u662f\u5c42\u65e0\u5173\u7684\uff0c\u800c\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u4e2d\u7684\u672a\u8bad\u7ec3\u8f85\u52a9\u5206\u7c7b\u5668\u5934\u4f1a\u901a\u8fc7\u68af\u5ea6\u5e72\u6270\u7834\u574f\u65e9\u671f\u8bad\u7ec3\u7a33\u5b9a\u6027", "method": "\u63d0\u51faLION-DG\u5c42\u611f\u77e5\u521d\u59cb\u5316\uff1a\u5bf9\u8f85\u52a9\u5206\u7c7b\u5668\u5934\u8fdb\u884c\u96f6\u521d\u59cb\u5316\uff0c\u5bf9\u4e3b\u5e72\u7f51\u7edc\u5e94\u7528\u6807\u51c6He\u521d\u59cb\u5316\uff0c\u5b9e\u73b0\u68af\u5ea6\u5524\u9192\u673a\u5236", "result": "\u5728CIFAR-10/100\u4e0a\u6d4b\u8bd5DenseNet-DS\u548cResNet-DS\uff1aDenseNet-DS\u6536\u655b\u901f\u5ea6\u63d0\u53478.3%\uff1bLSUV\u4e0eLION-DG\u7ed3\u5408\u8fbe\u523081.92%\u6700\u4f73\u51c6\u786e\u7387\uff1bResNet-DS\u5728CIFAR-100\u4e0a\u52a0\u901f11.3%", "conclusion": "LION-DG\u65b9\u6cd5\u7b80\u5355\u3001\u65e0\u9700\u8d85\u53c2\u6570\u3001\u65e0\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u6df1\u5ea6\u76d1\u7763\u67b6\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u521d\u59cb\u5316\u89e3\u51b3\u65b9\u6848\u548c\u5b9e\u7528\u6307\u5357"}}
{"id": "2601.02106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02106", "abs": "https://arxiv.org/abs/2601.02106", "authors": ["Ashish Rana", "Ammar Shaker", "Sascha Saralajew", "Takashi Suzuki", "Kosuke Yasuda", "Shintaro Kato", "Toshikazu Wada", "Toshiyuki Fujikawa", "Toru Kikutsuji"], "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI", "comment": "Accepted to the Demo Track at the IEEE International Conference on Data Mining (ICDM) 2025, where it received the Best Demo Award", "summary": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.", "AI": {"tldr": "ProtoPal\u6846\u67b6\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u5b9e\u73b0\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\uff0c\u63d0\u4f9b\u53ef\u7406\u89e3\u548c\u53ef\u9a8c\u8bc1\u7684\u9884\u6d4b\u4e0e\u5e72\u9884\uff0c\u5177\u6709\u524d\u540e\u7aef\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u5c55\u793a\u76f4\u89c2\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u4e2d\u5b58\u5728\u4e0d\u8db3\uff1a\u9884\u6d4b\u3001\u5e72\u9884\u548c\u63a8\u8350\u9700\u8981\u5bf9\u6240\u6709\u533b\u7597\u5229\u76ca\u76f8\u5173\u8005\u6765\u8bf4\u90fd\u662f\u53ef\u7406\u89e3\u548c\u53ef\u9a8c\u8bc1\u7684\u3002", "method": "\u63d0\u51faProtoPal\u6846\u67b6\uff0c\u91c7\u7528\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u524d\u7aef\u548c\u540e\u7aef\u4e24\u79cd\u6a21\u5f0f\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u5e72\u9884\u5c55\u793a\u548c\u6a21\u62df\u7ed3\u679c\u3002", "result": "\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5b9a\u91cf\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c\u7684\u76f4\u89c2\u5448\u73b0\u3002", "conclusion": "\u539f\u578b\u5b66\u4e60\u80fd\u591f\u6ee1\u8db3\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u4e2d\u5bf9\u53ef\u7406\u89e3\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u9700\u6c42\uff0cProtoPal\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02138", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.02138", "abs": "https://arxiv.org/abs/2601.02138", "authors": ["Weisen Yang", "Hanqing Zhang", "Wangren Qiu", "Xuan Xiao", "Weizhong Lin"], "title": "Edge-aware GAT-based protein binding site prediction", "comment": "24 pages, 10 figures, 6 tables", "summary": "Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.", "AI": {"tldr": "\u63d0\u51faEdge-aware GAT\u6a21\u578b\uff0c\u901a\u8fc7\u539f\u5b50\u7ea7\u56fe\u7ed3\u6784\u548c\u591a\u7ef4\u7279\u5f81\u96c6\u6210\uff0c\u5b9e\u73b0\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u7684\u7ec6\u7c92\u5ea6\u9884\u6d4b\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.93 ROC-AUC\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u7684\u51c6\u786e\u8bc6\u522b\u5bf9\u7406\u89e3\u751f\u7269\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u548c\u836f\u7269\u9776\u70b9\u7406\u6027\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u5728\u6355\u6349\u590d\u6742\u7a7a\u95f4\u6784\u8c61\u65f6\u96be\u4ee5\u5e73\u8861\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faEdge-aware Graph Attention Network\u6a21\u578b\uff0c\u6784\u5efa\u539f\u5b50\u7ea7\u56fe\u5e76\u96c6\u6210\u51e0\u4f55\u63cf\u8ff0\u7b26\u3001DSSP\u4e8c\u7ea7\u7ed3\u6784\u548c\u76f8\u5bf9\u6eb6\u5242\u53ef\u53ca\u6027\u7b49\u591a\u7ef4\u7ed3\u6784\u7279\u5f81\u3002\u901a\u8fc7\u5c06\u539f\u5b50\u95f4\u8ddd\u79bb\u548c\u65b9\u5411\u5411\u91cf\u4f5c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u8fb9\u7279\u5f81\uff0c\u589e\u5f3a\u6a21\u578b\u8868\u793a\u80fd\u529b\u3002\u91c7\u7528\u65b9\u5411\u5f20\u91cf\u4f20\u64ad\u548c\u6b8b\u57fa\u7ea7\u6ce8\u610f\u529b\u6c60\u5316\u6280\u672f\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u4e2d\u8fbe\u52300.93 ROC-AUC\uff0c\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u3002PyMOL\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5df2\u90e8\u7f72\u516c\u5f00\u53ef\u8bbf\u95ee\u7684Web\u670d\u52a1\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u86cb\u767d\u8d28\u529f\u80fd\u4f4d\u70b9\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u9896\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.02151", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02151", "abs": "https://arxiv.org/abs/2601.02151", "authors": ["Muxi Diao", "Lele Yang", "Wuxuan Gong", "Yutong Zhang", "Zhonghao Yan", "Yufei Han", "Kongming Liang", "Weiran Xu", "Zhanyu Ma"], "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "AI": {"tldr": "\u63d0\u51faEAFT\u65b9\u6cd5\uff0c\u5229\u7528token\u7ea7\u71b5\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\uff0c\u533a\u5206\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\uff0c\u5728\u4fdd\u6301\u4e0b\u6e38\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u7f13\u89e3\u901a\u7528\u80fd\u529b\u9000\u5316", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60(RL)\u80fd\u6709\u6548\u4fdd\u7559\u901a\u7528\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u6839\u672c\u539f\u56e0\u5728\u4e8e\u5206\u5e03\u5dee\u5f02\uff1aRL\u4e0e\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u5bf9\u9f50\uff0c\u800cSFT\u5f3a\u5236\u6a21\u578b\u62df\u5408\u5916\u90e8\u76d1\u7763\uff0c\u5bfc\u81f4\"\u81ea\u4fe1\u51b2\u7a81\"token\uff08\u4f4e\u6982\u7387\u4f46\u4f4e\u71b5\uff09\u5f15\u53d1\u7834\u574f\u6027\u68af\u5ea6\u66f4\u65b0", "method": "\u63d0\u51fa\u71b5\u81ea\u9002\u5e94\u5fae\u8c03(EAFT)\uff0c\u4e0d\u540c\u4e8e\u4ec5\u4f9d\u8d56\u9884\u6d4b\u6982\u7387\u7684\u65b9\u6cd5\uff0c\u5229\u7528token\u7ea7\u71b5\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\u6765\u533a\u5206\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002\u6a21\u578b\u4ece\u4e0d\u786e\u4fe1\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u540c\u65f6\u6291\u5236\u51b2\u7a81\u6570\u636e\u7684\u68af\u5ea6", "result": "\u5728Qwen\u548cGLM\u7cfb\u5217\u6a21\u578b\uff084B\u523032B\u53c2\u6570\uff09\u4e0a\uff0c\u5728\u6570\u5b66\u3001\u533b\u7597\u548c\u667a\u80fd\u4f53\u9886\u57df\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002EAFT\u5728\u4fdd\u6301\u6807\u51c6SFT\u4e0b\u6e38\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u901a\u7528\u80fd\u529b\u7684\u9000\u5316", "conclusion": "EAFT\u901a\u8fc7\u71b5\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u62a4\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\"\u81ea\u4fe1\u51b2\u7a81\"\u662f\u5bfc\u81f4\u9057\u5fd8\u7684\u5173\u952e\u56e0\u7d20"}}
{"id": "2601.02193", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02193", "abs": "https://arxiv.org/abs/2601.02193", "authors": ["Kasper Green Larsen", "Chirag Pabbaraju", "Abhishek Shetty"], "title": "Learning with Monotone Adversarial Corruptions", "comment": null, "summary": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5373\u4f7f\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u5bf9\u6297\u6027\u6570\u636e\u6c61\u67d3\uff0c\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e5f\u4f1a\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u7684\u53ef\u4ea4\u6362\u6027\u548c\u72ec\u7acb\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7a76\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u6570\u636e\u53ef\u4ea4\u6362\u6027\u548c\u72ec\u7acb\u6027\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u901a\u8fc7\u5f15\u5165\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\u6765\u6d4b\u8bd5\u7b97\u6cd5\u5728\u975e\u7406\u60f3\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\uff1a\u653b\u51fb\u8005\u5728\u67e5\u770b\"\u5e72\u51c0\"\u7684i.i.d.\u6570\u636e\u96c6\u540e\uff0c\u63d2\u5165\u81ea\u5df1\u9009\u62e9\u7684\"\u6c61\u67d3\"\u70b9\uff0c\u8fd9\u4e9b\u70b9\u88ab\u7ea6\u675f\u4e3a\u5355\u8c03\u6c61\u67d3\uff08\u5373\u6309\u7167\u771f\u5b9e\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u6807\u8bb0\uff09\u3002", "result": "\u6240\u6709\u5df2\u77e5\u7684\u4e8c\u5143\u5206\u7c7b\u6700\u4f18\u5b66\u4e60\u7b97\u6cd5\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u4f1a\u5728\u65b0\u72ec\u7acb\u6d4b\u8bd5\u70b9\u4e0a\u83b7\u5f97\u6b21\u4f18\u7684\u671f\u671b\u8bef\u5dee\uff1b\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u5219\u4e0d\u4f1a\u964d\u4f4e\u5176\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u6700\u4f18\u5b66\u4e60\u7b97\u6cd5\u5728\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u6c61\u67d3\u65f6\u4f1a\u5d29\u6e83\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u5bf9\u53ef\u4ea4\u6362\u6027\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.02196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02196", "abs": "https://arxiv.org/abs/2601.02196", "authors": ["Yu Li", "Sizhe Tang", "Rongqian Chen", "Fei Xu Yu", "Guangyu Jiang", "Mahdi Imani", "Nathaniel D. Bastian", "Tian Lan"], "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense", "comment": null, "summary": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728CAGE-4\u6311\u6218\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u9762\u4e34\u63a2\u7d22\u56f0\u96be\u3001\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u8bad\u7ec3\u6837\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u7814\u7a76\u8005\u5bfb\u6c42\u5f00\u53d1\u6837\u672c\u6548\u7387\u66f4\u9ad8\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u5c06\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u89c4\u5212\u4e2d\u5fc3\u9632\u5fa1\u7b56\u7565\u3002\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5c06\u7f51\u7edc\u89c2\u6d4b\u5d4c\u5165\u4e3a\u5c5e\u6027\u56fe\uff0c\u5b9e\u73b0\u4e3b\u673a\u53ca\u5176\u5173\u7cfb\u7684\u7f6e\u6362\u4e0d\u53d8\u63a8\u7406\u3002\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u56fe\u5d4c\u5165\u548c\u56fe\u7f16\u8f91\u52a8\u4f5c\u5148\u9a8c\u6765\u6307\u5bfcMCTS\uff0c\u878d\u5408\u65e0\u6a21\u578b\u6cdb\u5316\u3001\u7b56\u7565\u84b8\u998f\u548c\u524d\u5411\u89c4\u5212\u3002", "result": "\u5728CAGE-4\u6311\u6218\u7684\u5404\u79cd\u7f51\u7edc\u7ed3\u6784\u548c\u5bf9\u624b\u884c\u4e3a\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e\u641c\u7d22\u5f15\u5bfc\u548c\u56fe\u5d4c\u5165\u7684\u89c4\u5212\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9632\u5fa1\u5956\u52b1\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3001\u6027\u80fd\u66f4\u4f18\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u4e3a\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u5316\u9632\u5fa1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02201", "abs": "https://arxiv.org/abs/2601.02201", "authors": ["Keyu Wang", "Bingchen Miao", "Wendong Bu", "Yu Wu", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Jun Xiao", "Yueting Zhuang"], "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents", "comment": "19 pages, 12 figures", "summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.", "AI": {"tldr": "CORE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u7684\u9006\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u6269\u5c55\u8fde\u63a5\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u589e\u5f3a\u884c\u4e3a\u591a\u6837\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u8bad\u7ec3\u5b58\u5728\u4e24\u79cd\u4e3b\u6d41\u8303\u5f0f\u51b2\u7a81\uff1a\u884c\u4e3a\u514b\u9686\u7b80\u5355\u6709\u6548\u4f46\u884c\u4e3a\u591a\u6837\u6027\u4f4e\uff1b\u5f3a\u5316\u5b66\u4e60\u80fd\u53d1\u73b0\u65b0\u7b56\u7565\u4f46\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u51b2\u7a81\u3002", "method": "1. \u8bed\u4e49\u4ee3\u7801\u62bd\u8c61\uff1a\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff08\u6807\u7b7e\u51fd\u6570\uff09\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\uff1b2. \u7b56\u7565\u56fe\u6269\u5c55\uff1a\u6784\u5efa\u591a\u8def\u5f84\u7b56\u7565\u56fe\uff0c\u589e\u5f3a\u9886\u57df\u5185\u884c\u4e3a\u591a\u6837\u6027\uff1b3. \u8f68\u8ff9\u5f15\u5bfc\u5916\u63a8\uff1a\u5229\u7528\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\u6269\u5c55\u4efb\u52a1\u7a7a\u95f4\uff0c\u4e30\u5bcc\u9886\u57df\u5916\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728Web\u548cAndroid\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCORE\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u4f5c\u4e3a\u6784\u5efa\u5f3a\u5927\u865a\u62df\u4ee3\u7406\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u8bad\u7ec3\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "CORE\u901a\u8fc7\u8fde\u63a5\u6a21\u4eff\u5b66\u4e60\u548c\u63a2\u7d22\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u65e2\u4fc3\u8fdb\u4e86\u884c\u4e3a\u591a\u6837\u6027\uff0c\u53c8\u6d88\u9664\u4e86\u5bf9\u4eba\u5de5\u5956\u52b1\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u4e3a\u6784\u5efa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02213", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02213", "abs": "https://arxiv.org/abs/2601.02213", "authors": ["Haoyu Zhou", "Ping Xue", "Tianfan Fu", "Hao Zhang"], "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction", "comment": null, "summary": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u538b\u7f29\u548c\u52a0\u901fSO(3)-\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u6280\u672f\uff0c\u901a\u8fc7\u89e3\u8026\u91cf\u5316\u65b9\u6848\u3001\u5206\u652f\u5206\u79bb\u8bad\u7ec3\u7b56\u7565\u548c\u6ce8\u610f\u529b\u5f52\u4e00\u5316\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u548c\u7b49\u53d8\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5bf93D\u65cb\u8f6c\u7b49\u53d8\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u6311\u6218\uff0c\u9700\u8981\u538b\u7f29\u548c\u52a0\u901f\u8fd9\u4e9b\u6a21\u578b\u4ee5\u5728\u5b9e\u9645\u5316\u5b66\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\u6280\u672f\uff1a1) \u5e45\u5ea6-\u65b9\u5411\u89e3\u8026\u91cf\u5316\u65b9\u6848\uff0c\u5206\u522b\u91cf\u5316\u7b49\u53d8\u7279\u5f81\u7684\u8303\u6570\u548c\u65b9\u5411\uff1b2) \u5206\u652f\u5206\u79bb\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u6ce8\u610f\u529b\u57faSO(3)-GNN\u4e2d\u533a\u522b\u5904\u7406\u4e0d\u53d8\u548c\u7b49\u53d8\u7279\u5f81\u901a\u9053\uff1b3) \u9c81\u68d2\u6027\u589e\u5f3a\u7684\u6ce8\u610f\u529b\u5f52\u4e00\u5316\u673a\u5236\uff0c\u7a33\u5b9a\u4f4e\u7cbe\u5ea6\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728QM9\u548crMD17\u5206\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c8\u4f4d\u6a21\u578b\u5728\u80fd\u91cf\u548c\u529b\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.37-2.73\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c0f4\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7b49\u53d8\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u91cf\u5316\u6280\u672f\u80fd\u591f\u5728\u4fdd\u6301\u7cbe\u5ea6\u548c\u7269\u7406\u5bf9\u79f0\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347SO(3)-\u7b49\u53d8GNN\u7684\u6548\u7387\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9e\u9645\u5316\u5b66\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u4e3a\u5bf9\u79f0\u611f\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.02232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02232", "abs": "https://arxiv.org/abs/2601.02232", "authors": ["Shristi Das Biswas", "Yue Zhang", "Anwesan Pal", "Radhika Bhargava", "Kaushik Roy"], "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.", "AI": {"tldr": "ELLA\uff1a\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u6b63\u5411\u8fc1\u79fb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\u4e14\u4fb5\u72af\u9690\u79c1\uff0c\u800c\u4e25\u683c\u6b63\u4ea4\u6027\u65b9\u6cd5\u5728\u89c4\u6a21\u6269\u5c55\u65f6\u4f1a\u5d29\u6e83\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u90fd\u88ab\u6295\u5f71\u5230\u6b63\u4ea4\u8865\u7a7a\u95f4\uff0c\u9010\u6e10\u51cf\u5c11\u5269\u4f59\u81ea\u7531\u5ea6\u5e76\u7981\u6b62\u5171\u4eab\u8868\u793a\u7684\u91cd\u53e0\uff0c\u4ece\u800c\u6d88\u9664\u6b63\u5411\u8fc1\u79fb\u3002", "method": "ELLA\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u539f\u5219\uff0c\u901a\u8fc7\u660e\u786e\u8868\u5f81\u8fc7\u53bb\u66f4\u65b0\u7684\u7ed3\u6784\uff0c\u60e9\u7f5a\u6cbf\u9ad8\u80fd\u91cf\u3001\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u6b8b\u5dee\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u8fc1\u79fb\u3002\u8fd9\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668\u5728\u5355\u4e2a\u805a\u5408\u66f4\u65b0\u77e9\u9635\u4e0a\u5b9e\u73b0\uff0c\u5bf9\u5e94\u4e00\u4e2a\u5404\u5411\u5f02\u6027\u6536\u7f29\u7b97\u5b50\u6765\u9650\u5236\u5e72\u6270\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe9.6%\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1135\u500d\u3002\u65e0\u9700\u6570\u636e\u91cd\u653e\u3001\u67b6\u6784\u6269\u5c55\u548c\u53ef\u5ffd\u7565\u7684\u5b58\u50a8\u5f00\u9500\uff0c\u5e76\u80fd\u8de8\u67b6\u6784\u7a33\u5065\u6269\u5c55\uff0c\u4e3b\u52a8\u589e\u5f3a\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "ELLA\u4e3a\u6784\u5efa\u6027\u7ec8\u8eabLLM\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u6709\u6548\u5e73\u8861\u4e86\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u548c\u6b63\u5411\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7684\u6839\u672c\u9650\u5236\u3002"}}
{"id": "2601.02264", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02264", "abs": "https://arxiv.org/abs/2601.02264", "authors": ["Boris Kriuk", "Fedor Kriuk"], "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network", "comment": "8 pages, 14 figures", "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.", "AI": {"tldr": "POSEIDON\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5730\u9707\u4e8b\u4ef6\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86Gutenberg-Richter\u5b9a\u5f8b\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u7269\u7406\u7ea6\u675f\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5730\u9707\u9884\u6d4b\u548c\u5730\u9707\u5371\u9669\u6027\u8bc4\u4f30\u662f\u5730\u7403\u7269\u7406\u5b66\u7684\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f5c\u4e3a\u9ed1\u76d2\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u5df2\u5efa\u7acb\u7684\u7269\u7406\u5b9a\u5f8b\u3002\u9700\u8981\u5c06\u7269\u7406\u539f\u7406\u6574\u5408\u5230\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u4ee5\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faPOSEIDON\uff08\u7269\u7406\u4f18\u5316\u5730\u9707\u80fd\u91cf\u63a8\u65ad\u548c\u68c0\u6d4b\u64cd\u4f5c\u7f51\u7edc\uff09\uff0c\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u80fd\u91cf\u6a21\u578b\uff0c\u5c06Gutenberg-Richter\u9707\u7ea7-\u9891\u7387\u5173\u7cfb\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ea6\u675f\u5d4c\u5165\u5230\u80fd\u91cf\u5efa\u6a21\u6846\u67b6\u4e2d\u3002\u540c\u65f6\u5904\u7406\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9884\u6d4b\u4efb\u52a1\uff1a\u4f59\u9707\u5e8f\u5217\u8bc6\u522b\u3001\u6d77\u5578\u751f\u6210\u6f5c\u529b\u548c\u524d\u9707\u68c0\u6d4b\u3002", "result": "POSEIDON\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cCNN\u57fa\u7ebf\uff0c\u5728\u6240\u6709\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747F1\u5206\u6570\u3002\u5b66\u4e60\u5230\u7684\u7269\u7406\u53c2\u6570\u6536\u655b\u5230\u79d1\u5b66\u53ef\u89e3\u91ca\u7684\u503c\uff1aGutenberg-Richter b\u503c\u4e3a0.752\uff0cOmori-Utsu\u53c2\u6570p=0.835\uff0cc=0.1948\u5929\uff0c\u8fd9\u4e9b\u503c\u843d\u5728\u5df2\u5efa\u7acb\u7684\u5730\u9707\u5b66\u8303\u56f4\u5185\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "POSEIDON\u6210\u529f\u5730\u5c06\u7269\u7406\u5b9a\u5f8b\u6574\u5408\u5230\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u3002\u540c\u65f6\u53d1\u5e03\u4e86Poseidon\u6570\u636e\u96c6\u2014\u2014\u6700\u5927\u7684\u5f00\u6e90\u5168\u7403\u5730\u9707\u76ee\u5f55\uff0c\u5305\u542b280\u4e07\u6b21\u4e8b\u4ef6\uff0c\u8de8\u8d8a30\u5e74\uff0c\u4e3a\u7269\u7406\u4fe1\u606f\u5730\u9707\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8d44\u6e90\u3002"}}
{"id": "2601.02307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02307", "abs": "https://arxiv.org/abs/2601.02307", "authors": ["Dina El Zein", "James Henderson"], "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck", "comment": "11 pages, 2 figures", "summary": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with R\u00e9nyi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u975e\u53c2\u6570\u53d8\u5206\u5dee\u5206\u9690\u79c1\u7684\u6587\u672c\u6570\u636e\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728Transformer\u5d4c\u5165\u4e2d\u6ce8\u5165\u566a\u58f0\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "motivation": "Transformer\u9690\u85cf\u8868\u793a\u53ef\u80fd\u7f16\u7801\u654f\u611f\u4fe1\u606f\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u591f\u6062\u590d\u8f93\u5165\u6570\u636e\uff0c\u7279\u522b\u662f\u56e0\u4e3aTransformer\u5d4c\u5165\u5305\u542b\u6bcf\u4e2atoken\u7684\u591a\u4e2a\u5411\u91cf\uff0c\u52a0\u5267\u4e86\u9690\u79c1\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u975e\u53c2\u6570\u53d8\u5206\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\uff0c\u5728Transformer\u67b6\u6784\u4e2d\u96c6\u6210\u975e\u53c2\u6570\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u5c42\uff0c\u5411\u591a\u5411\u91cf\u5d4c\u5165\u6ce8\u5165\u566a\u58f0\uff0c\u4f7f\u7528R\u00e9nyi\u6563\u5ea6\u8861\u91cf\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u5dee\u5206\u9690\u79c1\u63d0\u4f9b\u4fdd\u8bc1\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u8c03\u6574\u566a\u58f0\u6c34\u5e73\u5b9e\u73b0\u4e86\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u7684\u6709\u6548\u6743\u8861\uff0c\u8f83\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u6a21\u578b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002", "conclusion": "NVDP\u65b9\u6cd5\u80fd\u591f\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5b9e\u7528\u6027\uff0c\u4e3a\u6587\u672c\u6570\u636e\u7684\u5b89\u5168\u5171\u4eab\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02310", "abs": "https://arxiv.org/abs/2601.02310", "authors": ["Ahmad Makinde"], "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay", "comment": "8 pages, 5 figures, Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests.Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests", "summary": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.", "AI": {"tldr": "\u4f7f\u7528T-KAN\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edfLSTM\u7684\u56fa\u5b9a\u7ebf\u6027\u6743\u91cd\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u5b66\u4e60\u5e02\u573a\u4fe1\u53f7\u7684\"\u5f62\u72b6\"\uff0c\u5728\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd", "motivation": "\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u4e2d\u7684\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u566a\u58f0\u5927\u3001\u975e\u7ebf\u6027\u5f3a\uff0c\u4f20\u7edf\u6a21\u578b\u5982DeepLOB\u968f\u7740\u65f6\u95f4\u8de8\u5ea6\u589e\u52a0\u9884\u6d4b\u80fd\u529b\u8870\u51cf\u4e25\u91cd\uff08alpha\u8870\u51cf\u95ee\u9898\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6a21\u578b\u6765\u5904\u7406\u5e02\u573a\u4fe1\u53f7\u7684\u590d\u6742\u6a21\u5f0f", "method": "\u63d0\u51faTemporal Kolmogorov-Arnold Networks (T-KAN)\uff0c\u7528\u53ef\u5b66\u4e60\u7684B\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u66ff\u4ee3\u6807\u51c6LSTM\u7684\u56fa\u5b9a\u7ebf\u6027\u6743\u91cd\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5e02\u573a\u4fe1\u53f7\u7684\"\u5f62\u72b6\"\u800c\u975e\u4ec5\u5e45\u5ea6\uff0c\u5e76\u9488\u5bf9\u4f4e\u5ef6\u8fdfFPGA\u5b9e\u73b0\u8fdb\u884c\u4f18\u5316", "result": "\u5728k=100\u65f6\u95f4\u8de8\u5ea6\u4e0a\u76f8\u5bf9F1\u5206\u6570\u63d0\u534719.1%\uff0c\u57281.0bps\u4ea4\u6613\u6210\u672c\u4e0b\u83b7\u5f97132.48%\u56de\u62a5\uff08\u76f8\u6bd4DeepLOB\u7684-82.76%\u56de\u64a4\uff09\uff0c\u6a21\u578b\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff08\u6837\u6761\u4e2d\u7684\"\u6b7b\u533a\"\u6e05\u6670\u53ef\u89c1\uff09", "conclusion": "T-KAN\u7f51\u7edc\u5728\u9ad8\u9891\u9650\u4ef7\u8ba2\u5355\u7c3f\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u786c\u4ef6\u4f18\u5316\u6f5c\u529b\uff0c\u4e3a\u9ad8\u9891\u4ea4\u6613\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02313", "abs": "https://arxiv.org/abs/2601.02313", "authors": ["Hanzaleh Akbari Nodehi", "Viveck R. Cadambe", "Mohammad Ali Maddah-Ali"], "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning", "comment": null, "summary": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\n  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u535a\u5f08\u8bba\u6846\u67b6\"\u7f16\u7801\u535a\u5f08\"\uff0c\u5c06\u7f16\u7801\u7406\u8bba\u6269\u5c55\u5230\u7406\u6027\u5bf9\u624b\u573a\u666f\uff0c\u5728\u8bda\u5b9e\u8282\u70b9\u4e0d\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u975e\u96f6\u6570\u636e\u6062\u590d\u6982\u7387\uff0c\u5e76\u5177\u6709Sybil\u6297\u6027\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\uff0c\u53c2\u4e0e\u8282\u70b9\u56e0\u8d21\u732e\u800c\u83b7\u5f97\u5956\u52b1\uff0c\u8fd9\u50ac\u751f\u4e86\u7406\u6027\u5bf9\u624b\u800c\u975e\u7eaf\u7cb9\u6076\u610f\u5bf9\u624b\u3002\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u5047\u8bbe\u6700\u574f\u60c5\u51b5\u5bf9\u6297\u6a21\u578b\uff0c\u8981\u6c42\u8bda\u5b9e\u8282\u70b9\u6570\u91cf\u8d85\u8fc7\u5bf9\u624b\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u8bda\u5b9e\u8282\u70b9\u53ef\u80fd\u4e0d\u5360\u591a\u6570\uff0c\u9700\u8981\u65b0\u7684\u7f16\u7801\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\"\u7f16\u7801\u535a\u5f08\"\u8fd9\u4e00\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u6269\u5c55\u7f16\u7801\u7406\u8bba\u5230\u4fe1\u4efb\u6700\u5c0f\u5316\u8bbe\u7f6e\u3002\u91cd\u70b9\u5173\u6ce8\u91cd\u590d\u7f16\u7801\uff0c\u5206\u6790\u7406\u6027\u5bf9\u624b\u7684\u6218\u7565\u884c\u4e3a\uff0c\u7814\u7a76\u5728\u5bf9\u624b\u5360\u591a\u6570\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u6570\u636e\u6062\u590d\u7684\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u8be5\u6846\u67b6\u5177\u6709Sybil\u6297\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\uff1a1\uff09\u5373\u4f7f\u5bf9\u624b\u8282\u70b9\u5360\u591a\u6570\uff0c\u4ecd\u80fd\u83b7\u5f97\u975e\u96f6\u7684\u6570\u636e\u6062\u590d\u6982\u7387\uff1b2\uff09Sybil\u6297\u6027\uff0c\u5373\u5747\u8861\u72b6\u6001\u4e0d\u968f\u5bf9\u624b\u8282\u70b9\u6570\u91cf\u589e\u52a0\u800c\u6539\u53d8\u3002\u4e3a\u7406\u6027\u5bf9\u624b\u573a\u666f\u4e0b\u7684\u7f16\u7801\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7f16\u7801\u535a\u5f08\u6846\u67b6\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u7406\u6027\u5bf9\u624b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u7684\u9650\u5236\u3002\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86\u5bf9\u624b\u7b56\u7565\u672a\u77e5\u7684\u60c5\u51b5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u82e5\u5e72\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2601.02316", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02316", "abs": "https://arxiv.org/abs/2601.02316", "authors": ["Siddharth Joshi", "Haoli Yin", "Rishabh Adiga", "Ricardo Monti", "Aldo Carranza", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Parth Doshi", "Paul Burstein", "Pratyush Maini", "Scott Loftin", "Spandan Das", "Tony Jiang", "Vineeth Dorna", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations", "comment": null, "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u4e2a\u6807\u51c6\uff1a\u5fe0\u5b9e\u6027\u3001\u53ef\u533a\u5206\u6027\u548c\u6548\u7387\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u591a\u9879\u7f3a\u9677\uff0c\u901a\u8fc7\u8f6c\u6362\u548c\u8fc7\u6ee4\u65b9\u6cd5\u6539\u8fdb\u8bc4\u4f30\u8d28\u91cf\uff0c\u53d1\u5e03\u6e05\u7406\u540e\u7684\u8bc4\u4f30\u5957\u4ef6\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u5de5\u4f5c\u4f17\u591a\uff0c\u4f46\u5176\u8bc4\u4f30\u65b9\u6cd5\u4ecd\u4e0d\u6210\u719f\u3002\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u591a\u79cd\u7f3a\u9677\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u8c28\u548c\u53ef\u6301\u7eed\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u5e94\u6ee1\u8db3\u7684\u4e09\u4e2a\u6807\u51c6\uff1a\u5fe0\u5b9e\u6027\u3001\u53ef\u533a\u5206\u6027\u548c\u6548\u7387\u3002\u8bc6\u522b\u73b0\u6709\u8bc4\u4f30\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u9009\u62e9\u9898\u683c\u5f0f\u95ee\u9898\u3001\u76f2\u76ee\u53ef\u89e3\u95ee\u9898\u548c\u9519\u8bef\u6807\u6ce8\u6837\u672c\u3002\u901a\u8fc7\u5c06\u9009\u62e9\u9898\u8f6c\u6362\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u8fc7\u6ee4\u76f2\u76ee\u53ef\u89e3\u548c\u9519\u8bef\u6807\u6ce8\u6837\u672c\u6765\u6539\u8fdb\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u9009\u62e9\u9898\u8f6c\u6362\u4e3a\u751f\u6210\u4efb\u52a1\u540e\u6a21\u578b\u80fd\u529b\u4e0b\u964d\u9ad8\u8fbe35%\u3002\u8fc7\u6ee4\u76f2\u76ee\u53ef\u89e3\u548c\u9519\u8bef\u6807\u6ce8\u6837\u672c\u63d0\u9ad8\u4e86\u533a\u5206\u80fd\u529b\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u53d1\u5e03DatBench-Full\uff0833\u4e2a\u6570\u636e\u96c6\uff09\u548cDatBench\uff08\u9ad8\u6548\u5b50\u96c6\uff09\uff0c\u540e\u8005\u5b9e\u73b013\u500d\u5e73\u5747\u52a0\u901f\uff08\u6700\u9ad850\u500d\uff09\u3002", "conclusion": "\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u548c\u53ef\u6301\u7eed\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u8f6c\u6362\u548c\u8fc7\u6ee4\u73b0\u6709\u57fa\u51c6\u6765\u63d0\u9ad8\u8bc4\u4f30\u7684\u5fe0\u5b9e\u6027\u548c\u533a\u5206\u80fd\u529b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2601.02360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02360", "abs": "https://arxiv.org/abs/2601.02360", "authors": ["Yazan Obeidi", "Amir Sarfi", "Joel Lidin", "Paul Janson", "Eugene Belilovsky"], "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs", "comment": null, "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.", "AI": {"tldr": "SparseLoCo\uff08\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\uff09\u4e0e\u4f4e\u5e26\u5bbd\u6d41\u6c34\u7ebf\u6a21\u578b\u5e76\u884c\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6fc0\u6d3b\u548c\u6fc0\u6d3b\u68af\u5ea6\u538b\u7f29\uff0c\u5b9e\u73b0\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u63d0\u9ad8LLM\u9884\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u4f46\u5e26\u5bbd\u9650\u5236\u4f7f\u5f97\u5728\u6570\u636e\u4e2d\u5fc3\u4e4b\u5916\u96be\u4ee5\u6269\u5c55\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u5e76\u884c\u9700\u8981\u9891\u7e41\u7684\u5927\u89c4\u6a21\u8bbe\u5907\u95f4\u901a\u4fe1\u65f6\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u4e0e\u4f4e\u5e26\u5bbd\u6d41\u6c34\u7ebf\u6a21\u578b\u5e76\u884c\u76f8\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\uff1a\u9ad8\u5e26\u5bbd\u53c2\u4e0e\u8005\u6258\u7ba1\u5b8c\u6574\u526f\u672c\uff0c\u8d44\u6e90\u53d7\u9650\u53c2\u4e0e\u8005\u5206\u7ec4\u4f7f\u7528\u6d41\u6c34\u7ebf\u5e76\u884c\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u8fdb\u884c\u9636\u6bb5\u95f4\u901a\u4fe1\u538b\u7f29\u3002\u5c06\u5b50\u7a7a\u95f4\u6d41\u6c34\u7ebf\u538b\u7f29\u4e0eSparseLoCo\u7ed3\u5408\uff0c\u7814\u7a76\u591a\u79cd\u9002\u914d\u65b9\u6848\u3002", "result": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\uff081.78\u4ebf-10\u4ebf\u53c2\u6570\uff09\u4e2d\uff0c\u6fc0\u6d3b\u538b\u7f29\u4e0eSparseLoCo\u7ed3\u5408\u6210\u672c\u9002\u4e2d\uff0c\u9009\u62e9\u6027\uff08\u5f02\u6784\uff09\u538b\u7f29\u76f8\u6bd4\u538b\u7f29\u6240\u6709\u526f\u672c\u80fd\u6301\u7eed\u6539\u5584\u635f\u5931-\u901a\u4fe1\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u4e00\u6761\u5c06\u4f4e\u5e26\u5bbd\u6a21\u578b\u5e76\u884c\u548c\u5f02\u6784\u53c2\u4e0e\u8005\u7eb3\u5165LLM\u9884\u8bad\u7ec3\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
