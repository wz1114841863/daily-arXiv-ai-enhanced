<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过降维技术提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制和MLP组件在中间层的分离模式，以及位置嵌入的高维螺旋结构等几何特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多自然语言任务中取得了最先进的结果，但其内部机制仍然难以解释。本研究旨在通过可视化潜在状态几何结构来支持对Transformer内部机制的系统分析。

Method: 在Transformer块中的多个点捕获逐层激活，通过主成分分析（PCA）和均匀流形逼近（UMAP）进行降维和可视化，在GPT-2和LLaMa模型上进行实验。

Result: 发现了注意力机制和MLP组件输出在中间层的明显分离模式，表征了初始序列位置潜在状态的高范数，可视化了潜在状态的逐层演化，展示了GPT-2位置嵌入的高维螺旋结构和LLaMa的序列级几何模式。

Conclusion: 该方法支持对Transformer内部机制的系统分析，有助于推进可复现的可解释性研究，为理解语言模型的内部工作机制提供了新的视角。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [2] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 研究发现算法效率提升主要依赖于计算规模，而非算法本身的小规模改进。LSTM到Transformer的转换贡献了大部分效率增益，而传统算法改进仅贡献不到100倍效率提升。


<details>
  <summary>Details</summary>
Motivation: 探究2012-2023年间AI训练FLOP效率提升22,000倍的真实原因，验证算法效率是否与计算规模相关。

Method: 进行小规模消融实验分析关键创新，进行扩展实验比较LSTM和Transformer在不同规模下的效率，使用实验外推和文献估计。

Result: 仅能解释不到100倍的效率提升，但考虑规模依赖效应后能解释6,930倍效率提升，其中LSTM到Transformer转换贡献主要部分。

Conclusion: 算法效率衡量具有强参考依赖性，小模型算法进展远慢于预期，效率增益与计算规模密切相关。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>
