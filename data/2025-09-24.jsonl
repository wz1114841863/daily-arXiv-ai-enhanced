{"id": "2509.16205", "categories": ["cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.16205", "abs": "https://arxiv.org/abs/2509.16205", "authors": ["Juhani Merilehto"], "title": "A 200-Line Python Micro-Benchmark Suite for NISQ Circuit Compilers", "comment": "9 pages, 1 figure. Includes reproducibility instructions and code\n  artifacts. Companion repository:\n  https://github.com/juhanimerilehto/microbench", "summary": "We present microbench.py, a compact (approx. 200 lines) Python script that\nautomates the collection of key compiler metrics, i.e., gate depth,\ntwo-qubit-gate count, wall-clock compilation time, and memory footprint, across\nmultiple open-source quantum circuit transpilers. The suite ships with six\ndidactic circuits (3 to 8 qubits) implementing fundamental quantum algorithms\nand supports Qiskit, tket, Cirq, and the Qiskit-Braket provider; in this paper\nwe showcase results for Qiskit 0.46 and Braket 1.16. The entire run completes\nin under three minutes on a laptop, emits a single CSV plus publisheable plot,\nand reproduces the figure here with one command. We release the code under the\nMIT licence to serve as a quick-start regression harness for NISQ compiler\nresearch."}
{"id": "2509.16213", "categories": ["cs.ET", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16213", "abs": "https://arxiv.org/abs/2509.16213", "authors": ["Xiaolei Zhu", "Xiaofei Jin", "Ziyang Kang", "Chonghui Sun", "Junjie Feng", "Dingwen Hu", "Zengyi Wang", "Hanyue Zhuang", "Qian Zheng", "Huajin Tang", "Shi Gu", "Xin Du", "De Ma", "Gang Pan"], "title": "DarwinWafer: A Wafer-Scale Neuromorphic Chip", "comment": null, "summary": "Neuromorphic computing promises brain-like efficiency, yet today's multi-chip\nsystems scale over PCBs and incur orders-of-magnitude penalties in bandwidth,\nlatency, and energy, undermining biological algorithms and system efficiency.\nWe present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip\ninterconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets\non a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based\nasynchronous wafer fabric with hierarchical time-step synchronization provide\nlow-latency, coherent operation across the wafer. Each chiplet implements 2.35\nM neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per\nwafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9\npJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by\na holistic chiplet-interposer co-design flow (including an in-house\ninterposer-bump planner with early SI/PI and electro-thermal closure) and a\nwarpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin\nconnections, enabling robust, demountable wafer-to-board integration.\nMeasurements confirm 10 mV supply droop and a uniform thermal profile (34-36\n{\\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations:\ntwo zebrafish brains per chiplet with high connectivity fidelity (Spearman r =\n0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our\nknowledge, DarwinWafer represents a pioneering demonstration of wafer-scale\nneuromorphic computing, establishing a viable and scalable path toward\nlarge-scale, brain-like computation on silicon by replacing PCB-level\ninterconnects with high-density, on-wafer integration."}
{"id": "2509.16497", "categories": ["cs.ET", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16497", "abs": "https://arxiv.org/abs/2509.16497", "authors": ["Ange-Thierry Ishimwe", "Raghuveer Shivakumar", "Heewoo Kim", "Tamara Lehman", "Joseph Izraelevitz"], "title": "PrediPrune: Reducing Verification Overhead in Souper with Machine Learning Driven Pruning", "comment": null, "summary": "Souper is a powerful enumerative superoptimizer that enhances the runtime\nperformance of programs by optimizing LLVM intermediate representation (IR)\ncode. However, its verification process, which relies on a computationally\nexpensive SMT solver to validate optimization candidates, must explore a large\nsearch space. This large search space makes the verification process\nparticularly expensive, increasing the burden to incorporate Souper into\ncompilation tools. We propose PrediPrune, a stochastic candidate pruning\nstrategy that effectively reduces the number of invalid candidates passed to\nthe SMT solver. By utilizing machine learning techniques to predict the\nvalidity of candidates based on features extracted from the code, PrediPrune\nprunes unlikely candidates early, decreasing the verification workload. When\ncombined with the state-of-the-art approach (Dataflow), PrediPrune decreases\ncompilation time by 51% compared to the Baseline and by 12% compared to using\nonly Dataflow, emphasizing the effectiveness of the combined approach that\nintegrates a purely ML-based method (PrediPrune) with a purely non-ML based\n(Dataflow) method. Additionally, PrediPrune offers a flexible interface to\ntrade-off compilation time and optimization opportunities, allowing end users\nto adjust the balance according to their needs."}
{"id": "2509.16676", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16676", "abs": "https://arxiv.org/abs/2509.16676", "authors": ["Nauman Ali Murad", "Safia Baloch"], "title": "Governed By Agents: A Survey On The Role Of Agentic AI In Future Computing Environments", "comment": null, "summary": "The emergence of agentic Artificial Intelligence (AI), which can operate\nautonomously, demonstrate goal-directed behavior, and adaptively learn,\nindicates the onset of a massive change in today's computing infrastructure.\nThis study investigates how agentic AI models' multiple characteristics may\nimpact the architecture, governance, and operation under which computing\nenvironments function. Agentic AI has the potential to reduce reliance on\nextremely large (public) cloud environments due to resource efficiency,\nespecially with processing and/or storage. The aforementioned characteristics\nprovide us with an opportunity to canvas the likelihood of strategic migration\nin computing infrastructures away from massive public cloud services, towards\nmore locally distributed architectures: edge computing and on-premises\ncomputing infrastructures. Many of these likely migrations will be spurred by\nfactors like on-premises processing needs, diminished data consumption\nfootprints, and cost savings. This study examines how a solution for\nimplementing AI's autonomy could result in a re-architecture of the systems and\nmodel a departure from today's governance models to help us manage these\nincreasingly autonomous agents, and an operational overhaul of processes over a\nvery diverse computing systems landscape that bring together computing via\ncloud, edge, and on-premises computing solutions. To enable us to explore these\nintertwined decisions, it will be fundamentally important to understand how to\nbest position agentic AI, and to navigate the future state of computing\ninfrastructures."}
{"id": "2509.17072", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17072", "abs": "https://arxiv.org/abs/2509.17072", "authors": ["Junyi Wu", "Chao Fang", "Zhongfeng Wang"], "title": "SnipSnap: A Joint Compression Format and Dataflow Co-Optimization Framework for Efficient Sparse LLM Accelerator Design", "comment": "To appear in the 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2026)", "summary": "The growing scale of large language models (LLMs) has intensified demands on\ncomputation and memory, making efficient inference a key challenge. While\nsparsity can reduce these costs, existing design space exploration (DSE)\nframeworks often overlook compression formats, a key factor for leveraging\nsparsity on accelerators. This paper proposes SnipSnap, a joint compression\nformat and dataflow co-optimization framework for efficient sparse LLM\naccelerator design. SnipSnap introduces: (1) a hierarchical compression format\nencoding to expand the design space; (2) an adaptive compression engine for\nselecting formats under diverse sparsity; and (3) a progressive co-search\nworkflow that jointly optimizes dataflow and compression formats. SnipSnap\nachieves 18.24\\% average memory energy savings via format optimization, along\nwith 2248.3$\\times$ and 21.0$\\times$ speedups over Sparseloop and DiMO-Sparse\nframeworks, respectively."}
{"id": "2509.16407", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.16407", "abs": "https://arxiv.org/abs/2509.16407", "authors": ["Hunter McCoy", "Prashant Pandey"], "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables", "comment": null, "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."}
{"id": "2509.16215", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16215", "abs": "https://arxiv.org/abs/2509.16215", "authors": ["Izavan dos S. Correia", "Henrique C. T. Santos", "Tiago A. E. Ferreira"], "title": "Discovering Software Parallelization Points Using Deep Neural Networks", "comment": "17 pages, 10 figures", "summary": "This study proposes a deep learning-based approach for discovering loops in\nprogramming code according to their potential for parallelization. Two genetic\nalgorithm-based code generators were developed to produce two distinct types of\ncode: (i) independent loops, which are parallelizable, and (ii) ambiguous\nloops, whose dependencies are unclear, making them impossible to define if the\nloop is parallelizable or not. The generated code snippets were tokenized and\npreprocessed to ensure a robust dataset. Two deep learning models - a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN) - were\nimplemented to perform the classification. Based on 30 independent runs, a\nrobust statistical analysis was employed to verify the expected performance of\nboth models, DNN and CNN. The CNN showed a slightly higher mean performance,\nbut the two models had a similar variability. Experiments with varying dataset\nsizes highlighted the importance of data diversity for model performance. These\nresults demonstrate the feasibility of using deep learning to automate the\nidentification of parallelizable structures in code, offering a promising tool\nfor software optimization and performance improvement."}
{"id": "2509.16723", "categories": ["cs.ET", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.16723", "abs": "https://arxiv.org/abs/2509.16723", "authors": ["Amjad Iqbal", "Ala'a Al-Habashna", "Gabriel Wainer", "Gary Boudreau"], "title": "Machine Learning in Near-Field Communication for 6G: A Survey", "comment": null, "summary": "6G wireless communication networks are expected to use extremely large-scale\nantenna arrays (ELAAs) to support higher throughput, massive connectivity, and\nimproved system performance. ELAAs would fundamentally alter wave\ncharacteristics, transforming them from plane waves into spherical waves,\nthereby operating in the near field. Near-field communications (NFC) offer\nunique advantages to enhance system performance, but also present significant\nchallenges in channel modeling, computational complexity, and beamforming\ndesign. The use of machine learning (ML) is emerging as a powerful approach to\ntackle such challenges and has the capabilities to enable intelligent, secure,\nand efficient 6G wireless communications. In this survey, we discuss ML-driven\napproaches for NFC. We first outline the fundamental concepts of NFC and ML. We\nthen discuss ML applications in channel estimation, beamforming design, and\nsecurity enhancement. We also highlight key challenges (e.g., data privacy and\ncomputational overhead). Finally, we discuss open issues and future directions\nto emphasize the role of advanced ML techniques in near-field system design."}
{"id": "2509.17721", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17721", "abs": "https://arxiv.org/abs/2509.17721", "authors": ["Pierre Boucher", "Victor Fr√©chard", "Diego Ramirez-Cardona", "Claudiane Ouellet-Plamondon"], "title": "Overcoming challenges in bamboo connections: A review of mechanical properties and structural considerations", "comment": null, "summary": "Over the past decades, bamboo has increasingly gained attention as a\nsustainable construction material, through its rapid growth, naturally\noptimized shape, high mechanical properties, and significant environmental\nbenefits. However, despite these advantages, the use of bamboo in its natural\nform for structural applications remains limited, partly due to insufficient\nknowledge of connection behavior, which is crucial for ensuring the long-term\nreliability and performance of bamboo structures. This article provides a\ncomprehensive review of the key factors to consider in the design of structural\nbamboo connections and discusses the existing connection classification methods\nused as guidelines by designers. By synthesizing findings from the literature,\nour research aims to identify the key parameters interacting with the\nconnection design process, focusing on the anatomical, geometric, and\nmechanical properties of bamboo, the mechanical requirements of the structure\ndesign, and the building methods. A critical analysis of Janssen's\nclassification of bamboo connections, based on force transfer modes and later\nrefined by Widyowijatnoko, is presented. Finally, we discuss the identified\nresearch gaps and emphasize the need for integrated design approaches supported\nby guidelines to support the broader adoption of bamboo in construction."}
{"id": "2509.16495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16495", "abs": "https://arxiv.org/abs/2509.16495", "authors": ["Mert Hidayetoglu", "Aurick Qiao", "Michael Wyatt", "Jeff Rasley", "Yuxiong He", "Samyam Rajbhandari"], "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads", "comment": null, "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."}
{"id": "2509.16233", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16233", "abs": "https://arxiv.org/abs/2509.16233", "authors": ["Dipayan Sanpui", "Anirban Chandra", "Henry Chan", "Sukriti Manna", "Subramanian KRS Sankaranarayanan"], "title": "Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing", "comment": null, "summary": "We present a probabilistic framework to accurately estimate dimensions of\nadditively manufactured components. Using a dataset of 405 parts from nine\nproduction runs involving two machines, three polymer materials, and two-part\nconfigurations, we examine five key design features. To capture both design\ninformation and manufacturing variability, we employ models integrating\ncontinuous and categorical factors. For predicting Difference from Target (DFT)\nvalues, we test deterministic and probabilistic machine learning methods.\nDeterministic models, trained on 80% of the dataset, provide precise point\nestimates, with Support Vector Regression (SVR) achieving accuracy close to\nprocess repeatability. To address systematic deviations, we adopt Gaussian\nProcess Regression (GPR) and Bayesian Neural Networks (BNNs). GPR delivers\nstrong predictive performance and interpretability, while BNNs capture both\naleatoric and epistemic uncertainties. We investigate two BNN approaches: one\nbalancing accuracy and uncertainty capture, and another offering richer\nuncertainty decomposition but with lower dimensional accuracy. Our results\nunderscore the importance of quantifying epistemic uncertainty for robust\ndecision-making, risk assessment, and model improvement. We discuss trade-offs\nbetween GPR and BNNs in terms of predictive power, interpretability, and\ncomputational efficiency, noting that model choice depends on analytical needs.\nBy combining deterministic precision with probabilistic uncertainty\nquantification, our study provides a rigorous foundation for uncertainty-aware\npredictive modeling in AM. This approach not only enhances dimensional accuracy\nbut also supports reliable, risk-informed design strategies, thereby advancing\ndata-driven manufacturing methodologies."}
{"id": "2509.17227", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17227", "abs": "https://arxiv.org/abs/2509.17227", "authors": ["Ekin Ince", "Murat Kuscu"], "title": "Hijacking Living Cells with Surface Engineering for the Internet of Bio-Nano Things", "comment": null, "summary": "The Internet of Bio-Nano Things (IoBNT) promises to revolutionize healthcare\nby interfacing the cyber domain with the living systems at unprecedented\nresolution. Realizing this vision hinges on the development of Bio-Nano Things\n(BNTs), i.e., functional nodes capable of sensing, actuation, and\ncommunications within biological environments. Existing BNT architectures,\ne.g., nanomaterial-based, biosynthetic, and passive molecular agents, face\nsignificant limitations, including toxicity, lack of autonomy, or the safety\nand metabolic burdens associated with genetic modification. This paper posits a\nfourth paradigm: the transient hijacking of living cells via non-genetic cell\nsurface engineering (NG-CSE) to enable living BNTs. NGCSE allows for the\nprecise, reversible functionalization of cell membranes with synthetic\nmolecular machinery, reprogramming cellular functions and interactions without\naltering the genome. It uniquely combines the inherent biocompatibility and\nagency of living cells with the programmability enabled by nanotechnology,\nmitigating the risks of genetic engineering. We critically review the toolbox\nof NG-CSE and explore the opportunities it unlocks for IoBNT, including\nprogrammable cell-cell communication, dynamic network topologies, and improved\nbio-cyber interfacing. Moreover, we propose novel IoBNT architectures that\nleverage these capabilities, such as circulating sentinel networks exploiting\ncellular agency for continuous liquid biopsy, and rationally designed, in vitro\nbiocomputers exploiting interkingdom interactions. We also outline the critical\nchallenges in modeling and exploiting cellular agency with NG-CSE, providing a\nroadmap for the effective utilization of NG-CSE-enabled living BNTs within\nIoBNT."}
{"id": "2509.17731", "categories": ["cs.AR", "cs.NE", "B.7.1; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.17731", "abs": "https://arxiv.org/abs/2509.17731", "authors": ["Amr Nabil", "T. Nandha Kumar", "Haider Abbas F. Almurib"], "title": "Minimal Neuron Circuits: Bursters", "comment": "11 pages, 15 figures, 1 table", "summary": "This work introduces a novel methodology for designing biologically plausible\nbursting neuron circuits using a minimal number of components. We hypothesize\nthat to design circuits capable of bursting, the neuron circuit design must\nmimic a neuron model that inherently exhibits bursting dynamics. Consequently,\nclassical models such as the Hodgkin-Huxley, $I_{Na,p}+I_{K}$, and\nFitzHugh-Nagumo models are not suitable choices. Instead, we propose a\nmethodology for designing neuron circuits that emulate the qualitative\ncharacteristics of the $I_{Na,p}+I_{K}+I_{K(M)}$ model, a well-established\nminimal bursting neuron model. Based on this methodology, we present two novel\nMOSFET-based circuits that exhibit bursting. Using the method of dissection of\nneural bursting, we demonstrate that the nullcline and bifurcation diagrams of\nthe fast subsystem in our circuits are qualitatively equivalent to those of the\n$I_{Na,p}+I_{K}+I_{K(M)}$ model. Furthermore, we examine the effect of the type\nof bifurcation at burst initiation and termination on the bursting\ncharacteristics, showing that our circuits can exhibit diverse bursting\nbehaviours. Importantly, the main contribution of this work lies not in the\nspecific circuit implementation, but in the methodology proposed for\nconstructing bursting neuron circuits."}
{"id": "2509.16504", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16504", "abs": "https://arxiv.org/abs/2509.16504", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "sat-QFL: Secure Quantum Federated Learning for Low Orbit Satellites", "comment": null, "summary": "Low Earth orbit (LEO) constellations violate core assumptions of standard\n(quantum) federated learning (FL): client-server connectivity is intermittent,\nparticipation is time varying, and latency budgets are strict. We present\nsat-QFL, a hierarchical, access aware quantum federated learning (QFL)\nframework that partitions satellites into primary (ground connected) and\nsecondary as inter-satellite links (ISL-only) roles, and schedules sequential,\nsimultaneous, or asynchronous edge training aligned with visibility windows.\nFor quantum-resilient confidentiality and integrity, sat-QFL integrates quantum\nkey distribution (QKD) based key establishment with authenticated encryption\nfor model exchange; we also assess teleportation as a feasibility primitive for\nquantum state transfer. Using derived constellation traces and QFL workloads\n(Qiskit), we show that sat-QFL sustains robust aggregation under varying\nparticipation and reduces communication bottlenecks with modest security\noverhead. Our implementation and results are available at\nhttps://github.com/s222416822/satQFL."}
{"id": "2509.16273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16273", "abs": "https://arxiv.org/abs/2509.16273", "authors": ["Jungseob Yi", "Seoyoung Choi", "Sun Kim", "Sangseon Lee"], "title": "SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening Enhancement Controlling False Positive", "comment": "33 pages, 12 figures", "summary": "Virtual screening (VS) aims to identify bioactive compounds from vast\nchemical libraries, but remains difficult in low-label regimes where only a few\nactives are known. Existing methods largely rely on general-purpose molecular\nfingerprints and overlook class-discriminative substructures critical to\nbioactivity. Moreover, they consider molecules independently, limiting\neffectiveness in low-label regimes. We introduce SubDyve, a network-based VS\nframework that constructs a subgraph-aware similarity network and propagates\nactivity signals from a small known actives. When few active compounds are\navailable, SubDyve performs iterative seed refinement, incrementally promoting\nnew candidates based on local false discovery rate. This strategy expands the\nseed set with promising candidates while controlling false positives from\ntopological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets\nunder zero-shot conditions and on the CDK7 target with a 10-million-compound\nZINC dataset. SubDyve consistently outperforms existing fingerprint or\nembedding-based approaches, achieving margins of up to +34.0 on the BEDROC and\n+24.6 on the EF1% metric."}
{"id": "2509.17290", "categories": ["cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17290", "abs": "https://arxiv.org/abs/2509.17290", "authors": ["Runlong Yu", "Xiaowei Jia"], "title": "Truth Without Comprehension: A BlueSky Agenda for Steering the Fourth Mathematical Crisis", "comment": null, "summary": "Machine-generated proofs are poised to reach large-scale, human-unreadable\nartifacts. They foreshadow what we call the Fourth Mathematical Crisis. This\ncrisis crystallizes around three fundamental tensions: trusting proofs that no\nhuman can inspect, understanding results that no one can fully read, and\nverifying systems that themselves resist verification. As a minimal yet\nprincipled response, we propose the Human Understandability (HU) meta-axiom,\nwhich requires that every proof admits at least one projection that is\nresource-bounded, divergence-measured, and acceptable to a verifier.\nConfronting these questions opens a timely research agenda and points toward\nnew directions in scalable reasoning, interpretable inference, and epistemic\ntrust for the era of machine-scale mathematics."}
{"id": "2509.16213", "categories": ["cs.ET", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16213", "abs": "https://arxiv.org/abs/2509.16213", "authors": ["Xiaolei Zhu", "Xiaofei Jin", "Ziyang Kang", "Chonghui Sun", "Junjie Feng", "Dingwen Hu", "Zengyi Wang", "Hanyue Zhuang", "Qian Zheng", "Huajin Tang", "Shi Gu", "Xin Du", "De Ma", "Gang Pan"], "title": "DarwinWafer: A Wafer-Scale Neuromorphic Chip", "comment": null, "summary": "Neuromorphic computing promises brain-like efficiency, yet today's multi-chip\nsystems scale over PCBs and incur orders-of-magnitude penalties in bandwidth,\nlatency, and energy, undermining biological algorithms and system efficiency.\nWe present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip\ninterconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets\non a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based\nasynchronous wafer fabric with hierarchical time-step synchronization provide\nlow-latency, coherent operation across the wafer. Each chiplet implements 2.35\nM neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per\nwafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9\npJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by\na holistic chiplet-interposer co-design flow (including an in-house\ninterposer-bump planner with early SI/PI and electro-thermal closure) and a\nwarpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin\nconnections, enabling robust, demountable wafer-to-board integration.\nMeasurements confirm 10 mV supply droop and a uniform thermal profile (34-36\n{\\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations:\ntwo zebrafish brains per chiplet with high connectivity fidelity (Spearman r =\n0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our\nknowledge, DarwinWafer represents a pioneering demonstration of wafer-scale\nneuromorphic computing, establishing a viable and scalable path toward\nlarge-scale, brain-like computation on silicon by replacing PCB-level\ninterconnects with high-density, on-wafer integration."}
{"id": "2509.16505", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16505", "abs": "https://arxiv.org/abs/2509.16505", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "orb-QFL: Orbital Quantum Federated Learning", "comment": null, "summary": "Recent breakthroughs in quantum computing present transformative\nopportunities for advancing Federated Learning (FL), particularly in\nnon-terrestrial environments characterized by stringent communication and\ncoordination constraints. In this study, we propose orbital QFL, termed\norb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low\nEarth Orbit (LEO) satellite constellations. Distinct from conventional FL\nparadigms, termed orb-QFL operates without centralized servers or global\naggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement\nand local quantum processing to facilitate decentralized, inter-satellite\ncollaboration. This design inherently addresses the challenges of orbital\ndynamics, such as intermittent connectivity, high propagation delays, and\ncoverage variability. The framework enables continuous model refinement through\ndirect quantum-based synchronization between neighboring satellites, thereby\nenhancing resilience and preserving data locality. To validate our approach, we\nintegrate the Qiskit quantum machine learning toolkit with Poliastro-based\norbital simulations and conduct experiments using Statlog dataset."}
{"id": "2509.16277", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16277", "abs": "https://arxiv.org/abs/2509.16277", "authors": ["Haobo Yang", "Shiyan Zhang", "Zhuoyi Yang", "Jilong Guo", "Jun Yang", "Xinyu Zhang"], "title": "Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception", "comment": null, "summary": "Deep perception networks in autonomous driving traditionally rely on\ndata-intensive training regimes and post-hoc anomaly detection, often\ndisregarding fundamental information-theoretic constraints governing stable\ninformation processing. We reconceptualize deep neural encoders as hierarchical\ncommunication chains that incrementally compress raw sensory inputs into\ntask-relevant latent features. Within this framework, we establish two\ntheoretically justified design principles for robust perception: (D1) smooth\nvariation of mutual information between consecutive layers, and (D2) monotonic\ndecay of latent entropy with network depth. Our analysis shows that, under\nrealistic architectural assumptions, particularly blocks comprising repeated\nlayers of similar capacity, enforcing smooth information flow (D1) naturally\nencourages entropy decay (D2), thus ensuring stable compression. Guided by\nthese insights, we propose Eloss, a novel entropy-based regularizer designed as\na lightweight, plug-and-play training objective. Rather than marginal accuracy\nimprovements, this approach represents a conceptual shift: it unifies\ninformation-theoretic stability with standard perception tasks, enabling\nexplicit, principled detection of anomalous sensor inputs through entropy\ndeviations. Experimental validation on large-scale 3D object detection\nbenchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss\nconsistently achieves competitive or improved accuracy while dramatically\nenhancing sensitivity to anomalies, amplifying distribution-shift signals by up\nto two orders of magnitude. This stable information-compression perspective not\nonly improves interpretability but also establishes a solid theoretical\nfoundation for safer, more robust autonomous driving perception systems."}
{"id": "2509.17324", "categories": ["cs.ET", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.17324", "abs": "https://arxiv.org/abs/2509.17324", "authors": ["Chi Zhang", "Mengxin Zheng", "Qian Lou", "Fan Chen"], "title": "DiffQ: Unified Parameter Initialization for Variational Quantum Algorithms via Diffusion Models", "comment": null, "summary": "Variational Quantum Algorithms (VQAs) are widely used in the noisy\nintermediate-scale quantum (NISQ) era, but their trainability and performance\ndepend critically on initialization parameters that shape the optimization\nlandscape. Existing machine learning-based initializers achieve\nstate-of-the-art results yet remain constrained to single-task domains and\nsmall datasets of only hundreds of samples. We address these limitations by\nreformulating VQA parameter initialization as a generative modeling problem and\nintroducing DiffQ, a parameter initializer based on the Denoising Diffusion\nProbabilistic Model (DDPM). To support robust training and evaluation, we\nconstruct a dataset of 15,085 instances spanning three domains and five\nrepresentative tasks. Experiments demonstrate that DiffQ surpasses baselines,\nreducing initial loss by up to 8.95 and convergence steps by up to 23.4%."}
{"id": "2509.17963", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17963", "abs": "https://arxiv.org/abs/2509.17963", "authors": ["Rudra Biswas", "Jiahui Duan", "Shan Deng", "Xuezhong Niu", "Yixin Qin", "Prapti Panigrahi", "Varun Parekh", "Rajiv Joshi", "Kai Ni", "Vijaykrishnan Narayanan"], "title": "Single-Cell Universal Logic-in-Memory Using 2T-nC FeRAM: An Area and Energy-Efficient Approach for Bulk Bitwise Computation", "comment": "6 Pages, 7 Figures, To be presented at System on Chip Conference 2025", "summary": "This work presents a novel approach to configure 2T-nC ferroelectric RAM\n(FeRAM) for performing single cell logic-in-memory operations, highlighting its\nadvantages in energy-efficient computation over conventional DRAM-based\napproaches. Unlike conventional 1T-1C dynamic RAM (DRAM), which incurs refresh\noverhead, 2T-nC FeRAM offers a promising alternative as a non-volatile memory\nsolution with low energy consumption. Our key findings include the potential of\nquasi-nondestructive readout (QNRO) sensing in 2T-nC FeRAM for logic-in-memory\n(LiM) applications, demonstrating its inherent capability to perform inverting\nlogic without requiring external modifications, a feature absent in traditional\n1T-1C DRAM. We successfully implement the MINORITY function within a single\ncell of 2T-nC FeRAM, enabling universal NAND and NOR logic, validated through\nSPICE simulations and experimental data. Additionally, the research\ninvestigates the feasibility of 3D integration with 2T-nC FeRAM, showing\nsubstantial improvements in storage and computational density, facilitating\nbulk-bitwise computation. Our evaluation of eight real-world, data-intensive\napplications reveals that 2T-nC FeRAM achieves 2x higher performance and 2.5x\nlower energy consumption compared to DRAM. Furthermore, the thermal stability\nof stacked 2T-nC FeRAM is validated, confirming its reliable operation when\nintegrated on a compute die. These findings emphasize the advantages of 2T-nC\nFeRAM for LiM, offering superior performance and energy efficiency over\nconventional DRAM."}
{"id": "2509.16513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16513", "abs": "https://arxiv.org/abs/2509.16513", "authors": ["Wesley Brewer", "Matthias Maiterth", "Damien Fay"], "title": "Trace Replay Simulation of MIT SuperCloud for Studying Optimal Sustainability Policies", "comment": "2 pages, 2 figures", "summary": "The rapid growth of AI supercomputing is creating unprecedented power\ndemands, with next-generation GPU datacenters requiring hundreds of megawatts\nand producing fast, large swings in consumption. To address the resulting\nchallenges for utilities and system operators, we extend ExaDigiT, an\nopen-source digital twin framework for modeling power, cooling, and scheduling\nof supercomputers. Originally developed for replaying traces from\nleadership-class HPC systems, ExaDigiT now incorporates heterogeneity,\nmulti-tenancy, and cloud-scale workloads. In this work, we focus on trace\nreplay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable\nreinforcement learning (RL)-based experimentation with sustainability policies.\nThe RAPS module provides a simulation environment with detailed power and\nperformance statistics, supporting the study of scheduling strategies,\nincentive structures, and hardware/software prototyping. Preliminary RL\nexperiments using Proximal Policy Optimization demonstrate the feasibility of\nlearning energy-aware scheduling decisions, highlighting ExaDigiT's potential\nas a platform for exploring optimal policies to improve throughput, efficiency,\nand sustainability."}
{"id": "2509.16287", "categories": ["cs.LG", "05C22, 05C90, 68R10"], "pdf": "https://arxiv.org/pdf/2509.16287", "abs": "https://arxiv.org/abs/2509.16287", "authors": ["Shanookha Ali", "Nitha Niralda", "Sunil Mathew"], "title": "Architectural change in neural networks using fuzzy vertex pooling", "comment": null, "summary": "The process of pooling vertices involves the creation of a new vertex, which\nbecomes adjacent to all the vertices that were originally adjacent to the\nendpoints of the vertices being pooled. After this, the endpoints of these\nvertices and all edges connected to them are removed. In this document, we\nintroduce a formal framework for the concept of fuzzy vertex pooling (FVP) and\nprovide an overview of its key properties with its applications to neural\nnetworks. The pooling model demonstrates remarkable efficiency in minimizing\nloss rapidly while maintaining competitive accuracy, even with fewer hidden\nlayer neurons. However, this advantage diminishes over extended training\nperiods or with larger datasets, where the model's performance tends to\ndegrade. This study highlights the limitations of pooling in later stages of\ndeep learning training, rendering it less effective for prolonged or\nlarge-scale applications. Consequently, pooling is recommended as a strategy\nfor early-stage training in advanced deep learning models to leverage its\ninitial efficiency."}
{"id": "2509.17533", "categories": ["cs.ET", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17533", "abs": "https://arxiv.org/abs/2509.17533", "authors": ["Anastasios Fanariotis", "Theofanis Orphanoudakis", "Vasilis Fotopoulos"], "title": "Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers", "comment": null, "summary": "The deployment of machine learning (ML) models on microcontrollers (MCUs) is\nconstrained by strict energy, latency, and memory requirements, particularly in\nbattery-operated and real-time edge devices. While software-level optimizations\nsuch as quantization and pruning reduce model size and computation, hardware\nacceleration has emerged as a decisive enabler for efficient embedded\ninference. This paper evaluates the impact of Neural Processing Units (NPUs) on\nMCU-based ML execution, using the ARM Cortex-M55 core combined with the\nEthos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a\nrepresentative platform. A rigorous measurement methodology was employed,\nincorporating per-inference net energy accounting via GPIO-triggered\nhigh-resolution digital multimeter synchronization and idle-state subtraction,\nensuring accurate attribution of energy costs. Experimental results across six\nrepresentative ML models -including MiniResNet, MobileNetV2, FD-MobileNet,\nMNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains\nwhen inference is offloaded to the NPU. For moderate to large networks, latency\nimprovements ranged from 7x to over 125x, with per-inference net energy\nreductions up to 143x. Notably, the NPU enabled execution of models unsupported\non CPU-only paths, such as SSD-MobileNet, highlighting its functional as well\nas efficiency advantages. These findings establish NPUs as a cornerstone of\nenergy-aware embedded AI, enabling real-time, power-constrained ML inference at\nthe MCU level."}
{"id": "2509.16857", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16857", "abs": "https://arxiv.org/abs/2509.16857", "authors": ["Xingyu Xiang", "Raj Joshi", "Yuhan Liu", "Jiayi Yao", "Chenxingyu Zhao", "Junchen Jiang", "Yang Zhou", "Eddie Kohler", "Minlan Yu"], "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching", "comment": null, "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."}
{"id": "2509.16293", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16293", "abs": "https://arxiv.org/abs/2509.16293", "authors": ["Borui Wan", "Gaohong Liu", "Zuquan Song", "Jun Wang", "Yun Zhang", "Guangming Sheng", "Shuguang Wang", "Houmin Wei", "Chenyuan Wang", "Weiqiang Lou", "Xi Yang", "Mofan Zhang", "Kaihua Jiang", "Cheng Ren", "Xiaoyun Zhi", "Menghan Yu", "Zhe Nan", "Zhuolin Zheng", "Baoquan Zhong", "Qinlong Wang", "Huan Yu", "Jinxin Chi", "Wang Zhang", "Yuhan Li", "Zixian Du", "Sida Zhao", "Yongqiang Zhang", "Jingzhe Tang", "Zherui Liu", "Chuan Wu", "Yanghua Peng", "Haibin Lin", "Wencong Xiao", "Xin Liu", "Liang Xiang"], "title": "Robust LLM Training Infrastructure at ByteDance", "comment": null, "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."}
{"id": "2509.17963", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17963", "abs": "https://arxiv.org/abs/2509.17963", "authors": ["Rudra Biswas", "Jiahui Duan", "Shan Deng", "Xuezhong Niu", "Yixin Qin", "Prapti Panigrahi", "Varun Parekh", "Rajiv Joshi", "Kai Ni", "Vijaykrishnan Narayanan"], "title": "Single-Cell Universal Logic-in-Memory Using 2T-nC FeRAM: An Area and Energy-Efficient Approach for Bulk Bitwise Computation", "comment": "6 Pages, 7 Figures, To be presented at System on Chip Conference 2025", "summary": "This work presents a novel approach to configure 2T-nC ferroelectric RAM\n(FeRAM) for performing single cell logic-in-memory operations, highlighting its\nadvantages in energy-efficient computation over conventional DRAM-based\napproaches. Unlike conventional 1T-1C dynamic RAM (DRAM), which incurs refresh\noverhead, 2T-nC FeRAM offers a promising alternative as a non-volatile memory\nsolution with low energy consumption. Our key findings include the potential of\nquasi-nondestructive readout (QNRO) sensing in 2T-nC FeRAM for logic-in-memory\n(LiM) applications, demonstrating its inherent capability to perform inverting\nlogic without requiring external modifications, a feature absent in traditional\n1T-1C DRAM. We successfully implement the MINORITY function within a single\ncell of 2T-nC FeRAM, enabling universal NAND and NOR logic, validated through\nSPICE simulations and experimental data. Additionally, the research\ninvestigates the feasibility of 3D integration with 2T-nC FeRAM, showing\nsubstantial improvements in storage and computational density, facilitating\nbulk-bitwise computation. Our evaluation of eight real-world, data-intensive\napplications reveals that 2T-nC FeRAM achieves 2x higher performance and 2.5x\nlower energy consumption compared to DRAM. Furthermore, the thermal stability\nof stacked 2T-nC FeRAM is validated, confirming its reliable operation when\nintegrated on a compute die. These findings emphasize the advantages of 2T-nC\nFeRAM for LiM, offering superior performance and energy efficiency over\nconventional DRAM."}
{"id": "2509.16995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16995", "abs": "https://arxiv.org/abs/2509.16995", "authors": ["Zheming Yang", "Qi Guo", "Yunqing Hu", "Chang Zhao", "Chang Zhang", "Jian Zhao", "Wen Ji"], "title": "MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with Edge-Cloud Collaboration for Efficient Multimodal LLM Inference", "comment": "5 pages, 4 figures", "summary": "Multimodal large language models (MLLMs) enable powerful cross-modal\ninference but impose significant computational and latency burdens, posing\nsevere challenges for deployment in resource-constrained environments. In this\npaper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading\nframework with edge-cloud collaboration for efficient MLLM inference. MoA-Off\nintroduces a lightweight heterogeneous modality-aware module that estimates the\ncomplexity of heterogeneous inputs through multi-dimensional feature analysis.\nThen, an adaptive edge-cloud collaborative offloading strategy is proposed that\ndynamically schedules workloads between edge and cloud based on modality-aware\ncomplexity scores and real-time system states. The experimental results\ndemonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65%\ndecrease in resource overhead while maintaining competitive accuracy compared\nto traditional approaches."}
{"id": "2509.16300", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16300", "abs": "https://arxiv.org/abs/2509.16300", "authors": ["Manh Cuong Dao", "The Hung Tran", "Phi Le Nguyen", "Thao Nguyen Truong", "Trong Nghia Hoang"], "title": "ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge", "comment": "The first two authors contributed equally", "summary": "This paper studies the black-box optimization task which aims to find the\nmaxima of a black-box function using a static set of its observed input-output\npairs. This is often achieved via learning and optimizing a surrogate function\nwith that offline data. Alternatively, it can also be framed as an inverse\nmodeling task that maps a desired performance to potential input candidates\nthat achieve it. Both approaches are constrained by the limited amount of\noffline data. To mitigate this limitation, we introduce a new perspective that\ncasts offline optimization as a distributional translation task. This is\nformulated as learning a probabilistic bridge transforming an implicit\ndistribution of low-value inputs (i.e., offline data) into another distribution\nof high-value inputs (i.e., solution candidates). Such probabilistic bridge can\nbe learned using low- and high-value inputs sampled from synthetic functions\nthat resemble the target function. These synthetic functions are constructed as\nthe mean posterior of multiple Gaussian processes fitted with different\nparameterizations on the offline data, alleviating the data bottleneck. The\nproposed approach is evaluated on an extensive benchmark comprising most recent\nmethods, demonstrating significant improvement and establishing a new\nstate-of-the-art performance."}
{"id": "2509.17322", "categories": ["cs.LG", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.17322", "abs": "https://arxiv.org/abs/2509.17322", "authors": ["Chi Zhang", "Mengxin Zheng", "Qian Lou", "Hui Min Leung", "Fan Chen"], "title": "VQEzy: An Open-Source Dataset for Parameter Initialize in Variational Quantum Eigensolvers", "comment": null, "summary": "Variational Quantum Eigensolvers (VQEs) are a leading class of noisy\nintermediate-scale quantum (NISQ) algorithms, whose performance is highly\nsensitive to parameter initialization. Although recent machine learning-based\ninitialization methods have achieved state-of-the-art performance, their\nprogress has been limited by the lack of comprehensive datasets. Existing\nresources are typically restricted to a single domain, contain only a few\nhundred instances, and lack complete coverage of Hamiltonians, ansatz circuits,\nand optimization trajectories. To overcome these limitations, we introduce\nVQEzy, the first large-scale dataset for VQE parameter initialization. VQEzy\nspans three major domains and seven representative tasks, comprising 12,110\ninstances with full VQE specifications and complete optimization trajectories.\nThe dataset is available online, and will be continuously refined and expanded\nto support future research in VQE optimization."}
{"id": "2509.17351", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17351", "abs": "https://arxiv.org/abs/2509.17351", "authors": ["Slava Kitaeff", "Luc Betbeder-Matibet", "Jake Carroll", "Stephen Giugni", "David Abramson", "John Zaitseff", "Sarah Walters", "David Powell", "Chris Bording", "Trung Nguyen", "Angus Macoustra", "Fabien Voisin", "Bowen Chen", "Jarrod Hurley"], "title": "Institutional Research Computing Capabilities in Australia: 2024", "comment": "9 pages in IEEE Proceedings format, International Conference on\n  eScience 2025, Accepted", "summary": "Institutional research computing infrastructure plays a vital role in\nAustralia's research ecosystem, complementing and extending national\nfacilities. This paper analyses research computing capabilities across\nAustralian universities and organisations, showing how institutional systems\nsupport research excellence through local compute resources, specialised\nhardware, and cluster solutions. Our study finds that nearly 112,258 CPU cores\nand 2,241 GPUs serve over 6,000 researchers as essential bridges between\ndesktops and national facilities, enabling workflows from development to\nlarge-scale computations. The estimated replacement value of this\ninfrastructure is $144M AUD. Drawing on detailed data from multiple\ninstitutions, we identify key patterns in deployment, utilisation, and\nstrategic alignment with research priorities. Institutional resources provide\ncritical support for data-intensive projects, facilitate training and\nhigher-degree student research, enable prototyping and development, and ensure\ndata sovereignty compliance when required. The analysis shows how these\nfacilities leverage national investments while addressing institution-specific\nneeds that national systems cannot meet. We present evidence that strategic\ninvestment in institutional capabilities yields significant returns through\ngreater research productivity, enhanced graduate training, and improved\noutcomes. The study offers insights for organisations planning computing\nstrategies and highlights the importance of maintaining robust institutional\nresources alongside national facilities."}
{"id": "2509.16324", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.16324", "abs": "https://arxiv.org/abs/2509.16324", "authors": ["Jiale Han", "Chun Gan", "Chengcheng Zhang", "Jie He", "Zhangang Lin", "Ching Law", "Xiaowu Dai"], "title": "Auto-bidding under Return-on-Spend Constraints with Uncertainty Quantification", "comment": null, "summary": "Auto-bidding systems are widely used in advertising to automatically\ndetermine bid values under constraints such as total budget and Return-on-Spend\n(RoS) targets. Existing works often assume that the value of an ad impression,\nsuch as the conversion rate, is known. This paper considers the more realistic\nscenario where the true value is unknown. We propose a novel method that uses\nconformal prediction to quantify the uncertainty of these values based on\nmachine learning methods trained on historical bidding data with contextual\nfeatures, without assuming the data are i.i.d. This approach is compatible with\ncurrent industry systems that use machine learning to predict values. Building\non prediction intervals, we introduce an adjusted value estimator derived from\nmachine learning predictions, and show that it provides performance guarantees\nwithout requiring knowledge of the true value. We apply this method to enhance\nexisting auto-bidding algorithms with budget and RoS constraints, and establish\ntheoretical guarantees for achieving high reward while keeping RoS violations\nlow. Empirical results on both simulated and real-world industrial datasets\ndemonstrate that our approach improves performance while maintaining\ncomputational efficiency."}
{"id": "2509.17357", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17357", "abs": "https://arxiv.org/abs/2509.17357", "authors": ["Yunzhao Liu", "Qiang Xu", "Y. Charlie Hu"], "title": "Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill", "comment": null, "summary": "Efficient LLM inference is critical for real-world applications, especially\nwithin heterogeneous GPU clusters commonly found in organizations and\non-premise datacenters as GPU architecture rapidly evolves. Current\ndisaggregated prefill strategies, which separate the prefill and decode stages\nof LLM inference across different GPUs, often suffer from suboptimal\nperformance due to imbalances between GPU capabilities and workload demands. On\nthe other hand, extending conventional data parallelism and pipeline\nparallelism to heterogeneous setups incurs high inference latencies. To address\nthese challenges, we introduce Cronus, a novel LLM inference system designed to\ndynamically balance workloads across heterogeneous GPUs using partially\ndisaggregated prefill. Cronus partitions each prefill stage and executes its\ninitial portion on the low-end GPU, while overlapping the remaining prefill and\ndecode stages of earlier requests on the high-end GPU. Extensive evaluations\nacross various high-end and low-end GPU combinations demonstrate that Cronus\nsignificantly improves the throughput over disaggregated prefill. It also\nreduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining\nsimilar or better throughput."}
{"id": "2509.16339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16339", "abs": "https://arxiv.org/abs/2509.16339", "authors": ["Josias K. Moukpe", "Philip K. Chan", "Ming Zhang"], "title": "Highly Imbalanced Regression with Tabular Data in SEP and Other Applications", "comment": "ICMLA 2025", "summary": "We investigate imbalanced regression with tabular data that have an imbalance\nratio larger than 1,000 (\"highly imbalanced\"). Accurately estimating the target\nvalues of rare instances is important in applications such as forecasting the\nintensity of rare harmful Solar Energetic Particle (SEP) events. For\nregression, the MSE loss does not consider the correlation between predicted\nand actual values. Typical inverse importance functions allow only convex\nfunctions. Uniform sampling might yield mini-batches that do not have rare\ninstances. We propose CISIR that incorporates correlation, Monotonically\nDecreasing Involution (MDI) importance, and stratified sampling. Based on five\ndatasets, our experimental results indicate that CISIR can achieve lower error\nand higher correlation than some recent methods. Also, adding our correlation\ncomponent to other recent methods can improve their performance. Lastly, MDI\nimportance can outperform other importance functions. Our code can be found in\nhttps://github.com/Machine-Earning/CISIR."}
{"id": "2509.17360", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17360", "abs": "https://arxiv.org/abs/2509.17360", "authors": ["Chaoyi Ruan", "Chao Bi", "Kaiwen Zheng", "Ziji Shi", "Xinyi Wan", "Jialin Li"], "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access", "comment": null, "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."}
{"id": "2509.16345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16345", "abs": "https://arxiv.org/abs/2509.16345", "authors": ["Minxiao Wang", "Runze Yan", "Carol Li", "Saurabh Kataria", "Xiao Hu", "Matthew Clark", "Timothy Ruchti", "Timothy G. Buchman", "Sivasubramanium V Bhavani", "Randall J. Lee"], "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach", "comment": null, "summary": "Clinical laboratory tests provide essential biochemical measurements for\ndiagnosis and treatment, but are limited by intermittent and invasive sampling.\nIn contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded\nsignal in intensive care units (ICUs) that reflects cardiovascular dynamics and\ncan serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a\nframework that combines a large-scale PPG foundation model for local waveform\nencoding with a patient-aware Mamba model for long-range temporal modeling. Our\narchitecture addresses three challenges: (1) capturing extended temporal trends\nin laboratory values, (2) accounting for patient-specific baseline variation\nvia FiLM-modulated initial states, and (3) performing multi-task estimation for\ninterrelated biomarkers. We evaluate our method on the two ICU datasets for\npredicting the five key laboratory tests. The results show substantial\nimprovements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$\namong most of the estimation targets. This work demonstrates the feasibility of\ncontinuous, personalized lab value estimation from routine PPG monitoring,\noffering a pathway toward non-invasive biochemical surveillance in critical\ncare."}
{"id": "2509.17388", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17388", "abs": "https://arxiv.org/abs/2509.17388", "authors": ["Manel Lurbe", "Miguel Avargues", "Salvador Petit", "Maria E. Gomez", "Rui Yang", "Guanhao Wang", "Julio Sahuquillo"], "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory", "comment": null, "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."}
{"id": "2509.16354", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16354", "abs": "https://arxiv.org/abs/2509.16354", "authors": ["Sivan Sarafian", "Yehudit Aperstein"], "title": "Improving Deep Tabular Learning", "comment": "18 pages, 4 figures", "summary": "Tabular data remain a dominant form of real-world information but pose\npersistent challenges for deep learning due to heterogeneous feature types,\nlack of natural structure, and limited label-preserving augmentations. As a\nresult, ensemble models based on decision trees continue to dominate benchmark\nleaderboards. In this work, we introduce RuleNet, a transformer-based\narchitecture specifically designed for deep tabular learning. RuleNet\nincorporates learnable rule embeddings in a decoder, a piecewise linear\nquantile projection for numerical features, and feature masking ensembles for\nrobustness and uncertainty estimation. Evaluated on eight benchmark datasets,\nRuleNet matches or surpasses state-of-the-art tree-based methods in most cases,\nwhile remaining computationally efficient, offering a practical neural\nalternative for tabular prediction tasks."}
{"id": "2509.17496", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17496", "abs": "https://arxiv.org/abs/2509.17496", "authors": ["Kaiji Yang", "Jingjing Zhang", "Junyao Zheng", "Qiwen Liu", "Weigang Wu", "Jieying Zhou"], "title": "pBeeGees: A Prudent Approach to Certificate-Decoupled BFT Consensus", "comment": "Accepted by the 25th International Conference on Algorithms and\n  Architectures for Parallel Processing (ICA3PP 2025)", "summary": "Pipelined Byzantine Fault Tolerant (BFT) consensus is fundamental to\npermissioned blockchains. However, many existing protocols are limited by the\nrequirement for view-consecutive quorum certificates (QCs). This constraint\nimpairs performance and creates liveness vulnerabilities under adverse network\nconditions. Achieving \"certificate decoupling\"-committing blocks without this\nrequirement-is therefore a key research goal. While the recent BeeGees\nalgorithm achieves this, our work reveals that it suffers from security and\nliveness issues. To address this problem, this paper makes two primary\ncontributions. First, we formally define these flaws as the Invalid Block\nProblem and the Hollow Chain Problem. Second, we propose pBeeGees, a new\nalgorithm that addresses these issues while preserving certificate decoupling\nwith no additional computational overhead. To achieve this, pBeeGees integrates\ntraceback and pre-commit validation to solve the Invalid Block Problem.Further,\nto mitigate the Hollow Chain Problem, we introduce a prudent validation\nmechanism, which prevents unverified branches from growing excessively. To\nsummarize, pBeeGees is the first protocol to simultaneously achieve safety,\nliveness, and certificate decoupling in a pipelined BFT framework. Experiments\nconfirm that our design significantly reduces block commit latency compared to\nclassic algorithms, particularly under frequent stopping faults."}
{"id": "2509.16357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16357", "abs": "https://arxiv.org/abs/2509.16357", "authors": ["Aniruddh Raghu", "Sebastian Ober", "Maxwell Kazman", "Hunter Elliott"], "title": "Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization", "comment": "GEM Workshop, ICLR 2025", "summary": "Therapeutic antibody candidates often require extensive engineering to\nimprove key functional and developability properties before clinical\ndevelopment. This can be achieved through iterative design, where starting\nmolecules are optimized over several rounds of in vitro experiments. While\nprotein structure can provide a strong inductive bias, it is rarely used in\niterative design due to the lack of structural data for continually evolving\nlead molecules over the course of optimization. In this work, we propose a\nstrategy for iterative antibody optimization that leverages both sequence and\nstructure as well as accumulating lab measurements of binding and\ndevelopability. Building on prior work, we first train a sequence-structure\ndiffusion generative model that operates on antibody-antigen complexes. We then\noutline an approach to use this model, together with carefully predicted\nantibody-antigen complexes, to optimize lead candidates throughout the\niterative design process. Further, we describe a guided sampling approach that\nbiases generation toward desirable properties by integrating models trained on\nexperimental data from iterative design. We evaluate our approach in multiple\nin silico and in vitro experiments, demonstrating that it produces\nhigh-affinity binders at multiple stages of an active antibody optimization\ncampaign."}
{"id": "2509.17532", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17532", "abs": "https://arxiv.org/abs/2509.17532", "authors": ["Guanxiong Sun", "Majid Mirmehdi", "Zahraa Abdallah", "Raul Santos-Rodriguez", "Ian Craddock", "Telmo de Menezes e Silva Filho"], "title": "TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning with Similarity-guided Model Aggregation", "comment": null, "summary": "Real-world federated learning faces two key challenges: limited access to\nlabelled data and the presence of heterogeneous multi-modal inputs. This paper\nproposes TACTFL, a unified framework for semi-supervised multi-modal federated\nlearning. TACTFL introduces a modality-agnostic temporal contrastive training\nscheme that conducts representation learning from unlabelled client data by\nleveraging temporal alignment across modalities. However, as clients perform\nself-supervised training on heterogeneous data, local models may diverge\nsemantically. To mitigate this, TACTFL incorporates a similarity-guided model\naggregation strategy that dynamically weights client models based on their\nrepresentational consistency, promoting global alignment. Extensive experiments\nacross diverse benchmarks and modalities, including video, audio, and wearable\nsensors, demonstrate that TACTFL achieves state-of-the-art performance. For\ninstance, on the UCF101 dataset with only 10% labelled data, TACTFL attains\n68.48% top-1 accuracy, significantly outperforming the FedOpt baseline of\n35.35%. Code will be released upon publication."}
{"id": "2509.16379", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16379", "abs": "https://arxiv.org/abs/2509.16379", "authors": ["Xinran Liu", "Shansita D. Sharma", "Soheil Kolouri"], "title": "EMPEROR: Efficient Moment-Preserving Representation of Distributions", "comment": null, "summary": "We introduce EMPEROR (Efficient Moment-Preserving Representation of\nDistributions), a mathematically rigorous and computationally efficient\nframework for representing high-dimensional probability measures arising in\nneural network representations. Unlike heuristic global pooling operations,\nEMPEROR encodes a feature distribution through its statistical moments. Our\napproach leverages the theory of sliced moments: features are projected onto\nmultiple directions, lightweight univariate Gaussian mixture models (GMMs) are\nfit to each projection, and the resulting slice parameters are aggregated into\na compact descriptor. We establish determinacy guarantees via Carleman's\ncondition and the Cram\\'er-Wold theorem, ensuring that the GMM is uniquely\ndetermined by its sliced moments, and we derive finite-sample error bounds that\nscale optimally with the number of slices and samples. Empirically, EMPEROR\ncaptures richer distributional information than common pooling schemes across\nvarious data modalities, while remaining computationally efficient and broadly\napplicable."}
{"id": "2509.17542", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17542", "abs": "https://arxiv.org/abs/2509.17542", "authors": ["Xing Chen", "Rong Shi", "Lu Zhao", "Lingbin Wang", "Xiao Jin", "Yueqiang Chen", "Hongfeng Sun"], "title": "Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs", "comment": null, "summary": "LLM-based applications have been widely used in various industries, but with\nthe increasing of models size, an efficient large language model (LLM)\ninference system is an urgent problem to be solved for service providers. Since\nthe inference system is divided into two stage with different characteristics:\nPrefill and Decode, the two stage will interfere with each other during the\ninference process. Toward this end, a P-D disaggregated inference framework is\nproposed by some researchers. Current research is done on homogeneous GPUs, and\nlacks deployment solutions based on business scenarios. Compared with\nhomogeneous GPUs, using heterogeneous GPUs to construct inference systems can\nbetter improve resource utilization and reduce costs. Even if GPUs from\ndifferent vendors are used to build inference systems, on the basis of reducing\ncosts, the resource utilization rate can be improved and the dependence on a\nsingle vendor can be reduced. Therefore, a P-D disaggreagetd inference system\nbased on heterogeneous GPUs is designed, and the heterogeneous compatible\ntransmission module in the system is designed to address heterogeneous GPU data\ncompatibility issues. Then, a joint optimization algorithm of parallel strategy\nand instance number allocation is proposed to obtain the deployment solutions.\nFinally, the experimental results show that the P-D disaggregated inference\nsystem can well solve the hybrid inference problem of heterogeneous GPUs from\ndifferent vendors, and the joint optimization algorithm can obtain the optimal\ndeployment solution."}
{"id": "2509.16391", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16391", "abs": "https://arxiv.org/abs/2509.16391", "authors": ["Yasser H. Khalil", "Mehdi Setayesh", "Hongliang Li"], "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning", "comment": null, "summary": "Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness."}
{"id": "2509.17771", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17771", "abs": "https://arxiv.org/abs/2509.17771", "authors": ["Christian Cachin", "Jinfeng Dou", "Christian Scheideler", "Philipp Schneider"], "title": "A Lightweight Approach for State Machine Replication", "comment": null, "summary": "We present a lightweight solution for state machine replication with\ncommitment certificates. Specifically, we adapt a simple median rule from the\nstabilizing consensus problem [Doerr11] to operate in a client-server setting\nwhere arbitrary servers may be blocked adaptively based on past system\ninformation. We further extend our protocol by compressing information about\ncommitted commands, thus keeping the protocol lightweight, while still enabling\nclients to easily prove that their commands have indeed been committed on the\nshared state. Our approach guarantees liveness as long as at most a constant\nfraction of servers are blocked, ensures safety under any number of blocked\nservers, and supports fast recovery from massive blocking attacks. In addition\nto offering near-optimal performance in several respects, our method is fully\ndecentralized, unlike other near-optimal solutions that rely on leaders. In\nparticular, our solution is robust against adversaries that target key servers\n(which captures insider-based denial-of-service attacks), whereas leader-based\napproaches fail under such a blocking model."}
{"id": "2509.16393", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.16393", "abs": "https://arxiv.org/abs/2509.16393", "authors": ["Manuel Noseda", "Alberto De Luca", "Lukas Von Briel", "Nathan Lacour"], "title": "Federated Learning for Financial Forecasting", "comment": null, "summary": "This paper studies Federated Learning (FL) for binary classification of\nvolatile financial market trends. Using a shared Long Short-Term Memory (LSTM)\nclassifier, we compare three scenarios: (i) a centralized model trained on the\nunion of all data, (ii) a single-agent model trained on an individual data\nsubset, and (iii) a privacy-preserving FL collaboration in which agents\nexchange only model updates, never raw data. We then extend the study with\nadditional market features, deliberately introducing not independent and\nidentically distributed data (non-IID) across agents, personalized FL and\nemploying differential privacy. Our numerical experiments show that FL achieves\naccuracy and generalization on par with the centralized baseline, while\nsignificantly outperforming the single-agent model. The results show that\ncollaborative, privacy-preserving learning provides collective tangible value\nin finance, even under realistic data heterogeneity and personalization\nrequirements."}
{"id": "2509.17863", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17863", "abs": "https://arxiv.org/abs/2509.17863", "authors": ["Ziming Liu", "Boyu Tian", "Guoteng Wang", "Zhen Jiang", "Peng Sun", "Zhenhua Han", "Tian Tang", "Xiaohe Hu", "Yanmin Jia", "Yan Zhang", "He Liu", "Mingjun Zhang", "Yiqi Zhang", "Qiaoling Chen", "Shenggan Cheng", "Mingyu Gao", "Yang You", "Siyuan Feng"], "title": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving", "comment": null, "summary": "Mixture-of-Experts (MoE) models challenge serving infrastructures with\ndynamic, sparse expert utilization, causing instability on conventional systems\ndesigned for dense architectures. We propose EaaS, a novel serving system to\nenable efficient, scalable, and robust MoE deployment. Our system disaggregates\nMoE modules into independent, stateless services. This design enables\nfine-grained resource scaling and provides inherent fault tolerance by\ndecoupling compute units. The architecture is powered by a high-performance,\nCPU-free peer-to-peer communication library that ensures minimal overhead and\nhigh throughput. Experiments confirm EaaS's scalability and efficiency,\nachieving performance comparable to monolithic systems while providing robust\nfault tolerance and strong scalability. EaaS incurs less than a 2% throughput\nreduction under simulated hardware failures that would otherwise halt\nmonolithic architectures. It further saves up to 37.5% of computing resources\nthrough dynamic fine-grained adaptation to serving traffic, demonstrating\nstrong resilience for large-scale MoE deployment in production."}
{"id": "2509.16397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16397", "abs": "https://arxiv.org/abs/2509.16397", "authors": ["Taqiya Ehsan", "Shuren Xia", "Jorge Ortiz"], "title": "GRID: Graph-based Reasoning for Intervention and Discovery in Built Environments", "comment": null, "summary": "Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per\nincident and achieves only 60 percent diagnostic accuracy, reflecting analytics\nthat stop at correlation instead of causation. To close this gap, we present\nGRID (Graph-based Reasoning for Intervention and Discovery), a three-stage\ncausal discovery pipeline that combines constraint-based search, neural\nstructural equation modeling, and language model priors to recover directed\nacyclic graphs from building sensor data. Across six benchmarks: synthetic\nrooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset,\nand a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00,\nwith exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden,\nPhysical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86\nin noisy conditions). The method outperforms ten baseline approaches across all\nevaluation scenarios. Intervention scheduling achieves low operational impact\nin most scenarios (cost <= 0.026) while reducing risk metrics compared to\nbaseline approaches. The framework integrates constraint-based methods, neural\narchitectures, and domain-specific language model prompts to address the\nobservational-causal gap in building analytics."}
{"id": "2509.17914", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17914", "abs": "https://arxiv.org/abs/2509.17914", "authors": ["Marcin Copik", "Eiman Alnuaimi", "Alok Kamatar", "Valerie Hayot-Sasson", "Alberto Madonna", "Todd Gamblin", "Kyle Chard", "Ian Foster", "Torsten Hoefler"], "title": "XaaS Containers: Performance-Portable Representation With Source and IR Containers", "comment": "Accepted at the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC'25)", "summary": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds."}
{"id": "2509.16447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16447", "abs": "https://arxiv.org/abs/2509.16447", "authors": ["Arwen Bradley"], "title": "Local Mechanisms of Compositional Generalization in Conditional Diffusion", "comment": "10 pages, 7 figures", "summary": "Conditional diffusion models appear capable of compositional generalization,\ni.e., generating convincing samples for out-of-distribution combinations of\nconditioners, but the mechanisms underlying this ability remain unclear. To\nmake this concrete, we study length generalization, the ability to generate\nimages with more objects than seen during training. In a controlled CLEVR\nsetting (Johnson et al., 2017), we find that length generalization is\nachievable in some cases but not others, suggesting that models only sometimes\nlearn the underlying compositional structure. We then investigate locality as a\nstructural mechanism for compositional generalization. Prior works proposed\nscore locality as a mechanism for creativity in unconditional diffusion models\n(Kamb & Ganguli, 2024; Niedoba et al., 2024), but did not address flexible\nconditioning or compositional generalization. In this paper, we prove an exact\nequivalence between a specific compositional structure (\"conditional projective\ncomposition\") (Bradley et al., 2025) and scores with sparse dependencies on\nboth pixels and conditioners (\"local conditional scores\"). This theory also\nextends to feature-space compositionality. We validate our theory empirically:\nCLEVR models that succeed at length generalization exhibit local conditional\nscores, while those that fail do not. Furthermore, we show that a causal\nintervention explicitly enforcing local conditional scores restores length\ngeneralization in a previously failing model. Finally, we investigate\nfeature-space compositionality in color-conditioned CLEVR, and find preliminary\nevidence of compositional structure in SDXL."}
{"id": "2509.16215", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16215", "abs": "https://arxiv.org/abs/2509.16215", "authors": ["Izavan dos S. Correia", "Henrique C. T. Santos", "Tiago A. E. Ferreira"], "title": "Discovering Software Parallelization Points Using Deep Neural Networks", "comment": "17 pages, 10 figures", "summary": "This study proposes a deep learning-based approach for discovering loops in\nprogramming code according to their potential for parallelization. Two genetic\nalgorithm-based code generators were developed to produce two distinct types of\ncode: (i) independent loops, which are parallelizable, and (ii) ambiguous\nloops, whose dependencies are unclear, making them impossible to define if the\nloop is parallelizable or not. The generated code snippets were tokenized and\npreprocessed to ensure a robust dataset. Two deep learning models - a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN) - were\nimplemented to perform the classification. Based on 30 independent runs, a\nrobust statistical analysis was employed to verify the expected performance of\nboth models, DNN and CNN. The CNN showed a slightly higher mean performance,\nbut the two models had a similar variability. Experiments with varying dataset\nsizes highlighted the importance of data diversity for model performance. These\nresults demonstrate the feasibility of using deep learning to automate the\nidentification of parallelizable structures in code, offering a promising tool\nfor software optimization and performance improvement."}
{"id": "2509.16463", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16463", "abs": "https://arxiv.org/abs/2509.16463", "authors": ["Spencer Compton", "Kristjan Greenewald", "Dmitriy Katz", "Murat Kocaoglu"], "title": "Entropic Causal Inference: Graph Identifiability", "comment": "Presented at ICML 2022. This version corrects a bug in semi-synthetic\n  experiments", "summary": "Entropic causal inference is a recent framework for learning the causal graph\nbetween two variables from observational data by finding the\ninformation-theoretically simplest structural explanation of the data, i.e.,\nthe model with smallest entropy. In our work, we first extend the causal graph\nidentifiability result in the two-variable setting under relaxed assumptions.\nWe then show the first identifiability result using the entropic approach for\nlearning causal graphs with more than two nodes. Our approach utilizes the\nproperty that ancestrality between a source node and its descendants can be\ndetermined using the bivariate entropic tests. We provide a sound sequential\npeeling algorithm for general graphs that relies on this property. We also\npropose a heuristic algorithm for small graphs that shows strong empirical\nperformance. We rigorously evaluate the performance of our algorithms on\nsynthetic data generated from a variety of models, observing improvement over\nprior work. Finally we test our algorithms on real-world datasets."}
{"id": "2509.16293", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16293", "abs": "https://arxiv.org/abs/2509.16293", "authors": ["Borui Wan", "Gaohong Liu", "Zuquan Song", "Jun Wang", "Yun Zhang", "Guangming Sheng", "Shuguang Wang", "Houmin Wei", "Chenyuan Wang", "Weiqiang Lou", "Xi Yang", "Mofan Zhang", "Kaihua Jiang", "Cheng Ren", "Xiaoyun Zhi", "Menghan Yu", "Zhe Nan", "Zhuolin Zheng", "Baoquan Zhong", "Qinlong Wang", "Huan Yu", "Jinxin Chi", "Wang Zhang", "Yuhan Li", "Zixian Du", "Sida Zhao", "Yongqiang Zhang", "Jingzhe Tang", "Zherui Liu", "Chuan Wu", "Yanghua Peng", "Haibin Lin", "Wencong Xiao", "Xin Liu", "Liang Xiang"], "title": "Robust LLM Training Infrastructure at ByteDance", "comment": null, "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."}
{"id": "2509.16475", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16475", "abs": "https://arxiv.org/abs/2509.16475", "authors": ["Tianchun Li", "Tianci Liu", "Xingchen Wang", "Rongzhe Wei", "Pan Li", "Lu Su", "Jing Gao"], "title": "Towards Universal Debiasing for Language Models-based Tabular Data Generation", "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) have achieved promising results in tabular data\ngeneration. However, inherent historical biases in tabular datasets often cause\nLLMs to exacerbate fairness issues, particularly when multiple advantaged and\nprotected features are involved. In this work, we introduce a universal\ndebiasing framework that minimizes group-level dependencies by simultaneously\nreducing the mutual information between advantaged and protected attributes. By\nleveraging the autoregressive structure and analytic sampling distributions of\nLLM-based tabular data generators, our approach efficiently computes mutual\ninformation, reducing the need for cumbersome numerical estimations. Building\non this foundation, we propose two complementary methods: a direct preference\noptimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly\nwith existing models, and a targeted debiasing technique, namely UDF-MIX, that\nachieves debiasing without tuning the parameters of LLMs. Extensive experiments\ndemonstrate that our framework effectively balances fairness and utility,\noffering a scalable and practical solution for debiasing in high-stakes\napplications."}
{"id": "2509.17695", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17695", "abs": "https://arxiv.org/abs/2509.17695", "authors": ["Leszek Sliwko"], "title": "Cluster Workload Allocation: A Predictive Approach Leveraging Machine Learning Efficiency", "comment": "This is the accepted version of the paper published in IEEE Access.\n  The final version is available at:\n  https://doi.org/10.1109/ACCESS.2024.3520422", "summary": "This research investigates how Machine Learning (ML) algorithms can assist in\nworkload allocation strategies by detecting tasks with node affinity operators\n(referred to as constraint operators), which constrain their execution to a\nlimited number of nodes. Using real-world Google Cluster Data (GCD) workload\ntraces and the AGOCS framework, the study extracts node attributes and task\nconstraints, then analyses them to identify suitable node-task pairings. It\nfocuses on tasks that can be executed on either a single node or fewer than a\nthousand out of 12.5k nodes in the analysed GCD cluster. Task constraint\noperators are compacted, pre-processed with one-hot encoding, and used as\nfeatures in a training dataset. Various ML classifiers, including Artificial\nNeural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge\nRegression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for\naccuracy and F1-scores. The final ensemble voting classifier model achieved 98%\naccuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable\nnode."}
{"id": "2509.16490", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16490", "abs": "https://arxiv.org/abs/2509.16490", "authors": ["Ziyao Cui", "Erick Jiang", "Nicholas Sortisio", "Haiyan Wang", "Eric Chen", "Cynthia Rudin"], "title": "Revisiting Broken Windows Theory", "comment": null, "summary": "We revisit the longstanding question of how physical structures in urban\nlandscapes influence crime. Leveraging machine learning-based matching\ntechniques to control for demographic composition, we estimate the effects of\nseveral types of urban structures on the incidence of violent crime in New York\nCity and Chicago. We additionally contribute to a growing body of literature\ndocumenting the relationship between perception of crime and actual crime rates\nby separately analyzing how the physical urban landscape shapes subjective\nfeelings of safety. Our results are twofold. First, in consensus with prior\nwork, we demonstrate a \"broken windows\" effect in which abandoned buildings, a\nsign of social disorder, are associated with both greater incidence of crime\nand a heightened perception of danger. This is also true of types of urban\nstructures that draw foot traffic such as public transportation infrastructure.\nSecond, these effects are not uniform within or across cities. The criminogenic\neffects of the same structure types across two cities differ in magnitude,\ndegree of spatial localization, and heterogeneity across subgroups, while\nwithin the same city, the effects of different structure types are confounded\nby different demographic variables. Taken together, these results emphasize\nthat one-size-fits-all approaches to crime reduction are untenable and policy\ninterventions must be specifically tailored to their targets."}
{"id": "2509.16491", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16491", "abs": "https://arxiv.org/abs/2509.16491", "authors": ["Lovely Yeswanth Panchumarthi", "Saurabh Kataria", "Yi Wu", "Xiao Hu", "Alex Fedorov", "Hyunjung Gloria Kwak"], "title": "FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG", "comment": null, "summary": "Foundation models pretrained on physiological data such as\nphotoplethysmography (PPG) signals are increasingly used to improve heart rate\n(HR) prediction across diverse settings. Fine-tuning these models for local\ndeployment is often seen as a practical and scalable strategy. However, its\nimpact on demographic fairness particularly under domain shifts remains\nunderexplored. We fine-tune PPG-GPT a transformer-based foundation model\npretrained on intensive care unit (ICU) data across three heterogeneous\ndatasets (ICU, wearable, smartphone) and systematically evaluate the effects on\nHR prediction accuracy and gender fairness. While fine-tuning substantially\nreduces mean absolute error (up to 80%), it can simultaneously widen fairness\ngaps, especially in larger models and under significant distributional\ncharacteristics shifts. To address this, we introduce FairTune, a bias-aware\nfine-tuning framework in which we benchmark three mitigation strategies: class\nweighting based on inverse group frequency (IF), Group Distributionally Robust\nOptimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and\nGroupDRO significantly reduce fairness gaps without compromising accuracy, with\neffectiveness varying by deployment domain. Representation analyses further\nreveal that mitigation techniques reshape internal embeddings to reduce\ndemographic clustering. Our findings highlight that fairness does not emerge as\na natural byproduct of fine-tuning and that explicit mitigation is essential\nfor equitable deployment of physiological foundation models."}
{"id": "2509.16499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16499", "abs": "https://arxiv.org/abs/2509.16499", "authors": ["Lianghe Shi", "Meng Wu", "Huijie Zhang", "Zekai Zhang", "Molei Tao", "Qing Qu"], "title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective", "comment": "NeurIPS 2025 Spotlight paper", "summary": "The widespread use of diffusion models has led to an abundance of\nAI-generated data, raising concerns about model collapse -- a phenomenon in\nwhich recursive iterations of training on synthetic data lead to performance\ndegradation. Prior work primarily characterizes this collapse via variance\nshrinkage or distribution shift, but these perspectives miss practical\nmanifestations of model collapse. This paper identifies a transition from\ngeneralization to memorization during model collapse in diffusion models, where\nmodels increasingly replicate training data instead of generating novel content\nduring iterative training on synthetic samples. This transition is directly\ndriven by the declining entropy of the synthetic training data produced in each\ntraining cycle, which serves as a clear indicator of model degradation.\nMotivated by this insight, we propose an entropy-based data selection strategy\nto mitigate the transition from generalization to memorization and alleviate\nmodel collapse. Empirical results show that our approach significantly enhances\nvisual quality and diversity in recursive generation, effectively preventing\ncollapse."}
{"id": "2509.16502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16502", "abs": "https://arxiv.org/abs/2509.16502", "authors": ["Jialin Chen", "Houyu Zhang", "Seongjun Yun", "Alejandro Mottini", "Rex Ying", "Xiang Song", "Vassilis N. Ioannidis", "Zheng Li", "Qingjun Cui"], "title": "GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly mitigated the\nhallucinations of Large Language Models (LLMs) by grounding the generation with\nexternal knowledge. Recent extensions of RAG to graph-based retrieval offer a\npromising direction, leveraging the structural knowledge for multi-hop\nreasoning. However, existing graph RAG typically decouples retrieval and\nreasoning processes, which prevents the retriever from adapting to the\nreasoning needs of the LLM. They also struggle with scalability when performing\nmulti-hop expansion over large-scale graphs, or depend heavily on annotated\nground-truth entities, which are often unavailable in open-domain settings. To\naddress these challenges, we propose a novel graph retriever trained end-to-end\nwith LLM, which features an attention-based growing and pruning mechanism,\nadaptively navigating multi-hop relevant entities while filtering out noise.\nWithin the extracted subgraph, structural knowledge and semantic features are\nencoded via soft tokens and the verbalized graph, respectively, which are\ninfused into the LLM together, thereby enhancing its reasoning capability and\nfacilitating interactive joint training of the graph retriever and the LLM\nreasoner. Experimental results across three QA benchmarks show that our\napproach consistently achieves state-of-the-art performance, validating the\nstrength of joint graph-LLM optimization for complex reasoning tasks. Notably,\nour framework eliminates the need for predefined ground-truth entities by\ndirectly optimizing the retriever using LLM logits as implicit feedback, making\nit especially effective in open-domain settings."}
{"id": "2509.16508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16508", "abs": "https://arxiv.org/abs/2509.16508", "authors": ["Marijan Fofonjka", "Shahryar Zehtabi", "Alireza Behtash", "Tyler Mauer", "David Stout"], "title": "Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever", "comment": "22 pages, 7 figures, 3 tables", "summary": "When existing retrieval-augmented generation (RAG) solutions are intended to\nbe used for new knowledge domains, it is necessary to update their encoders,\nwhich are taken to be pretrained large language models (LLMs). However, fully\nfinetuning these large models is compute- and memory-intensive, and even\ninfeasible when deployed on resource-constrained edge devices. We propose a\nnovel encoder architecture in this work that addresses this limitation by using\na frozen small language model (SLM), which satisfies the memory constraints of\nedge devices, and inserting a small adapter network before the transformer\nblocks of the SLM. The trainable adapter takes the token embeddings of the new\ncorpus and learns to produce enhanced soft embeddings for it, while requiring\nsignificantly less compute power to update than full fine-tuning. We further\npropose a novel retrieval mechanism by attaching a classifier head to the SLM\nencoder, which is trained to learn a similarity mapping of the input embeddings\nto their corresponding documents. Finally, to enable the online fine-tuning of\nboth (i) the encoder soft embeddings and (ii) the classifier-as-retriever on\nedge devices, we adopt federated learning (FL) and differential privacy (DP) to\nachieve an efficient, privacy-preserving, and product-grade training solution.\nWe conduct a theoretical analysis of our methodology, establishing convergence\nguarantees under mild assumptions on gradient variance when deployed for\ngeneral smooth nonconvex loss functions. Through extensive numerical\nexperiments, we demonstrate (i) the efficacy of obtaining soft embeddings to\nenhance the encoder, (ii) training a classifier to improve the retriever, and\n(iii) the role of FL in achieving speedup."}
{"id": "2509.16516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16516", "abs": "https://arxiv.org/abs/2509.16516", "authors": ["Md Mezbaur Rahman", "Cornelia Caragea"], "title": "LLM-Guided Co-Training for Text Classification", "comment": null, "summary": "In this paper, we introduce a novel weighted co-training approach that is\nguided by Large Language Models (LLMs). Namely, in our co-training approach, we\nuse LLM labels on unlabeled data as target labels and co-train two encoder-only\nbased networks that train each other over multiple iterations: first, all\nsamples are forwarded through each network and historical estimates of each\nnetwork's confidence in the LLM label are recorded; second, a dynamic\nimportance weight is derived for each sample according to each network's belief\nin the quality of the LLM label for that sample; finally, the two networks\nexchange importance weights with each other -- each network back-propagates all\nsamples weighted with the importance weights coming from its peer network and\nupdates its own parameters. By strategically utilizing LLM-generated guidance,\nour approach significantly outperforms conventional SSL methods, particularly\nin settings with abundant unlabeled data. Empirical results show that it\nachieves state-of-the-art performance on 4 out of 5 benchmark datasets and\nranks first among 14 compared methods according to the Friedman test. Our\nresults highlight a new direction in semi-supervised learning -- where LLMs\nserve as knowledge amplifiers, enabling backbone co-training models to achieve\nstate-of-the-art performance efficiently."}
{"id": "2509.16521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16521", "abs": "https://arxiv.org/abs/2509.16521", "authors": ["Yifan Yan", "Shuai Yang", "Xiuzhen Guo", "Xiangguang Wang", "Wei Chow", "Yuanchao Shu", "Shibo He"], "title": "mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding", "comment": "Accepted to ACM MobiHoc '25", "summary": "Millimeter-wave (mmWave) sensing technology holds significant value in\nhuman-centric applications, yet the high costs associated with data acquisition\nand annotation limit its widespread adoption in our daily lives. Concurrently,\nthe rapid evolution of large language models (LLMs) has opened up opportunities\nfor addressing complex human needs. This paper presents mmExpert, an innovative\nmmWave understanding framework consisting of a data generation flywheel that\nleverages LLMs to automate the generation of synthetic mmWave radar datasets\nfor specific application scenarios, thereby training models capable of\nzero-shot generalization in real-world environments. Extensive experiments\ndemonstrate that the data synthesized by mmExpert significantly enhances the\nperformance of downstream models and facilitates the successful deployment of\nlarge models for mmWave understanding."}
{"id": "2509.16548", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16548", "abs": "https://arxiv.org/abs/2509.16548", "authors": ["Yuyang Ding", "Xinyu Shi", "Juntao Li", "Xiaobo Liang", "Zhaopeng Tu", "Min Zhang"], "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning", "comment": "NeurIPS 2025. Project page: https://scan-prm.github.io/", "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training."}
{"id": "2509.16554", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16554", "abs": "https://arxiv.org/abs/2509.16554", "authors": ["Vahid Jebraeeli", "Hamid Krim", "Derya Cansever"], "title": "ViTCAE: ViT-based Class-conditioned Autoencoder", "comment": "-", "summary": "Vision Transformer (ViT) based autoencoders often underutilize the global\nClass token and employ static attention mechanisms, limiting both generative\ncontrol and optimization efficiency. This paper introduces ViTCAE, a framework\nthat addresses these issues by re-purposing the Class token into a generative\nlinchpin. In our architecture, the encoder maps the Class token to a global\nlatent variable that dictates the prior distribution for local, patch-level\nlatent variables, establishing a robust dependency where global semantics\ndirectly inform the synthesis of local details. Drawing inspiration from\nopinion dynamics, we treat each attention head as a dynamical system of\ninteracting tokens seeking consensus. This perspective motivates a\nconvergence-aware temperature scheduler that adaptively anneals each head's\ninfluence function based on its distributional stability. This process enables\na principled head-freezing mechanism, guided by theoretically-grounded\ndiagnostics like an attention evolution distance and a consensus/cluster\nfunctional. This technique prunes converged heads during training to\nsignificantly improve computational efficiency without sacrificing fidelity. By\nunifying a generative Class token with an adaptive attention mechanism rooted\nin multi-agent consensus theory, ViTCAE offers a more efficient and\ncontrollable approach to transformer-based generation."}
{"id": "2509.16577", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16577", "abs": "https://arxiv.org/abs/2509.16577", "authors": ["Antonio Tarizzo", "Mohammad Kazemi", "Deniz G√ºnd√ºz"], "title": "Learned Digital Codes for Over-the-Air Federated Learning", "comment": null, "summary": "Federated edge learning (FEEL) enables distributed model training across\nwireless devices without centralising raw data, but deployment is constrained\nby the wireless uplink. A promising direction is over-the-air (OTA)\naggregation, which merges communication with computation. Existing digital OTA\nmethods can achieve either strong convergence or robustness to noise, but\nstruggle to achieve both simultaneously, limiting performance in low\nsignal-to-noise ratios (SNRs) where many IoT devices operate. This work\nproposes a learnt digital OTA framework that extends reliable operation into\nlow-SNR conditions while maintaining the same uplink overhead as\nstate-of-the-art. The proposed method combines an unrolled decoder with a\njointly learnt unsourced random access codebook. Results show an extension of\nreliable operation by more than 7 dB, with improved global model convergence\nacross all SNR levels, highlighting the potential of learning-based design for\nFEEL."}
{"id": "2509.16586", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16586", "abs": "https://arxiv.org/abs/2509.16586", "authors": ["Yukuan Wei", "Xudong Li", "Lin F. Yang"], "title": "Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs", "comment": null, "summary": "Recent advances have significantly improved our understanding of the sample\ncomplexity of learning in average-reward Markov decision processes (AMDPs)\nunder the generative model. However, much less is known about the constrained\naverage-reward MDP (CAMDP), where policies must satisfy long-run average\nconstraints. In this work, we address this gap by studying the sample\ncomplexity of learning an $\\epsilon$-optimal policy in CAMDPs under a\ngenerative model. We propose a model-based algorithm that operates under two\nsettings: (i) relaxed feasibility, which allows small constraint violations,\nand (ii) strict feasibility, where the output policy satisfies the constraint.\nWe show that our algorithm achieves sample complexities of\n$\\tilde{O}\\left(\\frac{S A (B+H)}{ \\epsilon^2}\\right)$ and $\\tilde{O}\n\\left(\\frac{S A (B+H)}{\\epsilon^2 \\zeta^2} \\right)$ under the relaxed and\nstrict feasibility settings, respectively. Here, $\\zeta$ is the Slater constant\nindicating the size of the feasible region, $H$ is the span bound of the bias\nfunction, and $B$ is the transient time bound. Moreover, a matching lower bound\nof $\\tilde{\\Omega}\\left(\\frac{S A (B+H)}{ \\epsilon^2\\zeta^2}\\right)$ for the\nstrict feasibility case is established, thus providing the first\nminimax-optimal bounds for CAMDPs. Our results close the theoretical gap in\nunderstanding the complexity of constrained average-reward MDPs."}
{"id": "2509.16625", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16625", "abs": "https://arxiv.org/abs/2509.16625", "authors": ["Lorenzo Guerra", "Thomas Chapuis", "Guillaume Duc", "Pavlo Mozharovskyi", "Van-Tam Nguyen"], "title": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection", "comment": "Accepted at NeurIPS 2025", "summary": "Detecting intrusions in network traffic is a challenging task, particularly\nunder limited supervision and constantly evolving attack patterns. While recent\nworks have leveraged graph neural networks for network intrusion detection,\nthey often decouple representation learning from anomaly detection, limiting\nthe utility of the embeddings for identifying attacks. We propose GraphIDS, a\nself-supervised intrusion detection model that unifies these two stages by\nlearning local graph representations of normal communication patterns through a\nmasked autoencoder. An inductive graph neural network embeds each flow with its\nlocal topological context to capture typical network behavior, while a\nTransformer-based encoder-decoder reconstructs these embeddings, implicitly\nlearning global co-occurrence patterns via self-attention without requiring\nexplicit positional information. During inference, flows with unusually high\nreconstruction errors are flagged as potential intrusions. This end-to-end\nframework ensures that embeddings are directly optimized for the downstream\ntask, facilitating the recognition of malicious traffic. On diverse NetFlow\nbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,\noutperforming baselines by 5-25 percentage points."}
{"id": "2509.16629", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.16629", "abs": "https://arxiv.org/abs/2509.16629", "authors": ["Kaichen Xu", "Yihang Du", "Mianpeng Liu", "Zimu Yu", "Xiaobo Sun"], "title": "Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features", "comment": "Accepted by NeurIPS 2025", "summary": "Positional encoding is essential for supplementing transformer with\npositional information of tokens. Existing positional encoding methods demand\npredefined token/feature order, rendering them unsuitable for real-world data\nwith non-sequential yet causally-related features. To address this limitation,\nwe propose CAPE, a novel method that identifies underlying causal structure\nover non-sequential features as a weighted directed acyclic graph (DAG) using\ngeneralized structural equation modeling. The DAG is then embedded in\nhyperbolic space where its geometric structure is well-preserved using a\nhyperboloid model-based approach that effectively captures two important causal\ngraph properties (causal strength & causal specificity). This step yields\ncausality-aware positional encodings for the features, which are converted into\ntheir rotary form for integrating with transformer's self-attention mechanism.\nTheoretical analysis reveals that CAPE-generated rotary positional encodings\npossess three valuable properties for enhanced self-attention, including causal\ndistance-induced attenuation, causal generality-induced attenuation, and\nrobustness to positional disturbances. We evaluate CAPE over both synthetic and\nreal-word datasets, empirically demonstrating its theoretical properties and\neffectiveness in enhancing transformer for data with non-sequential features.\nOur code is available at https://github.com/Catchxu/CAPE."}
{"id": "2509.16664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16664", "abs": "https://arxiv.org/abs/2509.16664", "authors": ["Simone Ricci", "Niccol√≤ Biondi", "Federico Pernici", "Ioannis Patras", "Alberto Del Bimbo"], "title": "$\\boldsymbolŒª$-Orthogonality Regularization for Compatible Representation Learning", "comment": "Accepted at NeurIPS2025", "summary": "Retrieval systems rely on representations learned by increasingly powerful\nmodels. However, due to the high training cost and inconsistencies in learned\nrepresentations, there is significant interest in facilitating communication\nbetween representations and ensuring compatibility across independently trained\nneural networks. In the literature, two primary approaches are commonly used to\nadapt different learned representations: affine transformations, which adapt\nwell to specific distributions but can significantly alter the original\nrepresentation, and orthogonal transformations, which preserve the original\nstructure with strict geometric constraints but limit adaptability. A key\nchallenge is adapting the latent spaces of updated models to align with those\nof previous models on downstream distributions while preserving the newly\nlearned representation spaces. In this paper, we impose a relaxed orthogonality\nconstraint, namely $\\lambda$-orthogonality regularization, while learning an\naffine transformation, to obtain distribution-specific adaptation while\nretaining the original learned representations. Extensive experiments across\nvarious architectures and datasets validate our approach, demonstrating that it\npreserves the model's zero-shot performance and ensures compatibility across\nmodel updates. Code available at:\nhttps://github.com/miccunifi/lambda_orthogonality"}
{"id": "2509.16709", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16709", "abs": "https://arxiv.org/abs/2509.16709", "authors": ["Nicol√≤ Botteghi", "Matteo Tomasetto", "Urban Fasel", "Francesco Braghin", "Andrea Manzoni"], "title": "HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems", "comment": null, "summary": "Deep reinforcement learning has recently emerged as a promising feedback\ncontrol strategy for complex dynamical systems governed by partial differential\nequations (PDEs). When dealing with distributed, high-dimensional problems in\nstate and control variables, multi-agent reinforcement learning (MARL) has been\nproposed as a scalable approach for breaking the curse of dimensionality. In\nparticular, through decentralized training and execution, multiple agents\ncooperate to steer the system towards a target configuration, relying solely on\nlocal state and reward information. However, the principle of locality may\nbecome a limiting factor whenever a collective, nonlocal behavior of the agents\nis crucial to maximize the reward function, as typically happens in\nPDE-constrained optimal control problems. In this work, we propose HypeMARL: a\ndecentralized MARL algorithm tailored to the control of high-dimensional,\nparametric, and distributed systems. HypeMARL employs hypernetworks to\neffectively parametrize the agents' policies and value functions with respect\nto the system parameters and the agents' relative positions, encoded by\nsinusoidal positional encoding. Through the application on challenging control\nproblems, such as density and flow control, we show that HypeMARL (i) can\neffectively control systems through a collective behavior of the agents,\noutperforming state-of-the-art decentralized MARL, (ii) can efficiently deal\nwith parametric dependencies, (iii) requires minimal hyperparameter tuning and\n(iv) can reduce the amount of expensive environment interactions by a factor of\n~10 thanks to its model-based extension, MB-HypeMARL, which relies on\ncomputationally efficient deep learning-based surrogate models approximating\nthe dynamics locally, with minimal deterioration of the policy performance."}
{"id": "2509.16743", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16743", "abs": "https://arxiv.org/abs/2509.16743", "authors": ["Subhabrata Das", "Bodruzzaman Khan", "Xiao-Yang Liu"], "title": "A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction", "comment": null, "summary": "Accurately forecasting power outages is a complex task influenced by diverse\nfactors such as weather conditions [1], vegetation, wildlife, and load\nfluctuations. These factors introduce substantial variability and noise into\noutage data, making reliable prediction challenging. Long Short-Term Memory\n(LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly\neffective for modeling nonlinear and dynamic time-series data, with proven\napplications in stock price forecasting [2], energy demand prediction, demand\nresponse [3], and traffic flow management [4]. This paper introduces a hybrid\ndeep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates\nPrincipal Component Analysis (PCA), Poisson Regression (PR), a\nSequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is\nemployed to reduce dimensionality and stabilize data variance, while Poisson\nRegression effectively models discrete outage events. The Seq2Seq-Adam-LSTM\ncomponent enhances temporal feature learning through efficient gradient\noptimization and long-term dependency capture. The framework is evaluated using\nreal-world outage records from Michigan, and results indicate that the proposed\napproach significantly improves forecasting accuracy and robustness compared to\nexisting methods."}
{"id": "2509.16750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16750", "abs": "https://arxiv.org/abs/2509.16750", "authors": ["Alejandro Almod√≥var", "Patricia A. Apell√°niz", "Alba Garrido", "Fernando Fern√°ndez-Salvador", "Santiago Zazo", "Juan Parras"], "title": "Interpretable Clinical Classification with Kolgomorov-Arnold Networks", "comment": null, "summary": "Why should a clinician trust an Artificial Intelligence (AI) prediction?\nDespite the increasing accuracy of machine learning methods in medicine, the\nlack of transparency continues to hinder their adoption in clinical practice.\nIn this work, we explore Kolmogorov-Arnold Networks (KANs) for clinical\nclassification tasks on tabular data. Unlike traditional neural networks, KANs\nare function-based architectures that offer intrinsic interpretability through\ntransparent, symbolic representations. We introduce Logistic-KAN, a flexible\ngeneralization of logistic regression, and Kolmogorov-Arnold Additive Model\n(KAAM), a simplified additive variant that delivers transparent, symbolic\nformulas. Unlike black-box models that require post-hoc explainability tools,\nour models support built-in patient-level insights, intuitive visualizations,\nand nearest-patient retrieval. Across multiple health datasets, our models\nmatch or outperform standard baselines, while remaining fully interpretable.\nThese results position KANs as a promising step toward trustworthy AI that\nclinicians can understand, audit, and act upon."}
{"id": "2509.16756", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16756", "abs": "https://arxiv.org/abs/2509.16756", "authors": ["Yuchen Liang", "Yingbin Liang", "Lifeng Lai", "Ness Shroff"], "title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees", "comment": null, "summary": "Discrete diffusion models have recently gained significant prominence in\napplications involving natural language and graph data. A key factor\ninfluencing their effectiveness is the efficiency of discretized samplers.\nAmong these, $\\tau$-leaping samplers have become particularly popular due to\ntheir empirical success. However, existing theoretical analyses of\n$\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify\nregularity assumptions, and their convergence bounds contain quadratic\ndependence on the vocabulary size. In this work, we introduce a new analytical\napproach for discrete diffusion models that removes the need for such\nassumptions. For the standard $\\tau$-leaping method, we establish convergence\nguarantees in KL divergence that scale linearly with vocabulary size, improving\nupon prior results with quadratic dependence. Our approach is also more broadly\napplicable: it provides the first convergence guarantees for other widely used\nsamplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our\napproach is a novel technique based on differential inequalities, offering a\nmore flexible alternative to the traditional Girsanov change-of-measure\nmethods. This technique may also be of independent interest for the analysis of\nother stochastic processes."}
{"id": "2509.16769", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 62H30, 62M45", "I.2.6; I.5.1; I.5.2; G.3"], "pdf": "https://arxiv.org/pdf/2509.16769", "abs": "https://arxiv.org/abs/2509.16769", "authors": ["Prasanth K K", "Shubham Sharma"], "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes", "comment": "21 pages, 6 figures, 14 tables", "summary": "Many real world categories are multimodal, with single classes occupying\ndisjoint regions in feature space. Classical linear models (logistic\nregression, linear SVM) use a single global hyperplane and perform poorly on\nsuch data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal\nstructure but at the expense of interpretability, heavier tuning, and higher\ncomputational cost. We propose the Geometric Mixture Classifier (GMC), a\ndiscriminative model that represents each class as a mixture of hyperplanes.\nWithin each class, GMC combines plane scores via a temperature-controlled\nsoft-OR (log-sum-exp), smoothly approximating the max; across classes, standard\nsoftmax yields probabilistic posteriors. GMC optionally uses Random Fourier\nFeatures (RFF) for nonlinear mappings while keeping inference linear in the\nnumber of planes and features. Our practical training recipe: geometry-aware\nk-means initialization, silhouette-based plane budgeting, alpha annealing,\nusage-aware L2 regularization, label smoothing, and early stopping, makes GMC\nplug-and-play. Across synthetic multimodal datasets (moons, circles, blobs,\nspirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC\nconsistently outperforms linear baselines and k-NN, is competitive with\nRBF-SVM, Random Forests, and small MLPs, and provides geometric introspection\nvia per-plane and class responsibility visualizations. Inference scales\nlinearly in planes and features, making GMC CPU-friendly, with single-digit\nmicrosecond latency per example, often faster than RBF-SVM and compact MLPs.\nPost-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus\nstrikes a favorable balance of accuracy, interpretability, and efficiency: it\nis more expressive than linear models and lighter, more transparent, and faster\nthan kernel or deep models."}
{"id": "2509.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16820", "abs": "https://arxiv.org/abs/2509.16820", "authors": ["Max Torop", "Aria Masoomi", "Masih Eskandar", "Jennifer Dy"], "title": "DISCO: Disentangled Communication Steering for Large Language Models", "comment": null, "summary": "A variety of recent methods guide large language model outputs via the\ninference-time addition of steering vectors to residual-stream or\nattention-head representations. In contrast, we propose to inject steering\nvectors directly into the query and value representation spaces within\nattention heads. We provide evidence that a greater portion of these spaces\nexhibit high linear discriminability of concepts --a key property motivating\nthe use of steering vectors-- than attention head outputs. We analytically\ncharacterize the effect of our method, which we term DISentangled COmmunication\n(DISCO) Steering, on attention head outputs. Our analysis reveals that DISCO\ndisentangles a strong but underutilized baseline, steering attention inputs,\nwhich implicitly modifies queries and values in a rigid manner. In contrast,\nDISCO's direct modulation of these components enables more granular control. We\nfind that DISCO achieves superior performance over a number of steering vector\nbaselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with\nsteering efficacy scoring up to 19.1% higher than the runner-up. Our results\nsupport the conclusion that the query and value spaces are powerful building\nblocks for steering vector methods."}
{"id": "2509.16825", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16825", "abs": "https://arxiv.org/abs/2509.16825", "authors": ["Jin Lee", "Ziming Liu", "Xinling Yu", "Yixuan Wang", "Haewon Jeong", "Murphy Yuezhen Niu", "Zheng Zhang"], "title": "KANO: Kolmogorov-Arnold Neural Operator", "comment": null, "summary": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural\noperator jointly parameterized by both spectral and spatial bases with\nintrinsic symbolic interpretability. We theoretically demonstrate that KANO\novercomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO\nremains expressive over generic position-dependent dynamics for any physical\ninput, whereas FNO stays practical only for spectrally sparse operators and\nstrictly imposes a fast-decaying input Fourier tail. We verify our claims\nempirically on position-dependent differential operators, for which KANO\nrobustly generalizes but FNO fails to. In the quantum Hamiltonian learning\nbenchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic\nrepresentations accurate to the fourth decimal place in coefficients and\nattains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement\ndata, substantially outperforming that of the FNO trained with ideal full wave\nfunction data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude."}
{"id": "2509.16833", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16833", "abs": "https://arxiv.org/abs/2509.16833", "authors": ["Shaharyar Ahmed Khan Tareen", "Lei Fan", "Xiaojing Yuan", "Qin Lin", "Bin Hu"], "title": "SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training", "comment": "10 pages, 7 figures, 6 tables", "summary": "Once-for-All (OFA) training enables a single super-net to generate multiple\nsub-nets tailored to diverse deployment scenarios, supporting flexible\ntrade-offs among accuracy, robustness, and model-size without retraining.\nHowever, as the number of supported sub-nets increases, excessive parameter\nsharing in the backbone limits representational capacity, leading to degraded\ncalibration and reduced overall performance. To address this, we propose SOLAR\n(Switchable Output Layer for Accuracy and Robustness in Once-for-All Training),\na simple yet effective technique that assigns each sub-net a separate\nclassification head. By decoupling the logit learning process across sub-nets,\nthe Switchable Output Layer (SOL) reduces representational interference and\nimproves optimization, without altering the shared backbone. We evaluate SOLAR\non five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using\nfour super-net backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and\nMobileNetV2) for two OFA training frameworks (OATS and SNNs). Experiments show\nthat SOLAR outperforms the baseline methods: compared to OATS, it improves\naccuracy of sub-nets up to 1.26 %, 4.71 %, 1.67 %, and 1.76 %, and robustness\nup to 9.01 %, 7.71 %, 2.72 %, and 1.26 % on SVHN, CIFAR-10, STL-10, and\nCIFAR-100, respectively. Compared to SNNs, it improves TinyImageNet accuracy by\nup to 2.93 %, 2.34 %, and 1.35 % using ResNet-34, WideResNet-16-8, and\nMobileNetV2 backbones (with 8 sub-nets), respectively."}
{"id": "2509.16860", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16860", "abs": "https://arxiv.org/abs/2509.16860", "authors": ["Mohammad Abdul Hafeez Khan", "Marcello Mattei Di Eugeni", "Benjamin Diaz", "Ruth E. White", "Siddhartha Bhattacharyya", "Venkat Keshav Chivukula"], "title": "LVADNet3D: A Deep Autoencoder for Reconstructing 3D Intraventricular Flow from Sparse Hemodynamic Data", "comment": "Accepted to International Conference on Machine Learning and\n  Applications (ICMLA), 6 pages, 4 figure, 3 tables", "summary": "Accurate assessment of intraventricular blood flow is essential for\nevaluating hemodynamic conditions in patients supported by Left Ventricular\nAssist Devices (LVADs). However, clinical imaging is either incompatible with\nLVADs or yields sparse, low-quality velocity data. While Computational Fluid\nDynamics (CFD) simulations provide high-fidelity data, they are computationally\nintensive and impractical for routine clinical use. To address this, we propose\nLVADNet3D, a 3D convolutional autoencoder that reconstructs full-resolution\nintraventricular velocity fields from sparse velocity vector inputs. In\ncontrast to a standard UNet3D model, LVADNet3D incorporates hybrid downsampling\nand a deeper encoder-decoder architecture with increased channel capacity to\nbetter capture spatial flow patterns. To train and evaluate the models, we\ngenerate a high-resolution synthetic dataset of intraventricular blood flow in\nLVAD-supported hearts using CFD simulations. We also investigate the effect of\nconditioning the models on anatomical and physiological priors. Across various\ninput configurations, LVADNet3D outperforms the baseline UNet3D model, yielding\nlower reconstruction error and higher PSNR results."}
{"id": "2509.16875", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16875", "abs": "https://arxiv.org/abs/2509.16875", "authors": ["Qishuai Wen", "Zhiyuan Huang", "Chun-Guang Li"], "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few", "comment": "NeurIPS2025 Spotlight", "summary": "Attention mechanisms in Transformers have gained significant empirical\nsuccess. Nonetheless, the optimization objectives underlying their forward pass\nare still unclear. Additionally, the quadratic complexity of self-attention is\nincreasingly prohibitive. Unlike the prior work on addressing the\ninterpretability or efficiency issue separately, we propose a unified\noptimization objective to alleviate both issues simultaneously. By unrolling\nthe optimization over the objective, we derive an inherently interpretable and\nefficient attention mechanism, which compresses all tokens into low-dimensional\nstructures by contracting a few representative tokens and then broadcasting the\ncontractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism\ncan not only scale linearly but also generalize existing attention mechanisms\nas its special cases. Experiments further demonstrate comparable performance\nand even superior advantages of CBSA on several visual tasks. Code is available\nat this https URL."}
{"id": "2509.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16882", "abs": "https://arxiv.org/abs/2509.16882", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Xuming Hu"], "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation", "comment": "EMNLP 2025 Main Conference", "summary": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated\nexpert subnetworks, yet adapting them to multiple domains without catastrophic\nforgetting remains an open challenge. Existing approaches either incur\nprohibitive computation, suffer cross-domain interference, or require separate\nruns per domain. We propose DES-MoE, a dynamic expert specialization framework\nfor multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses\ncatastrophic forgetting through three innovations: (1) an adaptive router\nbalancing pre-trained knowledge retention and task-specific updates via\ndistillation, (2) real-time expert-domain correlation mapping to isolate\ndomain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule\nthat progressively freezes non-specialized parameters. Evaluated on six domains\n(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while\ntraining one unified model, reduces forgetting by 89% compared to full\nfine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence\nthan conventional methods. Our work establishes dynamic expert isolation as a\nscalable paradigm for multi-task MoE adaptation."}
{"id": "2509.16893", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16893", "abs": "https://arxiv.org/abs/2509.16893", "authors": ["Faramarz Farhangian", "Leandro A. Ensina", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "DRES: Fake news detection by dynamic representation and ensemble selection", "comment": "Accepted as oral presentation at EMNLP 2025", "summary": "The rapid spread of information via social media has made text-based fake\nnews detection critically important due to its societal impact. This paper\npresents a novel detection method called Dynamic Representation and Ensemble\nSelection (DRES) for identifying fake news based solely on text. DRES leverages\ninstance hardness measures to estimate the classification difficulty for each\nnews article across multiple textual feature representations. By dynamically\nselecting the textual representation and the most competent ensemble of\nclassifiers for each instance, DRES significantly enhances prediction accuracy.\nExtensive experiments show that DRES achieves notable improvements over\nstate-of-the-art methods, confirming the effectiveness of representation\nselection based on instance hardness and dynamic ensemble selection in boosting\nperformance. Codes and data are available at:\nhttps://github.com/FFarhangian/FakeNewsDetection_DRES"}
{"id": "2509.16898", "categories": ["cs.LG", "cs.CC", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.16898", "abs": "https://arxiv.org/abs/2509.16898", "authors": ["Jingming Yan", "Yiyuan Luo", "Vaggos Chatziafratis", "Ioannis Panageas", "Parnian Shahkar", "Stelios Stavroulakis"], "title": "The Complexity of Finding Local Optima in Contrastive Learning", "comment": "To appear as a conference paper in NeurIPS 2025", "summary": "Contrastive learning is a powerful technique for discovering meaningful data\nrepresentations by optimizing objectives based on $\\textit{contrastive\ninformation}$, often given as a set of weighted triplets $\\{(x_i, y_i^+,\nz_{i}^-)\\}_{i = 1}^m$ indicating that an \"anchor\" $x_i$ is more similar to a\n\"positive\" example $y_i$ than to a \"negative\" example $z_i$. The goal is to\nfind representations (e.g., embeddings in $\\mathbb{R}^d$ or a tree metric)\nwhere anchors are placed closer to positive than to negative examples. While\nfinding $\\textit{global}$ optima of contrastive objectives is\n$\\mathsf{NP}$-hard, the complexity of finding $\\textit{local}$ optima --\nrepresentations that do not improve by local search algorithms such as\ngradient-based methods -- remains open. Our work settles the complexity of\nfinding local optima in various contrastive learning problems by proving\n$\\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied\ntriplets) and $\\mathsf{CLS}$-hardness in continuous settings (e.g., minimize\nTriplet Loss), where $\\mathsf{PLS}$ (Polynomial Local Search) and\n$\\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes\ncapturing local search dynamics in discrete and continuous optimization,\nrespectively. Our results imply that no polynomial time algorithm (local search\nor otherwise) can find a local optimum for various contrastive learning\nproblems, unless $\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq\n\\mathsf{P}$ for continuous problems). Even in the unlikely scenario that\n$\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq \\mathsf{P}$), our\nreductions imply that there exist instances where local search algorithms need\nexponential time to reach a local optimum, even for $d=1$ (embeddings on a\nline)."}
{"id": "2509.16902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16902", "abs": "https://arxiv.org/abs/2509.16902", "authors": ["Letian Zhang", "Bo Chen", "Jieming Bian", "Lei Wang", "Jie Xu"], "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices", "comment": null, "summary": "Federated learning (FL) enables distributed devices to collaboratively train\nmachine learning models while maintaining data privacy. However, the\nheterogeneous hardware capabilities of devices often result in significant\ntraining delays, as straggler clients with limited resources prolong the\naggregation process. Existing solutions such as client selection, asynchronous\nFL, and partial training partially address these challenges but encounter\nissues such as reduced accuracy, stale updates, and compromised model\nperformance due to inconsistent training contributions. To overcome these\nlimitations, we propose FedEL, a federated elastic learning framework that\nenhances training efficiency while maintaining model accuracy. FedEL introduces\na novel window-based training process, sliding the window to locate the\ntraining part of the model and dynamically selecting important tensors for\ntraining within a coordinated runtime budget. This approach ensures progressive\nand balanced training across all clients, including stragglers. Additionally,\nFedEL employs a tensor importance adjustment module, harmonizing local and\nglobal tensor importance to mitigate biases caused by data heterogeneity. The\nexperiment results show that FedEL achieves up to 3.87x improvement in\ntime-to-accuracy compared to baselines while maintaining or exceeding final\ntest accuracy."}
{"id": "2509.16930", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16930", "abs": "https://arxiv.org/abs/2509.16930", "authors": ["Nathan Derhake", "Siddartha Devic", "Dutch Hansen", "Kuan Liu", "Vatsal Sharan"], "title": "Auditability and the Landscape of Distance to Multicalibration", "comment": "41 pages", "summary": "Calibration is a critical property for establishing the trustworthiness of\npredictors that provide uncertainty estimates. Multicalibration is a\nstrengthening of calibration which requires that predictors be calibrated on a\npotentially overlapping collection of subsets of the domain. As\nmulticalibration grows in popularity with practitioners, an essential question\nis: how do we measure how multicalibrated a predictor is? B{\\l}asiok et al.\n(2023) considered this question for standard calibration by introducing the\ndistance to calibration framework (dCE) to understand how calibration metrics\nrelate to each other and the ground truth. Building on the dCE framework, we\nconsider the auditability of the distance to multicalibration of a predictor\n$f$.\n  We begin by considering two natural generalizations of dCE to multiple\nsubgroups: worst group dCE (wdMC), and distance to multicalibration (dMC). We\nargue that there are two essential properties of any multicalibration error\nmetric: 1) the metric should capture how much $f$ would need to be modified in\norder to be perfectly multicalibrated; and 2) the metric should be auditable in\nan information theoretic sense. We show that wdMC and dMC each fail to satisfy\none of these two properties, and that similar barriers arise when considering\nthe auditability of general distance to multigroup fairness notions. We then\npropose two (equivalent) multicalibration metrics which do satisfy these\nrequirements: 1) a continuized variant of dMC; and 2) a distance to\nintersection multicalibration, which leans on intersectional fairness\ndesiderata. Along the way, we shed light on the loss-landscape of distance to\nmulticalibration and the geometry of the set of perfectly multicalibrated\npredictors. Our findings may have implications for the development of stronger\nmulticalibration algorithms as well as multigroup auditing more generally."}
{"id": "2509.16936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16936", "abs": "https://arxiv.org/abs/2509.16936", "authors": ["Cuiqianhe Du", "Chia-En Chiang", "Tianyi Huang", "Zikun Cui"], "title": "Adaptive Graph Convolution and Semantic-Guided Attention for Multimodal Risk Detection in Social Networks", "comment": null, "summary": "This paper focuses on the detection of potentially dangerous tendencies of\nsocial media users in an innovative multimodal way. We integrate Natural\nLanguage Processing (NLP) and Graph Neural Networks (GNNs) together. Firstly,\nwe apply NLP on the user-generated text and conduct semantic analysis,\nsentiment recognition and keyword extraction to get subtle risk signals from\nsocial media posts. Meanwhile, we build a heterogeneous user relationship graph\nbased on social interaction and propose a novel relational graph convolutional\nnetwork to model user relationship, attention relationship and content\ndissemination path to discover some important structural information and user\nbehaviors. Finally, we combine textual features extracted from these two models\nabove with graph structural information, which provides a more robust and\neffective way to discover at-risk users. Our experiments on real social media\ndatasets from different platforms show that our model can achieve significant\nimprovement over single-modality methods."}
{"id": "2509.16959", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16959", "abs": "https://arxiv.org/abs/2509.16959", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Gradient Interference-Aware Graph Coloring for Multitask Learning", "comment": null, "summary": "When different objectives conflict with each other in multi-task learning,\ngradients begin to interfere and slow convergence, thereby reducing the final\nmodel's performance. To address this, we introduce a scheduler that computes\ngradient interference, constructs an interference graph, and then applies\ngreedy graph-coloring to partition tasks into groups that align well with each\nother. At each training step, only one group (color class) of tasks are\nactivated. The grouping partition is constantly recomputed as task\nrelationships evolve throughout training. By ensuring that each mini-batch\ncontains only tasks that pull the model in the same direction, our method\nimproves the effectiveness of any underlying multi-task learning optimizer\nwithout additional tuning. Since tasks within these groups will update in\ncompatible directions, model performance will be improved rather than impeded.\nEmpirical results on six different datasets show that this interference-aware\ngraph-coloring approach consistently outperforms baselines and state-of-the-art\nmulti-task optimizers."}
{"id": "2509.16989", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16989", "abs": "https://arxiv.org/abs/2509.16989", "authors": ["He Xiao", "Runming Yang", "Qingyao Yang", "Wendong Xu", "Zheng Li", "Yupeng Su", "Zhengwu Liu", "Hongxia Yang", "Ngai Wong"], "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models", "comment": "under review", "summary": "Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments."}
{"id": "2509.16999", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16999", "abs": "https://arxiv.org/abs/2509.16999", "authors": ["Matteo Pegoraro"], "title": "Persistence Spheres: Bi-continuous Representations of Persistence Diagrams", "comment": null, "summary": "We introduce persistence spheres, a novel functional representation of\npersistence diagrams. Unlike existing embeddings (such as persistence images,\nlandscapes, or kernel methods), persistence spheres provide a bi-continuous\nmapping: they are Lipschitz continuous with respect to the 1-Wasserstein\ndistance and admit a continuous inverse on their image. This ensures, in a\ntheoretically optimal way, both stability and geometric fidelity, making\npersistence spheres the representation that most closely mirrors the\nWasserstein geometry of PDs in linear space. We derive explicit formulas for\npersistence spheres, showing that they can be computed efficiently and\nparallelized with minimal overhead. Empirically, we evaluate them on diverse\nregression and classification tasks involving functional data, time series,\ngraphs, meshes, and point clouds. Across these benchmarks, persistence spheres\nconsistently deliver state-of-the-art or competitive performance compared to\npersistence images, persistence landscapes, and the sliced Wasserstein kernel."}
{"id": "2509.17000", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17000", "abs": "https://arxiv.org/abs/2509.17000", "authors": ["Shuhao Jiang", "Songbo Wang", "Yang Qiao", "Chun Xu", "Chaoyang Zheng", "Shengyi Zhou", "Huanjun Wang", "Fangming Li", "Cong Zhang", "Jiyu Wang"], "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals", "comment": null, "summary": "Large Reasoning Models (LRMs) often suffer from computational inefficiency\ndue to overthinking, where a fixed reasoning budget fails to match the varying\ncomplexity of tasks. To address this issue, we propose Adaptive Overclocking, a\nmethod that makes the overclocking hyperparameter $\\alpha$ dynamic and\ncontext-aware. Our method adjusts reasoning speed in real time through two\ncomplementary signals: (1) token-level model uncertainty for fine-grained\nstep-wise control, and (2) input complexity estimation for informed\ninitialization. We implement this approach with three strategies:\nUncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha\nInitialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that\ncombines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves\nsuperior accuracy-latency trade-offs, reducing unnecessary computation on\nsimple problems while allocating more resources to challenging ones. By\nmitigating overthinking, Adaptive Overclocking enhances both efficiency and\noverall reasoning performance."}
{"id": "2509.17034", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17034", "abs": "https://arxiv.org/abs/2509.17034", "authors": ["Shuai Feng", "Yuxin Ge", "Yuntao Du", "Mingcai Chen", "Lei Feng"], "title": "Long-Tailed Out-of-Distribution Detection with Refined Separate Class Learning", "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models. However, when training data follows a long-tailed\ndistribution, the model's ability to accurately detect OOD samples is\nsignificantly compromised, due to the confusion between OOD samples and\nhead/tail classes. To distinguish OOD samples from both head and tail classes,\nthe separate class learning (SCL) approach has emerged as a promising solution,\nwhich separately conduct head-specific and tail-specific class learning. To\nthis end, we examine the limitations of existing works of SCL and reveal that\nthe OOD detection performance is notably influenced by the use of static\nscaling temperature value and the presence of uninformative outliers. To\nmitigate these limitations, we propose a novel approach termed Refined Separate\nClass Learning (RSCL), which leverages dynamic class-wise temperature\nadjustment to modulate the temperature parameter for each in-distribution class\nand informative outlier mining to identify diverse types of outliers based on\ntheir affinity with head and tail classes. Extensive experiments demonstrate\nthat RSCL achieves superior OOD detection performance while improving the\nclassification accuracy on in-distribution data."}
{"id": "2509.17051", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.17051", "abs": "https://arxiv.org/abs/2509.17051", "authors": ["Riccardo Doyle"], "title": "Enhancing Performance and Calibration in Quantile Hyperparameter Optimization", "comment": "19 pages, 15 figures, 1 table", "summary": "Bayesian hyperparameter optimization relies heavily on Gaussian Process (GP)\nsurrogates, due to robust distributional posteriors and strong performance on\nlimited training samples. GPs however underperform in categorical\nhyperparameter environments or when assumptions of normality,\nheteroskedasticity and symmetry are excessively challenged. Conformalized\nquantile regression can address these estimation weaknesses, while still\nproviding robust calibration guarantees. This study builds upon early work in\nthis area by addressing feedback covariate shift in sequential acquisition and\nintegrating a wider range of surrogate architectures and acquisition functions.\nProposed algorithms are rigorously benchmarked against a range of state of the\nart hyperparameter optimization methods (GP, TPE and SMAC). Findings identify\nquantile surrogate architectures and acquisition functions yielding superior\nperformance to the current quantile literature, while validating the beneficial\nimpact of conformalization on calibration and search performance."}
{"id": "2509.17063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17063", "abs": "https://arxiv.org/abs/2509.17063", "authors": ["Shuang Liang", "Chaochuan Hou", "Xu Yao", "Shiping Wang", "Minqi Jiang", "Songqiao Han", "Hailiang Huang"], "title": "TSGym: Design Choices for Deep Multivariate Time-Series Forecasting", "comment": null, "summary": "Recently, deep learning has driven significant advancements in multivariate\ntime series forecasting (MTSF) tasks. However, much of the current research in\nMTSF tends to evaluate models from a holistic perspective, which obscures the\nindividual contributions and leaves critical issues unaddressed. Adhering to\nthe current modeling paradigms, this work bridges these gaps by systematically\ndecomposing deep MTSF methods into their core, fine-grained components like\nseries-patching tokenization, channel-independent strategy, attention modules,\nor even Large Language Models and Time-series Foundation Models. Through\nextensive experiments and component-level analysis, our work offers more\nprofound insights than previous benchmarks that typically discuss models as a\nwhole.\n  Furthermore, we propose a novel automated solution called TSGym for MTSF\ntasks. Unlike traditional hyperparameter tuning, neural architecture searching\nor fixed model selection, TSGym performs fine-grained component selection and\nautomated model construction, which enables the creation of more effective\nsolutions tailored to diverse time series data, therefore enhancing model\ntransferability across different data sources and robustness against\ndistribution shifts. Extensive experiments indicate that TSGym significantly\noutperforms existing state-of-the-art MTSF and AutoML methods. All code is\npublicly available on https://github.com/SUFE-AILAB/TSGym."}
{"id": "2509.17092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17092", "abs": "https://arxiv.org/abs/2509.17092", "authors": ["Michelangelo Conserva", "Remo Sasso", "Paulo Rauber"], "title": "On the Limits of Tabular Hardness Metrics for Deep RL: A Study with the Pharos Benchmark", "comment": null, "summary": "Principled evaluation is critical for progress in deep reinforcement learning\n(RL), yet it lags behind the theory-driven benchmarks of tabular RL. While\ntabular settings benefit from well-understood hardness measures like MDP\ndiameter and suboptimality gaps, deep RL benchmarks are often chosen based on\nintuition and popularity. This raises a critical question: can tabular hardness\nmetrics be adapted to guide non-tabular benchmarking? We investigate this\nquestion and reveal a fundamental gap. Our primary contribution is\ndemonstrating that the difficulty of non-tabular environments is dominated by a\nfactor that tabular metrics ignore: representation hardness. The same\nunderlying MDP can pose vastly different challenges depending on whether the\nagent receives state vectors or pixel-based observations. To enable this\nanalysis, we introduce \\texttt{pharos}, a new open-source library for\nprincipled RL benchmarking that allows for systematic control over both\nenvironment structure and agent representations. Our extensive case study using\n\\texttt{pharos} shows that while tabular metrics offer some insight, they are\npoor predictors of deep RL agent performance on their own. This work highlights\nthe urgent need for new, representation-aware hardness measures and positions\n\\texttt{pharos} as a key tool for developing them."}
{"id": "2509.17095", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17095", "abs": "https://arxiv.org/abs/2509.17095", "authors": ["Jinbao Wang", "Jun Liu", "Shiliang Zhang", "Xuehui Ma"], "title": "Ultra-short-term solar power forecasting by deep learning and data reconstruction", "comment": null, "summary": "The integration of solar power has been increasing as the green energy\ntransition rolls out. The penetration of solar power challenges the grid\nstability and energy scheduling, due to its intermittent energy generation.\nAccurate and near real-time solar power prediction is of critical importance to\ntolerant and support the permeation of distributed and volatile solar power\nproduction in the energy system. In this paper, we propose a deep-learning\nbased ultra-short-term solar power prediction with data reconstruction. We\ndecompose the data for the prediction to facilitate extensive exploration of\nthe spatial and temporal dependencies within the data. Particularly, we\nreconstruct the data into low- and high-frequency components, using ensemble\nempirical model decomposition with adaptive noise (CEEMDAN). We integrate\nmeteorological data with those two components, and employ deep-learning models\nto capture long- and short-term dependencies towards the target prediction\nperiod. In this way, we excessively exploit the features in historical data in\npredicting a ultra-short-term solar power production. Furthermore, as\nultra-short-term prediction is vulnerable to local optima, we modify the\noptimization in our deep-learning training by penalizing long prediction\nintervals. Numerical experiments with diverse settings demonstrate that,\ncompared to baseline models, the proposed method achieves improved\ngeneralization in data reconstruction and higher prediction accuracy for\nultra-short-term solar power production."}
{"id": "2509.17105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17105", "abs": "https://arxiv.org/abs/2509.17105", "authors": ["Haoxin Guo", "Jiawen Pan", "Weixin Zhai"], "title": "GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization", "comment": null, "summary": "Hyperparameter optimization (HPO) plays a critical role in improving model\nperformance. Transformer-based HPO methods have shown great potential; however,\nexisting approaches rely heavily on large-scale historical optimization\ntrajectories and lack effective reinforcement learning (RL) techniques, thereby\nlimiting their efficiency and performance improvements. Inspired by the success\nof Group Relative Policy Optimization (GRPO) in large language models (LLMs),\nwe propose GRPOformer -- a novel hyperparameter optimization framework that\nintegrates reinforcement learning (RL) with Transformers. In GRPOformer,\nTransformers are employed to generate new hyperparameter configurations from\nhistorical optimization trajectories, while GRPO enables rapid trajectory\nconstruction and optimization strategy learning from scratch. Moreover, we\nintroduce Policy Churn Regularization (PCR) to enhance the stability of GRPO\ntraining. Experimental results on OpenML demonstrate that GRPOformer\nconsistently outperforms baseline methods across diverse tasks, offering new\ninsights into the application of RL for HPO."}
{"id": "2509.17119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17119", "abs": "https://arxiv.org/abs/2509.17119", "authors": ["Yifei Wu", "Bo Wang", "Jingshi Cui", "Pei-chun Lin", "Junzo Watada"], "title": "ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting", "comment": null, "summary": "To address the intermittency of renewable energy source (RES) generation,\nscenario forecasting offers a series of stochastic realizations for predictive\nobjects with superior flexibility and direct views. Based on a long time-series\nperspective, this paper explores uncertainties in the realms of renewable power\nand deep learning. Then, an uncertainty-aware model is meticulously designed\nfor renewable scenario forecasting, which leverages an attention mechanism and\ngenerative adversarial networks (GANs) to precisely capture complex\nspatial-temporal dynamics. To improve the interpretability of uncertain\nbehavior in RES generation, Bayesian deep learning and adaptive instance\nnormalization (AdaIN) are incorporated to simulate typical patterns and\nvariations. Additionally, the integration of meteorological information,\nforecasts, and historical trajectories in the processing layer improves the\nsynergistic forecasting capability for multiscale periodic regularities.\nNumerical experiments and case analyses demonstrate that the proposed approach\nprovides an appropriate interpretation for renewable uncertainty\nrepresentation, including both aleatoric and epistemic uncertainties, and shows\nsuperior performance over state-of-the-art methods."}
{"id": "2509.17145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17145", "abs": "https://arxiv.org/abs/2509.17145", "authors": ["Amaan Ansari", "Lukas Kirchdorfer", "Raheleh Hadian"], "title": "On the Simplification of Neural Network Architectures for Predictive Process Monitoring", "comment": null, "summary": "Predictive Process Monitoring (PPM) aims to forecast the future behavior of\nongoing process instances using historical event data, enabling proactive\ndecision-making. While recent advances rely heavily on deep learning models\nsuch as LSTMs and Transformers, their high computational cost hinders practical\nadoption. Prior work has explored data reduction techniques and alternative\nfeature encodings, but the effect of simplifying model architectures themselves\nremains underexplored. In this paper, we analyze how reducing model complexity,\nboth in terms of parameter count and architectural depth, impacts predictive\nperformance, using two established PPM approaches. Across five diverse event\nlogs, we show that shrinking the Transformer model by 85% results in only a\n2-3% drop in performance across various PPM tasks, while the LSTM proves\nslightly more sensitive, particularly for waiting time prediction. Overall, our\nfindings suggest that substantial model simplification can preserve predictive\naccuracy, paving the way for more efficient and scalable PPM solutions."}
{"id": "2509.17153", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17153", "abs": "https://arxiv.org/abs/2509.17153", "authors": ["Moule Lin", "Andrea Patane", "Weipeng Jing", "Shuhao Guan", "Goetz Botterweck"], "title": "Flow-Induced Diagonal Gaussian Processes", "comment": "15 pages", "summary": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression\nframework that incorporates a compact inducing weight matrix to project a\nneural network's weight uncertainty into a lower-dimensional subspace.\nCritically, FiD-GP relies on normalising-flow priors and spectral\nregularisations to augment its expressiveness and align the inducing subspace\nwith feature-gradient geometry through a numerically stable projection\nmechanism objective. Furthermore, we demonstrate how the prediction framework\nin FiD-GP can help to design a single-pass projection for Out-of-Distribution\n(OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation\nability on various tasks compared with SVGP-based baselines, satisfies tight\nspectral residual bounds with theoretically guaranteed OoD detection, and\nsignificantly compresses the neural network's storage requirements at the cost\nof increased inference computation dependent on the number of inducing weights\nemployed. Specifically, in a comprehensive empirical study spanning regression,\nimage classification, semantic segmentation, and out-of-distribution detection\nbenchmarks, it cuts Bayesian training cost by several orders of magnitude,\ncompresses parameters by roughly 51%, reduces model size by about 75%, and\nmatches state-of-the-art accuracy and uncertainty estimation."}
{"id": "2509.17156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17156", "abs": "https://arxiv.org/abs/2509.17156", "authors": ["Samar Hadou", "Alejandro Ribeiro"], "title": "Unrolled Graph Neural Networks for Constrained Optimization", "comment": null, "summary": "In this paper, we unroll the dynamics of the dual ascent (DA) algorithm in\ntwo coupled graph neural networks (GNNs) to solve constrained optimization\nproblems. The two networks interact with each other at the layer level to find\na saddle point of the Lagrangian. The primal GNN finds a stationary point for a\ngiven dual multiplier, while the dual network iteratively refines its estimates\nto reach an optimal solution. We force the primal and dual networks to mirror\nthe dynamics of the DA algorithm by imposing descent and ascent constraints. We\npropose a joint training scheme that alternates between updating the primal and\ndual networks. Our numerical experiments demonstrate that our approach yields\nnear-optimal near-feasible solutions and generalizes well to\nout-of-distribution (OOD) problems."}
{"id": "2509.17165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17165", "abs": "https://arxiv.org/abs/2509.17165", "authors": ["Sahar Koohfar", "Wubeshet Woldemariam"], "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer", "comment": null, "summary": "Time series data is a prevalent form of data found in various fields. It\nconsists of a series of measurements taken over time. Forecasting is a crucial\napplication of time series models, where future values are predicted based on\nhistorical data. Accurate forecasting is essential for making well-informed\ndecisions across industries. When it comes to electric vehicles (EVs), precise\npredictions play a key role in planning infrastructure development, load\nbalancing, and energy management. This study introduces a BI-LSTM embedding\ndenoising autoencoder model (BDM) designed to address time series problems,\nfocusing on short-term EV charging load prediction. The performance of the\nproposed model is evaluated by comparing it with benchmark models like\nTransformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the\nproposed model outperforms the benchmark models in four of the five-time steps,\ndemonstrating its effectiveness for time series forecasting. This research\nmakes a significant contribution to enhancing time series forecasting, thereby\nimproving decision-making processes."}
{"id": "2509.17175", "categories": ["cs.LG", "stat.AP", "62P12", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.17175", "abs": "https://arxiv.org/abs/2509.17175", "authors": ["Ni√°l Perry", "Peter P. Pedersen", "Charles N. Christensen", "Emanuel Nussli", "Sanelma Heinonen", "Lorena Gordillo Dagallier", "Rapha√´l Jacquat", "Sebastian Horstmann", "Christoph Franck"], "title": "Detecting Urban PM$_{2.5}$ Hotspots with Mobile Sensing and Gaussian Process Regression", "comment": "39 pages, 12 figures", "summary": "Low-cost mobile sensors can be used to collect PM$_{2.5}$ concentration data\nthroughout an entire city. However, identifying air pollution hotspots from the\ndata is challenging due to the uneven spatial sampling, temporal variations in\nthe background air quality, and the dynamism of urban air pollution sources.\nThis study proposes a method to identify urban PM$_{2.5}$ hotspots that\naddresses these challenges, involving four steps: (1) equip citizen scientists\nwith mobile PM$_{2.5}$ sensors while they travel; (2) normalise the raw data to\nremove the influence of background ambient pollution levels; (3) fit a Gaussian\nprocess regression model to the normalised data and (4) calculate a grid of\nspatially explicit 'hotspot scores' using the probabilistic framework of\nGaussian processes, which conveniently summarise the relative pollution levels\nthroughout the city. We apply our method to create the first ever map of\nPM$_{2.5}$ pollution in Kigali, Rwanda, at a 200m resolution. Our results\nsuggest that the level of ambient PM$_{2.5}$ pollution in Kigali is dangerously\nhigh, and we identify the hotspots in Kigali where pollution consistently\nexceeds the city-wide average. We also evaluate our method using simulated\nmobile sensing data for Beijing, China, where we find that the hotspot scores\nare probabilistically well calibrated and accurately reflect the 'ground truth'\nspatial profile of PM$_{2.5}$ pollution. Thanks to the use of open-source\nsoftware, our method can be re-applied in cities throughout the world with a\nhandful of low-cost sensors. The method can help fill the gap in urban air\nquality information and empower public health officials."}
{"id": "2509.17176", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17176", "abs": "https://arxiv.org/abs/2509.17176", "authors": ["Ganesh Khekare", "Shivam Sunda", "Yash Bothra"], "title": "A Comprehensive Performance Comparison of Traditional and Ensemble Machine Learning Models for Online Fraud Detection", "comment": "6 pages, 6 figures. Presented at IEEE INTERNATIONAL CONFERENCE ON\n  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT), 2025", "summary": "In the era of the digitally driven economy, where there has been an\nexponential surge in digital payment systems and other online activities,\nvarious forms of fraudulent activities have accompanied the digital growth, out\nof which credit card fraud has become an increasingly significant threat. To\ndeal with this, real-time fraud detection is essential for financial security\nbut remains challenging due to high transaction volumes and the complexity of\nmodern fraud patterns. This study presents a comprehensive performance\ncomparison between traditional machine learning models like Random Forest, SVM,\nLogistic Regression, XGBoost, and ensemble methods like Stacking and Voting\nClassifier for detecting credit card fraud on a heavily imbalanced public\ndataset, where the number of fraudulent transactions is 492 out of 284,807\ntotal transactions. Application-specific preprocessing techniques were applied,\nand the models were evaluated using various performance metrics. The ensemble\nmethods achieved an almost perfect precision of around 0.99, but traditional\nmethods demonstrated superior performance in terms of recall, which highlights\nthe trade-off between false positives and false negatives. The comprehensive\ncomparison reveals distinct performance strengths and limitations for each\nalgorithm, offering insights to guide practitioners in selecting the most\neffective model for robust fraud detection applications in real-world settings."}
{"id": "2509.17180", "categories": ["cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17180", "abs": "https://arxiv.org/abs/2509.17180", "authors": ["David Arbour", "Harsh Parikh", "Bijan Niknam", "Elizabeth Stuart", "Kara Rudolph", "Avi Feller"], "title": "Regularizing Extrapolation in Causal Inference", "comment": null, "summary": "Many common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression,\nallow for arbitrarily negative weights, which improve feature imbalance but\noften at the cost of increased dependence on parametric modeling assumptions\nand higher variance. By contrast, estimators like importance weighting and\nrandom forests (sometimes implicitly) restrict weights to be non-negative,\nreducing dependence on parametric modeling and variance at the cost of worse\nimbalance. In this paper, we propose a unified framework that directly\npenalizes the level of extrapolation, replacing the current practice of a hard\nnon-negativity constraint with a soft constraint and corresponding\nhyperparameter. We derive a worst-case extrapolation error bound and introduce\na novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature\nimbalance, model misspecification, and estimator variance; this tradeoff is\nespecially pronounced in high dimensions, particularly when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while\nminimizing imbalance and outline how to use this approach as a sensitivity\nanalysis for dependence on parametric modeling assumptions. We demonstrate the\neffectiveness of our approach through synthetic experiments and a real-world\napplication, involving the generalization of randomized controlled trial\nestimates to a target population of interest."}
{"id": "2509.17182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17182", "abs": "https://arxiv.org/abs/2509.17182", "authors": ["Sam Jacob Jacob", "Markus Mrosek", "Carsten Othmer", "Harald K√∂stler"], "title": "PMRT: A Training Recipe for Fast, 3D High-Resolution Aerodynamic Prediction", "comment": null, "summary": "The aerodynamic optimization of cars requires close collaboration between\naerodynamicists and stylists, while slow, expensive simulations remain a\nbottleneck. Surrogate models have been shown to accurately predict aerodynamics\nwithin the design space for which they were trained. However, many of these\nmodels struggle to scale to higher resolutions because of the 3D nature of the\nproblem and data scarcity. We propose Progressive Multi-Resolution Training\n(PMRT), a probabilistic multi-resolution training schedule that enables\ntraining a U-Net to predict the drag coefficient ($c_d$) and high-resolution\nvelocity fields (512 x 128 x 128) in 24 hours on a single NVIDIA H100 GPU, 7x\ncheaper than the high-resolution-only baseline, with similar accuracy. PMRT\nsamples batches from three resolutions based on probabilities that change\nduring training, starting with an emphasis on lower resolutions and gradually\nshifting toward higher resolutions. Since this is a training methodology, it\ncan be adapted to other high-resolution-focused backbones. We also show that a\nsingle model can be trained across five datasets from different solvers,\nincluding a real-world dataset, by conditioning on the simulation parameters.\nIn the DrivAerML dataset, our models achieve a $c_d$ $R^2$ of 0.975, matching\nliterature baselines at a fraction of the training cost."}
{"id": "2509.17186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17186", "abs": "https://arxiv.org/abs/2509.17186", "authors": ["Dehao Zhang", "Malu Zhang", "Shuai Wang", "Jingya Wang", "Wenjie Wei", "Zeyu Ma", "Guoqing Wang", "Yang Yang", "HaiZhou Li"], "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling", "comment": null, "summary": "The explosive growth in sequence length has intensified the demand for\neffective and efficient long sequence modeling. Benefiting from intrinsic\noscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently\nextract frequency components from input signals and encode them into\nspatiotemporal spike trains, making them well-suited for long sequence\nmodeling. However, RF neurons exhibit limited effective memory capacity and a\ntrade-off between energy efficiency and training speed on complex temporal\ntasks. Inspired by the dendritic structure of biological neurons, we propose a\nDendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a\nmulti-dendritic and soma architecture. Each dendritic branch encodes specific\nfrequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons,\nthereby collectively achieving comprehensive frequency representation.\nFurthermore, we introduce an adaptive threshold mechanism into the soma\nstructure that adjusts the threshold based on historical spiking activity,\nreducing redundant spikes while maintaining training efficiency in long\nsequence tasks. Extensive experiments demonstrate that our method maintains\ncompetitive accuracy while substantially ensuring sparse spikes without\ncompromising computational efficiency during training. These results underscore\nits potential as an effective and efficient solution for long sequence modeling\non edge platforms."}
{"id": "2509.17197", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17197", "abs": "https://arxiv.org/abs/2509.17197", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "comment": "11 pages", "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings."}
{"id": "2509.17205", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17205", "abs": "https://arxiv.org/abs/2509.17205", "authors": ["Wook Lee", "Frans A. Oliehoek"], "title": "Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization", "comment": null, "summary": "Leveraging machine learning methods to solve constraint satisfaction problems\nhas shown promising, but they are mostly limited to a static situation where\nthe problem description is completely known and fixed from the beginning. In\nthis work we present a new approach to constraint satisfaction and optimization\nin dynamically changing environments, particularly when variables in the\nproblem are statistically independent. We frame it as a reinforcement learning\nproblem and introduce a conditional policy generator by borrowing the idea of\nclass conditional generative adversarial networks (GANs). Assuming that the\nproblem includes both static and dynamic constraints, the former are used in a\nreward formulation to guide the policy training such that it learns to map to a\nprobabilistic distribution of solutions satisfying static constraints from a\nnoise prior, which is similar to a generator in GANs. On the other hand,\ndynamic constraints in the problem are encoded to different class labels and\nfed with the input noise. The policy is then simultaneously updated for maximum\nlikelihood of correctly classifying given the dynamic conditions in a\nsupervised manner. We empirically demonstrate a proof-of-principle experiment\nwith a multi-modal constraint satisfaction problem and compare between\nunconditional and conditional cases."}
{"id": "2509.17208", "categories": ["cs.LG", "physics.atm-clus", "I.6.5; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.17208", "abs": "https://arxiv.org/abs/2509.17208", "authors": ["Kevin Bachelor", "Sanya Murdeshwar", "Daniel Sabo", "Razvan Marinescu"], "title": "Active Learning for Machine Learning Driven Molecular Dynamics", "comment": "8 pages, 4 figures, for Neurips Workshop: Machine Learning and the\n  Physical Sciences 2025", "summary": "Machine learned coarse grained (CG) potentials are fast, but degrade over\ntime when simulations reach undersampled biomolecular conformations, and\ngenerating widespread all atom (AA) data to combat this is computationally\ninfeasible. We propose a novel active learning framework for CG neural network\npotentials in molecular dynamics (MD). Building on the CGSchNet model, our\nmethod employs root mean squared deviation (RMSD) based frame selection from MD\nsimulations in order to generate data on the fly by querying an oracle during\nthe training of a neural network potential. This framework preserves CG level\nefficiency while correcting the model at precise, RMSD identified coverage\ngaps. By training CGSchNet, a coarse grained neural network potential, we\nempirically show that our framework explores previously unseen configurations\nand trains the model on unexplored regions of conformational space. Our active\nlearning framework enables a CGSchNet model trained on the Chignolin protein to\nachieve a 33.05% improvement in the Wasserstein 1 (W1) metric in Time lagged\nIndependent Component Analysis (TICA) space on an in house benchmark suite."}
{"id": "2509.17228", "categories": ["cs.LG", "cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17228", "abs": "https://arxiv.org/abs/2509.17228", "authors": ["Zihan Liang", "Ziwen Pan", "Ruoxuan Xiong"], "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness", "comment": "To appear in Proc. of EMNLP 2025 (18 pages)", "summary": "Clinical notes contain rich patient information, such as diagnoses or\nmedications, making them valuable for patient representation learning. Recent\nadvances in large language models have further improved the ability to extract\nmeaningful representations from clinical texts. However, clinical notes are\noften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of\npatients have no available discharge summaries. In such cases, representations\ncan be learned from other modalities such as structured data, chest X-rays, or\nradiology reports. Yet the availability of these modalities is influenced by\nclinical decision-making and varies across patients, resulting in modality\nmissing-not-at-random (MMNAR) patterns. We propose a causal representation\nlearning framework that leverages observed data and informative missingness in\nmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion\ncomponent that integrates structured data, imaging, and text while conditioning\non missingness patterns to capture patient health and clinician-driven\nassignment; (2) a modality reconstruction component with contrastive learning\nto ensure semantic sufficiency in representation learning; and (3) a multitask\noutcome prediction model with a rectifier that corrects for residual bias from\nspecific modality observation patterns. Comprehensive evaluations across\nMIMIC-IV and eICU show consistent gains over the strongest baselines, achieving\nup to 13.8% AUC improvement for hospital readmission and 13.1% for ICU\nadmission."}
{"id": "2509.17235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17235", "abs": "https://arxiv.org/abs/2509.17235", "authors": ["Jiazhen Chen", "Mingbin Feng", "Tony S. Wirjanto"], "title": "Prospective Multi-Graph Cohesion for Multivariate Time Series Anomaly Detection", "comment": "Accepted by the 18th ACM International Conference on Web Search and\n  Data Mining (ACM WSDM 2025)", "summary": "Anomaly detection in high-dimensional time series data is pivotal for\nnumerous industrial applications. Recent advances in multivariate time series\nanomaly detection (TSAD) have increasingly leveraged graph structures to model\ninter-variable relationships, typically employing Graph Neural Networks (GNNs).\nDespite their promising results, existing methods often rely on a single graph\nrepresentation, which are insufficient for capturing the complex, diverse\nrelationships inherent in multivariate time series. To address this, we propose\nthe Prospective Multi-Graph Cohesion (PMGC) framework for multivariate TSAD.\nPMGC exploits spatial correlations by integrating a long-term static graph with\na series of short-term instance-wise dynamic graphs, regulated through a graph\ncohesion loss function. Our theoretical analysis shows that this loss function\npromotes diversity among dynamic graphs while aligning them with the stable\nlong-term relationships encapsulated by the static graph. Additionally, we\nintroduce a \"prospective graphing\" strategy to mitigate the limitations of\ntraditional forecasting-based TSAD methods, which often struggle with\nunpredictable future variations. This strategy allows the model to accurately\nreflect concurrent inter-series relationships under normal conditions, thereby\nenhancing anomaly detection efficacy. Empirical evaluations on real-world\ndatasets demonstrate the superior performance of our method compared to\nexisting TSAD techniques."}
{"id": "2509.17241", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17241", "abs": "https://arxiv.org/abs/2509.17241", "authors": ["Ali Faraji", "Manos Papagelis"], "title": "TraceHiding: Scalable Machine Unlearning for Mobility Data", "comment": null, "summary": "This work introduces TraceHiding, a scalable, importance-aware machine\nunlearning framework for mobility trajectory data. Motivated by privacy\nregulations such as GDPR and CCPA granting users \"the right to be forgotten,\"\nTraceHiding removes specified user trajectories from trained deep models\nwithout full retraining. It combines a hierarchical data-driven importance\nscoring scheme with teacher-student distillation. Importance scores--computed\nat token, trajectory, and user levels from statistical properties (coverage\ndiversity, entropy, length)--quantify each training sample's impact, enabling\ntargeted forgetting of high-impact data while preserving common patterns. The\nstudent model retains knowledge on remaining data and unlearns targeted\ntrajectories through an importance-weighted loss that amplifies forgetting\nsignals for unique samples and attenuates them for frequent ones. We validate\non Trajectory--User Linking (TUL) tasks across three real-world higher-order\nmobility datasets (HO-Rome, HO-Geolife, HO-NYC) and multiple architectures\n(GRU, LSTM, BERT, ModernBERT, GCN-TULHOR), against strong unlearning baselines\nincluding SCRUB, NegGrad, NegGrad+, Bad-T, and Finetuning. Experiments under\nuniform and targeted user deletion show TraceHiding, especially its\nentropy-based variant, achieves superior unlearning accuracy, competitive\nmembership inference attack (MIA) resilience, and up to 40\\times speedup over\nretraining with minimal test accuracy loss. Results highlight robustness to\nadversarial deletion of high-information users and consistent performance\nacross models. To our knowledge, this is the first systematic study of machine\nunlearning for trajectory data, providing a reproducible pipeline with public\ncode and preprocessing tools."}
{"id": "2509.17250", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17250", "abs": "https://arxiv.org/abs/2509.17250", "authors": ["Yigit Berkay Uslu", "Samar Hadou", "Sergio Rozada", "Shirin Saeedi Bidokhti", "Alejandro Ribeiro"], "title": "Graph Signal Generative Diffusion Models", "comment": "Submitted to 2026 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2026)", "summary": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for\nstochastic graph signal generation using denoising diffusion processes. The\narchitecture learns node features at different resolutions with skip\nconnections between the encoder and decoder paths, analogous to the\nconvolutional U-Net for image generation. The U-GNN is prominent for a pooling\noperation that leverages zero-padding and avoids arbitrary graph coarsening,\nwith graph convolutions layered on top to capture local dependencies. This\ntechnique permits learning feature embeddings for sampled nodes at deeper\nlevels of the architecture that remain convolutional with respect to the\noriginal graph. Applied to stock price prediction -- where deterministic\nforecasts struggle to capture uncertainties and tail events that are paramount\n-- we demonstrate the effectiveness of the diffusion model in probabilistic\nforecasting of stock prices."}
{"id": "2509.17281", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17281", "abs": "https://arxiv.org/abs/2509.17281", "authors": ["Raisa Amiruddin", "Nikolay Y. Yordanov", "Nazanin Maleki", "Pascal Fehringer", "Athanasios Gkampenis", "Anastasia Janas", "Kiril Krantchev", "Ahmed Moawad", "Fabian Umeh", "Salma Abosabie", "Sara Abosabie", "Albara Alotaibi", "Mohamed Ghonim", "Mohanad Ghonim", "Sedra Abou Ali Mhana", "Nathan Page", "Marko Jakovljevic", "Yasaman Sharifi", "Prisha Bhatia", "Amirreza Manteghinejad", "Melisa Guelen", "Michael Veronesi", "Virginia Hill", "Tiffany So", "Mark Krycia", "Bojan Petrovic", "Fatima Memon", "Justin Cramer", "Elizabeth Schrickel", "Vilma Kosovic", "Lorenna Vidal", "Gerard Thompson", "Ichiro Ikuta", "Basimah Albalooshy", "Ali Nabavizadeh", "Nourel Hoda Tahon", "Karuna Shekdar", "Aashim Bhatia", "Claudia Kirsch", "Gennaro D'Anna", "Philipp Lohmann", "Amal Saleh Nour", "Andriy Myronenko", "Adam Goldman-Yassen", "Janet R. Reid", "Sanjay Aneja", "Spyridon Bakas", "Mariam Aboian"], "title": "Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform", "comment": "23 pages, 9 figures, 1 table, 3 supplementary tables", "summary": "High-quality reference standard image data creation by neuroradiology experts\nfor automated clinical tools can be a powerful tool for neuroradiology &\nartificial intelligence education. We developed a multimodal educational\napproach for students and trainees during the MICCAI Brain Tumor Segmentation\nLighthouse Challenge 2025, a landmark initiative to develop accurate brain\ntumor segmentation algorithms. Fifty-six medical students & radiology trainees\nvolunteered to annotate brain tumor MR images for the BraTS challenges of 2023\n& 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56\nannotators, 14 select volunteers were then paired with neuroradiology faculty\nfor guided one-on-one annotation sessions for BraTS 2025. Lectures on\nneuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were\norganized online. Annotators & audience members completed surveys on their\nperceived knowledge before & after annotations & lectures respectively.\nFourteen coordinators, each paired with a neuroradiologist, completed the data\nannotation process, averaging 1322.9+/-760.7 hours per dataset per pair and\n1200 segmentations in total. On a scale of 1-10, annotation coordinators\nreported significant increase in familiarity with image segmentation software\npre- and post-annotation, moving from initial average of 6+/-2.9 to final\naverage of 8.9+/-1.1, and significant increase in familiarity with brain tumor\nfeatures pre- and post-annotation, moving from initial average of 6.2+/-2.4 to\nfinal average of 8.1+/-1.2. We demonstrate an innovative offering for providing\nneuroradiology & AI education through an image segmentation challenge to\nenhance understanding of algorithm development, reinforce the concept of data\nreference standard, and diversify opportunities for AI-driven image analysis\namong future physicians."}
{"id": "2509.17291", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17291", "abs": "https://arxiv.org/abs/2509.17291", "authors": ["Rahul Nandakumar", "Deepayan Chakrabarti"], "title": "GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories", "comment": "18 pages, 4 figures. Accepted at ECML-PKDD 2025", "summary": "Given a set of graphs from some unknown family, we want to generate new\ngraphs from that family. Recent methods use diffusion on either graph\nembeddings or the discrete space of nodes and edges. However, simple changes to\nembeddings (say, adding noise) can mean uninterpretable changes in the graph.\nIn discrete-space diffusion, each step may add or remove many nodes/edges. It\nis hard to predict what graph patterns we will observe after many diffusion\nsteps. Our proposed method, called GraphWeave, takes a different approach. We\nseparate pattern generation and graph construction. To find patterns in the\ntraining graphs, we see how they transform vectors during random walks. We then\ngenerate new graphs in two steps. First, we generate realistic random walk\n\"trajectories\" which match the learned patterns. Then, we find the optimal\ngraph that fits these trajectories. The optimization infers all edges jointly,\nwhich improves robustness to errors. On four simulated and five real-world\nbenchmark datasets, GraphWeave outperforms existing methods. The most\nsignificant differences are on large-scale graph structures such as PageRank,\ncuts, communities, degree distributions, and flows. GraphWeave is also 10x\nfaster than its closest competitor. Finally, GraphWeave is simple, needing only\na transformer and standard optimizers."}
{"id": "2509.17293", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17293", "abs": "https://arxiv.org/abs/2509.17293", "authors": ["Ryan Chappell", "Chayan Banerjee", "Kien Nguyen", "Clinton Fookes"], "title": "Physics-Informed Operator Learning for Hemodynamic Modeling", "comment": "To appear in the proceedings of DICTA 2025", "summary": "Accurate modeling of personalized cardiovascular dynamics is crucial for\nnon-invasive monitoring and therapy planning. State-of-the-art physics-informed\nneural network (PINN) approaches employ deep, multi-branch architectures with\nadversarial or contrastive objectives to enforce partial differential equation\nconstraints. While effective, these enhancements introduce significant training\nand implementation complexity, limiting scalability and practical deployment.\nWe investigate physics-informed neural operator learning models as efficient\nsupervisory signals for training simplified architectures through knowledge\ndistillation. Our approach pre-trains a physics-informed DeepONet (PI-DeepONet)\non high-fidelity cuffless blood pressure recordings to learn operator mappings\nfrom raw wearable waveforms to beat-to-beat pressure signals under embedded\nphysics constraints. This pre-trained operator serves as a frozen supervisor in\na lightweight knowledge-distillation pipeline, guiding streamlined base models\nthat eliminate complex adversarial and contrastive learning components while\nmaintaining performance. We characterize the role of physics-informed\nregularization in operator learning and demonstrate its effectiveness for\nsupervisory guidance. Through extensive experiments, our operator-supervised\napproach achieves performance parity with complex baselines (correlation: 0.766\nvs. 0.770, RMSE: 4.452 vs. 4.501), while dramatically reducing architectural\ncomplexity from eight critical hyperparameters to a single regularization\ncoefficient and decreasing training overhead by 4%. Our results demonstrate\nthat operator-based supervision effectively replaces intricate multi-component\ntraining strategies, offering a more scalable and interpretable approach to\nphysiological modeling with reduced implementation burden."}
{"id": "2509.17304", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17304", "abs": "https://arxiv.org/abs/2509.17304", "authors": ["Tian Xie", "Ding Zhu", "Jia Liu", "Mahdi Khalili", "Xueru Zhang"], "title": "SPRINT: Stochastic Performative Prediction With Variance Reduction", "comment": null, "summary": "Performative prediction (PP) is an algorithmic framework for optimizing\nmachine learning (ML) models where the model's deployment affects the\ndistribution of the data it is trained on. Compared to traditional ML with\nfixed data, designing algorithms in PP converging to a stable point -- known as\na stationary performative stable (SPS) solution -- is more challenging than the\ncounterpart in conventional ML tasks due to the model-induced distribution\nshifts. While considerable efforts have been made to find SPS solutions using\nmethods such as repeated gradient descent (RGD) and greedy stochastic gradient\ndescent (SGD-GD), most prior studies assumed a strongly convex loss until a\nrecent work established $O(1/\\sqrt{T})$ convergence of SGD-GD to SPS solutions\nunder smooth, non-convex losses. However, this latest progress is still based\non the restricted bounded variance assumption in stochastic gradient estimates\nand yields convergence bounds with a non-vanishing error neighborhood that\nscales with the variance. This limitation motivates us to improve convergence\nrates and reduce error in stochastic optimization for PP, particularly in\nnon-convex settings. Thus, we propose a new algorithm called stochastic\nperformative prediction with variance reduction (SPRINT) and establish its\nconvergence to an SPS solution at a rate of $O(1/T)$. Notably, the resulting\nerror neighborhood is independent of the variance of the stochastic gradients.\nExperiments on multiple real datasets with non-convex models demonstrate that\nSPRINT outperforms SGD-GD in both convergence rate and stability."}
{"id": "2509.17322", "categories": ["cs.LG", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.17322", "abs": "https://arxiv.org/abs/2509.17322", "authors": ["Chi Zhang", "Mengxin Zheng", "Qian Lou", "Hui Min Leung", "Fan Chen"], "title": "VQEzy: An Open-Source Dataset for Parameter Initialize in Variational Quantum Eigensolvers", "comment": null, "summary": "Variational Quantum Eigensolvers (VQEs) are a leading class of noisy\nintermediate-scale quantum (NISQ) algorithms, whose performance is highly\nsensitive to parameter initialization. Although recent machine learning-based\ninitialization methods have achieved state-of-the-art performance, their\nprogress has been limited by the lack of comprehensive datasets. Existing\nresources are typically restricted to a single domain, contain only a few\nhundred instances, and lack complete coverage of Hamiltonians, ansatz circuits,\nand optimization trajectories. To overcome these limitations, we introduce\nVQEzy, the first large-scale dataset for VQE parameter initialization. VQEzy\nspans three major domains and seven representative tasks, comprising 12,110\ninstances with full VQE specifications and complete optimization trajectories.\nThe dataset is available online, and will be continuously refined and expanded\nto support future research in VQE optimization."}
{"id": "2509.17325", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17325", "abs": "https://arxiv.org/abs/2509.17325", "authors": ["Weihua Du", "Hailei Gong", "Zhan Ling", "Kang Liu", "Lingfeng Shen", "Xuesong Yao", "Yufei Xu", "Dingyuan Shi", "Yiming Yang", "Jiecao Chen"], "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym", "comment": "22 pages. Project available at https://github.com/StigLidu/CodeGym", "summary": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage\nexternal tools to solve diverse tasks and interface with the real world.\nHowever, current training practices largely rely on supervised fine-tuning\n(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,\nand generalize poorly beyond development settings, leading to brittleness with\nnew tools and unseen workflows. Because code execution reflects many structures\nof real-world workflows, coding problems provide a natural basis for building\nagent training environments. Motivated by this, we introduce CodeGym, a\nscalable framework that synthesizes diverse, verifiable, and controllable\nmulti-turn tool-use environments for agent RL, enabling LLM agents to explore\nand master various workflows actively. CodeGym rewrites static coding problems\ninto interactive environments by extracting atomic functions or logic into\ncallable tools, yielding verifiable tasks that span various tool-execution\nworkflows. Models of varying sizes and chain-of-thought configurations, trained\nin CodeGym, exhibit consistent out-of-distribution generalizability; for\nexample, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points\non the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step\ntoward scalable general-purpose RL environments that align with real-world\nagent workflows."}
{"id": "2509.17400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17400", "abs": "https://arxiv.org/abs/2509.17400", "authors": ["Xiaoyang Xu", "Xiaofeng Lin", "Koh Takeuchi", "Kyohei Atarashi", "Hisashi Kashima"], "title": "Robust Anomaly Detection Under Normality Distribution Shift in Dynamic Graphs", "comment": null, "summary": "Anomaly detection in dynamic graphs is a critical task with broad real-world\napplications, including social networks, e-commerce, and cybersecurity. Most\nexisting methods assume that normal patterns remain stable over time; however,\nthis assumption often fails in practice due to the phenomenon we refer to as\nnormality distribution shift (NDS), where normal behaviors evolve over time.\nIgnoring NDS can lead models to misclassify shifted normal instances as\nanomalies, degrading detection performance. To tackle this issue, we propose\nWhENDS, a novel unsupervised anomaly detection method that aligns normal edge\nembeddings across time by estimating distributional statistics and applying\nwhitening transformations. Extensive experiments on four widely-used dynamic\ngraph datasets show that WhENDS consistently outperforms nine strong baselines,\nachieving state-of-the-art results and underscoring the importance of\naddressing NDS in dynamic graph anomaly detection."}
{"id": "2509.17405", "categories": ["cs.LG", "49Q22 (Primary) 90C57, 68Txx (Secondary)", "G.3; I.2"], "pdf": "https://arxiv.org/pdf/2509.17405", "abs": "https://arxiv.org/abs/2509.17405", "authors": ["Manish Acharya", "David Hyde"], "title": "Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization", "comment": "19 pages, 11 figures", "summary": "The sliced Wasserstein distance (SW) reduces optimal transport on\n$\\mathbb{R}^d$ to a sum of one-dimensional projections, and thanks to this\nefficiency, it is widely used in geometry, generative modeling, and\nregistration tasks. Recent work shows that quasi-Monte Carlo constructions for\ncomputing SW (QSW) yield direction sets with excellent approximation error.\nThis paper presents an alternate, novel approach: learning directions with\nBayesian optimization (BO), particularly in settings where SW appears inside an\noptimization loop (e.g., gradient flows). We introduce a family of drop-in\nselectors for projection directions: BOSW, a one-shot BO scheme on the unit\nsphere; RBOSW, a periodic-refresh variant; ABOSW, an adaptive hybrid that seeds\nfrom competitive QSW sets and performs a few lightweight BO refinements; and\nARBOSW, a restarted hybrid that periodically relearns directions during\noptimization. Our BO approaches can be composed with QSW and its variants\n(demonstrated by ABOSW/ARBOSW) and require no changes to downstream losses or\ngradients. We provide numerical experiments where our methods achieve\nstate-of-the-art performance, and on the experimental suite of the original QSW\npaper, we find that ABOSW and ARBOSW can achieve convergence comparable to the\nbest QSW variants with modest runtime overhead."}
{"id": "2509.17413", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17413", "abs": "https://arxiv.org/abs/2509.17413", "authors": ["Masako Kishida"], "title": "Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR", "comment": null, "summary": "Ensuring the safety of neural networks under input uncertainty is a\nfundamental challenge in safety-critical applications. This paper builds on and\nexpands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP)\nframework for neural network verification to a distributionally robust and\ntail-risk-aware setting by integrating worst-case Conditional Value-at-Risk\n(WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The\nresulting conditions remain SDP-checkable and explicitly account for tail risk.\nThis integration broadens input-uncertainty geometry-covering ellipsoids,\npolytopes, and hyperplanes-and extends applicability to safety-critical domains\nwhere tail-event severity matters. Applications to closed-loop reachability of\ncontrol systems and classification are demonstrated through numerical\nexperiments, illustrating how the risk level $\\varepsilon$ trades conservatism\nfor tolerance to tail events-while preserving the computational structure of\nprior QC/SDP methods for neural network verification and robustness analysis."}
{"id": "2509.17446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17446", "abs": "https://arxiv.org/abs/2509.17446", "authors": ["Haofeng Huang", "Yifei Han", "Long Zhang", "Bin Li", "Yangfan He"], "title": "MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion", "comment": "Submitted to ICASSP 2026", "summary": "Multimodal intent recognition (MMIR) suffers from weak semantic grounding and\npoor robustness under noisy or rare-class conditions. We propose MVCL-DAF++,\nwhich extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive\nalignment, aligning instances to class-level prototypes to enhance semantic\nconsistency; and (2) Coarse-to-fine attention fusion, integrating global\nmodality summaries with token-level features for hierarchical cross-modal\ninteraction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new\nstate-of-the-art results, improving rare-class recognition by +1.05\\% and\n+4.18\\% WF1, respectively. These results demonstrate the effectiveness of\nprototype-guided learning and coarse-to-fine fusion for robust multimodal\nunderstanding. The source code is available at\nhttps://github.com/chr1s623/MVCL-DAF-PlusPlus."}
{"id": "2509.17472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17472", "abs": "https://arxiv.org/abs/2509.17472", "authors": ["Jia Li", "Shiyu Long", "Ye Yuan"], "title": "Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector", "comment": null, "summary": "Multivariate time series (MTS) anomaly detection commonly encounters in\nvarious domains like finance, healthcare, and industrial monitoring. However,\nexisting MTS anomaly detection methods are mostly defined on the static graph\nstructure, which fails to perform an accurate representation of complex\nspatio-temporal correlations in MTS. To address this issue, this study proposes\na Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector (PGMA) with\nthe following two-fold ideas: a) designing a periodic time-slot allocation\nstrategy based Fast Fourier Transform (FFT), which enables the graph structure\nto reflect dynamic changes in MTS; b) utilizing graph neural network and\ntemporal extension convolution to accurate extract the complex spatio-temporal\ncorrelations from the reconstructed periodic graphs. Experiments on four real\ndatasets from real applications demonstrate that the proposed PGMA outperforms\nstate-of-the-art models in MTS anomaly detection."}
{"id": "2509.17491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17491", "abs": "https://arxiv.org/abs/2509.17491", "authors": ["Firuz Kamalov", "Mohmad Al Falasi", "Fadi Thabtah"], "title": "Path-Weighted Integrated Gradients for Interpretable Dementia Classification", "comment": null, "summary": "Integrated Gradients (IG) is a widely used attribution method in explainable\nartificial intelligence (XAI). In this paper, we introduce Path-Weighted\nIntegrated Gradients (PWIG), a generalization of IG that incorporates a\ncustomizable weighting function into the attribution integral. This\nmodification allows for targeted emphasis along different segments of the path\nbetween a baseline and the input, enabling improved interpretability, noise\nmitigation, and the detection of path-dependent feature relevance. We establish\nits theoretical properties and illustrate its utility through experiments on a\ndementia classification task using the OASIS-1 MRI dataset. Attribution maps\ngenerated by PWIG highlight clinically meaningful brain regions associated with\nvarious stages of dementia, providing users with sharp and stable explanations.\nThe results suggest that PWIG offers a flexible and theoretically grounded\napproach for enhancing attribution quality in complex predictive models."}
{"id": "2509.17495", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.17495", "abs": "https://arxiv.org/abs/2509.17495", "authors": ["Ke Ma", "Jialiang Lu", "Philippe Martins"], "title": "BiLCNet : BiLSTM-Conformer Network for Encrypted Traffic Classification with 5G SA Physical Channel Records", "comment": "6 pages, 5 figures", "summary": "Accurate and efficient traffic classification is vital for wireless network\nmanagement, especially under encrypted payloads and dynamic application\nbehavior, where traditional methods such as port-based identification and deep\npacket inspection (DPI) are increasingly inadequate. This work explores the\nfeasibility of using physical channel data collected from the air interface of\n5G Standalone (SA) networks for traffic sensing. We develop a preprocessing\npipeline to transform raw channel records into structured representations with\ncustomized feature engineering to enhance downstream classification\nperformance. To jointly capture temporal dependencies and both local and global\nstructural patterns inherent in physical channel records, we propose a novel\nhybrid architecture: BiLSTM-Conformer Network (BiLCNet), which integrates the\nsequential modeling capability of Bidirectional Long Short-Term Memory networks\n(BiLSTM) with the spatial feature extraction strength of Conformer blocks.\nEvaluated on a noise-limited 5G SA dataset, our model achieves a classification\naccuracy of 93.9%, outperforming a series of conventional machine learning and\ndeep learning algorithms. Furthermore, we demonstrate its generalization\nability under zero-shot transfer settings, validating its robustness across\ntraffic categories and varying environmental conditions."}
{"id": "2509.17514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17514", "abs": "https://arxiv.org/abs/2509.17514", "authors": ["Tianyi Chen", "Pengxiao Lin", "Zhiwei Wang", "Zhi-Qin John Xu"], "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data", "comment": null, "summary": "State Space Models (SSMs) have emerged as promising alternatives to attention\nmechanisms, with the Mamba architecture demonstrating impressive performance\nand linear complexity for processing long sequences. However, the fundamental\ndifferences between Mamba and Transformer architectures remain incompletely\nunderstood. In this work, we use carefully designed synthetic tasks to reveal\nMamba's inherent limitations. Through experiments, we identify that Mamba's\nnonlinear convolution introduces an asymmetry bias that significantly impairs\nits ability to recognize symmetrical patterns and relationships. Using\ncomposite function and inverse sequence matching tasks, we demonstrate that\nMamba strongly favors compositional solutions over symmetrical ones and\nstruggles with tasks requiring the matching of reversed sequences. We show\nthese limitations stem not from the SSM module itself but from the nonlinear\nconvolution preceding it, which fuses token information asymmetrically. These\ninsights provide a new understanding of Mamba's constraints and suggest\nconcrete architectural improvements for future sequence models."}
{"id": "2509.17530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17530", "abs": "https://arxiv.org/abs/2509.17530", "authors": ["Sayanta Adhikari", "Vishnuprasadh Kumaravelu", "P. K. Srijith"], "title": "An Unlearning Framework for Continual Learning", "comment": null, "summary": "Growing concerns surrounding AI safety and data privacy have driven the\ndevelopment of Machine Unlearning as a potential solution. However, current\nmachine unlearning algorithms are designed to complement the offline training\nparadigm. The emergence of the Continual Learning (CL) paradigm promises\nincremental model updates, enabling models to learn new tasks sequentially.\nNaturally, some of those tasks may need to be unlearned to address safety or\nprivacy concerns that might arise. We find that applying conventional\nunlearning algorithms in continual learning environments creates two critical\nproblems: performance degradation on retained tasks and task relapse, where\npreviously unlearned tasks resurface during subsequent learning. Furthermore,\nmost unlearning algorithms require data to operate, which conflicts with CL's\nphilosophy of discarding past data. A clear need arises for unlearning\nalgorithms that are data-free and mindful of future learning. To that end, we\npropose UnCLe, an Unlearning framework for Continual Learning. UnCLe employs a\nhypernetwork that learns to generate task-specific network parameters, using\ntask embeddings. Tasks are unlearned by aligning the corresponding generated\nnetwork parameters with noise, without requiring any data. Empirical\nevaluations on several vision data sets demonstrate UnCLe's ability to\nsequentially perform multiple learning and unlearning operations with minimal\ndisruption to previously acquired knowledge."}
{"id": "2509.17621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17621", "abs": "https://arxiv.org/abs/2509.17621", "authors": ["Khoa Tran", "Hung-Cuong Trinh", "Vy-Rin Nguyen", "T. Nguyen-Thoi", "Vin Nguyen-Thai"], "title": "SeqBattNet: A Discrete-State Physics-Informed Neural Network with Aging Adaptation for Battery Modeling", "comment": null, "summary": "Accurate battery modeling is essential for reliable state estimation in\nmodern applications, such as predicting the remaining discharge time and\nremaining discharge energy in battery management systems. Existing approaches\nface several limitations: model-based methods require a large number of\nparameters; data-driven methods rely heavily on labeled datasets; and current\nphysics-informed neural networks (PINNs) often lack aging adaptation, or still\ndepend on many parameters, or continuously regenerate states. In this work, we\npropose SeqBattNet, a discrete-state PINN with built-in aging adaptation for\nbattery modeling, to predict terminal voltage during the discharge process.\nSeqBattNet consists of two components: (i) an encoder, implemented as the\nproposed HRM-GRU deep learning module, which generates cycle-specific aging\nadaptation parameters; and (ii) a decoder, based on the equivalent circuit\nmodel (ECM) combined with deep learning, which uses these parameters together\nwith the input current to predict voltage. The model requires only three basic\nbattery parameters and, when trained on data from a single cell, still achieves\nrobust performance. Extensive evaluations across three benchmark datasets (TRI,\nRT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms\nclassical sequence models and PINN baselines, achieving consistently lower RMSE\nwhile maintaining computational efficiency."}
{"id": "2509.17625", "categories": ["cs.LG", "cs.CY", "physics.soc-ph", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17625", "abs": "https://arxiv.org/abs/2509.17625", "authors": ["Blas Kolic", "Corrado Monti", "Gianmarco De Francisci Morales", "Marco Pangallo"], "title": "Comparing Data Assimilation and Likelihood-Based Inference on Latent State Estimation in Agent-Based Models", "comment": null, "summary": "In this paper, we present the first systematic comparison of Data\nAssimilation (DA) and Likelihood-Based Inference (LBI) in the context of\nAgent-Based Models (ABMs). These models generate observable time series driven\nby evolving, partially-latent microstates. Latent states need to be estimated\nto align simulations with real-world data -- a task traditionally addressed by\nDA, especially in continuous and equation-based models such as those used in\nweather forecasting. However, the nature of ABMs poses challenges for standard\nDA methods. Solving such issues requires adaptation of previous DA techniques,\nor ad-hoc alternatives such as LBI. DA approximates the likelihood in a\nmodel-agnostic way, making it broadly applicable but potentially less precise.\nIn contrast, LBI provides more accurate state estimation by directly leveraging\nthe model's likelihood, but at the cost of requiring a hand-crafted,\nmodel-specific likelihood function, which may be complex or infeasible to\nderive. We compare the two methods on the Bounded-Confidence Model, a\nwell-known opinion dynamics ABM, where agents are affected only by others\nholding sufficiently similar opinions. We find that LBI better recovers latent\nagent-level opinions, even under model mis-specification, leading to improved\nindividual-level forecasts. At the aggregate level, however, both methods\nperform comparably, and DA remains competitive across levels of aggregation\nunder certain parameter settings. Our findings suggest that DA is well-suited\nfor aggregate predictions, while LBI is preferable for agent-level inference."}
{"id": "2509.17665", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17665", "abs": "https://arxiv.org/abs/2509.17665", "authors": ["Katharina Simbeck", "Mariam Mahran"], "title": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models", "comment": "Accepted at AEQUITAS 2025: Workshop on Fairness and Bias in AI |\n  co-located with ECAI, October 26th, 2025, Bologna, Italy. 12 pages, 1 figure", "summary": "Despite growing research on bias in large language models (LLMs), most work\nhas focused on gender and race, with little attention to religious identity.\nThis paper explores how religion is internally represented in LLMs and how it\nintersects with concepts of violence and geography. Using mechanistic\ninterpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we\nanalyze latent feature activations across five models. We measure overlap\nbetween religion- and violence-related prompts and probe semantic patterns in\nactivation contexts. While all five religions show comparable internal\ncohesion, Islam is more frequently linked to features associated with violent\nlanguage. In contrast, geographic associations largely reflect real-world\nreligious demographics, revealing how models embed both factual distributions\nand cultural stereotypes. These findings highlight the value of structural\nanalysis in auditing not just outputs but also internal representations that\nshape model behavior."}
{"id": "2509.17693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17693", "abs": "https://arxiv.org/abs/2509.17693", "authors": ["Adam Weso≈Çowski", "Ronin Wu", "Karim Essafi"], "title": "Fast, Accurate and Interpretable Graph Classification with Topological Kernels", "comment": null, "summary": "We introduce a novel class of explicit feature maps based on topological\nindices that represent each graph by a compact feature vector, enabling fast\nand interpretable graph classification. Using radial basis function kernels on\nthese compact vectors, we define a measure of similarity between graphs. We\nperform evaluation on standard molecular datasets and observe that\nclassification accuracies based on single topological-index feature vectors\nunderperform compared to state-of-the-art substructure-based kernels. However,\nwe achieve significantly faster Gram matrix evaluation -- up to $20\\times$\nfaster -- compared to the Weisfeiler--Lehman subtree kernel. To enhance\nperformance, we propose two extensions: 1) concatenating multiple topological\nindices into an \\emph{Extended Feature Vector} (EFV), and 2) \\emph{Linear\nCombination of Topological Kernels} (LCTK) by linearly combining Radial Basis\nFunction kernels computed on feature vectors of individual topological graph\nindices. These extensions deliver up to $12\\%$ percent accuracy gains across\nall the molecular datasets. A complexity analysis highlights the potential for\nexponential quantum speedup for some of the vector components. Our results\nindicate that LCTK and EFV offer a favourable trade-off between accuracy and\nefficiency, making them strong candidates for practical graph learning\napplications."}
{"id": "2509.17695", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17695", "abs": "https://arxiv.org/abs/2509.17695", "authors": ["Leszek Sliwko"], "title": "Cluster Workload Allocation: A Predictive Approach Leveraging Machine Learning Efficiency", "comment": "This is the accepted version of the paper published in IEEE Access.\n  The final version is available at:\n  https://doi.org/10.1109/ACCESS.2024.3520422", "summary": "This research investigates how Machine Learning (ML) algorithms can assist in\nworkload allocation strategies by detecting tasks with node affinity operators\n(referred to as constraint operators), which constrain their execution to a\nlimited number of nodes. Using real-world Google Cluster Data (GCD) workload\ntraces and the AGOCS framework, the study extracts node attributes and task\nconstraints, then analyses them to identify suitable node-task pairings. It\nfocuses on tasks that can be executed on either a single node or fewer than a\nthousand out of 12.5k nodes in the analysed GCD cluster. Task constraint\noperators are compacted, pre-processed with one-hot encoding, and used as\nfeatures in a training dataset. Various ML classifiers, including Artificial\nNeural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge\nRegression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for\naccuracy and F1-scores. The final ensemble voting classifier model achieved 98%\naccuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable\nnode."}
{"id": "2509.17728", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17728", "abs": "https://arxiv.org/abs/2509.17728", "authors": ["Yara Zgheib", "Luca Calatroni", "Marc Antonini", "Roula Nassif"], "title": "A non-smooth regularization framework for learning over multitask graphs", "comment": null, "summary": "In this work, we consider learning over multitask graphs, where each agent\naims to estimate its own parameter vector. Although agents seek distinct\nobjectives, collaboration among them can be beneficial in scenarios where\nrelationships between tasks exist. Among the various approaches to promoting\nrelationships between tasks and, consequently, enhancing collaboration between\nagents, one notable method is regularization. While previous multitask learning\nstudies have focused on smooth regularization to enforce graph smoothness, this\nwork explores non-smooth regularization techniques that promote sparsity,\nmaking them particularly effective in encouraging piecewise constant\ntransitions on the graph. We begin by formulating a global regularized\noptimization problem, which involves minimizing the aggregate sum of individual\ncosts, regularized by a general non-smooth term designed to promote\npiecewise-constant relationships between the tasks of neighboring agents. Based\non the forward-backward splitting strategy, we propose a decentralized learning\napproach that enables efficient solutions to the regularized optimization\nproblem. Then, under convexity assumptions on the cost functions and\nco-regularization, we establish that the proposed approach converges in the\nmean-square-error sense within $O(\\mu)$ of the optimal solution of the globally\nregularized cost. For broader applicability and improved computational\nefficiency, we also derive closed-form expressions for commonly used non-smooth\n(and, possibly, non-convex) regularizers, such as the weighted sum of the\n$\\ell_0$-norm, $\\ell_1$-norm, and elastic net regularization. Finally, we\nillustrate both the theoretical findings and the effectiveness of the approach\nthrough simulations."}
{"id": "2509.17729", "categories": ["cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.17729", "abs": "https://arxiv.org/abs/2509.17729", "authors": ["Siming Zheng", "Meifang Lan", "Tong Wang", "Yuanyuan Lin"], "title": "A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis", "comment": null, "summary": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach."}
{"id": "2509.17730", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17730", "abs": "https://arxiv.org/abs/2509.17730", "authors": ["Bonan Zhang", "Zhongqi Chen", "Bowen Song", "Qinya Li", "Fan Wu", "Guihai Chen"], "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods."}
{"id": "2509.17734", "categories": ["cs.LG", "cs.CE", "62M10, 68T05, 86A08, 62P12", "I.2.6; I.5.1; G.3; J.2"], "pdf": "https://arxiv.org/pdf/2509.17734", "abs": "https://arxiv.org/abs/2509.17734", "authors": ["Pablo Rodr√≠guez-Bocca", "Guillermo Pereira", "Diego Kiedanski", "Soledad Collazo", "Sebasti√°n Basterrech", "Gerardo Rubino"], "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures", "comment": "Manuscript to appear in the proceedings of IJCNN 2025, in the\n  workshop entitled \"AI for a Cooler Planet: Tackling Environmental Challenges\n  with Neural Networks.'' Total pages: 14. Total figures: 9 (containing a total\n  of 27 images). Total tables: 1", "summary": "In recent years, great progress has been made in the field of forecasting\nmeteorological variables. Recently, deep learning architectures have made a\nmajor breakthrough in forecasting the daily average temperature over a ten-day\nhorizon. However, advances in forecasting events related to the maximum\ntemperature over short horizons remain a challenge for the community. A problem\nthat is even more complex consists in making predictions of the maximum daily\ntemperatures in the short, medium, and long term. In this work, we focus on\nforecasting events related to the maximum daily temperature over medium-term\nperiods (90 days). Therefore, instead of addressing the problem from a\nmeteorological point of view, this article tackles it from a climatological\npoint of view. Due to the complexity of this problem, a common approach is to\nframe the study as a temporal classification problem with the classes: maximum\ntemperature \"above normal\", \"normal\" or \"below normal\". From a practical point\nof view, we created a large historical dataset (from 1981 to 2018) collecting\ninformation from weather stations located in South America. In addition, we\nalso integrated exogenous information from the Pacific, Atlantic, and Indian\nOcean basins. We applied the AutoGluonTS platform to solve the above-mentioned\nproblem. This AutoML tool shows competitive forecasting performance with\nrespect to large operational platforms dedicated to tackling this\nclimatological problem; but with a \"relatively\" low computational cost in terms\nof time and resources."}
{"id": "2509.17738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17738", "abs": "https://arxiv.org/abs/2509.17738", "authors": ["Ting Han", "Linara Adilova", "Henning Petzka", "Jens Kleesiek", "Michael Kamp"], "title": "Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking", "comment": "Preprint version", "summary": "Neural collapse, i.e., the emergence of highly symmetric, class-wise\nclustered representations, is frequently observed in deep networks and is often\nassumed to reflect or enable generalization. In parallel, flatness of the loss\nlandscape has been theoretically and empirically linked to generalization. Yet,\nthe causal role of either phenomenon remains unclear: Are they prerequisites\nfor generalization, or merely by-products of training dynamics? We disentangle\nthese questions using grokking, a training regime in which memorization\nprecedes generalization, allowing us to temporally separate generalization from\ntraining dynamics and we find that while both neural collapse and relative\nflatness emerge near the onset of generalization, only flatness consistently\npredicts it. Models encouraged to collapse or prevented from collapsing\ngeneralize equally well, whereas models regularized away from flat solutions\nexhibit delayed generalization. Furthermore, we show theoretically that neural\ncollapse implies relative flatness under classical assumptions, explaining\ntheir empirical co-occurrence. Our results support the view that relative\nflatness is a potentially necessary and more fundamental property for\ngeneralization, and demonstrate how grokking can serve as a powerful probe for\nisolating its geometric underpinnings."}
{"id": "2509.17752", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.17752", "abs": "https://arxiv.org/abs/2509.17752", "authors": ["Miao Li", "Phuc Nguyen", "Christopher Tam", "Alexandra Morgan", "Kenneth Ge", "Rahul Bansal", "Linzi Yu", "Rima Arnaout", "Ramy Arnaout"], "title": "GEM-T: Generative Tabular Data via Fitting Moments", "comment": "18 pages, 4 figures", "summary": "Tabular data dominates data science but poses challenges for generative\nmodels, especially when the data is limited or sensitive. We present a novel\napproach to generating synthetic tabular data based on the principle of maximum\nentropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for\ntables.'' GEM-T directly captures nth-order interactions -- pairwise,\nthird-order, etc. -- among columns of training data. In extensive testing,\nGEM-T matches or exceeds deep neural network approaches previously regarded as\nstate-of-the-art in 23 of 34 publicly available datasets representing diverse\nsubject domains (68\\%). Notably, GEM-T involves orders-of-magnitude fewer\ntrainable parameters, demonstrating that much of the information in real-world\ndata resides in low-dimensional, potentially human-interpretable correlations,\nprovided that the input data is appropriately transformed first. Furthermore,\nMaxEnt better handles heterogeneous data types (continuous vs. discrete vs.\ncategorical), lack of local structure, and other features of tabular data.\nGEM-T represents a promising direction for light-weight high-performance\ngenerative models for structured data."}
{"id": "2509.17755", "categories": ["cs.LG", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17755", "abs": "https://arxiv.org/abs/2509.17755", "authors": ["Fizza Rubab", "Ntumba Elie Nsampi", "Martin Balint", "Felix Mujkanovic", "Hans-Peter Seidel", "Tobias Ritschel", "Thomas Leimk√ºhler"], "title": "Learning Neural Antiderivatives", "comment": null, "summary": "Neural fields offer continuous, learnable representations that extend beyond\ntraditional discrete formats in visual computing. We study the problem of\nlearning neural representations of repeated antiderivatives directly from a\nfunction, a continuous analogue of summed-area tables. Although widely used in\ndiscrete domains, such cumulative schemes rely on grids, which prevents their\napplicability in continuous neural contexts. We introduce and analyze a range\nof neural methods for repeated integration, including both adaptations of prior\nwork and novel designs. Our evaluation spans multiple input dimensionalities\nand integration orders, assessing both reconstruction quality and performance\nin downstream tasks such as filtering and rendering. These results enable\nintegrating classical cumulative operators into modern neural systems and offer\ninsights into learning tasks involving differential and integral operators."}
{"id": "2509.17784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17784", "abs": "https://arxiv.org/abs/2509.17784", "authors": ["Jin Li", "Shoujin Wang", "Qi Zhang", "Feng Liu", "Tongliang Liu", "Longbing Cao", "Shui Yu", "Fang Chen"], "title": "Revealing Multimodal Causality with Large Language Models", "comment": "Accepted at NeurIPS 2025", "summary": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data."}
{"id": "2509.17791", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17791", "abs": "https://arxiv.org/abs/2509.17791", "authors": ["Robert Hu", "Carlo Luschi", "Paul Balanca"], "title": "Elucidating the Design Space of FP4 training", "comment": null, "summary": "The increasing computational demands of foundation models have spurred\nresearch into low-precision training, with 4-bit floating-point (\\texttt{FP4})\nformats emerging as a frontier for maximizing hardware throughput. While\nnumerous techniques have been proposed to stabilize \\texttt{FP4} training, they\noften present isolated solutions with varying, and not always clear,\ncomputational overheads. This paper aims to provide a unified view of the\ndesign space of \\texttt{FP4} training. We introduce a comprehensive,\nquantisation gradient-based framework for microscaling quantization that allows\nfor a theoretical analysis of the computational costs associated with different\nstabilization methods on both the forward and backward passes. Using a\nsimulator built on this framework, we conduct an extensive empirical study\nacross a wide range of machine learning tasks, including regression, image\nclassification, diffusion models, and language models. By systematically\nevaluating thousands of combinations of techniques, such as novel gradient\napproximations, rounding strategies, and scaling methods, we identify which\nconfigurations offer the most favourable performance-to-overhead trade-off. We\nfind that the techniques enabling the best trade-off involve carefully\ncombining Hadamard transformations, tensor scaling and stochastic rounding. We\nfurther find that using \\texttt{UE5M3} as a scaling factor potentially offers a\ngood compromise between range and precision with manageable computational\noverhead."}
{"id": "2509.17808", "categories": ["cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.17808", "abs": "https://arxiv.org/abs/2509.17808", "authors": ["Yuxi Lu", "Biao Wu", "Zhidong Li", "Kunqi Li", "Chenya Huang", "Huacan Wang", "Qizhen Lan", "Ronghao Chen", "Ling Chen", "Bin Liang"], "title": "Remote Sensing-Oriented World Model", "comment": "10 pages, 5 figures", "summary": "World models have shown potential in artificial intelligence by predicting\nand reasoning about world states beyond direct observations. However, existing\napproaches are predominantly evaluated in synthetic environments or constrained\nscene settings, limiting their validation in real-world contexts with broad\nspatial coverage and complex semantics. Meanwhile, remote sensing applications\nurgently require spatial reasoning capabilities for disaster response and urban\nplanning. This paper bridges these gaps by introducing the first framework for\nworld modeling in remote sensing. We formulate remote sensing world modeling as\ndirection-conditioned spatial extrapolation, where models generate semantically\nconsistent adjacent image tiles given a central observation and directional\ninstruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing\nWorld-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks\nacross four scenarios: general, flood, urban, and rural. RSWISE combines visual\nfidelity assessment with instruction compliance evaluation using GPT-4o as a\nsemantic judge, ensuring models genuinely perform spatial reasoning rather than\nsimple replication. Afterwards, we present RemoteBAGEL, a unified multimodal\nmodel fine-tuned on remote sensing data for spatial extrapolation tasks.\nExtensive experiments demonstrate that RemoteBAGEL consistently outperforms\nstate-of-the-art baselines on RSWISE."}
{"id": "2509.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17809", "abs": "https://arxiv.org/abs/2509.17809", "authors": ["Shuhan Zhong", "Weipeng Zhuo", "Sizhe Song", "Guanyao Li", "Zhongyi Yu", "S. -H. Gary Chan"], "title": "MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification", "comment": "KDD 2025", "summary": "Irregular multivariate time series (IMTS) is characterized by the lack of\nsynchronized observations across its different channels. In this paper, we\npoint out that this channel-wise asynchrony can lead to poor channel-wise\nmodeling of existing deep learning methods. To overcome this limitation, we\npropose MTM, a multi-scale token mixing transformer for the classification of\nIMTS. We find that the channel-wise asynchrony can be alleviated by\ndown-sampling the time series to coarser timescales, and propose to incorporate\na masked concat pooling in MTM that gradually down-samples IMTS to enhance the\nchannel-wise attention modules. Meanwhile, we propose a novel channel-wise\ntoken mixing mechanism which proactively chooses important tokens from one\nchannel and mixes them with other channels, to further boost the channel-wise\nlearning of our model. Through extensive experiments on real-world datasets and\ncomparison with state-of-the-art methods, we demonstrate that MTM consistently\nachieves the best performance on all the benchmarks, with improvements of up to\n3.8% in AUPRC for classification."}
{"id": "2509.17811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17811", "abs": "https://arxiv.org/abs/2509.17811", "authors": ["Thrinadh Pinjala", "Aswin Ram Kumar Gannina", "Debasis Dwibedy"], "title": "MSGAT-GRU: A Multi-Scale Graph Attention and Recurrent Model for Spatiotemporal Road Accident Prediction", "comment": "16 pages, 4 figures, 4 tables", "summary": "Accurate prediction of road accidents remains challenging due to intertwined\nspatial, temporal, and contextual factors in urban traffic. We propose\nMSGAT-GRU, a multi-scale graph attention and recurrent model that jointly\ncaptures localized and long-range spatial dependencies while modeling\nsequential dynamics. Heterogeneous inputs, such as traffic flow, road\nattributes, weather, and points of interest, are systematically fused to\nenhance robustness and interpretability. On the Hybrid Beijing Accidents\ndataset, MSGAT-GRU achieves an RMSE of 0.334 and an F1-score of 0.878,\nconsistently outperforming strong baselines. Cross-dataset evaluation on\nMETR-LA under a 1-hour horizon further supports transferability, with RMSE of\n6.48 (vs. 7.21 for the GMAN model) and comparable MAPE. Ablations indicate that\nthree-hop spatial aggregation and a two-layer GRU offer the best\naccuracy-stability trade-off. These results position MSGAT-GRU as a scalable\nand generalizable model for intelligent transportation systems, providing\ninterpretable signals that can inform proactive traffic management and road\nsafety analytics."}
{"id": "2509.17815", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17815", "abs": "https://arxiv.org/abs/2509.17815", "authors": ["Andrea Agazzi", "Vittorio Carlei", "Marco Romito", "Samuele Saviozzi"], "title": "Global Optimization via Softmin Energy Minimization", "comment": null, "summary": "Global optimization, particularly for non-convex functions with multiple\nlocal minima, poses significant challenges for traditional gradient-based\nmethods. While metaheuristic approaches offer empirical effectiveness, they\noften lack theoretical convergence guarantees and may disregard available\ngradient information. This paper introduces a novel gradient-based swarm\nparticle optimization method designed to efficiently escape local minima and\nlocate global optima. Our approach leverages a \"Soft-min Energy\" interacting\nfunction, $J_\\beta(\\mathbf{x})$, which provides a smooth, differentiable\napproximation of the minimum function value within a particle swarm. We define\na stochastic gradient flow in the particle space, incorporating a Brownian\nmotion term for exploration and a time-dependent parameter $\\beta$ to control\nsmoothness, similar to temperature annealing. We theoretically demonstrate that\nfor strongly convex functions, our dynamics converges to a stationary point\nwhere at least one particle reaches the global minimum, with other particles\nexhibiting exploratory behavior. Furthermore, we show that our method\nfacilitates faster transitions between local minima by reducing effective\npotential barriers with respect to Simulated Annealing. More specifically, we\nestimate the hitting times of unexplored potential wells for our model in the\nsmall noise regime and show that they compare favorably with the ones of\noverdamped Langevin. Numerical experiments on benchmark functions, including\ndouble wells and the Ackley function, validate our theoretical findings and\ndemonstrate better performance over the well-known Simulated Annealing method\nin terms of escaping local minima and achieving faster convergence."}
{"id": "2509.17845", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17845", "abs": "https://arxiv.org/abs/2509.17845", "authors": ["Kai Zhang", "Siming Sun", "Zhengyu Fan", "Qinmin Yang", "Xuejun Jiang"], "title": "Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series", "comment": null, "summary": "Time series analysis faces significant challenges in handling variable-length\ndata and achieving robust generalization. While Transformer-based models have\nadvanced time series tasks, they often struggle with feature redundancy and\nlimited generalization capabilities. Drawing inspiration from classical CNN\narchitectures' pyramidal structure, we propose a Multi-Scale Representation\nLearning Framework based on a Conv-like ScaleFusion Transformer. Our approach\nintroduces a temporal convolution-like structure that combines patching\noperations with multi-head attention, enabling progressive temporal dimension\ncompression and feature channel expansion. We further develop a novel\ncross-scale attention mechanism for effective feature fusion across different\ntemporal scales, along with a log-space normalization method for\nvariable-length sequences. Extensive experiments demonstrate that our framework\nachieves superior feature independence, reduced redundancy, and better\nperformance in forecasting and classification tasks compared to\nstate-of-the-art methods."}
{"id": "2509.17866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17866", "abs": "https://arxiv.org/abs/2509.17866", "authors": ["Xinyu He", "Xianghui Cao"], "title": "Understanding Post-Training Structural Changes in Large Language Models", "comment": "38 pages, 26 figures", "summary": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes."}
{"id": "2509.17870", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17870", "abs": "https://arxiv.org/abs/2509.17870", "authors": ["Xiao Mao", "Albert H. Schrotenboer", "Guohua Wu", "Willem van Jaarsveld"], "title": "Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences", "comment": null, "summary": "Problem definition: For original equipment manufacturers (OEMs), high-tech\nmaintenance is a strategic component in after-sales services, involving close\ncoordination between customers and service engineers. Each customer suggests\nseveral time slots for their maintenance task, from which the OEM must select\none. This decision needs to be made promptly to support customers' planning. At\nthe end of each day, routes for service engineers are planned to fulfill the\ntasks scheduled for the following day. We study this hierarchical and\nsequential decision-making problem-the Dynamic Time Slot Assignment Problem\nwith Commitments and Customer Preferences (DTSAP-CCP)-in this paper.\nMethodology/results: Two distinct approaches are proposed: 1) an\nattention-based deep reinforcement learning with rollout execution (ADRL-RE)\nand 2) a scenario-based planning approach (SBP). The ADRL-RE combines a\nwell-trained attention-based neural network with a rollout framework for online\ntrajectory simulation. To support the training, we develop a neural heuristic\nsolver that provides rapid route planning solutions, enabling efficient\nlearning in complex combinatorial settings. The SBP approach samples several\nscenarios to guide the time slot assignment. Numerical experiments demonstrate\nthe superiority of ADRL-RE and the stability of SBP compared to both rule-based\nand rollout-based approaches. Furthermore, the strong practicality of ADRL-RE\nis verified in a case study of after-sales service for large medical equipment.\nImplications: This study provides OEMs with practical decision-support tools\nfor dynamic maintenance scheduling, balancing customer preferences and\noperational efficiency. In particular, our ADRL-RE shows strong real-world\npotential, supporting timely and customer-aligned maintenance scheduling."}
{"id": "2509.17874", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17874", "abs": "https://arxiv.org/abs/2509.17874", "authors": ["Paulius Rauba", "Mihaela van der Schaar"], "title": "Deep Hierarchical Learning with Nested Subspace Networks", "comment": null, "summary": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels."}
{"id": "2509.17885", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17885", "abs": "https://arxiv.org/abs/2509.17885", "authors": ["Saad Mokssit", "Ouassim Karrakchou", "Alejandro Mousist", "Mounir Ghogho"], "title": "Confidence-gated training for efficient early-exit neural networks", "comment": null, "summary": "Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments."}
{"id": "2509.17889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17889", "abs": "https://arxiv.org/abs/2509.17889", "authors": ["Phuong Mai Dinh", "Van-Nam Huynh"], "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization", "comment": null, "summary": "Multi-objective optimization (MOO) is essential for solving complex\nreal-world problems involving multiple conflicting objectives. However, many\npractical applications - including engineering design, autonomous systems, and\nmachine learning - often yield non-convex, degenerate, or discontinuous Pareto\nfrontiers, which involve traditional scalarization and Pareto Set Learning\n(PSL) methods that struggle to approximate accurately. Existing PSL approaches\nperform well on convex fronts but tend to fail in capturing the diversity and\nstructure of irregular Pareto sets commonly observed in real-world scenarios.\nIn this paper, we propose Gaussian-PSL, a novel framework that integrates\nGaussian Splatting into PSL to address the challenges posed by non-convex\nPareto frontiers. Our method dynamically partitions the preference vector\nspace, enabling simple MLP networks to learn localized features within each\nregion, which are then integrated by an additional MLP aggregator. This\npartition-aware strategy enhances both exploration and convergence, reduces\nsensi- tivity to initialization, and improves robustness against local optima.\nWe first provide the mathematical formulation for controllable Pareto set\nlearning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL\narchitecture and evaluate its performance on synthetic and real-world\nmulti-objective benchmarks. Experimental results demonstrate that our approach\noutperforms standard PSL models in learning irregular Pareto fronts while\nmaintaining computational efficiency and model simplicity. This work offers a\nnew direction for effective and scalable MOO under challenging frontier\ngeometries."}
{"id": "2509.17894", "categories": ["cs.LG", "68T07", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.17894", "abs": "https://arxiv.org/abs/2509.17894", "authors": ["Siu Hang Ho", "Prasad Ganesan", "Nguyen Duong", "Daniel Schlabig"], "title": "Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark", "comment": "6 pages, 4 figures. Technical report", "summary": "Efficient inference is a critical challenge in deep generative modeling,\nparticularly as diffusion models grow in capacity and complexity. While\nincreased complexity often improves accuracy, it raises compute costs, latency,\nand memory requirements. This work investigates techniques such as pruning,\nquantization, knowledge distillation, and simplified attention to reduce\ncomputational overhead without impacting performance. The study also explores\nthe Mixture of Experts (MoE) approach to further enhance efficiency. These\nexperiments provide insights into optimizing inference for the state-of-the-art\nFast Diffusion Transformer (fast-DiT) model."}
{"id": "2509.17920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17920", "abs": "https://arxiv.org/abs/2509.17920", "authors": ["Jamiyan Sukhbaatar", "Satoshi Imamura", "Ibuki Inoue", "Shoya Murakami", "Kazi Mahmudul Hassan", "Seungwoo Han", "Ingon Chanpornpakdi", "Toshihisa Tanaka"], "title": "SingLEM: Single-Channel Large EEG Model", "comment": null, "summary": "Current deep learning models for electroencephalography (EEG) are often\ntask-specific and depend on large labeled datasets, limiting their\nadaptability. Although emerging foundation models aim for broader\napplicability, their rigid dependence on fixed, high-density multi-channel\nmontages restricts their use across heterogeneous datasets and in\nmissing-channel or practical low-channel settings. To address these\nlimitations, we introduce SingLEM, a self-supervised foundation model that\nlearns robust, general-purpose representations from single-channel EEG, making\nit inherently hardware agnostic. The model employs a hybrid encoder\narchitecture that combines convolutional layers to extract local features with\na hierarchical transformer to model both short- and long-range temporal\ndependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200\nsubjects and 357,000 single-channel hours of EEG. When evaluated as a fixed\nfeature extractor across six motor imagery and cognitive tasks, aggregated\nsingle-channel representations consistently outperformed leading multi-channel\nfoundation models and handcrafted baselines. These results demonstrate that a\nsingle-channel approach can achieve state-of-the-art generalization while\nenabling fine-grained neurophysiological analysis and enhancing\ninterpretability. The source code and pretrained models are available at\nhttps://github.com/ttlabtuat/SingLEM."}
{"id": "2509.17924", "categories": ["cs.LG", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2509.17924", "abs": "https://arxiv.org/abs/2509.17924", "authors": ["Xiuqi Ge", "Zhibo Yao", "Yaosong Du"], "title": "Medical priority fusion: achieving dual optimization of sensitivity and interpretability in nipt anomaly detection", "comment": "24 pages, 47 figures, publish to BIBM", "summary": "Clinical machine learning faces a critical dilemma in high-stakes medical\napplications: algorithms achieving optimal diagnostic performance typically\nsacrifice the interpretability essential for physician decision-making, while\ninterpretable methods compromise sensitivity in complex scenarios. This paradox\nbecomes particularly acute in non-invasive prenatal testing (NIPT), where\nmissed chromosomal abnormalities carry profound clinical consequences yet\nregulatory frameworks mandate explainable AI systems. We introduce Medical\nPriority Fusion (MPF), a constrained multi-objective optimization framework\nthat resolves this fundamental trade-off by systematically integrating Naive\nBayes probabilistic reasoning with Decision Tree rule-based logic through\nmathematically-principled weighted fusion under explicit medical constraints.\nRigorous validation on 1,687 real-world NIPT samples characterized by extreme\nclass imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold\ncross-validation with comprehensive ablation studies and statistical hypothesis\ntesting using McNemar's paired comparisons. MPF achieved simultaneous\noptimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with\n80% interpretability score, significantly outperforming individual algorithms\n(McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A\nclinical deployment criteria with large effect size (d = 1.24), establishing\nthe first clinically-deployable solution that maintains both diagnostic\naccuracy and decision transparency essential for prenatal care. This work\ndemonstrates that medical-constrained algorithm fusion can resolve the\ninterpretability-performance trade-off, providing a mathematical framework for\ndeveloping high-stakes medical decision support systems that meet both clinical\nefficacy and explainability requirements."}
{"id": "2509.17942", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17942", "abs": "https://arxiv.org/abs/2509.17942", "authors": ["Nicholas Kraabel", "Jiangtao Liu", "Yuchen Bian", "Daniel Kifer", "Chaopeng Shen"], "title": "StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions", "comment": null, "summary": "Stewarding natural resources, mitigating floods, droughts, wildfires, and\nlandslides, and meeting growing demands require models that can predict\nclimate-driven land-surface responses and human feedback with high accuracy.\nTraditional impact models, whether process-based, statistical, or machine\nlearning, struggle with spatial generalization due to limited observations and\nconcept drift. Recently proposed vision foundation models trained on satellite\nimagery demand massive compute and are ill-suited for dynamic land-surface\nprediction. We introduce StefaLand, a generative spatiotemporal earth\nfoundation model centered on landscape interactions. StefaLand improves\npredictions on three tasks and four datasets: streamflow, soil moisture, and\nsoil composition, compared to prior state-of-the-art. Results highlight its\nability to generalize across diverse, data-scarce regions and support broad\nland-surface applications. The model builds on a masked autoencoder backbone\nthat learns deep joint representations of landscape attributes, with a\nlocation-aware architecture fusing static and time-series inputs,\nattribute-based representations that drastically reduce compute, and residual\nfine-tuning adapters that enhance transfer. While inspired by prior methods,\ntheir alignment with geoscience and integration in one model enables robust\nperformance on dynamic land-surface tasks. StefaLand can be pretrained and\nfinetuned on academic compute yet outperforms state-of-the-art baselines and\neven fine-tuned vision foundation models. To our knowledge, this is the first\ngeoscience land-surface foundation model that demonstrably improves dynamic\nland-surface interaction predictions and supports diverse downstream\napplications."}
{"id": "2509.17970", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17970", "abs": "https://arxiv.org/abs/2509.17970", "authors": ["Yunchu Han", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Joint Memory Frequency and Computing Frequency Scaling for Energy-efficient DNN Inference", "comment": null, "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices."}
{"id": "2509.17971", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17971", "abs": "https://arxiv.org/abs/2509.17971", "authors": ["Tan-Ha Mai", "Hsuan-Tien Lin"], "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning", "comment": "22 pages, 10 figures", "summary": "In this paper, we investigate the challenges of complementary-label learning\n(CLL), a specialized form of weakly-supervised learning (WSL) where models are\ntrained with labels indicating classes to which instances do not belong, rather\nthan standard ordinary labels. This alternative supervision is appealing\nbecause collecting complementary labels is generally cheaper and less\nlabor-intensive. Although most existing research in CLL emphasizes the\ndevelopment of novel loss functions, the potential of data augmentation in this\ndomain remains largely underexplored. In this work, we uncover that the\nwidely-used Mixup data augmentation technique is ineffective when directly\napplied to CLL. Through in-depth analysis, we identify that the\ncomplementary-label noise generated by Mixup negatively impacts the performance\nof CLL models. We then propose an improved technique called Intra-Cluster Mixup\n(ICM), which only synthesizes augmented data from nearby examples, to mitigate\nthe noise effect. ICM carries the benefits of encouraging complementary label\nsharing of nearby examples, and leads to substantial performance improvements\nacross synthetic and real-world labeled datasets. In particular, our wide\nspectrum of experimental results on both balanced and imbalanced CLL settings\njustifies the potential of ICM in allying with state-of-the-art CLL algorithms,\nachieving significant accuracy increases of 30% and 10% on MNIST and CIFAR\ndatasets, respectively."}
{"id": "2509.17987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17987", "abs": "https://arxiv.org/abs/2509.17987", "authors": ["Sanju Xaviar", "Omid Ardakanian"], "title": "Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks", "comment": "12 pages", "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for anomaly\ndetection in sensor networks, particularly when analyzing multivariate time\nseries. In this work, we introduce BETA, a novel grey-box evasion attack\ntargeting such GNN-based detectors, where the attacker is constrained to\nperturb sensor readings from a limited set of nodes, excluding the target\nsensor, with the goal of either suppressing a true anomaly or triggering a\nfalse alarm at the target node. BETA identifies the sensors most influential to\nthe target node's classification and injects carefully crafted adversarial\nperturbations into their features, all while maintaining stealth and respecting\nthe attacker's budget. Experiments on three real-world sensor network datasets\nshow that BETA reduces the detection accuracy of state-of-the-art GNN-based\ndetectors by 30.62 to 39.16% on average, and significantly outperforms baseline\nattack strategies, while operating within realistic constraints."}
{"id": "2509.17990", "categories": ["cs.LG", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2509.17990", "abs": "https://arxiv.org/abs/2509.17990", "authors": ["Yanbo Zhang", "Michael Levin"], "title": "Equilibrium flow: From Snapshots to Dynamics", "comment": "17 pages, 8 figures", "summary": "Scientific data, from cellular snapshots in biology to celestial\ndistributions in cosmology, often consists of static patterns from underlying\ndynamical systems. These snapshots, while lacking temporal ordering, implicitly\nencode the processes that preserve them. This work investigates how strongly\nsuch a distribution constrains its underlying dynamics and how to recover them.\nWe introduce the Equilibrium flow method, a framework that learns continuous\ndynamics that preserve a given pattern distribution. Our method successfully\nidentifies plausible dynamics for 2-D systems and recovers the signature\nchaotic behavior of the Lorenz attractor. For high-dimensional Turing patterns\nfrom the Gray-Scott model, we develop an efficient, training-free variant that\nachieves high fidelity to the ground truth, validated both quantitatively and\nqualitatively. Our analysis reveals the solution space is constrained not only\nby the data but also by the learning model's inductive biases. This capability\nextends beyond recovering known systems, enabling a new paradigm of inverse\ndesign for Artificial Life. By specifying a target pattern distribution, we can\ndiscover the local interaction rules that preserve it, leading to the\nspontaneous emergence of complex behaviors, such as life-like flocking,\nattraction, and repulsion patterns, from simple, user-defined snapshots."}
{"id": "2509.17998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17998", "abs": "https://arxiv.org/abs/2509.17998", "authors": ["Richard Cornelius Suwandi", "Feng Yin", "Juntao Wang", "Renjie Li", "Tsung-Hui Chang", "Sergios Theodoridis"], "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs", "comment": "Accepted as Poster at NeurIPS 2025", "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/richardcsuwandi/cake."}
{"id": "2509.18001", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18001", "abs": "https://arxiv.org/abs/2509.18001", "authors": ["Haocheng Luo", "Mehrtash Harandi", "Dinh Phung", "Trung Le"], "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise", "comment": null, "summary": "Sharpness-aware minimization (SAM) has emerged as a highly effective\ntechnique for improving model generalization, but its underlying principles are\nnot fully understood. We investigated the phenomenon known as m-sharpness,\nwhere the performance of SAM improves monotonically as the micro-batch size for\ncomputing perturbations decreases. Leveraging an extended Stochastic\nDifferential Equation (SDE) framework, combined with an analysis of the\nstructure of stochastic gradient noise (SGN), we precisely characterize the\ndynamics of various SAM variants. Our findings reveal that the stochastic noise\nintroduced during SAM perturbations inherently induces a variance-based\nsharpness regularization effect. Motivated by our theoretical insights, we\nintroduce Reweighted SAM, which employs sharpness-weighted sampling to mimic\nthe generalization benefits of m-SAM while remaining parallelizable.\nComprehensive experiments validate the effectiveness of our theoretical\nanalysis and proposed method."}
{"id": "2509.18034", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18034", "abs": "https://arxiv.org/abs/2509.18034", "authors": ["Erkan Bayram", "Mohamed-Ali Belabbas", "Tamer Ba≈üar"], "title": "Control Disturbance Rejection in Neural ODEs", "comment": "Accepted for publication in IEEE CDC 2025", "summary": "In this paper, we propose an iterative training algorithm for Neural ODEs\nthat provides models resilient to control (parameter) disturbances. The method\nbuilds on our earlier work Tuning without Forgetting-and similarly introduces\ntraining points sequentially, and updates the parameters on new data within the\nspace of parameters that do not decrease performance on the previously learned\ntraining points-with the key difference that, inspired by the concept of flat\nminima, we solve a minimax problem for a non-convex non-concave functional over\nan infinite-dimensional control space. We develop a projected gradient descent\nalgorithm on the space of parameters that admits the structure of an\ninfinite-dimensional Banach subspace. We show through simulations that this\nformulation enables the model to effectively learn new data points and gain\nrobustness against control disturbance."}
{"id": "2509.18057", "categories": ["cs.LG", "cs.AI", "cs.CC", "math.CO"], "pdf": "https://arxiv.org/pdf/2509.18057", "abs": "https://arxiv.org/abs/2509.18057", "authors": ["Ansh Nagda", "Prabhakar Raghavan", "Abhradeep Thakurta"], "title": "Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory", "comment": null, "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."}
{"id": "2509.18058", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18058", "abs": "https://arxiv.org/abs/2509.18058", "authors": ["Alexander Panfilov", "Evgenii Kortukov", "Kristina Nikoliƒá", "Matthias Bethge", "Sebastian Lapuschkin", "Wojciech Samek", "Ameya Prabhu", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs", "comment": null, "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict."}
{"id": "2509.18067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18067", "abs": "https://arxiv.org/abs/2509.18067", "authors": ["Boyang Zhang", "Quanqi Hu", "Mingxuan Sun", "Qihang Lin", "Tianbao Yang"], "title": "Learning to Rank with Top-$K$ Fairness", "comment": "Already accepted: https://openreview.net/forum?id=SSPCc39XvO\n  @article{ zhang2025learning, title={Learning to Rank with Top-\\$K\\$\n  Fairness}, author={Boyang Zhang and Quanqi Hu and Mingxuan Sun and Qihang Lin\n  and Tianbao Yang}, journal={Transactions on Machine Learning Research},\n  issn={2835-8856}, year={2025},\n  url={https://openreview.net/forum?id=SSPCc39XvO}, note={} }", "summary": "Fairness in ranking models is crucial, as disparities in exposure can\ndisproportionately affect protected groups. Most fairness-aware ranking systems\nfocus on ensuring comparable average exposure for groups across the entire\nranked list, which may not fully address real-world concerns. For example, when\na ranking model is used for allocating resources among candidates or disaster\nhotspots, decision-makers often prioritize only the top-$K$ ranked items, while\nthe ranking beyond top-$K$ becomes less relevant. In this paper, we propose a\nlist-wise learning-to-rank framework that addresses the issues of inequalities\nin top-$K$ rankings at training time. Specifically, we propose a top-$K$\nexposure disparity measure that extends the classic exposure disparity metric\nin a ranked list. We then learn a ranker to balance relevance and fairness in\ntop-$K$ rankings. Since direct top-$K$ selection is computationally expensive\nfor a large number of items, we transform the non-differentiable selection\nprocess into a differentiable objective function and develop efficient\nstochastic optimization algorithms to achieve both high accuracy and sufficient\nfairness. Extensive experiments demonstrate that our method outperforms\nexisting methods."}
{"id": "2509.18071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18071", "abs": "https://arxiv.org/abs/2509.18071", "authors": ["Lorenzo Rosasco"], "title": "Learning functions, operators and dynamical systems with kernels", "comment": null, "summary": "This expository article presents the approach to statistical machine learning\nbased on reproducing kernel Hilbert spaces. The basic framework is introduced\nfor scalar-valued learning and then extended to operator learning. Finally,\nlearning dynamical systems is formulated as a suitable operator learning\nproblem, leveraging Koopman operator theory. The manuscript collects the\nsupporting material for the corresponding course taught at the CIME school\n\"Machine Learning: From Data to Mathematical Understanding\" in Cetraro."}
{"id": "2509.18085", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18085", "abs": "https://arxiv.org/abs/2509.18085", "authors": ["Sudhanshu Agrawal", "Risheek Garrepalli", "Raghavv Goel", "Mingu Lee", "Christopher Lott", "Fatih Porikli"], "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding", "comment": null, "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."}
{"id": "2509.16505", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16505", "abs": "https://arxiv.org/abs/2509.16505", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "orb-QFL: Orbital Quantum Federated Learning", "comment": null, "summary": "Recent breakthroughs in quantum computing present transformative\nopportunities for advancing Federated Learning (FL), particularly in\nnon-terrestrial environments characterized by stringent communication and\ncoordination constraints. In this study, we propose orbital QFL, termed\norb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low\nEarth Orbit (LEO) satellite constellations. Distinct from conventional FL\nparadigms, termed orb-QFL operates without centralized servers or global\naggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement\nand local quantum processing to facilitate decentralized, inter-satellite\ncollaboration. This design inherently addresses the challenges of orbital\ndynamics, such as intermittent connectivity, high propagation delays, and\ncoverage variability. The framework enables continuous model refinement through\ndirect quantum-based synchronization between neighboring satellites, thereby\nenhancing resilience and preserving data locality. To validate our approach, we\nintegrate the Qiskit quantum machine learning toolkit with Poliastro-based\norbital simulations and conduct experiments using Statlog dataset."}
{"id": "2509.16857", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16857", "abs": "https://arxiv.org/abs/2509.16857", "authors": ["Xingyu Xiang", "Raj Joshi", "Yuhan Liu", "Jiayi Yao", "Chenxingyu Zhao", "Junchen Jiang", "Yang Zhou", "Eddie Kohler", "Minlan Yu"], "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching", "comment": null, "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."}
{"id": "2509.17324", "categories": ["cs.ET", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.17324", "abs": "https://arxiv.org/abs/2509.17324", "authors": ["Chi Zhang", "Mengxin Zheng", "Qian Lou", "Fan Chen"], "title": "DiffQ: Unified Parameter Initialization for Variational Quantum Algorithms via Diffusion Models", "comment": null, "summary": "Variational Quantum Algorithms (VQAs) are widely used in the noisy\nintermediate-scale quantum (NISQ) era, but their trainability and performance\ndepend critically on initialization parameters that shape the optimization\nlandscape. Existing machine learning-based initializers achieve\nstate-of-the-art results yet remain constrained to single-task domains and\nsmall datasets of only hundreds of samples. We address these limitations by\nreformulating VQA parameter initialization as a generative modeling problem and\nintroducing DiffQ, a parameter initializer based on the Denoising Diffusion\nProbabilistic Model (DDPM). To support robust training and evaluation, we\nconstruct a dataset of 15,085 instances spanning three domains and five\nrepresentative tasks. Experiments demonstrate that DiffQ surpasses baselines,\nreducing initial loss by up to 8.95 and convergence steps by up to 23.4%."}
{"id": "2509.17533", "categories": ["cs.ET", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17533", "abs": "https://arxiv.org/abs/2509.17533", "authors": ["Anastasios Fanariotis", "Theofanis Orphanoudakis", "Vasilis Fotopoulos"], "title": "Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers", "comment": null, "summary": "The deployment of machine learning (ML) models on microcontrollers (MCUs) is\nconstrained by strict energy, latency, and memory requirements, particularly in\nbattery-operated and real-time edge devices. While software-level optimizations\nsuch as quantization and pruning reduce model size and computation, hardware\nacceleration has emerged as a decisive enabler for efficient embedded\ninference. This paper evaluates the impact of Neural Processing Units (NPUs) on\nMCU-based ML execution, using the ARM Cortex-M55 core combined with the\nEthos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a\nrepresentative platform. A rigorous measurement methodology was employed,\nincorporating per-inference net energy accounting via GPIO-triggered\nhigh-resolution digital multimeter synchronization and idle-state subtraction,\nensuring accurate attribution of energy costs. Experimental results across six\nrepresentative ML models -including MiniResNet, MobileNetV2, FD-MobileNet,\nMNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains\nwhen inference is offloaded to the NPU. For moderate to large networks, latency\nimprovements ranged from 7x to over 125x, with per-inference net energy\nreductions up to 143x. Notably, the NPU enabled execution of models unsupported\non CPU-only paths, such as SSD-MobileNet, highlighting its functional as well\nas efficiency advantages. These findings establish NPUs as a cornerstone of\nenergy-aware embedded AI, enabling real-time, power-constrained ML inference at\nthe MCU level."}
