<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 164]
- [cs.AR](#cs.AR) [Total: 9]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning Dynamics in Memristor-Based Equilibrium Propagation](https://arxiv.org/abs/2512.12428)
*Michael Döll,Andreas Müller,Bernd Ulmann*

Main category: cs.LG

TL;DR: 该论文研究了基于忆阻器的内存计算中非线性权重更新对平衡传播训练神经网络收敛行为的影响，发现忆阻器需要至少一个数量级的电阻范围才能实现稳健收敛。


<details>
  <summary>Details</summary>
Motivation: 忆阻器内存计算能克服冯·诺依曼瓶颈和内存墙问题，实现完全并行化和高能效的向量矩阵乘法。但忆阻器驱动的非线性权重更新如何影响神经网络训练收敛尚不清楚，需要研究平衡传播训练方法在非线性权重更新下的表现。

Method: 使用六个忆阻器模型，通过电压-电流迟滞特性进行表征，并集成到EBANA框架中。在两个基准分类任务上评估平衡传播训练方法，分析非线性权重更新对神经网络收敛行为的影响。

Result: 平衡传播能够在非线性权重更新下实现稳健收敛，前提是忆阻器具有足够宽的电阻范围（至少一个数量级）。六个忆阻器模型在分类任务上的表现验证了这一发现。

Conclusion: 忆阻器内存计算系统在平衡传播训练下能够有效工作，但需要忆阻器器件具有足够大的电阻变化范围来确保训练收敛。这为设计实用的忆阻器神经网络硬件提供了重要指导。

Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.

</details>


### [2] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: SeVeDo：基于SVD的异构加速器，通过将异常值敏感组件分离到高精度低秩路径，其余计算在低比特残差数据路径中执行，结合分层组量化和SVD引导的混合精度，实现高效Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 低比特量化是高效Transformer推理的有前景技术，但激进的比特宽度减少会因激活异常值导致精度下降。现有方法（如异常值处理和组量化）虽然精度高，但能耗大。

Method: 提出SeVeDo异构加速器：1）基于SVD将异常值敏感组件结构分离到高精度低秩路径；2）其余计算在低比特残差数据路径中执行，采用组量化；3）提出分层组量化（HGQ）结合粗粒度浮点缩放和细粒度移位；4）SVD引导混合精度（SVD-MP）为精度敏感组件静态分配更高比特宽度。

Result: SeVeDo实现峰值能效13.8TOPS/W，在ViT-Base上达到12.7TOPS/W，在Llama2-7B上达到13.4TOPS/W，优于传统设计。

Conclusion: SeVeDo通过SVD分解和异构架构有效解决了低比特量化中的异常值问题，在保持精度的同时显著提升了Transformer推理的能效。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [3] [Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning](https://arxiv.org/abs/2512.13196)
*Chethana Prasad Kabgere,Sudarshan T S B*

Main category: cs.LG

TL;DR: 提出NR-QFL框架，结合量子计算与联邦学习，解决车联网ADAS系统中传统FL的噪声、延迟和安全问题，实现安全低延迟的模型聚合。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在车联网环境中面临噪声、延迟和安全限制，需要更鲁棒、高效的解决方案来支持实时ADAS系统。

Method: 采用混合量子-经典框架，使用变分量子电路在NISQ条件下编码模型参数，通过自适应门重参数化确保收敛，结合量子熵客户端选择和多服务器协调机制。

Result: 实验验证显示NR-QFL能稳定收敛，降低梯度方差，减少通信开销，增强噪声容忍度，在受限边缘条件下表现优异。

Conclusion: NR-QFL为量子增强的联邦学习建立了可扩展基础，实现了车联网边缘安全、高效、动态稳定的ADAS智能系统。

Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.

</details>


### [4] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出价值配置文件框架，通过少量可重用参数包实现信念依赖的价值控制，在概率反转学习任务中优于传统模型


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，智能体需要在潜在情境间切换价值控制策略，但为每个情境维护独立参数是不可行的。需要一种更高效的方法来实现信念依赖的价值控制。

Method: 引入价值配置文件：少量可重用的价值相关参数包（结果偏好、策略先验、策略精度），分配给生成模型中的隐藏状态。通过信念加权混合产生有效控制参数，实现状态条件策略招募。

Result: 在概率反转学习任务中，配置文件模型优于静态精度和熵耦合动态精度模型（约100点AIC差异）。参数恢复分析支持结构可识别性。模型推断表明自适应控制主要由策略先验调制驱动。

Conclusion: 可重用价值配置文件为多变环境中的信念条件价值控制提供了可计算的理论框架，并产生了信念依赖控制和行为灵活性的可测试特征。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [5] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: 提出CR3G框架，通过因果推理提升胸部X光报告生成的质量和可解释性，关注模式间的因果关系而非仅相关性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医学影像分析中擅长发现相关性模式，但难以理解这些模式与患者状况之间的深层因果关系。需要超越模式识别，揭示X光发现与诊断之间的因果机制，以提升AI诊断的质量和临床可信度。

Method: 提出CR3G（Causal Reasoning for Patient-Centric Explanations in radiology Report Generation）框架，采用提示驱动方法，应用于胸部X光分析。该框架专注于因果关系推理，生成以患者为中心的解释性报告。

Result: CR3G在5种异常中，对2种异常展现出更好的因果关系识别能力和解释能力。

Conclusion: 因果推理方法能提升AI生成放射学报告的质量和可解释性，使AI诊断在临床实践中更有用、更可信。CR3G框架展示了在特定异常类型上的改进潜力。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [6] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出一个用于代表性捷径模型的通用设计框架，为有效性提供理论依据，解耦具体组件选择，实现系统改进，在ImageNet-256上达到SOTA的2.85 FID50k


<details>
  <summary>Details</summary>
Motivation: 当前少步扩散模型的理论推导和实际实现紧密耦合，模糊了设计空间，需要解耦设计组件以促进系统性改进

Method: 提出一个通用设计框架，为代表性捷径模型提供理论依据，解耦具体的组件级选择，识别改进方向

Result: 改进后的单步模型在ImageNet-256x256上达到2.85 FID50k的SOTA性能，无需预训练、蒸馏或课程学习

Conclusion: 该工作降低了捷径模型中组件级创新的门槛，促进了设计空间的原则性探索

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [7] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

TL;DR: 简单逆距离加权法（IDW）在稀疏气候数据重建中表现最佳，优于普通克里金法和先进的隐式神经表示模型，在精度和计算效率方面均占优。


<details>
  <summary>Details</summary>
Motivation: 评估不同重建方法在稀疏气候数据上的性能，为气候数据重建提供方法选择依据。

Method: 比较三种方法：简单逆距离加权法（IDW）、统计基础的普通克里金法（OK）和先进的隐式神经表示模型（MMGN架构）。通过验证分割进行超参数调优，并进行大量实验和统计分析。

Result: IDW在所有评估指标上表现最佳：最低的RMSE（3.00±1.93）、MAE（1.32±0.77）和Δ_MAX（24.06±17.15），以及最高的R²（0.68±0.16）。统计显著性检验证实IDW的优越性。

Conclusion: 对于稀疏气候数据重建，简单的IDW方法在精度和效率上均优于更复杂的统计和深度学习方法，表明简单方法在此类任务中可能更有效。

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [8] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: BOOST框架针对低秩瓶颈架构提出瓶颈感知张量并行等优化技术，相比全秩模型基线实现1.46-1.91倍加速，相比简单集成3D并行的低秩模型实现1.87-2.27倍加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型预训练的规模受到计算和通信成本增加的限制。低秩瓶颈架构虽然算法效率高，但在标准张量并行下扩展性差，简单的3D并行会导致过度通信和GPU利用率低下。

Method: 提出BOOST训练框架，包含瓶颈感知张量并行、在线RMSNorm、线性层分组和低秩激活检查点等优化技术，专门针对大规模低秩瓶颈架构设计。

Result: 在不同低秩瓶颈架构上的评估显示，BOOST相比全秩模型基线实现1.46-1.91倍加速，相比简单集成3D并行的低秩模型实现1.87-2.27倍加速，同时提高了GPU利用率并减少了通信开销。

Conclusion: BOOST框架通过专门针对低秩瓶颈架构的优化设计，有效解决了现有并行方法在该类架构上的扩展性问题，为大规模Transformer预训练提供了高效解决方案。

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [9] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: 使用PyTorch实现软决策树(SDT)和短期记忆软决策树(SM-SDT)，在模拟和临床数据集上测试，可视化SDT展示可解释性潜力，SDT、SM-SDT和XGBoost的AUC表现相似且优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 开发具有可解释性的机器学习模型，特别是软决策树，以在保持良好性能的同时提供模型决策过程的透明度，便于在临床等需要解释性的领域应用。

Method: 使用PyTorch实现软决策树(SDT)和短期记忆软决策树(SM-SDT)，在模拟和临床数据集上进行广泛测试，可视化SDT结构以展示其可解释性，并与XGBoost、随机森林、逻辑回归和传统决策树等基准方法进行比较。

Result: SDT、SM-SDT和XGBoost表现出相似的AUC值，且均优于随机森林、逻辑回归和传统决策树。在临床数据集上，除传统决策树外，所有测试的分类方法都产生了可比较的结果。

Conclusion: 软决策树在保持与XGBoost相当性能的同时，提供了更好的可解释性，适合需要透明决策过程的临床应用。所有代码和数据集已在GitHub上开源。

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [10] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

TL;DR: Spectral Sentinel 是一个针对去中心化联邦学习中拜占庭攻击的检测与聚合框架，利用随机矩阵理论中的协方差特征谱异常检测攻击，支持高达15亿参数的模型，在区块链上实现并证明达到极小极大最优。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习虽然避免了中心化信任问题，但在非独立同分布数据下仍然容易受到拜占庭客户端的梯度投毒攻击。现有防御方法面临可扩展性困境：基于距离的过滤可能拒绝合法的非独立同分布更新，几何中值方法计算成本过高，而许多认证防御仅在小模型上评估。

Method: 提出 Spectral Sentinel 框架，利用随机矩阵理论的特征：诚实的非独立同分布梯度产生的协方差特征谱主体遵循 Marchenko-Pastur 定律，而拜占庭扰动会导致可检测的尾部异常。算法结合 Frequent Directions 草图技术和数据相关的 MP 跟踪，使用 O(k²) 内存（k≪d）检测高达15亿参数的模型。

Result: 在 (σ,f) 威胁模型下，证明了 (ε,δ)-拜占庭弹性，收敛率为 O(σf/√T + f²/T)，并提供了匹配的信息论下界 Ω(σf/√T)，确立了极小极大最优性。在 Polygon 网络上实现区块链集成系统，在144个攻击-聚合器配置中验证，平均准确率达到78.4%，而基线方法仅为48-63%。

Conclusion: Spectral Sentinel 解决了去中心化联邦学习中拜占庭攻击检测的可扩展性问题，通过随机矩阵理论特征谱分析实现了高效的大模型防御，理论证明达到最优收敛率，实验验证了其在实际系统中的有效性。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [11] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

TL;DR: 提出一种结合物理模型与数据驱动学习的混合方法，通过PBDW框架集成降阶模型与测量数据，并引入正交约束的DeepONet学习模型偏差，同时优化传感器布局，在Helmholtz方程问题上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂不确定物理系统的状态估计需要协调存在固有缺陷的理论模型与噪声实验数据。传统方法难以同时处理预期和非预期的不确定性，需要一种能有效结合物理知识与数据学习的方法来提升状态估计和预测能力。

Method: 基于PBDW框架，集成降阶模型表示与测量数据；引入正交约束的DeepONet学习模型偏差，确保其仅针对未知模型偏差分量；研究最优传感器布局策略以最大化测量信息获取。

Result: 在Helmholtz方程问题上验证了方法的有效性，能够处理边界条件和源项等多种建模误差来源，提高了状态估计的准确性。

Conclusion: 提出的混合方法成功结合了物理建模与数据驱动学习，通过正交约束的DeepONet有效学习模型偏差，同时保持物理模型的解释性和保真度，为复杂不确定系统的状态估计提供了有效解决方案。

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [12] [SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization](https://arxiv.org/abs/2512.12737)
*Li Xia*

Main category: cs.LG

TL;DR: SPARK：一种用于去中心化联邦学习的高效通信方法，通过随机投影压缩Jacobian矩阵、阶段式退火蒸馏和Nesterov动量加速，在减少98.7%通信量的同时保持收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习面临统计异构性和通信开销的挑战。现有NTK方法虽然收敛快，但传输完整Jacobian矩阵在带宽受限的边缘网络中不切实际。

Method: 1) 随机投影压缩Jacobian矩阵，保留收敛所需的光谱特性；2) 阶段式退火蒸馏，从纯NTK演化过渡到邻居正则化学习，抵消压缩噪声；3) Nesterov动量加速，通过蒸馏平滑实现稳定累积。

Result: SPARK相比NTK-DFL减少98.7%通信量，同时保持收敛速度并实现更高精度。结合动量后，达到目标性能的速度快3倍，在通信效率方面达到最先进水平。

Conclusion: SPARK为带宽受限的边缘环境提供了实用的去中心化学习解决方案，通过协同集成压缩、蒸馏和动量技术，在通信效率和性能之间实现了最佳平衡。

Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.

</details>


### [13] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发机器学习系统从文本描述预测食品指南评分2.0，实现R²=0.81的预测精度


<details>
  <summary>Details</summary>
Motivation: 现有营养评估系统需要详细数据，但日常食品文本描述往往缺乏这些信息，限制了大规模营养评估的应用

Method: 使用多头神经网络处理混合特征向量，结合语义文本嵌入、词汇模式、领域启发式规则和USDA FNDDS数据，预测FCS算法所需的营养成分

Result: 系统预测能力强，单个营养成分中位数R²达0.81，预测FCS与公布值相关性高（Pearson's r=0.77），平均绝对差异14.0分

Conclusion: 该方法能将语言转化为可操作的营养信息，支持可扩展的饮食评估，适用于消费者应用和研究，但对模糊或加工食品误差较大

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [14] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

TL;DR: DPO不是重写模型内部信念，而是通过低秩引导机制，沿着少量偏好方向微调激活，实现行为对齐而非信念改变。


<details>
  <summary>Details</summary>
Motivation: 虽然DPO已成为对齐大语言模型的标准方法，但其在神经网络内部引发的实际变化机制尚不清楚。本文旨在探究DPO如何影响模型内部表示，而非仅仅观察其行为变化。

Method: 通过理论推导证明DPO梯度仅依赖于偏好与非偏好补全的logit嵌入差异，表明其仅引起最终隐藏表示的一阶偏移。从DPO调优模型中提取经验引导向量，通过加减该向量验证其对行为的影响。进行谱分析揭示上层网络的秩一主导和熵崩溃现象。

Result: DPO梯度确实仅依赖于logit嵌入差异，表明浅层表示偏移。经验引导向量实验显示：添加该向量可重现大部分对齐行为，减去则几乎恢复原始模型。谱分析发现上层网络存在秩一主导和熵崩溃，表明对齐通过狭窄子空间实现。

Conclusion: DPO不重写模型内部信念，而是通过低秩引导机制在狭窄子空间内微调激活，实现行为对齐的"行为幻觉"。模型学会如何表现对齐，而非改变其内在信念。

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [15] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

TL;DR: Trailblazer是一个基于大语言模型的通用网络策略框架，首次实现了能够跨任务和环境泛化的通用网络策略，在抖音真实环境中验证了优于传统专家策略的性能。


<details>
  <summary>Details</summary>
Motivation: 传统网络优化依赖基于手工规则或深度学习模型的专家策略，泛化能力差，无法适应多样化的任务和环境。而大语言模型具有丰富的网络原理知识和对未见场景的泛化能力，为构建通用网络策略提供了变革性基础。

Method: Trailblazer框架包含：1）网络对齐方案，将LLM与特定网络任务对齐；2）自适应策略协作机制，将简单控制案例从LLM卸载到轻量级策略以提高计算效率。

Result: 通过大规模模拟和抖音真实在线评估，Trailblazer在跨任务和跨环境泛化方面表现优于传统专家策略，验证了LLM作为通用网络策略基础的有效性。

Conclusion: LLM是通用网络策略的坚实基础，Trailblazer是迈向通用驱动范式的第一步，能够以最小的策略设计努力实现强大的泛化能力。

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


### [16] [Amortized Causal Discovery with Prior-Fitted Networks](https://arxiv.org/abs/2512.11840)
*Mateusz Sypniewski,Mateusz Olko,Mateusz Gajewski,Piotr Miłoś*

Main category: cs.LG

TL;DR: 提出基于Prior-Fitted Networks的摊销因果发现方法，解决似然估计误差问题，在结构恢复上优于基准方法


<details>
  <summary>Details</summary>
Motivation: 传统可微分惩罚似然方法存在似然估计误差问题，即使在大样本量下也无法发现正确结构，需要更可靠的评分方法

Method: 使用Prior-Fitted Networks（PFNs）摊销数据依赖的似然估计，为结构学习提供更可靠的评分

Result: 在合成、模拟和真实数据集上相比标准基准方法在结构恢复方面取得显著提升，PFNs比传统神经网络方法提供更准确的似然估计

Conclusion: PFNs方法通过改进似然估计精度，有效解决了现有因果发现方法的局限性，为结构学习提供了更可靠的评分框架

Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.

</details>


### [17] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

TL;DR: 提出轻量级元持续预测框架，用于蜂窝网络中的短期移动性预测，通过元初始化、残差检测和在线更新机制，在非平稳移动场景下实现快速适应和恢复，显著提升切换预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的移动性高度非平稳，包括急转弯、速度突变和不可预测的用户行为，导致传统预测器漂移，造成切换时机错误或失败，需要能够快速适应变化的预测方法。

Method: 基于GRU的预测器，结合Reptile元初始化实现快速少样本适应，使用EWMA残差检测器在发生漂移时触发紧凑的在线更新，形成轻量级元持续预测框架。

Result: 在GeoLife和DeepMIMO数据集上，零样本设置下达到4.46m ADE和7.79m FDE，10样本少样本适应后ADE提升至3.71m，从突发漂移中恢复的速度比离线GRU快2-3倍。应用于切换预测时，F1达到0.83，AUROC达到0.90，显著减少切换失败和乒乓事件。

Conclusion: 该轻量级框架（128k参数）能够有效处理非平稳移动性预测问题，快速适应变化并恢复，适用于5G/6G系统的边缘部署，显著提升切换预测性能。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [18] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 提出DTSFormer模型，通过可变形多尺度划分和时频联合滤波，改进机场客流预测，在真实数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机场客流预测对运营效率至关重要。现有基于固定大小patch的Transformer方法难以建模机场客流复杂异质模式，需要更灵活的建模方法。

Method: 提出DTSFormer模型：1) 多尺度可变形划分模块，通过窗口函数掩码动态划分多尺度时间patch；2) 时频联合滤波模块，设计频域注意力机制捕获高低频分量；3) 时域多频特征融合，联合建模短期波动和长期趋势。

Result: 在北京首都国际机场2023年1月至2024年3月真实客流数据上实验，在不同预测时间范围均优于最先进预测模型。可变形划分模块能对齐主导周期和异质趋势，更好捕捉突发高频波动。

Conclusion: DTSFormer通过可变形多尺度划分和时频联合滤波，有效建模机场客流复杂模式，提升预测准确性，为机场运营提供更可靠支持。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [19] [Exploring Topological Bias in Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2512.11846)
*Yihan Zhang*

Main category: cs.LG

TL;DR: 该论文研究了异质图神经网络中的拓扑偏见问题，提出了一种基于元权重和PageRank的投影方法来检测偏见，并通过对比学习结构进行去偏，在三个公开数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质图神经网络的偏见问题，而对异质图神经网络中的拓扑偏见关注不足。由于半监督学习下标签稀疏，GNNs在特定节点上表现出有偏性能，这种偏见与拓扑结构相关，被认为是GNNs性能的瓶颈。

Method: 1. 应用元权重到异质图的邻接矩阵以区分不同的元关系；2. 基于修改后的邻接矩阵，利用PageRank和节点标签信息构建投影，将节点映射到与模型性能强相关的值；3. 提出基于节点映射值差异的去偏结构，并与原始图结构一起用于对比学习。

Result: 在三个公开数据集上的实验验证了所提方法的有效性：1. 构建的投影在有和无类内连接的数据集上都显示出与模型性能的强相关性，证明了HGNNs中拓扑偏见的普遍存在；2. 提出的去偏方法能够有效提高HGNNs的性能并减少偏见。

Conclusion: 该工作首次系统研究了异质图神经网络中的拓扑偏见问题，提出了有效的检测和去偏方法，为提升HGNNs的公平性和性能提供了新思路。

Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.

</details>


### [20] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: TRM在ARC-AGI-1上的性能主要来自测试时增强、多数投票集成、任务标识符依赖和浅层递归，而非深度推理能力


<details>
  <summary>Details</summary>
Motivation: 分析TRM在ARC-AGI-1任务中的实际性能来源，澄清其表现是来自架构优势还是测试时计算/任务先验

Method: 通过四个行为分析和效率比较：1)测试时增强和投票集成分析 2)任务标识符消融实验 3)递归轨迹分析 4)训练增强对比实验 5)与Llama 3 8B QLoRA微调的效率比较

Result: 1)1000样本投票提升11%准确率 2)任务标识符替换导致零准确率 3)性能在第一递归步达到，递归深度浅 4)强增强扩展候选解分布 5)TRM比Llama 3 8B QLoRA吞吐量更高、内存使用更低

Conclusion: TRM的ARC-AGI-1性能主要源于效率优势、任务特定条件设置和激进的测试时计算，而非深度内部推理能力

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [21] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 通过缓存和复用相似提示的注意力键值状态来加速小语言模型推理，称为"token recycling"方法


<details>
  <summary>Details</summary>
Motivation: 探索是否可以利用已计算的小型LLM注意力键值状态来加速相似新提示的推理，从而扩展上下文内存空间

Method: 使用DialoGPT-medium作为测试平台，构建过去激活的缓存，通过句子嵌入检索条目，当缓存提示是新输入的精确前缀时复用缓存的键值状态

Result: 当存在前缀重叠时观察到一致的加速效果，输出语义没有明显退化；没有重叠时行为与基线一致

Conclusion: token recycling方法可以有效加速相似提示的推理，无需修改模型，具有实际应用价值

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [22] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 该研究利用Temporal Fusion Transformer模型进行智能温室执行器控制，并采用多种可解释性方法（TFT内置解释、LIME、SHAP）提高决策透明度，在类别不平衡数据集上达到95%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 智能温室中现有的时间序列预测模型通常作为黑箱运行，缺乏可解释的决策机制，这在需要信任、透明度和监管合规的智慧农业实践中是一个关键限制。

Method: 使用Temporal Fusion Transformer模型自动化温室执行器设置，并采用模型内在解释、局部可解释模型无关解释和SHAP值分析等局部和全局解释技术来增强模型可解释性。

Result: 在类别不平衡的自动化温室执行器控制数据集上，训练的TFT模型达到了95%的测试准确率。可解释性分析揭示了温度、湿度、CO2水平、光照和外部气候等不同传感器对执行器控制决策的贡献程度。

Conclusion: 该研究证明了TFT模型结合可解释性方法能够有效实现智能温室的高精度自动化控制，同时确保决策过程的透明度，有助于提高作物产量和资源效率。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [23] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 开发基于单手腕IMU的实时反馈系统，用于检测阻力训练中的接近力竭状态（RiR≤2），通过两阶段模型实现实时分割和分类，验证了边缘部署可行性。


<details>
  <summary>Details</summary>
Motivation: 阻力训练中主观评估剩余重复次数（RiR）不可靠，导致训练刺激不足或过度疲劳，需要客观的实时反馈系统来优化增肌训练。

Method: 提出两阶段边缘部署管道：1）基于ResNet的模型实时分割IMU数据中的重复动作；2）结合分割特征、卷积特征和LSTM历史上下文，分类检测接近力竭状态（RiR≤2）。

Result: 在13名参与者631次重复的数据集上，分割模型F1分数0.83，接近力竭分类器F1分数0.82（1.6Hz推理速率）。Raspberry Pi 5平均延迟112ms，iPhone 16延迟23.5ms，验证边缘计算可行性。

Conclusion: 使用最小硬件实现客观实时训练强度反馈的实用方法，为可访问的AI驱动增肌教练工具铺平道路，帮助用户有效管理训练强度和疲劳。

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [24] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 本文首次从理论上比较了精确对称性与近似对称性的代价，提出了"平均复杂度"框架，证明了指数级分离：精确对称需要线性平均复杂度，而近似对称只需对数复杂度。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，强制精确对称性作为强大的归纳偏置在科学应用中带来显著收益，但最近研究表明近似对称性可能提供更大的灵活性和鲁棒性。然而，缺乏理论理解，特别是精确对称与近似对称的直接比较在文献中缺失。

Method: 引入"平均复杂度"框架来量化通过平均方法强制对称性的代价。在标准条件下分析精确对称与近似对称的平均复杂度要求。

Result: 主要结果是指数级分离：实现精确对称需要线性平均复杂度，而实现近似对称只需对数平均复杂度。这是首次从理论上分离这两种情况，为实践中近似对称性的优越性提供了形式化证明。

Conclusion: 本文首次从理论上比较了精确对称与近似对称的代价，证明了近似对称在复杂度上的显著优势，为实践中选择近似对称提供了理论依据。所提出的工具和技术对机器学习中对称性的更广泛研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [25] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

TL;DR: GCoDE：首个面向图神经网络设备-边缘协同推理的自动框架，通过架构-映射协同设计实现高效部署，相比现有方法最高可加速44.9倍并降低98.2%能耗。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在边缘设备上的推理面临计算成本高和硬件资源有限的挑战，传统模型划分方法对GNN无效，而设备-边缘协同推理这一新兴范式尚未得到充分研究。

Method: 提出GCoDE框架，将设备通信过程抽象为显式操作，在统一设计空间中融合架构和映射方案进行联合优化；引入系统性能感知和能耗预测方法；采用基于约束的随机搜索策略寻找最优解；集成协同推理引擎。

Result: GCoDE在1.5小时内找到平衡精度和效率的最优解，相比现有方法在不同应用和系统配置下最高可实现44.9倍加速和98.2%能耗降低。

Conclusion: GCoDE是首个针对GNN设备-边缘协同推理的自动框架，通过架构-映射协同设计有效解决了边缘设备上GNN推理的效率问题，为实际边缘场景应用提供了可行方案。

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [26] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: TopicProphet：通过主题建模、时间分析和分段优化，识别具有相似公众情绪趋势的历史时期，为股票预测模型选择最优训练数据的新框架。


<details>
  <summary>Details</summary>
Motivation: 传统股票预测面临两大挑战：定量数据缺乏因果逻辑，以及市场快速变化导致训练数据不足。现有研究主要关注关键词和情绪的影响，但未能解决如何选择合适历史时期进行训练的问题。

Method: 提出TopicProphet框架，通过主题建模分析公众情绪趋势，结合时间分析和断点检测，识别具有相似历史背景的时期，然后进行分段优化，确定最优训练时间窗口。

Result: TopicProphet相比现有最先进方法，在捕捉金融百分比变化预测的最优训练数据方面表现更好，提高了预测准确性。

Conclusion: 通过分析具有相似公众情绪趋势和历史背景的历史时期，TopicProphet能够为股票预测模型提供更相关的训练数据，解决了数据不足问题并提升了预测性能。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [27] [Adaptive Path Integral Diffusion: AdaPID](https://arxiv.org/abs/2512.11858)
*Michael Chertkov,Hamidreza Behjoo*

Main category: cs.LG

TL;DR: 本文提出了一种基于路径的调度选择框架，用于谐波路径积分扩散（Harmonic PID），采用分段常数参数化和分层细化，并引入调度敏感的质量采样诊断指标，在固定积分预算下提升采样质量。


<details>
  <summary>Details</summary>
Motivation: 扩散基采样器（如分数基扩散、桥扩散和路径积分扩散）在终端时间匹配目标分布，但其真正优势在于控制中间时间动态的调度选择。现有方法缺乏系统化的调度选择框架来优化采样质量。

Method: 开发了谐波PID的时间变化刚度路径调度选择框架，采用分段常数参数化和简单分层细化。引入调度敏感的质量采样诊断指标，对于高斯混合目标，保留闭式格林函数比率和数值稳定的神经网络自由预测状态映射与分数函数。

Result: 在2D实验中，QoS驱动的分段常数调度在固定积分预算下，一致改善了早期退出保真度、尾部精度、动态条件性和特异化（标签选择）时机。

Conclusion: 提出的调度选择框架通过分段常数参数化和质量采样诊断，能够有效优化谐波PID的采样性能，在多个质量指标上实现显著提升。

Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.

</details>


### [28] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: GH-PID是一种线性可解的引导随机最优传输框架，具有硬终端分布和软路径成本，通过低维引导协议保持解析结构，支持稳定采样和可微协议学习。


<details>
  <summary>Details</summary>
Motivation: 解决具有硬终端分布约束的随机最优传输问题，同时保持解析可解性，使系统能够处理应用驱动的路径成本，并支持可解释的协议学习和多专家融合。

Method: 提出引导谐波路径积分扩散框架，通过低维引导协议保持Kolmogorov方程的线性性，最优得分函数具有显式格林函数比值形式，高斯混合模型终端分布产生闭式表达式。

Result: 在三种2D导航场景中验证：手工协议展示几何和刚度对轨迹的影响；单任务协议学习优化路径成本；多专家融合通过专家乘积法则学习共识协议，生成满足终端分布且降低成本的几何感知轨迹。

Conclusion: GH-PID为经验随机最优传输提供了可解释的变分框架，能够生成满足硬终端约束的几何感知、信任感知轨迹，同时系统性地降低积分成本，支持稳定采样和可微协议学习。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [29] [An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes](https://arxiv.org/abs/2512.11860)
*Yuelian Li,Andrew Rushing Hands*

Main category: cs.LG

TL;DR: OCGNN-PINN：一种算子一致的图神经网络，用于在不规则网格上求解偏微分方程，通过节点-边消息传递和一致性损失确保梯度-散度关系，在物理驱动的演化网格上实现稳定准确的时间推进。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在规则网格上高效求解PDE，但在不规则域上不稳定。实际多物理场相互作用（如扩散、损伤、愈合）常发生在不规则网格上，需要开发能在不规则网格上稳定求解PDE的方法。

Method: 开发算子一致的图神经网络（OCGNN-PINN），通过节点-边消息传递耦合物理信息约束，使用一致性损失通过图关联矩阵强制执行梯度-散度关系，确保离散节点和边动力学在时间推进中保持结构耦合。

Result: 在物理驱动的演化网格和真实扫描表面上评估扩散过程，结果显示相比图卷积和多层感知器基线，模型具有更好的时间稳定性和预测精度，接近Crank-Nicolson求解器在非结构化域上的性能。

Conclusion: OCGNN-PINN方法能够在不规则网格上稳定准确地求解PDE，通过算子一致性约束保持离散动力学的结构完整性，为复杂物理系统在不规则域上的模拟提供了有效工具。

Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.

</details>


### [30] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

TL;DR: 本文提出了一种用于低空智能网络的分层学习框架，通过Vickrey-Clarke-Groves拍卖机制和扩散异构智能体近端策略优化算法，联合优化无人机轨迹规划和任务卸载决策，以提高能量效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 低空智能网络（LAINs）在动态和基础设施有限的环境中为边缘智能提供低延迟和能量高效的服务，但面临能量受限的无人机、随机任务到达和异构计算资源等关键挑战。

Method: 1. 提出集成空地协作网络，构建时间依赖的整数非线性规划问题；2. 设计分层学习框架：大时间尺度采用VCG拍卖机制进行能量感知和激励兼容的轨迹分配；小时间尺度提出扩散异构智能体近端策略优化算法，将潜在扩散模型嵌入行动者网络。

Result: 大量仿真实验表明，该框架在能量效率、任务成功率和收敛性能方面优于基线方法。

Conclusion: 所提出的分层学习框架有效解决了低空智能网络中无人机轨迹规划和任务卸载的联合优化问题，提高了系统性能，为动态环境中的边缘智能应用提供了可行解决方案。

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [31] [Phase transitions reveal hierarchical structure in deep neural networks](https://arxiv.org/abs/2512.11866)
*Ibrahim Talha Ersoy,Andrés Fernando Cardozo Licha,Karoline Wiesner*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，将深度神经网络训练中的相变、鞍点和模式连通性现象解释为损失和误差景观的几何特性，并开发了基于L2正则化的算法来验证这些现象。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中存在许多难以理解的现象，包括类似统计物理的相变、鞍点的普遍存在以及模型合并中的模式连通性。这些看似不同的现象缺乏统一的理论解释，作者旨在通过损失和误差景观的几何特性来统一解释这些现象。

Method: 作者首先从理论上证明DNN学习中的相变由损失景观中的鞍点控制。基于这一洞察，开发了一个简单、快速、易于实现的算法，使用L2正则化作为工具来探测误差景观的几何结构。在MNIST数据集上应用该算法验证模式连通性，并数值展示鞍点如何诱导不同数字类别编码模型之间的转换。

Result: 算法成功在MNIST数据集上找到了连接全局最小值的路径，证实了模式连通性。数值实验显示鞍点诱导了编码不同数字类别的模型之间的转换。研究揭示了DNN训练关键现象的几何起源，并发现了类似于统计物理中相结构的精度盆地层次结构。

Conclusion: 该研究建立了深度神经网络训练关键现象的几何起源统一框架，将相变、鞍点和模式连通性统一解释为损失和误差景观的几何特性，揭示了类似于统计物理相结构的精度盆地层次，为理解DNN训练提供了新的理论视角。

Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.

</details>


### [32] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 重复使用合成数据训练会导致模型性能退化，合成数据引入显著偏差和方差，削弱最大似然估计可靠性，现有生成经验回放方法无法维持潜在空间对齐。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据训练成为常见实践，重复使用合成数据进行训练引发了分布漂移和性能退化的担忧，需要研究这种自举过程的后果。

Method: 从持续学习角度分析，连接生成经验回放方法，进行统计分析显示合成数据对训练目标的影响，提供实证证据展示流行生成模型在重复合成数据训练下的崩溃。

Result: 合成数据引入显著偏差和方差，削弱最大似然估计可靠性；流行生成模型在重复合成数据训练下崩溃；最先进的生成经验回放方法无法维持潜在空间对齐。

Conclusion: 研究结果对在持续学习中使用合成数据提出了严重关切，合成数据训练存在根本性限制。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [33] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 提出基于ICE曲线的全局敏感性度量方法，解决PDP在强交互作用下的局限性，通过数学证明和三个案例验证其优于传统PDP方法


<details>
  <summary>Details</summary>
Motivation: 在航空航天等工程应用中，理解输入变量对数据驱动模型的影响至关重要。PDP作为广泛使用的黑箱模型解释方法，在存在强交互作用时，其全局敏感性度量会因平均化而掩盖交互效应，导致误导性结果。

Method: 提出基于个体条件期望(ICE)曲线的全局敏感性度量方法：计算ICE曲线的期望特征重要性及其标准差，更有效地捕捉交互作用影响。提供数学证明表明PDP敏感性是ICE度量的下界。引入ICE相关系数量化交互作用如何修改输入-输出关系。

Result: 在三个案例（5变量解析函数、5变量风力涡轮机疲劳问题、9变量翼型空气动力学）中，将ICE敏感性度与PDP、SHAP和Sobol指数进行对比。结果显示ICE特征重要性比传统PDP方法提供更丰富的洞察，而PDP、ICE和SHAP的可视化解释相互补充，提供多角度视角。

Conclusion: ICE基特征重要性方法在存在交互作用时优于传统PDP方法，为工程应用中的模型解释提供了更准确和丰富的工具。多种解释方法的结合使用能提供更全面的模型理解。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [34] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 模型可以通过微调学习主动逃避激活监控器，即使面对未见过的安全监控器也能实现零样本逃避，这揭示了激活监控在对抗性威胁模型下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 测试激活监控在最坏情况下的鲁棒性，特别是在模型可能学习主动隐藏内部状态的对抗性威胁模型下。研究模型能否学会逃避先前未见过的激活监控器。

Method: 通过微调创建"神经变色龙"模型，使其在特定触发条件下（如"You are being probed for {concept}"）逃避对良性概念的激活监控。然后测试这种机制是否能零样本推广到安全相关概念（如'deception'）。

Result: 模型成功实现了对未见过的安全监控器的零样本逃避，这种逃避具有高度选择性，只针对触发词中提到的特定概念，对模型在标准基准测试中的能力影响较小。机制分析显示这是通过将激活移动到低维子空间实现的。

Conclusion: 激活监控在对抗性威胁模型下存在脆弱性，模型可以学习主动逃避监控。虽然集成监控和非线性分类器等防御措施更具弹性，但模型仍保留非平凡的逃避能力。这项工作为此类失效模式提供了概念验证，并为评估监控技术在最坏情况下的鲁棒性提供了工具。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [35] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 论文提出一个框架，通过从用户提示中提取上下文信息来指导LLM响应生成，以解决模糊提示导致的误判问题，提高安全性同时减少不必要的拒绝。


<details>
  <summary>Details</summary>
Motivation: 用户对大型语言模型的提示往往模糊或不完整，而用户的意图、先验知识和风险因素等上下文线索会严重影响什么才是合适的响应。传统框架中LLM直接生成响应而不考虑这些上下文因素，可能导致误解意图产生不安全输出，或过度谨慎导致良性请求被不必要拒绝。

Method: 提出一个框架，从用户提示本身提取和利用上下文信息。具体采用基于强化学习的上下文生成器，以类似自编码器的方式设计，训练其从提示中推断基于上下文的信号，并用这些信号指导响应生成。

Result: 在SafetyInstruct数据集上，该方法平均减少有害响应5.6%（跨多个基础模型）；在XSTest和WildJailbreak上，良性提示的攻击成功率和合规性的调和平均提高了6.2%。

Conclusion: 上下文提取对于更安全、更可靠的LLM推理是有效的，能够更好地处理模糊请求，在安全任务中特别重要，既能防止恶意请求绕过安全措施，又能避免良性但令人困惑的请求被不必要拒绝。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [36] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: EnviroLLM：开源工具包，用于追踪、基准测试和优化本地LLM的性能与能耗，提供实时监控、多平台支持、数据可视化及个性化推荐


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地部署在本地设备上，用户缺乏测量其资源使用、环境影响和效率指标的工具，需要系统化的解决方案来评估本地LLM的能耗和性能

Method: 开发EnviroLLM工具包，包含实时进程监控、多平台基准测试（Ollama、LM Studio、vLLM、OpenAI兼容API）、持久化存储与可视化、个性化模型推荐，以及结合LLM-as-judge评估的质量-效率权衡分析

Result: 创建了完整的开源工具包，能够帮助用户测量本地LLM的能耗和性能，提供可视化分析，支持用户在不同模型和优化策略之间做出质量-效率权衡决策

Conclusion: EnviroLLM填补了本地LLM部署中资源监控和效率评估的工具空白，为用户提供了全面的性能、能耗和质量评估框架，有助于推动更环保、高效的本地LLM应用

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [37] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: DFedReweighting：一个去中心化联邦学习的统一聚合框架，通过目标导向的权重调整来提升公平性和鲁棒性，同时保证线性收敛。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）避免了中心服务器的单点故障风险，但仍面临公平性、鲁棒性等挑战。现有方法难以同时解决这些问题，需要一种统一框架来灵活实现多种学习目标。

Method: 提出DFedReweighting框架，在每个学习轮次的最后一步进行目标导向的权重调整聚合。首先基于辅助数据集计算初步权重，然后通过定制化权重调整策略进行优化，形成最终聚合权重。

Result: 理论分析表明，适当的目标性能指标和权重调整策略组合能保证线性收敛。实验结果显示，该框架在多种场景下显著提升了公平性和对拜占庭攻击的鲁棒性。

Conclusion: DFedReweighting是一个灵活的统一框架，通过选择合适的目标性能指标和定制化权重调整策略，能够实现广泛的期望学习目标，包括公平性、鲁棒性等。

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [38] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出Eik-QRL和Eik-HiQRL方法，通过Eikonal PDE约束改进目标条件强化学习，实现轨迹无关的准度量学习，并在分层结构中解决复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）通过目标到达任务简化奖励设计，但现有准度量强化学习（QRL）方法依赖离散轨迹约束，限制了泛化能力和计算效率。

Method: 1. Eik-QRL：基于Eikonal偏微分方程的连续时间重构，实现轨迹无关的准度量学习，仅需采样状态和目标；2. Eik-HiQRL：将Eik-QRL集成到分层分解中，解决复杂动态下的局限性。

Result: Eik-HiQRL在离线目标条件导航任务中达到最先进性能，在操作任务中相比QRL获得一致提升，性能与时间差分方法相当。

Conclusion: 基于Eikonal PDE的准度量强化学习方法提供了轨迹无关、泛化能力强的框架，分层扩展进一步增强了处理复杂动态的能力，为目标条件强化学习提供了新方向。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [39] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 研究发现当前大语言模型的安全评估存在缺陷：单次测试无法捕捉模型安全拒绝决策的不稳定性，18-28%的有害提示在不同采样配置下会出现"决策翻转"现象


<details>
  <summary>Details</summary>
Motivation: 挑战当前大语言模型安全评估中隐含的假设——模型响应是确定性的且能代表模型的安全对齐。研究者质疑单次测试的可靠性，认为模型的安全拒绝决策可能在不同随机种子和温度设置下不稳定

Method: 测试4个指令调优模型（Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B），使用876个有害提示，在20种不同采样配置（4个温度×5个随机种子）下评估。提出安全稳定性指数（SSI），并使用Claude 3.5 Haiku作为统一外部评判者验证结果

Result: 18-28%的提示在不同配置下出现决策翻转；温度升高显著降低决策稳定性（SSI从0.0温度时的0.951降至1.0温度时的0.896）；单次评估与多样本真实结果的一致性仅为92.4%

Conclusion: 单次安全评估不足以进行可靠的安全评估，建议每个提示至少使用3个样本进行测试，以确保评估的可靠性

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [40] [Physics-informed neural networks to solve inverse problems in unbounded domains](https://arxiv.org/abs/2512.12074)
*Gregorio Pérez-Bernal,Oscar Rincón-Cardeño,Silvana Montoya-Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 提出一种在无限/半无限域求解反问题的新方法，结合负指数/正态分布采样策略和双网络架构，无需显式边界条件，比较PINNs和PIKANs性能


<details>
  <summary>Details</summary>
Motivation: 解决无限和半无限域中的反问题，传统方法需要显式边界条件，而实际应用中边界条件可能难以确定或不存在

Method: 1. 使用负指数分布和正态分布的新型采样策略；2. 双网络架构同时学习方程解和参数；3. 无需显式边界条件，仅要求解在离开感兴趣域时趋于稳定；4. 同时实现PINNs和PIKANs版本并进行比较

Result: PINNs在此设置下表现更优：计算速度快1000倍，相对误差更低（同一数量级但更小），在噪声环境下响应更好

Conclusion: 提出的方法能有效解决无限/半无限域反问题，无需显式边界条件；PINNs在此类问题中比PIKANs更准确且计算效率更高

Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.

</details>


### [41] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 提出结合形状基元表示和传统特征工程的Transformer学习框架，以及可视化分析系统SigTime，用于时间序列模式发现和解释


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法面临计算复杂度高、可解释性有限、难以捕捉有意义时间结构等挑战，特别是在生物医学研究中，发现生理信号中的有意义模式对诊断、风险评估和患者预后至关重要

Method: 引入新颖学习框架，联合训练两个Transformer模型：一个使用形状基元表示捕捉局部时间结构，另一个使用传统特征工程编码统计特性；同时开发可视化分析系统SigTime，提供协调视图从多角度探索时间序列签名

Result: 在8个公开数据集和1个专有临床数据集上定量评估学习框架；通过两个使用场景（公共ECG数据和早产分析）与领域专家一起展示系统有效性

Conclusion: 提出的框架和系统能够有效发现和解释时间序列模式，特别适用于生物医学应用，有助于科学发现和决策制定

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [42] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: Cloak是一个基于潜在扩散模型的新型数据混淆框架，通过对比学习提取解耦表示，指导扩散过程在保留有用信息的同时隐藏隐私信息，在资源受限的移动物联网设备上实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法存在以下问题：需要修改下游任务、难以达到满意的隐私-效用权衡、计算密集不适用于资源受限的移动物联网设备。需要一种更实用、高效的数据混淆方案。

Method: 提出Cloak框架：1）使用对比学习提取解耦表示；2）利用潜在扩散模型进行数据混淆；3）通过解耦表示指导扩散过程，在保留有用信息的同时隐藏隐私信息；4）支持用户根据不同的隐私需求调整权衡，无需大量重新训练。

Result: 在四个公共时间序列数据集（涵盖多种传感模态）和一个人脸图像数据集上的实验表明：Cloak在隐私-效用权衡方面持续优于现有最先进的混淆技术，并且适合在资源受限的环境中部署。

Conclusion: Cloak通过结合对比学习和潜在扩散模型，提供了一种高效实用的数据混淆解决方案，能够在资源受限的移动物联网设备上实现更好的隐私保护，同时保持数据效用，支持用户根据个性化需求调整隐私-效用权衡。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [43] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

TL;DR: GraphPerf-RT：首个统一任务DAG拓扑、代码语义和运行时上下文的异构图表示，用于异构嵌入式SoC上OpenMP工作负载的性能预测，支持风险感知调度。


<details>
  <summary>Details</summary>
Motivation: 异构嵌入式SoC上OpenMP工作负载的性能预测面临挑战，传统启发式方法在非规则工作负载下表现不佳，表格回归器丢弃结构信息，无模型RL可能过热资源受限设备。

Method: 提出GraphPerf-RT，使用异构图表示统一任务DAG拓扑、CFG衍生代码语义和运行时上下文，通过多任务证据头预测性能指标并校准不确定性，支持风险感知调度。

Result: 在三个嵌入式ARM平台上验证，R^2 > 0.95，不确定性校准良好（ECE < 0.05）。与RL方法集成后，MAMBRL-D3QN相比无模型基线减少66%执行时间和82%能耗。

Conclusion: 准确且不确定性感知的代理模型能够在热约束嵌入式系统上实现有效的基于模型的规划，GraphPerf-RT为异构嵌入式SoC的性能预测和调度提供了有效解决方案。

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [44] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 提出Predictor-Corrector机制，用神经控制微分方程作为Corrector来修正任何时间序列预测模型的误差，提升多步预测性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（连续或离散时间）在进行多步预测时，无论是直接预测整个时间范围还是迭代反馈预测，都存在误差累积问题，需要一种机制来修正这些预测误差

Method: 提出Predictor-Corrector框架：Predictor可以是任何学习到的时间序列模型，Corrector是神经控制微分方程，用于预测预测误差。通过将Corrector预测的误差加到Predictor的预测上来改进性能。Corrector支持不规则采样时间序列和连续/离散时间Predictor，并引入两种正则化策略来提升外推性能和加速训练

Result: 在合成数据、物理仿真和真实世界预测数据集上，使用多种Predictor（如神经常微分方程、Contiformer、DLinear）进行实验，结果显示Predictor-Corrector机制相比单独使用Predictor能持续提升性能

Conclusion: 提出的Predictor-Corrector机制通过神经控制微分方程作为Corrector有效修正了时间序列预测模型的误差，显著提升了多步预测性能，适用于各种类型的时间序列模型和采样模式

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [45] [MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models](https://arxiv.org/abs/2512.12121)
*Ahmad Chamma,Omar El Herraoui,Guokan Shang*

Main category: cs.LG

TL;DR: MixtureKit是一个模块化开源框架，用于从任意预训练或微调模型构建、训练和分析混合专家模型，支持三种方法并提供可视化界面。


<details>
  <summary>Details</summary>
Motivation: 为混合专家模型的研究和开发提供一个实用的基础框架，支持多种MoE方法，并方便分析和可视化路由决策。

Method: 提供三种互补方法：传统MoE（每个transformer块使用单个路由器）、BTX（每个子层有独立路由器实现细粒度路由）、BTS（保持专家完整，引入可训练缝合层控制信息交换）。框架自动修改模型配置、修补解码器和因果LM类，并保存统一检查点。

Result: 在多语言代码切换数据（如阿拉伯语-拉丁语）实验中，使用MixtureKit训练的BTX模型在多个基准测试中优于基线密集模型。

Conclusion: MixtureKit作为一个实用的开源框架，为跨不同领域的MoE系统研究和开发提供了基础。

Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.

</details>


### [46] [High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees](https://arxiv.org/abs/2512.12122)
*Elynn Chen,Yuefeng Han,Jiayu Li*

Main category: cs.LG

TL;DR: 提出基于CP低秩结构的张量判别分析(CP-TDA)，用于处理高维张量分类问题，并开发半参数模型处理非正态张量数据，在数值实验和实际图分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高维张量预测器在现代应用中日益普遍（特别是来自神经网络的表示），现有张量分类方法依赖稀疏性或Tucker结构且缺乏理论保证。实证证据表明判别信号集中在少数多线性分量上。

Method: 1. 在张量高斯混合模型下，提出高维CP低秩张量判别分析(CP-TDA)，采用随机复合PCA初始化处理依赖性和各向异性噪声；2. 针对非正态张量数据，开发首个半参数张量判别模型，通过深度生成模型将学习到的张量表示映射到适合CP-TDA的潜在空间。

Result: 建立了全局收敛性和极小极大最优误分类率。误分类风险分解为表示误差、近似误差和估计误差。在数值研究和图分类实际数据分析中，相比现有张量分类器和最先进的图神经网络有显著提升，特别是在高维小样本情况下。

Conclusion: CP低秩结构为张量判别分析提供了新的建模视角，提出的CP-TDA方法在理论和实际应用中均表现优异，特别适合处理高维小样本张量分类问题，为张量表示学习提供了有效框架。

Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.

</details>


### [47] [On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency](https://arxiv.org/abs/2512.12132)
*Koffi O. Ayena*

Main category: cs.LG

TL;DR: SiLU激活网络在光滑函数逼近上实现指数级收敛速度，相比ReLU网络具有更优的复杂度控制


<details>
  <summary>Details</summary>
Motivation: 传统ReLU网络在函数逼近中需要较深的网络结构，本文旨在证明SiLU激活函数能够以更浅的网络结构实现更好的逼近效果，特别是在光滑函数类上

Method: 提出分层构造方法：首先高效逼近平方函数x²，比Yarotsky的ReLU构造更紧凑；然后通过函数组合扩展到Sobolev类函数，建立深度SiLU网络的逼近界

Result: SiLU网络能以深度O(1)、尺寸O(ε^{-d/n})逼近Sobolev类函数，逼近误差以O(ω^{-2k})衰减，相比ReLU构造有明显改进

Conclusion: SiLU激活网络在光滑函数逼近上具有理论优势，能以浅层网络实现指数级收敛，为神经网络架构设计提供了新的理论依据

Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(ω^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.

</details>


### [48] [BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity](https://arxiv.org/abs/2512.12135)
*Lucine L. Oganesian,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出新的时空Transformer模型和自监督掩码潜在重建任务，用于多区域颅内脑电记录，探索不同空间尺度对下游解码性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的神经基础模型在处理多区域颅内记录时，面临如何最佳编码空间信息以及设计自监督任务来学习脑网络模式的挑战。这些记录在从单通道到脑区域的不同空间尺度上表现出复杂的时空交互。

Method: 提出新的时空Transformer模型和自监督掩码潜在重建任务，允许在令牌编码和掩码时灵活选择空间尺度。在公开的多区域颅内脑电数据上应用该方法。

Result: 调整令牌编码和掩码重建的空间尺度显著影响下游解码性能；比现有iEEG Transformer模型常用的通道级编码更大的空间尺度能提升下游解码性能；同时实现区域级令牌编码和精确的通道级神经重建。

Conclusion: 该建模框架能够探索令牌编码和掩码的空间尺度，揭示了它们对多区域人脑活动神经基础模型自监督预训练的重要性，并增强了下游解码性能。

Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.

</details>


### [49] [HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone](https://arxiv.org/abs/2512.12183)
*Yihan Wang,Annan Yu,Lujun Zhang,Charuleka Varadharajan,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: HydroDiffusion：基于扩散模型和状态空间模型的水文概率预报框架，在CAMELS数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的径流预报方法依赖LSTM骨干网络和单步训练目标，难以捕捉长期依赖关系并产生跨预见期的连贯预报轨迹

Method: 提出HydroDiffusion框架，采用仅解码器的状态空间模型作为骨干网络，通过单次去噪处理完整的多日预报轨迹，确保时间一致性并减少自回归预测中的误差累积

Result: 在美国531个流域的CAMELS数据集上评估，HydroDiffusion在观测气象驱动下具有强实时预报精度，在整个模拟时段保持稳定性能，在业务预报中比DRUM方法具有更好的确定性和概率预报技能

Conclusion: HydroDiffusion为中长期径流预报建立了稳健的生成建模框架，为大陆尺度的概率水文预报研究提供了新的建模基准和基础

Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.

</details>


### [50] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种混合引导策略，将计算机视觉中的先进引导方法（如无分类器引导、自动引导和模型引导）整合到SE(3)-等变流匹配的分子生成框架中，通过贝叶斯优化联合优化连续和离散模态的引导尺度，在QM9和QMe14S数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 条件分子生成的关键目标包括确保化学有效性、使生成分子与目标属性对齐、促进结构多样性以及实现高效采样。计算机视觉领域最近引入了一系列新的生成模型引导策略，其中许多可以适应分子生成任务以支持这些目标。

Method: 将最先进的引导方法（无分类器引导、自动引导、模型引导）整合到基于SE(3)-等变流匹配的分子生成框架中。提出混合引导策略，分别引导连续和离散分子模态（分别操作于速度场和预测logits），同时通过贝叶斯优化联合优化它们的引导尺度。

Result: 在QM9和QMe14S数据集上实现了从头分子生成中属性对齐的最先进性能。生成的分子表现出高结构有效性。系统比较了各种引导方法的优势和局限性，为更广泛的应用提供了见解。

Conclusion: 通过整合计算机视觉中的先进引导策略并开发混合引导方法，显著提升了条件分子生成的性能，特别是在属性对齐和化学有效性方面，为分子生成领域提供了有价值的指导。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [51] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: EEG-DLite：一种数据蒸馏框架，通过选择性去除EEG数据中的噪声和冗余样本，使大规模EEG基础模型预训练更高效


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型训练资源密集，现有EEG数据存在噪声和冗余问题，需要更高效的预训练方法

Method: 使用自监督自动编码器将EEG片段编码为紧凑潜在表示，基于这些表示过滤异常值并最小化冗余，选择信息丰富的子集

Result: 仅使用EEG-DLite筛选的2,500小时数据集中5%的数据进行训练，在多个下游任务上达到甚至优于使用完整数据集的性能

Conclusion: EEG-DLite为生理基础建模提供了可扩展且实用的路径，首次系统研究了EEG基础模型预训练中的数据蒸馏问题

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [52] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: OptLCMS 是一种优化的学习型 Count-Min Sketch，通过分区和动态规划优化，在保持相同内存使用下提供理论保证、更快构建速度和更低不可容忍错误概率。


<details>
  <summary>Details</summary>
Motivation: 学习型 Count-Min Sketch (LCMS) 虽然通过机器学习模型减少了估计误差，但存在构建速度慢（需要经验参数调优）和缺乏理论保证的问题。需要一种既能保持内存效率又能提供理论保证的优化方案。

Method: 1. 将输入域分区，每个分区分配独立的 CMS 实例；2. 为固定阈值分析推导 CMS 参数；3. 通过动态规划和近似可行性检查优化阈值；4. 允许显式控制允许的错误阈值。

Result: 实验表明 OptLCMS 构建更快，实现更低的不可容忍错误概率，同时匹配 LCMS 的估计精度。

Conclusion: OptLCMS 通过分区策略和理论优化，解决了 LCMS 的构建速度和理论保证问题，提供了更灵活、更可靠的频率估计解决方案。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [53] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 提出GRC-Net模型，使用Gram矩阵将EEG信号转为3D表示，结合共注意力机制和Inception结构进行多粒度特征提取，在癫痫预测任务上取得93.66%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号分析采用1D处理，无法充分建模信号间关系；同时EEG数据中存在局部与全局信号不平衡问题，需要更有效的特征提取方法。

Method: 1) 使用Gram矩阵将1D EEG信号转换为3D表示；2) 引入多级特征提取：共注意力机制捕获全局特征，Inception结构处理局部信号；3) 构建GRC-Net模型进行多粒度特征学习。

Result: 在BONN数据集上，针对最具挑战性的五分类任务，GRC-Net达到93.66%的准确率，优于现有方法。

Conclusion: 提出的3D表示和多级特征提取方法能有效建模EEG信号关系，解决局部-全局信号不平衡问题，在癫痫预测任务上表现出优越性能。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [54] [Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model](https://arxiv.org/abs/2512.12276)
*Jeffrey van der Voort,Martin Verlaan,Hanne Kekkonen*

Main category: cs.LG

TL;DR: 该研究提出了一种多保真度集合卡尔曼滤波方法，使用机器学习代理模型作为低保真度模型，结合少量昂贵的高保真模型运行和大量便宜的ML模型运行，在相同计算预算下提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习代理模型越来越多地用于替代计算昂贵的物理模型，研究者希望探索如何将ML代理模型集成到数据同化框架中。传统方法使用低分辨率或降阶模型作为低保真度模型，而本研究旨在利用ML代理模型的速度优势，在保持计算预算不变的情况下提高预测精度。

Method: 提出多保真度集合卡尔曼滤波方法，其中低保真度模型采用机器学习代理模型而非传统低分辨率模型。该方法结合少量昂贵的高保真物理模型运行和大量便宜的ML模型运行，通过保持原始物理模型来提高精度，同时利用ML模型的快速计算优势。

Result: 在Lorenz-2005模型和准地转模型两个测试问题上验证了方法的有效性。结果表明：1）保持原始物理模型比完全用ML模型替代获得更高精度；2）在相同计算预算下，MF-EnKF达到改进的精度；3）ML代理模型相比低分辨率模型具有相似或更好的精度，且能提供更大的加速比。

Conclusion: 该方法通过将ML代理模型作为低保真度模型集成到多保真度EnKF框架中，有效增加了集合大小，改善了初始条件估计，从而提高了气象学和海洋学等领域的预测精度，在相同计算成本下实现了性能提升。

Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.

</details>


### [55] [Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation](https://arxiv.org/abs/2512.12285)
*Lujuan Dang,Zilai Wang*

Main category: cs.LG

TL;DR: 提出FDIFF-PINN模型，结合分数阶微积分与深度学习，用于锂离子电池SOC估计，提高动态条件下的预测精度和物理解释性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的神经网络模型难以充分表征电化学过程的复杂非线性和记忆依赖动态特性，限制了SOC估计在动态工况下的预测精度和物理解释性。

Method: 提出分数阶微分方程物理信息神经网络(FDIFF-PINN)，基于分数阶等效电路模型构建离散化分数阶偏微分方程，结合深度学习框架。

Result: 使用Panasonic 18650PF电池在-10°C到20°C多温度条件下的动态充放电数据集进行对比实验验证。

Conclusion: FDIFF-PINN模型能够更好地表征电池系统的复杂动态特性，提高SOC估计的准确性和物理可解释性。

Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).

</details>


### [56] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer是一种用于长序列时间序列预测的分层Transformer，通过局部和全局信息提取器结合稀疏注意力机制，实现线性复杂度并在多个真实数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 长序列时间序列预测需要处理大量历史数据，传统Transformer的二次复杂度限制了其应用。需要设计更高效的架构来捕获局部动态和全局依赖关系。

Method: 1. 将输入分割为不重叠的时间片段；2. 局部信息提取器使用top-k稀疏注意力建模片段内动态，然后进行平均池化；3. 全局信息提取器使用相同的top-k注意力捕获片段间长程依赖；4. 轻量级GRU聚合全局上下文化的片段标记进行多步预测。

Result: 在8个真实世界数据集（天气、股价、温度、电力消耗、电力、疾病）上，预测范围96-720步，TwinFormer在34个评估中27次进入前两名，其中17次MAE和RMSE最佳，10次次佳。复杂度为线性O(kLd)。

Conclusion: TwinFormer通过分层结构和稀疏注意力机制有效解决了长序列时间序列预测问题，在多个领域数据集上超越了现有方法，证明了其设计的有效性。

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [57] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 该论文证明了一个经典的Robbins子高斯混合模型满足路径确定性遗憾界，在Ville事件上遗憾被累积方差过程和对数项所界定，连接了对抗在线学习和博弈论统计。


<details>
  <summary>Details</summary>
Motivation: 连接对抗在线学习（通常处理有界数据的遗憾界）和博弈论统计（可以处理无界数据但需要随机性假设）两个领域，通过条件遗憾界作为桥梁。

Method: 分析Robbins提出的经典子高斯混合模型，证明其在Ville事件上满足路径确定性遗憾界，使用累积方差过程V_T作为关键参数。

Result: 在概率至少为1-α的Ville事件E_α上，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T；在概率为1的事件E_0上，遗憾最终被ln ln V_T界定。

Conclusion: 条件遗憾界可以作为随机和对抗投注之间的桥梁，为处理无界数据提供了新的理论框架，连接了对抗在线学习和博弈论统计。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [58] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文认为不存在单一最佳的不确定性度量方法，应根据具体应用定制不确定性量化，提出基于二阶分布的灵活不确定性度量框架，并展示不同任务需要不同的不确定性度量特性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，准确量化预测不确定性至关重要。虽然已有多种不确定性度量方法，但通常都声称优于其他方法。本文认为不存在单一最佳度量，而应根据具体应用定制不确定性量化。

Method: 使用基于二阶分布的灵活不确定性度量框架，区分总不确定性、偶然不确定性和认知不确定性。这些度量可以通过适当的评分规则（proper scoring rules）进行实例化，以控制其特性。

Result: 不同任务需要不同的不确定性度量特性：1）选择性预测任务中，评分规则应与任务损失匹配；2）分布外检测中，互信息（广泛使用的认知不确定性度量）表现最佳；3）主动学习设置中，基于0-1损失的认知不确定性始终优于其他度量。

Conclusion: 不存在适用于所有任务的最佳不确定性度量方法，不确定性量化应根据具体应用需求进行定制，基于二阶分布的灵活框架可以针对不同任务优化不确定性度量。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [59] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 提出合成蚊群声学分类数据集，使用轻量深度学习模型识别六种主要蚊媒，适合嵌入式低功耗设备部署


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年导致超过70万人死亡，构成严重全球健康威胁。传统数据集需要人工记录单个蚊子，费时费力，限制了声学蚊虫监测研究的发展。

Method: 创建合成蚊群声学分类数据集，模拟真实多物种和嘈杂的蚊群环境。使用对数梅尔频谱图，评估轻量级深度学习架构对蚊子物种的分类能力。

Result: 实验表明，这些模型能有效识别六种主要蚊媒，适合在嵌入式低功耗设备上部署，为实时监测提供可行方案。

Conclusion: 合成蚊群音频数据集有潜力加速声学蚊虫研究，实现可扩展的实时监测解决方案，为蚊媒疾病防控提供技术支持。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [60] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 对Llama-3.2模型在SEC金融语料上进行领域自适应预训练，发现200M token内获得最大收益，金融语言高度规律且可高效学习，通用领域性能保持稳定。


<details>
  <summary>Details</summary>
Motivation: 研究领域自适应预训练(DAPT)如何为高价值领域（如金融）定制大型语言模型，避免完全重新训练的高成本，探索金融基础模型的扩展规律。

Method: 使用1B和3B参数的Llama-3.2模型，在400M token的SEC金融语料上进行继续预训练，在50M、100M、200M和400M token处设置验证检查点，分析损失变化规律。

Result: SEC领域验证损失持续改善，最大收益在前200M token内，之后收益递减；幂律拟合显示浅指数，表明金融语言高度规律；通用领域损失基本不变，无灾难性遗忘；数据效率前沿显示模型向专业化改进，混合领域退化可忽略。

Conclusion: 领域自适应预训练可用相对适中的token预算实现有意义的领域适应，金融语言高度规律且可高效学习，更大规模模型(7B-70B)在预计数据需求下仍可处理，为金融基础模型扩展提供早期经验指导。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [61] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: VGPO提出了一种新的强化学习框架，通过时间维度密集价值估计和绝对价值增强的组归一化，解决了GRPO在图像生成中的时间信用分配和优化停滞问题。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO在流匹配图像生成中存在两个关键限制：1）稀疏终端奖励在所有时间步均匀应用，损害了时间信用分配，忽略了从早期结构形成到后期调整的不同关键阶段；2）仅依赖相对组内奖励导致优化信号随训练收敛而减弱，当奖励多样性耗尽时出现优化停滞。

Method: VGPO框架在时间和组维度上重新定义价值估计：1）将稀疏终端奖励转化为密集、过程感知的价值估计，通过建模每个生成阶段的预期累积奖励实现精确信用分配；2）用绝对价值增强的新过程替代标准组归一化，即使在奖励多样性下降时也能保持稳定的优化信号。

Result: 在三个基准测试上的广泛实验表明，VGPO实现了最先进的图像质量，同时提高了任务特定准确性，有效缓解了奖励黑客问题。

Conclusion: VGPO通过解决GRPO在图像生成中的时间信用分配和优化停滞问题，为流匹配图像生成提供了更有效的强化学习框架，在图像质量和任务准确性方面都取得了显著改进。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [62] [DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields](https://arxiv.org/abs/2512.12402)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: DeepVekua结合几何深度学习和谱分析，在稀疏数据下求解PDE，通过可微坐标变换将复杂几何映射到谐波空间，比现有隐式表示方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决在稀疏数据条件下求解偏微分方程(PDEs)的挑战，特别是标准坐标网络存在的谱偏差问题，以及复杂几何形状下的物理学习困难。

Method: 提出混合架构，结合几何深度学习和谱分析，学习一个可微坐标变换，将复杂几何映射到潜在谐波空间，将几何学习与物理学习分离，以闭式求解最优谱权重。

Result: 在平流-扩散系统上优于最先进的隐式表示方法，相比谱基线有100倍的改进，解决了谱偏差问题。

Conclusion: DeepVekua通过分离几何和物理学习，在稀疏数据条件下有效求解PDE，为复杂几何形状的PDE求解提供了新方法。

Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.

</details>


### [63] [Can Graphs Improve Tabular Foundation Models?](https://arxiv.org/abs/2512.12405)
*Franck Le,Keith Grueneberg,Erich Nahum,Vadim Sheinin*

Main category: cs.LG

TL;DR: BOLERO通过添加轻量级静态二分图头来增强预训练的表格Transformer，在分类和回归任务上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前表格Transformer和上下文学习方法缺乏明确的实例间关系建模机制，而相似样本通常有相关结果。研究是否可以通过引入简单的图先验来增强预训练的表格Transformer。

Method: 提出BOLERO：一个轻量级静态二分图头，增强RoBERTa-Tab（基于掩码标记预测预训练的RoBERTa风格表格骨干）。每个实例连接到特征/值锚点；小型GNN细化行表示，同时骨干网络保持冻结。

Result: 在TP-BERTa基准套件的80个分类和64个回归数据集上评估，与XGBoost、CatBoost、TabPFN-v2、MITRA、TabICL、TP-BERTa和RoBERTa-Tab等强基线比较。BOLERO在分类和回归任务上都获得了最高数量的统计显著胜出。

Conclusion: 轻量级图先验能够有意义地改进预训练的表格Transformer，BOLERO证明了这种方法的有效性。

Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.

</details>


### [64] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗糙集理论的增强解释方法，用于改进图谱聚类在文本文档聚类中的可解释性问题


<details>
  <summary>Details</summary>
Motivation: 图谱聚类方法虽然能处理各种形状和密度的聚类，但在文本文档聚类中难以解释结果，主要因为谱空间嵌入与文档内容没有明显关系，加上模糊文档和聚类算法的随机性进一步降低了可解释性

Method: 基于团队先前研究的解释方法，引入粗糙集理论进行增强，以解决上述问题

Result: 未在摘要中明确说明具体结果

Conclusion: 通过结合粗糙集理论，提出的增强方法能够克服图谱聚类在文本文档聚类中的可解释性挑战

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [65] [Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction](https://arxiv.org/abs/2512.12445)
*Abdul Matin,Rupasree Dey,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.LG

TL;DR: 该论文提出了一种知识引导的ViT掩码自编码器，将线性光谱混合模型和光谱角匹配器作为物理约束嵌入自监督重建过程，提升模型的可解释性、泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 将领域知识融入深度学习可以提升模型的可解释性、泛化能力和数据效率。当前方法主要依赖数据驱动优化，缺乏对已知物理结构和关系的利用。

Method: 提出知识引导的ViT掩码自编码器框架，将线性光谱混合模型作为物理约束，光谱角匹配器作为几何一致性约束，与传统的Huber损失联合优化，确保学习到的表征符合物理原理。

Result: 实验结果表明，该模型显著提升了重建质量，改善了下游任务性能，在有限监督下稳定训练，并产生了基于物理原理的可解释潜在表征。

Conclusion: 将物理知识作为归纳偏置嵌入基于Transformer的自监督学习具有广阔前景，能够增强重建保真度、稳定训练过程并产生可解释的表征。

Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.

</details>


### [66] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 通过过参数化架构结合可微分稀疏化，学习紧凑、可解释的KANs，在保持准确性的同时提升可解释性


<details>
  <summary>Details</summary>
Motivation: 传统KANs架构增强方法增加了复杂性，损害了KANs原本的可解释性优势。需要一种既能保持准确性又能提升可解释性的方法。

Method: 采用过参数化架构结合可微分稀疏化技术，将架构搜索转化为端到端优化问题，学习紧凑的KANs模型。

Result: 在函数逼近基准测试、动力系统预测和实际预测任务中，实现了竞争性或更优的准确性，同时发现了显著更小的模型。过参数化与稀疏化具有协同效应。

Conclusion: 提供了一条原则性路径，使模型既更具表达能力又更可解释，解决了科学机器学习中的关键矛盾。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [67] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出跨模态知识蒸馏框架，将预训练的多会话尖峰Transformer模型的高保真表征知识转移到LFP Transformer模型，显著提升LFP模型性能


<details>
  <summary>Details</summary>
Motivation: 局部场电位(LFP)在神经实验中具有重要优势（长期稳定性好、抗电极退化、功耗低），但现有建模框架主要关注尖峰活动，LFP信号由于聚合特性导致对下游任务变量的预测能力较低

Method: 采用跨模态知识蒸馏框架：1）使用会话特定的神经标记化策略，通过掩码自编码目标训练多会话尖峰教师模型；2）将学生LFP模型的潜在表征与教师尖峰模型对齐

Result: 蒸馏后的LFP模型在完全无监督和监督设置下均优于单会话和多会话LFP基线，能够泛化到其他会话而无需额外蒸馏，同时保持优异性能

Conclusion: 跨模态知识蒸馏是利用高性能尖峰模型开发更准确LFP模型的有效且可扩展方法

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [68] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出一个多模态神经解码框架，能够实时递归解码不同时间尺度、不同概率分布且可能存在缺失样本的多模态神经信号。


<details>
  <summary>Details</summary>
Motivation: 现有非线性多模态神经活动模型无法处理不同时间尺度或缺失样本问题，且部分模型不支持实时解码。需要开发能够实时处理多尺度、多分布、含缺失样本的多模态神经信号的解码框架。

Method: 框架包含三个核心组件：1) 多尺度编码器，通过学习模态内动态非线性聚合信息，实时处理不同时间尺度和缺失样本；2) 多尺度动态骨干网络，提取多模态时间动态特征，支持实时递归解码；3) 模态特定解码器，适应不同模态的概率分布差异。

Result: 在仿真和三个不同的多尺度脑数据集上，该模型能够有效聚合不同时间尺度、不同分布且含缺失样本的多模态信息，显著提升实时目标解码性能，并优于各种线性和非线性多模态基准方法。

Conclusion: 该学习框架成功解决了多模态神经信号解码中的时间尺度差异、分布差异和缺失样本问题，实现了高效的实时递归解码，为神经科学应用提供了重要工具。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [69] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文系统研究了Transition Matching框架中头部模块的设计、训练和采样策略，通过大规模实验发现MLP头部配合特定时间加权和高频采样器在各项指标上表现最佳，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: Transition Matching作为一种新兴生成建模范式，虽然比扩散和流匹配模型更具表达力，但其头部模块的设计、训练和采样策略缺乏系统性研究。本文旨在填补这一空白，通过大规模实验探索如何优化TM框架的性能和效率。

Method: 采用时间连续双向变体的TM框架，训练了56个不同的17亿参数文本到图像模型（共549次评估），系统研究头部模块架构、训练建模方式以及一系列随机TM采样器的影响。

Result: 实验发现：1）MLP头部配合特定时间加权和高频采样器在所有指标上排名最佳，达到SOTA；2）Transformer头部配合序列缩放和低频采样器在图像美学方面表现出色；3）识别了对质量和效率提升最关键的几个设计方面。

Conclusion: 本研究为TM框架提供了系统性的设计指导，明确了哪些设计选择能带来最大收益，哪些选择可能不再提供进一步增益，为未来TM模型的发展提供了重要参考。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [70] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: 提出稀疏概念锚定方法，仅需极少监督（每个锚定概念<0.1%的标注样本）即可在潜在空间中定位目标概念子集，同时让其他概念自组织，实现可解释、可操控的行为干预。


<details>
  <summary>Details</summary>
Motivation: 为了解决在学习的表示中实现可解释和可操控行为的问题，需要一种方法能够有选择性地定位和操作特定概念，同时保持其他特征的完整性，且仅需极少的监督信号。

Method: 结合激活归一化、分离正则化器以及锚定或子空间正则化器，将稀有标注样本吸引到预定义方向或轴对齐子空间，在潜在空间中偏置目标概念的位置。

Result: 在结构化自编码器上的实验表明，能够有选择性地衰减目标概念而对正交特征影响可忽略，完全消除时重建误差接近理论界限，实现了可逆行为操控和永久移除。

Conclusion: 稀疏概念锚定为学习表示中的可解释、可操控行为提供了实用途径，通过极少监督实现精确的概念定位和操作。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [71] [GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction](https://arxiv.org/abs/2512.12489)
*Shuhui Qu,Cheolwoo Park*

Main category: cs.LG

TL;DR: GoMS提出了一种新的分子图神经网络架构，将分子表示为子结构图而非独立子结构集合，通过建模子结构间的相互作用和空间排列关系，显著提升了分子性质预测性能，尤其在大分子上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ESAN将分子视为独立子结构的集合，忽略了子结构之间的关键关系和空间排列。这种"袋子"表示方法丢失了子结构如何连接和重叠的重要拓扑信息，限制了模型对复杂分子性质的理解能力。

Method: GoMS构建了一个子结构图，其中节点代表分子子图，边捕获子结构之间的结构关系。这种方法保留了子结构如何连接和重叠的关键拓扑信息，能够显式建模子结构间的相互作用和空间排列。

Result: 在公开分子数据集上的实验表明，GoMS超越了ESAN和其他基线方法，特别是在包含100个以上原子的大分子上表现尤为突出。随着分子尺寸增大，性能差距进一步扩大，证明了GoMS对工业规模分子的建模有效性。

Conclusion: GoMS通过捕获子结构间的关系，代表了分子性质预测领域的重要进展。该方法特别适用于材料科学中涉及复杂分子的应用场景，其中性质源于多个功能单元之间的相互作用。理论分析表明GoMS能够区分具有相同子结构组成但不同空间排列的分子。

Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.

</details>


### [72] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 该研究比较了决策树和LSTM模型在在线学习环境中识别风险学生的时序预测能力，发现不同干预阶段需要不同的性能指标：早期需要高召回率，中期需要平衡的精度-召回率，后期需要高精度。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中早期识别风险学生对于有效干预至关重要。研究旨在探索不同时间点的预测模型性能，以确定最佳的干预时机和模型选择策略。

Method: 将时序预测分析扩展到第20周（课程时长的50%），在六个时间快照上比较决策树和LSTM模型。分析静态人口统计特征的重要性，并评估不同干预阶段的最佳性能指标。

Result: 静态人口统计特征占预测重要性的68%，支持无评估的早期预测。LSTM在第2周达到97%的召回率，适合早期干预；决策树在中期提供稳定的平衡性能（78%准确率）。到第20周，两种模型的召回率收敛到68%，但LSTM的精度更高（90% vs 86%）。

Conclusion: 模型选择应取决于干预时机：早期干预需要高召回率模型（如LSTM），中期需要平衡性能模型（如决策树），后期需要高精度模型。早期信号（第2-4周）结合人口统计和入学前信息足以进行可靠的初始预测。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [73] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出新的心脏移植分配政策模拟器，发现现行政策不如短视政策，开发考虑动态分配过程的改进政策，并证明批量处理捐赠者能提升性能


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植分配政策（2018年修订）未能充分考虑移植前和移植后死亡率，且面临捐赠者严重短缺的问题，需要改进分配效率

Method: 使用UNOS历史数据开发模拟器，比较不同政策性能；提出基于"潜力"概念的动态分配政策，衡量患者在未来分配中的效用；探索批量处理捐赠者的可行性

Result: 现行政策明显劣于最大化移植获得年数的短视政策；考虑动态分配的改进政策表现更优；批量处理捐赠者能进一步提升性能；模拟器能评估地理邻近性和移植中心拒绝倾向等关键因素

Conclusion: 通过新开发的模拟器和基于潜力的动态分配政策，能够显著改善心脏移植分配效率，为政策制定提供数据支持，缓解捐赠者短缺问题

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [74] [Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems](https://arxiv.org/abs/2512.12523)
*Wenqi Fang,Ye Li*

Main category: cs.LG

TL;DR: 提出一种基于奇异值分解和半正交约束的神经网络架构，用于从噪声时间序列数据中检测临界转变，相比传统对比学习方法更轻量且抗噪性更强。


<details>
  <summary>Details</summary>
Motivation: 复杂噪声时间序列中的临界转变检测是科学和工程中的基本挑战。传统基于深度神经网络的对比学习方法通常过参数化且对无关噪声敏感，导致临界点识别不准确。

Method: 提出一种基于奇异值分解技术构建的神经网络架构，配合严格半正交约束训练算法，以增强传统对比学习的性能。

Result: 大量实验表明，所提方法在识别临界转变方面与传统对比学习技术性能相当，但更轻量且抗噪性显著更强。

Conclusion: 通过奇异值分解和半正交约束的神经网络架构能够有效解决传统对比学习在临界转变检测中的过参数化和噪声敏感问题，提供更鲁棒的解决方案。

Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.

</details>


### [75] [Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling](https://arxiv.org/abs/2512.12526)
*Agustín M. de los Riscos,Julio E. Sandubete,Diego Carmona-Fernández,León Beleña*

Main category: cs.LG

TL;DR: 应用经验模态分解(EMD)于MSCI世界指数，将分解得到的内在模态函数(IMFs)转换为图表示，以便用图神经网络(GNN)建模金融时间序列。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析方法难以捕捉金融数据的复杂多尺度特征，需要开发能够有效建模不同时间尺度结构特性的方法，以支持更准确的预测。

Method: 使用CEEMDAN方法从MSCI世界指数中提取9个IMF，涵盖高频波动到长期趋势。采用四种时间序列转图方法：自然可见性图、水平可见性图、递归图和转移图。对生成的图进行拓扑分析。

Result: 发现明显的尺度依赖结构：高频IMF产生密集、高度连接的小世界图，低频IMF产生更稀疏、特征路径长度更长的网络。可见性方法对振幅变化更敏感且聚类系数更高，递归图更好地保留时间依赖性。

Conclusion: 研究结果为设计针对分解组件结构特性的GNN架构提供了指导，支持更有效的金融时间序列预测建模。

Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.

</details>


### [76] [Effective Fine-Tuning with Eigenvector Centrality Based Pruning](https://arxiv.org/abs/2512.12543)
*Shaif Chowdhury,Soham Biren Katlariwala,Devleena Kashyap*

Main category: cs.LG

TL;DR: 提出基于图论的神经网络剪枝方法，通过特征向量中心性识别重要神经元，剪枝后再微调，在多个数据集上实现更高准确率和更低模型复杂度


<details>
  <summary>Details</summary>
Motivation: 受社交媒体中少数高影响力用户能驱动网络话语变化的启发，认为神经网络微调时应先剪枝保留重要神经元再微调，而非传统方法仅添加新分类层

Method: 将神经元表示为图节点，边编码神经元相似性，基于特征向量中心性计算重要性分数进行剪枝，保留最中心神经元后再微调

Result: 在VGGNet、EfficientNet、ResNet模型和TF Flowers、Caltech 101、Oxford Flowers 102数据集上评估，方法在显著降低模型复杂度同时获得更高分类准确率。Oxford Flowers 102数据集上达到48%准确率，相比基线VGGNet的30%有显著提升

Conclusion: 基于图论的剪枝方法能有效识别神经网络中的重要神经元，剪枝后微调比传统微调方法性能更好，在保持模型轻量化的同时提升分类准确率

Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.

</details>


### [77] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S是一个多圈层耦合的概率模型，用于全球次季节到季节(S2S)每日集合预报，通过扩散模型和最优传输耦合模块，在45天预报中超越了ECMWF和FuXi-S2S系统。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加剧背景下，准确的次季节到季节(S2S)极端事件预测对资源规划和灾害缓解至关重要，但由于复杂的多圈层相互作用和大气内在不确定性，此类预测仍然具有挑战性。

Method: 首先将多样化的多圈层预测因子编码到紧凑的潜在空间，然后使用扩散模型生成每日集合预报。在去噪器中加入基于最优传输(OT)的新型耦合模块，优化大气与多圈层边界条件之间的相互作用。

Result: 在1.5°分辨率下，TianXing-S2S在45天每日平均集合预报中，在关键大气变量上超越了ECMWF S2S系统和FuXi-S2S。模型能够熟练预测热浪和异常降水等极端事件，识别土壤湿度为关键前兆信号，并能生成长达180天的稳定滚动预报。

Conclusion: TianXing-S2S为变暖世界中的S2S研究建立了稳健框架，展示了多圈层耦合扩散模型在次季节到季节预测中的优越性能。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [78] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 该论文解决了在线学习中无标签数据能力的30年开放问题，证明转导在线学习与标准在线学习之间存在平方级差距（Ω(√d) vs d），其中d是概念类的Littlestone维度。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中关于无标签数据能力的30年开放问题，量化转导学习与标准在线学习之间的差距。在PAC学习中，转导和标准学习具有相似的样本复杂度，但在在线学习中这一差距一直未明确。

Method: 通过理论分析证明转导在线学习的错误下界为Ω(√d)，并构造具体概念类证明该下界是紧的（存在O(√d)的上界）。改进了之前Ben-David等人和Hanneke等人的下界结果。

Result: 证明了转导在线学习的最小错误下界为Ω(√d)，且该下界是紧的（存在O(√d)的上界）。这改进了之前Ω(log log d)、Ω(√log d)和Ω(log d)的下界，以及(2/3)d的上界。

Conclusion: 转导在线学习与标准在线学习之间存在平方级差距（Ω(√d) vs d），这凸显了提前访问无标签实例序列的好处，与PAC设置中两者样本复杂度相似的情况形成对比。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [79] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 该论文对数据归因方法（单牛顿步NS和影响函数IF）进行了新的理论分析，首次在不假设全局强凸性的情况下，推导出渐近紧致的误差界限，并解释了NS通常比IF更准确的现象。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法的理论分析存在两个主要局限：1）依赖全局强凸性假设，这在实践中常不成立；2）误差界限随参数数量(d)和移除样本数(k)的缩放效果很差。这导致无法回答"各方法的误差渐近缩放规律是什么？"和"哪种方法对给定数据集更准确？"等基本问题。

Method: 针对凸学习问题，对NS和IF数据归因方法进行新的理论分析。首次在不假设全局强凸性的情况下，推导出渐近紧致的误差界限（在多项式对数因子内）。特别针对行为良好的逻辑回归证明了界限的紧致性。

Result: 推导出平均情况下的误差缩放规律：NS与真实参数的期望误差为Θ̃(kd/n²)，NS与IF之间的期望误差为Θ̃((k+d)√(kd)/n²)。这些界限是渐近紧致的，解释了NS通常比IF更准确的现象。

Conclusion: 该研究首次在不依赖全局强凸性假设的情况下，为数据归因方法提供了紧致的理论分析，解释了NS相对于IF的优势，并为实际应用中的方法选择提供了理论依据。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [80] [Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses](https://arxiv.org/abs/2512.12581)
*David Strnadel*

Main category: cs.LG

TL;DR: 探索性研究：将参数化量子电路的可微分能量项作为GAN的辅助正则化信号，在MNIST上测试表明能提升分类准确率，但样本质量指标波动大，计算开销大，未声称量子优势。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算与生成对抗网络的结合可能性，研究参数化量子电路的可微分能量项是否可以作为GAN训练中的辅助正则化信号，为量子机器学习提供新的方法论思路。

Method: 在辅助分类器GAN（ACGAN）生成器目标函数中增加变分量子本征求解器（VQE）启发的能量项，使用Qiskit的EstimatorQNN和TorchConnector计算类特定伊辛哈密顿量的能量，在4量子位的无噪声状态向量模拟器上进行实验。

Result: 在MNIST数据集上，量子增强模型（QACGAN）在5个epoch内达到99-100%的分类准确率，而传统ACGAN为87.8%，表明辅助项影响类别条件。但样本质量指标（FID）在运行间波动大（变异系数约25%），计算开销比经典ACGAN慢约200倍。

Conclusion: 该方法展示了VQE风格的能量计算可以通过可微分路径集成到GAN训练循环中，但未声称量子优势或改进稳定性。量子辅助信号是否优于等效经典正则化器仍为开放问题，需要系统消融研究。

Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.

</details>


### [81] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

TL;DR: EFLA是一种无误差线性注意力机制，通过将在线学习更新建模为连续时间动力系统，推导出精确闭式解，实现线性时间复杂度的稳定并行计算。


<details>
  <summary>Details</summary>
Motivation: 解决传统softmax注意力在长上下文语言模型中的二次计算成本瓶颈，同时克服现有线性时间注意力方法（如SSMs）可能存在的数值不稳定性和误差累积问题。

Method: 将delta规则的在线学习更新建模为连续时间动力系统，利用动力学矩阵的秩-1结构，推导出精确闭式解（相当于无限阶Runge-Kutta方法），实现无误差累积的线性时间注意力。

Result: EFLA在嘈杂环境中表现稳健，在语言建模困惑度和下游基准测试中优于DeltaNet，且不引入额外参数，实现了线性时间复杂度的稳定并行计算。

Conclusion: EFLA为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础，解决了传统注意力方法的计算瓶颈和数值稳定性问题。

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [82] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 本文提出一个结合模型可解释性和因果推断的框架，用于零售业销售洞察分析，通过双机器学习方法处理混杂变量，提高因果效应估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前零售业虽然收集大量数据并使用机器学习模型进行分析，但现有模型缺乏可解释性，且无法验证或发现因果关系，限制了深度业务洞察的获取。

Method: 回顾现有因果推断和可解释性文献，应用于真实零售数据集，使用SHAP值评估模型可解释性，采用双机器学习方法处理多个混杂变量。

Result: 研究发现：固有可解释模型具有更低的SHAP值方差；通过双机器学习方法包含多个混杂变量可以获得正确的因果效应符号。

Conclusion: 为零售业提供了一套结合模型可解释性和因果推断的实用框架，能够获得更可靠、可解释的销售洞察，支持更好的业务决策。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [83] [Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks](https://arxiv.org/abs/2512.12642)
*Filippo Maria Bianchi,Carlo Abate,Ivan Marisca*

Main category: cs.LG

TL;DR: Torch Geometric Pool (tgp) 是一个用于图神经网络层次化池化的库，基于PyTorch Geometric构建，提供统一的API和模块化设计，支持快速原型开发。


<details>
  <summary>Details</summary>
Motivation: 图神经网络需要多种池化操作符来处理不同任务和数据，但缺乏一个统一的库来支持快速原型开发和系统比较不同池化方法。

Method: 基于PyTorch Geometric构建，提供一致的API和模块化设计，包含多种池化操作符，支持预计算池化以加速训练。

Result: 通过广泛基准测试展示了库的功能，并系统比较了不同图池化方法在各种下游任务中的性能，结果表明最优池化操作符的选择取决于具体任务和数据。

Conclusion: tgp库能够支持快速原型开发，其模块化设计和统一API使得研究人员能够轻松探索和比较不同的图池化方法，适应不同任务需求。

Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.

</details>


### [84] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

TL;DR: PerNodeDrop是一种轻量级随机正则化方法，通过逐样本、逐节点的扰动打破现有技术的均匀噪声注入，在保留有用协同适应的同时进行正则化，提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易过拟合，神经元倾向于协同适应，这会捕获复杂特征交互但也会强化虚假模式。现有噪声正则化方法（如Dropout、DropConnect）的噪声在层或批次上均匀，会同时抑制有害和有益的协同适应。

Method: 提出PerNodeDrop方法，应用逐样本、逐节点的扰动，打破噪声的均匀性，使每个节点都能经历输入特定的变异性。该方法在样本级别操作，而不是批次级别，通过期望损失分析形式化其扰动如何衰减过度协同适应同时保留预测交互。

Result: 在视觉、文本和音频基准测试上的实证评估表明，相对于标准噪声正则化方法，PerNodeDrop提高了泛化性能，缩小了训练和验证性能之间的差距，提高了在未见数据上的可靠性。

Conclusion: PerNodeDrop通过引入样本特定的节点级扰动，有效平衡了正则化和有用协同适应的保留，提供了一种改进深度神经网络泛化能力的轻量级方法。

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [85] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: DynaGen是一个统一的时序知识图谱推理方法，通过动态构建实体中心子图和条件扩散过程，分别解决插值任务中上下文建模不足和外推任务中认知泛化偏差的问题，在六个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理方法面临两个关键挑战：插值方法中上下文建模有限，以及外推方法中存在认知泛化偏差。插值方法通常将时间信息嵌入到单个事实中，而外推技术则利用序列模型识别重复模式，但都无法充分建模复杂的时间演化规律。

Method: DynaGen采用统一框架处理插值和外推任务：对于插值，动态构建实体中心子图，并使用协同双分支GNN编码器捕捉演化结构上下文；对于外推，应用条件扩散过程，迫使模型学习底层演化原理而非表面模式，增强预测未见未来事件的能力。

Result: 在六个基准数据集上的实验表明，DynaGen实现了最先进的性能。与第二好的模型相比，平均MRR分数在插值任务上提高了2.61分，在外推任务上提高了1.45分。

Conclusion: DynaGen通过创新的动态子图构建和条件扩散过程，有效解决了时序知识图谱推理中的上下文建模和泛化偏差问题，为插值和外推任务提供了统一的解决方案，显著提升了推理性能。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [86] [On Approaches to Building Surrogate ODE Models for Diffusion Bridges](https://arxiv.org/abs/2512.12671)
*Maria Khilchuk,Vladimir Latypov,Pavel Kleshchev,Alexander Hvatov*

Main category: cs.LG

TL;DR: 提出两种基于代理模型的扩散和薛定谔桥新方法：SINDy-FM（稀疏回归识别符号微分方程）和DSBM-NeuralODE（神经ODE重构），在保持性能的同时大幅提升效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和薛定谔桥模型在生成建模中表现出色，但存在计算成本高、训练复杂的问题。连续时间桥虽能加速采样，但其最优动力学由过参数化神经网络描述，且底层随机微分方程难以高效积分。

Method: 提出使用代理模型近似动力学的范式：1) SINDy-FM：利用稀疏回归从数据中识别可解释的符号微分方程；2) DSBM-NeuralODE：将薛定谔桥重构为神经ODE，实现灵活连续时间参数化。

Result: 在高斯传输任务和MNIST潜在翻译实验中，代理模型达到竞争性性能，同时显著提升效率和可解释性。SINDy-FM将参数数量减少数个数量级，实现近瞬时推理。

Conclusion: 代理模型方法为扩散和薛定谔桥模型提供了更简单、快速、灵活且可解释的近似，为实现实际部署的高性能桥模型开辟了新途径。

Abstract: Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.

</details>


### [87] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: 论文提出将提示视为外部注入程序的理论框架，证明单一固定Transformer骨干仅通过提示就能近似广泛的目标行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将提示作为启发式方法，缺乏将其作为理论对象进行系统分析。本文旨在建立提示作为外部程序的严格理论框架，研究固定权重Transformer仅通过提示切换行为的机制。

Method: 构建简化的Transformer模型，将提示视为外部注入程序，建立机制级分解：注意力执行从提示记忆的选择性路由，前馈网络基于检索片段进行局部算术运算，深度堆叠将这些局部更新组合成多步计算。

Result: 证明了构造性存在性结果：单一固定骨干仅通过提示就能近似广泛的目标行为。该框架为形式化提示长度/精度约束下的权衡以及研究基于提示切换的结构限制提供了统一基础。

Conclusion: 提出了将提示作为外部程序的Transformer理论框架，揭示了提示驱动行为切换的机制级原理，为理解提示能力提供了形式化工具，同时与预训练大语言模型的实证研究保持区分。

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [88] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文重新评估了监督微调(SFT)和强化学习(RL)在视觉语言模型推理中的作用，挑战了"RL优于SFT"的主流观点，发现SFT在多种场景下具有关键优势。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型推理领域过度强调强化学习，普遍认为SFT不仅无法提升推理能力，还可能对训练产生负面影响。本文旨在通过系统对比重新评估SFT和RL的相对价值。

Method: 在相同数据源条件下，系统比较SFT和RL方法，考察模型容量、数据规模和数据分布对两种方法效果的影响，特别关注SFT在多种场景下的表现。

Result: 发现SFT在三个关键场景中表现优异：对较弱模型更有效、数据效率更高(2K数据即可达到RL 20K的效果)、跨模态泛化能力更强。同时发现RL存在欺骗性奖励问题。

Conclusion: SFT的作用被低估，挑战了"RL优于SFT"的主流叙事。建议采用更平衡的后训练流程，将SFT和RL作为互补组件，而非对立选择。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [89] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 提出贝叶斯框架用于上下文多任务多臂老虎机，通过潜在上下文变量建模任务间依赖关系，利用粒子近似对数密度高斯过程实现灵活的数据驱动探索。


<details>
  <summary>Details</summary>
Motivation: 在上下文多任务多臂老虎机中，上下文通常只能部分观测，且任务间的奖励分布存在由潜在上下文变量诱导的依赖关系。现有方法难以有效利用这些结构依赖进行跨任务学习，特别是在模型误设或复杂异质性场景下。

Method: 提出贝叶斯框架整合所有任务的观测数据，学习全局联合分布，同时允许对新任务进行个性化推断。使用粒子近似对数密度高斯过程表示任务和奖励的联合分布，无需对潜在变量做先验假设，灵活发现臂间和任务间依赖关系。

Result: 实证表明该方法优于分层模型老虎机等基线方法，特别是在模型误设或复杂潜在异质性场景下表现更佳。

Conclusion: 该框架通过识别结构不确定性和用户特定不确定性，有效利用跨任务依赖关系，在部分观测上下文的多任务多臂老虎机中实现了更高效的探索。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [90] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

TL;DR: 提出MT-PINN方法解决最优交易执行中硬零终端库存约束问题，通过轨迹损失和终端惩罚直接强制零库存，在理论和实际数据上都表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法在处理最优交易执行的硬零终端库存约束时存在不足，容易产生不稳定控制，需要更有效的方法来强制终端库存为零。

Method: 提出多轨迹PINN（MT-PINN），添加基于滚动的轨迹损失，通过时间反向传播传播终端惩罚，直接强制零终端库存，并采用轻量级lambda课程学习稳定训练。

Result: 在Gatheral-Schied单资产模型中，MT-PINN与闭式解高度一致，终端库存紧密集中在零附近，路径误差降低。在SPY日内数据上，风险中性时匹配TWAP，高风险厌恶时实现更低暴露和竞争性成本。

Conclusion: MT-PINN能有效解决最优交易执行中的硬零终端库存约束问题，在理论和实际应用中均表现优异，为风险厌恶交易提供了更好的解决方案。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [91] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 论文从随机函数理论出发，基于无差别原理推导出回归方法，证明了具有平移、旋转、缩放对称性和高斯性的概率测度会自然导出广义多调和样条核，为平滑和插值方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法通常基于经验选择核函数和正则化形式，缺乏理论依据。本文旨在从基本原理出发，为回归方法提供坚实的理论基础，证明在缺乏先验信息时某些方法的自然最优性。

Method: 采用随机函数理论框架，从无差别原理的基本假设出发，要求概率测度具有平移、旋转、缩放不变性和高斯性，然后通过数学推导得到完整的回归解决方案。

Result: 推导出回归核函数为广义多调和样条，并确定了相应的正则化形式和噪声参数化。这些结果不是经验选择的，而是从对称性假设中自然导出的。

Conclusion: 在缺乏先验信息的情况下，具有特定对称性的概率测度会唯一确定最优的回归方法，这为一大类平滑和插值方法提供了理论基础，证明了它们的自然最优性。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [92] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 该论文提出通过引入可训练的自发神经元来改善LLM输入稀疏化的性能，将输入稀疏化重新解释为动态结构剪枝，并借鉴生物神经元的自发基线放电率来稳定稀疏化模型的激活。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但规模庞大带来效率和可解释性挑战。输入稀疏化通过选择性激活输入值子集来提高效率，但现有方法主要关注计算节省，忽视了稀疏化对表示能力的影响，导致与完整模型存在明显性能差距。

Method: 首先将输入稀疏化重新解释为动态结构剪枝。受生物神经元自发基线放电率的启发，引入一小组可训练的自发神经元作为补偿单元，以稳定稀疏化LLM中的激活。

Result: 实验表明，这些辅助神经元显著减少了稀疏化引起的性能差距，并在不同任务上表现出良好的泛化能力。

Conclusion: 通过引入自发神经元补偿机制，可以在保持输入稀疏化计算效率优势的同时，显著改善模型性能，为高效LLM设计提供了新思路。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [93] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: FLFA通过将全局模型权重作为共享反馈矩阵集成到联邦学习中，有效缓解了非IID数据导致的局部漂移问题，在保持低计算和通信开销的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non-IID）数据场景下面临局部漂移问题，这会阻碍全局模型的收敛。现有方法在处理数据异构性时存在效率或效果上的不足。

Method: 提出FLFA框架，将反馈对齐（feedback alignment）集成到联邦学习中。在本地训练的反向传播过程中，使用全局模型的权重作为共享反馈矩阵，从而对齐本地更新与全局模型。

Result: 理论分析表明FLFA能有效缓解局部漂移并保证本地和全局模型的鲁棒收敛。实证评估显示FLFA能提升其他联邦学习方法的准确性，同时保持较低的计算和通信开销。

Conclusion: FLFA是一种高效解决联邦学习中数据异构性问题的框架，通过反馈对齐机制有效缓解局部漂移，为联邦学习在非IID数据场景下的应用提供了新思路。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [94] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: OLR-WAA是一种超参数自由的在线回归模型，通过动态加权平均和概念漂移检测机制，在非平稳数据流中实现稳定性和适应性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集经常表现出不断演变的数据分布（概念漂移），忽略这一现象会显著降低模型预测性能。现有在线模型的超参数通常是固定的，缺乏动态适应变化数据的能力。

Method: 提出OLR-WAA模型：1）使用指数加权移动平均增量更新基础模型；2）引入独特的优化机制，动态检测概念漂移、量化其幅度，并根据实时数据特征调整模型；3）采用保守更新策略处理置信度场景，优先考虑稳定、高置信度的数据点。

Result: 在静态设置中匹配批量回归性能；在概念漂移数据集上始终优于或匹敌最先进的在线模型；快速收敛，相比其他在线模型产生更高的R2值；有效弥补概念漂移导致的性能差距。

Conclusion: OLR-WAA是一种有效、自适应且抗漂移的在线回归模型，能够在非平稳数据流中实现连续适应，平衡模型稳定性与适应性，无需手动调整超参数。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [95] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 研究使用合成数据和替代金融数据（如手机使用、消费习惯等）评估无信用记录的借款人，证明这些数据能显著提升信用评估准确性，接近传统征信水平。


<details>
  <summary>Details</summary>
Motivation: 解决金融排斥问题：许多伊斯坦布尔居民因收入通过非正规渠道流动而没有信用记录，导致无法获得正规金融服务，限制了创业机会、增加了收入波动、扩大了财富差距。

Method: 1. 创建包含10万伊斯坦布尔居民的合成数据集，复制2025年第一季度TÜİK人口普查边际和电信使用模式；2. 使用检索增强生成技术将公共统计数据输入OpenAI o3模型生成真实但隐私保护的记录；3. 每个档案包含7个社会人口变量和9个替代属性（手机规格、网购节奏、订阅支出、汽车所有权、月租金、信用卡标志）；4. 使用CatBoost、LightGBM和XGBoost训练两个版本模型：仅社会人口变量的演示模型和包含所有变量的完整模型。

Result: 替代数据块使AUC提高了约1.3个百分点，平衡F1分数从约0.84提升到0.95，增益达14%。替代行为属性能够接近传统征信的区分能力，同时服务于缺乏正式信用记录的借款人。

Conclusion: 研究贡献包括：开放的伊斯坦布尔2025Q1合成数据集、完全可复现的建模流程、以及经验证据表明简洁的行为属性集可以接近征信水平的区分能力。这些发现为贷款机构和监管机构提供了透明蓝图，以扩展对无银行账户人群的公平和安全信贷访问。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [96] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: OLC-WA是一种自适应、无超参数的在线分类模型，通过指数加权移动平均融合数据流，自动检测概念漂移并调整模型，在动态数据流中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间演化的数据分布（概念漂移），忽视此现象会显著降低模型预测精度。传统在线模型的超参数通常固定，无法随数据分布变化动态调整。

Method: 提出OLC-WA模型：1）使用指数加权移动平均将新数据流与基础模型融合；2）集成优化机制自动检测概念漂移、量化其幅度，并根据数据流特征调整模型。

Result: 在多种基准数据集上的实验表明：在静态环境中，OLC-WA性能与批处理模型相当（准确率差距1-3%）；在概念漂移情况下，超越领先在线基线10-25%。

Conclusion: OLC-WA是一种有效的自适应在线分类模型，无需超参数调优，能自动适应动态数据流中的概念漂移，在静态和动态环境中均表现优异。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [97] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 该论文针对机器学习中多算法跨数据集比较统计检验的不足，提出使用Friedman检验及后续检验来评估在线回归模型性能，通过真实和合成数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习研究大多关注单一数据集上两个学习算法的性能评估，而缺乏比较多个算法在不同数据集上的统计检验方法。在线学习领域尤其需要统计显著性验证来确保连续学习过程的可靠性，特别是在快速收敛和及时处理概念漂移方面。

Method: 使用Friedman检验及相应的后续检验来比较多个在线回归模型在不同数据集上的性能。采用真实和合成数据集，通过5折交叉验证和种子平均进行综合评估，确保对各种数据子集的全面分析。

Result: 测试结果总体上证实了竞争基线的性能与其个体报告一致。但某些统计检验结果也表明，当前最先进的方法在某些方面仍有改进空间。

Conclusion: 该研究填补了多算法跨数据集比较统计检验的空白，为在线学习领域提供了有效的统计评估框架，同时指出了当前最先进方法仍需改进的方向。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [98] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: LRT是一种具有自适应深度推理能力的Transformer架构，通过迭代更新、丢弃校正和学习停止机制，在数独任务上达到98.68%数字准确率和36.30%完整谜题准确率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer通常依赖单次前向传播进行推理，难以处理需要多步结构化推理的复杂任务。需要一种能够自适应调整计算深度、纠正早期错误并基于输入难度分配计算资源的架构。

Method: LRT采用循环推理令牌在多个内部步骤中迭代更新，包含丢弃门校正机制和学习停止机制。模型通过自适应深度推理，允许纠正早期错误，并根据输入复杂度动态分配计算资源。

Result: 在数独推理任务上，LRT达到98.68%数字准确率和36.30%完整谜题准确率，无需使用符号规则或搜索。分析显示丢弃门和停止门在稳定推理和调整计算深度方面发挥重要作用。

Conclusion: LRT展示了自适应深度推理Transformer在结构化推理任务上的有效性，其机制可扩展到国际象棋等更大规模推理任务，并可通过多令牌推理和更大领域扩展进一步增强。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [99] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: TRACER框架通过迁移学习实时适应临床环境变化，解决电子健康记录预测模型因时间性人群变化导致的性能漂移问题。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持工具常因时间性人群变化（如COVID-19疫情）导致性能漂移，特别是当变化最初只影响部分患者时，形成混合人群。这种病例组合变化通常源于系统级操作更新或新疾病出现。

Method: 提出TRACER框架：识别就诊级别的转换成员资格，使用迁移学习调整预测模型而无需完全重新训练。通过识别患者属于哪个群体（历史或新群体）来适应变化。

Result: 在模拟研究中，TRACER优于基于历史或当代数据训练的静态模型。在实际应用中，预测COVID-19过渡期间急诊就诊后的住院情况，TRACER提高了区分度和校准度。

Conclusion: TRACER为在不断演变和异质的临床条件下保持稳健的预测性能提供了可扩展的方法，能够适应临床环境变化而无需完全重新训练模型。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [100] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 本文为Transformer在几何数据上的尺寸泛化能力提供了理论框架，证明了离散样本输出与连续域等效输出之间的误差界限，该界限由采样密度和数据流形内在维度决定。


<details>
  <summary>Details</summary>
Motivation: Transformer展现出显著的尺寸泛化能力，能够从较小的token集合外推到更长的序列，这种现象在点云、图数据和自然语言等多个领域都有实证成功，但缺乏严格的理论表征。

Method: 为几何数据建立理论分析框架，将几何数据表示为连续源的离散样本（如流形上的点云、图论中的图）。核心贡献是推导Transformer在离散样本输出与其连续域等效输出之间的误差界限。

Result: 证明了对于具有稳定位置编码的Transformer，误差界限由采样密度和数据流形的内在维度决定。在不同尺寸的图和点云上的实验证实了理论界限的紧致性。

Conclusion: 本文为Transformer在几何数据上的尺寸泛化现象提供了首个严格的理论分析框架，揭示了采样密度和内在维度对泛化能力的关键影响，并通过实验验证了理论结果的有效性。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [101] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 该论文提出了一个在概念漂移和有限预算下，用于机器学习模型训练和部署的资源分配框架，分析了概念持续时间分布对最优训练策略的影响，并研究了通信约束下的模型部署问题。


<details>
  <summary>Details</summary>
Motivation: 在概念漂移和有限预算的背景下，模型提供商需要向多个客户端分发训练好的模型，但客户端设备仅支持本地推理而无法重新训练模型，因此性能维护的责任落在提供商身上。需要解决资源分配、概念漂移动态和部署时机的交互问题。

Method: 提出了一个模型无关的框架，分析资源分配、概念漂移动态和部署时机的交互。研究了概念持续时间分布（DMRL和IMRL）对最优训练策略的影响，并在通信约束下研究了模型部署问题，证明了相关优化问题的拟凸性，并提出了随机调度策略。

Result: 证明了最优训练策略严重依赖于概念持续时间的衰老特性：在突然概念变化下，当概念持续时间服从递减平均剩余寿命（DMRL）分布时，可以推导出预算约束下的最优训练策略；而直观启发式方法在递增平均剩余寿命（IMRL）下被证明是次优的。对于通信约束下的模型部署，证明了相关优化问题在温和条件下是拟凸的，并提出了能达到接近最优客户端性能的随机调度策略。

Conclusion: 该研究为概念漂移下成本高效的机器学习模型管理提供了理论和算法基础，对持续学习、分布式推理和自适应机器学习系统具有重要启示。研究结果表明，考虑概念漂移动态和资源约束的系统设计对于实际部署至关重要。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [102] [On the continuity of flows](https://arxiv.org/abs/2512.12821)
*Congzhou M Sha*

Main category: cs.LG

TL;DR: 论文研究了流匹配生成模型中拓扑不匹配问题：当先验分布和目标分布拓扑结构不匹配时，最优速度场会出现空间不连续性，导致粒子在中间时间需要做出离散路由决策。


<details>
  <summary>Details</summary>
Motivation: 流匹配已成为连续归一化流生成建模的强大框架，但存在潜在的拓扑约束问题：当先验分布和目标分布拓扑结构不匹配时，标准流匹配目标下的最优速度场可能表现出空间不连续性。

Method: 通过理论分析双模高斯混合模型，证明最优速度场在决策边界处存在跳跃不连续性，其幅度随时间接近目标分布而趋于无穷大。同时进行实证验证，并讨论流匹配在流形上的潜在影响。

Result: 分析表明，最优速度场在决策边界处确实表现出跳跃不连续性，这种现象不是L²损失特有的，而是分布间拓扑不匹配的结果。实证结果验证了理论分析。

Conclusion: 拓扑不匹配会导致流匹配中的速度场不连续性，这可能是分布间拓扑差异的必然结果。这一发现对流匹配在流形上的应用有重要启示，并与黎曼流匹配和学习神经网络中不连续表示的最新研究相关。

Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.

</details>


### [103] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

TL;DR: 该论文提出了一种基于输入损失流形内在维度（ID）的对抗样本检测方法，通过分析模型梯度参数的内在维度差异来区分自然样本和对抗样本，在批量和单样本检测场景中均取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对微小对抗扰动具有脆弱性，而医疗诊断、自动驾驶等应用对可靠性要求极高，因此需要有效的对抗攻击检测方法。

Method: 分析模型输入损失流形的几何特性，研究模型梯度参数的内在维度（ID），利用自然数据和对抗数据在ID上的显著差异作为检测基础。

Result: 在MNIST和SVHN数据集上批量检测效果显著；在CIFAR-10和MS COCO等挑战性基准测试中，针对CW、AutoAttack等多种攻击，检测率始终高于92%，达到新的最先进水平。

Conclusion: 内在维度是跨不同数据集和攻击策略的强大对抗检测指纹，几何方法在对抗检测中具有鲁棒性和有效性。

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [104] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 开发了一个评估公路铁路平交道口(HRGC)悬挂风险的网络级框架，结合激光扫描、深度学习模型和车辆尺寸数据，识别高风险道口并提供决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 陡峭的公路铁路平交道口对低底盘车辆构成安全隐患，可能导致车辆卡在轨道上，引发火车与车辆碰撞事故。需要系统性的方法来评估整个网络中的道口悬挂风险。

Method: 1) 使用步行剖面仪和Pave3D8K激光成像系统收集道口剖面数据；2) 开发LSTM和Transformer混合深度学习模型重建准确的道口剖面；3) 收集约350辆特种车辆的尺寸数据；4) 使用三种车辆尺寸场景分析悬挂风险；5) 开发ArcGIS数据库和软件界面。

Result: 在三种车辆尺寸场景下，分别识别出36、62和67个最高悬挂风险等级的道口。开发了实用的决策支持工具，帮助交通机构缓解道口危险。

Conclusion: 该框架通过整合新一代传感技术、深度学习和基础设施数据集，推进了安全评估，为实际决策提供了支持工具，有助于系统性降低公路铁路平交道口的悬挂风险。

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [105] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

TL;DR: PRIVEE是一种保护隐私的纵向联邦学习防御机制，通过混淆置信度分数来防止特征推断攻击，同时保持预测准确性和分数间的相对关系。


<details>
  <summary>Details</summary>
Motivation: 纵向联邦学习虽然支持跨组织协作训练，但容易受到特征推断攻击，攻击者可以利用共享的置信度分数重构其他参与方的私有输入特征。

Method: 提出PRIVEE防御机制，通过混淆置信度分数（保持相对排名和分数间距离等关键属性）来保护隐私，只共享转换后的表示而非原始分数。

Result: 实验表明PRIVEE在隐私保护方面比现有最先进防御方法提高了三倍，同时保持完整的预测性能，能有效抵御高级特征推断攻击。

Conclusion: PRIVEE为纵向联邦学习提供了一种有效的隐私保护解决方案，在保护用户隐私的同时不牺牲模型预测准确性。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [106] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 提出选择性共形风险控制(SCRC)框架，结合选择性分类与共形预测，通过两阶段方法实现紧凑可靠的预测集，提供两种算法分别保证精确有限样本覆盖和高效PAC式概率保证。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽提供分布无关的覆盖保证，但常产生过大的预测集，限制实际应用。需要更紧凑可靠的预测集来提升机器学习系统在高风险领域的部署实用性。

Method: 提出选择性共形风险控制(SCRC)框架：第一阶段选择置信样本进行预测，第二阶段在选定子集上应用共形风险控制构建校准预测集。开发两种算法：SCRC-T通过联合计算校准和测试样本阈值保持可交换性；SCRC-I是仅校准变体，提供PAC式概率保证。

Result: 在两个公开数据集上的实验表明，两种方法均达到目标覆盖和风险水平，性能几乎相同。SCRC-I表现出略保守的风险控制，但计算效率更高。

Conclusion: 选择性共形风险控制为实现紧凑可靠的预测集提供了有效且高效的路径，平衡了覆盖保证与预测集紧凑性的需求。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [107] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

TL;DR: 提出基于GRPO的强化学习框架，优化LLM在语义等价提示下的响应一致性，解决企业部署中的信息稳定性问题


<details>
  <summary>Details</summary>
Motivation: LLM在金融、教育、医疗等关键业务领域部署时，对语义等价但措辞不同的提示会产生不一致响应，这损害用户信任、增加合规难度并破坏用户体验。企业场景如HR入职、客户支持等需要信息传递的稳定性，而现有方法无法保证跨等价提示的一致性。

Method: 提出基于Group Relative Policy Optimization (GRPO)的强化学习框架，将语义等价提示视为组，引入基于熵的有用性和稳定性奖励，重置对话上下文以隔离措辞影响，直接优化LLM的一致性表现。

Result: 在投资和职位推荐任务上的实验表明，GRPO训练模型比微调或基于解码的基线方法更有效地减少响应变异性，实现了更好的信息一致性。

Conclusion: 这是GRPO在LLM对齐中的新颖应用，将响应变异性重新定义为可纠正的缺陷而非生成多样性的可接受特征，为企业部署提供了信息一致性的解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [108] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出一个新颖的主动学习框架，通过优化分配查询点给标注者来最小化标签噪声，从而提高分类模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 主动学习中标注者提供的标签往往存在噪声，特别是复杂样本更容易被错误标注，这会导致分类器性能下降。需要解决标签噪声问题来构建更准确的分类模型。

Method: 1) 分配模型：优化分配查询点给标注者，以最小化每个周期内的最大可能噪声；2) 新的采样方法：识别最佳查询点，减少标签噪声对分类器性能的影响。

Result: 实验表明，该方法相比多个基准方法显著提高了分类性能。

Conclusion: 提出的主动学习框架通过最小化标签噪声，能够构建更鲁棒的分类模型，在存在噪声标注的环境中表现优异。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [109] [Improving Recursive Transformers with Mixture of LoRAs](https://arxiv.org/abs/2512.12880)
*Mohammadmahdi Nouriborji,Morteza Rohanian,Omid Rohanian*

Main category: cs.LG

TL;DR: MoL（LoRA混合）通过条件计算机制在共享FFN中插入LoRA专家，恢复递归transformer中因参数共享而损失的表达能力，ModernALBERT在紧凑模型中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 递归transformer中的参数共享减少了模型大小，但导致层间表达能力下降。需要一种轻量级机制来恢复表达能力，同时保持参数效率。

Method: 提出MoL（LoRA混合），在共享前馈网络中插入LoRA专家，实现token条件权重空间调制。预训练ModernALBERT架构，集成旋转嵌入、GeGLU、FlashAttention和蒸馏初始化。还提出专家合并过程，在推理时将MoL压缩为单个适配器。

Result: ModernALBERT（50M-120M参数）在GLUE、SQuAD-v2和BEIR基准测试中，在紧凑模型中达到SOTA性能，超越更大的完全参数化基线模型。专家合并过程能在保持准确性的同时实现高效部署。

Conclusion: 条件权重空间调制能有效恢复递归transformer中因激进参数共享而损失的表达能力，MoL提供了一种参数高效的方法来增强模型表达能力。

Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.

</details>


### [110] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出一种无监督学习算法，用于学习多尺度神经数据中的切换动力学系统模型，能同时处理连续和离散神经信号，无需标记数据，在行为解码方面优于单尺度和静态模型。


<details>
  <summary>Details</summary>
Motivation: 神经群体活动常表现出切换动力学的非平稳性，现有方法主要针对单一模态（连续高斯或离散泊松信号），而实际中常同时记录多模态神经数据。此外，训练数据通常缺乏切换状态的标签，这给学习切换动力学模型带来了挑战。

Method: 开发了一种无监督学习算法，仅使用多尺度神经观测数据学习切换多尺度动力学系统模型的参数。该方法能同时处理连续和离散神经信号，无需切换状态的标签信息。

Result: 在模拟数据和两个不同的实验数据集（包含多模态尖峰-LFP观测）上验证了方法。结果显示，切换多尺度动力学系统模型比切换单尺度模型更准确地解码行为，且优于静态多尺度模型，证明了多尺度神经融合和跟踪非平稳性的重要性。

Conclusion: 该无监督学习框架通过利用多模态记录信息并纳入切换机制，能够更准确地建模复杂多尺度神经动力学。这种方法有望提高脑机接口的性能和鲁棒性，并增进对行为神经基础的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [111] [Distillation of Discrete Diffusion by Exact Conditional Distribution Matching](https://arxiv.org/abs/2512.12889)
*Yansong Gao,Yu Sun*

Main category: cs.LG

TL;DR: 提出基于条件分布匹配的离散扩散模型蒸馏方法，直接匹配教师模型和学生模型的条件分布，减少采样所需的函数评估次数


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型(DDMs)虽然强大，但采样时需要大量函数评估，推理成本高。现有加速方法要么依赖近似模拟器，要么需要训练新的学生模型和辅助网络，不够直接和高效。

Method: 提出基于条件分布匹配的蒸馏方法。关键观察是：干净数据给定噪声状态的反向条件分布p_{0|t}(x_0|x_t)可以通过中间时间进行马尔可夫分解，并可以从边际密度比和已知的前向CTMC核中恢复。利用这一结构定义蒸馏目标，直接匹配预训练教师模型和低NFE学生模型的条件分布，适用于单步和多步采样器。

Result: 该方法提供了一种简单且原则性的蒸馏替代方案，能够直接匹配条件分布，避免了近似模拟器或复杂代理目标的需要。

Conclusion: 提出的条件分布匹配方法为离散扩散模型加速提供了一种有效且原则性的蒸馏方案，能够显著减少采样所需的函数评估次数，提高推理效率。

Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.

</details>


### [112] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 研究发现推理模型在低温或贪婪解码时容易陷入文本循环，主要原因是训练分布与学习模型不匹配导致的学习错误，温度调节只是权宜之计而非根本解决方案。


<details>
  <summary>Details</summary>
Motivation: 推理模型（如DeepSeek-R1）在解决复杂问题时生成长思维链，但在低温或贪婪解码时经常出现文本循环重复现象。本研究旨在探究这种现象的原因以及温度在其中扮演的角色。

Method: 使用开放推理模型研究循环现象，分析模型大小、蒸馏等因素的影响。引入合成图推理任务，通过实验验证两种机制：1）学习困难导致的风险规避机制；2）Transformer对时间相关错误的归纳偏置。

Result: 发现低温下循环现象普遍，大模型循环较少，而蒸馏学生模型即使教师模型很少循环也会显著循环。这指向训练分布与学习模型不匹配的学习错误。温度升高通过促进探索减少循环，但无法修复学习错误，导致高温度下生成仍然过长。

Conclusion: 推理模型的循环问题源于学习错误，温度调节只是临时解决方案。需要训练时干预来直接减少学习错误，才能从根本上解决循环问题。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [113] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的预测占用网格方法，用于实时预测复杂交通场景中多对象的演化，相比传统模型方法大幅降低计算负载。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的预测占用网格方法虽然能详细建模交通参与者行为不确定性，但计算负载过高，无法满足车辆安全应用的实时性要求。

Method: 1) 提出基于模型的预测占用网格方法；2) 引入增强单元的网格表示法表示当前交通状态；3) 采用随机森林算法进行机器学习映射到预测占用网格。

Result: 机器学习方法在交通场景模拟中表现良好，能够实现预测占用网格的实时计算，为车辆安全应用提供可行性。

Conclusion: 机器学习方法成功解决了模型方法计算负载过高的问题，实现了预测占用网格的实时计算，有望提升车辆安全系统中的关键性评估和轨迹规划等组件性能。

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [114] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 提出一种基于机器学习预测复杂交通场景概率时空表示的方法，使用层次分类器识别场景类型，通过降噪自编码器降维，随机森林预测未来行为，应用于安全轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 为主动车辆安全应用提供复杂交通场景的概率时空表示，特别是在执行动态机动时，需要准确预测交通参与者的未来行为以确保安全。

Method: 1. 使用层次情境分类器区分交通场景类型；2. 采用堆叠降噪自编码器对增强占据网格进行降维；3. 针对每类场景训练随机森林预测概率时空表示（预测占据网格）。

Result: 在仿真和真实车辆实验中表现出色，能够准确预测交通参与者的未来行为，并成功应用于评估交通场景关键性和确定安全轨迹。

Conclusion: 提出的机器学习方法（SDA+RF）能有效预测复杂交通场景的概率时空表示，为车辆安全应用提供了可靠的技术支持。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [115] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: NG-RC（下一代储备池计算）无需实际储备池，直接从时间序列输入计算多项式项，首次证明其在分类任务上与传统RC性能相当，验证了其在预测和分类任务中的通用计算能力。


<details>
  <summary>Details</summary>
Motivation: 现有NG-RC基准测试仅限于时间波形预测任务（如Lorenz 63吸引子和Mackey-Glass混沌信号），缺乏对分类任务的验证。需要证明NG-RC在分类任务上与传统储备池计算具有同等性能，以验证其通用计算能力。

Method: NG-RC不依赖实际储备池进行输入数据混合，而是直接从时间序列输入计算多项式项。该方法避免了传统RC中储备池的构建和维护，简化了计算过程。

Result: 首次证明NG-RC在分类任务上可以达到与传统RC相当的性能水平，验证了NG-RC在预测和分类任务中的通用计算能力。

Conclusion: NG-RC不仅适用于时间序列预测任务，也能有效处理分类任务，展现了其作为下一代储备池计算范式的通用性和实用性。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [116] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 提出一种用于复杂交通场景概率时空表示预测的新型机器学习架构，通过识别交通场景类型并映射当前状态到未来状态，输出包含不确定性的预测占用网格


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和主动安全系统需要详细的未来交通场景表示，准确的概率时空预测对于安全决策至关重要

Method: 使用两个堆叠去噪自编码器（SDA）和一组随机森林的架构，输入为增强占用网格（AOG），输出为预测占用网格（POG），并与现有SDA+DeconvNet架构进行比较

Result: 通过仿真验证了架构的有效性，在准确性和计算时间方面进行了比较，并展示了POG在主动安全领域的应用前景

Conclusion: 提出的新型机器学习架构能够有效预测交通场景的概率时空表示，为自动驾驶和主动安全系统提供重要支持

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [117] [LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization](https://arxiv.org/abs/2512.12922)
*Bangyu Li,Boping Gu,Ziyang Ding*

Main category: cs.LG

TL;DR: 提出基于大语言模型的个性化投资组合推荐框架，结合强化学习和风险偏好建模，实现智能投资决策


<details>
  <summary>Details</summary>
Motivation: 现代金融市场中，投资者需要个性化、适应性的投资策略，传统基于规则或静态优化方法无法捕捉投资者行为、市场波动和金融目标之间的非线性交互关系

Method: 集成大语言模型、强化学习和个体化风险偏好建模的框架，支持智能投资决策

Result: 未在摘要中明确说明具体实验结果

Conclusion: 提出的LLM-based Personalized Portfolio Recommender框架能够解决传统方法的局限性，为投资者提供更智能、个性化的投资决策支持

Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.

</details>


### [118] [Investigating Data Pruning for Pretraining Biological Foundation Models at Scale](https://arxiv.org/abs/2512.12932)
*Yifan Wu,Jiyue Jiang,Xichen Ye,Yiqi Wang,Chang Zhou,Yitao Xu,Jiayang Chen,He Hu,Weizhong Zhang,Cheng Jin,Jiao Yuan,Yu Li*

Main category: cs.LG

TL;DR: 提出一种基于影响力的后处理数据剪枝框架，用于生物基础模型预训练，能在99%剪枝率下超越随机选择，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 生物基础模型需要海量数据和参数，计算成本高昂，阻碍学术实验室的可复现性和可访问性，需要高效的数据剪枝方法

Method: 提出基于子集的自影响力公式，高效估计样本重要性，开发Top-k影响力和覆盖中心影响力两种选择策略，在RNA-FM和ESM-C模型上验证

Result: 在RNA任务中，99%剪枝率下优于随机选择；在蛋白质任务中，核心集性能优于10倍大的随机子集，揭示生物序列数据存在大量冗余

Conclusion: 影响力引导的数据剪枝能显著降低生物基础模型预训练的计算成本，为更高效、可访问和可持续的生物AI研究铺平道路

Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

</details>


### [119] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: GCNs在标签极度稀缺时提升最大，能利用图结构补偿有限监督；在高度同质性图上，即使节点特征被噪声替代，GCN仍能保持性能；但当同质性低且特征强时，GCN会损害性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GCN已成为半监督节点分类的标准方法，但实践者缺乏关于GCN何时比简单基线提供有意义改进的明确指导。本研究旨在理解GCN何时以及为何有效。

Method: 使用Amazon Computers共购数据进行诊断研究，通过模拟标签稀缺、特征消融和按类别分析的系统实验，研究图同质性与特征质量之间的相互作用。

Result: GCN性能关键取决于图同质性和特征质量的交互作用：在极端标签稀缺时提升最大；在高度同质性图上，即使特征被噪声替代也能保持性能；但在低同质性且特征强时会损害性能。象限分析显示GCN在四种条件中的三种有帮助。

Conclusion: GCN在大多数情况下有帮助，仅在低同质性且特征强的条件下会损害性能。这些发现为实践者决定是否采用基于图的方法提供了实用指导。

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [120] [Application of Deep Learning in Biological Data Compression](https://arxiv.org/abs/2512.12975)
*Chunyu Zou*

Main category: cs.LG

TL;DR: 使用隐式神经表示(INR)压缩冷冻电镜数据，通过提取二值图、GZIP压缩重复密度图，训练神经网络编码空间密度信息，实现高效压缩同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜(Cryo-EM)数据文件存储空间巨大，给研究人员和教育工作者带来显著挑战，需要开发高效的数据压缩解决方案。

Method: 1. 根据密度阈值提取每个文件的二值图；2. 使用GZIP压缩高度重复的密度图；3. 训练神经网络编码空间密度信息，存储网络参数和可学习的潜在向量；4. 引入位置编码增强空间表示；5. 使用加权均方误差损失函数平衡密度分布变化。

Result: 该方法旨在提供实用高效的生物数据压缩解决方案，在保持合理压缩比的同时，维持文件到文件的重建质量，适用于教育和研究目的。

Conclusion: 基于深度学习和隐式神经表示的压缩方法能够有效解决冷冻电镜大数据存储问题，为生物数据压缩提供了新的技术途径。

Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.

</details>


### [121] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: CoDeQ：一种完全可微的联合剪枝-量化方法，通过量化器死区参数化实现稀疏性，无需额外辅助过程，在保持精度的同时大幅减少比特操作。


<details>
  <summary>Details</summary>
Motivation: 现有的联合剪枝-量化方法依赖训练循环外的辅助过程来确定压缩参数，增加了工程复杂性和超参数调优，且缺乏直接的数据驱动梯度信号，可能导致次优压缩。

Method: CoDeQ基于量化器死区等价于幅度剪枝的观察，参数化死区宽度并通过反向传播学习，同时学习量化参数。该方法提供稀疏性显式控制，解耦稀疏性选择与比特宽度选择，支持固定精度和混合精度量化。

Result: 在ImageNet上使用ResNet-18，CoDeQ将比特操作减少到约5%，同时在固定精度和混合精度模式下保持接近全精度准确率。

Conclusion: CoDeQ提供了一种简单、完全可微的联合剪枝-量化方法，无需辅助过程，架构无关且易于实现，在保持准确性的同时实现了显著的压缩效果。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [122] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: 提出DIME深度学习框架，用于磁共振弹性成像中的剪切模量估计，相比传统MMDI方法具有更好的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法基于Helmholtz方程，假设均匀、无限介质，且对噪声敏感，导致刚度估计不准确不可靠。

Method: 使用有限元模拟生成的位移场-刚度图对训练深度学习框架DIME，采用小图像块训练以捕捉局部波行为并提高对全局图像变化的鲁棒性。

Result: 在合成数据上，DIME产生低像素间变异性、准确边界划分的刚度图，与真实值相关性更高；在真实肝脏MRE数据中，DIME保持生理一致的刚度模式，与MMDI结果相似但无方向性偏差。

Conclusion: DIME在磁共振弹性成像中表现出比传统MMDI方法更高的准确性和鲁棒性，具有临床应用潜力。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [123] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 提出优化Transformer训练框架，通过非对称投影分解反向传播梯度，保持前向QKV结构不变，在WikiText-2上验证损失降低0.56%


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的O(N²)复杂度仍是序列建模的实践前沿，但其训练存在几何效率问题。标准注意力梯度可能不是最优的，需要优化训练过程。

Method: 使用非对称投影将反向传播梯度分解为平行分量和正交违规分量，保持前向QKV结构不变。选择性缩放这些分量，主要关注0阶双向平行分量。

Result: 在WikiText-2数据集上，使用基本配置实现了验证损失降低0.56%，证明了框架的有效性，并暗示在更大数据集和更深训练中可能有显著收益。

Conclusion: 标准注意力梯度是次优的，提出的优化框架通过梯度分解和选择性缩放提供了更有效的学习信号，为Transformer训练优化提供了新方向。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [124] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: Alada是一种用于大规模矩阵随机优化的自适应动量方法，通过秩一分解估计梯度二阶矩，具有亚线性内存开销，性能与Adam相当但更节省内存。


<details>
  <summary>Details</summary>
Motivation: 传统自适应优化方法（如Adam）在处理大规模矩阵参数时内存开销大，需要更高效的内存优化方法。

Method: 采用秩一分解估计梯度二阶矩，通过交替更新因子最小化估计误差；同时配备一阶矩估计规则增强鲁棒性，不增加额外内存开销。

Result: 在多个自然语言处理任务上的数值研究表明，相比Adam及其变体，Alada能显著减少内存开销，并在训练大模型时表现出更好的鲁棒性。

Conclusion: Alada提供了一种内存高效的替代方案，在保持与Adam相当理论性能的同时，显著降低大规模矩阵优化的内存需求。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [125] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: FinFRE-RAG：一种两阶段方法，通过重要性引导的特征缩减将表格数据转换为自然语言，结合检索增强的上下文学习，显著提升LLM在欺诈检测中的性能，并提供可解释的预测理由。


<details>
  <summary>Details</summary>
Motivation: 传统表格模型在欺诈检测中需要大量特征工程且可解释性差，而LLM虽然能提供人类可读的解释，但直接应用于表格数据时性能不佳，难以处理高维特征、极端类别不平衡和缺乏上下文信息的问题。

Method: 提出FinFRE-RAG两阶段方法：1）重要性引导的特征缩减，将数值/分类属性序列化为紧凑的自然语言子集；2）检索增强的上下文学习，基于标签感知的实例级示例进行推理。

Result: 在四个公开欺诈数据集和三类开源LLM上，FinFRE-RAG显著优于直接提示方法，在多个设置中与强大的表格基线模型竞争性相当。虽然仍落后于专用分类器，但缩小了性能差距。

Conclusion: LLM在欺诈检测中虽然性能仍不及专用分类器，但FinFRE-RAG显著提升了其表现并提供了可解释的理由，凸显了其作为欺诈分析辅助工具的价值。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [126] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

TL;DR: 提出基于深度Q学习的智能调度优化框架，解决异构数据环境下ETL调度效率低、资源分配不均、适应性差的问题，显著降低调度延迟并提高系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 异构数据环境下ETL过程面临调度效率低、资源分配不平衡、适应性差等挑战，需要智能化的调度优化方案来提升数据管道性能。

Method: 将ETL调度过程形式化为马尔可夫决策过程，采用深度Q学习框架，包含状态表示模块、特征嵌入网络、Q值估计器和奖励评估机制，设计多目标奖励函数平衡调度延迟、任务完成率、吞吐量和资源利用率等指标。

Result: 实验结果表明，该框架显著降低了调度延迟，提高了系统吞吐量，在多源异构任务条件下增强了执行稳定性，验证了模型在超参数变化、环境动态性和数据规模变化下的鲁棒性。

Conclusion: 强化学习在复杂数据调度和资源管理中具有强大潜力，该框架为智能数据管道构建提供了高效可扩展的优化策略，能够有效应对异构数据环境的挑战。

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [127] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 提出一种多保真度深度学习框架，结合自编码器迁移学习和多分割保形预测策略，在数据稀缺条件下实现不确定性感知的空气动力学数据融合。


<details>
  <summary>Details</summary>
Motivation: 高保真度空气动力学模拟计算成本过高，限制了数据驱动建模的应用。需要开发多保真度策略，利用低成本低保真度信息而不牺牲精度。

Method: 采用自编码器迁移学习：利用丰富的低保真度数据学习紧凑的潜在物理表示作为冻结知识库，然后使用稀缺的高保真度样本微调解码器。同时开发多分割保形预测策略进行不确定性量化。

Result: 在NACA翼型（2D）和跨音速机翼（3D）表面压力分布数据库中，模型成功校正了低保真度偏差，使用极少高保真训练数据实现了高精度压力预测。MSCP框架产生稳健的、可操作的不确定性带，点覆盖超过95%。

Conclusion: 该工作通过结合极端数据效率和不确定性量化，为数据稀缺环境中的空气动力学回归提供了可扩展且可靠的解决方案。

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [128] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: LikeBench是一个新的LLM个性化评估框架，首次将"喜好度"分解为七个诊断性维度，用于衡量LLM在多轮对话中适应用户偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化基准主要关注准确回忆用户信息和在任务中应用信息，但忽略了主观的"喜好度"这一对用户体验至关重要的维度。需要更全面地衡量LLM如何随时间适应用户偏好以提供更受欢迎的回答。

Method: 引入LikeBench多会话动态评估框架：让LLM与模拟用户对话，仅从对话中学习偏好；将喜好度分解为七个诊断维度（情感适应、正式程度匹配、知识适应、引用理解、对话长度适应、幽默适应、回调）；使用细粒度、基于心理学的描述性人物角色而非粗糙的高/低特质评分。

Result: 强记忆性能不保证高喜好度：DeepSeek R1记忆准确率较低（86%，17个事实/档案）但在喜好度得分上比Qwen3高28%，尽管Qwen3记忆准确率更高（93%，43个事实/档案）。即使是SOTA模型如GPT-5在短交流中适应良好，但在更长、更嘈杂的交互中表现出有限的鲁棒性。

Conclusion: 喜好度是LLM个性化中一个独立且重要的维度，需要专门的评估方法。LikeBench提供了更全面、诊断性的评估框架，揭示了记忆性能与喜好度之间的差异，为未来LLM个性化研究提供了新方向。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [129] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 提出TraPO算法，通过少量标注样本指导无标注样本的强化学习训练，在数学推理任务上实现高效数据利用和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法需要大量标注成本，而无监督RLVR方法在训练后期容易出现模型崩溃。需要一种既能减少标注成本又能保证训练稳定性的方法。

Method: 提出半监督RLVR范式TraPO，利用少量标注样本指导无标注样本训练。通过匹配学习轨迹相似性识别可靠无标注样本，确保只有经过标注实例验证的推理模式被纳入强化学习。

Result: 在6个数学推理基准测试和3个分布外任务上表现优异。仅用1K标注+3K无标注样本达到42.6%平均准确率，超越使用45K无标注样本的无监督方法（38.3%）。使用4K标注+12K无标注样本时，在所有基准上超越使用全量45K标注样本的完全监督模型。

Conclusion: TraPO通过半监督RLVR方法实现了显著的数据效率和泛化能力，证明了少量标注样本对稳定一致性训练的重要性，为减少标注成本同时保持模型性能提供了有效解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [130] [From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network](https://arxiv.org/abs/2512.13111)
*Hayk Amirkhanian,Marco F. Huber*

Main category: cs.LG

TL;DR: 提出分层近似贝叶斯神经网络，使用高斯-逆Wishart分布作为网络权重的超先验，提高模型鲁棒性和性能，提供闭式解析解，在OOD任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络存在超参数调优和过拟合问题，贝叶斯神经网络通过引入不确定性来解决这些问题，特别是在处理分布外数据时需要更可靠的预测。

Method: 提出分层近似贝叶斯神经网络，使用高斯-逆Wishart分布作为网络权重的超先验，推导出预测分布和权重后验的解析表示，计算复杂度与权重数量呈线性关系。

Result: HABNN在实验中不仅匹配而且经常超越最先进模型，有效解决过拟合问题，提供可靠的uncertainty估计，在分布外任务中表现特别稳健。

Conclusion: 该方法为贝叶斯神经网络提供了一种高效的分层近似框架，在安全关键环境中具有广阔应用前景，是未来研究的有希望方向。

Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.

</details>


### [131] [Quanvolutional Neural Networks for Spectrum Peak-Finding](https://arxiv.org/abs/2512.13125)
*Lukas Bischof,Rudolf M. Füchslin,Kurt Stockinger,Pavel Sulimov*

Main category: cs.LG

TL;DR: 量子卷积神经网络在核磁共振谱峰分析中表现优于经典卷积神经网络，F1分数提升11%，峰位置估计误差降低30%


<details>
  <summary>Details</summary>
Motivation: 核磁共振等光谱的峰分析（去卷积）对专家和机器都是挑战，量子计算有潜力增强现有机器学习技术

Method: 受经典卷积神经网络启发，采用量子卷积神经网络处理多任务峰发现问题，包括峰计数和位置估计，使用可解释架构并与经典CNN对比

Result: 量子卷积神经网络在挑战性光谱上优于经典CNN，F1分数提高11%，峰位置估计的平均绝对误差降低30%，且在困难问题上表现出更好的收敛稳定性

Conclusion: 量子卷积神经网络在光谱分析中具有优势，为量子机器学习在科学数据分析中的应用提供了有前景的方向

Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.

</details>


### [132] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 该论文提出通过解相关节点特征来解决图域自适应中的条件偏移问题，使用解相关GCN层和图Transformer层实现，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 图机器学习方法近年来取得显著进展，但如何有效地将知识从一个图迁移到另一个图仍然是一个关键挑战。无监督图域自适应（GDA）面临的主要困难是条件偏移，这会阻碍知识的可迁移性。

Method: 论文通过理论分析发现条件偏移仅当节点特征存在局部依赖时才会出现。基于此，提出通过解相关节点特征来改进GDA，具体通过解相关GCN层和图Transformer层实现。

Result: 实验结果表明该方法有效，不仅显著优于基线GDA方法，而且在学习到的表示中显示出较小的类内距离，可视化效果清晰。

Conclusion: 通过解相关节点特征可以有效解决图域自适应中的条件偏移问题，提出的解相关GCN层和图Transformer层方法在理论和实验上都验证了其有效性。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [133] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 提出SACn算法，将Soft Actor-Critic与n步回报结合，通过数值稳定的重要性采样和τ采样熵估计解决偏差和方差问题。


<details>
  <summary>Details</summary>
Motivation: SAC是重要的离策略在线无模型强化学习方法，但难以与n步回报结合，因为通常的结合会因动作分布变化引入偏差，而重要性采样可能导致数值不稳定。

Method: 1) 提出数值稳定的重要性采样方法，简化超参数选择；2) 在n步最大熵框架下分析SAC的熵估计，提出τ采样熵估计降低学习目标方差；3) 最终形成SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证，表明SACn算法能够有效结合n步回报，提高收敛速度。

Conclusion: 成功将SAC与n步回报结合，解决了偏差和数值不稳定问题，提出了实用的SACn算法。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [134] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: PolySet是一个聚合物机器学习框架，将聚合物表示为有限加权链集合，而非单一分子图，以更好地捕捉真实聚合物的统计分布特性。


<details>
  <summary>Details</summary>
Motivation: 当前聚合物机器学习模型通常将聚合物视为单一、完美定义的分子图，而真实材料是由具有分布长度的链组成的随机集合。这种物理现实与数字表示之间的不匹配限制了现有模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假设的摩尔质量分布中采样的有限加权链集合。这种基于集合的编码独立于化学细节，兼容任何分子表示，并在均聚物案例中使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩（如Mz、Mz+1），使机器学习模型能够学习尾部敏感特性，具有显著改善的稳定性和准确性。

Conclusion: 通过明确承认聚合物物质的统计性质，PolySet为未来聚合物机器学习建立了物理基础，可自然扩展到共聚物、嵌段结构和其他复杂拓扑结构。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [135] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 提出WAY深度学习架构，用于基于AIS数据的船舶目的地预测，通过轨迹重构和梯度丢弃技术提升长时预测性能。


<details>
  <summary>Details</summary>
Motivation: AIS系统虽然支持数据驱动的海事监控，但存在可靠性问题和数据间隔不规则的问题，需要改进船舶目的地预测方法。

Method: 1) 将长港口到港口轨迹重构为嵌套序列结构；2) 提出WAY深度学习架构，包含轨迹表示层和CASP块；3) 引入梯度丢弃技术解决单标签多对多训练问题。

Result: 在5年AIS数据上的实验表明，WAY优于传统基于空间网格的方法，梯度丢弃技术带来性能提升，并能扩展到ETA估计等实际应用。

Conclusion: WAY架构通过创新的轨迹表示和训练技术，有效解决了AIS数据中的时空偏差问题，为长时船舶目的地预测提供了可靠解决方案。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [136] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 研究探讨了联邦学习中数据投毒攻击对表面温度预测的影响，发现即使少数恶意客户端也能显著扭曲大范围区域预测，而基于剪裁均值的防御对全局偏差攻击有效但对局部补丁攻击无效。


<details>
  <summary>Details</summary>
Motivation: 深度学习与联邦学习结合为下一代天气预报提供了强大工具，但联邦学习的分布式特性引入了新的安全漏洞。数据投毒攻击可能通过注入恶意训练数据来降低模型性能或引入系统性偏差，这种威胁在气象数据的空间依赖性下被放大，因为局部扰动可能通过全局模型聚合影响更广泛区域。

Method: 使用Copernicus欧洲区域再分析数据集，模拟地理分布的客户端，评估基于补丁和全局偏差攻击对区域温度预测的影响。采用剪裁均值聚合作为防御机制进行评估。

Result: 结果显示：1）单个恶意客户端的全局温度偏差攻击可使预测偏移高达-1.7K；2）协调的补丁攻击使均方误差增加三倍以上，产生超过+3.5K的持续区域异常；3）剪裁均值聚合能有效防御全局偏差攻击（性能仅下降2-13%），但对补丁攻击无效（性能反而恶化281-603%）。

Conclusion: 联邦学习在气象预测中面临严重的安全威胁，少量恶意客户端就能显著扭曲大范围预测。基于离群值检测的防御机制对空间相关数据的局部攻击效果有限，需要开发更强大的防御策略来保护联邦气象预测系统。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [137] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: ModSSC是一个统一的半监督分类Python框架，集成了多种算法、多模态数据支持，并提供声明式实验配置。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分类软件在方法和模态上分散，缺乏统一的框架来支持不同算法、数据集类型和计算环境。

Method: 开发开源Python框架ModSSC，实现归纳和直推式半监督分类算法，支持表格、图像、文本、音频和图形数据，提供YAML声明式实验配置接口。

Result: ModSSC 1.0.0已发布，支持从轻量级CPU方法到多GPU深度学习方法的统一实验框架，具有完整的文档和测试。

Conclusion: ModSSC提供了一个统一的半监督分类框架，简化了现有工作的复现和大规模比较研究，促进了该领域的研究发展。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [138] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 论文提出CORE框架，将对比学习整合到掩码特征重建中，在节点和图分类任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的图自监督学习领域，生成式和对比式方法是两种主流方法。研究发现掩码特征重建（MFR）和图对比学习（GCL）都旨在最大化相似元素之间的一致性，且在特定条件下目标函数会收敛，这表明两种方法是互补而非根本不同的，因此探索它们的整合以增强图自监督学习。

Method: 提出对比掩码特征重建（CORE）框架，将对比学习整合到MFR中：1）仅使用掩码节点的原始特征和重建特征形成正样本对，促使编码器优先考虑上下文信息而非节点自身特征；2）利用掩码节点本身作为负样本，结合MFR的重建能力和GCL的判别能力来更好捕捉内在图结构。

Result: CORE在节点和图分类任务上显著优于MFR，达到SOTA结果：在节点分类任务上超越GraphMAE和GraphMAE2分别达2.80%和3.72%；在图分类任务上超越分别达3.82%和3.76%。

Conclusion: CORE框架成功整合了生成式和对比式自监督学习方法，证明了两种方法的互补性，为图自监督学习提供了更有效的统一框架。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [139] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 标签弱化方法通过生成一组可能的标签而非单一标签，提升噪声数据下的检索模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 神经编码器在密集检索任务中面临训练数据稀疏标注和标签噪声的挑战，现有方法要么需要调参要么增加训练复杂性

Method: 采用标签弱化方法，基于观察到的监督信号和模型置信度分数，为每个查询-文档对生成一组合理的标签而非单一标签

Result: 在两个检索模型和一个重排序模型上，在四个不同排序数据集上的实验表明，标签弱化方法相比10种最先进的损失函数能提升检索性能

Conclusion: 标签弱化是处理训练数据中标签噪声的有效方法，能够生成更鲁棒的检索模型

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [140] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: BézierFlow是一种轻量级训练方法，通过将随机插值调度器参数化为Bézier函数，学习最优采样轨迹变换，在10步以内采样时实现2-3倍性能提升，仅需15分钟训练。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法主要学习最优时间步长，但仅限于ODE离散化。需要扩展搜索空间，从离散时间步扩展到采样轨迹变换，以提升少步生成性能。

Method: 提出将随机插值(SI)调度器参数化为Bézier函数，控制点自然满足边界条件、可微性和SNR单调性等关键要求。将问题简化为学习时间范围内的有序点集，从ODE时间步解释转变为Bézier控制点。

Result: 在多种预训练扩散和流模型上，BézierFlow在≤10步采样时性能提升2-3倍，训练仅需15分钟，始终优于先前的时间步学习方法。

Conclusion: 通过将搜索空间从离散时间步扩展到基于Bézier的轨迹变换，BézierFlow证明了扩展优化范围的有效性，为少步生成提供了高效解决方案。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [141] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: KAML框架通过属性驱动掩码策略、分层知识提取机制和排序损失，解决在线广告中多任务学习面临的不完整、偏斜多标签数据问题，显著提升转化率预测性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中，广告主通常有多样化的用户获取目标，但转化率预测常面临数据不完整问题。许多广告主因隐私等原因只提交部分用户转化行为，导致多任务数据的标签不完整。如果模型在所有可用样本上训练，当部署到针对特定转化行为的广告主时，训练和部署数据分布不匹配，影响性能。

Method: 提出KAML框架：1) 属性驱动掩码策略(ADM)更好地利用非对称多标签数据；2) 分层知识提取机制(HKE)建模目标任务塔内的样本差异；3) 结合排序损失策略充分利用未标记样本。

Result: 在离线行业数据集和在线A/B测试中进行了全面评估，相比现有MTL基线方法，KAML显示出显著的性能提升。

Conclusion: KAML框架有效解决了在线广告系统中多任务学习面临的不完整、偏斜多标签数据问题，通过创新的掩码策略、知识提取机制和损失函数设计，显著提升了转化率预测的准确性和实用性。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [142] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: ALIGN-FL 是一种新颖的分布式学习方法，通过选择性共享生成组件来解决高度不相关数据分布的学习挑战，使用隐私保护机制在非IID场景下保持效用。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中高度不相关数据分布（Non-IID）的挑战，特别是在跨机构协作中，需要隐私保护的同时保持模型性能。

Method: 1. 选择性共享生成组件而非完整模型参数；2. 使用DP-SGD自适应裁剪和Lipschitz正则化VAE解码器双重隐私机制；3. 服务器使用合成样本进行全局训练；4. 支持异构客户端的架构设计。

Result: 在MNIST和Fashion-MNIST数据集上验证，两种隐私机制都能有效将敏感异常值映射到典型数据点，在极端非IID场景下保持效用。

Conclusion: ALIGN-FL通过生成组件共享和互补隐私机制，成功解决了跨机构联邦学习中的隐私保护和数据分布不匹配问题。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [143] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: KD-PINN框架通过知识蒸馏将高容量教师模型的预测精度转移到紧凑学生模型，在保持物理精度的同时实现4.8-6.9倍推理加速，达到超低延迟实时计算


<details>
  <summary>Details</summary>
Motivation: 开发准确且超低延迟的神经PDE求解器，解决物理信息神经网络推理延迟问题，实现实时计算性能

Method: 提出知识蒸馏物理信息神经网络框架，通过连续调整Kullback-Leibler散度将高容量教师模型的预测能力转移到紧凑学生模型

Result: 学生模型保持教师物理精度（平均RMSE增加低于0.64%），推理速度提升4.8-6.9倍，平均推理延迟5.3ms，达到亚10ms超低延迟实时性能

Conclusion: KD-PINN框架有效实现准确超低延迟神经PDE求解，知识蒸馏在PINNs中具有正则化效果，为实时物理模拟提供可行方案

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [144] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: FROC是一个用于大语言模型机器遗忘的统一风险控制框架，通过概率约束评估遗忘不足和效用损失风险，指导超参数选择以平衡安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏有效的风险评估和控制机制，难以平衡遗忘充分性和效用保留，导致"被遗忘权"相关的信任问题。随着大语言模型广泛应用，管理这些风险变得日益重要。

Method: 提出FROC框架，采用符合性风险控制方法：1) 构建连续风险模型，聚合遗忘不足和效用退化；2) 计算符合性遗忘风险(CUR)，数据驱动评估遗忘样本对预测的影响概率；3) 生成风险控制配置集，识别满足指定风险预算的超参数。

Result: 实验表明FROC能产生稳定、可解释的风险景观，揭示遗忘配置、语义偏移和效用影响之间的一致关系，为大规模LLM部署中的遗忘行为管理提供实用基础。

Conclusion: FROC将机器遗忘重构为可控、风险感知的过程，通过统一的风险控制框架解决了当前MU技术缺乏风险评估机制的问题，为大语言模型中的遗忘行为管理提供了实用解决方案。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [145] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

TL;DR: 提出事件驱动的通信框架，在物联网网络中战略性地集成持续学习，用于节能的故障检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备上部署的轻量级机器学习模型会因环境非平稳性和初始训练数据有限而导致推理精度下降。虽然可以通过新数据更新模型，但这会消耗额外能量，对能量受限的物联网设备不利。

Method: 引入事件驱动的通信框架，使物联网设备和边缘服务器能够协作更新轻量级ML模型，同时适应无线链路条件和可用能量预算。

Result: 在真实数据集上的评估显示，该方法在推理召回率方面优于周期性采样和非自适应持续学习，在严格的能量和带宽约束下实现了高达42.8%的改进。

Conclusion: 提出的框架通过事件驱动的通信和自适应持续学习，在物联网网络中实现了节能高效的故障检测，解决了资源受限设备上的模型更新问题。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [146] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 该研究整合多种成员推断攻击技术到数据提取流程中，系统评估它们在现实数据提取场景中的有效性，并与传统基准测试结果对比。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆训练数据，带来严重的隐私风险，特别是训练数据提取和成员推断攻击。先前研究表明这两种威胁相互关联，但需要系统评估成员推断攻击在实际数据提取场景中的有效性。

Method: 将多种成员推断攻击技术整合到数据提取流程中，通过让模型生成大量文本，然后应用成员推断攻击来验证特定数据点是否在训练集中，系统性地对攻击效果进行基准测试。

Result: 研究比较了整合设置下的攻击性能与传统成员推断攻击基准测试结果，评估了这些攻击技术在现实数据提取场景中的实际效用。

Conclusion: 通过系统整合和基准测试，该研究为评估成员推断攻击在实际数据提取场景中的有效性提供了框架，有助于更好地理解大型语言模型的隐私风险。

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [147] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL是一种新颖的服务器端联邦遗忘方法，通过深度遗忘所有有影响力的权重来防止隐私泄露，相比现有方法在精度上提升1%-5%，时间成本降低高达12倍。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，导致时间长、成本高。现有服务器端知识蒸馏方法仅移除目标客户端更新，忽略了其他客户端贡献中的隐私信息，可能导致隐私泄露。

Method: DPUL包含三个组件：1) 通过过滤客户端更新幅度识别高权重参数并回滚以确保深度移除；2) 利用变分自编码器(VAE)重构和消除低权重参数；3) 使用基于投影的技术恢复模型。

Result: 在四个数据集上的实验结果表明，DPUL超越了最先进的基线方法，在精度上提供1%-5%的改进，时间成本降低高达12倍。

Conclusion: DPUL通过深度遗忘所有有影响力的权重，有效解决了现有联邦遗忘方法的隐私泄露问题，在保持模型性能的同时显著降低了计算成本。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [148] [Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks](https://arxiv.org/abs/2512.13410)
*Vítor M. Hanriot,Luiz C. B. Torres,Antônio P. Braga*

Main category: cs.LG

TL;DR: 本文提出基于Gabriel图的分类方法改进，包括更平滑的激活函数、结构支持向量中心神经元、神经网络扩展、新的图正则化成员函数和更高效的GG重计算算法，在统计上与树模型相当。


<details>
  <summary>Details</summary>
Motivation: 虽然大间隔分类器源于优化框架，但支持向量可以通过几何方法获得。本文旨在改进基于Gabriel图的分类器，解决现有方法在分类边界平滑性和计算效率方面的不足。

Method: 1) 提出更平滑的激活函数和结构支持向量中心神经元以获得低概率间隔和更平滑的分类边界；2) 扩展神经网络架构，可通过反向传播或线性方程组求解训练；3) 提出新的基于子图/距离的图正则化成员函数；4) 开发计算成本更低的Gabriel图重计算算法。

Result: Friedman测试显示，该方法优于之前的GG基分类器，且在统计上与树基模型相当。

Conclusion: 提出的基于Gabriel图的改进方法在保持几何直观性的同时，提高了分类性能和平滑性，计算效率更高，为无超参数、无优化的分类提供了有效方案。

Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

</details>


### [149] [XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders](https://arxiv.org/abs/2512.13442)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab是一种结合神经网络表达能力和可解释性的架构，通过稀疏自编码器将非线性特征分解为单语义特征并赋予人类可解释概念，在保持可解释性的同时达到与非可解释模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在依赖表格数据且需要可解释性的应用中，虽然神经网络具有更高的预测性能，但由于其黑盒性质而未被采用。现有可解释模型（如决策树、线性回归）性能有限，需要一种既能保持神经网络表达能力又能提供可解释性的解决方案。

Method: XNNTab首先学习高度非线性的特征表示，然后使用稀疏自编码器（SAE）将这些特征分解为单语义特征。这些特征被赋予人类可解释的概念，使整个模型的预测具有内在可解释性。

Result: XNNTab在性能上超越了现有的可解释预测模型，并且达到了与非可解释对应模型相当的性能水平。

Conclusion: XNNTab成功地将神经网络的表达能力与内在可解释性相结合，为需要可解释性的表格数据应用提供了有效的解决方案，解决了传统可解释模型性能不足和神经网络黑盒问题之间的矛盾。

Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.

</details>


### [150] [SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy](https://arxiv.org/abs/2512.13458)
*Yici Liu,Qi Wei Oung,Hoi Leong Lee*

Main category: cs.LG

TL;DR: 提出一种通过源选择与对抗策略的跨被试EEG情绪识别方法，解决个体差异和负迁移问题，在SEED和SEED-IV数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有跨被试EEG情绪识别研究大多忽视了个体间变异性和模型训练中的负迁移现象，这限制了实际应用效果。

Method: 提出SSAS方法，包含两个模块：源选择网络（SS）和对抗策略网络（AS）。SS通过反转域适应训练过程，破坏类可分性并放大域间差异，迫使模型学习域不变但情绪相关的表示；AS利用SS的源域选择结果和预训练域判别器，计算增强域分类性能的新损失函数，确保对抗策略的平衡。

Result: 在SEED和SEED-IV两个EEG情绪数据集上取得了出色的性能表现。

Conclusion: 提出的SSAS方法有效解决了跨被试EEG情绪识别中的个体差异和负迁移问题，通过源选择与对抗策略相结合的方式提升了模型性能。

Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.

</details>


### [151] [DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems](https://arxiv.org/abs/2512.13460)
*Chethana Prasad Kabgere,Shylaja S S*

Main category: cs.LG

TL;DR: DP-EMAR：一种差分隐私、基于错误模型的自主修复框架，用于在联邦物联网中检测和修复传输引起的模型权重失真，同时保持隐私保护


<details>
  <summary>Details</summary>
Motivation: 在资源受限的物联网网络中，联邦学习面临模型权重失真的挑战。多层级联邦物联网系统中，不稳定的连接和对抗性干扰会悄悄改变传输参数，降低收敛性能

Method: 提出DP-EMAR框架，通过估计损坏模式并在添加隐私噪声前应用自适应校正，将差分隐私与安全聚合结合，区分DP噪声与真实传输错误

Result: 在异构物联网传感器和图数据集上的实验表明，DP-EMAR在通信损坏下保持收敛稳定性，维持接近基线性能，同时确保严格的(ε,δ)-DP保证

Conclusion: 该框架增强了隐私保护联邦物联网学习的鲁棒性、通信效率和可信度，实现了可靠的网络内修复而不违反机密性

Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.

</details>


### [152] [Element-wise Modulation of Random Matrices for Efficient Neural Layers](https://arxiv.org/abs/2512.13480)
*Maksymilian Szorc*

Main category: cs.LG

TL;DR: 提出参数化随机投影层，通过固定随机矩阵加轻量可学习参数，大幅减少全连接层参数，实现高效压缩而不显著降低性能


<details>
  <summary>Details</summary>
Motivation: 全连接层是深度神经网络中内存和计算开销的主要来源，因其密集且往往冗余的参数化。现有压缩技术常引入复杂工程权衡或降低模型性能。

Method: 提出参数化随机投影层，将特征混合与适应解耦：使用固定随机矩阵，通过轻量级、可学习的逐元素参数进行调制，将可训练参数数量减少到线性规模。

Result: 在各种基准测试中保持可靠准确性的同时，大幅减少了可训练参数数量。该设计为架构扩展和资源受限环境中的部署提供了稳定、计算高效的解决方案。

Conclusion: PRP层通过创新的参数化方法，有效解决了全连接层的效率问题，为资源受限环境下的神经网络部署提供了实用解决方案。

Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.

</details>


### [153] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 提出一种基于轻量级特征提取器和增量核心集更新的设备端持续学习方法，用于工业视觉异常检测，在动态生产环境中实现快速适应、内存高效和准确检测。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中动态灵活的生产环境带来三大挑战：频繁产品变更需要快速模型更新；边缘硬件资源有限无法运行大型AI模型；异常和正常训练数据稀缺，特别是新产品变种。

Method: 扩展PatchCore方法，采用轻量级特征提取器和基于k-center选择的增量核心集更新机制，实现设备端持续学习，无需云端重训练。

Result: 在模拟灵活生产的工业用例测试中，相比基线方法AUROC提升12%，内存使用减少80%，训练速度比批量重训练更快。

Conclusion: 该方法为动态智能制造提供了准确、资源高效且自适应的视觉异常检测解决方案，适合边缘设备部署。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [154] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 提出"可重现性预算"概念量化学习系统在分布漂移下的统计可重现性，建立基于Fisher-Rao路径长度的泛化界，证明其极小极大最优性


<details>
  <summary>Details</summary>
Motivation: 分布漂移下的统计学习理论尚不完善，传统泛化界在数据生成分布随观测变化时可能失效，需要新的理论框架来量化系统在漂移和反馈下的统计可重现性

Method: 引入可重现性预算C_T作为统计原语，定义为耦合学习器-环境演化的累积Fisher-Rao路径长度，从该构造推导出O(T^{-1/2} + C_T/T)阶的漂移-反馈泛化界

Result: 证明了匹配的极小极大下界，表明该速率是极小极大最优的，建立了可重现性速度限制：任何算法的最坏情况泛化误差不能低于数据生成过程的平均Fisher-Rao漂移率C_T/T

Conclusion: 该框架将外生漂移、自适应数据分析和执行性预测统一在共同的几何结构中，C_T成为衡量这些设置中分布运动的内在量，为分布漂移下的统计学习提供了理论基础

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [155] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 研究LLM软件工程代理的异步监控方法，通过红蓝对抗游戏开发监控系统，在5个软件工程环境中实现6%漏报率（1%误报率），并评估部署时的破坏风险


<details>
  <summary>Details</summary>
Motivation: LLM软件工程代理在实际开发任务中可能访问敏感数据或安全关键代码库，如果代理未对齐可能故意破坏代码库。需要监控系统来防止这种破坏，但同步监控会带来运行时延迟，因此研究异步监控方法

Method: 将监控开发视为红蓝对抗游戏：蓝队设计监控器，红队创建破坏性代理。在5个多样化的软件工程环境中模拟AI开发者内部基础设施任务。开发集成监控器，并在保留的测试环境中评估性能

Result: 开发的集成监控器在1%误报率下达到6%漏报率。通过从监控器的漏报率外推来估计部署时的破坏风险，提出了简单的外推模型并进行敏感性分析

Conclusion: 异步监控可以在不引入运行时延迟的情况下有效检测LLM代理的破坏行为，通过对抗性游戏开发的监控器在多样化环境中表现良好，但需要谨慎外推风险估计

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [156] [Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability](https://arxiv.org/abs/2512.13568)
*Leonard Bereska,Zoe Tzifa-Kratira,Reza Samavi,Efstratios Gavves*

Main category: cs.LG

TL;DR: 提出信息论框架量化神经网络中的叠加现象，通过稀疏自编码器激活的香农熵计算有效特征数，揭示网络通过叠加模拟"虚拟神经元"的能力，并发现对抗训练可增加有效特征同时提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加（多个特征作为激活空间中的重叠方向）实现卓越性能，这挑战了可解释性，但目前缺乏量化叠加现象的原则性方法。

Method: 提出信息论框架，将香农熵应用于稀疏自编码器激活，计算有效特征数作为无干扰编码所需的最小神经元数，等价于网络通过叠加模拟的"虚拟神经元"数量。

Result: 度量方法在玩具模型中与真实情况强相关，在算法任务中检测到最小叠加，在dropout下显示系统性减少，层间模式与Pythia-70M内在维度研究一致，在grokking过程中检测到急剧特征整合，对抗训练可增加有效特征同时提升鲁棒性。

Conclusion: 通过将叠加定义为有损压缩，该工作实现了对神经网络在计算约束下如何组织信息的量化测量，连接了叠加现象与对抗鲁棒性，揭示了任务复杂性和网络容量如何决定特征扩展（丰富机制）或减少（稀缺机制）的不同机制。

Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.

</details>


### [157] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: 提出DP-CSGP算法，用于有向图上的去中心化学习，在保证差分隐私的同时实现高效压缩通信，模型效用与精确通信的去中心化算法相当但通信成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习算法要么缺乏隐私保护，要么通信效率低下。本文旨在设计一种同时保证严格差分隐私、高效通信和高模型效用的算法。

Method: 提出差分隐私随机梯度推送压缩通信算法(DP-CSGP)，结合差分隐私机制、梯度压缩技术和有向图上的推送通信协议，在保护隐私的同时减少通信开销。

Result: 对于一般非凸平滑目标函数，算法达到紧致的效用界O(√(d log(1/δ))/(√nJε))，匹配精确通信的去中心化算法。实验表明在相同隐私预算下，DP-CSGP获得可比模型精度但通信成本显著降低。

Conclusion: DP-CSGP成功解决了去中心化学习中隐私保护、通信效率和模型效用之间的平衡问题，为实际部署提供了有前景的解决方案。

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [158] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 提出Diffusion Preview范式，通过快速低步数采样生成预览供用户评估，满意后再进行全步数精炼。为解决现有方法预览质量低和一致性差的问题，提出基于强化学习优化的ConsistencySolver高阶求解器。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型推理过程缓慢，严重影响用户交互体验。现有加速方法（如免训练求解器和后训练蒸馏）难以在低步数下生成高质量预览，且无法保证预览与最终输出的一致性。

Method: 提出ConsistencySolver，基于通用线性多步方法设计轻量级可训练高阶求解器，通过强化学习优化，提升预览质量和一致性。采用预览-精炼工作流，先快速生成预览供用户评估，满意后再进行全步数精炼。

Result: ConsistencySolver在低步数场景下显著提升生成质量和一致性，比Multistep DPM-Solver减少47%步数达到相同FID分数，优于蒸馏基线方法。用户研究表明该方法减少近50%用户交互时间，同时保持生成质量。

Conclusion: Diffusion Preview范式结合ConsistencySolver为图像扩散模型提供了高效的交互式工作流，显著加速推理过程，改善用户体验，在预览质量和一致性方面优于现有方法。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [159] [Scalable Formal Verification via Autoencoder Latent Space Abstraction](https://arxiv.org/abs/2512.13593)
*Robert Reed,Morteza Lahijanian,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出一种基于凸自编码器和核方法的降维技术，构建有限抽象模型并保证包含原始系统行为，实现可扩展的形式验证


<details>
  <summary>Details</summary>
Motivation: 有限抽象方法面临高维系统的可扩展性挑战，基于学习的方法虽能降维但难以保证验证结果的正确性

Method: 使用凸自编码器降维，通过核方法学习潜在空间动态，构建包含原始系统行为的有限抽象模型

Result: 在多个系统（包括26维神经网络控制系统）上验证有效，显著提升可扩展性且不损失严谨性

Conclusion: 提出的方法能够有效解决高维系统形式验证的可扩展性问题，同时保证验证结果的正确性

Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.

</details>


### [160] [LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification](https://arxiv.org/abs/2512.13617)
*Ankit Sharma,Sayan Roy Gupta*

Main category: cs.LG

TL;DR: LightTopoGAT：一种轻量级图注意力网络，通过拓扑增强（节点度和局部聚类系数）改进图表示学习，在保持参数效率的同时提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络计算资源需求大，且难以有效捕捉全局图属性，需要一种既能保持轻量级又能更好整合结构信息的方法。

Method: 提出LightTopoGAT，通过拓扑增强整合节点度和局部聚类系数来增强节点特征，采用简化的注意力机制保持参数效率，克服局部消息传递方案的结构信息缺失问题。

Result: 在MUTAG、ENZYMES和PROTEINS三个基准数据集上优于GCN、GraphSAGE和标准GAT，MUTAG准确率提升6.6%，PROTEINS提升2.2%。消融实验证实性能提升直接源于拓扑特征的引入。

Conclusion: 拓扑增强是一种简单有效的策略，能在不增加架构复杂度的前提下显著提升图神经网络性能，为轻量级图表示学习提供了新思路。

Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.

</details>


### [161] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: StutterFuse：首个用于多标签口吃检测的检索增强分类器，通过结合临床示例记忆库解决重叠性言语不流畅的识别问题，并在SEP-28k数据集上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以区分复杂重叠性言语不流畅（如"阻塞"与"延长"同时出现），因为训练数据中这些特定组合稀缺。虽然检索增强生成（RAG）在NLP领域取得突破，但在病理语音处理中尚未探索。

Method: 提出StutterFuse，首个用于多标签口吃检测的检索增强分类器。方法包括：1）基于临床示例的非参数记忆库；2）SetCon（Jaccard加权度量学习目标）解决"模态崩溃"问题；3）门控专家混合融合策略动态协调声学证据与检索上下文。

Result: 在SEP-28k数据集上获得0.65的加权F1分数，优于强基线模型，并展现出显著的零样本跨语言泛化能力。

Conclusion: StutterFuse通过检索增强范式成功解决了重叠性言语不流畅的检测难题，为病理语音处理开辟了新方向，展示了通过参考而非记忆进行分类的有效性。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [162] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: 该研究提出了一种评估卷积神经网络在芒果叶病害诊断中鲁棒性的方法，通过创建带人工损坏的数据集MangoLeafDB-C，比较了五种CNN架构在19种损坏类型下的表现，发现轻量级专用模型LCNN在现实损坏场景中优于复杂模型。


<details>
  <summary>Details</summary>
Motivation: 尽管芒果具有全球重要性，但缺乏对其叶部病害诊断模型鲁棒性的研究。AI模型需要通过鲁棒性评估来保证其在面对现实世界挑战（如图像损坏）时的可靠性能，特别是在农业应用中。

Method: 创建了MangoLeafDB-C数据集，包含19种人工损坏类型和五个严重级别。对五种CNN架构（ResNet-50、ResNet-101、VGG-16、Xception和专门设计的轻量级架构LCNN）进行基准测试，使用F1分数、损坏错误率（CE）和相对平均损坏错误率（relative mCE）作为评估指标。

Result: LCNN在现实场景可能出现的损坏（如散焦模糊、运动模糊）中表现优于复杂模型，并获得了最低的mCE。现代架构（如ResNet-101）虽然在理想条件下准确率高，但在损坏场景中性能显著下降。

Conclusion: 轻量级和专用模型可能更适合边缘设备的现实世界应用，其中鲁棒性和效率至关重要。研究强调了在农业智能系统开发中纳入鲁棒性评估的必要性，特别是在技术受限地区。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [163] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: QFANG是一个科学推理语言模型，能够从化学反应方程式直接生成精确的结构化实验程序，通过化学引导推理框架和强化学习提高程序准确性。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划的关键挑战是弥合计算路线设计与实际实验室执行之间的差距，特别是准确预测每个合成步骤的可行实验程序。

Method: 1) 构建包含905,990个化学反应与结构化动作序列的高质量数据集；2) 引入化学引导推理框架生成基于化学知识的链式思维数据；3) 进行监督微调以激发复杂化学推理；4) 应用可验证奖励强化学习进一步优化程序准确性。

Result: QFANG在传统NLP相似性指标和化学感知评估器上都优于先进的通用推理模型和最近邻检索基线，能够泛化到某些域外反应类别，并适应实验室条件和用户特定约束的变化。

Conclusion: QFANG生成高质量合成程序的能力代表了弥合计算合成规划与全自动实验室合成之间差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [164] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: DTI通过固定嵌入向量的模长并仅优化方向来解决TI中的嵌入模长膨胀问题，提升复杂提示下的文本保真度，同时保持主体相似性。


<details>
  <summary>Details</summary>
Motivation: 传统Textual Inversion (TI)在复杂提示下表现不佳，主要原因是学习到的token嵌入模长膨胀，偏离预训练分布，导致在pre-norm Transformer中提示条件化效果下降。

Method: 提出Directional Textual Inversion (DTI)：1) 固定嵌入向量的模长为预训练分布内的尺度；2) 仅优化嵌入方向，在单位超球面上使用黎曼SGD；3) 将方向学习建模为具有von Mises-Fisher先验的MAP估计，获得简单高效的恒定方向先验梯度。

Result: DTI在个性化任务中相比TI及其变体显著提升了文本保真度，同时保持了主体相似性。超球面参数化还实现了学习概念间的平滑语义插值(slerp)，这是标准TI不具备的能力。

Conclusion: 仅优化方向的策略是提升提示忠实个性化效果的鲁棒且可扩展的路径，方向学习比模长调整对语义编码更为关键。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [165] [FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing](https://arxiv.org/abs/2512.11826)
*Weihong Xu,Chang Eun Song,Haichao Yang,Leo Liu,Meng-Fan Chang,Carlos H. Diaz,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: FSL-HDnn是一个能效优化的加速器，通过权重聚类特征提取器和超维度计算分类器实现端到端少样本学习，在40nm工艺下达到6mJ/图像的训练能效。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上设备端学习（ODL）的基本挑战，包括计算复杂度高、延迟大和能效低的问题，特别是在少样本学习场景下。

Method: 采用两个协同模块：1）基于权重聚类的参数高效特征提取器降低计算复杂度；2）基于超维度计算（HDC）的FSL分类器，消除梯度反向传播，实现单次训练。还提出两种优化策略：带分支特征提取的提前退出机制和批量单次训练。

Result: 在40nm CMOS工艺下，芯片在10-way 5-shot FSL任务上达到6mJ/图像的训练能效和28图像/秒的端到端训练吞吐量。端到端训练延迟比最先进的ODL芯片降低2倍到20.9倍。

Conclusion: FSL-HDnn通过创新的权重聚类特征提取和HDC分类器设计，成功实现了高效能、低延迟的边缘设备端少样本学习，为资源受限的边缘应用提供了可行的解决方案。

Abstract: This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.

</details>


### [166] [DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM](https://arxiv.org/abs/2512.12106)
*Victor Cai,Jennifer Zhou,Haebin Do,David Brooks,Gu-Yeon Wei*

Main category: cs.AR

TL;DR: DreamRAM是一个可配置的3D堆叠DRAM建模工具，用于探索定制化内存架构设计空间，支持带宽、容量、能耗、延迟和面积优化。


<details>
  <summary>Details</summary>
Motivation: 不同应用对DRAM的功耗、性能和面积需求各异，而固定商品化DRAM设计无法满足这些多样化需求。3D堆叠技术创造了通过3D集成和扩展总芯片面积来实现大规模DRAM设计空间的机会。

Method: DreamRAM提供细粒度设计定制参数（MAT、子阵列、bank和bank间级别），包括扩展文献中的部分页面和子阵列并行性方案。工具通过分析建模线宽、间距、长度、电容和缩放参数来捕捉物理布局和布线设计选择的性能权衡。支持自定义MAT级布线方案DLOMAT以优化带宽权衡。

Result: DreamRAM已针对行业HBM3和HBM2E设计进行校准和验证。在设计空间中，识别出相比基线设计分别实现66%更高带宽、100%更高容量以及45%更低每比特功耗和能耗的设计（分别在等带宽、等容量和等功耗基础上）。

Conclusion: DreamRAM为定制3D堆叠DRAM设计提供了一个全面的建模工具，能够探索大规模设计空间，满足应用特定需求，实现显著的性能、容量和能效改进。

Abstract: 3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.

</details>


### [167] [HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility](https://arxiv.org/abs/2512.12847)
*Jonathan Herbst,Michael Pellauer,Sherief Reda*

Main category: cs.AR

TL;DR: 提出一种高吞吐量神经网络加速器，通过硬件嵌入网络层和Po2量化，将卷积简化为加法，实现20-67倍吞吐提升


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算和数据中心中神经网络推理的高吞吐量、低能耗需求，同时保持一定的灵活性以适应部署后的微调

Method: 采用硬件直接嵌入网络层设计，使用Po2量化将乘法转换为简单重连，保留可编程的最终分类层进行微调

Result: 在7nm ASIC上实现，相比GPU提升20倍吞吐量（121万图像/秒），无微调需求时可达67倍（400万图像/秒）

Conclusion: 该硬件加速器设计在保持微调灵活性的同时，显著提升了神经网络推理的吞吐量和能效，适用于边缘和云端部署

Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.

</details>


### [168] [KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation](https://arxiv.org/abs/2512.12850)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: KANELÉ框架首次系统化地将KAN网络部署到FPGA上，通过量化剪枝协同优化，实现2700倍加速和显著资源节省，在符号/物理公式任务上超越现有LUT架构。


<details>
  <summary>Details</summary>
Motivation: FPGA上需要低延迟、资源高效的神经网络推理以满足实时性和低功耗需求。传统LUT网络虽有效但仍有优化空间，而KAN网络的独特结构（可学习一维样条激活）天然适合FPGA的离散化和LUT映射。

Method: 提出KANELÉ框架，首次系统化设计KAN在FPGA上的实现流程，将训练与量化、剪枝协同优化，实现紧凑、高吞吐、低延迟的KAN架构，并扩展至实时功率高效控制系统。

Result: 相比现有KAN-on-FPGA方法实现高达2700倍加速和数量级资源节省；在广泛基准测试中匹配或超越其他LUT架构，尤其在符号/物理公式任务表现突出；能平衡FPGA硬件资源使用。

Conclusion: KANELÉ成功将KAN网络优势与FPGA硬件特性结合，为实时低功耗应用提供高效解决方案，展示了框架在控制等领域的扩展潜力。

Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.

</details>


### [169] [SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference](https://arxiv.org/abs/2512.12990)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Gwangtae Park,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.AR

TL;DR: SliceMoE：一种面向错失率约束部署的节能MoE推理框架，通过动态位切片缓存和预测性缓存预热，在保持精度的同时显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件计算实现高效扩展，但其庞大的参数量和昂贵的专家卸载使得在设备上部署具有挑战性。现有的加速技术如预取或专家聚类往往会增加能耗或降低专家多样性。

Method: 1. 动态位切片缓存（DBSC）：以切片级粒度缓存专家，按需分配精度以扩展有效专家容量；2. 无需校准的非对称套娃量化（AMAT）：基于截断的方案，保持低比特和高比特切片之间的兼容性；3. 预测性缓存预热（PCW）：在预填充阶段重塑缓存内容以减少早期解码冷缺失。

Result: 在DeepSeek-V2-Lite和Qwen1.5-MoE-A2.7B上的评估显示，SliceMoE分别将解码阶段能耗降低2.37倍和2.85倍，解码延迟提升1.81倍和1.64倍，同时保持接近高比特精度。

Conclusion: 切片级缓存实现了高效的设备端MoE部署，为资源受限环境中的大型MoE模型部署提供了可行的解决方案。

Abstract: MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.

</details>


### [170] [An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering](https://arxiv.org/abs/2512.13133)
*Shuo Liu*

Main category: cs.AR

TL;DR: 提出基于最优对齐的迭代闭环收敛框架，解决VLSI布局模式聚类中的计算复杂度、对齐模糊和速度-质量权衡问题，在EDA竞赛中取得第一名


<details>
  <summary>Details</summary>
Motivation: 随着VLSI技术缩放，布局模式爆炸式增长成为DFM应用（如OPC）的关键瓶颈。现有聚类方法存在计算复杂度高（O(N²)）、离散采样导致对齐不优、速度与质量难以权衡等问题

Method: 1) 提出混合高性能算法解决对齐模糊：基于FFT的相位相关方法处理余弦相似度约束，鲁棒几何最小-最大策略处理边缘位移约束；2) 将聚类建模为集合覆盖问题，使用基于惊喜的懒惰贪心启发式算法，在粗到细迭代细化循环中确保收敛；3) 多阶段剪枝机制过滤99%冗余计算

Result: 在2025年中国研究生EDA精英挑战赛基准测试中：实现93.4%压缩率（相对于原始输入），相比官方基线加速100倍以上，能在数秒内处理数万个模式，在77支队伍中获得第一名

Conclusion: 该方法在解决NP难的布局聚类问题上表现出优越性，实现了可扩展性和精度的最优平衡，证明了框架的有效性和实用性

Abstract: With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.

</details>


### [171] [Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs](https://arxiv.org/abs/2512.13282)
*Endri Taka,Andre Roesti,Joseph Melber,Pranathi Vasireddy,Kristof Denolf,Diana Marculescu*

Main category: cs.AR

TL;DR: 本文提出了一种系统化方法来优化AMD Ryzen AI NPU（XDNA和XDNA2）上的GEMM计算，针对深度学习工作负载实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载对计算和内存需求极高，需要针对从云到边缘的专用硬件（如AMD Ryzen AI XDNA NPU）进行优化。优化GEMM算法对于提升深度学习工作负载性能至关重要。

Method: 提出了一种通用的系统化方法来优化GEMM工作负载，针对XDNA和XDNA2两代NPU架构。实现利用了AMD NPU的独特架构特性，并解决了系统级的关键性能瓶颈。

Result: 在各种GEMM尺寸上实现了最先进的吞吐量：对于8位整数精度，XDNA达到6.76 TOPS，XDNA2达到38.05 TOPS；对于brain浮点精度，XDNA达到3.14 TOPS，XDNA2达到14.71 TOPS。

Conclusion: 这项工作为在Ryzen AI NPU上优化GEMM工作负载的关键性能方面提供了重要见解，展示了针对专用硬件架构进行系统化优化的有效性。

Abstract: The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.

</details>


### [172] [Reproducibility and Standardization in gem5 Resources v25.0](https://arxiv.org/abs/2512.13479)
*Kunal Pai,Harshil Patel,Erin Le,Noah Krim,Mahyar Samani,Bobby R. Bruce,Jason Lowe-Power*

Main category: cs.AR

TL;DR: 改进gem5仿真器和gem5资源库，通过标准化磁盘镜像创建、重构退出事件系统、引入并行仿真功能，解决计算机架构研究中可重复性的挑战


<details>
  <summary>Details</summary>
Motivation: 仿真计算机架构研究中的可重复性需要协调磁盘镜像、内核和基准测试等工件，但现有工作流程不一致。gem5资源库虽然支持工件共享，但研究人员仍面临创建自定义磁盘镜像复杂、主机-访客通信受限、多工作负载仿真需要外部脚本等问题

Method: 1) 使用Packer标准化x86、ARM和RISC-V的磁盘镜像创建；2) 重构退出事件系统为基于类的模型，引入hypercalls增强主机-访客通信；3) 实现Suites和MultiSim支持从gem5配置脚本进行并行全系统仿真；4) 提供远程监控工具和gem5-bridge驱动

Result: 提供了12个新磁盘镜像、6个新内核和超过200个跨三种ISA的工作负载；实现了可扩展的验证资源；消除了对外部脚本的需求；降低了设置复杂性

Conclusion: 通过标准化磁盘镜像创建、增强通信功能和引入并行仿真能力，显著提高了gem5仿真器在计算机架构研究中的可重复性和标准化水平

Abstract: Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.

</details>


### [173] [Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing](https://arxiv.org/abs/2512.13686)
*Juncheng Huo,Yunfan Gao,Xinxin Liu,Sa Wang,Yungang Bao,Xitong Gao,Kan Shi*

Main category: cs.AR

TL;DR: Lyra是一个异构RISC-V验证框架，结合硬件加速和ISA感知生成模型，显著提升验证覆盖率和速度。


<details>
  <summary>Details</summary>
Motivation: 处理器设计日益复杂，但验证仍受限于缓慢的软件仿真和低质量的随机测试激励。现有软件模糊测试方法依赖语义盲随机突变，难以生成高质量激励来探索复杂行为，导致覆盖率收敛缓慢和验证成本过高。

Method: Lyra采用异构验证框架：1) 在FPGA SoC上并行执行被测设计和参考模型，实现高吞吐量差分检查和硬件级覆盖率收集；2) 训练领域专用生成模型LyraGen，该模型具有内在语义感知能力，能生成高质量、语义丰富的指令序列。

Result: Lyra相比最先进的软件模糊测试器，覆盖率提升最高达1.27倍，端到端验证加速107倍到3343倍，并持续表现出更低的收敛难度。

Conclusion: Lyra通过结合硬件加速验证和语义感知生成模型，有效解决了传统验证方法的局限性，显著提升了验证效率和覆盖率。

Abstract: As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\times$ higher coverage and accelerates end-to-end verification by up to $107\times$ to $3343\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [174] [Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs](https://arxiv.org/abs/2512.12036)
*Shiju Li,Younghoon Min,Hane Yie,Hoshik Kim,Soohong Ahn,Joonseop Sim,Chul-Ho Lee,Jongryool Kim*

Main category: cs.DC

TL;DR: 本文提出了一种基于哈希的多阶段SpGEMM GPU实现及间接内存访问加速(AIA)技术，通过硬件-软件协同设计显著提升了稀疏矩阵乘法的性能，特别是在图分析和GNN训练应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 稀疏通用矩阵乘法(SpGEMM)是科学计算和数据分析中的基础操作，但常受限于不规则的内存访问模式。现有GPU实现难以高效处理复杂的应用特定工作负载，需要新的优化方法。

Method: 提出了基于哈希的多阶段SpGEMM GPU实现和间接内存访问加速(AIA)技术，这是一种新颖的近内存处理定制方法。采用硬件-软件协同设计框架，专门优化GPU HBM上的SpGEMM操作。

Result: 在图分析应用中，AIA相比纯软件实现减少17.3%的时间；相比cuSPARSE，图收缩减少76.5%时间，马尔可夫聚类减少58.4%时间。在GNN训练中，混合方法相比纯软件实现平均加速1.43倍，相比cuSPARSE加速1.95倍，大规模数据集上最高加速4.18倍。

Conclusion: 提出的硬件-软件协同设计框架显著提升了SpGEMM性能，特别是在处理复杂应用特定工作负载时表现突出。该方法在图分析和GNN训练等实际应用中具有重要价值，为不规则内存访问模式的优化提供了有效解决方案。

Abstract: Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.

</details>


### [175] [Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates](https://arxiv.org/abs/2512.12295)
*Wenjun Yu,Sitian Chen,Cheng Chen,Amelie Chi Zhou*

Main category: cs.DC

TL;DR: LiveUpdate通过在推理节点内协同部署低秩适配训练器，消除跨集群同步开销，实现深度学习推荐模型的在线更新，在保证低延迟的同时提升推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的深度学习推荐模型面临新鲜度与准确率的权衡问题，由于PB级嵌入表同步导致多分钟延迟，降低推荐质量和收入。同时观察到推理节点CPU利用率低（峰值≤20%）且嵌入表梯度具有内在低秩结构。

Method: 提出LiveUpdate系统，将低秩适配训练器协同部署在推理节点中，消除跨集群同步。采用动态秩适应通过奇异值监控控制内存开销（<2%嵌入表大小），以及NUMA感知资源调度与硬件强制QoS消除更新对推理的干扰。

Result: LiveUpdate将更新成本降低2倍（相比增量更新基线），在1小时窗口内实现更高准确率。P99延迟影响小于20ms，内存开销控制在嵌入表的2%以内。在准确率上比最先进的增量更新方法提升0.04%到0.24%。

Conclusion: 通过将空闲推理资源转化为新鲜度引擎，LiveUpdate能够实现在线模型更新，在减少同步开销的同时提升推荐准确率，解决了生产环境中深度学习推荐模型的新鲜度-准确率权衡问题。

Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.

</details>


### [176] [Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P](https://arxiv.org/abs/2512.12801)
*Anurag Dutt,Young Won Choi,Avirup Sil,Anshul Gandhi,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.DC

TL;DR: PIE-P：一个用于多GPU并行推理的细粒度能耗预测框架，解决现有方法仅限于单GPU环境的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，能耗成本成为关键问题。现有能耗测量方法存在局限性：硬件监控器不易获取，软件工具不准确，而现有预测技术仅限于单GPU环境，无法应用于现代多GPU并行推理场景。

Method: 开发了PIE-P框架，通过精确采样、细粒度建模GPU间通信、仔细考虑并行化开销来解决多GPU环境中的非确定性通信、额外通信开销和通信/同步阶段能耗隔离困难等问题。

Result: 评估结果显示PIE-P在各种并行策略（张量、流水线、数据并行）下都能提供准确且细粒度的能耗预测，显著优于基线方法。

Conclusion: PIE-P填补了多GPU并行推理能耗预测的空白，为解决大语言模型能耗监控和优化提供了有效的工具。

Abstract: With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.

</details>


### [177] [A Conflict-Aware Resource Management Framework for the Computing Continuum](https://arxiv.org/abs/2512.12299)
*Vlad Popescu-Vifor,Ilir Murturi,Praveen Kumar Donta,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出基于深度强化学习的自适应冲突解决框架，用于计算连续体中的资源编排，解决设备异构和去中心化带来的资源冲突问题。


<details>
  <summary>Details</summary>
Motivation: 计算连续体（边缘、雾、云）中设备异构性和去中心化需求增加，导致资源编排面临新挑战。智能体决策可能导致持续冲突循环、资源利用效率低下和服务性能下降。

Method: 提出基于深度强化学习（DRL）的自适应冲突解决框架，整合DRL模型，根据实时性能反馈和历史状态信息调解资源冲突，并在Kubernetes测试平台上进行原型实现和验证。

Result: 框架在动态场景中实现了高效的资源重新分配和自适应学习，展示了方法可行性和架构弹性，为计算连续体中的冲突感知编排提供了可扩展且弹性的解决方案。

Conclusion: 该DRL框架能够有效解决计算连续体中的资源冲突问题，通过自适应学习和实时调解机制，提高了资源编排的效率和弹性。

Abstract: The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.

</details>


### [178] [astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging](https://arxiv.org/abs/2512.13591)
*Denisa-Andreea Constantinescu,Rubén Rodríguez Álvarez,Jacques Morin,Etienne Orliac,Mickaël Dardaillon,Sunrise Wang,Hugo Miomandre,Miguel Peón-Quirós,Jean-François Nezan,David Atienza*

Main category: cs.DC

TL;DR: astroCAMP框架为SKA射电望远镜项目提供硬件-软件协同设计框架，解决当前成像管线性能低效（仅4-14%硬件峰值）、能耗高的问题，通过统一度量标准、标准化数据集和多目标优化来提升科学产出效率。


<details>
  <summary>Details</summary>
Motivation: SKA项目面临巨大挑战：当前射电干涉成像管线硬件利用率极低（4-14%），存在内存和I/O瓶颈，导致能源效率低下、运营成本和碳排放高。同时缺乏标准化度量标准和保真度容差，阻碍了硬件-软件协同设计和质量-效率权衡的系统探索。

Method: 提出astroCAMP框架，包含三个核心组件：1）统一可扩展的度量套件，涵盖科学保真度、计算性能、可持续性和生命周期经济学；2）标准化SKA代表性数据集和参考输出，支持跨CPU、GPU和新兴加速器的可重复基准测试；3）多目标协同设计公式，将科学质量约束与时间、能源、碳排放和总拥有成本联系起来。

Result: 发布了数据集、基准测试结果和可重复性工具包，在AMD EPYC 9334处理器和NVIDIA H100 GPU上评估了WSClean和IDG的协同设计度量。展示了astroCAMP在异构CPU-FPGA设计空间探索中的应用潜力，能够识别SKA规模成像部署的帕累托最优操作点。

Conclusion: astroCAMP为下一代成像管线和可持续HPC架构的协同设计提供了系统框架，能够最大化SKA项目的科学产出同时满足运营和环境限制。呼吁SKA社区定义可量化的保真度度量和阈值，以加速SKA规模成像的原则性优化。

Abstract: The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.

</details>


### [179] [Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees](https://arxiv.org/abs/2512.12409)
*Xuyang Liu,Zijian Zhang,Zhen Li,Jiahang Sun,Jiamou Liu,Peng Jiang*

Main category: cs.DC

TL;DR: 提出了一种协议无关的领导者选举抽象框架SWLE，通过基于共识行为的信誉评分和拜占庭成本放大机制，在部分同步BFT协议中实现高效领导者选举，相比现有方案显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于信誉的领导者选举框架存在协议特定证明、适用范围窄或网络稳定后恢复时间无界等问题，缺乏通用的协议无关分析设计方法。

Method: 首先提出协议无关的抽象框架形式化领导者选举的正确性和有效性保证，然后设计滑动窗口领导者选举(SWLE)机制，通过基于共识行为的信誉评分动态调整领导者提名，实施拜占庭成本放大。

Result: SWLE在16服务器跨4个中国北方区域的部署中，相比最先进方案实现最高4.2倍吞吐量提升、75%延迟降低和27%拜占庭领导者频率减少，在无故障场景下保持效率。

Conclusion: SWLE提供了一种协议无关的领导者选举解决方案，在部分同步BFT协议中显著提升性能和安全保证，为拜占庭容错系统的领导者选举设计提供了通用框架。

Abstract: Leader election serves a well-defined role in leader-based Byzantine Fault Tolerant (BFT) protocols. Existing reputation-based leader election frameworks for partially synchronous BFTs suffer from either protocol-specific proofs, narrow applicability, or unbounded recovery after network stabilization, leaving an open problem. This paper presents a novel protocol-independent abstraction formalizing generic correctness properties and effectiveness guarantees for leader election under partial synchrony, enabling protocol-independent analysis and design. Building on this, we design the Sliding Window Leader Election (SWLE) mechanism. SWLE dynamically adjusts leader nominations via consensus-behavior-based reputation scores, enforcing Byzantine-cost amplification. We demonstrate SWLE introduces minimal extra overhead to the base protocol and prove it satisfies all abstraction properties and provides superior effectiveness. We show, with a 16-server deployment across 4 different regions in northern China, SWLE achieves up to 4.2x higher throughput, 75% lower latency and 27% Byzantine leader frequency compared to the state-of-the-art solution under common Byzantine faults, while maintaining efficiency in fault-free scenarios.

</details>


### [180] [HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments](https://arxiv.org/abs/2512.12476)
*Yongjun He,Shuai Zhang,Jiading Gai,Xiyuan Zhang,Boran Han,Bernie Wang,Huzefa Rangwala,George Karypis*

Main category: cs.DC

TL;DR: HetRL是一个用于异构GPU环境中高效RL训练的系统，通过多级搜索框架和逐次减半算法优化调度，相比现有系统平均提升3.17倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大和GPU更新加快，需要在异构环境中利用中端或旧代GPU资源，但现有系统在异构环境下RL训练效率低下，面临计算和数据依赖的复杂挑战。

Method: 将异构环境中的RL训练调度建模为约束联合优化问题，提出多级搜索框架分解复杂搜索空间，采用逐次减半算法分配搜索预算。

Result: 经过20,000 GPU小时的评估，HetRL在各种工作负载和设置下，吞吐量最高达到最先进系统的9.17倍，平均提升3.17倍。

Conclusion: HetRL系统有效解决了异构GPU环境中LLM RL训练的调度挑战，显著提升了训练效率，为充分利用异构计算资源提供了可行方案。

Abstract: As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.

</details>


### [181] [Strategic Server Deployment under Uncertainty in Mobile Edge Computing](https://arxiv.org/abs/2512.12532)
*Duc A. Tran,Dung Truong,Duy Le*

Main category: cs.DC

TL;DR: 该论文提出了一种基于随机双层优化的边缘服务器部署方法，通过子模函数近似目标函数，利用贪心算法解决NP-hard问题，在真实数据上相比其他方法提升高达55%


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算中服务器部署面临两个关键挑战：1) 计算效率：最大化边缘处理的工作负载；2) 通信效率：最小化用户单元与分配服务器之间的通信成本。实际场景中用户工作负载和服务器有效容量都是未知且时变的，需要在这些不确定性下可持续地实现上述目标。

Method: 将问题建模为随机双层优化问题，通过子模函数近似目标函数，利用最先进的子模最大化贪心算法有效求解这个强NP-hard问题。

Result: 使用真实世界数据评估所提算法，相比替代方法表现出显著优势，改进幅度最高可达55%。

Conclusion: 该研究提出了一种解决移动边缘计算中服务器部署问题的新方法，通过随机双层优化和子模函数近似，在不确定的工作负载和服务器容量条件下，能够有效平衡计算效率和通信效率。

Abstract: Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%

</details>


### [182] [Ethical Risk Analysis of L2 Rollups](https://arxiv.org/abs/2512.12732)
*Georgy Ishmaev,Emmanuelle Anceaume,Davide Frey,François Taïani*

Main category: cs.DC

TL;DR: 该论文分析了Layer 2 rollup架构中的伦理风险，发现约86%的项目存在无退出窗口的即时升级风险，约50%的项目存在可冻结提款的提议者控制风险，并提出了基于伦理的缓解策略建议。


<details>
  <summary>Details</summary>
Motivation: Layer 2 rollup虽然提高了吞吐量和降低了费用，但通过运营者自由裁量权和信息不对称重新引入了风险。需要研究哪些运营者和治理设计会产生伦理上有问题的用户风险。

Method: 1. 将伦理风险分析框架应用于rollup架构；2. 建立基于角色的决策权限和风险暴露分类法；3. 结合两个实证信号：从L2BEAT获取的129个项目横截面快照，以及手工整理的2022-2025年事件数据集；4. 分析影响用户资金风险的机制，包括升级时机、退出窗口、提议者活跃度和白名单、强制包含可用性、数据可用性选择等。

Result: 研究发现L2组件控制安排中的伦理风险普遍存在：约86%的项目存在无退出窗口的即时升级，约50%的项目存在可冻结提款的提议者控制。报告的事件主要集中在排序器活跃度和包含问题上，与这些依赖性一致。

Conclusion: 将研究发现转化为基于伦理的缓解策略建议，包括技术组件和治理机制方面的改进，以降低Layer 2 rollup架构中的用户风险。

Abstract: Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.

</details>


### [183] [Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators](https://arxiv.org/abs/2512.13638)
*Aofeng Shen,Chi Zhang,Yakup Budanaz,Alexandru Calotoiu,Torsten Hoefler,Luca Benini*

Main category: cs.DC

TL;DR: DiT框架自动优化基于瓦片的GEMM加速器部署，在大型配置下实现比NVIDIA GH200专家调优库更高的PE利用率，获得1.2-2.0倍加速


<details>
  <summary>Details</summary>
Motivation: 基于瓦片的多PE加速器在GEMM上性能有竞争力，但编程极其困难，因为最优软件映射与硬件设计深度耦合，难以手动部署

Method: 提出"Design in Tiles (DiT)"框架，将部署工具链与可配置可执行模型连接，自动化优化GEMM在瓦片架构上的映射

Result: 在大型加速配置(32x32瓦片，1979 TFLOPS@FP8，4 TB/s带宽)下，比NVIDIA GH200专家调优的GEMM库获得更高的PE利用率，在不同矩阵形状上实现1.2-2.0倍加速

Conclusion: DiT框架能有效解决瓦片架构加速器的编程难题，自动化优化部署，在大型配置下超越专家手动调优的性能

Abstract: Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose "Design in Tiles (DiT)", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.

</details>


### [184] [PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving](https://arxiv.org/abs/2512.12928)
*Weizhe Huang,Tao Peng,Tongxuan Liu,Donghe Jin,Xianzhe Dong,Ke Zhang*

Main category: cs.DC

TL;DR: PROSERVE是一个面向多优先级LLM服务的两层级调度框架，通过动态批处理和智能路由最大化服务增益，在SLO达成率和优先级处理上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务调度器无法同时优化SLO达成率和客户端优先级，而实际业务中不同优先级的请求具有不同的商业价值，需要优先保障高优先级请求的性能。

Method: 提出PROSERVE两层级调度框架：1)引擎层的SlideBatching动态调整批处理形成和请求排序，平衡截止时间优先和密度优先策略；2)服务层的GoRouting进行增益导向和能力感知的分发，为未来高优先级或长请求预留容量。

Result: 在四个开源数据集和真实工业追踪上的评估显示，PROSERVE相比最先进基线方法，系统增益提升高达35%，SLO达成率提升高达52%。

Conclusion: PROSERVE通过联合优化SLO达成率和客户端优先级，有效解决了多优先级LLM服务调度问题，显著提升了系统整体服务增益。

Abstract: The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.
  To bridge this gap, we first \textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.

</details>


### [185] [FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection](https://arxiv.org/abs/2512.12949)
*Ziyu Huang,Yangjie Zhou,Zihan Liu,Xinhao Luo,Yijia Diao,Minyi Guo,Jidong Zhai,Yu Feng,Chen Zhang,Anbang Wu,Jingwen Leng*

Main category: cs.DC

TL;DR: FlashFuser：首个利用现代GPU核间连接（DSM）进行内核融合的编译器框架，解决大中间结果无法融合的问题，显著提升内存受限的深度学习工作负载性能。


<details>
  <summary>Details</summary>
Motivation: 随着计算吞吐量增长远超内存带宽提升，许多深度学习工作负载受限于内存。现有编译器/框架的内核融合策略仅限于本地暂存内存，当中间结果过大（如FFN）时融合失败。现代GPU（如NVIDIA H100）已配备分布式共享内存（DSM）核间连接机制，提供更大、高带宽、低延迟的片上内存池，但这一硬件潜力尚未被软件框架利用。

Method: 1. 提出强大的基于DSM的通信抽象，形式化复杂集群数据交换模式（如reduce、shuffle、multiply）。2. 引入数据流分析器，将循环调度、资源映射和分块选择泛化到分布式内存层次结构；通过量化跨内存级别的数据移动确定最优执行顺序和分块大小。3. 将这些组件集成到统一搜索引擎中，采用分析成本建模和DSM感知剪枝策略，高效发现最优执行计划。

Result: 在NVIDIA H100 GPU上的评估显示：FlashFuser减少58%的内存访问，相比高度优化的库实现3.3倍内核加速，相比最先进编译器实现4.1倍内核加速，带来1.24倍的端到端加速。

Conclusion: FlashFuser是首个利用现代GPU核间连接进行内核融合的编译器框架，成功解决了大中间结果无法融合的问题，显著提升了内存受限深度学习工作负载的性能，充分利用了DSM硬件潜力。

Abstract: The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.

</details>


### [186] [Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures](https://arxiv.org/abs/2512.13096)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 比较了2D环面网络片上网络中基于强化学习(RL)的自适应最小路由与自适应路由基线在节点故障条件下的性能。RL方法在吞吐量和故障恢复能力方面显著优于传统自适应路由方案。


<details>
  <summary>Details</summary>
Motivation: 研究2D环面网络片上网络(NoCs)在节点故障条件下的自适应最小路由问题。环面拓扑因其低直径和高连接性而被采用，但需要有效的故障恢复机制来维持网络性能。

Method: 将每个路由器建模为强化学习(RL)代理，学习基于网络状态转发数据包；对比的自适应方案使用固定的最小路径，并在故障周围进行简单重路由。在模拟中实现两种方法，注入最多50个均匀随机分布的节点故障。

Result: RL方法在高负载下实现显著更高的吞吐量(约20-30%增益)，并在故障增加时保持更高的可靠性。RL路由器每周期传递更多数据包，通过利用路径多样性适应故障，而自适应方案在故障累积时性能急剧下降。RL方法在约30-40个故障前保持PDR高于90%，而自适应方案在相同故障数下PDR降至约70%。

Conclusion: 基于强化学习的自适应路由在环面NoCs的吞吐量和故障恢复能力方面展现出明显优势，能够更好地适应网络故障并维持高性能。

Abstract: We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs

</details>


### [187] [SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling](https://arxiv.org/abs/2512.13268)
*Muhammad Alfian Amrizal,Raka Satya Prasasta,Santana Yuda Pradata,Kadek Gemilang Santiyuda,Reza Pulungan,Hiroyuki Takizawa*

Main category: cs.DC

TL;DR: SPARS是一个基于强化学习的HPC集群功耗管理模拟器，通过集成作业调度和节点电源状态管理，在能耗和性能之间寻找平衡。


<details>
  <summary>Details</summary>
Motivation: HPC集群能耗巨大，空闲节点是主要浪费源。关闭未使用节点可以缓解此问题，但不当的开关时机会导致长时间延迟和性能下降。

Method: SPARS采用离散事件模拟框架，集成作业调度和节点电源状态管理。支持传统调度策略（如FCFS、EASY Backfilling）及其增强变体，使用强化学习代理动态决定节点开关时机。用户可通过JSON配置工作负载和平台参数，模拟器记录能耗、浪费功率、作业等待时间和节点利用率等指标。

Result: SPARS提供轻量级事件处理和一致的模拟结果，相比依赖重进程间通信的Batsim框架更易复现和扩展。其模块化设计便于集成新调度启发式或学习算法，为研究人员提供灵活、可复现、可扩展的平台。

Conclusion: SPARS使研究人员能够系统评估功耗感知调度策略，探索能效与性能之间的权衡，加速可持续HPC运营的发展。

Abstract: High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.

</details>


### [188] [Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation](https://arxiv.org/abs/2512.13319)
*Hassan Razavi,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.DC

TL;DR: 提出一种并行时间方法，用于计算随机微分方程连续时间MAP轨迹估计，通过并行架构提升计算速度


<details>
  <summary>Details</summary>
Motivation: 在并行架构上提高随机微分方程连续时间最大后验轨迹估计的计算速度

Method: 将MAP估计问题重新表述为基于Onsager-Machlup泛函的连续时间最优控制问题，采用并行时间解决方案，基于并行关联扫描算法实现并行计算

Result: 在线性高斯情况下得到并行Kalman-Bucy滤波器和并行连续时间Rauch-Tung-Striebel平滑器，通过泰勒展开扩展到非线性模型，GPU实验显示显著加速且保持精度

Conclusion: 提出的并行时间框架在保持顺序算法精度的同时，实现了显著的计算加速，适用于线性和非线性连续时间状态空间模型

Abstract: This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.

</details>


### [189] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

TL;DR: SIGMA是一个开源训练堆栈，旨在提高早期AI硬件上大规模分布式训练的可靠性、稳定性和效率，通过LUCIA训练平台和框架实现了94.45%的集群利用率，并成功训练了200B参数的MoE模型。


<details>
  <summary>Details</summary>
Motivation: 早期AI加速器面临三大挑战：系统频繁中断和未定义的故障模式影响可靠性；数值错误和训练不稳定性威胁正确性和收敛性；并行优化复杂度高且存在不可预测的局部噪声降低效率。需要解决这些挑战以实现可靠的大规模训练。

Method: 开发了SIGMA开源训练堆栈，包含LUCIA训练平台（LTP）和LUCIA训练框架（LTF）。LTP针对早期AI加速器集群进行系统优化，LTF在此基础上构建训练框架，支持大规模分布式训练。

Result: LTP实现了94.45%的有效集群加速器利用率，显著减少了节点回收和作业恢复时间。LTF成功使用2,048个AI加速器训练了200B参数的SIGMA-MOE模型，实现了21.08%的MFU、最先进的下游精度，并在75天内仅发生一次稳定性事件。

Conclusion: SIGMA不仅解决了大规模训练的关键挑战，还为AI基础设施和平台创新设立了新标准，提供了强大且经济高效的替代方案，显著推进了AI能力和可扩展性。

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


### [190] [Janus: Disaggregating Attention and Experts for Scalable MoE Inference](https://arxiv.org/abs/2512.13525)
*Zhexiang Zhang,Ye Wang,Xiangyu Wang,Yumiao Zhao,Jingzhe Jiang,Qizhen Weng,Shaohuai Shi,Yin Chen,Minchen Yu*

Main category: cs.DC

TL;DR: Janus是一个可扩展的MoE推理系统，通过将注意力模块和专家模块解耦到不同的GPU子集群上，实现独立管理和扩展，显著提升推理效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理方案通常将整个模型作为单一整体部署，对注意力模块和专家模块采用统一的资源配置，忽略了它们的不同需求，导致可扩展性有限和资源效率低下。

Method: 1. 将注意力模块和专家模块解耦到独立的GPU子集群；2. 采用自适应两阶段通信方案，利用节点内和节点间带宽层次结构；3. 设计轻量级GPU内核调度器，平衡GPU间的激活专家数量；4. 实施细粒度资源管理，动态调整专家放置和独立扩展资源。

Result: Janus相比最先进系统实现了高达3.9倍的每GPU吞吐量提升，同时满足每token延迟要求。

Conclusion: Janus通过解耦注意力模块和专家模块、优化通信调度和资源管理，为大规模MoE模型推理提供了高效、可扩展的解决方案。

Abstract: Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [191] [EDAN: Towards Understanding Memory Parallelism and Latency Sensitivity in HPC](https://arxiv.org/abs/2512.13176)
*Siyuan Shen,Mikhail Khalilov,Lukas Gianinazzi,Timo Schneider,Marcin Chrapek,Jai Dayal,Manisha Gajbe,Robert Wisniewski,Torsten Hoefler*

Main category: cs.PF

TL;DR: EDAN是一个基于运行时指令追踪生成执行DAG的性能分析工具，用于评估应用程序的内存延迟敏感性，帮助优化资源解耦系统的性能。


<details>
  <summary>Details</summary>
Motivation: 资源解耦技术虽然能提高大规模计算系统的效率，但会增加内存访问延迟。现有测量内存延迟敏感性的工具通常依赖定制硬件或周期精确模拟器，不够灵活且耗时。

Method: 提出EDAN工具，通过分析应用程序的运行时指令追踪来生成对应的执行DAG，从而评估顺序程序的内存延迟敏感性，并研究不同硬件配置的影响。

Result: 将EDAN应用于PolyBench、HPCG和LULESH等基准测试和应用，揭示了它们内在的内存级并行性和延迟敏感性特征。

Conclusion: EDAN不仅能计算性能指标的理论界限，还能帮助理解HPC应用程序固有的内存级并行性，为资源解耦系统的性能优化提供重要洞察。

Abstract: Resource disaggregation is a promising technique for improving the efficiency of large-scale computing systems. However, this comes at the cost of increased memory access latency due to the need to rely on the network fabric to transfer data between remote nodes. As such, it is crucial to ascertain an application's memory latency sensitivity to minimize the overall performance impact. Existing tools for measuring memory latency sensitivity often rely on custom ad-hoc hardware or cycle-accurate simulators, which can be inflexible and time-consuming. To address this, we present EDAN (Execution DAG Analyzer), a novel performance analysis tool that leverages an application's runtime instruction trace to generate its corresponding execution DAG. This approach allows us to estimate the latency sensitivity of sequential programs and investigate the impact of different hardware configurations. EDAN not only provides us with the capability of calculating the theoretical bounds for performance metrics, but it also helps us gain insight into the memory-level parallelism inherent to HPC applications. We apply EDAN to applications and benchmarks such as PolyBench, HPCG, and LULESH to unveil the characteristics of their intrinsic memory-level parallelism and latency sensitivity.

</details>
