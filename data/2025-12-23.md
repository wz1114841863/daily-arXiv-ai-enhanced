<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.LG](#cs.LG) [Total: 130]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference](https://arxiv.org/abs/2512.19606)
*George Karfakis,Faraz Tahmasebi,Binglu Chen,Lime Yao,Saptarshi Mitra,Tianyue Pan,Hyoukjun Kwon,Puneet Gupta*

Main category: cs.PF

TL;DR: RAPID-LLM是一个统一的性能建模框架，用于GPU集群上的大语言模型训练和推理，结合了DeepFlow前端生成硬件感知的执行轨迹和Astra-Sim后端模拟网络拓扑，预测精度在10.4%以内。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和计算需求的增长，需要准确预测训练和推理性能以优化资源配置、并行策略和硬件设计，但现有工具难以统一建模复杂的硬件-软件交互。

Method: 采用统一框架：DeepFlow前端从抽象LLM规范生成硬件感知的操作级执行轨迹，考虑算子延迟、内存流量和配置剪枝；Astra-Sim后端在显式多维网络拓扑上执行轨迹，支持拥塞感知路由和故障链路。

Result: 在A100验证案例中，预测Llama推理延迟和GPT规模训练时间与实测误差在10.4%以内，与ns-3包级结果匹配度在8%以内；能快速扫描混合并行配置、量化软链路故障敏感性、评估GPU设计变体。

Conclusion: RAPID-LLM提供了一个准确、统一的性能建模框架，能够有效支持大语言模型训练和推理的系统优化、故障分析和硬件设计探索。

Abstract: RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.
  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\% relative to published measurements, and matches ns-3 packet-level results within 8\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States](https://arxiv.org/abs/2512.17934)
*Soheil Hashtarkhani,Brianna M. White,Benyamin Hoseini,David L. Schwartz,Arash Shaban-Nejad*

Main category: cs.LG

TL;DR: 使用随机森林、梯度提升回归和线性回归预测美国县级肺癌死亡率，随机森林表现最佳，吸烟率是最重要预测因子，空间分析显示中东部地区存在高死亡率聚集。


<details>
  <summary>Details</summary>
Motivation: 肺癌是美国癌症相关死亡的主要原因，准确预测肺癌死亡率对于指导针对性干预和解决健康差异至关重要。虽然传统回归模型常用，但可解释的机器学习模型可能提供更好的预测准确性和对影响因素更深入的理解。

Method: 使用三种模型预测美国县级肺癌死亡率：随机森林(RF)、梯度提升回归(GBR)和线性回归(LR)。使用R平方和均方根误差评估模型性能，使用SHAP值确定变量重要性及其方向性影响，通过Getis-Ord热点分析分析地理差异。

Result: 随机森林模型表现最佳，R²为41.9%，RMSE为12.8。SHAP分析显示吸烟率是最重要预测因子，其次是房屋中位价值和西班牙裔人口比例。空间分析显示美国中东部县存在显著的肺癌死亡率高聚集区。

Conclusion: 随机森林模型在预测肺癌死亡率方面表现出优越性能，强调了吸烟率、房屋价值和西班牙裔人口比例的关键作用。这些发现为设计针对性干预措施、促进筛查和解决美国肺癌影响最严重地区的健康差异提供了有价值的可操作见解。

Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.

</details>


### [3] [What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD](https://arxiv.org/abs/2512.17945)
*Petr Koklev*

Main category: cs.LG

TL;DR: 量化单调性约束在信用风险模型中的性能代价，发现大型数据集上代价可忽略（<0.2% AUC损失），小型数据集代价约2-3% AUC损失


<details>
  <summary>Details</summary>
Motivation: 金融机构在部署机器学习模型时面临预测准确性与可解释性的权衡。单调性约束能确保模型行为符合领域知识，但其性能代价（单调性价格）尚未得到充分量化

Method: 在五个公共数据集和三个库上，对单调约束与无约束梯度提升模型进行基准测试。定义单调性价格（PoM）为从无约束模型转向约束模型时标准性能指标的相对变化，通过配对比较和bootstrap不确定性进行估计

Result: 实验中，PoM在AUC指标上的范围从基本为零到约2.9%。约束在大型数据集上几乎无成本（通常小于0.2%，常与零无区别），在约束覆盖广泛的小型数据集上代价最高（约2-3%）

Conclusion: 适当指定的单调性约束通常能以较小的准确性损失提供可解释性，特别是在大规模信用组合中。这为金融机构在准确性和可解释性之间做出权衡提供了量化依据

Abstract: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.

</details>


### [4] [Convolutional-neural-operator-based transfer learning for solving PDEs](https://arxiv.org/abs/2512.17969)
*Peng Fan,Guofei Pang*

Main category: cs.LG

TL;DR: 将卷积神经算子扩展到少样本学习场景，通过预训练和参数调整策略，发现神经元线性变换策略在多个PDE求解中具有最高精度


<details>
  <summary>Details</summary>
Motivation: 卷积神经算子虽然在某些情况下优于基线模型，但尚未在少样本学习场景中得到验证。本文旨在扩展该模型到少样本学习，通过预训练和参数调整来提高在有限目标数据下的性能

Method: 首先使用源数据集预训练卷积神经算子，然后仅使用少量目标数据集调整训练好的神经算子参数。研究了三种参数调整策略：微调、低秩适应和神经元线性变换

Result: 在求解Kuramoto-Sivashinsky方程、Brusselator扩散反应系统和Navier-Stokes方程等PDE时，神经元线性变换策略获得了最高的代理精度

Conclusion: 卷积神经算子可以成功扩展到少样本学习场景，神经元线性变换是最有效的参数调整策略，在多个PDE求解任务中表现出最佳性能

Abstract: Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.

</details>


### [5] [CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs](https://arxiv.org/abs/2512.17970)
*Gunho Park,Jeongin Bae,Byeongwook Kim,Baeseong park,Jiwon Ryu,Hoseung Kim,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: CodeGEMM：一种代码本中心的GEMM内核，通过预计算质心与激活的内积来替代反量化，在2位量化下相比现有方法实现显著加速


<details>
  <summary>Details</summary>
Motivation: 当前基于代码本的量化方法在极低位宽（如2位）下虽然能保持良好精度，但其内核依赖反量化操作，需要反复获取质心并重构权重，导致显著的延迟和缓存压力

Method: 提出CodeGEMM内核，用预计算的质心与激活内积（存储在轻量级Psumbook中）替代反量化。推理时，代码索引直接收集这些部分和，消除逐元素查找并减少片上占用

Result: 在Llama-3模型上，CodeGEMM在2位配置下相比最先进的基于代码本的量化方法，实现了1.83倍（8B）和8.93倍（70B）的加速，同时保持可比精度，并提高了计算效率和内存子系统利用率

Conclusion: CodeGEMM通过代码本中心的GEMM内核设计，在统一的实现框架下系统探索延迟-内存-精度权衡，为极低位宽LLM推理提供了高效的解决方案

Abstract: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.

</details>


### [6] [Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models](https://arxiv.org/abs/2512.17983)
*Irina Seregina,Philippe Lalanda,German Vega*

Main category: cs.LG

TL;DR: 该论文研究在人类活动识别任务中，使用LoRA和QLoRA等参数高效微调技术替代全模型微调，在保持性能的同时显著减少可训练参数、内存使用和训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督学习和Transformer架构显著提升了人类活动识别性能，但在目标设备计算资源有限的情况下，将大型预训练模型适配到新领域仍具有挑战性。需要寻找更高效的微调方法。

Method: 提出基于掩码自编码器骨干网络的适配框架，采用LoRA和QLoRA参数高效微调技术，在五个公开HAR数据集上使用Leave-One-Dataset-Out验证协议进行评估。

Result: 实验表明LoRA和QLoRA能够匹配全模型微调的识别性能，同时显著减少可训练参数、内存使用和训练时间。LoRA在有限监督下保持鲁棒性能，适配器秩提供准确性与效率的可控权衡。

Conclusion: 参数高效微调技术为资源受限设备上的HAR应用提供了可行的解决方案，LoRA和QLoRA在保持性能的同时大幅提升了部署效率。

Abstract: Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.

</details>


### [7] [A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations](https://arxiv.org/abs/2512.17984)
*Mohammadmahdi Rahimiasl,Ynte Vanderhoydonc,Siegfried Mercelis*

Main category: cs.LG

TL;DR: HINT提出了一种混合归纳-转导网络，结合速度的转导信号和流量的归纳学习，通过空间变换器、扩散GCN和校准层，在三个真实交通数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通流量估算面临挑战：环形检测器数据精确但稀疏，探针车辆速度数据广泛但仅与流量弱相关，相邻路段流量规模差异大（如匝道与主线），这打破了标准GNN的假设。

Method: HINT采用混合归纳-转导训练策略，将速度作为转导的网络级信号，同时归纳学习流量以泛化到未见位置。网络包含：1）归纳空间变换器学习节点特征的长程交互；2）基于FiLM条件化的扩散GCN，使用OSM属性和交通模拟作为静态上下文；3）节点级校准层纠正每个路段的规模偏差。训练采用掩码重建、逐轮节点采样、困难节点挖掘和可见流量的噪声注入。

Result: 在三个真实数据集（MOW、UTD19-Torino、UTD19-Essen）上，HINT一致超越最先进的归纳基线。相比KITS，HINT在MOW上MAE降低约42%（基础模拟）和50%（校准模拟）；在Torino降低约22%；在Essen降低约12%。即使没有模拟，HINT在MOW和Torino上仍保持优势。

Conclusion: 结合归纳流量估算、转导速度信号、交通模拟和外部地理空间信息，能显著提高交通流量估算的准确性，为解决稀疏传感器网络中的流量估算问题提供了有效方案。

Abstract: Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.
  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.

</details>


### [8] [MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements](https://arxiv.org/abs/2512.17985)
*Ruichen Tan,Jiawei Xue,Kota Tsubouchi,Takahiro Yabe,Satish V. Ukkusuri*

Main category: cs.LG

TL;DR: MoE-TransMov：基于Transformer和混合专家架构的模型，通过区分用户熟悉与陌生区域的移动模式，提升下一个兴趣点预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效区分用户在熟悉与陌生区域的移动模式差异，而研究表明用户在这两种情境下的兴趣点选择行为不同，因此需要将用户熟悉度纳入预测模型。

Method: 提出MoE-TransMov模型，基于Transformer架构结合混合专家系统，将移动轨迹分类为熟悉与陌生区域，使用专门的专家网络处理不同情境，通过自注意力机制和自适应门控网络动态选择最相关的专家模型。

Result: 在两个真实数据集（Foursquare NYC和Kyoto）上的实验表明，MoE-TransMov在Top-1、Top-5、Top-10准确率和平均倒数排名上均优于现有最佳基线方法。

Conclusion: 该方法能有效提升不同移动情境下的兴趣点预测性能，增强推荐系统的个性化能力，推动城市应用的发展。

Abstract: Accurate prediction of the next point of interest (POI) within human mobility trajectories is essential for location-based services, as it enables more timely and personalized recommendations. In particular, with the rise of these approaches, studies have shown that users exhibit different POI choices in their familiar and unfamiliar areas, highlighting the importance of incorporating user familiarity into predictive models. However, existing methods often fail to distinguish between the movements of users in familiar and unfamiliar regions. To address this, we propose MoE-TransMov, a Transformer-based model with a Transformer model with a Mixture-of-Experts (MoE) architecture designed to use one framework to capture distinct mobility patterns across different moving contexts without requiring separate training for certain data. Using user-check-in data, we classify movements into familiar and unfamiliar categories and develop a specialized expert network to improve prediction accuracy. Our approach integrates self-attention mechanisms and adaptive gating networks to dynamically select the most relevant expert models for different mobility contexts. Experiments on two real-world datasets, including the widely used but small open-source Foursquare NYC dataset and the large-scale Kyoto dataset collected with LY Corporation (Yahoo Japan Corporation), show that MoE-TransMov outperforms state-of-the-art baselines with notable improvements in Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). Given the results, we find that by using this approach, we can efficiently improve mobility predictions under different moving contexts, thereby enhancing the personalization of recommendation systems and advancing various urban applications.

</details>


### [9] [FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability](https://arxiv.org/abs/2512.17986)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedOAED是一种新颖的联邦学习算法，通过在客户端集成设备端自编码器降噪器，解决异构数据下客户端漂移和部分客户端参与带来的方差问题。


<details>
  <summary>Details</summary>
Motivation: 数据隐私法规（如GDPR和HIPAA）限制了数据共享，联邦学习虽然能保护隐私，但面临数据异构性带来的挑战：梯度噪声、客户端漂移以及部分客户端参与导致的方差增加。

Method: 提出FedOAED算法，在客户端侧集成设备端自编码器降噪器，用于缓解由多个本地训练更新引起的客户端漂移，以及部分客户端参与导致的方差问题。

Result: 在多个视觉数据集上的非独立同分布设置实验中，FedOAED始终优于最先进的基线方法。

Conclusion: FedOAED通过设备端自编码器降噪器有效缓解了联邦学习中客户端漂移和方差问题，在隐私保护前提下提升了异构数据下的学习性能。

Abstract: Over the last few decades, machine learning (ML) and deep learning (DL) solutions have demonstrated their potential across many applications by leveraging large amounts of high-quality data. However, strict data-sharing regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) have prevented many data-driven applications from being realised. Federated Learning (FL), in which raw data never leaves local devices, has shown promise in overcoming these limitations. Although FL has grown rapidly in recent years, it still struggles with heterogeneity, which produces gradient noise, client-drift, and increased variance from partial client participation. In this paper, we propose FedOAED, a novel federated learning algorithm designed to mitigate client-drift arising from multiple local training updates and the variance induced by partial client participation. FedOAED incorporates an on-device autoencoder denoiser on the client side to mitigate client-drift and variance resulting from heterogeneous data under limited client availability. Experiments on multiple vision datasets under Non-IID settings demonstrate that FedOAED consistently outperforms state-of-the-art baselines.

</details>


### [10] [A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients](https://arxiv.org/abs/2512.18031)
*Sarah Nassar,Nooshin Maghsoodi,Sophia Mannina,Shamel Addas,Stephanie Sibley,Gabor Fichtinger,David Pichora,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: 该研究发布了ICU房颤检测的标注数据集和基准测试，比较了三种AI方法，发现ECG基础模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 房颤是ICU患者最常见的心律失常，可能造成不良健康影响。目前缺乏ICU环境下房颤检测的标注数据集和系统性的AI方法比较

Method: 使用加拿大ICU和PhysioNet/Computing in Cardiology Challenge 2021的心电图数据，比较三种AI方法：基于特征的分类器、深度学习和ECG基础模型，测试多种训练配置（从零样本推理到迁移学习）

Result: 平均而言，ECG基础模型表现最佳，其次是深度学习，最后是基于特征的分类器。在ICU测试集上获得最高F1分数（0.89）的是通过迁移学习策略的ECG基础模型

Conclusion: 该研究展示了使用AI构建自动患者监测系统的潜力，通过发布标注ICU数据集和性能基准，推动ICU房颤检测研究的发展

Abstract: Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.

</details>


### [11] [Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models](https://arxiv.org/abs/2512.18035)
*Wei Qian,Chenxu Zhao,Yangyi Li,Mengdi Huai*

Main category: cs.LG

TL;DR: 该论文提出了首个针对选择性遗忘（机器去学习）中隐私漏洞的全面基准测试，系统评估了不同数据、攻击方法、去学习技术和模型架构下的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域的部署，确保隐私和符合人类价值观变得至关重要。选择性遗忘（机器去学习）虽然为隐私和数据移除提供了新范式，但其本身可能引发新的隐私漏洞。现有研究缺乏统一的评估标准，导致不同攻击方法之间的比较不公平且可能过于乐观。

Method: 作者建立了首个全面的基准测试框架，广泛调查机器去学习技术的隐私漏洞，并在多种受害者数据、最先进的去学习隐私攻击方法、去学习技术和模型架构上进行基准测试。系统性地评估和识别了与去学习引发的隐私泄露相关的关键因素。

Result: 通过系统评估获得了关于去学习引发隐私泄露的新见解，识别了影响隐私风险的关键因素。该基准测试为从业者提供了标准化的评估工具。

Conclusion: 该研究填补了选择性遗忘领域隐私评估的空白，为部署定制化去学习应用提供了可靠的隐私评估工具，有助于促进该领域的标准化发展。

Abstract: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.

</details>


### [12] [Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics](https://arxiv.org/abs/2512.18056)
*Daniel David*

Main category: cs.LG

TL;DR: 提出概率数字孪生框架，用潜在随机状态建模用户行为，通过变分自编码器学习可解释、不确定性感知的用户表示


<details>
  <summary>Details</summary>
Motivation: 现有用户建模方法依赖确定性嵌入或黑盒模型，缺乏不确定性量化和潜在表示的可解释性，需要更透明、可解释的用户表示框架

Method: 提出概率数字孪生框架，将用户建模为生成观测行为的潜在随机状态，使用摊销变分推理进行学习，基于变分自编码器实现，并引入统计解释流程分析潜在维度

Result: 用户结构主要是连续而非离散聚类的，少数主导潜在轴出现弱但有意义的结构，特定维度对应可解释特征如意见强度和决策力

Conclusion: 概率数字孪生能提供超越确定性用户嵌入的可解释、不确定性感知表示，为个性化、推荐等应用提供更透明的用户建模方法

Abstract: Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.

</details>


### [13] [Timely Parameter Updating in Over-the-Air Federated Learning](https://arxiv.org/abs/2512.19103)
*Jiaqi Zhu,Zhongyuan Zhao,Xiao Li,Ruihao Du,Shi Jin,Howard H. Yang*

Main category: cs.LG

TL;DR: 提出FAIR-k算法，在无线计算联邦学习中平衡梯度更新的新鲜度和重要性，解决正交波形数量有限与深度学习模型高维度不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 无线计算联邦学习(OAC-FL)通过利用多址信道的叠加特性缓解通信瓶颈，但实际系统中正交波形数量有限，与现代深度学习模型的高维度不匹配。

Method: 提出FAIR-k算法，结合Round-Robin和Top-k算法的优势，在每轮通信中选择最具影响力的梯度子集进行无线更新，平衡参数更新的新鲜度和梯度重要性。

Result: 通过马尔可夫分析刻画参数陈旧性分布，建立FAIR-k的收敛率，揭示数据异构性、信道噪声和参数陈旧性对训练效率的联合影响。FAIR-k促进新鲜参数更新，加速收敛并提升通信效率。

Conclusion: FAIR-k算法有效解决OAC-FL中波形资源限制问题，通过平衡新鲜度和重要性实现更高效的联邦学习训练，支持更长的本地训练周期而不显著影响整体训练效率。

Abstract: Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.

</details>


### [14] [Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins](https://arxiv.org/abs/2512.18104)
*Andreas E. Robertson,Samuel B. Inman,Ashley T. Lenau,Ricardo A. Lebensohn,Dongil Shin,Brad L. Boyce,Remi M. Dingreville*

Main category: cs.LG

TL;DR: VDMN是一种物理信息代理模型，通过变分分布和层次机制架构捕捉微观结构不确定性，实现高效的概率正向和逆向材料行为预测。


<details>
  <summary>Details</summary>
Motivation: 解决材料数字孪生中微观结构形态、组分行为和加工条件等不可消除的随机不确定性带来的挑战，开发具有不确定鲁棒性的材料数字孪生。

Method: 提出变分深度材料网络（VDMN），在其层次机制架构中嵌入变分分布，使用基于泰勒级数展开和自动微分的解析传播方案，在训练和预测期间高效传播不确定性。

Result: 成功应用于两个数字孪生驱动场景：1）作为不确定性感知材料数字孪生，预测并实验验证了增材制造聚合物复合材料的非线性力学变异性；2）作为逆向校准引擎，分离并定量识别了组分性能中的重叠不确定性来源。

Conclusion: VDMN为不确定鲁棒性材料数字孪生奠定了基础，能够有效处理材料系统中的随机不确定性，实现高效的概率正向和逆向预测。

Abstract: Aleatoric uncertainties - irremovable variability in microstructure morphology, constituent behavior, and processing conditions - pose a major challenge to developing uncertainty-robust digital twins. We introduce the Variational Deep Material Network (VDMN), a physics-informed surrogate model that enables efficient and probabilistic forward and inverse predictions of material behavior. The VDMN captures microstructure-induced variability by embedding variational distributions within its hierarchical, mechanistic architecture. Using an analytic propagation scheme based on Taylor-series expansion and automatic differentiation, the VDMN efficiently propagates uncertainty through the network during training and prediction. We demonstrate its capabilities in two digital-twin-driven applications: (1) as an uncertainty-aware materials digital twin, it predicts and experimentally validates the nonlinear mechanical variability in additively manufactured polymer composites; and (2) as an inverse calibration engine, it disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties. Together, these results establish the VDMN as a foundation for uncertainty-robust materials digital twins.

</details>


### [15] [Learning Generalizable Neural Operators for Inverse Problems](https://arxiv.org/abs/2512.18120)
*Adam J. Thorpe,Stepan Tretiakov,Dibakar Roy Sarkar,Krishna Kumar,Ufuk Topcu*

Main category: cs.LG

TL;DR: B2B⁻¹框架通过分离函数表示与逆映射，解决了逆问题中神经算子架构的连续性、唯一性和稳定性假设被违反的问题，在六个逆PDE基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子架构在处理逆问题时面临挑战，因为不适定逆映射违反了连续性、唯一性和稳定性假设，需要新的框架来解决这些限制。

Method: 提出B2B⁻¹框架，将函数表示与逆映射解耦：首先为输入和输出空间学习神经基函数，然后在得到的系数空间上训练逆模型。这种结构允许在单一框架内学习确定性、可逆和概率模型。

Result: 在六个逆PDE基准测试（包括两个新数据集）上评估，相比现有可逆神经算子基线表现更好。学习的概率模型能够捕捉不确定性和输入变异性，并由于系数计算中的隐式去噪而对测量噪声保持鲁棒。

Conclusion: 通过分离表示与反演，该框架为逆问题提供了可扩展的代理模型，能够跨实例、领域和不适定程度进行泛化，在不同不适定程度下保持一致的重新模拟性能。

Abstract: Inverse problems challenge existing neural operator architectures because ill-posed inverse maps violate continuity, uniqueness, and stability assumptions. We introduce B2B${}^{-1}$, an inverse basis-to-basis neural operator framework that addresses this limitation. Our key innovation is to decouple function representation from the inverse map. We learn neural basis functions for the input and output spaces, then train inverse models that operate on the resulting coefficient space. This structure allows us to learn deterministic, invertible, and probabilistic models within a single framework, and to choose models based on the degree of ill-posedness. We evaluate our approach on six inverse PDE benchmarks, including two novel datasets, and compare against existing invertible neural operator baselines. We learn probabilistic models that capture uncertainty and input variability, and remain robust to measurement noise due to implicit denoising in the coefficient calculation. Our results show consistent re-simulation performance across varying levels of ill-posedness. By separating representation from inversion, our framework enables scalable surrogate models for inverse problems that generalize across instances, domains, and degrees of ill-posedness.

</details>


### [16] [TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates](https://arxiv.org/abs/2512.18129)
*Maxmillan Ries,Sohan Seth*

Main category: cs.LG

TL;DR: TraCeR：基于Transformer的生存分析框架，用于处理纵向协变量，无需比例风险假设，并评估模型校准性


<details>
  <summary>Details</summary>
Motivation: 现有深度学习生存分析模型主要处理横截面特征，难以有效纳入纵向协变量，且评估时主要关注区分度而忽视校准性评估

Method: 提出基于分解自注意力架构的Transformer框架，从测量序列估计风险函数，自然捕捉时间协变量交互，无需对数据生成过程做假设，能处理删失数据和竞争事件

Result: 在多个真实世界数据集上，TraCeR相比最先进方法取得显著且统计上显著的性能提升，评估不仅包括区分度指标，还评估了模型校准性

Conclusion: TraCeR成功解决了生存分析中纳入纵向协变量和评估校准性的关键挑战，为时间到事件数据的建模提供了更全面的框架

Abstract: Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.

</details>


### [17] [Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection](https://arxiv.org/abs/2512.18133)
*Jie Yang,Rui Zhang,Ziyang Cheng,Dawei Cheng,Guang Yang,Bo Wang*

Main category: cs.LG

TL;DR: 提出Grad模型，通过关系扩散增强图对比学习来应对金融场景中欺诈者的自适应伪装问题，显著提升欺诈检测性能


<details>
  <summary>Details</summary>
Motivation: 现实金融场景中，有组织的犯罪团伙采用更专业的自适应伪装策略，通过模仿平台收集的行为数据，使关键特征与良性用户高度一致，导致现有图欺诈检测模型失效

Method: 提出基于关系扩散的图增强模型Grad：1) 使用监督图对比学习模块增强欺诈-良性差异；2) 采用引导关系扩散生成器从零生成辅助同质关系；3) 通过聚合过程增强微弱欺诈信号

Result: 在微信支付提供的两个真实数据集和三个公共数据集上实验，Grad模型在AUC和AP指标上分别最高提升11.10%和43.95%，优于现有最优方法

Conclusion: Grad模型能有效应对欺诈者的自适应伪装问题，通过关系扩散增强微弱欺诈信号，显著提升图欺诈检测性能，在真实金融场景中具有重要应用价值

Abstract: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.

</details>


### [18] [Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation](https://arxiv.org/abs/2512.18174)
*Lena Libon,Meghana Bhange,Rushabh Solanki,Elliot Creager,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: 论文探讨在LLM推理过程中产生的中间计算痕迹（CoT）应被视为用户个人数据，并提出了基于社区知识聚合的蒸馏方法，让低效用社区能够创建更符合自身目标的对齐模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI发展强调大规模数据集训练大模型，这催生了LLM聊天机器人等新产品，但也引发了数据隐私和用户选择权的担忧。论文关注在LLM使用链式思维推理时产生的中间计算痕迹的数据可移植性和用户自主权问题。

Method: 首先从数据隐私和可移植性法律角度论证LLM推理过程中的中间计算应被视为用户个人数据。然后基于"有意识数据贡献"框架，提出让低效用社区聚合共享知识并蒸馏到更适合其目标的替代模型中的方法。通过实证验证该方法，并研究社区多样性、推理粒度和社区规模对蒸馏性能的影响。

Result: 论文验证了基于社区知识聚合的蒸馏方法的有效性，并研究了社区多样性、推理粒度和社区规模等因素对模型蒸馏性能的具体影响。

Conclusion: LLM推理过程中的中间计算痕迹应被视为用户个人数据，社区可以通过知识聚合和蒸馏创建更符合自身目标的模型，这为数据隐私保护和用户自主权提供了新的技术路径。

Abstract: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that "reason" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.

</details>


### [19] [FairExpand: Individual Fairness on Graphs with Partial Similarity Information](https://arxiv.org/abs/2512.18180)
*Rebecca Salganik,Yibin Wang,Guillaume Salha-Galvan,Jian Kang*

Main category: cs.LG

TL;DR: FairExpand是一个用于图表示学习的个体公平性框架，在只有部分节点对相似性信息的情况下，通过交替优化节点表示和传播相似性信息来增强个体公平性。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习中的个体公平性方法需要所有节点对的预定义相似性信息，这在现实中往往不切实际，阻碍了实际应用。本文针对只有部分节点对相似性信息的更现实场景，提出解决方案。

Method: FairExpand采用两步流水线：1) 使用骨干模型（如图神经网络）优化节点表示；2) 逐步传播相似性信息。通过交替执行这两个步骤，将公平性约束有效扩展到整个图。

Result: 大量实验表明，FairExpand能够一致地增强个体公平性，同时保持模型性能，使其成为在现实世界部分相似性信息场景中实现基于图的个体公平性的实用解决方案。

Conclusion: FairExpand为图表示学习中的个体公平性提供了一个灵活框架，解决了现有方法需要完整相似性信息的不现实假设问题，使得个体公平性能够在更现实的场景中实现。

Abstract: Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.

</details>


### [20] [When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics](https://arxiv.org/abs/2512.18209)
*Yizhou Zhang*

Main category: cs.LG

TL;DR: 论文提出了广义分辨率壳动力学框架，识别了深度学习系统呈现幂律标度的充分条件，并证明幂律标度是重正化壳动力学与时间重标度协变性共同作用的刚性结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统中广泛观察到经验幂律标度现象，但其理论起源和适用范围尚未完全理解。需要建立理论框架来解释幂律标度的出现条件和机制。

Method: 使用广义分辨率壳动力学框架，将学习过程建模为跨对数分辨率壳的光谱能量传输。识别了重正化壳动力学描述成立的充分条件，包括计算图中梯度传播的有界性、初始化的弱功能非相干性、训练过程中雅可比矩阵的受控演化，以及重正化壳耦合的对数平移不变性。

Result: 证明了幂律标度不是重正化性单独的结果，而是对数平移不变性与梯度流内在时间重标度协变性结合产生的刚性后果，这迫使重正化的GRSD速度场呈现幂律形式。

Conclusion: 深度学习中的幂律标度现象可以通过广义分辨率壳动力学框架得到解释，其出现需要特定的结构条件，并且是重正化壳动力学与时间协变性共同作用的必然结果，这为理解深度学习的标度行为提供了理论基础。

Abstract: Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.
  In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.

</details>


### [21] [Stable and Efficient Single-Rollout RL for Multimodal Reasoning](https://arxiv.org/abs/2512.18215)
*Rui Liu,Dian Yu,Lei Ke,Haolin Liu,Yujun Zhou,Zhenwen Liang,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.LG

TL;DR: MSSR是一个用于多模态大语言模型的稳定单轮次强化学习框架，通过基于熵的优势塑造机制解决训练稳定性问题，在计算效率和性能上都优于传统的组基方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态强化学习方法（如GRPO）需要多轮次采样，计算效率低。虽然文本领域已有单轮次变体，但在多模态环境中存在严重的不稳定性问题，容易导致训练崩溃。需要解决训练效率与稳定性之间的权衡问题。

Method: 提出MSSR（多模态稳定单轮次）框架，采用基于熵的优势塑造机制，自适应地正则化优势幅度，防止训练崩溃并保持稳定性。该机制在单轮次多模态设置中不仅是有益的，而且是稳定性的必要条件。

Result: 在分布内评估中，MSSR表现出优越的训练计算效率，仅用一半的训练步数就能达到与组基基线相似的验证准确率。当训练相同步数时，MSSR性能超越组基基线，并在五个不同的推理密集型基准测试中展现出一致的泛化改进。

Conclusion: MSSR能够实现稳定、计算高效且有效的强化学习，适用于复杂的多模态推理任务，解决了单轮次多模态强化学习中的稳定性问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.

</details>


### [22] [Offline Behavioral Data Selection](https://arxiv.org/abs/2512.18246)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文发现离线行为数据存在显著的数据饱和现象，并提出SDR方法从大规模数据集中提取紧凑且信息丰富的子集，显著提升离线行为数据的选择效果。


<details>
  <summary>Details</summary>
Motivation: 离线行为克隆方法从专家演示中学习策略，但大规模离线行为数据集在下游任务训练时计算成本高。研究发现离线行为数据存在数据饱和现象：仅使用数据集的一小部分训练时，策略性能就迅速饱和，这表明策略性能与测试损失之间对齐较弱，通过数据选择有巨大改进空间。

Method: 提出Stepwise Dual Ranking (SDR)方法，基于两个关键原则：(1) stepwise clip：优先选择早期阶段的数据；(2) dual ranking：选择同时具有高动作价值排名和低状态密度排名的样本。

Result: 在D4RL基准测试上的大量实验和消融研究表明，SDR显著提升了离线行为数据的选择效果。

Conclusion: SDR方法能够从大规模离线行为数据集中提取紧凑且信息丰富的子集，有效解决数据饱和问题，为离线行为数据选择提供了简单而有效的解决方案。

Abstract: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.

</details>


### [23] [On the Convergence Rate of LoRA Gradient Descent](https://arxiv.org/abs/2512.18248)
*Siqiao Mu,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文首次对原始LoRA梯度下降算法进行了非渐进收敛性分析，证明了其在O(1/log T)速率下收敛到驻点，无需Lipschitz光滑性假设。


<details>
  <summary>Details</summary>
Motivation: LoRA算法因其卓越性能和低计算需求而广受欢迎，但其收敛性缺乏理论理解。现有理论结果要么考虑渐进行为，要么假设强有界条件人为强制Lipschitz光滑性，无法反映实际应用。

Method: 通过三个关键步骤：i) 将问题重新表述为堆叠适配器矩阵的外积形式；ii) 为"类Lipschitz"重参数化函数建立改进的下降引理；iii) 控制步长。从而分析原始LoRA梯度下降算法的收敛性。

Result: 证明了LoRA梯度下降以O(1/log T)的速率收敛到驻点，其中T为迭代次数。这是首次对原始LoRA算法进行的非渐进收敛分析，无需Lipschitz光滑性假设。

Conclusion: 本文首次为原始LoRA梯度下降算法提供了严格的理论收敛保证，填补了该领域的重要理论空白，为LoRA算法的实际应用提供了理论基础。

Abstract: The low-rank adaptation (LoRA) algorithm for fine-tuning large models has grown popular in recent years due to its remarkable performance and low computational requirements. LoRA trains two ``adapter" matrices that form a low-rank representation of the model parameters, thereby massively reducing the number of parameters that need to be updated at every step. Although LoRA is simple, its convergence is poorly understood due to the lack of Lipschitz smoothness, a key condition for classic convergence analyses. As a result, current theoretical results only consider asymptotic behavior or assume strong boundedness conditions which artificially enforce Lipschitz smoothness. In this work, we provide for the first time a non-asymptotic convergence analysis of the \textit{original LoRA gradient descent} algorithm, which reflects widespread practice, without such assumptions. Our work relies on three key steps: i) reformulating the problem in terms of the outer product of the stacked adapter matrices, ii) a modified descent lemma for the ``Lipschitz-like" reparametrized function, and iii) controlling the step size. With this approach, we prove that LoRA gradient descent converges to a stationary point at rate $O(\frac{1}{\log T})$, where $T$ is the number of iterations.

</details>


### [24] [LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform](https://arxiv.org/abs/2512.18266)
*Lizhi Ma,Yi-Xiang Hu,Yuke Wang,Yifang Zhao,Yihui Ren,Jian-Xiang Liao,Feng Wu,Xiang-Yang Li*

Main category: cs.LG

TL;DR: LeJOT是一个基于机器学习的智能作业成本编排框架，通过执行时间预测和实时资源优化，在Databricks平台上平均降低20%的云计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着大数据技术的快速发展，Databricks平台成为企业和研究机构的核心工具，但作业执行带来的不断上升的运营成本管理仍然是一个关键挑战。现有解决方案依赖静态配置或被动调整，无法适应工作负载的动态变化。

Method: LeJOT框架结合机器学习进行执行时间预测，并使用基于求解器的优化模型进行实时资源分配。它主动预测工作负载需求，动态分配计算资源，在满足性能要求的同时最小化成本。

Result: 在真实世界的Databricks工作负载实验中，LeJOT在分钟级调度时间范围内平均降低了20%的云计算成本，优于传统的静态分配策略。

Conclusion: LeJOT为Data Lakehouse环境提供了一个可扩展且自适应的成本高效作业调度解决方案，能够有效应对动态工作负载带来的成本挑战。

Abstract: With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a critical challenge. Existing solutions rely on static configurations or reactive adjustments, which fail to adapt to the dynamic nature of workloads. To address this, we introduce LeJOT, an intelligent job cost orchestration framework that leverages machine learning for execution time prediction and a solver-based optimization model for real-time resource allocation. Unlike conventional scheduling techniques, LeJOT proactively predicts workload demands, dynamically allocates computing resources, and minimizes costs while ensuring performance requirements are met. Experimental results on real-world Databricks workloads demonstrate that LeJOT achieves an average 20% reduction in cloud computing costs within a minute-level scheduling timeframe, outperforming traditional static allocation strategies. Our approach provides a scalable and adaptive solution for cost-efficient job scheduling in Data Lakehouse environments.

</details>


### [25] [FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation](https://arxiv.org/abs/2512.18275)
*Runze You,Shi Pu*

Main category: cs.LG

TL;DR: FedSUM算法家族支持任意客户端参与模式，无需额外数据异质性假设，通过两种延迟指标建模参与变化性，提供统一收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法通常针对特定客户端参与模式设计，限制了在实际部署中的适用性。需要一种能够处理任意客户端参与模式的方法，而不需要对数据异质性做出额外假设。

Method: 提出FedSUM算法家族，包括FedSUM-B（基础版）、FedSUM（标准版）和FedSUM-CR（通信减少版）。使用两种延迟指标（最大延迟τ_max和平均延迟τ_avg）来建模客户端参与的变化性。

Result: 提供了统一的收敛保证，证明了FedSUM方法在各种不同参与模式下的有效性，从而扩展了联邦学习在现实场景中的适用性。

Conclusion: FedSUM算法家族能够处理任意客户端参与模式，无需额外数据异质性假设，为联邦学习在实际部署中提供了更广泛的应用前景。

Abstract: Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $τ_{\max}$ and the average delay $τ_{\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.

</details>


### [26] [AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning](https://arxiv.org/abs/2512.18295)
*Xuling Zhang,Jindong Li,Yifei Zhang,Menglin Yang*

Main category: cs.LG

TL;DR: AL GNN是一个无需反向传播和回放缓冲区的持续图学习框架，通过解析学习理论将学习过程转化为递归最小二乘优化，实现高效单次训练并保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 现有基于经验回放的持续图学习方法存在隐私风险、效率低下等问题，需要存储和重访历史图数据，这限制了实际应用。

Method: 采用解析学习理论，将学习过程转化为递归最小二乘优化，通过闭式分类器更新和正则化特征自相关矩阵来维护和更新模型知识，无需反向传播和回放缓冲区。

Result: 在多个动态图分类基准测试中，AL GNN达到或优于现有方法性能：在CoraFull上平均性能提升10%，在Reddit上遗忘减少超过30%，训练时间减少近50%。

Conclusion: AL GNN提供了一种高效、隐私保护的持续图学习解决方案，通过解析学习方法消除了传统方法的主要限制，在性能、遗忘减少和训练效率方面均有显著提升。

Abstract: Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.

</details>


### [27] [Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings](https://arxiv.org/abs/2512.18309)
*Harsh Rathva,Ojas Srivastava,Pruthwik Mishra*

Main category: cs.LG

TL;DR: ESAI是一个多智能体强化学习框架，通过可微分内部对齐嵌入将安全约束直接编码到智能体内部表示中，使用反事实推理预测外部伤害并通过注意力机制调整策略更新。


<details>
  <summary>Details</summary>
Motivation: 传统的外部奖励塑造或事后安全约束方法存在局限性，需要一种将安全对齐约束直接嵌入智能体内部表示的方法，以实现更有效的伤害减少和策略调整。

Method: ESAI框架整合了四种机制：基于软参考分布的可微分反事实对齐惩罚、对齐加权的感知注意力、支持时间信用分配的Hebbian联想记忆，以及带有偏差缓解控制的相似性加权图扩散。

Result: 分析了在Lipschitz连续性和谱约束下有界内部嵌入的稳定性条件，讨论了计算复杂度，并研究了收缩行为和公平性-性能权衡等理论特性。

Conclusion: ESAI作为多智能体系统中可微分对齐机制的概念性贡献，提出了关于收敛保证、嵌入维度和高维环境扩展等开放理论问题，实证评估留待未来工作。

Abstract: We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.
  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.
  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.

</details>


### [28] [Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems](https://arxiv.org/abs/2512.18317)
*Vincent Bezold,Patrick Wagner,Jakob Hofmann,Marco Huber,Alexander Sauer*

Main category: cs.LG

TL;DR: 提出一种可信赖的强化学习方法用于工业压缩空气系统控制，通过多级可解释性管道确保安全高效运行，相比现有控制器节能约4%且无需显式物理模型。


<details>
  <summary>Details</summary>
Motivation: 工业压缩空气系统能耗巨大，现有控制器效率有限且缺乏透明度。需要开发既节能又可信赖的控制方法，确保在现实边界条件下的安全运行。

Method: 开发可信赖强化学习框架，结合输入扰动测试、基于梯度的敏感性分析和SHAP特征归因的多级可解释性管道，在多种压缩机配置下进行实证评估。

Result: 学习到的策略具有物理合理性，能预测未来需求并始终尊重系统边界。相比工业控制器减少不必要过压，节能约4%。系统压力和预测信息主导决策，压缩机级输入起次要作用。

Conclusion: 结合效率提升、预测行为和透明验证，支持强化学习在工业能源系统中的可信赖部署。多级可解释性管道为工业应用提供了必要的透明度。

Abstract: This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\,\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.

</details>


### [29] [Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale](https://arxiv.org/abs/2512.18373)
*Ansh Nagwekar*

Main category: cs.LG

TL;DR: 该论文系统分析神经网络优化算法，从SGD到高阶方法，揭示算法设计如何解释深度学习训练过程，并提供实用部署策略。


<details>
  <summary>Details</summary>
Motivation: 神经网络优化是AI研究的关键但理解不足的挑战，SGD等方法的成功更多是经验性的而非原理性的。论文旨在通过追踪优化算法演变来揭示原理性算法设计如何解释训练过程。

Method: 从SGD和自适应梯度方法的第一性原理出发，分析传统方法在真实数据各向异性下的局限性，探索基于曲率信息的二阶近似技术、分层预处理、自适应学习率等高级替代方案，并研究优化算法与训练工具（如最大更新参数化、学习率调度、指数移动平均）的相互作用。

Result: 揭示了原理性算法设计可以解释深度学习训练过程，传统方法在真实数据各向异性下存在局限性，基于曲率信息的高阶方法能提供更好解决方案，优化算法与训练工具的协同对实证成功至关重要。

Conclusion: 通过系统分析优化算法演变，论文提供了将理论理解转化为实际部署的实用处方和实现策略，帮助将先进优化方法整合到现代深度学习工作流程中。

Abstract: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.

</details>


### [30] [The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?](https://arxiv.org/abs/2512.18390)
*Vassilis Digalakis,Christophe Pérignon,Sébastien Saurin,Flore Sentenac*

Main category: cs.LG

TL;DR: 论文研究组织何时应该用依赖新特征的新模型替换现有模型，提出了结合经济学和统计学的框架，开发了三种实用算法，并在信贷评分数据集上验证了前瞻性序列方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着新数据源的不断出现，组织面临何时用依赖新特征的新模型替换现有训练好的模型的决策问题。需要综合考虑学习曲线动态、数据获取和重新训练成本、未来收益折现等因素，制定经济上合理的模型转换策略。

Method: 1. 开发了统一的经济学和统计学框架，结合学习曲线动态、数据获取成本、重新训练成本和未来收益折现；2. 在理想化场景中推导最优切换时间的闭式解；3. 提出三种实用算法：一次性基准方法、贪婪序列方法和前瞻性序列方法；4. 在真实信贷评分数据集上验证方法效果。

Result: 1. 最优切换时间系统地取决于成本参数和学习曲线行为；2. 前瞻性序列方法优于其他方法，能够接近具有完全预见性的oracle的价值；3. 建立了有限样本保证，包括前瞻性序列方法相对于oracle实现次线性遗憾的条件。

Conclusion: 研究为组织在新数据源可用时进行经济上合理的模型转换提供了操作蓝图，前瞻性序列方法在实践中表现优异，能够有效平衡短期成本和长期收益。

Abstract: We study the problem of deciding whether, and when an organization should replace a trained incumbent model with a challenger relying on newly available features. We develop a unified economic and statistical framework that links learning-curve dynamics, data-acquisition and retraining costs, and discounting of future gains. First, we characterize the optimal switching time in stylized settings and derive closed-form expressions that quantify how horizon length, learning-curve curvature, and cost differentials shape the optimal decision. Second, we propose three practical algorithms: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit-scoring dataset with gradually arriving alternative data, we show that (i) optimal switching times vary systematically with cost parameters and learning-curve behavior, and (ii) the look-ahead sequential method outperforms other methods and is able to approach in value an oracle with full foresight. Finally, we establish finite-sample guarantees, including conditions under which the sequential look-ahead method achieve sublinear regret relative to that oracle. Our results provide an operational blueprint for economically sound model transitions as new data sources become available.

</details>


### [31] [Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem](https://arxiv.org/abs/2512.18409)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的框架来分析乐观型随机赌博机算法，通过识别最小分析要素来简化对数后悔的证明。


<details>
  <summary>Details</summary>
Motivation: 尽管UCB、UCB-V、线性UCB和有限臂GP-UCB等乐观型随机赌博机算法都实现了对数后悔，但它们的证明虽然表面不同，本质上遵循相似结构。作者希望提取这些分析中的最小要素，为经典算法提供统一、接近最优的证明，并自然扩展到当代赌博机变体。

Method: 论文提出了一个分析框架，主要包含三个要素：1）对估计器的高概率集中条件；2）描述半径崩溃的确定性引理；3）描述乐观强制偏差的确定性引理。在这个框架下，对数后悔的证明简化为验证这些最小条件。

Result: 该框架为UCB、UCB-V、线性UCB和有限臂GP-UCB等经典乐观型随机赌博机算法提供了统一、简洁的证明，这些证明接近理论下界，并且能够自然地扩展到许多当代赌博机变体。

Conclusion: 乐观型随机赌博机算法的对数后悔证明可以简化为验证一个高概率集中条件，然后应用两个简单的确定性引理。这种统一的框架不仅简化了经典算法的分析，还为扩展新算法提供了系统化的方法。

Abstract: Several optimism-based stochastic bandit algorithms -- including UCB, UCB-V, linear UCB, and finite-arm GP-UCB -- achieve logarithmic regret using proofs that, despite superficial differences, follow essentially the same structure. This note isolates the minimal ingredients behind these analyses: a single high-probability concentration condition on the estimators, after which logarithmic regret follows from two short deterministic lemmas describing radius collapse and optimism-forced deviations. The framework yields unified, near-minimal proofs for these classical algorithms and extends naturally to many contemporary bandit variants.

</details>


### [32] [MoE Pathfinder: Trajectory-driven Expert Pruning](https://arxiv.org/abs/2512.18425)
*Xican Yang,Yuanhe Tian,Yan Song*

Main category: cs.LG

TL;DR: 提出基于专家激活轨迹的MoE剪枝方法，将专家选择转化为全局最优路径规划问题，实现跨层非均匀剪枝


<details>
  <summary>Details</summary>
Motivation: MoE架构在实际部署中面临复杂性高和激活效率低的问题，现有专家剪枝方法依赖局部重要性指标和均匀层间剪枝，忽略了专家在不同层的异质性贡献

Method: 将MoE视为加权计算图，基于专家跨层激活轨迹，将专家选择转化为全局最优路径规划问题，整合重构误差、路由概率和激活强度等轨迹级重要性信号

Result: 实验表明该方法在几乎所有任务上都优于现有剪枝方法，实现了跨层非均匀的专家保留

Conclusion: 基于专家激活轨迹的全局剪枝方法能有效解决MoE模型部署复杂性和效率问题，通过考虑专家在不同层的异质性贡献实现更优的剪枝性能

Abstract: Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.

</details>


### [33] [On the Universality of Transformer Architectures; How Much Attention Is Enough?](https://arxiv.org/abs/2512.18445)
*Amirreza Abbasi,Mohsen Hooshmand*

Main category: cs.LG

TL;DR: 该论文综述了Transformer架构的普适性问题，回顾了近期在结构最小性和逼近速率等方面的进展，旨在阐明Transformer表达能力的已知内容，区分稳健与脆弱的理论保证，并指出未来理论研究的关键方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在众多AI领域（如大语言模型、计算机视觉、强化学习）中占据核心地位，这源于其相对于其他架构的普适性和可扩展性。然而，关于Transformer普适性的理论基础尚不清晰，需要系统梳理现有研究成果，区分可靠的理论保证与脆弱的假设，为未来研究指明方向。

Method: 采用文献综述方法，系统回顾和分析近期关于Transformer普适性的理论研究进展，包括架构改进（如结构最小性、逼近速率等），并综合评估当前最先进的研究成果。

Result: 论文梳理了Transformer表达能力的现有理论认识，区分了稳健的理论保证与脆弱的假设，识别了当前理论研究的空白和局限性，为理解Transformer的普适性提供了系统框架。

Conclusion: Transformer的普适性研究仍处于发展阶段，需要更多理论工作来深入理解其表达能力的边界和限制。未来的理论研究应关注架构改进的理论基础、逼近效率的量化分析，以及在不同应用场景下的普适性保证。

Abstract: Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture's perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.

</details>


### [34] [Secret mixtures of experts inside your LLM](https://arxiv.org/abs/2512.18452)
*Enric Boix-Adsera*

Main category: cs.LG

TL;DR: MLP层在LLM中可能秘密执行稀疏计算，可近似为稀疏激活的MoE层，这解释了MoE架构的有效性并为高效设计提供新方向。


<details>
  <summary>Details</summary>
Motivation: MLP层作为transformer架构中最古老但最不理解的组件之一，其密集计算和难以可视化的特性阻碍了对其工作机制的理解。本文旨在揭示LLM中MLP层的本质工作原理。

Method: 提出理论连接：MoE模型与激活空间中的稀疏自编码器结构之间存在联系。在预训练LLM上进行实证验证，分析激活分布的重要性，并探索基于低秩路由器的MoE架构设计。

Result: 实证验证了MLP层可被稀疏激活的MoE层良好近似，但这种近似依赖于神经网络激活分布的结构特性，在随机高斯数据上不成立。结果揭示了MLP层的一般工作原理。

Conclusion: MLP层在LLM中秘密执行稀疏计算，这解释了现代基于MoE的transformer的有效性。研究结果为基于低秩路由器的更高效MoE架构设计提供了新方向。

Abstract: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.
  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.
  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.

</details>


### [35] [NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic](https://arxiv.org/abs/2512.18453)
*Jayant Lohia*

Main category: cs.LG

TL;DR: NOVA框架通过数值优化发现分数插值点，解决了Winograd卷积在低精度计算中的数值不稳定问题，使大尺寸卷积核在FP16/Int8下可用。


<details>
  <summary>Details</summary>
Motivation: Winograd卷积作为高效推理标准算法，在现代低精度计算时代面临数值不稳定性的关键障碍。随着卷积核尺寸增大（如F(6,3), F(8,3)），传统整数变换的条件数急剧恶化，在FP16或Int8精度下无法使用。

Method: 提出NOVA框架，将Winograd插值点选择视为连续优化问题。通过进化策略在R^n-1流形上搜索，将候选点转换为简单有理数，并通过符号验证保证正确性。该方法打破了数十年来的整数插值惯例。

Result: NOVA显著改善了变换条件数：F(8,3)的1D条件数提升415倍，2D卷积提升172,484倍。在FP16 ImageNet推理中，标准变换准确率仅4.7%，而NOVA恢复至75-78%，无需重新训练或校准。

Conclusion: NOVA发现的分数变换可作为即插即用替代方案，解锁了大尺寸Winograd卷积在下一代硬件上的效率潜力，解决了低精度计算中的数值稳定性问题。

Abstract: Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the condition numbers of standard integer based transforms explode, reaching kappa = 2 x 10^5 for F(8,3), rendering them unusable in FP16 or Int8. We introduce NOVA (Numerical Optimization of Vandermonde Arithmetic), a discovery framework that breaks the decades old convention of integer interpolation. Treating Winograd point selection as a continuous optimization problem, NOVA searches the manifold R^n-1 via Evolution Strategy, snaps candidates to simple rationals, and guarantees correctness via symbolic verification. This process uncovers a hidden landscape of stable, fractional configurations such as {+-5/6, +-7/6, +-3/5} that defy traditional vocabulary constraints. The impact is transformative: NOVA improves the conditioning of F(8,3) by 415x in 1D, which squares to a 172,484x improvement for 2D convolution. In real world FP16 ImageNet inference, where standard transforms collapse to random chance (e.g., 4.7 percent accuracy on VGG16), NOVA's points restore full accuracy (75 to 78 percent), recovering over 70 percentage points without retraining, calibration, or learned parameters. These discovered transforms act as drop in replacements, effectively unlocking the efficiency of large tile Winograd convolution for next generation hardware.

</details>


### [36] [Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs](https://arxiv.org/abs/2512.18454)
*David Graber,Victor Armegioiu,Rebecca Buller,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出基于扩散模型的概率OOD检测框架，用于处理结合连续几何和分类特征的复杂3D图数据，通过统一连续扩散和轨迹特征分析实现无监督OOD检测。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在分布外数据上性能下降，而复杂3D图数据（如蛋白质-配体复合物）结合连续几何和分类特征，且无序构造，使得OOD检测特别困难，需要可靠的检测方法。

Method: 提出基于扩散模型的概率OOD检测框架：1）统一连续扩散处理3D坐标和离散特征，将分类特征嵌入连续空间并用交叉熵训练；2）通过后验均值插值从预测类别概率获得扩散分数；3）建立自洽的概率流ODE产生样本对数似然；4）利用多尺度轨迹特征（路径曲折度、流刚度、向量场不稳定性）提供补充信息。

Result: PF-ODE似然能识别保留的蛋白质家族为OOD，与独立结合亲和力模型（GEMS）的预测误差强相关，可实现新复合物的先验可靠性估计。结合轨迹特征的联合分布建模提高了检测灵敏度，优于仅使用似然的基线方法。

Conclusion: 该框架为几何深度学习提供了无标签的OOD量化工作流，通过统一连续扩散和轨迹特征分析，实现了对复杂3D图数据的可靠OOD检测，具有实际应用价值。

Abstract: Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.

</details>


### [37] [Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review](https://arxiv.org/abs/2512.18466)
*Oraib Almegdadi,João Marcelino,Sarah Fakhreddine,João Manso,Nuno C. Marques*

Main category: cs.LG

TL;DR: 本文综述了自组织映射（SOM）这一无监督AI技术在水质评估中的应用，重点探讨了其在处理多维数据、揭示隐藏模式以支持有效水资源管理方面的作用。


<details>
  <summary>Details</summary>
Motivation: 湖泊和水库的水质评估与管理面临数据稀疏性、异质性和参数间非线性关系等挑战，而环境监测数据的日益丰富为分析提供了新机遇，需要有效工具来处理这些复杂数据集。

Method: 采用文献综述方法，系统分析SOM在水质评估中的应用研究，包括参数选择、时空采样策略、聚类方法，以及SOM如何处理多维数据和揭示隐藏模式。

Result: SOM在分析复杂数据集方面表现出色，特别是在标记数据有限的情况下，能够实现高维数据可视化，检测隐藏的生态模式，识别水质指标间的关键相关性，在生态评估、营养状态分类、藻华监测和流域影响评估中具有广泛适用性。

Conclusion: SOM为湖泊和水库生态系统的监测和可持续管理提供了有力的分析工具，该综述为未来研究和实际应用提供了全面见解，有助于改进水资源管理实践。

Abstract: Sustainable water quality underpins ecological balance and water security. Assessing and managing lakes and reservoirs is difficult due to data sparsity, heterogeneity, and nonlinear relationships among parameters. This review examines how Self-Organizing Map (SOM), an unsupervised AI technique, is applied to water quality assessment. It synthesizes research on parameter selection, spatial and temporal sampling strategies, and clustering approaches. Emphasis is placed on how SOM handles multidimensional data and uncovers hidden patterns to support effective water management. The growing availability of environmental data from in-situ sensors, remote sensing imagery, IoT technologies, and historical records has significantly expanded analytical opportunities in environmental monitoring. SOM has proven effective in analysing complex datasets, particularly when labelled data are limited or unavailable. It enables high-dimensional data visualization, facilitates the detection of hidden ecological patterns, and identifies critical correlations among diverse water quality indicators. This review highlights SOMs versatility in ecological assessments, trophic state classification, algal bloom monitoring, and catchment area impact evaluations. The findings offer comprehensive insights into existing methodologies, supporting future research and practical applications aimed at improving the monitoring and sustainable management of lake and reservoir ecosystems.

</details>


### [38] [The Geometry of Abstraction: Continual Learning via Recursive Quotienting](https://arxiv.org/abs/2512.18471)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出递归度量收缩方法解决持续学习中的几何障碍，通过拓扑形变压缩局部流形，实现有界容量嵌入和线性可分性。


<details>
  <summary>Details</summary>
Motivation: 固定维度空间中的持续学习系统面临几何障碍：欧几里得空间中线性轨迹的测地距离随时间线性增长，导致覆盖数发散，在固定维度硬件中必然引发轨迹重叠和灾难性遗忘。

Method: 提出递归度量收缩框架，将抽象视为拓扑形变：在验证的时间邻域内压缩度量张量的商映射，使局部子流形直径趋近于零。通过流形和支架流形的正交划分实现稳定性。

Result: 1. 有界容量定理：递归商映射可将任意长轨迹嵌入有界表示体积；2. 拓扑坍缩可分性定理：递归商化使非线性可分时序序列在极限下线性可分；3. 奇偶划分稳定性定理：正交划分确保主动学习的度量形变不干扰存储记忆稳定性。

Conclusion: 递归度量收缩为持续学习提供几何解决方案，神经架构中的令牌可物理实现为连接时序流形远点的奇点或虫洞，解决了固定维度系统中的灾难性遗忘问题。

Abstract: Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.

</details>


### [39] [APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification](https://arxiv.org/abs/2512.18473)
*Khaled Berkani*

Main category: cs.LG

TL;DR: APC-GNN++是一种用于糖尿病分类的自适应患者中心图神经网络，通过上下文感知边注意力、置信度引导的特征融合和邻域一致性正则化来捕捉患者间的临床关系，并采用小图方法处理新患者，在真实糖尿病数据集上优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够更好捕捉患者间临床意义关系、处理未见患者并提供实时可解释预测的糖尿病分类模型，为医疗专业人员提供交互式工具。

Method: 提出APC-GNN++模型，包含：1) 上下文感知边注意力机制；2) 置信度引导的节点特征与图表示融合；3) 邻域一致性正则化；4) 处理新患者的小图方法，利用最近邻构建局部图进行预测；5) 集成到Tkinter GUI中供医疗专业人员交互使用。

Result: 在阿尔及利亚地区医院收集的真实糖尿病数据集上，APC-GNN++在测试准确率和宏观F1分数上均优于传统机器学习模型（MLP、随机森林、XGBoost）和普通GCN。节点级置信度分析揭示了模型在不同患者群体中如何平衡自身信息和基于图的证据。

Conclusion: APC-GNN++通过自适应患者中心设计有效提升了糖尿病分类性能，提供可解释的患者中心洞察，小图方法实现了无需重新训练全局模型的新患者实时预测，集成GUI增强了医疗实践中的实用性。

Abstract: We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .

</details>


### [40] [Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts](https://arxiv.org/abs/2512.18522)
*Hatim M. E. Geli,Islam Omar,Mona Y. Elshinawy,David W. DuBios,Lara Prehodko,Kelly H Smith,Abdel-Hameed A. Badawy*

Main category: cs.LG

TL;DR: 该研究应用机器学习技术，结合干旱指数和历史干旱影响记录，生成短期干旱影响预测，为干旱预警和决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 干旱是影响生态和人类系统的复杂自然灾害，近年来干旱严重性、频率和持续时间增加，需要有效的监测和缓解策略。预测干旱影响（而非仅干旱条件）可为早期预警系统和主动决策提供机会。

Method: 使用机器学习技术将干旱指数（DSCI和ESI）与历史干旱影响记录（2005-2024年）结合，生成短期影响预测。采用eXtreme Gradient Boosting（XGBoost）模型，结合前8周数据预测未来8周的干旱影响。

Result: 火灾和救援影响预测准确率最高，其次是农业和水资源影响，而植物和社会影响预测变异性较大。模型成功为新墨西哥州生成县级和州级预测，支持开发生态干旱信息通信系统。

Conclusion: 该研究展示了机器学习在干旱影响预测中的应用潜力，支持干旱缓解和适应策略的制定，可为干旱易发地区提供更有效的决策支持。

Abstract: Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.

</details>


### [41] [Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study](https://arxiv.org/abs/2512.18524)
*Janek Dyer,Jagdeep Ahluwalia,Javad Zarrin*

Main category: cs.LG

TL;DR: 该研究提出了一种结合图神经网络与图论特征的混合方法，用于区分五种生成图模型，在大型合成数据集上取得了高达98.5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 区分生成图模型对于理解合成图和真实世界结构的复杂模式至关重要。虽然图神经网络在图分类任务中效果显著，但很少有研究探索其与可解释图论特征的结合。

Method: 1) 生成包含五种代表性生成图家族的大型合成数据集；2) 提取全面的节点和图级特征，使用随机森林进行特征选择；3) 将特征集成到六种GNN架构中；4) 使用Optuna优化超参数；5) 与仅使用手工特征的SVM基线比较。

Result: GraphSAGE和GTN达到最高分类性能（98.5%准确率），t-SNE和UMAP可视化显示强类别分离。GCN和GIN表现良好，GAT类模型因捕捉全局结构能力有限而表现较差。SVM基线证实了消息传递功能对性能提升的重要性。

Conclusion: 结合图神经网络与图论特征的混合方法能有效区分生成图模型，GraphSAGE和GTN表现最佳，消息传递机制对分类性能至关重要。

Abstract: The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.

</details>


### [42] [Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment](https://arxiv.org/abs/2512.18566)
*Ruiqi Chen,Giacomo Vedovati,Todd Braver,ShiNung Ching*

Main category: cs.LG

TL;DR: DFORM框架通过非线性坐标变换对齐两个动力系统的状态空间，解决比较学习动力学和识别高维模型中低维动力学模式的挑战。


<details>
  <summary>Details</summary>
Motivation: 在理论神经科学中，评估循环神经网络等动力系统模型的动力学对于理解其生成机制至关重要，但面临两个主要挑战：1）不同模型坐标系统不兼容导致比较困难；2）在高维非线性模型中识别重要的低维动力学模式（如极限集）非常困难。

Method: 提出DFORM框架，学习两个动力系统状态空间之间的非线性坐标变换，使它们的轨迹以最大的一对一方式对齐。该方法不仅能评估两个模型是否具有拓扑等价性，还能作为副产品定位高维系统中嵌入的低维流形上的动力学模式。

Result: 验证了DFORM识别线性和非线性坐标变换的能力，能够量化拓扑不同系统之间的相似性，定位高维模型中的重要动力学模式（如不变流形和鞍点极限集），并在基于人类fMRI数据训练的RNN模型中成功识别极限环，与先前的数值分析结果一致。

Conclusion: DFORM提供了一个全面的框架，解决了比较学习动力学和识别高维模型中低维动力学模式的关键挑战，为理论神经科学中的模型评估和机制理解提供了有力工具。

Abstract: Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.

</details>


### [43] [Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing](https://arxiv.org/abs/2512.18575)
*Effiong Blessing,Chiung-Yi Tseng,Somshubhra Roy,Junaid Rehman,Isaac Nkrumah*

Main category: cs.LG

TL;DR: 该研究首次全面评估了脉冲神经网络中记忆机制在跨模态任务中的表现，发现不同记忆机制（Hopfield网络、HGRN、SCL）在视觉和听觉任务上存在显著的模态依赖性性能差异，揭示了记忆机制的任务特定性而非通用性。


<details>
  <summary>Details</summary>
Motivation: 尽管记忆增强的脉冲神经网络在能效方面具有优势，但其在不同感官模态（视觉、听觉）之间的泛化能力尚未被探索。研究旨在填补这一空白，通过跨模态消融研究来理解记忆机制在神经形态计算中的通用性。

Method: 采用系统性评估方法，在视觉（N-MNIST）和听觉（SHD）神经形态数据集上测试了五种架构：Hopfield网络、分层门控循环网络（HGRN）、监督对比学习（SCL）。进行了跨模态消融研究，包括联合多模态训练和定量记忆痕迹分析。

Result: 发现显著的模态依赖性性能模式：Hopfield网络在视觉任务上达到97.68%准确率，但在听觉任务上仅76.15%（21.53点差距），表现出严重的模态特定性；SCL表现更均衡（视觉96.72%，听觉82.16%，14.56点差距）。HGRN联合多模态训练达到94.41%视觉和79.37%听觉准确率（平均88.78%）。定量记忆痕迹分析显示跨模态对齐较弱（0.038相似度）。

Conclusion: 记忆机制表现出任务特定优势而非通用适用性，为神经形态系统中模态特定记忆优化提供了首个实证证据。研究实现了比传统神经网络603倍的能效提升，并验证了并行架构设计的合理性。

Abstract: Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.

</details>


### [44] [SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models](https://arxiv.org/abs/2512.18583)
*Pengcheng Li,Qiang Fang,Tong Zhao,Yixing Lan,Xin Xu*

Main category: cs.LG

TL;DR: SD2AIL利用扩散模型生成合成示范来增强对抗模仿学习，通过优先重放策略选择最有价值的示范，在Hopper任务中达到3441平均回报，超越SOTA方法89分。


<details>
  <summary>Details</summary>
Motivation: 对抗模仿学习需要大量专家示范，但在某些场景中收集这些示范很困难。受扩散模型在数据生成方面的成功启发，研究者希望通过生成合成示范来增强专家数据。

Method: 1. 在判别器中使用扩散模型生成合成示范作为伪专家数据来增强专家示范；2. 引入优先专家示范重放策略(PEDR)，从大量(伪)专家示范中选择最有价值的示范进行重放。

Result: 在模拟任务中验证了方法的有效性和鲁棒性。特别是在Hopper任务中，平均回报达到3441，比最先进方法高出89分。

Conclusion: SD2AIL通过扩散模型生成合成示范并结合优先重放策略，有效解决了对抗模仿学习中专家示范不足的问题，显著提升了性能。

Abstract: Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.

</details>


### [45] [Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows](https://arxiv.org/abs/2512.18595)
*Runze Mao,Rui Zhang,Xuan Bai,Tianhao Wu,Teng Zhang,Zhenyi Chen,Minqi Lin,Bocheng Zeng,Yangchen Xu,Yingxuan Xiang,Haoze Zhang,Shubham Goswami,Pierre A. Dawe,Yifan Xu,Zhenhua An,Mengtao Yan,Xiaoyi Lu,Yi Wang,Rongbo Bai,Haobu Gao,Xiaohang Fang,Han Li,Hao Sun,Zhi X. Chen*

Main category: cs.LG

TL;DR: REALM是一个针对多物理场神经代理模型的严格基准测试框架，包含11个高保真数据集和标准化评估协议，揭示了当前模型在现实多物理场流动中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多物理场动力学预测面临计算成本高、多尺度耦合复杂的挑战。现有神经代理模型评估过于依赖简化的低维代理问题，未能暴露模型在现实场景中的脆弱性，存在"掌握幻觉"问题。

Method: 提出了REALM基准测试框架，包含11个从典型多物理场问题到复杂推进和火灾安全场景的高保真数据集，采用标准化的端到端训练和评估协议，包含多物理场感知预处理和鲁棒的展开策略。

Result: 系统评估了十多个代表性代理模型家族，发现三个稳健趋势：1）受维度、刚度和网格不规则性共同控制的缩放障碍；2）性能主要受架构归纳偏置而非参数数量控制；3）名义精度指标与物理可信行为之间存在持续差距。

Conclusion: REALM揭示了当前神经代理模型在现实多物理场流动中的局限性，为开发下一代物理感知架构提供了严格的测试平台。

Abstract: Predicting multiphysics dynamics is computationally expensive and challenging due to the severe coupling of multi-scale, heterogeneous physical processes. While neural surrogates promise a paradigm shift, the field currently suffers from an "illusion of mastery", as repeatedly emphasized in top-tier commentaries: existing evaluations overly rely on simplified, low-dimensional proxies, which fail to expose the models' inherent fragility in realistic regimes. To bridge this critical gap, we present REALM (REalistic AI Learning for Multiphysics), a rigorous benchmarking framework designed to test neural surrogates on challenging, application-driven reactive flows. REALM features 11 high-fidelity datasets spanning from canonical multiphysics problems to complex propulsion and fire safety scenarios, alongside a standardized end-to-end training and evaluation protocol that incorporates multiphysics-aware preprocessing and a robust rollout strategy. Using this framework, we systematically benchmark over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks, and identify three robust trends: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities. Taken together, REALM exposes the limits of current neural surrogates on realistic multiphysics flows and offers a rigorous testbed to drive the development of next-generation physics-aware architectures.

</details>


### [46] [EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture](https://arxiv.org/abs/2512.18596)
*Quanxi Zhou,Wencan Mao,Yilei Liang,Manabu Tsukada,Yunling Liu,Jon Crowcroft*

Main category: cs.LG

TL;DR: 提出EIA-SEC框架解决多无人机智能农业系统中的轨迹规划问题，通过精英模仿和共享集成批评器提升性能


<details>
  <summary>Details</summary>
Motivation: 无线通信技术推动智能农业发展，无人机在数据收集、图像采集和通信任务中发挥多功能作用。需要解决多无人机协同工作的轨迹规划问题。

Method: 将问题建模为马尔可夫决策过程，提出精英模仿演员-共享集成批评器框架。智能体自适应地从精英智能体学习以减少试错成本，共享集成批评器与每个智能体的本地批评器协作确保无偏目标值估计并防止过估计。

Result: 实验结果表明EIA-SEC在奖励性能、训练稳定性和收敛速度方面优于最先进的基线方法。

Conclusion: 提出的EIA-SEC框架能有效解决多无人机智能农业系统的轨迹规划问题，在多个性能指标上表现优异。

Abstract: The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.

</details>


### [47] [Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning](https://arxiv.org/abs/2512.18604)
*Wencan Mao,Quanxi Zhou,Tomas Couso Coddou,Manabu Tsukada,Yunling Liu,Yusheng Ji*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多智能体强化学习的无人机农业轨迹规划方法，采用新型ITDQN算法提升杂草识别和数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机在智慧农业中具有广阔应用前景，但面临环境不确定性高、观测不完整、电池容量有限等挑战，需要高效的轨迹规划方法。

Method: 将轨迹规划问题建模为马尔可夫决策过程，采用多智能体强化学习框架，提出ITDQN算法：精英模仿机制降低探索成本，中介Q网络加速训练并提升稳定性。

Result: 在仿真和真实环境中的实验验证了方案有效性，ITDQN相比DDQN在杂草识别率上提升4.43%，数据收集率提升6.94%。

Conclusion: 提出的ITDQN算法能有效解决无人机农业轨迹规划问题，显著提升杂草识别和数据收集性能，为智慧农业提供了实用解决方案。

Abstract: Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\% in weed recognition rate and 6.94\% in data collection rate.

</details>


### [48] [The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation](https://arxiv.org/abs/2512.18607)
*Huiqi Deng,Qihan Ren,Zhuofan Chen,Zhenyuan Cui,Wen Shen,Peng Zhang,Hongbin Pei,Quanshi Zhang*

Main category: cs.LG

TL;DR: 深度神经网络存在普遍的交互瓶颈：容易学习低阶和高阶交互，但始终难以充分表示中阶交互，这源于中阶交互的上下文变异性最高，梯度方差大，难以学习。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络能表示何种合作结构仍是一个基础但未充分理解的问题。本研究将交互作为此类结构的基本单元，探究神经网络在不同上下文复杂度下如何编码交互，以及这些微观交互模式如何塑造宏观表示能力。

Method: 使用多阶交互量化上下文复杂度，每阶反映评估变量对联合交互效用所需的上下文信息量。基于此进行分层分析，并通过引入损失函数引导模型强调特定阶数的交互。

Result: 发现普遍的交互瓶颈：跨架构和任务，DNNs容易学习低阶和高阶交互，但始终难以充分表示中阶交互。理论证明中阶交互具有最高的上下文变异性，导致梯度方差大，难以学习。强调低阶交互的模型表现出更强的泛化性和鲁棒性，而强调高阶交互的模型则具有更强的结构建模和拟合能力。

Conclusion: 揭示了现代DNNs固有的表示偏置，并确立交互阶数作为解释和指导深度表示的有力视角。微观交互结构与宏观表示行为之间存在明确联系，为理解和调控神经网络表示能力提供了新框架。

Abstract: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.

</details>


### [49] [The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss](https://arxiv.org/abs/2512.18610)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Hao Wang,Zhiqiang Ge,Daoyi Dong,Yong Liu,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出期望优化偏差(EOB)理论，量化点损失函数在时间序列建模中的偏差，并提出基于序列长度缩减和结构正交化的去偏方案。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型使用点损失函数（如MSE）依赖于错误的时间点独立同分布假设，忽略了因果时间结构。虽然这个问题日益受到关注，但缺乏正式的理论基础。

Method: 1. 从第一性原理分析期望优化偏差(EOB)，用信息论形式化真实联合分布与错误i.i.d.对应分布之间的差异；2. 推导线性和非线性系统中非确定性EOB的闭式量化；3. 提出基于序列长度缩减和结构正交化的去偏方案，通过DFT或DWT同时实现这两个原则；4. 提出调和ℓp范数框架来修正高方差序列的梯度问题。

Result: 1. 揭示了基本范式悖论：时间序列越确定性和结构化，点损失函数造成的偏差越严重；2. 证明EOB是内在数据属性，仅由序列长度和提出的结构信噪比(SSNR)决定；3. 广泛实验验证了EOB理论的普适性和去偏方案的优越性能。

Conclusion: 论文首次为时间序列建模中点损失函数的偏差问题提供了正式的理论基础，提出了可量化的EOB理论，并开发了有效的去偏方案，为时间序列建模提供了新的理论框架和实践指导。

Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a flawed point-wise independent and identically distributed (i.i.d.) assumption that disregards the causal temporal structure, an issue with growing awareness yet lacking formal theoretical grounding. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB), formalizing it information-theoretically as the discrepancy between the true joint distribution and its flawed i.i.d. counterpart. Our analysis reveals a fundamental paradigm paradox: the more deterministic and structured the time series, the more severe the bias by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and our proposed Structural Signal-to-Noise Ratio (SSNR). This theoretical diagnosis motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution that simultaneously achieves both principles via DFT or DWT. Furthermore, a novel harmonized $\ell_p$ norm framework is proposed to rectify gradient pathologies of high-variance series. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program.

</details>


### [50] [ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs](https://arxiv.org/abs/2512.18633)
*Han-Seul Jeong,Youngjoon Park,Hyungseok Song,Woohyung Lim*

Main category: cs.LG

TL;DR: ARC框架通过学习解耦的属性表示（内在属性嵌入和上下文交互嵌入），实现车辆路径问题的跨问题泛化，在多种测试场景中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的车辆路径问题具有多样化的属性，需要能够跨问题变体高效泛化的学习方法，以应对不同属性组合带来的挑战。

Method: 提出ARC框架，通过解耦学习将属性表示分解为两部分：内在属性嵌入（捕获不变属性语义）和上下文交互嵌入（捕获属性组合效应），并通过在嵌入空间中强制类比一致性来实现解耦。

Result: ARC在分布内测试、零样本泛化、少样本适应和现实世界基准测试中都达到了最先进的性能。

Conclusion: 通过解耦属性表示学习，ARC能够有效泛化到未见过的属性组合，为车辆路径问题的跨问题学习提供了有效框架。

Abstract: Vehicle Routing Problems (VRPs) with diverse real-world attributes have driven recent interest in cross-problem learning approaches that efficiently generalize across problem variants. We propose ARC (Attribute Representation via Compositional Learning), a cross-problem learning framework that learns disentangled attribute representations by decomposing them into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. This disentanglement is achieved by enforcing analogical consistency in the embedding space to ensure the semantic transformation of adding an attribute (e.g., a length constraint) remains invariant across different problem contexts. This enables our model to reuse invariant semantics across trained variants and construct representations for unseen combinations. ARC achieves state-of-the-art performance across in-distribution, zero-shot generalization, few-shot adaptation, and real-world benchmarks.

</details>


### [51] [From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers](https://arxiv.org/abs/2512.18634)
*Ryotaro Kawata,Yujin Song,Alberto Bietti,Naoki Nishikawa,Taiji Suzuki,Samuel Vaiter,Denny Wu*

Main category: cs.LG

TL;DR: 论文研究了预训练数据分布如何影响浅层Transformer学习归纳头算法还是位置捷径，通过理论分析和实验证明数据多样性决定模型泛化能力


<details>
  <summary>Details</summary>
Motivation: Transformer可以学习通用算法（如归纳头）或简单位置捷径，但预训练数据分布如何引导模型选择哪种行为尚不清楚。本文旨在理解数据分布如何影响Transformer的算法偏好，为控制其学习行为提供指导。

Method: 聚焦最小触发-输出预测任务（复制第二次出现的特殊触发后的token），对单层Transformer进行梯度训练的理论分析。在无限和有限样本情况下，通过"最大-和"比率衡量输入序列多样性，分析模型学习机制转变。还研究了预训练上下文长度与OOD泛化的权衡，推导最小化计算成本的优化预训练分布。

Result: 理论证明：当输入序列多样性足够（触发-触发距离的"最大-和"比率低）时，模型学习归纳头并泛化到未见上下文；当比率高时，模型采用位置捷径且无法OOD泛化。揭示了预训练上下文长度与OOD泛化的权衡关系，并推导了最优预训练分布。合成实验验证了理论预测。

Conclusion: 预训练数据分布对Transformer学习算法行为有决定性影响。数据多样性（低"最大-和"比率）促进归纳头学习和OOD泛化。研究结果为理解Transformer算法偏见提供了理论框架，并为通过数据驱动控制其学习行为提供了概念指导。

Abstract: Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.

</details>


### [52] [Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2512.18670)
*Xue Yang,Michael Schukat,Junlin Lu,Patrick Mannion,Karl Mason,Enda Howley*

Main category: cs.LG

TL;DR: 论文提出DGCRL方法，通过外部自演化的演示库直接指导RL探索，解决持续强化学习中的稳定性-可塑性困境


<details>
  <summary>Details</summary>
Motivation: 传统RL在动态环境中表现不佳，持续强化学习面临稳定性（保留先验知识）与可塑性（获取新知识）的平衡难题。现有方法主要通过优化机制间接影响知识，很少直接影响智能体行为，这阻碍了有效的知识重用和学习效率。

Method: 提出演示引导的持续强化学习（DGCRL），将先验知识存储在外部自演化的演示库中，直接指导RL探索和适应。对每个任务，智能体动态选择最相关的演示，并遵循基于课程学习的策略，从演示引导探索逐步过渡到完全自主探索。

Result: 在2D导航和MuJoCo运动任务上的大量实验表明，该方法具有优越的平均性能、增强的知识迁移能力、减轻遗忘效果和训练效率。额外的敏感性分析和消融研究进一步验证了其有效性。

Conclusion: DGCRL通过外部演示库直接指导探索，有效解决了持续强化学习中的稳定性-可塑性困境，实现了更好的知识重用和学习效率。

Abstract: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.

</details>


### [53] [Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs](https://arxiv.org/abs/2512.18673)
*Ning Lyu,Junjie Jiang,Lu Chang,Chihui Shao,Feng Chen,Chong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种结构感知驱动的调度图建模方法，通过结构引导的调度图构建和多尺度图语义聚合，提高复杂系统调度行为异常识别的准确性和表征能力。


<details>
  <summary>Details</summary>
Motivation: 复杂系统调度行为异常识别面临多任务并发、资源竞争和阶段转换等复杂场景的挑战，现有方法在捕捉全局调度关系和异常特征方面存在不足，需要更有效的建模方法来提高异常检测的准确性和表征能力。

Method: 1. 结构引导的调度图构建机制：整合任务执行阶段、资源节点状态和调度路径信息，构建动态演化的调度行为图；2. 多尺度图语义聚合模块：通过局部邻接语义集成和全局拓扑对齐，实现调度特征的语义一致性建模。

Result: 在真实调度数据集上的实验表明，该方法在多个指标上表现出显著性能优势，对结构扰动和语义变化具有敏感响应。可视化分析显示，在结构引导和语义聚合的共同作用下，调度行为图展现出更强的异常可分性和模式表征能力。

Conclusion: 提出的结构感知驱动调度图建模方法有效提升了调度异常检测的准确性和表征能力，验证了该方法在复杂系统调度异常检测任务中的有效性和适应性。

Abstract: This paper proposes a structure-aware driven scheduling graph modeling method to improve the accuracy and representation capability of anomaly identification in scheduling behaviors of complex systems. The method first designs a structure-guided scheduling graph construction mechanism that integrates task execution stages, resource node states, and scheduling path information to build dynamically evolving scheduling behavior graphs, enhancing the model's ability to capture global scheduling relationships. On this basis, a multi-scale graph semantic aggregation module is introduced to achieve semantic consistency modeling of scheduling features through local adjacency semantic integration and global topology alignment, thereby strengthening the model's capability to capture abnormal features in complex scenarios such as multi-task concurrency, resource competition, and stage transitions. Experiments are conducted on a real scheduling dataset with multiple scheduling disturbance paths set to simulate different types of anomalies, including structural shifts, resource changes, and task delays. The proposed model demonstrates significant performance advantages across multiple metrics, showing a sensitive response to structural disturbances and semantic shifts. Further visualization analysis reveals that, under the combined effect of structure guidance and semantic aggregation, the scheduling behavior graph exhibits stronger anomaly separability and pattern representation, validating the effectiveness and adaptability of the method in scheduling anomaly detection tasks.

</details>


### [54] [Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding](https://arxiv.org/abs/2512.18689)
*Xiangrui Cai,Shaocheng Ma,Lei Cao,Jie Li,Tianyu Liu,Yilin Dong*

Main category: cs.LG

TL;DR: 提出EEG-CSANet，一种用于EEG信号解码的集中式稀疏注意力网络，通过多分支并行架构处理时空异质性，在多个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有固有的时空异质性，需要有效的方法来提取多尺度时空特征并进行高效融合，以提升脑电信号解码的准确性和鲁棒性。

Method: 采用多分支并行架构，每个时间尺度配备独立的空间特征提取模块；提出EEG-CSANet，使用主-辅助分支结构：主分支通过多尺度自注意力建模核心时空模式，辅助分支通过稀疏交叉注意力促进高效局部交互。

Result: 在五个公开数据集（BCIC-IV-2A、BCIC-IV-2B、HGD、SEED、SEED-VIG）上分别达到88.54%、91.09%、99.43%、96.03%、90.56%的准确率，展现了SOTA性能和跨任务的强适应性。

Conclusion: EEG-CSANet在多个EEG解码任务中表现出优异的性能和鲁棒性，有望成为EEG信号解码领域的基准模型，并通过消融研究增强了模型的可解释性。

Abstract: Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet

</details>


### [55] [Generating Risky Samples with Conformity Constraints via Diffusion Models](https://arxiv.org/abs/2512.18722)
*Han Yu,Hao Zou,Xingxuan Zhang,Zhengyi Wang,Yue He,Kehan Li,Peng Cui*

Main category: cs.LG

TL;DR: RiskyDiff：一种利用扩散模型生成高风险样本的新方法，通过文本和图像嵌入作为隐式约束，结合一致性评分来确保样本与目标类别的一致性，从而提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法发现高风险样本的能力受限于现有数据集的覆盖范围，而基于扩散模型的方法在生成样本与目标类别一致性方面存在不足，这会引入标签噪声并限制应用效果。

Method: 提出RiskyDiff方法：1）使用文本和图像嵌入作为隐式约束确保类别一致性；2）设计一致性评分进一步显式强化类别一致性；3）引入嵌入筛选和高风险梯度指导机制提升生成样本的风险性。

Result: 实验表明RiskyDiff在风险程度、生成质量和类别一致性方面显著优于现有方法，且使用高一致性生成样本进行数据增强能有效提升模型的泛化能力。

Conclusion: RiskyDiff通过结合隐式和显式约束解决了扩散模型生成高风险样本时的类别一致性问题，为模型鲁棒性评估和数据增强提供了有效工具。

Abstract: Although neural networks achieve promising performance in many tasks, they may still fail when encountering some examples and bring about risks to applications. To discover risky samples, previous literature attempts to search for patterns of risky samples within existing datasets or inject perturbation into them. Yet in this way the diversity of risky samples is limited by the coverage of existing datasets. To overcome this limitation, recent works adopt diffusion models to produce new risky samples beyond the coverage of existing datasets. However, these methods struggle in the conformity between generated samples and expected categories, which could introduce label noise and severely limit their effectiveness in applications. To address this issue, we propose RiskyDiff that incorporates the embeddings of both texts and images as implicit constraints of category conformity. We also design a conformity score to further explicitly strengthen the category conformity, as well as introduce the mechanisms of embedding screening and risky gradient guidance to boost the risk of generated samples. Extensive experiments reveal that RiskyDiff greatly outperforms existing methods in terms of the degree of risk, generation quality, and conformity with conditioned categories. We also empirically show the generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity.

</details>


### [56] [ML Inference Scheduling with Predictable Latency](https://arxiv.org/abs/2512.18725)
*Haidong Zhao,Nikolaos Georgantas*

Main category: cs.LG

TL;DR: 现有机器学习推理调度系统在GPU干扰预测方面存在不足，本文评估其局限性并提出改进方向


<details>
  <summary>Details</summary>
Motivation: 机器学习推理服务系统需要调度请求以提高GPU利用率并满足SLO或截止时间要求，但提高GPU利用率可能损害延迟敏感型调度，因为并发任务会竞争GPU资源并引入干扰。现有干扰预测方法存在局限性，可能影响调度效果。

Method: 评估现有干扰预测方法的局限性，包括：1）粒度较粗，忽略运行时共置动态；2）使用静态预测模型，难以适应不同工作负载特征。在此基础上，提出实现高效ML推理调度的研究方向。

Result: 识别出现有干扰预测方法的两大主要限制：预测粒度不足和模型静态性，这些限制影响了调度准确性。为改进ML推理调度提供了明确的研究方向。

Conclusion: 现有干扰预测方法在准确性和适应性方面存在不足，需要开发更精细、动态的干扰预测技术来支持高效的机器学习推理调度，以满足SLO和截止时间要求。

Abstract: Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. To this end, we evaluate the potential limitations of existing interference prediction approaches and outline our ongoing work toward achieving efficient ML inference scheduling.

</details>


### [57] [A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models](https://arxiv.org/abs/2512.18730)
*Zhiquan Tan,Yinrong Hong*

Main category: cs.LG

TL;DR: 论文利用KL正则化强化学习的最优策略的闭式能量基模型结构，为LLMs提供统一变分分析，证明指令调优模型满足细致平衡条件，推理模型训练等价于期望KL最小化。


<details>
  <summary>Details</summary>
Motivation: 尽管通过KL正则化强化学习训练的大语言模型展现出强大的指令跟随、自我修正和推理能力，但其理论基础仍然有限。本文旨在填补这一理论空白。

Method: 利用KL正则化最优策略的闭式能量基模型结构，为LLMs提供统一变分分析。对指令调优模型，在奖励势函数和预训练对称性的自然假设下，证明转移核满足细致平衡条件。对推理模型，展示目标等价于期望KL最小化。

Result: 证明了指令调优模型满足细致平衡条件，导致单调KL收敛到高质量稳态分布，有界到达优越状态的命中时间，以及由谱隙控制的指数混合。推理模型训练等价于期望KL最小化，次优性间隙简化为目标与当前准确率之间的伯努利KL散度。

Conclusion: 该理论框架为理解KL正则化强化学习训练的大语言模型提供了统一的分析基础，解释了指令调优模型的收敛特性和推理模型的熵-准确率权衡现象。

Abstract: Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.
  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.
  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.

</details>


### [58] [Is Your Conditional Diffusion Model Actually Denoising?](https://arxiv.org/abs/2512.18736)
*Daniel Pfrommer,Zehao Dou,Christopher Scarvelis,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: 扩散模型在条件生成时偏离理论去噪过程，这种偏差与模型容量和训练数据量无关，源于条件空间不同部分去噪流难以桥接的平滑性归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 研究条件扩散模型（如文本到图像生成、观测到控制策略）的归纳偏置，发现这些模型在条件查询时，其生成过程会偏离理论上的"去噪"过程，导致不同采样算法（如DDPM、DDIM）之间产生不一致。

Method: 引入"调度偏差"作为量化指标来测量偏离标准去噪过程的程度，并提供计算方法。通过理论分析表明这种偏差源于条件空间不同部分去噪流难以桥接的平滑性归纳偏置。

Result: 条件扩散模型在生成时确实会偏离理想去噪过程，这种偏差与模型容量或训练数据量无关。调度偏差能够有效量化这种偏离现象。

Conclusion: 条件扩散模型的归纳偏置导致其在条件生成时偏离理论去噪过程，这种偏差源于模型倾向于平滑连接条件空间不同部分的去噪流，对采样算法的选择和模型理解有重要影响。

Abstract: We study the inductive biases of diffusion models with a conditioning-variable, which have seen widespread application as both text-conditioned generative image models and observation-conditioned continuous control policies. We observe that when these models are queried conditionally, their generations consistently deviate from the idealized "denoising" process upon which diffusion models are formulated, inducing disagreement between popular sampling algorithms (e.g. DDPM, DDIM). We introduce Schedule Deviation, a rigorous measure which captures the rate of deviation from a standard denoising process, and provide a methodology to compute it. Crucially, we demonstrate that the deviation from an idealized denoising process occurs irrespective of the model capacity or amount of training data. We posit that this phenomenon occurs due to the difficulty of bridging distinct denoising flows across different parts of the conditioning space and show theoretically how such a phenomenon can arise through an inductive bias towards smoothness.

</details>


### [59] [PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation](https://arxiv.org/abs/2512.18737)
*Zichuan Lin,Xiaokai Huang,Jiate Liu,Yuxuan Han,Jia Chen,Xiapeng Wu,Deheng Ye*

Main category: cs.LG

TL;DR: PIPCFR是一种新的反事实回归方法，通过纳入治疗后变量改进伪结果插补，显著降低个体治疗效果估计误差


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽视治疗后变量对结果的影响，导致无法完全捕捉结果变异性，增加反事实预测的方差

Method: 提出PIPCFR方法，建立连接治疗后变量与ITE估计精度的新理论边界，学习保留信息成分同时减轻偏差的有效表示

Result: 在真实世界和模拟数据集上的实证评估显示，PIPCFR相比现有方法显著降低了ITE误差

Conclusion: 纳入治疗后变量可以改进伪结果插补，提高个体治疗效果估计的准确性，PIPCFR为此提供了有效的理论和方法框架

Abstract: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.

</details>


### [60] [Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning](https://arxiv.org/abs/2512.18763)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 论文提出将高斯混合模型(GMMs)作为Q函数损失的直接替代物，而非传统的概率密度估计器。这种称为GMM-QFs的参数模型具有强大的表示能力，可作为通用逼近器，并通过黎曼流形优化进行学习，在多种基准RL任务中展现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中GMMs主要用作概率密度函数估计器，本文探索GMMs作为函数逼近器的新角色，旨在提供一种计算效率高且具有强大表示能力的Q函数逼近方法。

Method: 提出GMM-QFs模型，将其嵌入贝尔曼残差中，通过黎曼流形优化学习固定数量的混合权重、高斯均值向量和协方差矩阵参数。该方法将几何视角融入策略评估步骤，无需经验数据。

Result: 理论证明GMM-QFs是通用逼近器。实验表明，即使在没有经验数据的情况下，GMM-QFs在多个基准RL任务中表现出竞争力，有时甚至优于最先进方法，同时计算开销显著小于依赖经验数据的深度学习方法。

Conclusion: GMMs作为Q函数损失的直接替代物具有可行性和有效性，提供了一种计算高效、无需经验数据的强化学习函数逼近方法，扩展了GMMs在RL中的应用范围。

Abstract: Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.

</details>


### [61] [Label-Informed Outlier Detection Based on Granule Density](https://arxiv.org/abs/2512.18774)
*Baiyang Chen,Zhong Yuan,Dezhong Peng,Hongmei Chen,Xiaomin Song,Huiming Zheng*

Main category: cs.LG

TL;DR: 提出一种基于粒计算和模糊集的异构数据异常检测方法GDOF，通过标签引导的模糊粒化处理异构数据，利用少量标记异常样本来评估属性相关性并整合粒度密度进行异常评分。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法通常将数据视为纯数值且确定性处理，忽略了现实世界复杂数据集中固有的异质性和不确定性，需要一种能有效处理异构数据且仅需少量标记异常样本的方法。

Method: 提出粒度密度异常因子(GDOF)方法：1) 使用标签引导的模糊粒化有效表示各种数据类型；2) 开发粒度密度进行精确密度估计；3) 利用少量标记异常样本评估属性相关性，整合各属性的粒度密度进行异常评分。

Result: 在多个真实世界数据集上的实验结果表明，GDOF在异构数据异常检测方面表现突出，仅需少量标记异常样本即可有效工作，优于现有方法。

Conclusion: GDOF将模糊集和粒计算相结合，为复杂多样数据类型的异常检测提供了一个实用框架，能有效处理现实世界数据中的异质性和不确定性。

Abstract: Outlier detection, crucial for identifying unusual patterns with significant implications across numerous applications, has drawn considerable research interest. Existing semi-supervised methods typically treat data as purely numerical and} in a deterministic manner, thereby neglecting the heterogeneity and uncertainty inherent in complex, real-world datasets. This paper introduces a label-informed outlier detection method for heterogeneous data based on Granular Computing and Fuzzy Sets, namely Granule Density-based Outlier Factor (GDOF). Specifically, GDOF first employs label-informed fuzzy granulation to effectively represent various data types and develops granule density for precise density estimation. Subsequently, granule densities from individual attributes are integrated for outlier scoring by assessing attribute relevance with a limited number of labeled outliers. Experimental results on various real-world datasets show that GDOF stands out in detecting outliers in heterogeneous data with a minimal number of labeled outliers. The integration of Fuzzy Sets and Granular Computing in GDOF offers a practical framework for outlier detection in complex and diverse data types. All relevant datasets and source codes are publicly available for further research. This is the author's accepted manuscript of a paper published in IEEE Transactions on Fuzzy Systems. The final version is available at https://doi.org/10.1109/TFUZZ.2024.3514853

</details>


### [62] [Controllable Probabilistic Forecasting with Stochastic Decomposition Layers](https://arxiv.org/abs/2512.18815)
*John S. Schreck,William E. Chapman,Charlie Becker,David John Gagne,Dhamma Kimpara,Nihanth Cherukuru,Judith Berner,Kirsten J. Mayer,Negin Sobhani*

Main category: cs.LG

TL;DR: 提出Stochastic Decomposition Layers (SDL)方法，通过分层噪声注入将确定性ML天气模型转换为概率集合系统，相比传统CRPS方法计算成本更低且具有物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于CRPS的集合方法训练策略和噪声注入机制不统一，大多通过条件归一化全局注入噪声，这增加了训练成本且限制了随机扰动的物理可解释性。

Method: 引入Stochastic Decomposition Layers (SDL)，借鉴StyleGAN的分层噪声注入思想，在三个解码器尺度上通过潜在驱动调制、逐像素噪声和通道缩放应用学习到的扰动。

Result: 应用于WXFormer时，SDL仅需基线模型训练计算成本的不到2%。集合成员从紧凑潜在张量（5 MB）生成，具有完美可重现性。在2022年ERA5再分析数据上评估，集合的spread-skill比接近1，rank直方图在中期预报中趋于均匀，校准性能可与业务IFS-ENS竞争。

Conclusion: SDL提供了一种计算高效、可解释的集合天气预报方法，分层不确定性结构（粗层调制天气尺度模式，细层控制中尺度变率）为业务预报和气候应用提供了可解释的不确定性量化。

Abstract: AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.

</details>


### [63] [Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection](https://arxiv.org/abs/2512.18826)
*Souhail Abdelmouaiz Sadat,Mohamed Yacine Touahria Miliani,Khadidja Hab El Hames,Hamida Seba,Mohammed Haddad*

Main category: cs.LG

TL;DR: 该调查回顾了双曲图嵌入模型，并在异常检测任务上评估它们，突出了其在捕捉复杂结构方面优于欧几里得方法的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索双曲空间在异常检测中的潜力，因为传统欧几里得方法在处理复杂图结构数据时存在局限性，而双曲几何能更好地表示层次结构和复杂关系。

Method: 方法包括对多种双曲图嵌入模型（如HGCAE、P-VAE、HGCN）的系统性评估，并与欧几里得方法（如DOMINANT、GraphSage）在异常检测任务上进行对比分析。

Result: 实验结果显示双曲方法表现优异：P-VAE在Elliptic数据集上达到94%的F1分数，HGCAE在Cora数据集上达到80%的F1分数，而欧几里得方法在处理复杂数据时表现较差。

Conclusion: 结论强调双曲空间在改善异常检测方面具有巨大潜力，并提供了一个开源库以促进该领域的进一步研究。

Abstract: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \textit{HGCAE}, \textit{\(\mathcal{P}\)-VAE}, and \textit{HGCN} demonstrates high performance, with \textit{\(\mathcal{P}\)-VAE} achieving an F1-score of 94\% on the \textit{Elliptic} dataset and \textit{HGCAE} scoring 80\% on \textit{Cora}. In contrast, Euclidean methods like \textit{DOMINANT} and \textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.

</details>


### [64] [Generative Modeling through Spectral Analysis of Koopman Operator](https://arxiv.org/abs/2512.18837)
*Yuanchao Xu,Fengyi Li,Masahiro Fujisawa,Youssef Marzouk,Isao Ishikawa*

Main category: cs.LG

TL;DR: KSWGD是一种结合算子理论谱分析和最优传输的生成建模框架，通过Koopman算子近似从轨迹数据直接估计加速Wasserstein梯度下降所需的谱结构，无需目标势能显式知识或神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein梯度下降方法需要显式目标势能知识或神经网络训练，限制了其在复杂系统中的应用。本文旨在开发一种直接从轨迹数据中学习谱结构的方法，以加速收敛并提高样本质量。

Method: 提出Koopman Spectral Wasserstein Gradient Descent (KSWGD)框架，通过Koopman算子近似从轨迹数据直接估计谱结构，结合最优传输理论，建立与Feynman-Kac理论的联系，提供严格的收敛性分析。

Result: 在多种设置下的实验表明，KSWGD相比现有方法能实现更快的收敛速度，同时保持高样本质量，包括紧致流形采样、亚稳态多阱系统、图像生成和高维随机偏微分方程等场景。

Conclusion: KSWGD通过结合算子理论谱分析和最优传输，提供了一种直接从数据中学习加速结构的有效框架，在保持样本质量的同时显著加速收敛，为复杂系统的生成建模提供了新途径。

Abstract: We propose Koopman Spectral Wasserstein Gradient Descent (KSWGD), a generative modeling framework that combines operator-theoretic spectral analysis with optimal transport. The novel insight is that the spectral structure required for accelerated Wasserstein gradient descent can be directly estimated from trajectory data via Koopman operator approximation which can eliminate the need for explicit knowledge of the target potential or neural network training. We provide rigorous convergence analysis and establish connection to Feynman-Kac theory that clarifies the method's probabilistic foundation. Experiments across diverse settings, including compact manifold sampling, metastable multi-well systems, image generation, and high dimensional stochastic partial differential equation, demonstrate that KSWGD consistently achieves faster convergence than other existing methods while maintaining high sample quality.

</details>


### [65] [Merging of Kolmogorov-Arnold networks trained on disjoint datasets](https://arxiv.org/abs/2512.18921)
*Andrew Polar,Michael Poluektov*

Main category: cs.LG

TL;DR: 论文提出在不相交数据集上训练KANs可加速训练，最佳组合是Newton-Kaczmarz优化方法和分段线性基函数


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究主要关注分布式节点的训练收敛性，而本文专门针对训练加速问题，这取决于优化方法和基函数的选择

Method: 使用不相交数据集训练Kolmogorov-Arnold网络(KANs)，采用Newton-Kaczmarz优化方法和分段线性基函数，通过参数平均进行模型合并

Result: 实验表明在不相交数据集上训练能进一步提升性能，分段线性基函数配合Newton-Kaczmarz方法是目前最快的组合

Conclusion: 不相交数据集训练能有效加速KANs训练，为联邦学习提供了更高效的训练方案，所有代码已公开

Abstract: Training on disjoint datasets can serve two primary goals: accelerating data processing and enabling federated learning. It has already been established that Kolmogorov-Arnold networks (KANs) are particularly well suited for federated learning and can be merged through simple parameter averaging. While the federated learning literature has mostly focused on achieving training convergence across distributed nodes, the present paper specifically targets acceleration of the training, which depends critically on the choice of an optimisation method and the type of the basis functions. To the best knowledge of the authors, the fastest currently-available combination is the Newton-Kaczmarz method and the piecewise-linear basis functions. Here, it is shown that training on disjoint datasets (or disjoint subsets of the training dataset) can further improve the performance. Experimental comparisons are provided, and all corresponding codes are publicly available.

</details>


### [66] [The Ensemble Schr{ö}dinger Bridge filter for Nonlinear Data Assimilation](https://arxiv.org/abs/2512.18928)
*Feng Bao,Hui Sun*

Main category: cs.LG

TL;DR: 提出了一种名为Ensemble Schrödinger Bridge非线性滤波器的新方法，将标准预测过程与扩散生成建模相结合，实现无模型误差、无需导数计算、无需训练且高度并行的滤波算法。


<details>
  <summary>Details</summary>
Motivation: 传统非线性滤波方法（如集合卡尔曼滤波和粒子滤波）在处理高度非线性系统时存在局限性，特别是在高维混沌环境中。需要一种更有效、更稳健的非线性滤波方法。

Method: 将标准预测过程与扩散生成建模相结合，通过Schrödinger Bridge理论实现滤波步骤。该方法无结构模型误差，无需导数计算，无需训练，且具有高度并行性。

Result: 在高达40维或更高的混沌环境中，该算法在高度非线性动态下表现良好。在多个测试中，相比集合卡尔曼滤波和粒子滤波等经典方法，在不同非线性程度下都显示出更好的性能。

Conclusion: Ensemble Schrödinger Bridge非线性滤波器是一种有效的非线性滤波方法，特别适用于高维混沌系统。未来工作将扩展到实际气象应用并建立严格的收敛性分析。

Abstract: This work puts forward a novel nonlinear optimal filter namely the Ensemble Schr{ö}dinger Bridge nonlinear filter. The proposed filter finds marriage of the standard prediction procedure and the diffusion generative modeling for the analysis procedure to realize one filtering step. The designed approach finds no structural model error, and it is derivative free, training free and highly parallizable. Experimental results show that the designed algorithm performs well given highly nonlinear dynamics in (mildly) high dimension up to 40 or above under a chaotic environment. It also shows better performance than classical methods such as the ensemble Kalman filter and the Particle filter in numerous tests given different level of nonlinearity. Future work will focus on extending the proposed approach to practical meteorological applications and establishing a rigorous convergence analysis.

</details>


### [67] [DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems](https://arxiv.org/abs/2512.18932)
*Sarwan Ali*

Main category: cs.LG

TL;DR: DPSR是一个三阶段去噪框架，通过利用评分矩阵的稀疏性、低秩特性和协同模式，在保持差分隐私的同时显著提升推荐质量，甚至在中等隐私预算下超越非隐私基线。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私机制面临隐私-效用权衡的根本挑战：随着隐私预算收紧，推荐质量必然下降。需要一种能同时保护隐私并保持高质量推荐的新方法。

Method: DPSR包含三个阶段：1) 信息论噪声校准，自适应减少高信息评分的噪声；2) 基于协同过滤的去噪，利用物品相似性去除隐私噪声；3) 低秩矩阵补全，利用潜在结构恢复信号。所有去噪操作在噪声注入后进行，通过后处理免疫定理保持差分隐私。

Result: 在隐私预算ε=0.1到10.0范围内，DPSR比最先进的拉普拉斯和高斯机制实现5.57%到9.23%的RMSE改进。在ε=1.0时，DPSR的RMSE为0.9823，甚至优于非隐私基线(1.0983)，表明去噪管道能有效去除数据和隐私噪声。

Conclusion: DPSR通过三阶段去噪框架，在保持差分隐私的同时显著提升推荐质量，打破了传统隐私-效用权衡的限制，甚至能超越非隐私基线的性能，为隐私保护推荐系统提供了新方向。

Abstract: Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.
  DPSR consists of three synergistic stages: (1) \textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.
  Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\% to 9.23\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\varepsilon=0.1$ to $\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.

</details>


### [68] [When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://arxiv.org/abs/2512.18934)
*Michael S. Zhang,Rishi A. Ruia,Arnav Kewalram,Saathvik Dharmapuram,Utkarsh Sharma,Kevin Zhu*

Main category: cs.LG

TL;DR: 量化模型在持续学习中意外优于高精度模型，INT8在计算效率和持续学习动态间达到最佳平衡


<details>
  <summary>Details</summary>
Motivation: 研究量化精度与回放缓冲策略在大型语言模型持续学习中的相互作用，挑战"精度越高越好"的传统观念

Method: 系统研究不同量化精度（FP16、INT8、INT4）与回放缓冲策略在持续学习中的表现，分析量化噪声的隐式正则化效应

Result: 量化模型在后续任务中超越FP16模型8-15%，INT4在代码生成任务中性能接近FP16的两倍；即使0.1%的小回放缓冲也能显著提升知识保留

Conclusion: INT8量化在计算效率和持续学习动态间达到最佳平衡，量化噪声起到隐式正则化作用，防止过拟合新任务梯度

Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.

</details>


### [69] [Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement](https://arxiv.org/abs/2512.18950)
*Saman Forouzandeh,Wei Peng,Parham Moradi,Xinghuo Yu,Mahdi Jalili*

Main category: cs.LG

TL;DR: MACLA是一个将推理与学习解耦的框架，通过冻结大语言模型参数，在外部分层程序记忆中进行所有适应。它提取可重用程序、跟踪可靠性、选择动作并优化程序，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的智能体需要微调模型参数，这既计算成本高又缺乏可解释性。MACLA旨在通过外部程序记忆实现样本高效、可解释且持续改进的智能体，无需更新LLM参数。

Method: 1. 从轨迹中提取可重用程序；2. 通过贝叶斯后验跟踪程序可靠性；3. 通过期望效用评分选择动作；4. 通过对比成功与失败案例来优化程序。所有适应都在外部分层程序记忆中进行，保持LLM冻结。

Result: 在四个基准测试（ALFWorld、WebShop、TravelPlanner、InterCodeSQL）上平均性能达78.1%，优于所有基线。在ALFWorld未见任务上达到90.3%性能，有3.1%的正向泛化。系统在56秒内构建记忆，比最先进的LLM参数训练基线快2800倍，将2851条轨迹压缩为187个程序。

Conclusion: 具有贝叶斯选择和对比优化的结构化外部记忆能够实现样本高效、可解释且持续改进的智能体，无需LLM参数更新。该方法在性能、效率和可扩展性方面都表现出色。

Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.

</details>


### [70] [Learning Through Little Eyes: Attribute Discrimination Beyond Objects](https://arxiv.org/abs/2512.18951)
*Patrick Batsell,Tsutsui Satoshi,Bihan Wen*

Main category: cs.LG

TL;DR: 该研究比较了婴儿视角对比学习模型(CVCL)与CLIP在颜色、大小和纹理属性识别上的差异，发现CVCL更擅长大小识别，CLIP更擅长颜色识别，两者在纹理的视觉-语言对齐上都存在困难。


<details>
  <summary>Details</summary>
Motivation: 婴儿在前两年不仅能学习物体类别，还能学习颜色、大小、纹理等细粒度属性。现有CVCL模型仅关注类别识别，不清楚婴儿尺度的学习是否支持属性识别，因此需要系统评估。

Method: 引入一个系统变化颜色、大小和纹理的基准测试，用于控制性测试类别内属性识别。比较CVCL（基于婴儿自我中心视频训练的CLIP风格模型）与标准CLIP模型在属性识别上的表现。

Result: CVCL在大小识别上表现更好，CLIP在颜色识别上准确率更高。两个模型都能在图像嵌入中表示纹理信息，但都无法将纹理信息与语言空间对齐，表明视觉和语言空间之间存在差距。

Conclusion: 婴儿尺度的学习确实支持属性识别，但不同属性（大小vs颜色）在不同模型中有不同优势。纹理的视觉-语言对齐是一个挑战，揭示了多模态学习中的表征差距。

Abstract: Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.

</details>


### [71] [Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation](https://arxiv.org/abs/2512.18957)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 提出一种在线分布鲁棒强化学习算法，无需先验模型或离线数据，通过与环境交互学习最优鲁棒策略，适用于高维任务，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实应用中常因训练与部署环境不匹配导致性能下降。现有分布鲁棒RL方法通常需要大量先验知识（如生成模型或大型离线数据集），且主要局限于表格方法，难以扩展到复杂领域。

Method: 提出一种在线分布鲁棒强化学习算法，采用通用函数逼近，仅通过与环境的交互学习最优鲁棒策略，无需先验模型或离线数据。该方法在总变差不确定性集下工作。

Result: 算法能够学习到最优鲁棒策略，适用于高维任务。理论分析表明该方法具有接近最优的次线性遗憾界，证明了样本效率和有效性。

Conclusion: 该方法克服了现有分布鲁棒RL的限制，通过在线学习方式实现了无需先验知识的高维任务部署，为强化学习在现实世界应用中的鲁棒性提供了有效解决方案。

Abstract: The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.

</details>


### [72] [Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling](https://arxiv.org/abs/2512.18965)
*Sutashu Tomonaga,Kenji Doya,Noboru Murata*

Main category: cs.LG

TL;DR: 提出了一种基于滞后算子的离散时间结构化状态空间模型（SSMs）直接构建框架，避免了传统连续时间建模和离散化的复杂过程，为设计灵活序列模型提供了新理论工具。


<details>
  <summary>Details</summary>
Motivation: 结构化状态空间模型（SSMs）是Mamba架构的核心，但其理论基础依赖于复杂的连续时间建模和离散化多阶段过程，这可能会模糊直觉理解。需要一种更直接、基于第一原理的离散时间SSMs构建框架。

Method: 引入了一种基于新型滞后算子的直接框架，通过测量系统基函数在时间步之间的"滑动"和变化来几何推导离散时间递推关系。状态矩阵通过该算子的单个内积计算，提供了结合不同基函数和时间扭曲方案的模块化设计空间。

Result: 特定实例能够精确恢复有影响力的HiPPO模型的递推关系，数值模拟验证了推导的正确性，为设计灵活和鲁棒的序列模型提供了新的理论工具。

Conclusion: 提出的基于滞后算子的直接框架为离散时间结构化状态空间模型提供了更直观、灵活和模块化的构建方法，简化了传统复杂建模过程，为序列模型设计开辟了新途径。

Abstract: Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions "slide" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.

</details>


### [73] [Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets](https://arxiv.org/abs/2512.18977)
*Baiyang Chen,Zhong Yuan,Dezhong Peng,Xiaoliang Chen,Hongmei Chen*

Main category: cs.LG

TL;DR: 提出一种基于模糊粗糙集理论的半监督异质数据异常检测算法COD，利用少量标记异常构建标签信息模糊相似关系，结合分类一致性和异常因子进行检测


<details>
  <summary>Details</summary>
Motivation: 当前半监督异常检测方法主要针对数值数据，忽视了数据信息的异质性，需要开发能处理异质数据并利用部分标签监督的方法

Method: 1) 利用少量标记异常构建标签信息模糊相似关系；2) 引入模糊决策系统一致性评估属性对知识分类的贡献；3) 基于模糊相似类定义异常因子，结合分类一致性和异常因子预测异常

Result: 在15个新提出的数据集上广泛评估，实验结果表明COD优于或与领先的异常检测器相当

Conclusion: 提出的COD算法能有效处理异质数据的半监督异常检测问题，利用模糊粗糙集理论结合标签信息和数据一致性，在多个数据集上表现出色

Abstract: Outlier detection aims to find samples that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes' contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 15 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.asoc.2024.112070

</details>


### [74] [Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy](https://arxiv.org/abs/2512.18978)
*Baiyang Chen,Zhong Yuan,Zheng Liu,Dezhong Peng,Yongxiang Li,Chang Liu,Guiduo Duan*

Main category: cs.LG

TL;DR: 提出基于模糊粗糙集的半监督离群点检测方法FROD，有效处理混合属性数据的不确定性和异质性，在16个公开数据集上表现优于或媲美主流算法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督离群点检测方法通常忽略真实世界混合属性数据的不确定性和异质性，需要一种能有效处理这些挑战的方法。

Method: 首先利用少量标记数据构建模糊决策系统，基于模糊近似引入属性分类精度评估属性集贡献；然后使用未标记数据计算模糊相对熵从不确定性角度表征离群点；最后结合属性分类精度和模糊相对熵开发检测算法。

Result: 在16个公开数据集上的实验结果表明，FROD与领先的检测算法相当或更好。

Conclusion: FROD方法能有效处理混合属性数据的不确定性和异质性，为半监督离群点检测提供了新的解决方案。

Abstract: Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373

</details>


### [75] [OPBO: Order-Preserving Bayesian Optimization](https://arxiv.org/abs/2512.18980)
*Wei Peng,Jianchen Hu,Kang Liu,Qiaozhu Zhai*

Main category: cs.LG

TL;DR: 提出OPBO方法，用保持顺序而非数值的神经网络替代高斯过程，解决高维贝叶斯优化问题


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化使用高斯过程作为代理模型，但在高维空间（如超过500维）中会失效，因为高斯过程依赖精确数值拟合，计算复杂度过高

Method: 提出顺序保持贝叶斯优化（OPBO）方法，使用保持目标函数顺序而非数值的神经网络作为代理模型，并从序数集中选择足够好的解而非最优解来降低计算成本

Result: 实验结果表明，对于超过500维的黑盒优化问题，OPBO方法显著优于基于回归神经网络和高斯过程的传统贝叶斯优化方法

Conclusion: OPBO通过保持顺序而非数值的代理模型，有效解决了高维贝叶斯优化问题，在高维空间中比传统方法表现更好

Abstract: Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.

</details>


### [76] [R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression](https://arxiv.org/abs/2512.18986)
*Kun Zhao,Siyuan Dai,Yingying Zhang,Guodong Liu,Pengfei Gu,Chenghua Lin,Paul M. Thompson,Alex Leow,Heng Huang,Lifang He,Liang Zhan,Haoteng Tang*

Main category: cs.LG

TL;DR: R-GenIMA：一种可解释的多模态大语言模型，通过结合ROI视觉Transformer和遗传提示，联合建模结构MRI和SNPs，在阿尔茨海默病早期检测中实现最先进的四分类性能，并提供生物学解释。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测需要整合宏观神经解剖学改变和微观遗传易感性的模型，但现有多模态方法难以对齐这些异质信号。

Method: 提出R-GenIMA模型，采用新颖的ROI视觉Transformer将每个解剖分割的脑区表示为视觉token，将SNP谱编码为结构化文本，通过跨模态注意力机制连接区域萎缩模式与遗传因素。

Result: 在ADNI队列中，R-GenIMA在正常认知、主观记忆关注、轻度认知障碍和AD的四分类中达到最先进性能。模型识别出阶段特异性脑区和基因特征，注意力归因揭示了与GWAS支持的AD风险位点一致的基因富集。

Conclusion: 可解释的多模态AI能够综合成像和遗传学数据揭示机制见解，为临床可部署工具奠定基础，实现更早的风险分层并指导精准治疗策略。

Abstract: Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.

</details>


### [77] [The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results](https://arxiv.org/abs/2512.19007)
*Konstantin Kaulen,Tobias Ladner,Stanley Bak,Christopher Brix,Hai Duong,Thomas Flinkow,Taylor T. Johnson,Lukas Koller,Edoardo Manino,ThanhVu H Nguyen,Haoze Wu*

Main category: cs.LG

TL;DR: VNN-COMP 2025是第六届神经网络验证竞赛，在SAIV/CAV会议上举办，旨在公平比较神经网络验证工具，促进工具接口标准化，汇集验证社区。8个团队参与了16个常规和9个扩展基准测试。


<details>
  <summary>Details</summary>
Motivation: 竞赛旨在促进神经网络验证工具的公平客观比较，推动工具接口标准化，并汇集神经网络验证社区。通过标准化格式、统一硬件环境和公开测试集，确保评估的公正性。

Method: 采用标准化网络格式(ONNX)和规范格式(VNN-LIB)，使用基于AWS实例的自动评估管道确保硬件成本相等，参赛者在最终测试集公开前选择工具参数，确保公平比较。

Result: 2025年竞赛有8个团队参与，测试了16个常规基准和9个扩展基准。报告总结了竞赛规则、基准测试、参与工具、结果和经验教训。

Conclusion: VNN-COMP竞赛成功促进了神经网络验证工具的比较和标准化，通过年度竞赛持续推动该领域的发展，汇集了验证社区并提供了有价值的基准测试平台。

Abstract: This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.

</details>


### [78] [Optimizer Dynamics at the Edge of Stability with Differential Privacy](https://arxiv.org/abs/2512.19019)
*Ayana Hussain,Ricky Fang*

Main category: cs.LG

TL;DR: DP训练改变了神经网络优化动态，但边缘稳定性模式仍然存在，尽管DP通常降低锐度并阻止优化器完全达到经典稳定性阈值


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私（DP）如何改变神经网络训练动态，特别是梯度裁剪和高斯噪声对优化器稳定性模式的影响

Method: 比较梯度下降（GD）和Adam及其隐私保护变体，分析DP训练中梯度裁剪和噪声如何改变锐度和损失演化

Result: DP通常降低锐度并阻止优化器完全达到经典稳定性阈值，但边缘稳定性模式仍然存在，最大学习率和最大隐私预算有时会接近或超过这些阈值

Conclusion: DP在神经网络优化中引入了不可预测性，但经典的稳定性模式在DP训练中仍然持续存在

Abstract: Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.

</details>


### [79] [A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development](https://arxiv.org/abs/2512.19031)
*Yuan Fang,Fabian Waschkowski,Maximilian Reissmann,Richard D. Sandberg,Takuo Oda,Koichi Tanimoto*

Main category: cs.LG

TL;DR: 提出了一种结合代理模型的符号CFD驱动训练框架，通过实时学习ML生成模型的误差来大幅降低训练成本，同时保持与原方法相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: 原始CFD驱动训练框架需要将每个ML生成的候选模型嵌入CFD求解器进行评估，需要数百到数千次高保真模拟，对于复杂流动计算成本过高。

Method: 提出扩展框架，在符号CFD驱动训练中实时集成代理建模：1）代理模型基于先前CFD评估学习ML生成模型的误差近似；2）新模型先用代理评估，仅预测误差小或不确定性高的模型进行完整CFD模拟；3）将离散符号表达式映射到连续空间；4）扩展为多输出代理模型以支持多目标训练。

Result: 在一维和二维统计流动中验证，包括单表达式和多表达式模型优化。在所有情况下，该框架显著降低了训练成本，同时保持了与原CFD驱动方法相当的预测精度。

Conclusion: 提出的代理增强CFD驱动训练框架通过智能选择需要完整CFD评估的模型，有效解决了原始方法计算成本过高的问题，为复杂流动的物理一致性闭包模型开发提供了实用解决方案。

Abstract: Computational Fluid Dynamics (CFD)-driven training combines machine learning (ML) with CFD solvers to develop physically consistent closure models with improved predictive accuracy. In the original framework, each ML-generated candidate model is embedded in a CFD solver and evaluated against reference data, requiring hundreds to thousands of high-fidelity simulations and resulting in prohibitive computational cost for complex flows. To overcome this limitation, we propose an extended framework that integrates surrogate modeling into symbolic CFD-driven training in real time to reduce training cost. The surrogate model learns to approximate the errors of ML-generated models based on previous CFD evaluations and is continuously refined during training. Newly generated models are first assessed using the surrogate, and only those predicted to yield small errors or high uncertainty are subsequently evaluated with full CFD simulations. Discrete expressions generated by symbolic regression are mapped into a continuous space using averaged input-symbol values as inputs to a probabilistic surrogate model. To support multi-objective model training, particularly when fixed weighting of competing quantities is challenging, the surrogate is extended to a multi-output formulation by generalizing the kernel to a matrix form, providing one mean and variance prediction per training objective. Selection metrics based on these probabilistic outputs are used to identify an optimal training setup. The proposed surrogate-augmented CFD-driven training framework is demonstrated across a range of statistically one- and two-dimensional flows, including both single- and multi-expression model optimization. In all cases, the framework substantially reduces training cost while maintaining predictive accuracy comparable to that of the original CFD-driven approach.

</details>


### [80] [Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building](https://arxiv.org/abs/2512.19038)
*Liping Sun,Yucheng Guo,Siliang Lu,Zhenzhen Li*

Main category: cs.LG

TL;DR: 开发时间序列预测模型，预测美国建筑区域空气温度，支持2周时间跨度的预测，用于HVAC系统智能控制和混合建筑能耗建模。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化导致极端天气频发，需要更节能、灵活的HVAC系统控制。传统仿真方法如EnergyPlus和DOE2存在局限性，需要结合物理和数据驱动的混合方法。现有研究在短期和长期预测方面仍有空白。

Method: 采用结合物理和数据驱动的混合方法，开发时间序列预测模型，针对美国建筑区域空气温度进行2周时间跨度的预测。

Result: 开发了能够预测建筑区域空气温度的模型，支持2周时间跨度的预测。

Conclusion: 该模型可进一步改进以支持HVAC系统的智能控制和运行（需求灵活性），并可用于混合建筑能耗建模。

Abstract: With the press of global climate change, extreme weather and sudden weather changes are becoming increasingly common. To maintain a comfortable indoor environment and minimize the contribution of the building to climate change as much as possible, higher requirements are placed on the operation and control of HVAC systems, e.g., more energy-efficient and flexible to response to the rapid change of weather. This places demands on the rapid modeling and prediction of zone air temperatures of buildings. Compared to the traditional simulation-based approach such as EnergyPlus and DOE2, a hybrid approach combined physics and data-driven is more suitable. Recently, the availability of high-quality datasets and algorithmic breakthroughs have driven a considerable amount of work in this field. However, in the niche of short- and long-term predictions, there are still some gaps in existing research. This paper aims to develop a time series forecast model to predict the zone air temperature in a building located in America on a 2-week horizon. The findings could be further improved to support intelligent control and operation of HVAC systems (i.e. demand flexibility) and could also be used as hybrid building energy modeling.

</details>


### [81] [Efficient Personalization of Generative Models via Optimal Experimental Design](https://arxiv.org/abs/2512.19057)
*Guy Schacht,Ziyad Sheebaelhamd,Riccardo De Santi,Mojmír Mutný,Andreas Krause*

Main category: cs.LG

TL;DR: 提出ED-PBRL算法，利用最优实验设计选择信息量最大的偏好查询，高效学习用户潜在奖励函数，减少人工反馈需求


<details>
  <summary>Details</summary>
Motivation: 人类反馈获取成本高、耗时长，需要数据高效的查询选择方法来减少对齐生成模型所需的人工偏好数据

Method: 将偏好查询选择问题形式化为最大化潜在偏好模型信息获取的凸优化问题，提出ED-PBRL算法，支持高效构建结构化查询（如图像、文本）

Result: 在个性化文本到图像生成模型任务中，相比随机查询选择，该方法需要更少的偏好查询就能学习用户特定风格

Conclusion: 基于最优实验设计的偏好查询选择方法能显著提高数据效率，减少对齐生成模型所需的人工反馈量

Abstract: Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.

</details>


### [82] [Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation](https://arxiv.org/abs/2512.19061)
*Chi Liu*

Main category: cs.LG

TL;DR: 提出基于图变换的欺诈检测框架，通过区分硬链接和软链接，将大规模异构图转换为适合聚类的小规模加权图，显著提升检测覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 协同欺诈中多个欺诈账户形成复杂网络结构，传统方法要么依赖高置信度身份链接导致覆盖有限，要么使用所有链接导致图碎片化、聚类效果差，需要解决大规模异构图聚类的挑战。

Method: 1) 区分硬链接（高置信度身份关系如电话号码、信用卡、身份证）和软链接（行为关联如设备指纹、cookie、IP地址）；2) 通过硬链接识别连通分量并合并为超节点；3) 重建加权软链接图；4) 使用LINE进行表示学习；5) 使用HDBSCAN进行密度聚类。

Result: 在真实支付平台数据集上：1) 图规模从2500万节点减少到770万节点；2) 检测覆盖率比仅使用硬链接的基线方法提高一倍；3) 在识别的欺诈集群中保持高精度。

Conclusion: 该框架为工业级欺诈检测系统提供了可扩展的实用解决方案，通过图变换方法有效解决了大规模异构图聚类的挑战，显著提升了欺诈检测的覆盖率和效率。

Abstract: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.

</details>


### [83] [DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale](https://arxiv.org/abs/2512.19097)
*Danny Dongyeop Han,Yonghyeon Gwon,Ahhyun Lucy Lee,Taeyang Lee,Seong Jin Lee,Jubin Choi,Sebin Lee,Jihyun Bang,Seungju Lee,David Keetae Park,Shinjae Yoo,Chun Kee Chung,Jiook Cha*

Main category: cs.LG

TL;DR: DIVER-1是首个大规模脑电信号基础模型家族，包含iEEG和EEG模型，在最大规模数据集上训练，参数达18.2亿，通过系统化缩放定律分析发现数据受限的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 现有脑电信号基础模型规模有限，而缩放已被证明能提升性能。需要建立大规模、多样化的脑电基础模型来推动神经科学、脑机接口和临床应用的发展。

Method: 1) 使用最大规模数据集：5.3k小时iEEG和54k小时EEG（来自17.7k+受试者）；2) 模型参数达18.2亿；3) 引入架构创新：任意变量注意力、滑动时间条件位置编码、多域重建；4) 进行系统化缩放定律分析。

Result: 发现数据受限的缩放定律：在给定数据和计算资源下，较小模型长时间训练优于大模型短期训练；iEEG和EEG模型分别在各自基准测试中达到最先进性能。

Conclusion: DIVER-1建立了脑电基础模型开发的高效缩放和资源分配指南，挑战了先前强调模型大小而非训练时长的做法，为领域提供了系统化的缩放框架。

Abstract: Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.

</details>


### [84] [Dual Model Deep Learning for Alzheimer Prognostication](https://arxiv.org/abs/2512.19099)
*Alireza Moayedikia,Sara Fin,Uffe Kock Wiil*

Main category: cs.LG

TL;DR: PROGRESS是一个深度学习框架，仅使用单次基线脑脊液生物标志物评估，无需纵向数据即可预测阿尔茨海默病认知衰退轨迹和痴呆转化时间，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病预测模型需要纵向观察数据且缺乏不确定性量化，在关键的首次就诊时无法为治疗决策提供支持，而治疗时机决策对疾病修饰疗法至关重要。

Method: 开发了PROGRESS双模型深度学习框架：1）概率轨迹网络预测个体化认知衰退并提供校准的不确定性边界；2）深度生存模型估计从轻度认知障碍到痴呆的转化时间。使用NACC数据库中43个研究中心的3000多名参与者数据，通过留一中心验证评估泛化能力。

Result: PROGRESS在生存预测方面显著优于Cox比例风险、随机生存森林和梯度提升方法。风险分层显示患者组间转化率存在7倍差异，留一中心验证显示在不同测量条件下具有强泛化能力，不确定性边界达到接近标称覆盖率。

Conclusion: PROGRESS通过结合优越的生存预测和可信的轨迹不确定性量化，填补了生物标志物测量与个体化临床决策之间的空白，支持基于单次基线评估的治疗优先排序。

Abstract: Disease modifying therapies for Alzheimer's disease demand precise timing decisions, yet current predictive models require longitudinal observations and provide no uncertainty quantification, rendering them impractical at the critical first visit when treatment decisions must be made. We developed PROGRESS (PRognostic Generalization from REsting Static Signatures), a dual-model deep learning framework that transforms a single baseline cerebrospinal fluid biomarker assessment into actionable prognostic estimates without requiring prior clinical history. The framework addresses two complementary clinical questions: a probabilistic trajectory network predicts individualized cognitive decline with calibrated uncertainty bounds achieving near-nominal coverage, enabling honest prognostic communication; and a deep survival model estimates time to conversion from mild cognitive impairment to dementia. Using data from over 3,000 participants across 43 Alzheimer's Disease Research Centers in the National Alzheimer's Coordinating Center database, PROGRESS substantially outperforms Cox proportional hazards, Random Survival Forests, and gradient boosting methods for survival prediction. Risk stratification identifies patient groups with seven-fold differences in conversion rates, enabling clinically meaningful treatment prioritization. Leave-one-center-out validation demonstrates robust generalizability, with survival discrimination remaining strong across held-out sites despite heterogeneous measurement conditions spanning four decades of assay technologies. By combining superior survival prediction with trustworthy trajectory uncertainty quantification, PROGRESS bridges the gap between biomarker measurement and personalized clinical decision-making.

</details>


### [85] [HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction](https://arxiv.org/abs/2512.19114)
*Haoyu Jiang,Boan Qu,Junjie Zhu,Fanjie Zeng,Xiaojie Lin,Wei Zhong*

Main category: cs.LG

TL;DR: HyperLoad是一个利用预训练大语言模型解决绿色数据中心小样本负载预测问题的跨模态框架，通过知识对齐和多尺度特征建模提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展导致数据中心能耗和碳排放激增，绿色数据中心需要分钟级协调可再生能源、储能和负载，但现有方法难以应对冷启动、负载失真、数据碎片化和分布偏移等小样本场景挑战。

Method: 提出HyperLoad框架：1)跨模态知识对齐阶段，将文本先验和时间序列数据映射到共同潜在空间；2)多尺度特征建模阶段，通过自适应前缀调优注入领域对齐先验，使用增强全局交互注意力机制捕捉跨设备时序依赖。

Result: 在公开数据集DCData上，无论是数据充足还是数据稀缺场景，HyperLoad均持续超越现有最优基线方法，验证了其在可持续绿色数据中心管理中的实用性。

Conclusion: HyperLoad通过利用预训练LLMs的跨模态能力，有效解决了绿色数据中心负载预测中的数据稀缺问题，为实现可持续数据中心管理提供了实用解决方案。

Abstract: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.

</details>


### [86] [A Composable Channel-Adaptive Architecture for Seizure Classification](https://arxiv.org/abs/2512.19123)
*Francesco Carzaniga,Michael Hersche,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出一种通道自适应架构，可处理任意通道数的多变量时间序列（特别是颅内脑电图），通过预训练和快速微调实现跨受试者训练，在癫痫检测任务中超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在处理多通道iEEG数据时的局限性：1）无法适应不同受试者间通道数量的差异；2）缺乏足够的临床相关时间上下文；3）需要大量数据和训练时间进行个性化。

Method: 1）对每个单通道独立应用最先进模型处理；2）使用向量符号算法融合特征，通过可训练标量重建空间关系；3）在长达2分钟的长时记忆中累积融合特征进行分类；4）先在大规模多受试者iEEG语料库上预训练，再通过快速微调个性化到每个受试者。

Result: 在癫痫检测任务中，CA-EEGWaveNet仅用测试受试者的单个癫痫发作训练，而基线EEGWaveNet使用除一个外的所有数据训练，CA-EEGWaveNet仍超越基线（中位F1分数0.78 vs 0.76）。CA-EEGNet也超越其基线（0.79 vs 0.74）。微调所需数据量相当或更少，但时间仅为现有模型的1/5。

Conclusion: CA模型解决了两个关键问题：1）通道自适应，可在不同受试者间训练而不损失性能；2）将有效时间上下文扩展到临床相关长度。该模型可作为现有模型的直接替代品，带来更好的特性和全面性能提升。

Abstract: Objective: We develop a channel-adaptive (CA) architecture that seamlessly processes multi-variate time-series with an arbitrary number of channels, and in particular intracranial electroencephalography (iEEG) recordings. Methods: Our CA architecture first processes the iEEG signal using state-of-the-art models applied to each single channel independently. The resulting features are then fused using a vector-symbolic algorithm which reconstructs the spatial relationship using a trainable scalar per channel. Finally, the fused features are accumulated in a long-term memory of up to 2 minutes to perform the classification. Each CA-model can then be pre-trained on a large corpus of iEEG recordings from multiple heterogeneous subjects. The pre-trained model is personalized to each subject via a quick fine-tuning routine, which uses equal or lower amounts of data compared to existing state-of-the-art models, but requiring only 1/5 of the time. Results: We evaluate our CA-models on a seizure detection task both on a short-term (~20 hours) and a long-term (~2500 hours) dataset. In particular, our CA-EEGWaveNet is trained on a single seizure of the tested subject, while the baseline EEGWaveNet is trained on all but one. Even in this challenging scenario, our CA-EEGWaveNet surpasses the baseline in median F1-score (0.78 vs 0.76). Similarly, CA-EEGNet based on EEGNet, also surpasses its baseline in median F1-score (0.79 vs 0.74). Conclusion and significance: Our CA-model addresses two issues: first, it is channel-adaptive and can therefore be trained across heterogeneous subjects without loss of performance; second, it increases the effective temporal context size to a clinically-relevant length. Therefore, our model is a drop-in replacement for existing models, bringing better characteristics and performance across the board.

</details>


### [87] [A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage](https://arxiv.org/abs/2512.19142)
*Francis Bach*

Main category: cs.LG

TL;DR: 提出基于Choquet积分的凸损失函数，用于集合预测的监督学习，优化条件概率覆盖与集合"大小"之间的权衡


<details>
  <summary>Details</summary>
Motivation: 在监督学习中，集合预测可以提供明确的不确定性估计。现有方法通常关注边际覆盖，但需要优化条件覆盖与集合大小之间的权衡，特别是在非对称损失场景下。

Method: 使用Choquet积分（Lovász扩展）为实值函数水平集得到的非递减子集值函数设计凸损失函数。扩展了非对称损失下的二元分类损失函数，提出基于随机梯度下降或重加权最小二乘的高效优化算法。

Result: 在分类和回归任务的合成数据集实验中，相比追求边际覆盖的方法，所提方法表现出改进效果，能够优化条件覆盖并获得适当大小的预测集合。

Conclusion: 提出的基于Choquet积分的凸损失函数框架能够有效处理集合预测问题，在条件覆盖与集合大小之间实现最优权衡，适用于各种监督学习任务。

Abstract: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{á}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.

</details>


### [88] [RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling](https://arxiv.org/abs/2512.19147)
*Haoran Yang,Yinan Zhang,Wenjie Zhang,Dongxia Wang,Peiyu Liu,Yuqi Ye,Kexin Chen,Wenhai Wang*

Main category: cs.LG

TL;DR: 提出RP-CATE模型，通过通道注意力、循环感知机模块和伪图像/序列数据转换，解决工业混合建模中机器学习架构不全面和数据关联利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业混合建模方法存在两个主要局限：1) 仅使用单一机器学习方法处理特定任务，缺乏适合建模任务的综合架构；2) 工业数据中的潜在关联（如单调性、周期性）未被充分利用，影响预测性能。

Method: 提出RP-CATE模型：1) 用通道注意力替代自注意力机制，并加入循环感知机模块；2) 提出伪图像数据及其生成方法；3) 提出伪序列数据概念及转换方法，使模型能更好捕捉数据关联。

Result: 在化学工程混合建模实验中，RP-CATE相比其他基线模型取得了最佳性能。

Conclusion: RP-CATE通过创新的架构设计和数据处理方法，有效解决了工业混合建模中的现有局限，提升了模型性能。

Abstract: Nowadays, industrial hybrid modeling which integrates both mechanistic modeling and machine learning-based modeling techniques has attracted increasing interest from scholars due to its high accuracy, low computational cost, and satisfactory interpretability. Nevertheless, the existing industrial hybrid modeling methods still face two main limitations. First, current research has mainly focused on applying a single machine learning method to one specific task, failing to develop a comprehensive machine learning architecture suitable for modeling tasks, which limits their ability to effectively represent complex industrial scenarios. Second, industrial datasets often contain underlying associations (e.g., monotonicity or periodicity) that are not adequately exploited by current research, which can degrade model's predictive performance. To address these limitations, this paper proposes the Recurrent Perceptron-based Channel Attention Transformer Encoder (RP-CATE), with three distinctive characteristics: 1: We developed a novel architecture by replacing the self-attention mechanism with channel attention and incorporating our proposed Recurrent Perceptron (RP) Module into Transformer, achieving enhanced effectiveness for industrial modeling tasks compared to the original Transformer. 2: We proposed a new data type called Pseudo-Image Data (PID) tailored for channel attention requirements and developed a cyclic sliding window method for generating PID. 3: We introduced the concept of Pseudo-Sequential Data (PSD) and a method for converting industrial datasets into PSD, which enables the RP Module to capture the underlying associations within industrial dataset more effectively. An experiment aimed at hybrid modeling in chemical engineering was conducted by using RP-CATE and the experimental results demonstrate that RP-CATE achieves the best performance compared to other baseline models.

</details>


### [89] [Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments](https://arxiv.org/abs/2512.19154)
*Geraud Nangue Tasse,Matthew Riemer,Benjamin Rosman,Tim Klinger*

Main category: cs.LG

TL;DR: 提出Adaptive Stacking元算法，通过维护小型自适应记忆堆栈来处理高度非马尔可夫环境，显著降低计算和内存需求


<details>
  <summary>Details</summary>
Motivation: 现实世界智能体部署面临高度非马尔可夫观测依赖问题，传统帧堆叠方法需要大窗口导致计算和内存成本过高。作者发现许多环境中，环境只因果依赖于相对少量的观测，因此需要自适应记忆管理方法

Method: 提出Adaptive Stacking元算法，维护小型自适应记忆堆栈，能够表达高度非马尔可夫时间依赖，同时减少每步考虑的观测数量。算法具有收敛保证，并量化了MLP、LSTM和Transformer智能体的计算和内存节省

Result: 实验使用流行的记忆任务，控制非马尔可夫依赖程度。结果表明适当元算法能够学习移除对未来奖励无预测性的记忆，同时不过度移除重要经验

Conclusion: Adaptive Stacking元算法有效解决了高度非马尔可夫环境中的计算和内存挑战，通过自适应记忆管理实现了显著效率提升

Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking

</details>


### [90] [Practical Quantum-Classical Feature Fusion for complex data Classification](https://arxiv.org/abs/2512.19180)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi*

Main category: cs.LG

TL;DR: 提出一种跨注意力中融合架构，将量子特征与经典神经网络通过注意力机制融合，在多个数据集上表现优于纯量子模型和标准混合模型


<details>
  <summary>Details</summary>
Motivation: 现有混合量子经典学习方法通常将量子电路作为孤立特征提取器，通过直接拼接方式与经典表示合并，忽略了量子与经典分支是不同计算模态，限制了在复杂高维表格和半结构化数据上的可靠性能

Method: 提出多模态混合学习框架，采用跨注意力中融合架构，其中经典表示通过带有残差连接的注意力块查询量子派生特征标记；量子分支保持在实用NISQ预算内，最多使用9个量子比特

Result: 在Wine、Breast Cancer、Forest CoverType、FashionMNIST和SteelPlatesFaults数据集上评估，纯量子模型和标准混合设计由于测量引起的信息丢失而表现不佳，而跨注意力中融合模型在大多数情况下对更复杂数据集有性能提升

Conclusion: 量子派生信息通过原则性多模态融合集成时最有价值，而不是孤立使用或松散附加到经典特征上

Abstract: Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.

</details>


### [91] [Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning](https://arxiv.org/abs/2512.19184)
*Mahdi Mohammadigohari,Giuseppe Di Fatta,Giuseppe Nicosia,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 该论文提出了向量值神经网络和深度核方法的新泛化界，通过算子理论框架专注于多任务学习，结合Koopman方法获得比传统范数界更紧的泛化保证，并引入草图技术解决计算挑战。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在深度学习架构中的泛化特性研究相对较少，现有方法主要基于范数的泛化界不够紧，且Koopman方法存在计算挑战，需要开发新的理论框架来提供更优的泛化保证。

Method: 结合Koopman方法与现有技术，引入草图技术处理向量值神经网络的计算问题，提出深度向量值再生核希尔伯特空间(vvRKHS)框架，利用Perron-Frobenius算子增强深度核方法，并开发核细化策略。

Result: 获得了比传统范数界更紧的泛化保证，在通用Lipschitz损失下得到超额风险界，适用于鲁棒和多重分位数回归，提出了新的Rademacher泛化界，并通过核细化策略解决欠拟合和过拟合问题。

Conclusion: 该工作为多任务学习在深度学习架构中的泛化特性提供了新的理论见解，开发了具有计算效率的算子理论框架，为向量值神经网络和深度核方法提供了更强的理论保证。

Abstract: This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.

</details>


### [92] [Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction](https://arxiv.org/abs/2512.19194)
*Leming Zhou,Zuo Wang,Zhigang Liu*

Main category: cs.LG

TL;DR: 提出一种基于因果异构图表示学习的COPD共病风险预测方法，通过结合因果推理与异构图学习，提高基层医疗对COPD急性加重的早期识别能力。


<details>
  <summary>Details</summary>
Motivation: 基层医疗对慢性阻塞性肺疾病（COPD）急性加重的早期识别和预警能力不足，导致患病率高、负担重但筛查率低，需要改进这一现状。

Method: 开发了因果异构图表示学习（CHGRL）方法：1）构建包含患者与疾病交互的异构图数据集；2）建立因果感知的异构图学习架构，将因果推理机制与异构图学习结合；3）在模型设计中加入因果损失函数，在交叉熵分类损失基础上增加反事实推理学习损失和因果正则化损失。

Result: 与强基线GNN方法相比，提出的模型表现出较高的检测准确性。

Conclusion: 该方法能有效提高COPD共病风险预测的准确性，有助于改善基层医疗对COPD急性加重的早期识别和预警能力。

Abstract: Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.

</details>


### [93] [On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning](https://arxiv.org/abs/2512.19199)
*Mahdi Mohammadigohari,Giuseppe Di Fatta,Giuseppe Nicosia,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 该论文使用算子理论技术为多任务深度神经网络建立了泛化界，通过利用权重矩阵的小条件数和引入定制的Sobolev空间作为扩展假设空间，提出了比传统范数方法更紧的界。


<details>
  <summary>Details</summary>
Motivation: 现有基于范数的多任务深度学习泛化界可能不够紧，特别是在权重矩阵条件数较小的情况下。作者希望利用算子理论技术获得更精确的理论理解，特别是在核方法背景下。

Method: 使用算子理论技术，利用权重矩阵的小条件数，引入定制的Sobolev空间作为扩展假设空间，推导出比传统范数方法更紧的泛化界。

Result: 提出的泛化界比现有基于Koopman的界更优，即使在单输出设置下也有效。该框架保持了灵活性、与网络宽度无关等关键优势。

Conclusion: 该工作为多任务深度学习提供了更精确的理论理解，特别是在核方法背景下，通过算子理论技术获得了比传统方法更紧的泛化界。

Abstract: The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.

</details>


### [94] [MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning](https://arxiv.org/abs/2512.19206)
*Tao Zhang,Ziqian Zeng,Hao Peng,Huiping Zhuang,Cen Chen*

Main category: cs.LG

TL;DR: MixKVQ：一种轻量级、查询感知的KV缓存量化方法，通过识别关键通道并应用逐令牌量化，在保持推理性能的同时大幅减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理显著提升了LLM能力，但带来了巨大的KV缓存内存和延迟开销。现有低比特量化方法在复杂推理任务上性能下降严重，需要更有效的量化策略。

Method: 提出MixKVQ方法，基于两个关键因素设计：1）关键通道的内在量化难度；2）其与查询的相关性。使用轻量级查询感知算法识别需要高精度的关键通道，同时对值缓存应用逐令牌量化。

Result: 在复杂推理数据集上的实验表明，该方法显著优于现有低比特方法，在显著减少内存占用的同时，性能可与全精度基线相媲美。

Conclusion: MixKVQ是一种即插即用的有效KV缓存量化方法，通过综合考虑量化难度和查询相关性，实现了在保持推理性能的同时大幅降低内存开销。

Abstract: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.

</details>


### [95] [Phase-space entropy at acquisition reflects downstream learnability](https://arxiv.org/abs/2512.19223)
*Xiu-Cheng Wang,Jun-Jie Zhanga,Nan Cheng,Long-Gang Pang,Taijiao Du,Deyu Meng*

Main category: cs.LG

TL;DR: 提出基于仪器分辨相空间的采集级标量ΔS_B，量化采集如何混合或移除联合空间-频率结构，无需训练即可预测下游学习性能


<details>
  <summary>Details</summary>
Motivation: 现代学习系统处理跨领域数据，但都依赖于测量中已有的结构。需要一种通用的、模态无关的方法来量化采集本身如何保留或破坏下游学习者可用的信息。

Method: 提出基于仪器分辨相空间的采集级标量ΔS_B，直接量化采集如何混合或移除仪器尺度的联合空间-频率结构。该方法无需训练即可评估采样几何。

Result: ΔS_B理论上正确识别周期性采样的相空间相干性作为混叠的物理来源，恢复经典采样定理结果。在掩码图像分类、加速MRI和大规模MIMO等实验中，|ΔS_B|能一致地排序采样几何并预测下游重建/识别难度。

Conclusion: 采集阶段的相空间熵反映了下游可学习性，能够在训练前选择候选采样策略，并作为跨模态信息保留的共享概念。

Abstract: Modern learning systems work with data that vary widely across domains, but they all ultimately depend on how much structure is already present in the measurements before any model is trained. This raises a basic question: is there a general, modality-agnostic way to quantify how acquisition itself preserves or destroys the information that downstream learners could use? Here we propose an acquisition-level scalar $ΔS_{\mathcal B}$ based on instrument-resolved phase space. Unlike pixelwise distortion or purely spectral errors that often saturate under aggressive undersampling, $ΔS_{\mathcal B}$ directly quantifies how acquisition mixes or removes joint space--frequency structure at the instrument scale. We show theoretically that \(ΔS_{\mathcal B}\) correctly identifies the phase-space coherence of periodic sampling as the physical source of aliasing, recovering classical sampling-theorem consequences. Empirically, across masked image classification, accelerated MRI, and massive MIMO (including over-the-air measurements), $|ΔS_{\mathcal B}|$ consistently ranks sampling geometries and predicts downstream reconstruction/recognition difficulty \emph{without training}. In particular, minimizing $|ΔS_{\mathcal B}|$ enables zero-training selection of variable-density MRI mask parameters that matches designs tuned by conventional pre-reconstruction criteria. These results suggest that phase-space entropy at acquisition reflects downstream learnability, enabling pre-training selection of candidate sampling policies and as a shared notion of information preservation across modalities.

</details>


### [96] [Regression generation adversarial network based on dual data evaluation strategy for industrial application](https://arxiv.org/abs/2512.19232)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: 提出基于多任务学习的回归GAN框架，通过集成回归信息到判别器和生成器，并采用浅层共享机制，解决工业软测量中数据不足问题，同时提升生成样本质量和算法效率。


<details>
  <summary>Details</summary>
Motivation: 工业软测量中数据不足问题影响可靠性，传统GAN未考虑标签与特征的映射关系，现有方法未能同时兼顾性能与效率。

Method: 提出多任务学习回归GAN框架：1)将回归信息集成到判别器和生成器；2)实现判别器与回归器的浅层共享机制；3)设计双数据评估策略，考虑训练样本和生成样本的重要性。

Result: 在四个工业软测量案例（污水处理厂、地表水、CO₂吸收塔、工业燃气轮机）上验证了方法的优越性，显著提升生成样本质量并提高算法运行效率。

Conclusion: 该方法能有效解决工业软测量数据不足问题，通过集成回归信息和浅层共享机制，在提升生成样本质量的同时保证算法效率，增强后续建模的泛化能力。

Abstract: Soft sensing infers hard-to-measure data through a large number of easily obtainable variables. However, in complex industrial scenarios, the issue of insufficient data volume persists, which diminishes the reliability of soft sensing. Generative Adversarial Networks (GAN) are one of the effective solutions for addressing insufficient samples. Nevertheless, traditional GAN fail to account for the mapping relationship between labels and features, which limits further performance improvement. Although some studies have proposed solutions, none have considered both performance and efficiency simultaneously. To address these problems, this paper proposes the multi-task learning-based regression GAN framework that integrates regression information into both the discriminator and generator, and implements a shallow sharing mechanism between the discriminator and regressor. This approach significantly enhances the quality of generated samples while improving the algorithm's operational efficiency. Moreover, considering the importance of training samples and generated samples, a dual data evaluation strategy is designed to make GAN generate more diverse samples, thereby increasing the generalization of subsequent modeling. The superiority of method is validated through four classic industrial soft sensing cases: wastewater treatment plants, surface water, $CO_2$ absorption towers, and industrial gas turbines.

</details>


### [97] [From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis](https://arxiv.org/abs/2512.19246)
*Moncef Garouani,Ayah Barhrhouj*

Main category: cs.LG

TL;DR: MetaSHAP：一种基于元学习和Shapley值的可解释AI方法，用于提供可操作的超参数调优洞察，帮助理解超参数重要性、交互作用和有效取值范围。


<details>
  <summary>Details</summary>
Motivation: 超参数调优是机器学习模型优化的关键步骤，但计算成本高昂。除了优化外，理解超参数的相对重要性和交互作用对于高效模型开发至关重要。现有方法缺乏可解释性和数据集感知能力。

Method: MetaSHAP结合元学习和Shapley值分析，基于超过900万个评估过的机器学习管道基准数据。对于给定算法和数据集，它从历史配置中学习代理性能模型，使用基于SHAP的分析计算超参数交互作用，并从最有影响力的超参数中推导出可解释的调优范围。

Result: 在164个分类数据集和14个分类器的多样化基准上实证验证，MetaSHAP能产生可靠的重要性排名，并且在指导贝叶斯优化时展现出有竞争力的性能。

Conclusion: MetaSHAP为超参数调优提供了可扩展、半自动化的可解释AI方法，不仅帮助确定哪些超参数需要调优，还能理解它们的方向性和交互作用，从而支持更高效的模型开发。

Abstract: Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.

</details>


### [98] [Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems](https://arxiv.org/abs/2512.19250)
*Prathamesh Devadiga*

Main category: cs.LG

TL;DR: 该论文评估了小型语言模型（约10亿参数）在编译器自动并行化任务中的表现，相比传统基于启发式规则的编译器，在11个真实世界内核上取得了平均6.81倍加速，峰值达到43.25倍。


<details>
  <summary>Details</summary>
Motivation: 传统自动并行化编译器依赖僵化的启发式规则，难以应对现代异构系统的复杂性。需要探索基于语言模型的智能编译器优化方法。

Method: 使用三种小型语言模型（gemma3、llama3.2、qwen2.5）和六种推理策略，在11个真实世界内核（来自科学计算、图算法和机器学习）上进行自动并行化评估。系统与LLVM Polly、TVM和Triton等强基线编译器进行对比。

Result: 在376次评估中，该方法平均加速比为6.81倍，在卷积操作上达到峰值性能43.25倍。验证了正确性、可扩展性，并在不同编译器和硬件平台上表现出鲁棒性。

Conclusion: 小型高效的语言模型可以作为复杂编译器优化任务的强大推理引擎，为编译器设计提供了新的方向。

Abstract: Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.

</details>


### [99] [Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study](https://arxiv.org/abs/2512.19253)
*Carla Crivoi,Radu Tudor Ionescu*

Main category: cs.LG

TL;DR: 首次对混合量子-经典神经网络中的机器遗忘进行实证研究，发现量子模型支持有效遗忘，但结果受电路深度、纠缠结构和任务复杂度影响显著


<details>
  <summary>Details</summary>
Motivation: 机器遗忘在经典深度学习中已有广泛研究，但在变分量子电路和量子增强架构中的行为仍基本未被探索，需要建立量子机器遗忘的实证基础

Method: 将多种遗忘方法（梯度基、蒸馏基、正则化基和认证技术）适配到量子设置，并引入两种针对混合模型的新遗忘策略，在Iris、MNIST和Fashion-MNIST数据集上进行实验

Result: 浅层VQC表现出高内在稳定性，记忆最小；深层混合模型在效用、遗忘强度和与重训练基准对齐之间表现出更强的权衡；EU-k、LCA和Certified Unlearning方法在各项指标上提供最佳平衡

Conclusion: 量子模型支持有效遗忘，但需要量子感知算法和理论保证，为量子机器学习系统扩展提供基线实证见解

Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.

</details>


### [100] [Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals](https://arxiv.org/abs/2512.19280)
*Chang Dong,Jianfeng Tao,Chengliang Liu*

Main category: cs.LG

TL;DR: 提出基于数字孪生的零样本故障诊断框架，使用流体噪声信号，通过校准数字孪生模型生成合成故障数据训练深度学习分类器，实现高精度故障诊断。


<details>
  <summary>Details</summary>
Motivation: 轴向柱塞泵是流体动力系统的关键部件，传统数据驱动方法需要大量标记故障数据难以获取，而基于模型的方法受参数不确定性影响。需要解决数据稀缺场景下的可靠故障诊断问题。

Method: 1) 仅使用健康状态数据校准高保真数字孪生模型；2) 生成合成故障信号训练深度学习分类器；3) 采用物理信息神经网络作为虚拟传感器进行流量脉动估计；4) 集成Grad-CAM可视化神经网络决策过程。

Result: 实验验证显示，使用校准数字孪生模型生成的信号训练，在真实基准测试中诊断准确率超过95%，而未校准模型性能显著降低。Grad-CAM分析发现，时域输入中匹配子序列长度的大核和时频域输入中的小核能聚焦物理意义特征，提高诊断精度。

Conclusion: 提出的数字孪生驱动零样本故障诊断框架在数据稀缺场景下有效，通过校准数字孪生生成高质量合成数据，结合物理信息神经网络和可解释AI技术，实现了高精度、可解释的故障诊断。

Abstract: Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.

</details>


### [101] [Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring](https://arxiv.org/abs/2512.19309)
*Keivan Faghih Niresi,Jun Qing,Mengjie Zhao,Olga Fink*

Main category: cs.LG

TL;DR: 提出Time-Vertex Machine Learning (TVML)框架，结合图信号处理、时域分析和机器学习，用于结构健康监测中的传感器优化布置，减少冗余同时保持关键信息。


<details>
  <summary>Details</summary>
Motivation: 随着传感器网络规模和复杂度增加，需要在降低部署成本的同时不损害监测质量。传统图信号处理方法往往忽略结构行为的时域动态特性，需要更全面的解决方案。

Method: 提出TVML框架，整合图信号处理(GSP)、时域分析和机器学习，通过识别代表性节点来实现可解释且高效的传感器布置，最小化冗余同时保留关键信息。

Result: 在两个桥梁数据集上进行损伤检测和时变图信号重建任务评估，结果显示该方法能有效增强SHM系统，提供鲁棒、自适应且高效的传感器布置解决方案。

Conclusion: TVML框架通过同时考虑空间相关性和时域动态，为结构健康监测提供了更全面、高效的传感器优化布置方法，显著提升系统性能。

Abstract: Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.

</details>


### [102] [MAGIC: Achieving Superior Model Merging via Magnitude Calibration](https://arxiv.org/abs/2512.19320)
*Yayuan Li,Jian Zhang,Jintao Guo,Zihan Cheng,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 提出MAGIC框架，通过校准特征和权重空间的幅度来解决模型合并中的性能下降问题，无需额外训练即可提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要关注特征方向对齐，忽视了幅度的重要性。幅度对常见的合并操作（如参数融合和稀疏化）引入的扰动非常敏感，导致合并模型与原始模型特征偏差，进而造成性能下降。

Method: 提出MAGIC框架，包含三种变体：特征空间校准（FSC）使用少量无标签数据重新对齐合并模型的特征；权重空间校准（WSC）将校准扩展到权重空间，无需额外数据；双重空间校准（DSC）结合两者。

Result: 在计算机视觉任务上（8个数据集）平均提升4.3%，在NLP任务上（Llama模型）提升8.0%，且无需额外训练。

Conclusion: MAGIC框架通过校准特征和权重空间的幅度，有效解决了模型合并中的性能下降问题，显著提升了合并模型在各种任务上的表现。

Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC

</details>


### [103] [Alternative positional encoding functions for neural transformers](https://arxiv.org/abs/2512.19323)
*Ezequiel Lopez-Rubio,Macoris Decena-Gimenez,Rafael Marcos Luque-Baena*

Main category: cs.LG

TL;DR: 提出一种新的位置编码函数替代传统的正弦函数，在实验中显著优于原始版本，有望在更多Transformer架构中应用


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的位置编码模块使用正弦函数来捕捉不同周期的重复模式，但可能存在改进空间。本文旨在探索替代的位置编码函数，以提升性能。

Method: 提出一组替代的周期函数用于位置编码，这些函数保留了正弦函数的关键特性，但在根本上有所不同。进行了初步实验验证。

Result: 实验结果显示，提出的替代函数在性能上显著优于原始的正弦函数版本。

Conclusion: 提出的替代位置编码函数具有潜力，可能在其他Transformer架构中得到更广泛的应用。

Abstract: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.

</details>


### [104] [A Logical View of GNN-Style Computation and the Role of Activation Functions](https://arxiv.org/abs/2512.19332)
*Pablo Barceló,Floris Geerts,Matthias Lanzinger,Klara Pakhomenko,Jan Van den Bussche*

Main category: cs.LG

TL;DR: 本文研究了MPLang（一种描述图神经网络计算的语言）的表达能力，分析了不同激活函数（有界/无界）对表达力的影响，证明了ReLU比有界激活函数具有更强的数值表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络计算语言MPLang的表达能力，特别关注激活函数类型（有界vs无界）如何影响其数值和布尔表达能力，以理解不同GNN架构的表达力差异。

Method: 1. 首先分析无激活函数的A-MPLang片段，用walk-summed特征刻画其表达能力；2. 研究有界激活函数，证明在温和条件下所有最终常数激活函数具有相同的表达能力；3. 在线性层存在的情况下，首次证明无界激活函数（如ReLU）比有界激活函数（如截断ReLU）具有更强的数值表达能力。

Result: 1. A-MPLang的表达能力可以用walk-summed特征完全刻画；2. 所有最终常数激活函数具有相同的数值和布尔表达能力；3. 在线性层存在时，ReLU比最终常数激活函数具有更强的数值表达能力，这是首次证明无界和有界激活函数之间的表达力分离。

Conclusion: 激活函数的类型显著影响GNN的表达能力：有界激活函数（如截断ReLU）的表达力有限且相同，而无界激活函数（如ReLU）在线性层存在时具有更强的数值表达能力，表明使用ReLU的GNN比使用有界激活函数的GNN更强大。

Abstract: We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.

</details>


### [105] [Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation](https://arxiv.org/abs/2512.19361)
*Isshaan Singh,Divyansh Chawla,Anshu Garg,Shivin Mangal,Pallavi Gupta,Khushi Agarwal,Nimrat Singh Khalsa,Nandan Patel*

Main category: cs.LG

TL;DR: 提出一个结合LSTM和RNN的混合强化学习框架，用于物联网食品供应链中的实时腐败预测，通过基于规则的分类器环境实现可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代物联网驱动的食品供应链需要智能实时腐败预测系统，现有方法缺乏对动态条件的适应性，无法实时优化决策。

Method: 提出混合强化学习框架，整合LSTM和RNN捕捉传感器数据的时间依赖性，采用基于规则的分类器环境提供透明的地面真实标签，使用可解释性驱动指标监控模型行为。

Result: 在模拟和实时硬件数据上的评估表明，基于LSTM和RNN的智能体在预测准确性和决策效率上优于其他强化学习方法，同时保持可解释性。

Conclusion: 混合深度强化学习与集成可解释性在可扩展的物联网食品监控系统中具有潜力。

Abstract: The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.

</details>


### [106] [From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples](https://arxiv.org/abs/2512.19363)
*Canran Xiao,Jiabao Dou,Zhiming Lin,Zong Ke,Liwei Hou*

Main category: cs.LG

TL;DR: HCDV提出了一种分层对比数据估值框架，通过几何保持表示、分层聚类和局部蒙特卡洛博弈，将数据估值复杂度从O(n!)降低到O(TK_max log n)，支持大规模异构数据集的价值评估。


<details>
  <summary>Details</summary>
Motivation: 传统Data-Shapley方法在评估大规模、异构、几何结构数据集时面临O(n!)计算复杂度和点对点视角的局限性，无法适应现代数据规模需求。

Method: HCDV采用三阶段框架：1)学习对比几何保持表示；2)构建平衡的粗到细分层聚类；3)通过局部蒙特卡洛博弈分配Shapley式收益，预算向下传播。

Result: 在四个基准测试（表格、视觉、流式、4500万样本CTR任务）和OpenDataVal套件上，HCDV将准确率提升高达5个百分点，估值时间减少100倍，支持增强过滤、低延迟流式更新和公平市场支付等任务。

Conclusion: HCDV为大规模异构数据集提供了高效、可扩展的数据估值方案，在保持Shapley公理近似满足的同时，显著提升了计算效率和实际应用价值。

Abstract: How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.

</details>


### [107] [Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture](https://arxiv.org/abs/2512.19367)
*Christian Hägg,Kathlén Kohn,Giovanni Luca Marchetti,Boris Shapiro*

Main category: cs.LG

TL;DR: Sprecher Networks (SNs) 是一种受经典 Kolmogorov-Arnold-Sprecher 构造启发的新型神经网络架构，使用可学习的共享样条和结构化块，相比 MLP 和 KAN 具有更好的参数效率和内存效率。


<details>
  <summary>Details</summary>
Motivation: 受到经典 Kolmogorov-Arnold-Sprecher (KAS) 多元连续函数逼近构造的启发，旨在开发比传统多层感知机(MLP)和Kolmogorov-Arnold网络(KAN)更高效的新型神经网络架构。MLP使用固定节点激活函数，KAN使用可学习的边缘激活函数，而SNs希望利用共享可学习样条实现更好的参数和内存效率。

Method: SNs采用共享可学习样条（单调和通用）在结构化块中，包含显式移位参数和混合权重。单层变体直接实现了Sprecher 1965年的移位样条求和公式，并扩展到更深的多层组合。进一步通过可选的横向混合连接增强架构，实现输出维度间的块内通信，作为全注意力机制的高效替代方案。

Result: SNs具有O(LN + LG)的参数缩放（G为共享样条的节点数），相比MLP的O(LN^2)更高效。采用顺序评估策略将前向中间内存峰值从O(N^2)降低到O(N)，使得在内存限制下可以构建更宽的架构。实验表明将这些块组合成深度网络能产生高度参数和内存高效的模型。

Conclusion: Sprecher Networks提供了一种新颖的神经网络架构设计，结合了经典函数逼近理论和现代深度学习需求，在参数效率和内存效率方面优于传统MLP和KAN，为构建更高效的大规模神经网络提供了有前景的途径。

Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).

</details>


### [108] [OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation](https://arxiv.org/abs/2512.19379)
*Xueming Yan,Boyan Xu,Yaochu Jin,Lixian Xiao,Wenlong Ye,Runyang Cai,Zeqi Zheng,Jingfa Liu,Aimin Yang*

Main category: cs.LG

TL;DR: 提出了首个印尼语多模态情感识别基准数据集IndoMER和基于Qwen2.5-Omni的OmniMER框架，通过辅助感知任务提升多模态情感识别性能


<details>
  <summary>Details</summary>
Motivation: 印尼语作为东南亚社交媒体主要语言，拥有超过2亿使用者，但在多模态情感识别研究中服务不足，需要专门的数据集和方法来应对印尼文化背景下的实际挑战

Method: 1) 创建IndoMER数据集：包含1,944个视频片段，来自203位说话者，具有时间对齐的文本、音频和视觉标注，涵盖7种情感类别；2) 提出OmniMER框架：基于Qwen2.5-Omni，通过三个辅助模态特定感知任务增强情感识别：文本情感关键词提取、视频面部表情分析、音频韵律分析

Result: OmniMER在IndoMER数据集上取得0.582的Macro-F1（情感分类）和0.454（情感识别），分别比基准模型提升7.6和22.1个绝对百分点；在中文CH-SIMS数据集上的跨语言评估进一步证明了框架的泛化能力

Conclusion: IndoMER填补了印尼语多模态情感识别基准的空白，OmniMER框架通过辅助感知任务有效提升了低资源环境下的情感识别性能，为多语言多模态情感分析提供了有价值的工具

Abstract: Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER

</details>


### [109] [Real-Time Machine Learning for Embedded Anomaly Detection](https://arxiv.org/abs/2512.19383)
*Abdelmadjid Benmachiche,Khadija Rais,Hamda Slimi*

Main category: cs.LG

TL;DR: 这篇综述论文系统分析了在资源受限的物联网边缘设备上进行实时异常检测的机器学习方法，重点关注计算效率、内存和功耗约束下的算法选择权衡。


<details>
  <summary>Details</summary>
Motivation: 物联网和嵌入式设备的普及对边缘实时异常检测提出了挑战，需要在严格的延迟、内存和功耗约束下实现有效的异常检测，特别是在带宽受限且可能涉及安全关键的环境中。

Method: 通过系统综述和比较分析，评估了轻量级算法如孤立森林、一类SVM、循环架构和统计技术在嵌入式实现中的实际表现，考虑了硬件约束对算法选择的根本性影响。

Result: 揭示了异常检测精度与计算效率之间的显著权衡关系，硬件约束从根本上重新定义了算法选择标准，为不同设备配置提供了实用的算法选择建议。

Conclusion: 该论文为在资源受限的边缘环境中部署异常检测提供了战略路线图，结合TinyML新趋势，有助于缩小检测能力与嵌入式现实之间的差距，为工程师提供实用的部署指导。

Abstract: The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.

</details>


### [110] [Brain-Grounded Axes for Reading and Steering LLM States](https://arxiv.org/abs/2512.19399)
*Sandro Andric*

Main category: cs.LG

TL;DR: 使用人类大脑活动作为坐标系统来解读和调控大语言模型状态，通过脑电数据构建词汇级脑图谱，训练轻量级适配器将LLM隐藏状态映射到大脑轴，实现基于神经生理学的模型调控。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型可解释性方法通常依赖于文本监督，缺乏外部基础。作者希望利用人类大脑活动作为坐标系统，为LLM状态提供神经生理学基础的可解释和可控方向。

Method: 使用SMN4Lang MEG数据集构建词汇级脑图谱（PLV模式），通过ICA提取潜在轴。验证轴后，训练轻量级适配器将LLM隐藏状态映射到这些大脑轴，而不微调LLM本身。在不同模型（TinyLlama、Qwen2-0.5B、GPT-2）上进行验证。

Result: 在TinyLlama中间层发现稳健的词汇（频率相关）轴，在困惑度匹配控制中保持稳定。大脑轴相比文本探针显示更大的对数频率偏移且困惑度更低。功能/内容轴（轴13）在三个模型中表现一致。轴结构在不同嵌入方法下保持稳定（|r|=0.64-0.95）。

Conclusion: 神经生理学基础轴为LLM行为提供了可解释和可控的接口，支持使用大脑活动作为坐标系统来解读和调控语言模型状态的新方法。

Abstract: Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.

</details>


### [111] [Symplectic Reservoir Representation of Legendre Dynamics](https://arxiv.org/abs/2512.19409)
*Robert Simon Fong,Gouhei Tanaka,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 该论文提出了一种名为"辛储层"的新型神经网络架构，通过在表示层面注入辛几何和哈密顿力学约束，确保表示本身满足辛守恒定律。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统基于数据的内部表示进行操作，但这些表示如何编码底层物理或统计结构往往不明确。物理学中哈密顿系统的守恒定律（如辛性）保证了长期稳定性，现有工作主要在损失或输出层面硬编码这些约束。本文探讨如何在表示层面实现辛守恒定律。

Method: 通过勒让德对偶性表达辛约束：原始参数和对偶参数之间的配对成为表示必须保持的结构。将勒让德动力学形式化为保持勒让德图的随机过程，确保演化的原始-对偶参数保持勒让德对偶关系。几何上证明保持所有勒让德图的映射正是余切丛的辛同胚。基于此设计了辛储层架构，这是一种特殊的循环神经网络，其循环核心由哈密顿系统生成。

Result: 主要定理证明每个辛储层更新都具有特定正规形式，因此能够将勒让德图传输到勒让德图，在每个时间步保持勒让德对偶性。该架构包含了线性时不变高斯过程回归和Ornstein-Uhlenbeck动力学作为特例。

Conclusion: 辛储层实现了几何约束的、保持勒让德对偶性的表示映射，将辛几何和哈密顿力学直接注入到表示层面，为学习系统提供了具有物理意义的结构化表示。

Abstract: Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?
  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.
  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form "cotangent lift of a base diffeomorphism followed by an exact fibre translation". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.
  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.

</details>


### [112] [Research Program: Theory of Learning in Dynamical Systems](https://arxiv.org/abs/2512.19410)
*Elad Hazan,Shai Shalev Shwartz,Nathan Srebro*

Main category: cs.LG

TL;DR: 该论文提出了一个研究计划，通过下一个标记预测的视角来理解动态系统的可学习性，强调应从有限样本角度研究，并基于系统动态特性而非序列统计特性。


<details>
  <summary>Details</summary>
Motivation: 现代学习系统越来越多地与随时间演变且依赖隐藏内部状态的数据交互。核心问题是：何时仅从观测数据就能学习这样的动态系统？需要建立动态系统可学习性的理论基础。

Method: 提出动态可学习性的研究框架，将可学习性定义为有限样本问题，基于系统动态特性（如稳定性、混合性、可观测性、谱特性）。给出随机过程可学习性的形式化定义，关注在有限预热期后每个时间步上的一致保证。在线性动态系统案例中，展示了无需系统辨识，通过基于谱滤波的不恰当方法，在有限观测后实现准确预测。

Result: 建立了动态可学习性的概念框架，将系统结构特性与所需观测数量联系起来。在线性动态系统中证明了通过谱滤波方法可以在有限观测后实现准确预测，无需完全系统辨识。

Conclusion: 动态系统的可学习性应作为有限样本问题研究，基于系统动态特性而非序列统计特性。提出的动态可学习性框架为理解系统结构如何影响预测能力提供了理论基础，并指出了研究非线性和受控系统的方向。

Abstract: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.

</details>


### [113] [Attention Is Not What You Need](https://arxiv.org/abs/2512.19428)
*Zhang Chong*

Main category: cs.LG

TL;DR: 论文提出了一种基于格拉斯曼流（Grassmann flows）的无注意力架构，替代传统的自注意力机制，在语言建模和自然语言推理任务上取得了与Transformer相当的性能。


<details>
  <summary>Details</summary>
Motivation: 重新审视序列建模的基本问题：显式自注意力是否真的必要？作者认为标准多头注意力本质上是张量提升（tensor lifting），虽然表达能力强但数学上不透明，难以用少量显式不变量描述深层模型。

Method: 提出基于格拉斯曼流的无注意力架构：因果格拉斯曼层（Causal Grassmann layer）通过（1）线性降维token状态，（2）使用Plücker坐标将局部token对编码为格拉斯曼流形上的二维子空间，（3）通过门控混合将这些几何特征融合回隐藏状态。信息通过低秩子空间在多尺度局部窗口上的受控变形传播。

Result: 在Wikitext-2语言建模任务中，1300-1800万参数的纯格拉斯曼模型验证困惑度比同等规模Transformer低10-15%。在SNLI自然语言推理任务中，基于DistilBERT的格拉斯曼-Plücker头部略优于Transformer头部，最佳验证和测试准确率分别为0.8550/0.8538 vs 0.8545/0.8511。

Conclusion: 基于流形的设计为神经推理的几何和不变量解释提供了更结构化的路径，格拉斯曼混合具有线性序列长度复杂度，展示了无注意力架构的潜力。

Abstract: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.
  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.
  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.

</details>


### [114] [An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning](https://arxiv.org/abs/2512.19439)
*Rixin Yu*

Main category: cs.LG

TL;DR: 提出IS-FNO方法，通过结合逆散射变换的可逆性和谱演化结构，改进神经算子在非线性PDE中的长期稳定性


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法（如FNO和Koopman扩展）在非线性PDE的短期预测中表现良好，但长期稳定性受限，主要问题是潜在表示无约束和累积误差

Method: 引入逆散射启发的傅里叶神经算子(IS-FNO)，通过显式可逆神经变换强制提升和投影映射之间的近可逆配对，使用指数傅里叶层建模潜在时间演化

Result: 在多个基准PDE（Michelson-Sivashinsky、Kuramoto-Sivashinsky、KdV、KP方程）上测试，IS-FNO实现了更低的短期误差和显著改善的长期稳定性

Conclusion: 将物理结构（特别是可逆性和谱演化）融入神经算子设计能显著增强非线性PDE动力学的鲁棒性和长期预测保真度

Abstract: Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.

</details>


### [115] [Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm](https://arxiv.org/abs/2512.19440)
*Antonio Consolo,Andrea Manno,Edoardo Amaldi*

Main category: cs.LG

TL;DR: 提出一种稀疏核逻辑回归方法，通过扩展Keerthi等人的训练公式，在保持良好测试精度的同时实现模型稀疏性，并设计基于二阶信息的SMO分解算法。


<details>
  <summary>Details</summary>
Motivation: 核逻辑回归（KLR）虽然能提供类别成员条件概率估计，但通常缺乏稀疏性。现有方法如Import Vector Machine（IVM）和ℓ₁/₂正则化要么是启发式方法，要么是临时解决方案。如何在预测精度和稀疏性之间取得良好平衡仍然是一个具有重要应用价值的挑战性问题。

Method: 扩展Keerthi等人提出的训练公式，使其能够诱导稀疏性。设计一种基于二阶信息的序列最小优化（SMO）分解算法来高效求解该公式的对偶问题，并建立了算法的全局收敛性。

Result: 在12个文献数据集上的数值实验表明，所提出的二元KLR方法在精度和稀疏性之间取得了有竞争力的平衡，优于IVM、ℓ₁/₂正则化KLR和SVM，同时保留了提供类别成员概率信息估计的优势。

Conclusion: 提出的稀疏核逻辑回归方法成功解决了KLR的稀疏性问题，在保持良好预测精度的同时实现了模型稀疏性，为实际应用提供了有效的概率分类工具。

Abstract: Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.

</details>


### [116] [Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications](https://arxiv.org/abs/2512.19472)
*Lorenzo Capelli,Leandro de Souza Rosa,Gianluca Setti,Mauro Mangia,Riccardo Rovatti*

Main category: cs.LG

TL;DR: MACS是一个统一的后处理框架，通过分析中间激活来生成分类图，从中推导出可用于置信度估计、检测分布偏移和对抗攻击的分数，在VGG16和ViTb16模型上以较低计算开销超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的黑盒使用在关键领域存在透明度和可信度问题，现有方法要么需要重新训练模型（嵌入式置信度指标），要么只能独立解决单个问题（后处理方法），缺乏统一的解决方案。

Method: 提出MACS（多层分析置信度评分）框架，分析预训练模型的中间激活来生成分类图，从分类图中推导出统一的置信度分数，该分数可同时用于置信度估计、分布偏移检测和对抗攻击检测。

Result: 在VGG16和ViTb16模型上的实验表明，MACS在置信度估计、分布偏移检测和对抗攻击检测三个任务上均超越了现有最先进方法，且计算开销仅为这些方法的一小部分。

Conclusion: MACS提供了一个统一的后处理框架，能够同时解决多个模型可信度问题，无需重新训练现有模型，具有广泛适用性和计算效率优势。

Abstract: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.

</details>


### [117] [Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks](https://arxiv.org/abs/2512.19488)
*Hafsa Benaddi,Mohammed Jouhari,Nouha Laamech,Anas Motii,Khalil Ibrahimi*

Main category: cs.LG

TL;DR: 提出一种轻量级物联网入侵检测系统，结合SHAP特征剪枝与知识蒸馏的Kronecker网络，在保持高准确率的同时大幅压缩模型规模


<details>
  <summary>Details</summary>
Motivation: 物联网设备广泛部署需要高精度的入侵检测系统，但传统深度学习模型太大且计算密集，不适合边缘部署

Method: 1. 使用高容量教师模型通过SHAP解释识别最相关特征；2. 采用Kronecker结构层构建压缩学生模型；3. 通过知识蒸馏将软化的决策边界从教师转移到学生

Result: 在TON_IoT数据集上，学生模型比教师模型小近三个数量级，但仍保持0.986以上的宏F1分数，推理延迟达到毫秒级

Conclusion: 可解释性驱动的特征剪枝和结构化压缩可以共同实现可扩展、低延迟、高能效的物联网入侵检测系统

Abstract: The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.

</details>


### [118] [Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico](https://arxiv.org/abs/2512.19491)
*Martí Medina-Hern ández,Janos Kertész,Mihály Fazekas*

Main category: cs.LG

TL;DR: 该研究使用正例-未标记学习算法，结合传统腐败风险指标和网络特征，识别墨西哥政府采购中的腐败合同，相比传统方法性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 政府采购中的欺诈和腐败检测是全球性挑战，现有研究主要基于领域知识的腐败风险指标和网络模式分析，但监督学习面临缺乏确认的非腐败负例样本的问题。

Method: 使用墨西哥联邦政府采购公开数据和公司制裁记录，实施正例-未标记学习算法，整合基于领域知识的腐败风险指标和网络衍生特征来识别可能的腐败合同。

Result: 最佳PU模型平均捕获已知正例数量增加32%，性能比随机猜测平均好2.3倍，显著优于仅基于传统风险指标的方法。网络衍生特征（特别是网络核心合同和高特征向量中心性的供应商）最重要。

Conclusion: 该方法可支持墨西哥执法工作，并能适应其他国家的背景。网络特征与传统风险指标结合能有效识别腐败合同，特别是在竞争性招标中。

Abstract: Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.

</details>


### [119] [Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset](https://arxiv.org/abs/2512.19494)
*Nikita Volzhin,Soowhan Yoon*

Main category: cs.LG

TL;DR: KAGNNs在无机纳米材料数据集CHILI上显著超越传统GNNs，取得SOTA分类结果


<details>
  <summary>Details</summary>
Motivation: 现有KAN-based GNNs研究主要关注有机分子数据集，忽略了无机纳米材料数据集，需要填补这一空白

Method: 将Kolmogorov-Arnold图神经网络(KAGNNs)应用于大型无机纳米材料数据集CHILI，并进行适配和测试

Result: 在CHILI数据集（特别是CHILI-3K）上，KAGNNs在分类任务中显著超越传统GNNs，达到最先进水平

Conclusion: KAGNNs在无机纳米材料数据集上表现出色，填补了现有研究空白，为无机材料分析提供了有效工具

Abstract: The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.

</details>


### [120] [DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast](https://arxiv.org/abs/2512.19506)
*Hongliang Li,Nong Zhang,Zhewen Xu,Xiang Li,Changzheng Liu,Chongbo Zhao,Jie Wu*

Main category: cs.LG

TL;DR: 提出DK-STN网络，结合数值天气预报与神经网络优势，实现高效稳定的MJO预测，精度与ECMWF相当但效率显著提升


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报方法资源密集、耗时且不稳定，而现有神经网络方法虽节省资源但精度不足，无法达到ECMWF的28天预测水平，需要结合两者优势

Method: 基于时空网络，通过两种关键方法嵌入领域知识：(1)应用领域知识增强方法，(2)将领域知识处理方法集成到网络训练中

Result: DK-STN使用7天气候数据输入，可在1-2秒内生成未来28天的可靠预测，不同季节误差仅为2-3天，精度与ECMWF相当，但效率和稳定性显著更优

Conclusion: DK-STN成功结合了数值天气预报和神经网络方法的优势，在保持高效率和高稳定性的同时，显著提升了神经网络方法的预测精度，为MJO预测提供了新的有效解决方案

Abstract: Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.

</details>


### [121] [Toward Scalable and Valid Conditional Independence Testing with Spectral Representations](https://arxiv.org/abs/2512.19510)
*Alek Frohlich,Vladimir Kostic,Karim Lounici,Daniel Perazzo,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 提出一种基于表示学习的条件独立性检验方法，通过偏协方差算子的奇异值分解学习表示，构建类似HSIC的检验统计量，并引入双层对比学习算法，在理论和实验上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 条件独立性检验在因果推断、特征选择和图模型中至关重要，但在许多设置下无法测试。现有方法依赖限制性结构条件，核方法虽然更理论化但存在适应性差、收敛慢和可扩展性差的问题。本文探索表示学习是否能解决这些限制。

Method: 基于偏协方差算子的奇异值分解学习表示，构建类似希尔伯特-施密特独立性准则（HSIC）的简单检验统计量，并引入实用的双层对比学习算法来学习这些表示。

Result: 理论分析将表示学习误差与检验性能联系起来，建立了渐近有效性和功效保证。初步实验表明该方法为可扩展的条件独立性检验提供了实用且统计基础良好的路径。

Conclusion: 该方法将基于核的理论与现代表示学习相结合，为可扩展的条件独立性检验提供了有前景的方向，在保持理论严谨性的同时提高了实际应用性。

Abstract: Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.

</details>


### [122] [LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning](https://arxiv.org/abs/2512.19516)
*Xueming Yan,Bo Yin,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出LacaDM模型，通过潜在因果扩散模型增强多目标强化学习在动态环境中的适应性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习方法在处理目标冲突和动态环境适应方面存在局限，特别是在大规模复杂状态-动作空间中泛化能力不足

Method: 提出潜在因果扩散模型(LacaDM)，学习环境状态与策略之间的潜在时序因果关系，并将这些因果结构嵌入到基于扩散模型的框架中，实现跨多目标强化学习场景的知识迁移

Result: 在MOGymnasium框架的各种任务上，LacaDM在超体积、稀疏性和期望效用最大化等指标上持续优于现有最优基线方法

Conclusion: LacaDM通过建模潜在因果结构，有效平衡了多目标强化学习中的目标冲突，并在未见环境中展现出强大的泛化能力，为复杂多目标任务提供了有效解决方案

Abstract: Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.

</details>


### [123] [Initialization of a Polyharmonic Cascade, Launch and Testing](https://arxiv.org/abs/2512.19524)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出了一种基于超八面体对称星座的通用初始化方法，使多谐级联架构能够稳定训练数百层深度网络，无需跳跃连接，并将线性代数简化为2D操作，在多个数据集上展示了可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从无差别原理和随机函数理论推导出的多谐级联架构需要解决深度网络训练不稳定的问题，特别是对于没有跳跃连接的深层网络，需要一种有效的初始化方法来确保训练收敛。

Method: 提出基于超八面体对称星座（带中心点）的通用初始化程序，将线性代数简化为2D操作，在GPU上高效执行，支持多达500层无跳跃连接的级联网络训练。

Result: 在MNIST上达到98.3%准确率（无卷积或数据增强），HIGGS数据集上AUC约0.885（1100万样本），Epsilon数据集上AUC约0.963（2000特征），展示了良好的可扩展性和鲁棒性。

Conclusion: 该初始化方法不仅确保了深层级联网络的稳定训练，还显著简化了计算，为深度机器学习架构提供了理论支撑和实用工具，所有代码已公开以确保可复现性。

Abstract: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.

</details>


### [124] [Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions](https://arxiv.org/abs/2512.19527)
*Diego Hitzges,Guillaume Sagnol*

Main category: cs.LG

TL;DR: 提出一种用于无关并行机器调度的深度学习模型，能够处理任意规模的问题，在离线确定性调度中最小化完工时间和加权延迟成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的调度方法在处理无关并行机器问题时面临挑战，因为作业和机器数量可变，且每对作业-机器都有独特的处理时间，导致特征维度动态变化。需要一种能够处理任意规模问题、考虑整体输入的离线调度方法。

Method: 设计了一种新颖的神经网络架构，借鉴NLP技术处理可变数量的作业和机器以及变化的特征维度。模型能够一次性处理整个输入生成完整调度，支持在小规模实例上进行监督训练，并能泛化到更大规模的问题。

Result: 在8作业4机器的实例上训练和测试，成本仅比最优解高2.51%。在所有测试配置中（最多100作业10机器），网络始终优于高级调度规则，后者平均成本高出22.22%。

Conclusion: 该方法通过模拟数据快速重新训练并适应各种调度条件，有潜力成为无关机器调度和类似问题环境的标准化学习方法。

Abstract: Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.

</details>


### [125] [Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement](https://arxiv.org/abs/2512.19530)
*Hongsheng Xing,Qiuxin Si*

Main category: cs.LG

TL;DR: 该研究提出了Catechol Benchmark数据集，用于评估模型在连续溶剂组成范围内预测反应产率的能力，并开发了一种混合GNN架构，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在有机合成和过程化学中，预测连续溶剂组成范围内的反应结果是一个关键挑战。传统机器学习方法通常将溶剂视为离散分类变量，无法在溶剂空间进行系统性的插值和外推。

Method: 提出了混合GNN架构，整合了图注意力网络（GATs）、差分反应指纹（DRFP）和学习得到的混合物感知溶剂编码。使用Catechol Benchmark数据集（包含1,227个实验产率测量值），在24种纯溶剂及其二元混合物上进行评估。

Result: 混合GNN架构取得了MSE 0.0039（±0.0003）的优异结果，相比竞争基线误差减少60%，比表格集成方法提升超过25倍。传统表格方法（MSE 0.099）和大型语言模型嵌入（MSE 0.129）表现较差。

Conclusion: 显式的分子图消息传递和连续混合物编码对于稳健的泛化至关重要。该研究为数据高效的反应预测和连续溶剂表示学习提供了完整的基准数据集和评估协议。

Abstract: Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.
  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.

</details>


### [126] [DFORD: Directional Feedback based Online Ordinal Regression Learning](https://arxiv.org/abs/2512.19550)
*Naresh Manwani,M Elamparithy,Tanish Taneja*

Main category: cs.LG

TL;DR: 提出一种使用方向反馈的在线序数回归算法，通过探索-利用策略从弱监督反馈中学习，实现O(log T)期望遗憾


<details>
  <summary>Details</summary>
Motivation: 在序数回归中引入方向反馈的弱监督设置，相比完全信息设置（可直接访问标签），方向反馈只提供预测标签相对于真实标签在左侧还是右侧的信息，这在实际应用中更现实且成本更低

Method: 提出在线算法使用探索-利用方案从方向反馈中高效学习；引入基于核的变体学习非线性序数回归模型；使用截断技巧提高核实现的内存效率；算法在期望意义上保持阈值排序

Result: 算法实现O(log T)的期望遗憾；在合成和真实数据集上，使用方向反馈的方法与完全信息方法表现相当（有时更好）

Conclusion: 方向反馈作为序数回归的弱监督形式是有效的，提出的在线算法能够从这种弱监督反馈中高效学习，性能可与完全信息方法媲美

Abstract: In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.

</details>


### [127] [CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal](https://arxiv.org/abs/2512.19554)
*Yongxin Wang,Zhicheng Yang,Meng Cao,Mingfei Han,Haokun Lin,Yingying Zhu,Xiaojun Chang,Xiaodan Liang*

Main category: cs.LG

TL;DR: CARE是一个针对多模态推理的失败中心化后训练框架，通过对比锚定和反思引导重采样，将错误转化为监督信号，提升可验证推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的群体相对强化学习（RLVR）在处理失败数据时效率低下：当所有轨迹都错误时梯度停滞；当有一个正确时，更新通常忽略其他接近但错误的轨迹，导致信用分配错误。需要一种能有效利用失败数据的方法。

Method: CARE包含两个核心组件：1) 锚定对比目标：围绕最佳轨迹形成紧凑子组和语义相近的困难负例，进行组内z-score归一化（仅负例缩放），包含全负例救援机制防止零信号批次；2) 反思引导重采样（RGR）：一次性结构化自修复，重写代表性失败案例并用相同验证器重新评分，将接近正确的失败转化为可用正例。

Result: 在Qwen2.5-VL-7B上，CARE在六个可验证视觉推理基准上的宏观平均准确率比GRPO提升4.6个百分点；使用Qwen3-VL-8B时，在相同评估协议下，在MathVista和MMMU-Pro上达到竞争性或最先进的结果。

Conclusion: CARE通过将错误转化为监督信号，显著提高了多模态推理任务的准确性和训练平滑度，明确增加了来自失败的学习信号比例，为可验证推理提供了有效的后训练框架。

Abstract: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.

</details>


### [128] [KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning](https://arxiv.org/abs/2512.19605)
*Eric Zimmermann,Harley Wiltzer,Justin Szeto,David Alvarez-Melis,Lester Mackey*

Main category: cs.LG

TL;DR: 提出KerJEPA家族，使用核基正则化器改进自监督学习，相比现有JEPA方法具有更好的训练稳定性和设计灵活性


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用欧几里得表示向各向同性高斯先验的正则化，虽然能提高训练稳定性和下游泛化能力，但正则化方法有限。需要更灵活的正则化框架来改进自监督学习算法。

Method: 引入KerJEPA家族，使用核基正则化器。扩展可行的核函数和先验分布，计算切片最大均值差异（MMD）的闭式高维极限，开发具有改进特性的替代KerJEPA算法。

Result: 开发了具有多种有利特性的替代KerJEPA算法，包括改进的训练稳定性和设计灵活性。其中一个实例对应最近提出的LeJEPA Epps-Pulley正则化器。

Conclusion: KerJEPA家族通过核基正则化器为自监督学习提供了更灵活、更稳定的框架，扩展了JEPA方法的正则化能力，具有更好的训练性能和设计选择。

Abstract: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.

</details>


### [129] [The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643)
*Rajyasri Roy,Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: ANCHOR提出了一种在线混合推理框架，通过自适应耦合预训练的神经算子和传统数值求解器，使用基于残差的误差估计器来检测和纠正累积误差，实现非线性时变PDE的稳定长期预测。


<details>
  <summary>Details</summary>
Motivation: 神经算子（NO）替代模型在参数化和函数输入上提供快速推理，但大多数自回归NO框架容易受到累积误差的影响，且集成平均指标对单个推理轨迹的保证有限。在实际应用中，误差累积在训练范围外可能变得不可接受，现有方法缺乏在线监测或纠正机制。

Method: ANCHOR将预训练的NO作为主要推理引擎，使用基于物理的残差误差估计器自适应地将其与经典数值求解器耦合。该方法受数值分析中自适应时间步长的启发，通过监测归一化PDE残差的指数移动平均（EMA）来检测累积误差，并在不需要真实解的情况下触发纠正性求解器干预。

Result: 在四个典型PDE（1D和2D Burgers方程、2D Allen-Cahn方程和3D热传导方程）上的评估表明，ANCHOR能可靠地限制长期误差增长，稳定外推推演，并显著提高相对于独立神经算子的鲁棒性，同时比高保真数值求解器更高效。

Conclusion: ANCHOR框架通过自适应混合神经算子和数值求解器，提供了一种有效的在线误差监测和纠正机制，解决了神经算子长期预测中的误差累积问题，在保持计算效率的同时提高了预测的稳定性和可靠性。

Abstract: Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.

</details>


### [130] [Deep Legendre Transform](https://arxiv.org/abs/2512.19649)
*Aleksey Minabutdinov,Patrick Cheridito*

Main category: cs.LG

TL;DR: 提出一种基于深度学习计算可微凸函数凸共轭的新算法，通过隐式Fenchel公式实现高效梯度优化，能处理高维问题并提供后验误差估计。


<details>
  <summary>Details</summary>
Motivation: 凸共轭是凸分析中的基本运算，在优化、控制理论、物理和经济学等领域有广泛应用。传统数值方法面临维度灾难，而现有神经网络方法主要针对最优传输问题且需要解决复杂的优化或最大最小问题。

Method: 使用隐式Fenchel公式表示凸共轭，构建高效的基于梯度的框架来最小化近似误差。该方法还利用Kolmogorov-Arnold网络进行符号回归，以获得特定凸函数的精确凸共轭。

Result: 数值实验表明该方法能够在不同高维示例中提供准确结果。通过符号回归技术，能够获得特定凸函数的精确凸共轭。

Conclusion: 该方法为计算凸共轭提供了一种可扩展的深度学习框架，克服了传统方法的维度限制，同时提供了误差估计能力，并能通过符号回归获得精确解。

Abstract: We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.

</details>


### [131] [Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies](https://arxiv.org/abs/2512.19673)
*Yuqiao Tan,Minzheng Wang,Shizhu He,Huanxuan Liao,Chengfeng Zhao,Qiunan Lu,Tian Liang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的强化学习范式BuPO，通过分解语言模型内部层策略，自下而上优化早期层，提升复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法将大语言模型视为单一统一策略，忽略了其内部机制。理解策略在不同层和模块间的演化对于实现更有针对性的优化和揭示复杂推理机制至关重要。

Method: 通过Transformer残差流的内在分割和隐藏状态与解嵌入矩阵的等价性，分解语言模型策略为内部层策略和内部模块策略。提出Bottom-up Policy Optimization (BuPO)，在早期训练中直接优化内部层策略，自下而上重建基础推理能力。

Result: 发现：(a) 早期层保持高熵用于探索，顶层收敛到接近零熵用于精炼，收敛模式在不同模型系列间有差异；(b) LLama在最后一层快速收敛，而Qwen系列（特别是Qwen3）展现出更类似人类的渐进结构化推理模式。BuPO在复杂推理基准测试中表现出优越性能。

Conclusion: 通过分解和优化语言模型的内部策略，BuPO提供了一种新的RL范式，能够更有效地提升模型的推理能力，特别是在复杂任务中表现出色。

Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [132] [Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA](https://arxiv.org/abs/2512.17910)
*Allison Li,Kristjan Greenewald,Thomas Parnell,Navid Azizan*

Main category: cs.DC

TL;DR: 提出aLoRA（Activated LoRA）方法，在LLM推理服务中实现跨模型的KV缓存复用，显著减少多轮多适配器管道中的重复计算开销。


<details>
  <summary>Details</summary>
Motivation: 现代LLM系统越来越多地使用由多个任务特定适配器组成的多轮管道，但现有服务框架在适配器切换时会产生大量重复计算开销，效率低下。

Method: 基于vLLM框架扩展，引入基础对齐块哈希和激活感知掩码，通过Activated LoRA（aLoRA）实现跨模型的KV缓存复用，支持动态适配器激活。

Result: 在代表性多轮多适配器管道评估中，相比标准LoRA基线，端到端延迟降低高达58倍，首次令牌时间改善超过100倍，且模型规模和序列长度越大效果越明显。

Conclusion: 该工作将参数高效模型适应与高性能服务相结合，首次在现代LLM推理引擎中完整实现了跨模型KV缓存复用。

Abstract: Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.

</details>


### [133] [Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation](https://arxiv.org/abs/2512.17913)
*Nihir Chadderwala*

Main category: cs.DC

TL;DR: 提出一种拜占庭容错的多智能体医疗系统，结合流言协议和密码学验证，能在不信任环境中实现安全的协作医疗决策


<details>
  <summary>Details</summary>
Motivation: 当前基于生成式AI的多智能体医疗系统在对抗性或不信任环境中面临消息完整性和容错性挑战，需要确保分布式系统的安全性和可靠性

Method: 采用拜占庭共识协议（n=3f+1），结合流言协议进行去中心化消息传播，使用密码学签名和时间戳验证，包含诊断、治疗规划、应急响应等专业AI智能体

Result: 系统能成功验证医疗消息，防止重放攻击，在最多33%拜占庭节点下保持100%共识准确率，提供实时可视化的共识轮次、投票统计和网络拓扑

Conclusion: 该工作为在不信任环境中构建安全、弹性的医疗多智能体系统提供了实用框架，支持协作医疗决策

Abstract: Recent advances in generative AI have enabled sophisticated multi-agent architectures for healthcare, where large language models power collaborative clinical decision-making. However, these distributed systems face critical challenges in ensuring message integrity and fault tolerance when operating in adversarial or untrusted environments.This paper presents a novel Byzantine fault-tolerant multi-agent system specifically designed for healthcare applications, integrating gossip-based message propagation with cryptographic validation mechanisms. Our system employs specialized AI agents for diagnosis, treatment planning, emergency response, and data analysis, coordinated through a Byzantine consensus protocol that tolerates up to f faulty nodes among n = 3f + 1 total nodes. We implement a gossip protocol for decentralized message dissemination, achieving consensus with 2f + 1 votes while maintaining system operation even under Byzantine failures. Experimental results demonstrate that our approach successfully validates medical messages with cryptographic signatures, prevents replay attacks through timestamp validation, and maintains consensus accuracy of 100% with up to 33% Byzantine nodes. The system provides real-time visualization of consensus rounds, vote tallies, and network topology, enabling transparent monitoring of fault-tolerant operations. This work contributes a practical framework for building secure, resilient healthcare multi-agent systems capable of collaborative medical decision-making in untrusted environments.

</details>


### [134] [QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments](https://arxiv.org/abs/2512.17918)
*Irwindeep Singh,Sukhpal Singh Gill,Jinzhao Sun,Jan Mol*

Main category: cs.DC

TL;DR: 提出QAISim工具包，使用量子强化学习解决量子云环境中物联网应用的资源分配问题，相比经典方法显著降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算需求增长，云量子资源共享平台面临高效分配硬件资源的挑战，特别是支持大规模物联网应用的计算需求。现有资源分配方法包括启发式和机器学习方法，但需要更高效的解决方案。

Method: 采用基于参数化量子电路的量子强化学习，开发了名为QAISim的Python工具包，用于模拟和建模量子人工智能模型，设计量子云环境中的资源管理策略。模拟了策略梯度和深度Q学习算法。

Result: QAISim相比经典对应方法显著减少了模型复杂度，具有更少的可训练变量。

Conclusion: 量子强化学习为量子云环境中的资源分配问题提供了有效的解决方案，QAISim工具包展示了在降低模型复杂度方面的优势。

Abstract: Quantum computing offers new ways to explore the theory of computation via the laws of quantum mechanics. Due to the rising demand for quantum computing resources, there is growing interest in developing cloud-based quantum resource sharing platforms that enable researchers to test and execute their algorithms on real quantum hardware. These cloud-based systems face a fundamental challenge in efficiently allocating quantum hardware resources to fulfill the growing computational demand of modern Internet of Things (IoT) applications. So far, attempts have been made in order to make efficient resource allocation, ranging from heuristic-based solutions to machine learning. In this work, we employ quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem to support large IoT networks. We propose a python-based toolkit called QAISim for the simulation and modeling of Quantum Artificial Intelligence (QAI) models for designing resource management policies in quantum cloud environments. We have simulated policy gradient and Deep Q-Learning algorithms for reinforcement learning. QAISim exhibits a substantial reduction in model complexity compared to its classical counterparts with fewer trainable variables.

</details>


### [135] [Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU](https://arxiv.org/abs/2512.17941)
*Bin Xu,Ayan Banerjee,Midhat Urooj,Sandeep K. S. Gupta*

Main category: cs.DC

TL;DR: 提出基于FPGA的数字孪生学习框架，相比云GPU实现8.8倍能效提升、28.5倍内存占用减少和1.67倍加速，适用于精准医疗边缘计算


<details>
  <summary>Details</summary>
Motivation: 精准医疗需要快速、资源高效的患者特定动态数字孪生学习，但现有模型恢复技术依赖迭代求解器，计算和内存需求高，不适合边缘设备

Method: 提出通用数字孪生学习框架，优化以适应FPGA等可重构硬件加速，实现边缘计算部署，并与移动GPU和云GPU进行对比

Result: FPGA实现相比云GPU：能效提升8.8倍，DRAM占用减少28.5倍，运行速度提升1.67倍；相比移动GPU：能效更好，运行时间减半，内存占用减少10倍

Conclusion: FPGA加速的数字孪生学习框架在边缘设备上实现了显著的速度和能效优势，为精准医疗应用如1型糖尿病和冠心病检测提供了可行的边缘计算解决方案

Abstract: Digital twins (DTs) can enable precision healthcare by continually learning a mathematical representation of patient-specific dynamics. However, mission critical healthcare applications require fast, resource-efficient DT learning, which is often infeasible with existing model recovery (MR) techniques due to their reliance on iterative solvers and high compute/memory demands. In this paper, we present a general DT learning framework that is amenable to acceleration on reconfigurable hardware such as FPGAs, enabling substantial speedup and energy efficiency. We compare our FPGA-based implementation with a multi-processing implementation in mobile GPU, which is a popular choice for AI in edge devices. Further, we compare both edge AI implementations with cloud GPU baseline. Specifically, our FPGA implementation achieves an 8.8x improvement in \text{performance-per-watt} for the MR task, a 28.5x reduction in DRAM footprint, and a 1.67x runtime speedup compared to cloud GPU baselines. On the other hand, mobile GPU achieves 2x better performance per watts but has 2x increase in runtime and 10x more DRAM footprint than FPGA. We show the usage of this technique in DT guided synthetic data generation for Type 1 Diabetes and proactive coronary artery disease detection.

</details>


### [136] [Fast Online Digital Twinning on FPGA for Mission Critical Applications](https://arxiv.org/abs/2512.17942)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.DC

TL;DR: 提出基于FPGA加速的数字孪生框架，用于边缘设备上的实时任务关键应用


<details>
  <summary>Details</summary>
Motivation: 任务关键应用（如空中防撞）需要极低延迟的实时数字孪生，但边缘设备的计算和内存带宽限制了复杂模型恢复管道的执行

Method: 开发FPGA加速的数字孪生框架，将GRU和密集层等关键神经网络组件卸载到可重构硬件进行高效并行执行

Result: 系统实现实时响应，运行速度比典型人类反应时间快5倍，证明在边缘平台上部署时间敏感、安全关键的数字孪生具有实际可行性

Conclusion: FPGA加速的数字孪生框架能够满足任务关键应用的实时性要求，为边缘设备上的时间敏感安全关键环境提供了可行的解决方案

Abstract: Digital twinning enables real-time simulation and predictive modeling by maintaining a continuously updated virtual representation of a physical system. In mission-critical applications, such as mid-air collision avoidance, these models must operate online with extremely low latency to ensure safety. However, executing complex Model Recovery (MR) pipelines on edge devices is limited by computational and memory bandwidth constraints. This paper introduces a fast, FPGA-accelerated digital twinning framework that offloads key neural components, including gated recurrent units (GRU) and dense layers, to reconfigurable hardware for efficient parallel execution. Our system achieves real-time responsiveness, operating five times faster than typical human reaction time, and demonstrates the practical viability of deploying digital twins on edge platforms for time-sensitive, safety-critical environments.

</details>


### [137] [ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training](https://arxiv.org/abs/2512.18127)
*Yi Yang,Ziyu Lin,Liesheng Wei*

Main category: cs.DC

TL;DR: ACE-Sync是一个自适应云边同步框架，通过注意力梯度预测、分层压缩和优化策略，在分布式训练中显著降低通信开销同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型在分布式训练中产生巨大通信开销，特别是在带宽受限或异构的云边环境中。传统的同步或固定压缩方法难以平衡通信成本、收敛稳定性和模型精度。

Method: 提出ACE-Sync框架，包含：(1)基于注意力的梯度重要性预测器，(2)差异化参数压缩策略，(3)分层云边协调机制。采用背包优化策略最大化重要梯度保留，同时减少冗余通信，结合残差误差补偿和设备聚类确保长期收敛和跨设备个性化。

Result: 相比FullSync，通信成本从112.5GB降至44.7GB（减少60%），收敛轮数从41轮缩短至39轮。在激进通信减少下仍保持82.1%的Top-1准确率，仅比全同步基线低0.3%。

Conclusion: ACE-Sync为大规模云边分布式模型训练提供了一个可扩展、通信高效且精度保持的解决方案，在显著降低通信开销的同时维持了竞争性的模型精度。

Abstract: Large-scale deep learning models impose substantial communication overh ead in distributed training, particularly in bandwidth-constrained or heterogeneous clo ud-edge environments. Conventional synchronous or fixed-compression techniques o ften struggle to balance communication cost, convergence stability, and model accura cy. To address these challenges, we propose ACE-Sync, an Adaptive Cloud-Edge Sy nchronization Framework that integrates (1) an attention-based gradient importance p redictor, (2) a differentiated parameter compression strategy, and (3) a hierarchical cl oud-edge coordination mechanism. ACE-Sync dynamically selects which parameter groups to synchronize and determines appropriate compression levels under per-devic e bandwidth budgets. A knapsack-based optimization strategy is adopted to maximize important gradient preservation while reducing redundant communication. Furthermo re, residual-based error compensation and device clustering ensure long-term converg ence and cross-device personalization. Experiments show that ACE-Sync substantiall y reduces communication overhead while maintaining competitive accuracy. Compar ed with FullSync, ACE-Sync lowers communication cost from 112.5 GB to 44.7 GB (a 60% reduction) and shortens convergence from 41 to 39 epochs. Despite aggressiv e communication reduction, ACE-Sync preserves high model quality, achieving 82. 1% Top-1 accuracy-only 0.3% below the full-synchronization baseline-demonstrating its efficiency and scalability for large-scale distributed training. These results indicate that ACE-Sync provides a scalable, communication-efficient, and accuracy-preservin g solution for large-scale cloud-edge distributed model training.

</details>


### [138] [TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale](https://arxiv.org/abs/2512.18194)
*Dongha Yoon,Younghoon Min,Hoshik Kim,Sam H. Noh,Jongryool Kim*

Main category: cs.DC

TL;DR: TraCT是一个基于CXL共享内存的机架级LLM服务系统，通过消除NIC跳数来优化KV张量传输，显著降低TTFT和延迟，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有解耦LLM服务架构中，KV张量传输成为瓶颈，随着模型规模和上下文长度增加，KV传输主导TTFT和吞吐量，且对网络竞争高度敏感。

Method: 使用CXL共享内存作为KV传输基板和机架级前缀感知KV缓存，让GPU通过CXL加载/存储和DMA操作直接读写KV块，提出两层级节点间同步机制解决非一致性CXL内存的同步、一致性和数据管理问题。

Result: 在静态和合成工作负载上，TraCT相比RDMA和DRAM缓存基线，平均TTFT降低高达9.8倍，P99延迟降低高达6.2倍，峰值吞吐量提升高达1.6倍。

Conclusion: TraCT通过CXL共享内存有效解决了解耦LLM服务中的KV传输瓶颈，显著提升了服务性能和资源效率。

Abstract: Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.

</details>


### [139] [Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching](https://arxiv.org/abs/2512.18334)
*Hussein Amro,Basel Fakhri,Amer E. Mouawad,Izzat El Hajj*

Main category: cs.DC

TL;DR: 提出一种新颖的GPU解决方案，用于顶点覆盖问题，通过检测图分裂为组件并独立分支处理，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有GPU解决方案无法扩展到大型复杂图，因为它们无法检测图何时分裂为组件，导致冗余计算，且内存占用高限制了并发工作线程数量

Method: 1) 检测图分裂为组件并独立分支处理；2) 通过将后处理委托给每个分支的最后后代来解决非尾递归分支的负载均衡问题；3) 通过缩减图和诱导子图来减少内存占用

Result: 解决方案显著优于现有GPU方案，在现有方案需要6小时以上的情况下，新方案仅需数秒完成

Conclusion: 这是首个在GPU上以负载均衡方式并行化非尾递归分支模式的工作，为大规模图顶点覆盖问题提供了高效的GPU解决方案

Abstract: Algorithms for finding minimum or bounded vertex covers in graphs use a branch-and-reduce strategy, which involves exploring a highly imbalanced search tree. Prior GPU solutions assign different thread blocks to different sub-trees, while using a shared worklist to balance the load. However, these prior solutions do not scale to large and complex graphs because their unawareness of when the graph splits into components causes them to solve these components redundantly. Moreover, their high memory footprint limits the number of workers that can execute concurrently. We propose a novel GPU solution for vertex cover problems that detects when a graph splits into components and branches on the components independently. Although the need to aggregate the solutions of different components introduces non-tail-recursive branches which interfere with load balancing, we overcome this challenge by delegating the post-processing to the last descendant of each branch. We also reduce the memory footprint by reducing the graph and inducing a subgraph before exploring the search tree. Our solution substantially outperforms the state-of-the-art GPU solution, finishing in seconds when the state-of-the-art solution exceeds 6 hours. To the best of our knowledge, our work is the first to parallelize non-tail-recursive branching patterns on GPUs in a load balanced manner.

</details>


### [140] [Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing](https://arxiv.org/abs/2512.18674)
*Wentao Liu,Yuhao Hu,Ruiting Zhou,Baochun Li,Ne Wang*

Main category: cs.DC

TL;DR: Remoe：面向无服务器计算的异构MoE推理系统，通过将专家模块分配到CPU、将不常用专家卸载到独立函数，结合语义相似性预测、内存预分配和联合优化，降低推理成本57%、冷启动延迟47%


<details>
  <summary>Details</summary>
Motivation: MoE架构在大语言模型中广泛应用，但大量专家导致内存密集型参数缓存，推理成本高昂。无服务器计算虽然适合突发工作负载，但MoE的输入依赖专家激活模式使得简单模型分区难以降低成本。

Method: 1) 异构架构：非专家模块分配到GPU，专家模块分配到CPU，不常用专家卸载到独立无服务器函数；2) SPS算法：基于输入语义相似性预测专家激活模式；3) MMP算法：通过最坏情况内存估计确保SLO；4) 联合内存和副本优化框架：利用拉格朗日对偶和LPT算法

Result: 在Kubernetes上实现Remoe，多LLM基准测试显示：相比最先进基线，推理成本降低达57%，冷启动延迟降低47%

Conclusion: Remoe通过异构架构设计和智能资源管理，有效解决了MoE在无服务器环境中的高成本问题，为大规模MoE模型部署提供了经济高效的解决方案。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.

</details>


### [141] [A Real-Time Digital Twin for Adaptive Scheduling](https://arxiv.org/abs/2512.18894)
*Yihe Zhang,Yash Kurkure,Yiheng Tao,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: SchedTwin：一个实时数字孪生系统，通过预测性仿真自适应指导HPC集群调度决策，相比静态启发式策略表现更优且开销低。


<details>
  <summary>Details</summary>
Motivation: HPC工作负载日益多样化，作业特征差异大，但集群调度长期依赖静态启发式策略，缺乏自适应能力。

Method: SchedTwin周期性从物理调度器获取运行时事件，使用高保真离散事件仿真器快速评估多种策略的假设情景，根据管理员配置的优化目标动态选择最佳策略。

Result: SchedTwin在开源实现并与生产PBS调度器集成后，初步结果显示其持续优于广泛使用的静态调度策略，同时保持低开销（每个调度周期仅几秒）。

Conclusion: 实时数字孪生为自适应HPC调度提供了一条实用有效的路径。

Abstract: High-performance computing (HPC) workloads are becoming increasingly diverse, exhibiting wide variability in job characteristics, yet cluster scheduling has long relied on static, heuristic-based policies. In this work we present SchedTwin, a real-time digital twin designed to adaptively guide scheduling decisions using predictive simulation. SchedTwin periodically ingests runtime events from the physical scheduler, performs rapid what-if evaluations of multiple policies using a high-fidelity discrete-event simulator, and dynamically selects the one satisfying the administrator configured optimization goal. We implement SchedTwin as an open-source software and integrate it with the production PBS scheduler. Preliminary results show that SchedTwin consistently outperforms widely used static scheduling policies, while maintaining low overhead (a few seconds per scheduling cycle). These results demonstrate that real-time digital twins offer a practical and effective path toward adaptive HPC scheduling.

</details>


### [142] [Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT](https://arxiv.org/abs/2512.19131)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Murmura是一个基于证据深度学习的去中心化联邦学习框架，通过证据不确定性评估节点兼容性，实现信任感知的模型个性化聚合，在异构数据环境下显著提升性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习面临统计异构性的挑战：节点需要学习适应本地分布的个性化模型，同时与兼容的节点进行选择性协作。现有方法要么强制使用单一全局模型（对所有节点都不理想），要么依赖启发式的节点选择机制（无法区分真正不兼容的数据分布和有价值的互补知识）。

Method: Murmura利用证据深度学习，通过狄利克雷证据模型产生的认知不确定性来评估节点兼容性。当节点的模型评估本地数据时，高认知不确定性表明分布不匹配。框架引入信任感知聚合机制：1）通过本地验证样本的交叉评估计算节点兼容性分数；2）基于证据信任和自适应阈值进行个性化模型聚合。

Result: 在三个可穿戴物联网数据集（UCI HAR、PAMAP2、PPG-DaLiA）上的评估显示：1）从IID到非IID条件下，性能下降显著减少（0.9% vs. 19.3%基线）；2）实现7.4倍的更快收敛；3）在不同超参数选择下保持稳定的准确性。

Conclusion: 证据不确定性为去中心化异构环境中的兼容性感知个性化提供了原则性基础，Murmura框架通过信任感知的模型个性化，有效解决了统计异构性挑战，在保持个性化模型的同时实现选择性协作。

Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.

</details>


### [143] [L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179)
*Yitao Yuan,Chenqi Zhao,Bohan Zhao,Zane Cao,Yongchao He,Wenfei Wu*

Main category: cs.DC

TL;DR: L4是一个针对大语言模型服务的运行时系统，通过动态重新调度请求到专门处理特定长度范围的实例组，减少长度异质性带来的性能瓶颈，显著提升GPU利用率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理引擎调度器忽视了注意力后端对批次内请求长度异质性的敏感性。随着模型支持超过128K tokens的上下文窗口，这种低效性已成为主要系统瓶颈，导致GPU利用率不足和延迟增加。

Method: L4将多个服务同一LLM的实例划分为长度专门化组，每个组处理特定长度范围的请求，形成流水线。采用动态规划算法寻找最佳QoE的阶段划分，结合运行时范围细化和跨组/组内去中心化负载（重新）平衡。

Result: 在相同配置下，L4将端到端延迟降低高达67%，尾部延迟降低高达69%，同时相比最先进的多实例调度系统，整体系统吞吐量提升高达2.89倍。

Conclusion: L4通过动态重新调度请求到长度专门化实例组，有效解决了LLM服务中因请求长度异质性导致的性能瓶颈，显著提升了GPU利用率和系统效率。

Abstract: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.

</details>


### [144] [Simulations between Strongly Sublinear MPC and Node-Capacitated Clique](https://arxiv.org/abs/2512.19326)
*Philipp Schneider,Julian Werthmann*

Main category: cs.DC

TL;DR: 本文研究了强次线性MPC模型与经典图中心分布式模型（特别是Node-Capacitated Clique）之间的关系，探讨了在总内存和总带宽匹配条件下，模型间轮次保持模拟的可能性和不可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解强次线性MPC模型与经典分布式计算模型（如Node-Capacitated Clique）之间的理论关系。这些模型在内存限制和通信能力方面存在差异，研究者希望确定在什么条件下可以实现高效的模型间模拟，以及这种模拟的局限性。

Method: 研究方法包括：1）建立模型间的形式化对应关系，特别是在总内存和总带宽匹配的条件下；2）开发正面的模拟技术，能够复制输入表示、机器数量和本地内存等特定行为；3）证明负面的不可能性结果，展示模拟的固有局限性。

Result: 研究结果包括：1）提出了能够以常数开销实现模型间模拟的技术；2）证明了在某些情况下模拟是不可能的，这些不可能性结果显示了所提出模拟技术的局限性是必要的；3）确定了不同问题家族和图类别下模拟的可能性和不可能性条件。

Conclusion: 结论是：在强次线性机制下，当总内存和总带宽匹配时，MPC和NCC模型之间存在复杂的模拟关系。虽然在某些条件下可以实现高效的常数开销模拟，但也存在固有的不可能性，这些结果共同描绘了分布式计算模型间转换的理论边界。

Abstract: We study how the strongly sublinear MPC model relates to the classic, graph-centric distributed models, focusing on the Node-Capacitated Clique (NCC), a bandwidth-parametrized generalization of the Congested Clique. In MPC, $M$ machines with per-machine memory $S$ hold a partition of the input graph, in NCC, each node knows its full neighborhood but can send/receive only a bounded number of $C$ words per round. We are particularly interested in the strongly sublinear regime where $S=C=n^δ$ for some constant $0 < δ<1$.
  Our goal is determine when round-preserving simulations between these models are possible and when they are not, when total memory and total bandwidth $SM=nC$ in both models are matched, for different problem families and graph classes. On the positive side, we provide techniques that allow us to replicate the specific behavior regarding input representation, number of machines and local memory from one model to the other to obtain simulations with only constant overhead. On the negative side, we prove simulation impossibility results, which show that the limitations of our simulations are necessary.

</details>


### [145] [Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342)
*Kiril Dichev,Filip Pawlowski,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: 提出有界滞后同步(BLS)的alltoallv操作，用于减少分布式深度学习推荐模型推理中的通信瓶颈，特别适用于不平衡负载场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型(DLRM)推理的主要瓶颈是跨多个节点的稀疏特征查找，这通过不规则的alltoallv通信实现。现有方法都是同步的，当进程负载不平衡时会造成性能下降。

Method: 提出有界滞后同步(BLS)版本的alltoallv操作，允许较慢进程在最快进程阻塞前滞后一定迭代次数。在PyTorch Distributed中实现BLS后端，并与参考DLRM代码集成。

Result: 对于平衡的、同质访问的DLRM运行，BLS技术没有显著优势。但对于不平衡运行（如高度不规则的嵌入表访问或进程间延迟），BLS技术改善了推理延迟和吞吐量。在最佳情况下，可以减少同步开销完全掩盖进程间延迟。

Conclusion: BLS alltoallv操作特别适用于推理场景，能够在不影响准确性的前提下，通过减少同步开销来提升不平衡负载下的DLRM推理性能。

Abstract: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [146] [Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory](https://arxiv.org/abs/2512.19445)
*Guan-Cheng Chen,Chieh-Lin Tsai,Pei-Hsuan Tsai,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: 提出一种结合敏感度分析和混合精度的结构化量化方法，用于优化ReRAM存内计算系统的性能与能效


<details>
  <summary>Details</summary>
Motivation: 传统量化压缩技术无法充分优化存内计算架构的性能和效率，特别是基于ReRAM的系统需要更有效的优化方法

Method: 结合敏感度分析和混合精度策略的结构化量化方法，优化ReRAM交叉开关阵列的利用率

Result: 在70%压缩率下达到86.33%准确率，功耗降低40%，显著减少延迟和计算负载

Conclusion: 该方法有效提升了ReRAM存内计算系统的能效和性能，适用于功耗受限的应用场景

Abstract: Compute-In-Memory (CIM) systems, particularly those utilizing ReRAM and memristive technologies, offer a promising path toward energy-efficient neural network computation. However, conventional quantization and compression techniques often fail to fully optimize performance and efficiency in these architectures. In this work, we present a structured quantization method that combines sensitivity analysis with mixed-precision strategies to enhance weight storage and computational performance on ReRAM-based CIM systems. Our approach improves ReRAM Crossbar utilization, significantly reducing power consumption, latency, and computational load, while maintaining high accuracy. Experimental results show 86.33% accuracy at 70% compression, alongside a 40% reduction in power consumption, demonstrating the method's effectiveness for power-constrained applications.

</details>


### [147] [Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference](https://arxiv.org/abs/2512.18152)
*Rui Xie,Yunhua Fang,Asad Ul Haq,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: REACH是一种控制器管理的ECC设计，通过两级Reed-Solomon编码方案，在保持HBM接口不变的情况下，将可容忍的原始误码率提高约三个数量级，同时减少ECC面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 当前HBM堆栈中的片上ECC设计限制了芯片筛选、提高了价格，并将可靠性策略固定在设备内部。本文探讨系统是否能在不改变HBM PHY和32B传输大小的情况下，容忍更高的原始HBM误码率，同时保持端到端正确性和吞吐量。

Method: 提出REACH控制器管理ECC设计，采用两级Reed-Solomon方案：内层编码用于本地检查和纠正大多数故障，无法修复的块标记为擦除；外层编码跨越千字节，仅以擦除模式运行，修复标记的块而避免昂贵的定位步骤。对于小型随机写入，使用差分奇偶校验更新外层奇偶校验，并可选择重要性自适应位平面策略保护关键字段。

Result: 在8K上下文下的三个LLM上，REACH在零BER时保持约79%的片上ECC吞吐量，在原始BER为1e-3时仍保持合格，将可容忍设备误码率提高约三个数量级，同时保持每秒令牌数几乎不变。在ASAP7中，完整REACH控制器占用15.2 mm²，功耗17.5W，相比朴素的长Reed-Solomon基线减少ECC面积11.6倍，功耗降低约60%。

Conclusion: 通过将强ECC移至控制器，REACH将长代码可靠性转变为系统选择，可以在相同标准接口下实现更低成本的HBM。

Abstract: LLM inference is increasingly memory bound, and HBM cost per GB dominates system cost. Current HBM stacks include short on-die ECC that tightens binning, raises price, and fixes reliability policy inside the device. This paper asks whether a system can tolerate a much higher raw HBM bit error rate and still keep end-to-end correctness and throughput, without changing the HBM PHY or the fixed 32 B transaction size. We propose REACH, a controller managed ECC design that keeps the HBM link and 32 B transfers unchanged. REACH uses a two level Reed-Solomon scheme: each 32 B chunk uses an inner code to check and correct most faults locally, while chunks that cannot be fixed are marked as erasures. An outer code spans kilobytes and runs in erasure only mode, repairing only flagged chunks and avoiding the expensive locator step. For small random writes, REACH updates outer parity with differential parity to avoid recomputing parity over the whole span, and an optional importance adaptive bit plane policy can protect only critical fields such as BF16 exponents to reduce ECC work and traffic. On three LLMs at 8K context, REACH keeps about 79 percent of on-die ECC throughput at zero BER and remains qualified up to a raw BER of 1e-3, extending tolerable device error rates by about three orders of magnitude while keeping tokens per second nearly flat. In ASAP7, a full REACH controller occupies 15.2 mm2 and consumes 17.5 W at 3.56 TB/s, and it reduces ECC area by 11.6x and power by about 60 percent compared to a naive long Reed-Solomon baseline. By moving strong ECC into the controller, REACH turns long code reliability into a system choice that can enable lower cost HBM under the same standard interface.

</details>


### [148] [PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM](https://arxiv.org/abs/2512.18158)
*Tsung-Han Lu,Zheyu Li,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: PIM-FW：一种基于内存处理（PIM）架构的硬件-软件协同设计，用于加速Floyd-Warshall全对最短路径算法，通过内存内计算消除数据移动瓶颈，相比GPU方案获得18.7倍加速和3200倍DRAM能耗降低。


<details>
  <summary>Details</summary>
Motivation: 传统Floyd-Warshall算法在CPU/GPU上存在立方时间复杂度和大量数据移动问题，限制了其可扩展性。需要新的架构设计来解决这些瓶颈。

Method: 提出PIM-FW硬件架构：1）使用HBM3堆栈的内存处理架构；2）设计专用位串行bank PE和channel PE实现细粒度并行；3）采用交错映射策略实现负载均衡；4）混合内存内和近内存计算模型；5）在内存bank内完成所有距离更新计算和存储。

Result: 在8192×8192图上，PIM-FW相比最先进的GPU-only Floyd-Warshall实现：1）端到端执行速度提升18.7倍；2）DRAM能耗降低3200倍。

Conclusion: PIM-FW通过硬件-软件协同设计和内存内计算方法，有效解决了传统FW算法的数据移动瓶颈，显著提升了全对最短路径算法的性能和能效。

Abstract: All-pairs shortest paths (APSP) is a fundamental algorithm used for routing, logistics, and network analysis, but the cubic time complexity and heavy data movement of the canonical Floyd-Warshall (FW) algorithm severely limits its scalability on conventional CPUs or GPUs. In this paper, we propose PIM-FW, a novel co-designed hardware architecture and dataflow that leverages processing in and near memory architecture designed to accelerate blocked FW algorithm on an HBM3 stack. To enable fine-grained parallelism, we propose a massively parallel array of specialized bit-serial bank PE and channel PE designed to accelerate the core min-plus operations. Our novel dataflow complements this hardware, employing an interleaved mapping policy for superior load balancing and hybrid in and near memory computing model for efficient computation and reduction. The novel in-bank computing approach allows all distance updates to be performed and stored in memory bank, a key contribution is that eliminates the data movement bottleneck inherent in GPU-based approaches. We implement a full software and hardware co-design using a cycle-accurate simulator to simulate an 8-channel, 4-Hi HBM3 PIM stack on real road-network traces. Experimental results show that, for a 8192 x 8192 graph, PIM-FW achieves a 18.7x speedup in end-to-end execution, and consumes 3200x less DRAM energy compared to a state-of-the-art GPU-only Floyd-Warshall.

</details>


### [149] [BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism](https://arxiv.org/abs/2512.18300)
*Suhas Vittal,Moinuddin Qureshi*

Main category: cs.AR

TL;DR: 提出BARD（Bank-Aware Replacement Decisions）缓存替换策略，通过考虑DDR5 DRAM写入延迟差异来优化写入流，提高系统性能


<details>
  <summary>Details</summary>
Motivation: 现代系统在DDR5 DRAM写入时会阻塞读取请求，增加读取延迟。研究发现DDR5写入延迟差异巨大（1x到24x），但当前缓存替换策略不考虑这种差异，导致写入流效率低下

Method: 提出BARD策略：修改缓存替换策略，优先选择没有待处理写入的bank中的脏行。包括BARD-E（基于驱逐）、BARD-C（基于清理）和混合策略BARD-H

Result: 在SPEC2017、LIGRA、STREAM和Google服务器跟踪等负载上，BARD-H平均提升性能4.3%，最高达8.5%，仅需每个LLC切片8字节SRAM

Conclusion: 通过考虑DRAM写入延迟差异的银行感知缓存替换策略，可以有效减少写入阻塞时间，显著提升系统性能

Abstract: This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.
  Our paper proposes {\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\% on average and up-to 8.5\%. BARD requires only 8 bytes of SRAM per LLC slice.

</details>


### [150] [Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework](https://arxiv.org/abs/2512.18459)
*Akul Malhotra,Sumeet Kumar Gupta*

Main category: cs.AR

TL;DR: 提出两种无需训练的权重转换技术（sign-flip和bit-flip），增强多比特DNN在比特切片交叉阵列中的SAF容错能力，与CVM协同工作，无需重新训练或额外内存。


<details>
  <summary>Details</summary>
Motivation: CiM加速器部署DNN能减少数据移动，但内存单元中的SAF会损坏存储权重导致精度下降。现有CVM方法在高SAF率或复杂任务下容错能力不足。

Method: 提出两种权重转换技术：sign-flip在权重列级别选择权重或其负值；bit-flip在更细粒度上选择性反转单个比特切片。两者都扩展了故障感知映射的搜索空间，并与CVM协同工作。引入基于LUT的框架加速最优变换计算。

Result: 在ResNet-18、ResNet-50和ViT模型上实验表明，这些技术能恢复大部分SAF注入导致的精度损失。硬件分析显示开销可忽略：sign-flip几乎无能耗、延迟和面积成本，bit-flip以适度开销提供更高容错性。

Conclusion: sign-flip和bit-flip是CiM-based DNN加速器实用且可扩展的SAF缓解策略，无需训练，与CVM协同，硬件开销小。

Abstract: The deployment of deep neural networks (DNNs) on compute-in-memory (CiM) accelerators offers significant energy savings and speed-up by reducing data movement during inference. However, the reliability of CiM-based systems is challenged by stuck-at faults (SAFs) in memory cells, which corrupt stored weights and lead to accuracy degradation. While closest value mapping (CVM) has been shown to partially mitigate these effects for multibit DNNs deployed on bit-sliced crossbars, its fault tolerance is often insufficient under high SAF rates or for complex tasks. In this work, we propose two training-free weight transformation techniques, sign-flip and bit-flip, that enhance SAF tolerance in multi-bit DNNs deployed on bit-sliced crossbar arrays. Sign-flip operates at the weight-column level by selecting between a weight and its negation, whereas bit-flip provides finer granularity by selectively inverting individual bit slices. Both methods expand the search space for fault-aware mappings, operate synergistically with CVM, and require no retraining or additional memory. To enable scalability, we introduce a look-up-table (LUT)-based framework that accelerates the computation of optimal transformations and supports rapid evaluation across models and fault rates. Extensive experiments on ResNet-18, ResNet-50, and ViT models with CIFAR-100 and ImageNet demonstrate that the proposed techniques recover most of the accuracy lost under SAF injection. Hardware analysis shows that these methods incur negligible overhead, with sign-flip leading to negligible energy, latency, and area cost, and bit-flip providing higher fault resilience with modest overheads. These results establish sign-flip and bit-flip as practical and scalable SAF-mitigation strategies for CiM-based DNN accelerators.

</details>


### [151] [Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA](https://arxiv.org/abs/2512.19304)
*Emir Devlet Ertörer,Cem Ünsalan*

Main category: cs.AR

TL;DR: 该论文提出了一种完全定制的二进制神经网络推理加速器，用于手写数字识别，采用纯Verilog设计，无需高级综合工具，在FPGA上实现实时分类。


<details>
  <summary>Details</summary>
Motivation: 二进制神经网络通过用位运算替代浮点运算，为资源受限平台（如FPGA）提供了低功耗、高速推理的解决方案。然而，现有方法往往依赖高级综合工具，缺乏透明度和灵活性。

Method: 采用纯Verilog硬件描述语言手动设计BNN推理加速器，不使用高级综合工具。针对Xilinx Artix-7 FPGA平台，实现完全定制的硬件架构，支持80MHz实时分类。

Result: 在MNIST测试集上达到84%的准确率，在80MHz频率下实现实时分类，具有低功耗和可预测的时序特性。项目包含完整的训练脚本和Verilog源代码，已在GitHub开源。

Conclusion: 手动HDL设计为嵌入式系统中的BNN部署提供了透明、高效和灵活的解决方案，特别适合资源受限的FPGA平台，为可复现和未来开发奠定了基础。

Abstract: Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.

</details>
