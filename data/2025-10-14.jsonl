{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper."}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively."}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers."}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy."}
{"id": "2510.10484", "categories": ["cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10484", "abs": "https://arxiv.org/abs/2510.10484", "authors": ["Buqing Xu", "Jianfeng Zhu", "Yichi Zhang", "Qinyi Cai", "Guanhua Li", "Shaojun Wei", "Leibo Liu"], "title": "CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor", "comment": null, "summary": "CPU simulators are vital for computer architecture research, primarily for\nestimating performance under different programs. This poses challenges for fast\nand accurate simulation of modern CPUs, especially in multi-core systems.\nModern CPU peformance simulators such as GEM5 adopt the cycle-accurate and\nevent-driven approach, which is timeconsuming to simulate the extensive\nmicroarchitectural behavior of a real benchmark running on out-of-order CPUs.\nRecently, machine leaning based approach has been proposed to improve\nsimulation speed, but they are currently limited to estimating the cycles of\nbasic blocks rather than the complete benchmark program. This paper introduces\na novel ML-based CPU simulator named CAPSim, which uses an attention-based\nneural network performance predictor and instruction trace sampling method\nannotated with context. The attention mechanism effectively captures long-range\ninfluence within the instruction trace, emphasizing critical context\ninformation. This allows the model to improve performance prediction accuracy\nby focusing on important code instruction. CAPSim can predict the execution\ntime of unseen benchmarks at a significantly fast speed compared with an\naccurate O3 simulator built with gem5. Our evaluation on a commercial Intel\nXeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to\nusing gem5 built simulator, which is superior to the cutting-edge deep learning\napproach"}
{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."}
{"id": "2510.09643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09643", "abs": "https://arxiv.org/abs/2510.09643", "authors": ["Yuguang Liu", "Yiyun Miao", "Luyao Xia"], "title": "Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations", "comment": null, "summary": "Multi-task learning (MTL) has emerged as a successful strategy in\nindustrial-scale recommender systems, offering significant advantages such as\ncapturing diverse users' interests and accurately detecting different behaviors\nlike ``click\" or ``dwell time\". However, negative transfer and the seesaw\nphenomenon pose challenges to MTL models due to the complex and often\ncontradictory task correlations in real-world recommendations. To address the\nproblem while making better use of personalized information, we propose a\npersonalized Direct Routing Gradient framework (DRGrad), which consists of\nthree key components: router, updater and personalized gate network. DRGrad\njudges the stakes between tasks in the training process, which can leverage all\nvalid gradients for the respective task to reduce conflicts. We evaluate the\nefficiency of DRGrad on complex MTL using a real-world recommendation dataset\nwith 15 billion samples. The results show that DRGrad's superior performance\nover competing state-of-the-art MTL models, especially in terms of AUC (Area\nUnder the Curve) metrics, indicating that it effectively manages task conflicts\nin multi-task learning environments without increasing model complexity, while\nalso addressing the deficiencies in noise processing. Moreover, experiments on\nthe public Census-income dataset and Synthetic dataset, have demonstrated the\ncapability of DRGrad in judging and routing the stakes between tasks with\nvarying degrees of correlation and personalization."}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["João Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations."}
{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."}
{"id": "2510.09851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09851", "abs": "https://arxiv.org/abs/2510.09851", "authors": ["Haci Ismail Aslan", "Syed Muhammad Mahmudul Haque", "Joel Witzke", "Odej Kao"], "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters", "comment": "Accepted at the International Conference on Service-Oriented\n  Computing (ICSOC) 2025", "summary": "Modern applications increasingly span across cloud, fog, and edge\nenvironments, demanding orchestration systems that can adapt to diverse\ndeployment contexts while meeting Quality-of-Service (QoS) requirements.\nStandard Kubernetes schedulers do not account for user-defined objectives such\nas energy efficiency, cost optimization, and global performance, often leaving\noperators to make manual, cluster-by-cluster placement decisions. To address\nthis need, we present QONNECT, a vendor-agnostic orchestration framework that\nenables declarative, QoS-driven application deployment across heterogeneous\nKubernetes and K3s clusters. QONNECT introduces a distributed architecture\ncomposed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and\nlightweight Resource Agents in each cluster. Through a minimal YAML-based\ninterface, users specify high-level QoS goals, which the system translates into\nconcrete placement and migration actions. Our implementation is evaluated on a\nfederated testbed of up to nine cloud-fog-edge clusters using the Istio\nBookinfo microservice application. The system demonstrates dynamic,\npolicy-driven microservice placement, automated failover, QoS-compliant\nrescheduling, and leader re-election after node failure, all without manual\nintervention. By bridging the gap between declarative deployment models and\noperational QoS goals, QONNECT transforms the cloud-edge continuum into a\nunified, self-optimizing platform."}
{"id": "2510.09644", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09644", "abs": "https://arxiv.org/abs/2510.09644", "authors": ["Shaharyar Alam Ansari", "Mohammad Luqman", "Aasim Zafar", "Savir Ali"], "title": "Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining", "comment": "24 pages, 9 figures", "summary": "Rapid urbanization has intensified traffic congestion, environmental strain,\nand inefficiencies in transportation systems, creating an urgent need for\nintelligent and adaptive traffic management solutions. Conventional systems\nrelying on static signals and manual monitoring are inadequate for the dynamic\nnature of modern traffic. This research aims to develop a unified framework\nthat integrates CCTV surveillance videos with multi-source data descriptors to\nenhance real-time urban traffic prediction. The proposed methodology\nincorporates spatio-temporal feature fusion, Frequent Episode Mining for\nsequential traffic pattern discovery, and a hybrid LSTM-Transformer model for\nrobust traffic state forecasting. The framework was evaluated on the CityFlowV2\ndataset comprising 313,931 annotated bounding boxes across 46 cameras. It\nachieved a high prediction accuracy of 98.46 percent, with a macro precision of\n0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis\nrevealed significant sequential patterns such as moderate-congested transitions\nwith confidence levels exceeding 55 percent. The 46 sustained congestion alerts\nare system-generated, which shows practical value for proactive congestion\nmanagement. This emphasizes the need for the incorporation of video stream\nanalytics with data from multiple sources for the design of real-time,\nresponsive, adaptable multi-level intelligent transportation systems, which\nmakes urban mobility smarter and safer."}
{"id": "2510.10862", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10862", "abs": "https://arxiv.org/abs/2510.10862", "authors": ["Samuel Yuan", "Divyanshu Saxena", "Jiayi Chen", "Nihal Sharma", "Aditya Akella"], "title": "A Joint Learning Approach to Hardware Caching and Prefetching", "comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs)."}
{"id": "2510.10126", "categories": ["cs.DC", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10126", "abs": "https://arxiv.org/abs/2510.10126", "authors": ["Sehar Zehra", "Hassan Jamil Syed", "Ummay Faseeha"], "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments", "comment": "7 pages , 6 figures , 1 table and it is a conference paper", "summary": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving\nanomaly detection. Existing eBPF-based monitors provide low-overhead system and\nnetwork visibility but are limited to single clusters, while centralized\napproaches incur bandwidth, privacy, and heterogeneity challenges. We propose\nFedMon, a federated eBPF framework that unifies kernel-level telemetry with\nfederated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF\nagents capture syscalls and network events, extract local statistical and\nsequence features, and share only model updates with a global server. A hybrid\ndetection engine combining Variational Autoencoders (VAEs) with Isolation\nForests enables both temporal pattern modeling and outlier detection. Deployed\nacross three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,\nand an F1-score of 0.92, while cutting bandwidth usage by 60% relative to\ncentralized baselines. Results demonstrate that FedMon enhances accuracy,\nscalability, and privacy, providing an effective defense for large-scale,\nmulti-tenant cloud-native environments."}
{"id": "2510.09657", "categories": ["cs.LG", "cs.AI", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.09657", "abs": "https://arxiv.org/abs/2510.09657", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Fabrizio Frezza", "Aurelio Uncini", "Danilo Comminiello"], "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials", "comment": "Accepted at EUSIPCO 2025", "summary": "Accurate simulation of wave propagation in complex acoustic materials is\ncrucial for applications in sound design, noise control, and material\nengineering. Traditional numerical solvers, such as finite element methods, are\ncomputationally expensive, especially when dealing with large-scale or\nreal-time scenarios. In this work, we introduce a dataset of 31,000 acoustic\nmaterials, named HA30K, designed and simulated solving the Helmholtz equations.\nFor each material, we provide the geometric configuration and the corresponding\npressure field solution, enabling data-driven approaches to learn Helmholtz\nequation solutions. As a baseline, we explore a deep learning approach based on\nStable Diffusion with ControlNet, a state-of-the-art model for image\ngeneration. Unlike classical solvers, our approach leverages GPU\nparallelization to process multiple simulations simultaneously, drastically\nreducing computation time. By representing solutions as images, we bypass the\nneed for complex simulation software and explicit equation-solving.\nAdditionally, the number of diffusion steps can be adjusted at inference time,\nbalancing speed and quality. We aim to demonstrate that deep learning-based\nmethods are particularly useful in early-stage research, where rapid\nexploration is more critical than absolute accuracy."}
{"id": "2510.11484", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.11484", "abs": "https://arxiv.org/abs/2510.11484", "authors": ["Lion Mueller", "Alberto Garcia-Ortiz", "Ardalan Najafi", "Adam Fuks", "Lennart Bamberg"], "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware", "comment": "Submitted to IEEE Embedded Systems Letters", "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems."}
{"id": "2510.10166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10166", "abs": "https://arxiv.org/abs/2510.10166", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing", "comment": null, "summary": "Edge computing allows for the decentralization of computing resources. This\ndecentralization is achieved through implementing microservice architectures,\nwhich require low latencies to meet stringent service level agreements (SLA)\nsuch as performance, reliability, and availability metrics. While cloud\ncomputing offers the large data storage and computation resources necessary to\nhandle peak demands, a hybrid cloud and edge environment is required to ensure\nSLA compliance. Several auto-scaling algorithms have been proposed to try to\nachieve these compliance challenges, but they suffer from performance issues\nand configuration complexity. This chapter provides a brief overview of edge\ncomputing architecture, its uses, benefits, and challenges for resource\nscaling. We then introduce Service Level Agreements, and existing research on\ndevising algorithms used in edge computing environments to meet these\nagreements, along with their benefits and drawbacks."}
{"id": "2510.09658", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09658", "abs": "https://arxiv.org/abs/2510.09658", "authors": ["Filippo Rinaldi", "Aniello Panariello", "Giacomo Salici", "Fengyuan Liu", "Marco Ciccone", "Angelo Porrello", "Simone Calderara"], "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models", "comment": null, "summary": "When a new release of a foundation model is published, practitioners\ntypically need to repeat full fine-tuning, even if the same task has already\nbeen solved in the previous version. A promising alternative is to reuse the\nparameter changes (i.e., task vectors) that capture how a model adapts to a\nspecific task. However, they often fail to transfer across different\npre-trained models due to their misaligned parameter space. In this work, we\nshow that the key to successful transfer lies in the sign structure of the\ngradients of the new model. Based on this insight, we propose GradFix, a novel\nmethod that approximates the ideal gradient sign structure and leverages it to\ntransfer knowledge using only a handful of labeled samples. Notably, this\nrequires no additional fine-tuning: the adaptation is achieved by computing a\nfew gradients at the target model and masking the source task vector\naccordingly. This yields an update that is locally aligned with the target loss\nlandscape, effectively rebasing the task vector onto the new pre-training. We\nprovide a theoretical guarantee that our method ensures first-order descent.\nEmpirically, we demonstrate significant performance gains on vision and\nlanguage benchmarks, consistently outperforming naive task vector addition and\nfew-shot fine-tuning."}
{"id": "2510.10302", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10302", "abs": "https://arxiv.org/abs/2510.10302", "authors": ["Liangkun Chen", "Zijian Wen", "Tian Wu", "Xiaoxi Zhang", "Chuan Wu"], "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large\nlanguage models (LLMs) to reduce computation cost through model sparsity.\nEmploying speculative decoding (SD) can further accelerate MoE inference by\ndrafting multiple tokens per step and verifying them in parallel. However,\ncombining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth\ncontention during multi-token verification. Existing MoE offloading systems are\nSD-agnostic and do not address this bottleneck. We present SP-MoE, the first\nSD-aware expert-offloading and compute-communication pipelining framework.\nSP-MoE introduces: (1) speculative expert prefetching that exploits structural\ncorrespondence between the draft and target models to prefetch likely experts\nahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch\ndepth based on empirical profiles and an analytical latency model, guaranteeing\njust-in-time availability without overfetch; and (3) a pipelined runtime with\nasynchronous prefetch threads and batched I/O to hide loading latency.\nExtensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT\nspeedup over state-of-the-art methods across diverse datasets, environments,\nand MoE-based models."}
{"id": "2510.09659", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.09659", "abs": "https://arxiv.org/abs/2510.09659", "authors": ["Edgar E. Robles", "Dikshant Sagar", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi", "NOvA Collaboration"], "title": "Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors", "comment": "Submitted to Machine Learning and the Physical Sciences Workshop\n  (ML4PS) at NeurIPS 2025", "summary": "NOvA is a long-baseline neutrino oscillation experiment that detects neutrino\nparticles from the NuMI beam at Fermilab. Before data from this experiment can\nbe used in analyses, raw hits in the detector must be matched to their source\nparticles, and the type of each particle must be identified. This task has\ncommonly been done using a mix of traditional clustering approaches and\nconvolutional neural networks (CNNs). Due to the construction of the detector,\nthe data is presented as two sparse 2D images: an XZ and a YZ view of the\ndetector, rather than a 3D representation. We propose a point set neural\nnetwork that operates on the sparse matrices with an operation that mixes\ninformation from both views. Our model uses less than 10% of the memory\nrequired using previous methods while achieving a 96.8% AUC score, a higher\nscore than obtained when both views are processed independently (85.4%)."}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines."}
{"id": "2510.09660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09660", "abs": "https://arxiv.org/abs/2510.09660", "authors": ["Luca Scimeca", "Thomas Jiralerspong", "Berton Earnshaw", "Jason Hartford", "Yoshua Bengio"], "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have achieved strong generative\nperformance, yet their inductive biases remain largely implicit. In this work,\nwe aim to build inductive biases into the training and sampling of diffusion\nmodels to better accommodate the target distribution of the data to model. We\nintroduce an anisotropic noise operator that shapes these biases by replacing\nthe isotropic forward covariance with a structured, frequency-diagonal\ncovariance. This operator unifies band-pass masks and power-law weightings,\nallowing us to emphasize or suppress designated frequency bands, while keeping\nthe forward process Gaussian. We refer to this as spectrally anisotropic\nGaussian diffusion (SAGD). In this work, we derive the score relation for\nanisotropic covariances and show that, under full support, the learned score\nconverges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the\nprobability-flow path from noise to data. Empirically, we show the induced\nanisotropy outperforms standard diffusion across several vision datasets, and\nenables selective omission: learning while ignoring known corruptions confined\nto specific bands. Together, these results demonstrate that carefully designed\nanisotropic forward noise provides a simple, yet principled, handle to tailor\ninductive bias in DPMs."}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks."}
{"id": "2510.09662", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.09662", "abs": "https://arxiv.org/abs/2510.09662", "authors": ["Ali Jaberi", "Amin Sadeghi", "Runze Zhang", "Zhaoyang Zhao", "Qiuyu Shi", "Robert Black", "Zoya Sadighi", "Jason Hattrick-Simpers"], "title": "Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data", "comment": null, "summary": "Electrochemical impedance spectroscopy (EIS) data is typically modeled using\nan equivalent circuit model (ECM), with parameters obtained by minimizing a\nloss function via nonlinear least squares fitting. This paper introduces two\nnew loss functions, log-B and log-BW, derived from the Bode representation of\nEIS. Using a large dataset of generated EIS data, the performance of proposed\nloss functions was evaluated alongside existing ones in terms of R2 scores,\nchi-squared, computational efficiency, and the mean absolute percentage error\n(MAPE) between the predicted component values and the original values.\nStatistical comparisons revealed that the choice of loss function impacts\nconvergence, computational efficiency, quality of fit, and MAPE. Our analysis\nshowed that X2 loss function (squared sum of residuals with proportional\nweighting) achieved the highest performance across multiple quality of fit\nmetrics, making it the preferred choice when the quality of fit is the primary\ngoal. On the other hand, log-B offered a slightly lower quality of fit while\nbeing approximately 1.4 times faster and producing lower MAPE for most circuit\ncomponents, making log-B as a strong alternative. This is a critical factor for\nlarge-scale least squares fitting in data-driven applications, such as training\nmachine learning models on extensive datasets or iterations."}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs)."}
{"id": "2510.09664", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09664", "abs": "https://arxiv.org/abs/2510.09664", "authors": ["Changchang Sun", "Vickie Chen", "Yan Yan"], "title": "Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing", "comment": null, "summary": "Recently, deep supervised cross-modal hashing methods have achieve compelling\nsuccess by learning semantic information in a self-supervised way. However,\nthey still suffer from the key limitation that the multi-label semantic\nextraction process fail to explicitly interact with raw multimodal data, making\nthe learned representation-level semantic information not compatible with the\nheterogeneous multimodal data and hindering the performance of bridging\nmodality gap. To address this limitation, in this paper, we propose a novel\nsemantic cohesive knowledge distillation scheme for deep cross-modal hashing,\ndubbed as SODA. Specifically, the multi-label information is introduced as a\nnew textual modality and reformulated as a set of ground-truth label prompt,\ndepicting the semantics presented in the image like the text modality. Then, a\ncross-modal teacher network is devised to effectively distill cross-modal\nsemantic characteristics between image and label modalities and thus learn a\nwell-mapped Hamming space for image modality. In a sense, such Hamming space\ncan be regarded as a kind of prior knowledge to guide the learning of\ncross-modal student network and comprehensively preserve the semantic\nsimilarities between image and text modality. Extensive experiments on two\nbenchmark datasets demonstrate the superiority of our model over the\nstate-of-the-art methods."}
{"id": "2510.10818", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10818", "abs": "https://arxiv.org/abs/2510.10818", "authors": ["Kevin Chalmers", "Jan Bækgaard Pedersen"], "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes", "comment": null, "summary": "We present the first spin-free, kernel-lock-free mutex that cooperates with\nuser-mode schedulers and is formally proven FIFO-fair and linearizable using\nCSP/FDR. Our fairness oracle and stability-based proof method are reusable\nacross coroutine runtime designs. We designed the claim/release protocol for a\nprocess-oriented language -- ProcessJ -- to manage the race for claiming shared\ninter-process communication channels. Internally, we use a lock-free queue to\npark waiting processes for gaining access to a shared object, such as exclusive\naccess to a shared channel to read from or write to. The queue ensures control\nand fairness for processes wishing to access a shared resource, as the protocol\nhandles claim requests in the order they are inserted into the queue. We\nproduce CSP models of our protocol and a mutex specification, demonstrating\nwith FDR that our protocol behaves as a locking mutex."}
{"id": "2510.09665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09665", "abs": "https://arxiv.org/abs/2510.09665", "authors": ["Yihua Cheng", "Yuhan Liu", "Jiayi Yao", "Yuwei An", "Xiaokun Chen", "Shaoting Feng", "Yuyang Huang", "Samuel Shen", "Kuntai Du", "Junchen Jiang"], "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference", "comment": null, "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."}
{"id": "2510.10833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10833", "abs": "https://arxiv.org/abs/2510.10833", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed and\nreliable systems. In the proposed framework we have used three parts to\nincrease Satisfaction and Performance of this framework. At first we analyze\nprevious frameworks related to integrated systems, then represent new proposed\nframework in order to improving previous framework, and we discuss its\ndifferent phases. Finally we compare the results of simulation of the new\nframework with previous ones. In FIDRS framework, the technique of\nheterogeneous distributed data base is used to improve Performance and speed in\nresponding to users and in this way we can improve dependability and\nreliability of framework simultaneously. In extraction phase of the new\nframework we have used RMSD algorithm that decreases responding time in big\ndatabase. Finally by using FDIRS framework we succeeded to increase Efficiency,\nPerformance and reliability of integrated systems and remove some of previous\nframeworks problems."}
{"id": "2510.09666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09666", "abs": "https://arxiv.org/abs/2510.09666", "authors": ["Aditya Chakravarty"], "title": "Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning", "comment": null, "summary": "Climate change is intensifying wildfire risks globally, making reliable\nforecasting critical for adaptation strategies. While machine learning shows\npromise for wildfire prediction from Earth observation data, current approaches\nlack uncertainty quantification essential for risk-aware decision making. We\npresent the first systematic analysis of spatial uncertainty in wildfire spread\nforecasting using multimodal Earth observation inputs. We demonstrate that\npredictive uncertainty exhibits coherent spatial structure concentrated near\nfire perimeters. Our novel distance metric reveals high-uncertainty regions\nform consistent 20-60 meter buffer zones around predicted firelines - directly\napplicable for emergency planning. Feature attribution identifies vegetation\nhealth and fire activity as primary uncertainty drivers. This work enables more\nrobust wildfire management systems supporting communities adapting to\nincreasing fire risk under climate change."}
{"id": "2510.11189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11189", "abs": "https://arxiv.org/abs/2510.11189", "authors": ["Yangyang Wen", "Paul Townend", "Per-Olov Östberg", "Abel Souza", "Clément Courageux-Sudan"], "title": "A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems", "comment": "9 pages, 4 figures. Accepted at the 2025 IEEE Joint Cloud Computing\n  (JCC) track of IEEE CISOSE 2025. Conference: IEEE JCC 2025, 16th IEEE\n  International Conference on Joint Cloud Computing, Tucson, Arizona, USA, from\n  July 21 to 24, 2025", "summary": "As microservice-based systems scale across the cloud-edge continuum,\ntraditional centralized scheduling mechanisms increasingly struggle with\nlatency, coordination overhead, and fault tolerance. This paper presents a new\narchitectural direction: leveraging service mesh sidecar proxies as\ndecentralized, in-situ schedulers to enable scalable, low-latency coordination\nin large-scale, cloud-native environments. We propose embedding lightweight,\nautonomous scheduling logic into each sidecar, allowing scheduling decisions to\nbe made locally without centralized control. This approach leverages the\ngrowing maturity of service mesh infrastructures, which support programmable\ndistributed traffic management. We describe the design of such an architecture\nand present initial results demonstrating its scalability potential in terms of\nresponse time and latency under varying request rates. Rather than delivering a\nfinalized scheduling algorithm, this paper presents a system-level\narchitectural direction and preliminary evidence to support its scalability\npotential."}
{"id": "2510.09668", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making."}
{"id": "2510.11211", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11211", "abs": "https://arxiv.org/abs/2510.11211", "authors": ["Sheikh Azizul Hakim", "Saem Hasan"], "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models", "comment": null, "summary": "Large language models (LLM) are advanced AI systems trained on extensive\ntextual data, leveraging deep learning techniques to understand and generate\nhuman-like language. Today's LLMs with billions of parameters are so huge that\nhardly any single computing node can train, fine-tune, or infer from them.\nTherefore, several distributed computing techniques are being introduced in the\nliterature to properly utilize LLMs. We have explored the application of\ndistributed computing techniques in LLMs from two angles.\n  \\begin{itemize}\n  \\item We study the techniques that democratize the LLM, that is, how large\nmodels can be run on consumer-grade computers. Here, we also implement a novel\nmetaheuristics-based modification to an existing system.\n  \\item We perform a comparative study on three state-of-the-art LLM serving\ntechniques. \\end{itemize}"}
{"id": "2510.09669", "categories": ["cs.LG", "cs.CY", "cs.SI", "physics.soc-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09669", "abs": "https://arxiv.org/abs/2510.09669", "authors": ["Jacopo Lenti", "Lorenzo Costantini", "Ariadna Fosch", "Anna Monticelli", "David Scala", "Marco Pangallo"], "title": "Population synthesis with geographic coordinates", "comment": null, "summary": "It is increasingly important to generate synthetic populations with explicit\ncoordinates rather than coarse geographic areas, yet no established methods\nexist to achieve this. One reason is that latitude and longitude differ from\nother continuous variables, exhibiting large empty spaces and highly uneven\ndensities. To address this, we propose a population synthesis algorithm that\nfirst maps spatial coordinates into a more regular latent space using\nNormalizing Flows (NF), and then combines them with other features in a\nVariational Autoencoder (VAE) to generate synthetic populations. This approach\nalso learns the joint distribution between spatial and non-spatial features,\nexploiting spatial autocorrelations. We demonstrate the method by generating\nsynthetic homes with the same statistical properties of real homes in 121\ndatasets, corresponding to diverse geographies. We further propose an\nevaluation framework that measures both spatial accuracy and practical utility,\nwhile ensuring privacy preservation. Our results show that the NF+VAE\narchitecture outperforms popular benchmarks, including copula-based methods and\nuniform allocation within geographic areas. The ability to generate geolocated\nsynthetic populations at fine spatial resolution opens the door to applications\nrequiring detailed geography, from household responses to floods, to epidemic\nspread, evacuation planning, and transport modeling."}
{"id": "2510.11513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11513", "abs": "https://arxiv.org/abs/2510.11513", "authors": ["Alex Elwood", "Tom Deakin", "Justin Lovegrove", "Chris Nelson"], "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems", "comment": null, "summary": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a\nchallenge to scale due to complex data dependencies, memory access patterns and\na high-dimensional domain. In this paper, we review the performance bottlenecks\nwithin the shared memory parallelization scheme of an existing transport solver\non modern many-core architectures with high core counts. With this analysis, we\nthen survey the performance of this solver across a variety of compute\nhardware. We then present a new Asynchronous Many-Task (AMT) algorithm for\nshared memory parallelism, present results showing an increase in computational\nperformance over the existing method, and evaluate why performance is improved."}
{"id": "2510.09670", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09670", "abs": "https://arxiv.org/abs/2510.09670", "authors": ["Xinlun Cheng", "Bingzhe Chen", "Joseph Choi", "Yen T. Nguyen", "Pradeep Seshadri", "Mayank Verma", "H. S. Udaykumar", "Stephen Baek"], "title": "A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials", "comment": null, "summary": "Modeling shock-to-detonation phenomena in energetic materials (EMs) requires\ncapturing complex physical processes such as strong shocks, rapid changes in\nmicrostructural morphology, and nonlinear dynamics of chemical reaction fronts.\nThese processes participate in energy localization at hotspots, which initiate\nchemical energy release leading to detonation. This study addresses the\nformation of hotspots in crystalline EMs subjected to weak-to-moderate shock\nloading, which, despite its critical relevance to the safe storage and handling\nof EMs, remains underexplored compared to the well-studied strong shock\nconditions. To overcome the computational challenges associated with direct\nnumerical simulations, we advance the Physics-Aware Recurrent Convolutional\nNeural Network (PARCv2), which has been shown to be capable of predicting\nstrong shock responses in EMs. We improved the architecture of PARCv2 to\nrapidly predict shear localizations and plastic heating, which play important\nroles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two\nwidely used physics-informed models, namely, Fourier neural operator and neural\nordinary differential equation; we demonstrate its superior performance in\ncapturing the spatiotemporal dynamics of shear band formation. While all models\nexhibit certain failure modes, our findings underscore the importance of\ndomain-specific considerations in developing robust AI-accelerated simulation\ntools for reactive materials."}
{"id": "2510.11697", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11697", "abs": "https://arxiv.org/abs/2510.11697", "authors": ["Matteo Mordacchini", "Emanuele Carlini", "Patrizio Dazzi"], "title": "A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem", "comment": null, "summary": "We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in\na decentralized network. MWVC, a classical NP-hard problem, is foundational in\napplications such as network monitoring and resource placement. We propose a\nfully decentralized protocol where each node makes decisions using only local\nknowledge and communicates with its neighbors. The method is adaptive,\ncommunication-efficient, and avoids centralized coordination. We evaluate the\nprotocol on real-world and synthetic graphs, comparing it to both centralized\nand decentralized baselines. Our results demonstrate competitive solution\nquality with reduced communication overhead, highlighting the feasibility of\nMWVC computation in decentralized environments."}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks."}
{"id": "2510.10028", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10028", "abs": "https://arxiv.org/abs/2510.10028", "authors": ["Yang Li", "Ruichen Zhang", "Yinqiu Liu", "Guangyuan Liu", "Dusit Niyato", "Abbas Jamalipour", "Xianbin Wang", "Dong In Kim"], "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization", "comment": null, "summary": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled\na variety of applications, including aerial surveillance, environmental\nsensing, and semantic data collection. To support these scenarios, unmanned\naerial vehicles (UAVs) equipped with onboard vision-language models (VLMs)\noffer a promising solution for real-time multimodal inference. However,\nensuring both inference accuracy and communication efficiency remains a\nsignificant challenge due to limited onboard resources and dynamic network\nconditions. In this paper, we first propose a UAV-enabled LAENet system model\nthat jointly captures UAV mobility, user-UAV communication, and the onboard\nvisual question answering (VQA) pipeline. Based on this model, we formulate a\nmixed-integer non-convex optimization problem to minimize task latency and\npower consumption under user-specific accuracy constraints. To solve the\nproblem, we design a hierarchical optimization framework composed of two parts:\n(i) an Alternating Resolution and Power Optimization (ARPO) algorithm for\nresource allocation under accuracy constraints, and (ii) a Large Language\nModel-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV\ntrajectory optimization. The large language model (LLM) serves as an expert in\nrefining reward design of reinforcement learning in an offline fashion,\nintroducing no additional latency in real-time decision-making. Numerical\nresults demonstrate the efficacy of our proposed framework in improving\ninference performance and communication efficiency under dynamic LAENet\nconditions."}
{"id": "2510.09684", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09684", "abs": "https://arxiv.org/abs/2510.09684", "authors": ["Chris Engh", "P. M. Aronow"], "title": "Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation", "comment": null, "summary": "We propose a simple yet effective use of LLM-powered AI tools to improve\ncausal estimation. In double machine learning, the accuracy of causal estimates\nof the effect of a treatment on an outcome in the presence of a\nhigh-dimensional confounder depends on the performance of estimators of\nconditional expectation functions. We show that predictions made by generative\nmodels trained on historical data can be used to improve the performance of\nthese estimators relative to approaches that solely rely on adjusting for\nembeddings extracted from these models. We argue that the historical knowledge\nand reasoning capacities associated with these generative models can help\novercome curse-of-dimensionality problems in causal inference problems. We\nconsider a case study using a small dataset of online jewelry auctions, and\ndemonstrate that inclusion of LLM-generated guesses as predictors can improve\nefficiency in estimation."}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness."}
{"id": "2510.09685", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NA", "math.NA", "A.1; I.2; I.4"], "pdf": "https://arxiv.org/pdf/2510.09685", "abs": "https://arxiv.org/abs/2510.09685", "authors": ["Yongshuai Liu", "Lianfang Wang", "Kuilin Qin", "Qinghua Zhang", "Faqiang Wang", "Li Cui", "Jun Liu", "Yuping Duan", "Tieyong Zeng"], "title": "Deep Neural Networks Inspired by Differential Equations", "comment": "35 Pages, 3 figures", "summary": "Deep learning has become a pivotal technology in fields such as computer\nvision, scientific computing, and dynamical systems, significantly advancing\nthese disciplines. However, neural Networks persistently face challenges\nrelated to theoretical understanding, interpretability, and generalization. To\naddress these issues, researchers are increasingly adopting a differential\nequations perspective to propose a unified theoretical framework and systematic\ndesign methodologies for neural networks. In this paper, we provide an\nextensive review of deep neural network architectures and dynamic modeling\nmethods inspired by differential equations. We specifically examine deep neural\nnetwork models and deterministic dynamical network constructs based on ordinary\ndifferential equations (ODEs), as well as regularization techniques and\nstochastic dynamical network models informed by stochastic differential\nequations (SDEs). We present numerical comparisons of these models to\nillustrate their characteristics and performance. Finally, we explore promising\nresearch directions in integrating differential equations with deep learning to\noffer new insights for developing intelligent computational methods that boast\nenhanced interpretability and generalization capabilities."}
{"id": "2510.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09687", "abs": "https://arxiv.org/abs/2510.09687", "authors": ["Stanisław Pawlak"], "title": "On the Occurence of Critical Learning Periods in Neural Networks", "comment": "8 pages, 8 figures", "summary": "This study delves into the plasticity of neural networks, offering empirical\nsupport for the notion that critical learning periods and warm-starting\nperformance loss can be avoided through simple adjustments to learning\nhyperparameters. The critical learning phenomenon emerges when training is\ninitiated with deficit data. Subsequently, after numerous deficit epochs, the\nnetwork's plasticity wanes, impeding its capacity to achieve parity in accuracy\nwith models trained from scratch, even when extensive clean data training\nfollows deficit epochs. Building upon seminal research introducing critical\nlearning periods, we replicate key findings and broaden the experimental scope\nof the main experiment from the original work. In addition, we consider a\nwarm-starting approach and show that it can be seen as a form of deficit\npretraining. In particular, we demonstrate that these problems can be averted\nby employing a cyclic learning rate schedule. Our findings not only impact\nneural network training practices but also establish a vital link between\ncritical learning periods and ongoing research on warm-starting neural network\ntraining."}
{"id": "2510.09691", "categories": ["cs.LG", "cs.AI", "68T07, 68M14", "I.2.6; I.2.11; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.09691", "abs": "https://arxiv.org/abs/2510.09691", "authors": ["Tejash Varsani"], "title": "Evaluation of Differential Privacy Mechanisms on Federated Learning", "comment": "Supervised by Prof. Dr.-Ing. habil. Alois C. Knoll; Advisor:\n  Nagacharan Teja Tangirala, M.Sc", "summary": "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy."}
{"id": "2510.09693", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09693", "abs": "https://arxiv.org/abs/2510.09693", "authors": ["Jiakang Chen"], "title": "Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs", "comment": "50 pages, 13 figures", "summary": "Partial differential equations (PDEs) underpin models across science and\nengineering, yet analytical solutions are atypical and classical mesh-based\nsolvers can be costly in high dimensions. This dissertation presents a unified\ncomparison of three mesh-free neural PDE solvers, physics-informed neural\nnetworks (PINNs), the deep Ritz method (DRM), and weak adversarial networks\n(WANs), on Poisson problems (up to 5D) and the time-independent Schr\\\"odinger\nequation in 1D/2D (infinite well and harmonic oscillator), and extends the\nstudy to a laser-driven case of Schr\\\"odinger's equation via the\nKramers-Henneberger (KH) transformation.\n  Under a common protocol, all methods achieve low $L_2$ errors\n($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),\nforced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs\nare the most reliable for accuracy and recovery of excited spectra; DRM offers\nthe best accuracy-runtime trade-off on stationary problems; WAN is more\nsensitive but competitive when weak-form constraints and FN/OG are used\neffectively. Sensitivity analyses show that FBC removes boundary-loss tuning,\nnetwork width matters more than depth for single-network solvers, and most\ngains occur within 5000-10,000 epochs. The same toolkit solves the KH case,\nindicating transfer beyond canonical benchmarks.\n  We provide practical guidelines for method selection and outline the\nfollowing extensions: time-dependent formulations for DRM and WAN, adaptive\nresidual-driven sampling, parallel multi-state training, and neural domain\ndecomposition. These results support physics-guided neural solvers as credible,\nscalable tools for solving complex PDEs."}
{"id": "2510.09694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09694", "abs": "https://arxiv.org/abs/2510.09694", "authors": ["Xiaodan Li", "Mengjie Wu", "Yao Zhu", "Yunna Lv", "YueFeng Chen", "Cen Chen", "Jianmei Guo", "Hui Xue"], "title": "Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection", "comment": null, "summary": "Large models (LMs) are powerful content generators, yet their open-ended\nnature can also introduce potential risks, such as generating harmful or biased\ncontent. Existing guardrails mostly perform post-hoc detection that may expose\nunsafe content before it is caught, and the latency constraints further push\nthem toward lightweight models, limiting detection accuracy. In this work, we\npropose Kelp, a novel plug-in framework that enables streaming risk detection\nwithin the LM generation pipeline. Kelp leverages intermediate LM hidden states\nthrough a Streaming Latent Dynamics Head (SLD), which models the temporal\nevolution of risk across the generated sequence for more accurate real-time\nrisk detection. To ensure reliable streaming moderation in real applications,\nwe introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic\nharm predictions by embedding a benign-then-harmful temporal prior. Besides,\nfor a rigorous evaluation of streaming guardrails, we also present\nStreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from\neach protected model, reflecting real-world streaming scenarios in both text\nand vision-language tasks. Across diverse models and datasets, Kelp\nconsistently outperforms state-of-the-art post-hoc guardrails and prior plug-in\nprobes (15.61% higher average F1), while using only 20M parameters and adding\nless than 0.5 ms of per-token latency."}
{"id": "2510.09696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09696", "abs": "https://arxiv.org/abs/2510.09696", "authors": ["Lorenzo Nikiforos", "Charalampos Antoniadis", "Luciano Prono", "Fabio Pareschi", "Riccardo Rovatti", "Gianluca Setti"], "title": "Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form", "comment": "Code available at https://github.com/foros15/vanishing-contributions", "summary": "The increasing scale of deep neural networks has led to a growing need for\ncompression techniques such as pruning, quantization, and low-rank\ndecomposition. While these methods are very effective in reducing memory,\ncomputation and energy consumption, they often introduce severe accuracy\ndegradation when applied directly. We introduce Vanishing Contributions (VCON),\na general approach for smoothly transitioning neural models into compressed\nform. Rather than replacing the original network directly with its compressed\nversion, VCON executes the two in parallel during fine-tuning. The contribution\nof the original (uncompressed) model is progressively reduced, while that of\nthe compressed model is gradually increased. This smooth transition allows the\nnetwork to adapt over time, improving stability and mitigating accuracy\ndegradation. We evaluate VCON across computer vision and natural language\nprocessing benchmarks, in combination with multiple compression strategies.\nAcross all scenarios, VCON leads to consistent improvements: typical gains\nexceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus\nprovides a generalizable method that can be applied to the existing compression\ntechniques, with evidence of consistent gains across multiple benchmarks."}
{"id": "2510.09704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09704", "abs": "https://arxiv.org/abs/2510.09704", "authors": ["Matthew Schlegel", "Matthew E. Taylor", "Mostafa Farrokhabadi"], "title": "Operator Learning for Power Systems Simulation", "comment": null, "summary": "Time domain simulation, i.e., modeling the system's evolution over time, is a\ncrucial tool for studying and enhancing power system stability and dynamic\nperformance. However, these simulations become computationally intractable for\nrenewable-penetrated grids, due to the small simulation time step required to\ncapture renewable energy resources' ultra-fast dynamic phenomena in the range\nof 1-50 microseconds. This creates a critical need for solutions that are both\nfast and scalable, posing a major barrier for the stable integration of\nrenewable energy resources and thus climate change mitigation. This paper\nexplores operator learning, a family of machine learning methods that learn\nmappings between functions, as a surrogate model for these costly simulations.\nThe paper investigates, for the first time, the fundamental concept of\nsimulation time step-invariance, which enables models trained on coarse time\nsteps to generalize to fine-resolution dynamics. Three operator learning\nmethods are benchmarked on a simple test system that, while not incorporating\npractical complexities of renewable-penetrated grids, serves as a first\nproof-of-concept to demonstrate the viability of time step-invariance. Models\nare evaluated on (i) zero-shot super-resolution, where training is performed on\na coarse simulation time step and inference is performed at super-resolution,\nand (ii) generalization between stable and unstable dynamic regimes. This work\naddresses a key challenge in the integration of renewable energy for the\nmitigation of climate change by benchmarking operator learning methods to model\nphysical systems."}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge."}
{"id": "2510.09712", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09712", "abs": "https://arxiv.org/abs/2510.09712", "authors": ["Zhao Tong", "Chunlin Gong", "Yimeng Gu", "Haichao Shi", "Qiang Liu", "Shu Wu", "Xiao-Yu Zhang"], "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments", "comment": "10 pages, 12 figures", "summary": "The spread of fake news online distorts public judgment and erodes trust in\nsocial media platforms. Although recent fake news detection (FND) models\nperform well in standard settings, they remain vulnerable to adversarial\ncomments-authored by real users or by large language models (LLMs)-that subtly\nshift model decisions. In view of this, we first present a comprehensive\nevaluation of comment attacks to existing fake news detectors and then\nintroduce a group-adaptive adversarial training strategy to improve the\nrobustness of FND models. To be specific, our approach comprises three steps:\n(1) dividing adversarial comments into three psychologically grounded\ncategories: perceptual, cognitive, and societal; (2) generating diverse,\ncategory-specific attacks via LLMs to enhance adversarial training; and (3)\napplying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting\nMechanism) that dynamically adjusts the learning focus across different comment\ncategories during training. Experiments on benchmark datasets show that our\nmethod maintains strong detection accuracy while substantially increasing\nrobustness to a wide range of adversarial comment perturbations."}
{"id": "2510.09717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09717", "abs": "https://arxiv.org/abs/2510.09717", "authors": ["Zhenlong Liu", "Hao Zeng", "Weiran Huang", "Hongxin Wei"], "title": "High-Power Training Data Identification with Provable Statistical Guarantees", "comment": null, "summary": "Identifying training data within large-scale models is critical for copyright\nlitigation, privacy auditing, and ensuring fair evaluation. The conventional\napproaches treat it as a simple binary classification task without statistical\nguarantees. A recent approach is designed to control the false discovery rate\n(FDR), but its guarantees rely on strong, easily violated assumptions. In this\npaper, we introduce Provable Training Data Identification (PTDI), a rigorous\nmethod that identifies a set of training data with strict false discovery rate\n(FDR) control. Specifically, our method computes p-values for each data point\nusing a set of known unseen data, and then constructs a conservative estimator\nfor the data usage proportion of the test set, which allows us to scale these\np-values. Our approach then selects the final set of training data by\nidentifying all points whose scaled p-values fall below a data-dependent\nthreshold. This entire procedure enables the discovery of training data with\nprovable, strict FDR control and significantly boosted power. Extensive\nexperiments across a wide range of models (LLMs and VLMs), and datasets\ndemonstrate that PTDI strictly controls the FDR and achieves higher power."}
{"id": "2510.09718", "categories": ["cs.LG", "68T05, 68T10", "I.5.3; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.09718", "abs": "https://arxiv.org/abs/2510.09718", "authors": ["A. Jung"], "title": "Federated k-Means via Generalized Total Variation Minimization", "comment": null, "summary": "We consider the problem of federated clustering, where interconnected devices\nhave access to private local datasets and need to jointly cluster the overall\ndataset without sharing their local dataset. Our focus is on hard clustering\nbased on the k-means principle. We formulate federated k-means clustering as an\ninstance of GTVMin. This formulation naturally lends to a federated k-means\nalgorithm where each device updates local cluster centroids by solving a\nmodified local k-means problem. The modification involves adding a penalty term\nto measure the discrepancy between the cluster centroid of neighbouring\ndevices. Our federated k-means algorithm is privacy-friendly as it only\nrequires sharing aggregated information among interconnected devices."}
{"id": "2510.09719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09719", "abs": "https://arxiv.org/abs/2510.09719", "authors": ["Chenxu Wang", "Hao Li", "Yiqun Zhang", "Linyao Chen", "Jianhao Chen", "Ping Jian", "Peng Ye", "Qiaosheng Zhang", "Shuyue Hu"], "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "comment": null, "summary": "Large language models (LLMs) often exhibit complementary strengths. Model\nrouting harnesses these strengths by dynamically directing each query to the\nmost suitable model, given a candidate model pool. However, routing performance\nrelies on accurate model representations, and adding new models typically\nrequires retraining, limiting scalability. To address these challenges, we\npropose a novel routing method using in-context vectors to represent model\ncapabilities. The method proceeds in two stages. First, queries are embedded\nand projected into vectors, with a projector and LLM-based router trained to\nreconstruct the original queries, aligning vector representations with the\nrouter's semantic space. Second, each candidate model is profiled on a query\nset, and the router learns -- based on in-context vectors of query and model\nperformance -- to predict whether each model can correctly answer new queries.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nrouting performance in both in-distribution and out-of-distribution tasks.\nMoreover, our method allows for seamless integration of new models without\nretraining the router. The code is available at\nhttps://github.com/lalalamdbf/ICL-Router."}
{"id": "2510.09723", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05 (Primary), 68T50", "I.2.6; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09723", "abs": "https://arxiv.org/abs/2510.09723", "authors": ["Gregory D. Baker"], "title": "It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning", "comment": "18 pages, 5 figures", "summary": "In this paper, we introduce Narrative Learning, a methodology where models\nare defined entirely in natural language and iteratively refine their\nclassification criteria using explanatory prompts rather than traditional\nnumerical optimisation. We report on experiments to evaluate the accuracy and\npotential of this approach using 3 synthetic and 3 natural datasets and compare\nthem against 7 baseline explainable machine learning models. We demonstrate\nthat on 5 out of 6 of these datasets, Narrative Learning became more accurate\nthan the baseline explainable models in 2025 or earlier because of improvements\nin language models. We also report on trends in the lexicostatistics of these\nmodels' outputs as a proxy for the comprehensibility of the explanations."}
{"id": "2510.09732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09732", "abs": "https://arxiv.org/abs/2510.09732", "authors": ["P. van Oerle", "R. H. Bemthuis", "F. A. Bukhsh"], "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction", "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Large Language Models (LLMs) are increasingly used to generate textual\nexplanations of process models discovered from event logs. Producing\nexplanations from large behavioral abstractions (e.g., directly-follows graphs\nor Petri nets) can be computationally expensive. This paper reports an\nexploratory evaluation of explanation quality under progressive\nbehavioral-input reduction, where models are discovered from progressively\nsmaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple\ninput sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a\nsecond LLM to assess completeness, bottleneck identification, and suggested\nimprovements. On synthetic logs, explanation quality is largely preserved under\nmoderate reduction, indicating a practical cost-quality trade-off. The study is\nexploratory, as the scores are LLM-based (comparative signals rather than\nground truth) and the data are synthetic. The results suggest a path toward\nmore computationally efficient, LLM-assisted process analysis in\nresource-constrained settings."}
{"id": "2510.09734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09734", "abs": "https://arxiv.org/abs/2510.09734", "authors": ["Jindong Tian", "Yifei Ding", "Ronghui Xu", "Hao Miao", "Chenjuan Guo", "Bin Yang"], "title": "ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting", "comment": "16 pages, 6 figures, conference", "summary": "Weather forecasting is a fundamental task in spatiotemporal data analysis,\nwith broad applications across a wide range of domains. Existing data-driven\nforecasting methods typically model atmospheric dynamics over a fixed short\ntime interval (e.g., 6 hours) and rely on naive autoregression-based rollout\nfor long-term forecasting (e.g., 138 hours). However, this paradigm suffers\nfrom two key limitations: (1) it often inadequately models the spatial and\nmulti-scale temporal dependencies inherent in global weather systems, and (2)\nthe rollout strategy struggles to balance error accumulation with the capture\nof fine-grained atmospheric variations. In this study, we propose ARROW, an\nAdaptive-Rollout Multi-scale temporal Routing method for Global Weather\nForecasting. To contend with the first limitation, we construct a\nmulti-interval forecasting model that forecasts weather across different time\nintervals. Within the model, the Shared-Private Mixture-of-Experts captures\nboth shared patterns and specific characteristics of atmospheric dynamics\nacross different time scales, while Ring Positional Encoding accurately encodes\nthe circular latitude structure of the Earth when representing spatial\ninformation. For the second limitation, we develop an adaptive rollout\nscheduler based on reinforcement learning, which selects the most suitable time\ninterval to forecast according to the current weather state. Experimental\nresults demonstrate that ARROW achieves state-of-the-art performance in global\nweather forecasting, establishing a promising paradigm in this field."}
{"id": "2510.09735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09735", "abs": "https://arxiv.org/abs/2510.09735", "authors": ["Qianyou Sun", "Jiexin Zheng", "Bohan Jin", "Lihua Chen", "Yijie Peng"], "title": "InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models", "comment": null, "summary": "Identifying inter-firm relationships such as supply and competitive ties is\ncritical for financial analysis and corporate governance, yet remains\nchallenging due to the scale, sparsity, and contextual dependence of corporate\ndata. Graph-based methods capture structure but miss semantic depth, while\nlarge language models (LLMs) excel at text but remain limited in their ability\nto represent relational dependencies. To address this, we propose\nInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,\nsupported by a proprietary dataset derived from FactSet supply chain records\nand three tailored training tasks: company graph matching, industry\nclassification, and supply relation prediction. This design enables effective\njoint modeling of structure and semantics. Experiments show that\nInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,\non a supply relation identification task, achieving an F-score of 0.8543 vs.\n0.2287 with only a 7B-parameter backbone and lightweight training. The model\nalso generalizes to zero-shot competitor identification, underscoring its\nability to capture nuanced inter-firm dynamics. Our framework thus provides\nanalysts and strategists with a robust tool for mapping and reasoning about\ncomplex corporate networks, enhancing decision-making and risk management in\ndynamic markets."}
{"id": "2510.09739", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09739", "abs": "https://arxiv.org/abs/2510.09739", "authors": ["Ayoub Bouguettaya", "Elizabeth M. Stuart"], "title": "Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings", "comment": "1 figure, 12 pages", "summary": "The lexical hypothesis posits that personality traits are encoded in language\nand is foundational to models like the Big Five. We created a bottom-up\npersonality model from a classic adjective list using machine learning and\ncompared its descriptive utility against the Big Five by analyzing one million\nReddit comments. The Big Five, particularly Agreeableness, Conscientiousness,\nand Neuroticism, provided a far more powerful and interpretable description of\nthese online communities. In contrast, our machine-learning clusters provided\nno meaningful distinctions, failed to recover the Extraversion trait, and\nlacked the psychometric coherence of the Big Five. These results affirm the\nrobustness of the Big Five and suggest personality's semantic structure is\ncontext-dependent. Our findings show that while machine learning can help check\nthe ecological validity of established psychological theories, it may not be\nable to replace them."}
{"id": "2510.09740", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09740", "abs": "https://arxiv.org/abs/2510.09740", "authors": ["Atharv Goel", "Sharat Agarwal", "Saket Anand", "Chetan Arora"], "title": "Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry", "comment": "Accepted to NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "Active Learning (AL) promises to reduce annotation cost by prioritizing\ninformative samples, yet its reliability is undermined when labels are noisy or\nwhen the data distribution shifts. In practice, annotators make mistakes, rare\ncategories are ambiguous, and conventional AL heuristics (uncertainty,\ndiversity) often amplify such errors by repeatedly selecting mislabeled or\nredundant samples. We propose Reliable Active Learning via Neural Collapse\nGeometry (NCAL-R), a framework that leverages the emergent geometric\nregularities of deep networks to counteract unreliable supervision. Our method\nintroduces two complementary signals: (i) a Class-Mean Alignment Perturbation\nscore, which quantifies how candidate samples structurally stabilize or distort\ninter-class geometry, and (ii) a Feature Fluctuation score, which captures\ntemporal instability of representations across training checkpoints. By\ncombining these signals, NCAL-R prioritizes samples that both preserve class\nseparation and highlight ambiguous regions, mitigating the effect of noisy or\nredundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R\nconsistently outperforms standard AL baselines, achieving higher accuracy with\nfewer labels, improved robustness under synthetic label noise, and stronger\ngeneralization to out-of-distribution data. These results suggest that\nincorporating geometric reliability criteria into acquisition decisions can\nmake Active Learning less brittle to annotation errors and distribution shifts,\na key step toward trustworthy deployment in real-world labeling pipelines. Our\ncode is available at https://github.com/Vision-IIITD/NCAL."}
{"id": "2510.09752", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09752", "abs": "https://arxiv.org/abs/2510.09752", "authors": ["Sai Krishna Reddy Mudhiganti", "Juanyan Wang", "Ruo Yang", "Manali Sharma"], "title": "Patentformer: A demonstration of AI-assisted automated patent drafting", "comment": null, "summary": "Patent drafting presents significant challenges due to its reliance on the\nextensive experience and specialized expertise of patent attorneys, who must\npossess both legal acumen and technical understanding of an invention to craft\npatent applications in a formal legal writing style. This paper presents a\ndemonstration of Patentformer, an AI-powered automated patent drafting platform\ndesigned to support patent attorneys by rapidly producing high-quality patent\napplications adhering to legal writing standards."}
{"id": "2510.09762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09762", "abs": "https://arxiv.org/abs/2510.09762", "authors": ["Ruo Yang", "Sai Krishna Reddy Mudhiganti", "Manali Sharma"], "title": "PatentVision: A multimodal method for drafting patent applications", "comment": null, "summary": "Patent drafting is complex due to its need for detailed technical\ndescriptions, legal compliance, and visual elements. Although Large Vision\nLanguage Models (LVLMs) show promise across various tasks, their application in\nautomating patent writing remains underexplored. In this paper, we present\nPatentVision, a multimodal framework that integrates textual and visual inputs\nsuch as patent claims and drawings to generate complete patent specifications.\nBuilt on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned\nvision language models with domain specific training tailored to patents.\nExperiments reveal it surpasses text only methods, producing outputs with\ngreater fidelity and alignment with human written standards. Its incorporation\nof visual data allows it to better represent intricate design features and\nfunctional connections, leading to richer and more precise results. This study\nunderscores the value of multimodal techniques in patent automation, providing\na scalable tool to reduce manual workloads and improve consistency.\nPatentVision not only advances patent drafting but also lays the groundwork for\nbroader use of LVLMs in specialized areas, potentially transforming\nintellectual property management and innovation processes."}
{"id": "2510.09764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09764", "abs": "https://arxiv.org/abs/2510.09764", "authors": ["Wanting Mao", "Maxwell A Xu", "Harish Haresamudram", "Mithun Saha", "Santosh Kumar", "James Matthew Rehg"], "title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "comment": null, "summary": "Modeling multi-modal time-series data is critical for capturing system-level\ndynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,\nand accelerometry provide complementary perspectives on interconnected\nphysiological processes. While recent self-supervised learning (SSL) advances\nhave improved unimodal representation learning, existing multi-modal approaches\noften rely on CLIP-style contrastive objectives that overfit to easily aligned\nfeatures and misclassify valid cross-modal relationships as negatives,\nresulting in fragmented and non-generalizable embeddings. To overcome these\nlimitations, we propose ProtoMM, a novel SSL framework that introduces a shared\nprototype dictionary to anchor heterogeneous modalities in a common embedding\nspace. By clustering representations around shared prototypes rather than\nexplicit negative sampling, our method captures complementary information\nacross modalities and provides a coherent \"common language\" for physiological\nsignals. In this work, we focus on developing a Pulse Motion foundation model\nwith ProtoMM and demonstrate that our approach outperforms contrastive-only and\nprior multimodal SSL methods, achieving state-of-the-art performance while\noffering improved interpretability of learned features."}
{"id": "2510.09767", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09767", "abs": "https://arxiv.org/abs/2510.09767", "authors": ["Yifan Lu", "Ziyun Zou", "Belal Alsinglawi", "Islam Al-Qudah", "Izzat Alsmadi", "Feilong Tang", "Pengfei Jiao", "Shoaib Jameel"], "title": "HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network", "comment": null, "summary": "Graph Transformers have recently achieved remarkable progress in graph\nrepresentation learning by capturing long-range dependencies through\nself-attention. However, their quadratic computational complexity and inability\nto effectively model heterogeneous semantics severely limit their scalability\nand generalization on real-world heterogeneous graphs. To address these issues,\nwe propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for\nefficient and expressive heterogeneous graph representation learning. HeSRN\nintroduces a slot-aware structure encoder that explicitly disentangles\nnode-type semantics by projecting heterogeneous features into independent slots\nand aligning their distributions through slot normalization and retention-based\nfusion, effectively mitigating the semantic entanglement caused by forced\nfeature-space unification in previous Transformer-based models. Furthermore, we\nreplace the self-attention mechanism with a retention-based encoder, which\nmodels structural and contextual dependencies in linear time complexity while\nmaintaining strong expressive power. A heterogeneous retentive encoder is\nfurther employed to jointly capture both local structural signals and global\nheterogeneous semantics through multi-scale retention layers. Extensive\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHeSRN consistently outperforms state-of-the-art heterogeneous graph neural\nnetworks and Graph Transformer baselines on node classification tasks,\nachieving superior accuracy with significantly lower computational complexity."}
{"id": "2510.09768", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09768", "abs": "https://arxiv.org/abs/2510.09768", "authors": ["Khang Ngo", "Siamak Ravanbakhsh"], "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "comment": "22 pages, 10 figures", "summary": "We present an empirical study in the geometric task of learning interatomic\npotentials, which shows equivariance matters even more at larger scales; we\nshow a clear power-law scaling behaviour with respect to data, parameters and\ncompute with ``architecture-dependent exponents''. In particular, we observe\nthat equivariant architectures, which leverage task symmetry, scale better than\nnon-equivariant models. Moreover, among equivariant architectures, higher-order\nrepresentations translate to better scaling exponents. Our analysis also\nsuggests that for compute-optimal training, the data and model sizes should\nscale in tandem regardless of the architecture. At a high level, these results\nsuggest that, contrary to common belief, we should not leave it to the model to\ndiscover fundamental inductive biases such as symmetry, especially as we scale,\nbecause they change the inherent difficulty of the task and its scaling laws."}
{"id": "2510.09775", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09775", "abs": "https://arxiv.org/abs/2510.09775", "authors": ["Alex Hiles", "Bashar I. Ahmad"], "title": "A Generic Machine Learning Framework for Radio Frequency Fingerprinting", "comment": null, "summary": "Fingerprinting Radio Frequency (RF) emitters typically involves finding\nunique emitter characteristics that are featured in their transmitted signals.\nThese fingerprints are nuanced but sufficiently detailed, motivating the\npursuit of methods that can successfully extract them. The most granular\ndownstream task is known as Specific Emitter Identification (SEI), which\nrequires a well informed RF fingerprinting (RFF) approach for it to be\nsuccessful. RFF and SEI have a long history, with numerous application areas in\ndefence and civilian contexts such as signal intelligence, electronic\nsurveillance, physical-layer authentication of wireless communication devices,\nto name a few. RFF methods also support many other downstream tasks such as\nEmitter Data Association (EDA) and RF Emitter Clustering (RFEC) and are\napplicable to a range of transmission types. In recent years, data-driven\napproaches have become popular in the RFF domain due to their ability to\nautomatically learn intricate fingerprints from raw data. These methods\ngenerally deliver superior performance when compared to traditional techniques.\nThe more traditional approaches are often labour-intensive, inflexible and only\napplicable to a particular emitter type or transmission scheme. Therefore, we\nconsider data-driven Machine Learning (ML)-enabled RFF. In particular, we\npropose a generic framework for ML-enabled RFF which is inclusive of several\npopular downstream tasks such as SEI, EDA and RFEC. Each task is formulated as\na RF fingerprint-dependent task. A variety of use cases using real RF datasets\nare presented here to demonstrate the framework for a range of tasks and\napplication areas, such as spaceborne surveillance, signal intelligence and\ncountering drones."}
{"id": "2510.09776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09776", "abs": "https://arxiv.org/abs/2510.09776", "authors": ["Yufa Zhou", "Yixiao Wang", "Surbhi Goel", "Anru R. Zhang"], "title": "Why Do Transformers Fail to Forecast Time Series In-Context?", "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series", "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR($p$) data, we establish that: (1) Linear\nSelf-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny."}
{"id": "2510.09780", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09780", "abs": "https://arxiv.org/abs/2510.09780", "authors": ["ChengAo Shen", "Ziming Zhao", "Hanghang Tong", "Dongjin Song", "Dongsheng Luo", "Qingsong Wen", "Jingchao Ni"], "title": "SVTime: Small Time Series Forecasting Models Informed by \"Physics\" of Large Vision Model Forecasters", "comment": null, "summary": "Time series AI is crucial for analyzing dynamic web content, driving a surge\nof pre-trained large models known for their strong knowledge encoding and\ntransfer capabilities across diverse tasks. However, given their\nenergy-intensive training, inference, and hardware demands, using large models\nas a one-fits-all solution raises serious concerns about carbon footprint and\nsustainability. For a specific task, a compact yet specialized, high-performing\nmodel may be more practical and affordable, especially for resource-constrained\nusers such as small businesses. This motivates the question: Can we build\ncost-effective lightweight models with large-model-like performance on core\ntasks such as forecasting? This paper addresses this question by introducing\nSVTime, a novel Small model inspired by large Vision model (LVM) forecasters\nfor long-term Time series forecasting (LTSF). Recently, LVMs have been shown as\npowerful tools for LTSF. We identify a set of key inductive biases of LVM\nforecasters -- analogous to the \"physics\" governing their behaviors in LTSF --\nand design small models that encode these biases through meticulously crafted\nlinear layers and constraint functions. Across 21 baselines spanning\nlightweight, complex, and pre-trained large models on 8 benchmark datasets,\nSVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large\nmodels with 10^3 fewer parameters than LVMs, while enabling efficient training\nand inference in low-resource settings."}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems."}
{"id": "2510.09783", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09783", "abs": "https://arxiv.org/abs/2510.09783", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Thin Nguyen", "Taylor Braund", "Alexis Whitton", "Svetha Venkatesh"], "title": "Large Language Models for Imbalanced Classification: Diversity makes the difference", "comment": null, "summary": "Oversampling is one of the most widely used approaches for addressing\nimbalanced classification. The core idea is to generate additional minority\nsamples to rebalance the dataset. Most existing methods, such as SMOTE, require\nconverting categorical variables into numerical vectors, which often leads to\ninformation loss. Recently, large language model (LLM)-based methods have been\nintroduced to overcome this limitation. However, current LLM-based approaches\ntypically generate minority samples with limited diversity, reducing robustness\nand generalizability in downstream classification tasks. To address this gap,\nwe propose a novel LLM-based oversampling method designed to enhance diversity.\nFirst, we introduce a sampling strategy that conditions synthetic sample\ngeneration on both minority labels and features. Second, we develop a new\npermutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the\nLLM not only on minority samples but also on interpolated samples to further\nenrich variability. Extensive experiments on 10 tabular datasets demonstrate\nthat our method significantly outperforms eight SOTA baselines. The generated\nsynthetic samples are both realistic and diverse. Moreover, we provide\ntheoretical analysis through an entropy-based perspective, proving that our\nmethod encourages diversity in the generated samples."}
{"id": "2510.09784", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09784", "abs": "https://arxiv.org/abs/2510.09784", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "comment": null, "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional\nspaces. In molecular science, where data collection is expensive and important\nevents are rare, compression to lower-dimensional manifolds is especially\nimportant for various downstream tasks, including generation. We combine a\ntime-lagged information bottleneck designed to characterize molecular important\nrepresentations and a diffusion model in one joint training objective. The\nresulting protocol, which we term Diffusive State Predictive Information\nBottleneck (D-SPIB), enables the balancing of representation learning and\ngeneration aims in one flexible architecture. Additionally, the model is\ncapable of combining temperature information from different molecular\nsimulation trajectories to learn a coherent and useful internal representation\nof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase\nits potential for exploring physical conditions outside the training set."}
{"id": "2510.09792", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.09792", "abs": "https://arxiv.org/abs/2510.09792", "authors": ["Vahidreza Jahanmard", "Ali Ramezani-Kebrya", "Robinson Hordoir"], "title": "Principled Operator Learning in Ocean Dynamics: The Role of Temporal Structure", "comment": "Accepted at NeurIPS ML4PS 2025", "summary": "Neural operators are becoming the default tools to learn solutions to\ngoverning partial differential equations (PDEs) in weather and ocean\nforecasting applications. Despite early promising achievements, significant\nchallenges remain, including long-term prediction stability and adherence to\nphysical laws, particularly for high-frequency processes. In this paper, we\ntake a step toward addressing these challenges in high-resolution ocean\nprediction by incorporating temporal Fourier modes, demonstrating how this\nmodification enhances physical fidelity. This study compares the standard\nFourier Neural Operator (FNO) with its variant, FNOtD, which has been modified\nto internalize the dispersion relation while learning the solution operator for\nocean PDEs. The results demonstrate that entangling space and time in the\ntraining of integral kernels enables the model to capture multiscale wave\npropagation and effectively learn ocean dynamics. FNOtD substantially improves\nlong-term prediction stability and consistency with underlying physical\ndynamics in challenging high-frequency settings compared to the standard FNO.\nIt also provides competitive predictive skill relative to a state-of-the-art\nnumerical ocean model, while requiring significantly lower computational cost."}
{"id": "2510.09794", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09794", "abs": "https://arxiv.org/abs/2510.09794", "authors": ["Lianghuan Huang", "Yingshan Chang"], "title": "Causality $\\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs", "comment": null, "summary": "Mechanistic interpretability seeks to uncover how internal components of\nneural networks give rise to predictions. A persistent challenge, however, is\ndisentangling two often conflated notions: decodability--the recoverability of\ninformation from hidden states--and causality--the extent to which those states\nfunctionally influence outputs. In this work, we investigate their relationship\nin vision transformers (ViTs) fine-tuned for object counting. Using activation\npatching, we test the causal role of spatial and CLS tokens by transplanting\nactivations across clean-corrupted image pairs. In parallel, we train linear\nprobes to assess the decodability of count information at different depths. Our\nresults reveal systematic mismatches: middle-layer object tokens exert strong\ncausal influence despite being weakly decodable, whereas final-layer object\ntokens support accurate decoding yet are functionally inert. Similarly, the CLS\ntoken becomes decodable in mid-layers but only acquires causal power in the\nfinal layers. These findings highlight that decodability and causality reflect\ncomplementary dimensions of representation--what information is present versus\nwhat is used--and that their divergence can expose hidden computational\ncircuits."}
{"id": "2510.09796", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML", "47A52, 47J30, 65J22, 65K10, 68T01, 68T07, 68W15, 94A08"], "pdf": "https://arxiv.org/pdf/2510.09796", "abs": "https://arxiv.org/abs/2510.09796", "authors": ["Xiaoyu Wang", "Alexandra Valavanis", "Azhir Mahmood", "Andreas Mang", "Martin Benning", "Audrey Repetti"], "title": "A Unified Framework for Lifted Training and Inversion Approaches", "comment": null, "summary": "The training of deep neural networks predominantly relies on a combination of\ngradient-based optimisation and back-propagation for the computation of the\ngradient. While incredibly successful, this approach faces challenges such as\nvanishing or exploding gradients, difficulties with non-smooth activations, and\nan inherently sequential structure that limits parallelisation. Lifted training\nmethods offer an alternative by reformulating the nested optimisation problem\ninto a higher-dimensional, constrained optimisation problem where the\nconstraints are no longer enforced directly but penalised with penalty terms.\nThis chapter introduces a unified framework that encapsulates various lifted\ntraining strategies, including the Method of Auxiliary Coordinates, Fenchel\nLifted Networks, and Lifted Bregman Training, and demonstrates how diverse\narchitectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and\nProximal Neural Networks fit within this structure. By leveraging tools from\nconvex optimisation, particularly Bregman distances, the framework facilitates\ndistributed optimisation, accommodates non-differentiable proximal activations,\nand can improve the conditioning of the training landscape. We discuss the\nimplementation of these methods using block-coordinate descent strategies,\nincluding deterministic implementations enhanced by accelerated and adaptive\noptimisation techniques, as well as implicit stochastic gradient methods.\nFurthermore, we explore the application of this framework to inverse problems,\ndetailing methodologies for both the training of specialised networks (e.g.,\nunrolled architectures) and the stable inversion of pre-trained networks.\nNumerical results on standard imaging tasks validate the effectiveness and\nstability of the lifted Bregman approach compared to conventional training,\nparticularly for architectures employing proximal activations."}
{"id": "2510.09805", "categories": ["cs.LG", "cs.AI", "35Q30, 76D05, 65M70, 68T07, 68T27, 03D45", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.09805", "abs": "https://arxiv.org/abs/2510.09805", "authors": ["Jeffrey Camlin"], "title": "Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems", "comment": "6 pages, 1 figure, 1 table, 1 algorithm", "summary": "We present a latent-space formulation of adaptive temporal reparametrization\nfor continuous-time dynamical systems. The method, called *temporal lifting*,\nintroduces a smooth monotone mapping $t \\mapsto \\tau(t)$ that regularizes\nnear-singular behavior of the underlying flow while preserving its conservation\nlaws. In the lifted coordinate, trajectories such as those of the\nincompressible Navier-Stokes equations on the torus $\\mathbb{T}^3$ become\nglobally smooth. From the standpoint of machine-learning dynamics, temporal\nlifting acts as a continuous-time normalization or time-warping operator that\ncan stabilize physics-informed neural networks and other latent-flow\narchitectures used in AI systems. The framework links analytic regularity\ntheory with representation-learning methods for stiff or turbulent processes."}
{"id": "2510.09825", "categories": ["cs.LG", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09825", "abs": "https://arxiv.org/abs/2510.09825", "authors": ["Mohsen Joneidi"], "title": "Decomposer Networks: Deep Component Analysis and Synthesis", "comment": "13 Pages, 4 figures", "summary": "We propose the Decomposer Networks (DecompNet), a semantic autoencoder that\nfactorizes an input into multiple interpretable components. Unlike classical\nautoencoders that compress an input into a single latent representation, the\nDecomposer Network maintains N parallel branches, each assigned a residual\ninput defined as the original signal minus the reconstructions of all other\nbranches. By unrolling a Gauss--Seidel style block-coordinate descent into a\ndifferentiable network, DecompNet enforce explicit competition among\ncomponents, yielding parsimonious, semantically meaningful representations. We\nsituate our model relative to linear decomposition methods (PCA, NMF), deep\nunrolled optimization, and object-centric architectures (MONet, IODINE, Slot\nAttention), and highlight its novelty as the first semantic autoencoder to\nimplement an all-but-one residual update rule."}
{"id": "2510.09827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09827", "abs": "https://arxiv.org/abs/2510.09827", "authors": ["Michael Crawshaw", "Chirag Modi", "Mingrui Liu", "Robert M. Gower"], "title": "An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants", "comment": null, "summary": "To define a steepest descent method over a neural network, we need to choose\na norm for each layer, a way to aggregate these norms across layers, and\nwhether to use normalization. We systematically explore different alternatives\nfor aggregating norms across layers, both formalizing existing combinations of\nAdam and the recently proposed Muon as a type of non-Euclidean gradient\ndescent, and deriving new variants of the Muon optimizer. Through a\ncomprehensive experimental evaluation of the optimizers within our framework,\nwe find that Muon is sensitive to the choice of learning rate, whereas a new\nvariant we call MuonMax is significantly more robust. We then show how to\ncombine any non-Euclidean gradient method with model based momentum (known as\nMomo). The new Momo variants of Muon are significantly more robust to\nhyperparameter tuning, and often achieve a better validation score. Thus for\nnew tasks, where the optimal hyperparameters are not known, we advocate for\nusing Momo in combination with MuonMax to save on costly hyperparameter tuning."}
{"id": "2510.09845", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09845", "abs": "https://arxiv.org/abs/2510.09845", "authors": ["Nicholas LaHaye", "Thilanka Munashinge", "Hugo Lee", "Xiaohua Pan", "Gonzalo Gonzalez Abad", "Hazem Mahmoud", "Jennifer Wei"], "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data", "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611", "summary": "This work demonstrates the possibilities for improving wildfire and air\nquality management in the western United States by leveraging the unprecedented\nhourly data from NASA's TEMPO satellite mission and advances in self-supervised\ndeep learning. Here we demonstrate the efficacy of deep learning for mapping\nthe near real-time hourly spread of wildfire fronts and smoke plumes using an\ninnovative self-supervised deep learning-system: successfully distinguishing\nsmoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across\nthe smoke and fire masks generated from different sensing modalities as well as\nsignificant improvement over operational products for the same cases."}
{"id": "2510.09846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09846", "abs": "https://arxiv.org/abs/2510.09846", "authors": ["Zhenjiang Fan", "Zengyi Qin", "Yuanning Zheng", "Bo Xiong", "Summer Han"], "title": "CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes", "comment": null, "summary": "Causal discovery from observational data is fundamental to scientific fields\nlike biology, where controlled experiments are often impractical. However,\nexisting methods, including constraint-based (e.g., PC, causalMGM) and\nscore-based approaches (e.g., NOTEARS), face significant limitations. These\ninclude an inability to resolve causal direction, restrictions to linear\nassociations, sensitivity to violations of the faithfulness assumption, and\ninefficiency in searching vast hypothesis spaces. While large language models\n(LLMs) offer powerful reasoning capabilities, their application is hindered by\na fundamental discrepancy: they are designed for text, while most causal data\nis tabular. To address these challenges, we introduce CALM, a novel causal\nanalysis language model specifically designed for tabular data in complex\nsystems. CALM leverages a Mamba-based architecture to classify causal patterns\nfrom pairwise variable relationships. It integrates a comprehensive suite of\nevidence, including local causal scores, conditional independence tests, and\nrelational attributes, to capture a wide spectrum of linear, nonlinear, and\nconditional causal mechanisms. Trained on a diverse corpus of synthetic data\n(from linear, mixed, and nonlinear models) and 10 real-world biological\ndatasets with rigorously validated causal relationships, our model ensures\nrobustness and generalizability. Empirical evaluation demonstrates that CALM\nsignificantly outperforms existing methods in both simulation studies,\nachieving over 91% accuracy, and in a real-world application identifying causal\nfactors in Hepatitis C virus progression. This work represents a significant\nstep towards accurate and generalizable causal discovery by successfully\nadapting the pattern recognition capabilities of language models to the\nintricacies of tabular data."}
{"id": "2510.09852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09852", "abs": "https://arxiv.org/abs/2510.09852", "authors": ["Shivam Patel", "Neharika Jali", "Ankur Mallick", "Gauri Joshi"], "title": "ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers", "comment": null, "summary": "Large language model (LLM) query routers are critical to modern AI platforms\nas they seek to improve efficiency by assigning inference queries to accurate,\nyet low-cost models. Parametric routers typically use trained neural networks\nfor LLM selection but suffer from retraining and maintenance overheads.\nNonparametric routers are training-free, instead estimating LLM accuracy and\ncost via similarity between encodings of the input query and training set\nqueries. However, like their parametric counterparts, nonparametric routers\nstruggle to generalize to outlier queries, an issue exacerbated by limited\ndiversity in training sets which are costly to expand and difficult to keep\ncurrent with ever-evolving use cases. We propose ProxRouter, which applies an\nexponentially tilted aggregation mechanism to balance bias and variance in\nnonparametric routers, improving their robustness to outliers. Experiments show\nProxRouter enhances outlier routing while preserving inlier performance with\nminimal overhead."}
{"id": "2510.09872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09872", "abs": "https://arxiv.org/abs/2510.09872", "authors": ["Sanjari Srivastava", "Gang Li", "Cheng Chang", "Rishu Garg", "Manpreet Kaur", "Charlene Y. Lee", "Yuezhang Li", "Yining Mao", "Ignacio Cases", "Yanan Xie", "Peng Qi"], "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions", "comment": null, "summary": "Training web agents to navigate complex, real-world websites requires them to\nmaster $\\textit{subtasks}$ - short-horizon interactions on multiple UI\ncomponents (e.g., choosing the correct date in a date picker, or scrolling in a\ncontainer to extract information). We introduce WARC-Bench (Web Archive\nBenchmark), a novel web navigation benchmark featuring 438 tasks designed to\nevaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed\ninteractions with dynamic and realistic webpages using Web ARChive files. We\nshow that WARC-Bench is challenging for leading computer-use models, with the\nhighest observed success rate being 64.8%. To improve open source models on\nsubtask, we explore two common training techniques: supervised fine-tuning\n(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments\nshow that SFT models obtain a 48.8% success rate on the benchmark. Training\nwith RLVR over SFT checkpoints, even in data-scarce settings, improves the\nscore to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis\nconcludes that mastering these subtasks is essential for robust web planning\nand navigation, and is a capability not extensively evaluated by existing\nbenchmarks."}
{"id": "2510.09877", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09877", "abs": "https://arxiv.org/abs/2510.09877", "authors": ["Kangping Hu", "Stephen Mussmann"], "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling", "comment": null, "summary": "Over the past couple of decades, many active learning acquisition functions\nhave been proposed, leaving practitioners with an unclear choice of which to\nuse. Bayesian Decision Theory (BDT) offers a universal principle to guide\ndecision-making. In this work, we derive BDT for (Bayesian) active learning in\nthe myopic framework, where we imagine we only have one more point to label.\nThis derivation leads to effective algorithms such as Expected Error Reduction\n(EER), Expected Predictive Information Gain (EPIG), and other algorithms that\nappear in the literature. Furthermore, we show that BAIT (active learning based\non V-optimal experimental design) can be derived from BDT and asymptotic\napproximations. A key challenge of such methods is the difficult scaling to\nlarge batch sizes, leading to either computational challenges (BatchBALD) or\ndramatic performance drops (top-$B$ selection). Here, using a particular\nformulation of the decision process, we derive Partial Batch Label Sampling\n(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets\nthat ParBaLS EPIG gives superior performance for a fixed budget and Bayesian\nLogistic Regression on Neural Embeddings. Our code is available at\nhttps://github.com/ADDAPT-ML/ParBaLS."}
{"id": "2510.09884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09884", "abs": "https://arxiv.org/abs/2510.09884", "authors": ["Soheila Farokhi", "Xiaojun Qi", "Hamid Karimi"], "title": "TAWRMAC: A Novel Dynamic Graph Representation Learning Method", "comment": null, "summary": "Dynamic graph representation learning has become essential for analyzing\nevolving networks in domains such as social network analysis, recommendation\nsystems, and traffic analysis. However, existing continuous-time methods face\nthree key challenges: (1) some methods depend solely on node-specific memory\nwithout effectively incorporating information from neighboring nodes, resulting\nin embedding staleness; (2) most fail to explicitly capture correlations\nbetween node neighborhoods, limiting contextual awareness; and (3) many fail to\nfully capture the structural dynamics of evolving graphs, especially in absence\nof rich link attributes. To address these limitations, we introduce TAWRMAC-a\nnovel framework that integrates Temporal Anonymous Walks with Restart, Memory\nAugmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embedding\nstability through a memory-augmented GNN with fixedtime encoding and improves\ncontextual representation by explicitly capturing neighbor correlations.\nAdditionally, its Temporal Anonymous Walks with Restart mechanism distinguishes\nbetween nodes exhibiting repetitive interactions and those forming new\nconnections beyond their immediate neighborhood. This approach captures\nstructural dynamics better and supports strong inductive learning. Extensive\nexperiments on multiple benchmark datasets demonstrate that TAWRMAC\nconsistently outperforms state-of-the-art methods in dynamic link prediction\nand node classification under both transductive and inductive settings across\nthree different negative sampling strategies. By providing stable,\ngeneralizable, and context-aware embeddings, TAWRMAC advances the state of the\nart in continuous-time dynamic graph learning. The code is available at\nhttps://anonymous.4open.science/r/tawrmac-A253 ."}
{"id": "2510.09888", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09888", "abs": "https://arxiv.org/abs/2510.09888", "authors": ["Yunlong Feng", "Qiang Wu"], "title": "Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise", "comment": null, "summary": "We investigate robust nonparametric regression in the presence of\nheavy-tailed noise, where the hypothesis class may contain unbounded functions\nand robustness is ensured via a robust loss function $\\ell_\\sigma$. Using Huber\nregression as a close-up example within Tikhonov-regularized risk minimization\nin reproducing kernel Hilbert spaces (RKHS), we address two central challenges:\n(i) the breakdown of standard concentration tools under weak moment\nassumptions, and (ii) the analytical difficulties introduced by unbounded\nhypothesis spaces. Our first message is conceptual: conventional\ngeneralization-error bounds for robust losses do not faithfully capture\nout-of-sample performance. We argue that learnability should instead be\nquantified through prediction error, namely the $L_2$-distance to the truth\n$f^\\star$, which is $\\sigma$-independent and directly reflects the target of\nrobust estimation. To make this workable under unboundedness, we introduce a\n\\emph{probabilistic effective hypothesis space} that confines the estimator\nwith high probability and enables a meaningful bias--variance decomposition\nunder weak $(1+\\epsilon)$-moment conditions. Technically, we establish new\ncomparison theorems linking the excess robust risk to the $L_2$ prediction\nerror up to a residual of order $\\mathcal{O}(\\sigma^{-2\\epsilon})$, clarifying\nthe robustness--bias trade-off induced by the scale parameter $\\sigma$.\nBuilding on this, we derive explicit finite-sample error bounds and convergence\nrates for Huber regression in RKHS that hold without uniform boundedness and\nunder heavy-tailed noise. Our study delivers principled tuning rules, extends\nbeyond Huber to other robust losses, and highlights prediction error, not\nexcess generalization risk, as the fundamental lens for analyzing robust\nlearning."}
{"id": "2510.09891", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09891", "abs": "https://arxiv.org/abs/2510.09891", "authors": ["Parsa Gooya", "Reinel Sospedra-Alfonso"], "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration", "comment": null, "summary": "Seasonal forecast of Arctic sea ice concentration is key to mitigate the\nnegative impact and assess potential opportunities posed by the rapid decline\nof sea ice coverage. Seasonal prediction systems based on climate models often\nshow systematic biases and complex spatio-temporal errors that grow with the\nforecasts. Consequently, operational predictions are routinely bias corrected\nand calibrated using retrospective forecasts. For predictions of Arctic sea ice\nconcentration, error corrections are mainly based on one-to-one post-processing\nmethods including climatological mean or linear regression correction and, more\nrecently, machine learning. Such deterministic adjustments are confined at best\nto the limited number of costly-to-run ensemble members of the raw forecast.\nHowever, decision-making requires proper quantification of uncertainty and\nlikelihood of events, particularly of extremes. We introduce a probabilistic\nerror correction framework based on a conditional Variational Autoencoder model\nto map the conditional distribution of observations given the biased model\nprediction. This method naturally allows for generating large ensembles of\nadjusted forecasts. We evaluate our model using deterministic and probabilistic\nmetrics and show that the adjusted forecasts are better calibrated, closer to\nthe observational distribution, and have smaller errors than climatological\nmean adjusted forecasts."}
{"id": "2510.09895", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09895", "abs": "https://arxiv.org/abs/2510.09895", "authors": ["Yubo Li", "Rema Padman"], "title": "Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings", "comment": null, "summary": "Modeling clinical time-series data is hampered by the challenge of capturing\nlatent, time-varying dependencies among features. State-of-the-art approaches\noften rely on black-box mechanisms or simple aggregation, failing to explicitly\nmodel how the influence of one clinical variable propagates through others over\ntime. We propose $\\textbf{Chain-of-Influence (CoI)}$, an interpretable deep\nlearning framework that constructs an explicit, time-unfolded graph of feature\ninteractions. CoI leverages a multi-level attention architecture: first, a\ntemporal attention layer identifies critical time points in a patient's record;\nsecond, a cross-feature attention layer models the directed influence from\nfeatures at these time points to subsequent features. This design enables the\ntracing of influence pathways, providing a granular audit trail that shows how\nany feature at any time contributes to the final prediction, both directly and\nthrough its influence on other variables. We evaluate CoI on mortality and\ndisease progression tasks using the MIMIC-IV dataset and a private chronic\nkidney disease cohort. Our framework significantly outperforms existing methods\nin predictive accuracy. More importantly, through case studies, we show that\nCoI can uncover clinically meaningful, patient-specific patterns of disease\nprogression that are opaque to other models, offering unprecedented\ntransparency into the temporal and cross-feature dependencies that inform\nclinical decision-making."}
{"id": "2510.09898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09898", "abs": "https://arxiv.org/abs/2510.09898", "authors": ["Hung Phan", "Son Le Vu", "Ali Jannesari"], "title": "Learning Bug Context for PyTorch-to-JAX Translation with LLMs", "comment": null, "summary": "Despite recent progress of large language models (LLMs) on code translation\namong mainstream languages, translating PyTorch to JAX remains nontrivial. The\ntwo libraries, though both embedded in Python, differ in core design, execution\nsemantics, and ecosystem maturity; JAX is newer and comparatively\nunderrepresented in public code, and parallel PyTorch--JAX corpora are limited.\nWeaknesses in existing evaluation further complicate cross-framework\nbenchmarking. We present T2J, a prompt-augmentation framework that strengthens\nLLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch\nsources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a\nGitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini\nto produce initial JAX drafts; (ii) engages two professional developers to\niteratively repair those drafts until functional equivalence, yielding a\ncurated fixed-bug dataset of common errors and patches; and (iii) constructs\naugmented prompts that inject structured guidance from these fixes to steer\nlightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored\nto PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based\nestimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge).\nEmpirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50%\non T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100%\non T2J Comparison Score; moreover, the generated code runs up to 2.5x faster\nthan the baseline."}
{"id": "2510.09904", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.09904", "abs": "https://arxiv.org/abs/2510.09904", "authors": ["Kelvin Kan", "Xingjian Li", "Benjamin J. Zhang", "Tuhin Sahai", "Stanley Osher", "Krishna Kumar", "Markos A. Katsoulakis"], "title": "Stability of Transformers under Layer Normalization", "comment": null, "summary": "Despite their widespread use, training deep Transformers can be unstable.\nLayer normalization, a standard component, improves training stability, but its\nplacement has often been ad-hoc. In this paper, we conduct a principled study\non the forward (hidden states) and backward (gradient) stability of\nTransformers under different layer normalization placements. Our theory\nprovides key insights into the training dynamics: whether training drives\nTransformers toward regular solutions or pathological behaviors. For forward\nstability, we derive explicit bounds on the growth of hidden states in trained\nTransformers. For backward stability, we analyze how layer normalization\naffects the backpropagation of gradients, thereby explaining the training\ndynamics of each layer normalization placement. Our analysis also guides the\nscaling of residual steps in Transformer blocks, where appropriate choices can\nfurther improve stability and performance. Our numerical results corroborate\nour theoretical findings. Beyond these results, our framework provides a\nprincipled way to sanity-check the stability of Transformers under new\narchitectural modifications, offering guidance for future designs."}
{"id": "2510.09914", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09914", "abs": "https://arxiv.org/abs/2510.09914", "authors": ["Aditya Malusare", "Vineet Punyamoorty", "Vaneet Aggarwal"], "title": "Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery", "comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Artificial Intelligence, October 2025", "summary": "Recent breakthroughs in generative modeling have demonstrated remarkable\ncapabilities in molecular generation, yet the integration of comprehensive\nbiomedical knowledge into these models has remained an untapped frontier. In\nthis study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model),\na novel framework that leverages knowledge graphs to augment diffusion-based\ngenerative models for drug discovery. By embedding structured information from\nlarge-scale knowledge graphs, K-DREAM directs molecular generation toward\ncandidates with higher biological relevance and therapeutic suitability. This\nintegration ensures that the generated molecules are aligned with specific\ntherapeutic targets, moving beyond traditional heuristic-driven approaches. In\ntargeted drug design tasks, K-DREAM generates drug candidates with improved\nbinding affinities and predicted efficacy, surpassing current state-of-the-art\ngenerative models. It also demonstrates flexibility by producing molecules\ndesigned for multiple targets, enabling applications to complex disease\nmechanisms. These results highlight the utility of knowledge-enhanced\ngenerative models in rational drug design and their relevance to practical\ntherapeutic development."}
{"id": "2510.09916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09916", "abs": "https://arxiv.org/abs/2510.09916", "authors": ["Manuel Segura", "Pere Vergés", "Richard Ky", "Ramesh Arangott", "Angela Kristine Garcia", "Thang Dihn Trong", "Makoto Hyodo", "Alexandru Nicolau", "Tony Givargis", "Sergio Gago-Masague"], "title": "Advancing Intoxication Detection: A Smartwatch-Based Approach", "comment": null, "summary": "Excess alcohol consumption leads to serious health risks and severe\nconsequences for both individuals and their communities. To advocate for\nhealthier drinking habits, we introduce a groundbreaking mobile smartwatch\napplication approach to just-in-time interventions for intoxication warnings.\nIn this work, we have created a dataset gathering TAC, accelerometer,\ngyroscope, and heart rate data from the participants during a period of three\nweeks. This is the first study to combine accelerometer, gyroscope, and heart\nrate smartwatch data collected over an extended monitoring period to classify\nintoxication levels. Previous research had used limited smartphone motion data\nand conventional machine learning (ML) algorithms to classify heavy drinking\nepisodes; in this work, we use smartwatch data and perform a thorough\nevaluation of different state-of-the-art classifiers such as the Transformer,\nBidirectional Long Short-Term Memory (bi-LSTM), Gated Recurrent Unit (GRU),\nOne-Dimensional Convolutional Neural Networks (1D-CNN), and Hyperdimensional\nComputing (HDC). We have compared performance metrics for the algorithms and\nassessed their efficiency on resource-constrained environments like mobile\nhardware. The HDC model achieved the best balance between accuracy and\nefficiency, demonstrating its practicality for smartwatch-based applications."}
{"id": "2510.09923", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09923", "abs": "https://arxiv.org/abs/2510.09923", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-Côté", "Trevor Campbell"], "title": "AutoGD: Automatic Learning Rate Selection for Gradient Descent", "comment": null, "summary": "The performance of gradient-based optimization methods, such as standard\ngradient descent (GD), greatly depends on the choice of learning rate. However,\nit can require a non-trivial amount of user tuning effort to select an\nappropriate learning rate schedule. When such methods appear as inner loops of\nother algorithms, expecting the user to tune the learning rates may be\nimpractical. To address this, we introduce AutoGD: a gradient descent method\nthat automatically determines whether to increase or decrease the learning rate\nat a given iteration. We establish the convergence of AutoGD, and show that we\ncan recover the optimal rate of GD (up to a constant) for a broad class of\nfunctions without knowledge of smoothness constants. Experiments on a variety\nof traditional problems and variational inference optimization tasks\ndemonstrate strong performance of the method, along with its extensions to\nAutoBFGS and AutoLBFGS."}
{"id": "2510.09926", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09926", "abs": "https://arxiv.org/abs/2510.09926", "authors": ["Naman Agrawal"], "title": "Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications", "comment": null, "summary": "This study explores the design and application of Complex-Valued\nConvolutional Neural Networks (CVCNNs) in audio signal processing, with a focus\non preserving and utilizing phase information often neglected in real-valued\nnetworks. We begin by presenting the foundational theoretical concepts of\nCVCNNs, including complex convolutions, pooling layers, Wirtinger-based\ndifferentiation, and various complex-valued activation functions. These are\ncomplemented by critical adaptations of training techniques, including complex\nbatch normalization and weight initialization schemes, to ensure stability in\ntraining dynamics. Empirical evaluations are conducted across three stages.\nFirst, CVCNNs are benchmarked on standard image datasets, where they\ndemonstrate competitive performance with real-valued CNNs, even under synthetic\ncomplex perturbations. Although our focus is audio signal processing, we first\nevaluate CVCNNs on image datasets to establish baseline performance and\nvalidate training stability before applying them to audio tasks. In the second\nexperiment, we focus on audio classification using Mel-Frequency Cepstral\nCoefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperform\nreal CNNs, while preserving phase in input workflows highlights challenges in\nexploiting phase without architectural modifications. Finally, a third\nexperiment introduces GNNs to model phase information via edge weighting, where\nthe inclusion of phase yields measurable gains in both binary and multi-class\ngenre classification. These results underscore the expressive capacity of\ncomplex-valued architectures and confirm phase as a meaningful and exploitable\nfeature in audio processing applications. While current methods show promise,\nespecially with activations like cardioid, future advances in phase-aware\ndesign will be essential to leverage the potential of complex representations\nin neural networks."}
{"id": "2510.09930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09930", "abs": "https://arxiv.org/abs/2510.09930", "authors": ["Ching Chang", "Ming-Chih Lo", "Chiao-Tung Chan", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation", "comment": "This paper is currently under review. The code will be made available\n  upon acceptance", "summary": "Web platforms, mobile applications, and connected sensing systems generate\nmultivariate time series with states at multiple levels of granularity, from\ncoarse regimes to fine-grained events. Effective segmentation in these settings\nrequires integrating across granularities while supporting iterative refinement\nthrough sparse prompt signals, which provide a compact mechanism for injecting\ndomain knowledge. Yet existing prompting approaches for time series\nsegmentation operate only within local contexts, so the effect of a prompt\nquickly fades and cannot guide predictions across the entire sequence. To\novercome this limitation, we propose MemPromptTSS, a framework for iterative\nmulti-granularity segmentation that introduces persistent prompt memory. A\nmemory encoder transforms prompts and their surrounding subsequences into\nmemory tokens stored in a bank. This persistent memory enables each new\nprediction to condition not only on local cues but also on all prompts\naccumulated across iterations, ensuring their influence persists across the\nentire sequence. Experiments on six datasets covering wearable sensing and\nindustrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy\nimprovements over the best baseline in single- and multi-granularity\nsegmentation under single iteration inference, and provides stronger refinement\nin iterative inference with average per-iteration gains of 2.66 percentage\npoints compared to 1.19 for PromptTSS. These results highlight the importance\nof persistent memory for prompt-guided segmentation, establishing MemPromptTSS\nas a practical and effective framework for real-world applications."}
{"id": "2510.09942", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09942", "abs": "https://arxiv.org/abs/2510.09942", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Meiyu Zhong", "Guangyi Zhang", "Osvaldo Simeone", "Ravi Tandon"], "title": "Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: AI and ML for Next-Generation Wireless Communications and\n  Networking (AI4NextG)", "summary": "Edge-cloud speculative decoding (SD) accelerates inference by having a\ncloud-based large language model (LLM) that verifies draft tokens generated by\na resource-constrained small language model (SLM) at the edge. A central\nbottleneck is the limited bandwidth of the edge-cloud link, which necessitates\nefficient compression of draft token distributions. We first derive an\ninformation-theoretic bound that decomposes the token rejection rate into\ncontributions from SLM-LLM distribution mismatch and from quantization\ndistortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample\nSD (SQS-SD) framework, which exploits distributional sparsity through\nstructured sparsification and lattice-based quantization. Within this\nframework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts\nthe retained token set via online conformal prediction to ensure bounded\ndeviation from the dense distribution. Empirical results confirm that both\napproaches improve end-to-end latency and rejection rates in complimentary\noperating regimes."}
{"id": "2510.09959", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09959", "abs": "https://arxiv.org/abs/2510.09959", "authors": ["Jun Yin", "Runcheng Cai", "Shiliang Sun"], "title": "Clustering Result Re-guided Incomplete Multi-view Spectral Clustering", "comment": null, "summary": "Incomplete multi-view spectral clustering generalizes spectral clustering to\nmulti-view data and simultaneously realizes the partition of multi-view data\nwith missing views. For this category of method, K-means algorithm needs to be\nperformed to generate the clustering result after the procedure of feature\nextraction. More importantly, the connectivity of samples reflected by the\nclustering result is not utilized effectively. To overcome these defects, we\npropose Clustering Result re-Guided Incomplete Multi-view Spectral Clustering\n(CRG_IMSC). CRG_IMSC obtains the clustering result directly by imposing\nnonnegative constraint to the extracted feature. Furthermore, it constructs the\nconnectivity matrix according to the result of spectral clustering, and\nminimizes the residual of self-representation based on the connectivity matrix.\nA novel iterative algorithm using multiplicative update is developed to solve\nthe optimization problem of CRG_IMSC, and its convergence is proved rigorously.\nOn benchmark datasets, for multi-view data, CRG_IMSC performs better than\nstate-of-the-art clustering methods, and the experimental results also\ndemonstrate the convergence of CRG_IMSC algorithm."}
{"id": "2510.09965", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09965", "abs": "https://arxiv.org/abs/2510.09965", "authors": ["Shuo Zhao", "Yongqiang Li", "Yu Feng", "Zhongsheng Hou", "Yuanjing Feng"], "title": "Homomorphic Mappings for Value-Preserving State Aggregation in Markov Decision Processes", "comment": null, "summary": "State aggregation aims to reduce the computational complexity of solving\nMarkov Decision Processes (MDPs) while preserving the performance of the\noriginal system. A fundamental challenge lies in optimizing policies within the\naggregated, or abstract, space such that the performance remains optimal in the\nground MDP-a property referred to as {\"}optimal policy equivalence {\"}.\n  This paper presents an abstraction framework based on the notion of\nhomomorphism, in which two Markov chains are deemed homomorphic if their value\nfunctions exhibit a linear relationship. Within this theoretical framework, we\nestablish a sufficient condition for the equivalence of optimal policy.\n  We further examine scenarios where the sufficient condition is not met and\nderive an upper bound on the approximation error and a performance lower bound\nfor the objective function under the ground MDP. We propose Homomorphic Policy\nGradient (HPG), which guarantees optimal policy equivalence under sufficient\nconditions, and its extension, Error-Bounded HPG (EBHPG), which balances\ncomputational efficiency and the performance loss induced by aggregation. In\nthe experiments, we validated the theoretical results and conducted comparative\nevaluations against seven algorithms."}
{"id": "2510.09976", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL."}
{"id": "2510.09977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09977", "abs": "https://arxiv.org/abs/2510.09977", "authors": ["Frida Cantu", "Salomon Ibarra", "Arturo Gonzales", "Jesus Barreda", "Chenang Liu", "Li Zhang"], "title": "An Unsupervised Time Series Anomaly Detection Approach for Efficient Online Process Monitoring of Additive Manufacturing", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "Online sensing plays an important role in advancing modern manufacturing. The\nreal-time sensor signals, which can be stored as high-resolution time series\ndata, contain rich information about the operation status. One of its popular\nusages is online process monitoring, which can be achieved by effective anomaly\ndetection from the sensor signals. However, most existing approaches either\nheavily rely on labeled data for training supervised models, or are designed to\ndetect only extreme outliers, thus are ineffective at identifying subtle\nsemantic off-track anomalies to capture where new regimes or unexpected\nroutines start. To address this challenge, we propose an matrix profile-based\nunsupervised anomaly detection algorithm that captures fabrication cycle\nsimilarity and performs semantic segmentation to precisely identify the onset\nof defect anomalies in additive manufacturing. The effectiveness of the\nproposed method is demonstrated by the experiments on real-world sensor data."}
{"id": "2510.09984", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09984", "abs": "https://arxiv.org/abs/2510.09984", "authors": ["Kartikeya Aneja", "Nagender Aneja", "Murat Kantarcioglu"], "title": "Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection", "comment": null, "summary": "Software systems can be represented as graphs, capturing dependencies among\nfunctions and processes. An interesting aspect of software systems is that they\ncan be represented as different types of graphs, depending on the extraction\ngoals and priorities. For example, function calls within the software can be\ncaptured to create function call graphs, which highlight the relationships\nbetween functions and their dependencies. Alternatively, the processes spawned\nby the software can be modeled to generate process interaction graphs, which\nfocus on runtime behavior and inter-process communication. While these graph\nrepresentations are related, each captures a distinct perspective of the\nsystem, providing complementary insights into its structure and operation.\nWhile previous studies have leveraged graph neural networks (GNNs) to analyze\nsoftware behaviors, most of this work has focused on a single type of graph\nrepresentation. The joint modeling of both function call graphs and process\ninteraction graphs remains largely underexplored, leaving opportunities for\ndeeper, multi-perspective analysis of software systems. This paper presents a\npipeline for constructing and training Function Call Graphs (FCGs) and Process\nCall Graphs (PCGs) and learning joint embeddings. We demonstrate that joint\nembeddings outperform a single-graph model. In this paper, we propose\nGeminiNet, a unified neural network approach that learns joint embeddings from\nboth FCGs and PCGs. We construct a new dataset of 635 Windows executables (318\nmalicious and 317 benign), extracting FCGs via Ghidra and PCGs via Any.Run\nsandbox. GeminiNet employs dual graph convolutional branches with an adaptive\ngating mechanism that balances contributions from static and dynamic views."}
{"id": "2510.10000", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10000", "abs": "https://arxiv.org/abs/2510.10000", "authors": ["Bach C. Le", "Tung V. Dao", "Binh T. Nguyen", "Hong T. M. Chu"], "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks", "comment": null, "summary": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA"}
{"id": "2510.10004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10004", "abs": "https://arxiv.org/abs/2510.10004", "authors": ["Jiahui Hong", "Siqing Li", "Muqing Jian", "Luming Yang"], "title": "Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification", "comment": "Accepted to IEEE BIBM 2025", "summary": "Existing EEG recognition models suffer from poor cross-paradigm\ngeneralization due to dataset-specific constraints and individual variability.\nTo overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid\nNetwork), an end-to-end unified architecture featuring robust multistream\nsynergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive\nconvolutions. The framework uniquely integrates: 1) Aligned time-frequency\nstreams maintaining temporal synchronization with STFT for bidirectional\nmodeling, 2) PTFA-based multi-scale feature enhancement amplifying critical\nneural patterns, 3) BiTCN with learnable fusion capturing forward/backward\nneural dynamics. Demonstrating enhanced robustness, BITE achieves\nstate-of-the-art performance across four divergent paradigms (BCICIV-2A/2B,\nHGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject\ngeneralization. As a unified architecture, it combines robust performance\nacross both MI and SSVEP tasks with exceptional computational efficiency. Our\nwork validates that paradigm-aligned spectral-temporal processing is essential\nfor reliable BCI systems. Just as its name suggests, BITE \"takes a bite out of\nEEG.\" The source code is available at https://github.com/cindy-hong/BiteEEG."}
{"id": "2510.10023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10023", "abs": "https://arxiv.org/abs/2510.10023", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "title": "Skill-Targeted Adaptive Training", "comment": null, "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT."}
{"id": "2510.10028", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10028", "abs": "https://arxiv.org/abs/2510.10028", "authors": ["Yang Li", "Ruichen Zhang", "Yinqiu Liu", "Guangyuan Liu", "Dusit Niyato", "Abbas Jamalipour", "Xianbin Wang", "Dong In Kim"], "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization", "comment": null, "summary": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled\na variety of applications, including aerial surveillance, environmental\nsensing, and semantic data collection. To support these scenarios, unmanned\naerial vehicles (UAVs) equipped with onboard vision-language models (VLMs)\noffer a promising solution for real-time multimodal inference. However,\nensuring both inference accuracy and communication efficiency remains a\nsignificant challenge due to limited onboard resources and dynamic network\nconditions. In this paper, we first propose a UAV-enabled LAENet system model\nthat jointly captures UAV mobility, user-UAV communication, and the onboard\nvisual question answering (VQA) pipeline. Based on this model, we formulate a\nmixed-integer non-convex optimization problem to minimize task latency and\npower consumption under user-specific accuracy constraints. To solve the\nproblem, we design a hierarchical optimization framework composed of two parts:\n(i) an Alternating Resolution and Power Optimization (ARPO) algorithm for\nresource allocation under accuracy constraints, and (ii) a Large Language\nModel-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV\ntrajectory optimization. The large language model (LLM) serves as an expert in\nrefining reward design of reinforcement learning in an offline fashion,\nintroducing no additional latency in real-time decision-making. Numerical\nresults demonstrate the efficacy of our proposed framework in improving\ninference performance and communication efficiency under dynamic LAENet\nconditions."}
{"id": "2510.10029", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10029", "abs": "https://arxiv.org/abs/2510.10029", "authors": ["Ruoxing Yang"], "title": "Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training", "comment": null, "summary": "We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,\nmodel-free deep-reinforcement-learning algorithm that leverages pretraining to\nachieve high training efficiency and stability on very small training samples\nin physics-based environments. Reinforcement learning agents typically rely on\nlarge samples of environment interactions to learn a policy. However, frequent\ninteractions with a (computer-simulated) environment may incur high\ncomputational costs, especially when the environment is complex. Our main\ninnovation is a new policy neural network architecture that consists of a\npretrained neural network middle section sandwiched between two fully-connected\nnetworks. Pretraining part of the network on a different environment with\nsimilar physics will help the agent learn the target environment with high\nefficiency because it will leverage a general understanding of the\ntransferrable physics characteristics from the pretraining environment. We\ndemonstrate that PPOPT outperforms baseline classic PPO on small training\nsamples both in terms of rewards gained and general training stability. While\nPPOPT underperforms against classic model-based methods such as DYNA DDPG, the\nmodel-free nature of PPOPT allows it to train in significantly less time than\nits model-based counterparts. Finally, we present our implementation of PPOPT\nas open-source software, available at github.com/Davidrxyang/PPOPT."}
{"id": "2510.10041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10041", "abs": "https://arxiv.org/abs/2510.10041", "authors": ["Sahng-Min Han", "Minjae Kim", "Jinho Cha", "Se-woon Choe", "Eunchan Daniel Cha", "Jungwon Choi", "Kyudong Jung"], "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis", "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine\n  (Elsevier, under review)", "summary": "Deep learning in small and imbalanced biomedical datasets remains\nfundamentally constrained by unstable optimization and poor generalization. We\npresent the first biomedical implementation of FOSSIL (Flexible Optimization\nvia Sample-Sensitive Importance Learning), a regret-minimizing weighting\nframework that adaptively balances training emphasis according to sample\ndifficulty. Using softmax-based uncertainty as a continuous measure of\ndifficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate\nFOSSIL into both convolutional and transformer-based architectures for Mpox\nskin lesion diagnosis. Across all settings, FOSSIL substantially improves\ndiscrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under\nreal-world perturbations, outperforming conventional baselines without\nmetadata, manual curation, or synthetic augmentation. The results position\nFOSSIL as a generalizable, data-efficient, and interpretable framework for\ndifficulty-aware learning in medical imaging under data scarcity."}
{"id": "2510.10057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10057", "abs": "https://arxiv.org/abs/2510.10057", "authors": ["Lei Gao", "Shihong Huang", "Shengjie Wang", "Hong Ma", "Feng Zhang", "Hengda Bao", "Qichang Chen", "Weihua Zhou"], "title": "One4Many-StablePacker: An Efficient Deep Reinforcement Learning Framework for the 3D Bin Packing Problem", "comment": null, "summary": "The three-dimensional bin packing problem (3D-BPP) is widely applied in\nlogistics and warehousing. Existing learning-based approaches often neglect\npractical stability-related constraints and exhibit limitations in generalizing\nacross diverse bin dimensions. To address these limitations, we propose a novel\ndeep reinforcement learning framework, One4Many-StablePacker (O4M-SP). The\nprimary advantage of O4M-SP is its ability to handle various bin dimensions in\na single training process while incorporating support and weight constraints\ncommon in practice. Our training method introduces two innovative mechanisms.\nFirst, it employs a weighted reward function that integrates loading rate and a\nnew height difference metric for packing layouts, promoting improved bin\nutilization through flatter packing configurations. Second, it combines clipped\npolicy gradient optimization with a tailored policy drifting method to mitigate\npolicy entropy collapse, encouraging exploration at critical decision nodes\nduring packing to avoid suboptimal solutions. Extensive experiments demonstrate\nthat O4M-SP generalizes successfully across diverse bin dimensions and\nsignificantly outperforms baseline methods. Furthermore, O4M-SP exhibits strong\npractical applicability by effectively addressing packing scenarios with\nstability constraints."}
{"id": "2510.10060", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10060", "abs": "https://arxiv.org/abs/2510.10060", "authors": ["Hehe Fan", "Yi Yang", "Mohan Kankanhalli", "Fei Wu"], "title": "Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling", "comment": "technical report", "summary": "When modeling a given type of data, we consider it to involve two key\naspects: 1) identifying relevant elements (e.g., image pixels or textual words)\nto a central element, as in a convolutional receptive field, or to a query\nelement, as in self-attention, and 2) encoding these tokens effectively.\nSelf-attention can adaptively identify these elements but relies on absolute\npositional embedding for structural representation learning. In contrast,\nconvolution encodes elements in a relative manner, yet their fixed kernel size\nlimits their ability to adaptively select the relevant elements. In this paper,\nwe introduce Translution, an operation that unifies the adaptive identification\ncapability of self-attention and the relative encoding advantage of\nconvolution. However, this integration leads to a substantial increase in the\nnumber of parameters, exceeding most currently available computational\nresources. Therefore, we propose a lightweight variant of Translution, named\n{\\alpha}-Translution. Experiments on computer vision and natural language\nprocessing tasks show that Translution (including {\\alpha}-Translution)\nachieves superior accuracy compared to self-attention. The code is available at\nhttps://github.com/hehefan/Translution."}
{"id": "2510.10071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10071", "abs": "https://arxiv.org/abs/2510.10071", "authors": ["Jinyang Zhang", "Yue Fang", "Hongxin Ding", "Weibin Liao", "Muyang Ye", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning", "comment": null, "summary": "Conventional continual pretraining (CPT) for large language model (LLM)\ndomain adaptation often suffers from catastrophic forgetting and limited domain\ncapacity. Existing strategies adopt layer expansion, introducing additional\ntrainable parameters to accommodate new knowledge. However, the uniform\nexpansion and updates still entangle general and domain learning, undermining\nits effectiveness. Our pilot studies reveal that LLMs exhibit functional\nspecialization, where layers and units differentially encode general-critical\ncapabilities, suggesting that parameter expansion and optimization should be\nfunction-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled\nTuning for continual pretraining, a two-stage framework for domain-adaptive\nCPT. ADEPT first performs General-Competence Guided Selective Layer Expansion,\nduplicating layers least critical for the general domain to increase\nrepresentational capacity while minimizing interference with general knowledge.\nIt then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter\nunits within expanded layers according to their general-domain importance and\nassigning asymmetric learning rates to balance knowledge injection and\nretention. Experiments on mathematical and medical benchmarks show that ADEPT\noutperforms full-parameter CPT by up to 5.76% on the general domain and 5.58%\non the target domain with only 15% of parameters tuned and less than 50%\ntraining time. Ablation studies, theoretical analysis, and extended\ninvestigations further demonstrate the necessity of targeted expansion and\ndecoupled optimization, providing new principles for efficient and robust\ndomain-adaptive CPT. Our code is open-sourced at\nhttps://github.com/PuppyKnightUniversity/ADEPT"}
{"id": "2510.10075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10075", "abs": "https://arxiv.org/abs/2510.10075", "authors": ["Salomon Ibarra", "Frida Cantu", "Kaixiong Zhou", "Li Zhang"], "title": "Gradient-based Model Shortcut Detection for Time Series Classification", "comment": "Code available at: https://github.com/IvorySnake02/SAG.git", "summary": "Deep learning models have attracted lots of research attention in time series\nclassification (TSC) task in the past two decades. Recently, deep neural\nnetworks (DNN) have surpassed classical distance-based methods and achieved\nstate-of-the-art performance. Despite their promising performance, deep neural\nnetworks (DNNs) have been shown to rely on spurious correlations present in the\ntraining data, which can hinder generalization. For instance, a model might\nincorrectly associate the presence of grass with the label ``cat\" if the\ntraining set have majority of cats lying in grassy backgrounds. However, the\nshortcut behavior of DNNs in time series remain under-explored. Most existing\nshortcut work are relying on external attributes such as gender, patients\ngroup, instead of focus on the internal bias behavior in time series models.\n  In this paper, we take the first step to investigate and establish\npoint-based shortcut learning behavior in deep learning time series\nclassification. We further propose a simple detection method based on other\nclass to detect shortcut occurs without relying on test data or clean training\nclasses. We test our proposed method in UCR time series datasets."}
{"id": "2510.10089", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10089", "abs": "https://arxiv.org/abs/2510.10089", "authors": ["Zixuan Gong", "Jiaye Teng", "Yong Liu"], "title": "What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)", "comment": null, "summary": "While looped transformers (termed as Looped-Attn) often outperform standard\ntransformers (termed as Single-Attn) on complex reasoning tasks, the\ntheoretical basis for this advantage remains underexplored. In this paper, we\nexplain this phenomenon through the lens of loss landscape geometry, inspired\nby empirical observations of their distinct dynamics at both sample and Hessian\nlevels. To formalize this, we extend the River-Valley landscape model by\ndistinguishing between U-shaped valleys (flat) and V-shaped valleys (steep).\nBased on empirical observations, we conjecture that the recursive architecture\nof Looped-Attn induces a landscape-level inductive bias towards River-V-Valley.\nTheoretical derivations based on this inductive bias guarantee a better loss\nconvergence along the river due to valley hopping, and further encourage\nlearning about complex patterns compared to the River-U-Valley induced by\nSingle-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical\nFramework for Progressive Training), a staged training framework that\naccelerates the training process of Looped-Attn while achieving comparable\nperformances."}
{"id": "2510.10101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10101", "abs": "https://arxiv.org/abs/2510.10101", "authors": ["Martin Carrasco", "Caio Deberaldini Netto", "Vahan A. Martirosyan", "Aneeqa Mehrab", "Ehimare Okoyomon", "Caterina Graziani"], "title": "Rademacher Meets Colors: More Expressivity, but at What Cost ?", "comment": null, "summary": "The expressive power of graph neural networks (GNNs) is typically understood\nthrough their correspondence with graph isomorphism tests such as the\nWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a\nricher set of graphs, they are also observed to suffer from higher\ngeneralization error. This work provides a theoretical explanation for this\ntrade-off by linking expressivity and generalization through the lens of\ncoloring algorithms. Specifically, we show that the number of equivalence\nclasses induced by WL colorings directly bounds the GNNs Rademacher complexity\n-- a key data-dependent measure of generalization. Our analysis reveals that\ngreater expressivity leads to higher complexity and thus weaker generalization\nguarantees. Furthermore, we prove that the Rademacher complexity is stable\nunder perturbations in the color counts across different samples, ensuring\nrobustness to sampling variability across datasets. Importantly, our framework\nis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN\narchitectures and expressivity measures that partition graphs into equivalence\nclasses. These results unify the study of expressivity and generalization in\nGNNs, providing a principled understanding of why increasing expressive power\noften comes at the cost of generalization."}
{"id": "2510.10102", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10102", "abs": "https://arxiv.org/abs/2510.10102", "authors": ["Guilin Li", "Yun Zhang", "Xiuyuan Chen", "Chengqi Li", "Bo Wang", "Linghe Kong", "Wenjia Wang", "Weiran Huang", "Matthias Hwai Yong Tan"], "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling", "comment": null, "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."}
{"id": "2510.10105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10105", "abs": "https://arxiv.org/abs/2510.10105", "authors": ["Yanping Zheng", "Zhewei Wei", "Frank de Hoog", "Xu Chen", "Hongteng Xu", "Yuhang Ye", "Jiadeng Huang"], "title": "Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in\nrecommendation systems. However, conventional graph-based recommenders, such as\nLightGCN, require maintaining embeddings of size $d$ for each node, resulting\nin a parameter complexity of $\\mathcal{O}(n \\times d)$, where $n$ represents\nthe total number of users and items. This scaling pattern poses significant\nchallenges for deployment on large-scale graphs encountered in real-world\napplications. To address this scalability limitation, we propose\n\\textbf{Lighter-X}, an efficient and modular framework that can be seamlessly\nintegrated with existing GNN-based recommender architectures. Our approach\nsubstantially reduces both parameter size and computational complexity while\npreserving the theoretical guarantees and empirical performance of the base\nmodels, thereby enabling practical deployment at scale. Specifically, we\nanalyze the original structure and inherent redundancy in their parameters,\nidentifying opportunities for optimization. Based on this insight, we propose\nan efficient compression scheme for the sparse adjacency structure and\nhigh-dimensional embedding matrices, achieving a parameter complexity of\n$\\mathcal{O}(h \\times d)$, where $h \\ll n$. Furthermore, the model is optimized\nthrough a decoupled framework, reducing computational complexity during the\ntraining process and enhancing scalability. Extensive experiments demonstrate\nthat Lighter-X achieves comparable performance to baseline models with\nsignificantly fewer parameters. In particular, on large-scale interaction\ngraphs with millions of edges, we are able to attain even better results with\nonly 1\\% of the parameter over LightGCN."}
{"id": "2510.10116", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10116", "abs": "https://arxiv.org/abs/2510.10116", "authors": ["Xing Wei", "Chunchun Chen", "Rui Fan", "Xiaofeng Cao", "Sourav Medya", "Wei Ye"], "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification", "comment": "Accepted at NeurIPS 2025", "summary": "Graph neural networks (GNNs) can efficiently process text-attributed graphs\n(TAGs) due to their message-passing mechanisms, but their training heavily\nrelies on the human-annotated labels. Moreover, the complex and diverse local\ntopologies of nodes of real-world TAGs make it challenging for a single\nmechanism to handle. Large language models (LLMs) perform well in\nzero-/few-shot learning on TAGs but suffer from a scalability challenge.\nTherefore, we propose a preference-driven knowledge distillation (PKD)\nframework to synergize the complementary strengths of LLMs and various GNNs for\nfew-shot node classification. Specifically, we develop a GNN-preference-driven\nnode selector that effectively promotes prediction distillation from LLMs to\nteacher GNNs. To further tackle nodes' intricate local topologies, we develop a\nnode-preference-driven GNN selector that identifies the most suitable teacher\nGNN for each node, thereby facilitating tailored knowledge distillation from\nteacher GNNs to the student GNN. Extensive experiments validate the efficacy of\nour proposed framework in few-shot node classification on real-world TAGs."}
{"id": "2510.10129", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10129", "abs": "https://arxiv.org/abs/2510.10129", "authors": ["Bin Yang", "Qiuyu Leng", "Jun Zeng", "Zhenhua Wu"], "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."}
{"id": "2510.10136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10136", "abs": "https://arxiv.org/abs/2510.10136", "authors": ["Lancheng Zou", "Shuo Yin", "Zehua Pei", "Tsung-Yi Ho", "Farzan Farnia", "Bei Yu"], "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "Channel permutation is a powerful technique for enhancing the accuracy of N:M\nsparse models by reordering the channels of weight matrices to prioritize the\nretention of important weights. However, traditional channel permutation\nmethods rely on handcrafted quality metrics, which often fail to accurately\ncapture the true impact of pruning on model performance. To address this\nlimitation, we propose PermLLM, a novel post-training pruning framework that\nintroduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages\nSinkhorn normalization to transform discrete permutation matrices into\ndifferentiable soft permutation matrices, enabling end-to-end optimization.\nAdditionally, PermLLM incorporates an efficient block-wise channel permutation\nstrategy, which significantly reduces the number of learnable parameters and\ncomputational complexity. PermLLM seamlessly integrates with existing one-shot\npruning methods to adaptively optimize channel permutations, effectively\nmitigating pruning-induced errors. Extensive experiments on the LLaMA series,\nQwen, and OPT models demonstrate that PermLLM achieves superior performance in\noptimizing N:M sparse models. The code is available at\nhttps://github.com/lanchengzou/PermLLM."}
{"id": "2510.10140", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10140", "abs": "https://arxiv.org/abs/2510.10140", "authors": ["Yue Deng", "Francisco Santos", "Pang-Ning Tan", "Lifeng Luo"], "title": "Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction", "comment": null, "summary": "Deep learning based weather forecasting (DLWF) models leverage past weather\nobservations to generate future forecasts, supporting a wide range of\ndownstream tasks, including tropical cyclone (TC) trajectory prediction. In\nthis paper, we investigate their vulnerability to adversarial attacks, where\nsubtle perturbations to the upstream weather forecasts can alter the downstream\nTC trajectory predictions. Although research on adversarial attacks in DLWF\nmodels has grown recently, generating perturbed upstream forecasts that\nreliably steer downstream output toward attacker-specified trajectories remains\na challenge. First, conventional TC detection systems are opaque,\nnon-differentiable black boxes, making standard gradient-based attacks\ninfeasible. Second, the extreme rarity of TC events leads to severe class\nimbalance problem, making it difficult to develop efficient attack methods that\nwill produce the attacker's target trajectories. Furthermore, maintaining\nphysical consistency in adversarially generated forecasts presents another\nsignificant challenge. To overcome these limitations, we propose Cyc-Attack, a\nnovel method that perturbs the upstream forecasts of DLWF models to generate\nadversarial trajectories. First, we pre-train a differentiable surrogate model\nto approximate the TC detector's output, enabling the construction of\ngradient-based attacks. Cyc-Attack also employs skewness-aware loss function\nwith kernel dilation strategy to address the imbalance problem. Finally, a\ndistance-based gradient weighting scheme and regularization are used to\nconstrain the perturbations and eliminate spurious trajectories to ensure the\nadversarial forecasts are realistic and not easily detectable."}
{"id": "2510.10145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10145", "abs": "https://arxiv.org/abs/2510.10145", "authors": ["Cheng He", "Xijie Liang", "Zengrong Zheng", "Patrick P. C. Lee", "Xu Huang", "Zhaoyi Li", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting", "comment": null, "summary": "Current approaches for time series forecasting, whether in the time or\nfrequency domain, predominantly use deep learning models based on linear layers\nor transformers. They often encode time series data in a black-box manner and\nrely on trial-and-error optimization solely based on forecasting performance,\nleading to limited interpretability and theoretical understanding. Furthermore,\nthe dynamics in data distribution over time and frequency domains pose a\ncritical challenge to accurate forecasting. We propose FIRE, a unified\nfrequency domain decomposition framework that provides a mathematical\nabstraction for diverse types of time series, so as to achieve interpretable\nand robust time series forecasting. FIRE introduces several key innovations:\n(i) independent modeling of amplitude and phase components, (ii) adaptive\nlearning of weights of frequency basis components, (iii) a targeted loss\nfunction, and (iv) a novel training paradigm for sparse data. Extensive\nexperiments demonstrate that FIRE consistently outperforms state-of-the-art\nmodels on long-term forecasting benchmarks, achieving superior predictive\nperformance and significantly enhancing interpretability of time series"}
{"id": "2510.10149", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10149", "abs": "https://arxiv.org/abs/2510.10149", "authors": ["Xin Chen", "Gillian Dobbie", "Xinyu Wang", "Feng Liu", "Di Wang", "Jingfeng Zhang"], "title": "Robust Learning of Diffusion Models with Extremely Noisy Conditions", "comment": null, "summary": "Conditional diffusion models have the generative controllability by\nincorporating external conditions. However, their performance significantly\ndegrades with noisy conditions, such as corrupted labels in the image\ngeneration or unreliable observations or states in the control policy\ngeneration. This paper introduces a robust learning framework to address\nextremely noisy conditions in conditional diffusion models. We empirically\ndemonstrate that existing noise-robust methods fail when the noise level is\nhigh. To overcome this, we propose learning pseudo conditions as surrogates for\nclean conditions and refining pseudo ones progressively via the technique of\ntemporal ensembling. Additionally, we develop a Reverse-time Diffusion\nCondition (RDC) technique, which diffuses pseudo conditions to reinforce the\nmemorization effect and further facilitate the refinement of the pseudo\nconditions. Experimentally, our approach achieves state-of-the-art performance\nacross a range of noise levels on both class-conditional image generation and\nvisuomotor policy generation tasks.The code can be accessible via the project\npage https://robustdiffusionpolicy.github.io"}
{"id": "2510.10150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10150", "abs": "https://arxiv.org/abs/2510.10150", "authors": ["Zhezheng Hao", "Hong Wang", "Haoyang Liu", "Jian Luo", "Jiarui Yu", "Hande Dong", "Qiang Lin", "Can Wang", "Jiawei Chen"], "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective", "comment": null, "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM\nreasoning, its training process poses a critical risk: entropy collapse. This\nphenomenon is a rapid loss of policy diversity, stemming from the\nexploration-exploitation imbalance and leading to a lack of generalization.\nRecent entropy-intervention methods aim to prevent \\coloredtext{entropy\ncollapse}, yet their underlying mechanisms remain unclear. In this paper, we\nconduct a quantitative analysis to reveal token-level entropy changes and how\nexisting entropy intervention methods help avoid entropy collapse. Our findings\npoint out a fundamental limitation of existing methods: they attempt to control\nentropy dynamics indirectly. By only affecting related factors, such as the\nadvantage signal and generation probability, their effectiveness is inherently\nlimited and could potentially fail. To address this limitation, we introduce an\nentropy-change-aware reweighting scheme, namely Stabilizing Token-level\nEntropy-changE via Reweighting (STEER), that adaptively stabilizes entropy\ndynamics through fine-grained token-level adjustments. Our approach mitigates\nover-exploitation while fostering robust exploration. Extensive experiments\ndemonstrate that STEER significantly mitigates entropy collapse, stabilizes\nentropy dynamics, and achieves stronger downstream performance across various\nmathematical reasoning benchmarks \\footnote{Our code is available at\nhttps://github.com/zz-haooo/STEER."}
{"id": "2510.10188", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10188", "abs": "https://arxiv.org/abs/2510.10188", "authors": ["Linfei Li", "Fengyi Zhang", "Zhong Wang", "Lin Zhang", "Ying Shen"], "title": "INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction", "comment": null, "summary": "Implicit Neural Representations (INRs) have gained success in various signal\nprocessing tasks due to their advantages of continuity and infinite resolution.\nHowever, the factors influencing their effectiveness and limitations remain\nunderexplored. To better understand these factors, we leverage insights from\nNeural Tangent Kernel (NTK) theory to analyze how model architectures (classic\nMLP and emerging KAN), positional encoding, and nonlinear primitives affect the\nresponse to signals of varying frequencies. Building on this analysis, we\nintroduce INR-Bench, the first comprehensive benchmark specifically designed\nfor multimodal INR tasks. It includes 56 variants of Coordinate-MLP models\n(featuring 4 types of positional encoding and 14 activation functions) and 22\nCoordinate-KAN models with distinct basis functions, evaluated across 9\nimplicit multimodal tasks. These tasks cover both forward and inverse problems,\noffering a robust platform to highlight the strengths and limitations of\ndifferent neural models, thereby establishing a solid foundation for future\nresearch. The code and dataset are available at\nhttps://github.com/lif314/INR-Bench."}
{"id": "2510.10195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10195", "abs": "https://arxiv.org/abs/2510.10195", "authors": ["Hong-Kun Zhang", "Xin Li", "Sikun Yang", "Zhihong Xia"], "title": "CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions", "comment": null, "summary": "A novel neural network inspired by Cauchy's integral formula, is proposed for\nfunction approximation tasks that include time series forecasting, missing data\nimputation, etc. Hence, the novel neural network is named CauchyNet. By\nembedding real-valued data into the complex plane, CauchyNet efficiently\ncaptures complex temporal dependencies, surpassing traditional real-valued\nmodels in both predictive performance and computational efficiency. Grounded in\nCauchy's integral formula and supported by the universal approximation theorem,\nCauchyNet offers strong theoretical guarantees for function approximation. The\narchitecture incorporates complex-valued activation functions, enabling robust\nlearning from incomplete data while maintaining a compact parameter footprint\nand reducing computational overhead. Through extensive experiments in diverse\ndomains, including transportation, energy consumption, and epidemiological\ndata, CauchyNet consistently outperforms state-of-the-art models in predictive\naccuracy, often achieving a 50% lower mean absolute error with fewer\nparameters. These findings highlight CauchyNet's potential as an effective and\nefficient tool for data-driven predictive modeling, particularly in\nresource-constrained and data-scarce environments."}
{"id": "2510.10201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10201", "abs": "https://arxiv.org/abs/2510.10201", "authors": ["Jinghao Zhang", "Naishan Zheng", "Ruilin Li", "Dongzhou Cheng", "Zheming Liang", "Feng Zhao", "Jiaqi Wang"], "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment", "comment": "Project Website: https://jinghaoleven.github.io/RLFR/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals."}
{"id": "2510.10211", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10211", "abs": "https://arxiv.org/abs/2510.10211", "authors": ["Yida Xiong", "Jiameng Chen", "Kun Li", "Hongzhi Zhang", "Xiantao Cai", "Wenbin Hu"], "title": "Hierarchical Bayesian Flow Networks for Molecular Graph Generation", "comment": null, "summary": "Molecular graph generation is essentially a classification generation\nproblem, aimed at predicting categories of atoms and bonds. Currently,\nprevailing paradigms such as continuous diffusion models are trained to predict\ncontinuous numerical values, treating the training process as a regression\ntask. However, the final generation necessitates a rounding step to convert\nthese predictions back into discrete classification categories, which is\nintrinsically a classification operation. Given that the rounding operation is\nnot incorporated during training, there exists a significant discrepancy\nbetween the model's training objective and its inference procedure. As a\nconsequence, an excessive emphasis on point-wise precision can lead to\noverfitting and inefficient learning. This occurs because considerable efforts\nare devoted to capturing intra-bin variations that are ultimately irrelevant to\nthe discrete nature of the task at hand. Such a flaw results in diminished\nmolecular diversity and constrains the model's generalization capabilities. To\naddress this fundamental limitation, we propose GraphBFN, a novel hierarchical\ncoarse-to-fine framework based on Bayesian Flow Networks that operates on the\nparameters of distributions. By innovatively introducing Cumulative\nDistribution Function, GraphBFN is capable of calculating the probability of\nselecting the correct category, thereby unifying the training objective with\nthe sampling rounding operation. We demonstrate that our method achieves\nsuperior performance and faster generation, setting new state-of-the-art\nresults on the QM9 and ZINC250k molecular graph generation benchmarks."}
{"id": "2510.10232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10232", "abs": "https://arxiv.org/abs/2510.10232", "authors": ["Xuening Wu", "Shenqin Yin", "Yanlan Kang", "Xinhang Zhang", "Qianya Xu", "Zeping Chen", "Wenqiang Zhang"], "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification", "comment": null, "summary": "Recursive self-modification is increasingly central in AutoML, neural\narchitecture search, and adaptive optimization, yet no existing framework\nensures that such changes are made safely. Godel machines offer a principled\nsafeguard by requiring formal proofs of improvement before rewriting code;\nhowever, such proofs are unattainable in stochastic, high-dimensional settings.\nWe introduce the Statistical Godel Machine (SGM), the first statistical safety\nlayer for recursive edits. SGM replaces proof-based requirements with\nstatistical confidence tests (e-values, Hoeffding bounds), admitting a\nmodification only when superiority is certified at a chosen confidence level,\nwhile allocating a global error budget to bound cumulative risk across\nrounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which\nindexes spending by confirmation events rather than rounds, concentrating the\nerror budget on promising edits while preserving familywise\nvalidity.Experiments across supervised learning, reinforcement learning, and\nblack-box optimization validate this role: SGM certifies genuine gains on\nCIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates\nrobustness on RL and optimization benchmarks.Together, these results position\nSGM as foundational infrastructure for continual, risk-aware self-modification\nin learning systems.Code is available at:\nhttps://github.com/gravitywavelet/sgm-anon."}
{"id": "2510.10244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10244", "abs": "https://arxiv.org/abs/2510.10244", "authors": ["Ziyu Zhou", "Keyan Hu", "Ling Zhang", "Zhaohui Xue", "Yutian Fang", "Yusha Zheng"], "title": "Progressive Scale Convolutional Network for Spatio-Temporal Downscaling of Soil Moisture: A Case Study Over the Tibetan Plateau", "comment": null, "summary": "Soil moisture (SM) plays a critical role in hydrological and meteorological\nprocesses. High-resolution SM can be obtained by combining coarse passive\nmicrowave data with fine-scale auxiliary variables. However, the inversion of\nSM at the temporal scale is hindered by the incompleteness of surface auxiliary\nfactors. To address this issue, first, we introduce validated high temporal\nresolution ERA5-Land variables into the downscaling process of the\nlow-resolution SMAP SM product. Subsequently, we design a progressive scale\nconvolutional network (PSCNet), at the core of which are two innovative\ncomponents: a multi-frequency temporal fusion module (MFTF) for capturing\ntemporal dynamics, and a bespoke squeeze-and-excitation (SE) block designed to\npreserve fine-grained spatial details. Using this approach, we obtained\nseamless SM products for the Tibetan Plateau (TP) from 2016 to 2018 at 10-km\nspatial and 3-hour temporal resolution. The experimental results on the TP\ndemonstrated the following: 1) In the satellite product validation, the PSCNet\nexhibited comparable accuracy and lower error, with a mean R value of 0.881,\noutperforming other methods. 2) In the in-situ site validation, PSCNet\nconsistently ranked among the top three models for the R metric across all\nsites, while also showing superior performance in overall error reduction. 3)\nIn the temporal generalization validation, the feasibility of using\nhigh-temporal resolution ERA5-Land variables for downscaling was confirmed, as\nall methods maintained an average relative error within 6\\% for the R metric\nand 2\\% for the ubRMSE metric. 4) In the temporal dynamics and visualization\nvalidation, PSCNet demonstrated excellent temporal sensitivity and vivid\nspatial details. Overall, PSCNet provides a promising solution for\nspatio-temporal downscaling by effectively modeling the intricate\nspatio-temporal relationships in SM data."}
{"id": "2510.10248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10248", "abs": "https://arxiv.org/abs/2510.10248", "authors": ["Jiaxi Zhuang", "Yaorui Shi", "Jue Hou", "Yunong He", "Mingwei Ye", "Mingjun Xu", "Yuming Su", "Linfeng Zhang", "Linfeng Zhang", "Guolin Ke", "Hengxing Cai"], "title": "Reasoning-Enhanced Large Language Models for Molecular Property Prediction", "comment": null, "summary": "Molecular property prediction is crucial for drug discovery and materials\nscience, yet existing approaches suffer from limited interpretability, poor\ncross-task generalization, and lack of chemical reasoning capabilities.\nTraditional machine learning models struggle with task transferability, while\nspecialized molecular language models provide little insight into their\ndecision-making processes. To address these limitations, we propose\n\\textbf{MPPReasoner}, a multimodal large language model that incorporates\nchemical reasoning for molecular property prediction. Our approach, built upon\nQwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to\nenable comprehensive molecular understanding. We develop a two-stage training\nstrategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning\ntrajectories generated through expert knowledge and multiple teacher models,\nfollowed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR\nemploys verifiable, rule-based rewards that systematically evaluate chemical\nprinciple application, molecular structure analysis, and logical consistency\nthrough computational verification. Extensive experiments across 8 datasets\ndemonstrate significant performance improvements, with MPPReasoner\noutperforming the best baselines by 7.91\\% and 4.53\\% on in-distribution and\nout-of-distribution tasks respectively. MPPReasoner exhibits exceptional\ncross-task generalization and generates chemically sound reasoning paths that\nprovide valuable insights into molecular property analysis, substantially\nenhancing both interpretability and practical utility for chemists. Code is\navailable at https://anonymous.4open.science/r/MPPReasoner-12687."}
{"id": "2510.10262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10262", "abs": "https://arxiv.org/abs/2510.10262", "authors": ["Jingwen Li", "Zhiguang Cao", "Yaoxin Wu", "Tang Liu"], "title": "Enhancing the Cross-Size Generalization for Solving Vehicle Routing Problems via Continual Learning", "comment": null, "summary": "Exploring machine learning techniques for addressing vehicle routing problems\nhas attracted considerable research attention. To achieve decent and efficient\nsolutions, existing deep models for vehicle routing problems are typically\ntrained and evaluated using instances of a single size. This substantially\nlimits their ability to generalize across different problem sizes and thus\nhampers their practical applicability. To address the issue, we propose a\ncontinual learning based framework that sequentially trains a deep model with\ninstances of ascending problem sizes. Specifically, on the one hand, we design\nan inter-task regularization scheme to retain the knowledge acquired from\nsmaller problem sizes in the model training on a larger size. On the other\nhand, we introduce an intra-task regularization scheme to consolidate the model\nby imitating the latest desirable behaviors during training on each size.\nAdditionally, we exploit the experience replay to revisit instances of formerly\ntrained sizes for mitigating the catastrophic forgetting. Experimental results\nshow that our approach achieves predominantly superior performance across\nvarious problem sizes (either seen or unseen in the training), as compared to\nstate-of-the-art deep models including the ones specialized for\ngeneralizability enhancement. Meanwhile, the ablation studies on the key\ndesigns manifest their synergistic effect in the proposed framework."}
{"id": "2510.10276", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10276", "abs": "https://arxiv.org/abs/2510.10276", "authors": ["Nikolaus Salvatore", "Hao Wang", "Qiong Zhang"], "title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "comment": null, "summary": "The performance of Large Language Models (LLMs) often degrades when crucial\ninformation is in the middle of a long context, a \"lost-in-the-middle\"\nphenomenon that mirrors the primacy and recency effects in human memory. We\npropose that this behavior is not simply a flaw indicative of information loss\nbut an adaptation to different information retrieval demands during\npre-training: some tasks require uniform recall across the entire input (a\nlong-term memory demand), while others prioritize the most recent information\n(a short-term memory demand). Consistent with this view, we show that this\nU-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are\ntrained from scratch on two simple human memory paradigms simulating long-term\nand short-term memory demands. Our analysis reveals that while the recency\neffect directly aligns with short-term memory demand in the training data, the\nprimacy effect is induced by the uniform long-term memory demand and is\nadditionally influenced by the model's autoregressive properties and the\nformation of attention sinks. Our main findings from simple human memory\nparadigms also generalize to a sequence completion task, which more closely\nresembles the next-token prediction process in LLM pre-training. Together, our\nfindings reveal how information retrieval demands, model architecture, and\nstructural attention dynamics during model training can jointly produce\npositional bias observed in LLMs."}
{"id": "2510.10278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10278", "abs": "https://arxiv.org/abs/2510.10278", "authors": ["Christopher Chiu", "Silviu Pitis", "Mihaela van der Schaar"], "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models", "comment": null, "summary": "Clinical reasoning in medicine is a hypothesis-driven process where\nphysicians refine diagnoses from limited information through targeted history,\nphysical examination, and diagnostic investigations. In contrast, current\nmedical benchmarks for large language models (LLMs) primarily assess knowledge\nrecall through single-turn questions, where complete clinical information is\nprovided upfront. To address this gap, we introduce VivaBench, a multi-turn\nbenchmark that evaluates sequential clinical reasoning in LLM agents. Our\ndataset consists of 1762 physician-curated clinical vignettes structured as\ninteractive scenarios that simulate a (oral) examination in medical training,\nrequiring agents to actively probe for relevant findings, select appropriate\ninvestigations, and synthesize information across multiple steps to reach a\ndiagnosis. While current LLMs demonstrate competence in diagnosing conditions\nfrom well-described clinical presentations, their performance degrades\nsignificantly when required to navigate iterative diagnostic reasoning under\nuncertainty in our evaluation. Our analysis identified several failure modes\nthat mirror common cognitive errors in clinical practice, including: (1)\nfixation on initial hypotheses, (2) inappropriate investigation ordering, (3)\npremature diagnostic closure, and (4) failing to screen for critical\nconditions. These patterns reveal fundamental limitations in how current LLMs\nreason and make decisions under uncertainty. Through VivaBench, we provide a\nstandardized benchmark for evaluating conversational medical AI systems for\nreal-world clinical decision support. Beyond medical applications, we\ncontribute to the larger corpus of research on agentic AI by demonstrating how\nsequential reasoning trajectories can diverge in complex decision-making\nenvironments."}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences."}
{"id": "2510.10341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10341", "abs": "https://arxiv.org/abs/2510.10341", "authors": ["Shiyu Chen", "Ningyuan", "Huang", "Soledad Villar"], "title": "Multi-View Graph Learning with Graph-Tuple", "comment": "Submitted to TAG workshop", "summary": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach."}
{"id": "2510.10364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10364", "abs": "https://arxiv.org/abs/2510.10364", "authors": ["Ali Mirzazadeh", "Simon Cadavid", "Kaiwen Zha", "Chao Li", "Sultan Alzahrani", "Manar Alawajy", "Joshua Korzenik", "Kreshnik Hoti", "Charles Reynolds", "David Mischoulon", "John Winkelman", "Maurizio Fava", "Dina Katabi"], "title": "Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker", "comment": null, "summary": "Antidepressant nonadherence is pervasive, driving relapse, hospitalization,\nsuicide risk, and billions in avoidable costs. Clinicians need tools that\ndetect adherence lapses promptly, yet current methods are either invasive\n(serum assays, neuroimaging) or proxy-based and inaccurate (pill counts,\npharmacy refills). We present the first noninvasive biomarker that detects\nantidepressant intake from a single night of sleep. A transformer-based model\nanalyzes sleep data from a consumer wearable or contactless wireless sensor to\ninfer antidepressant intake, enabling remote, effortless, daily adherence\nassessment at home. Across six datasets comprising 62,000 nights from >20,000\nparticipants (1,800 antidepressant users), the biomarker achieved AUROC = 0.84,\ngeneralized across drug classes, scaled with dose, and remained robust to\nconcomitant psychotropics. Longitudinal monitoring captured real-world\ninitiation, tapering, and lapses. This approach offers objective, scalable\nadherence surveillance with potential to improve depression care and outcomes."}
{"id": "2510.10374", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10374", "abs": "https://arxiv.org/abs/2510.10374", "authors": ["Ziyi Wei", "Huaiyang Zhong", "Xiaocheng Li"], "title": "Exploration-free Algorithms for Multi-group Mean Estimation", "comment": null, "summary": "We address the problem of multi-group mean estimation, which seeks to\nallocate a finite sampling budget across multiple groups to obtain uniformly\naccurate estimates of their means. Unlike classical multi-armed bandits, whose\nobjective is to minimize regret by identifying and exploiting the best arm, the\noptimal allocation in this setting requires sampling every group on the order\nof $\\Theta(T)$ times. This fundamental distinction makes exploration-free\nalgorithms both natural and effective. Our work makes three contributions.\nFirst, we strengthen the existing results on subgaussian variance concentration\nusing the Hanson-Wright inequality and identify a class of strictly subgaussian\ndistributions that yield sharper guarantees. Second, we design exploration-free\nnon-adaptive and adaptive algorithms, and we establish tighter regret bounds\nthan the existing results. Third, we extend the framework to contextual bandit\nsettings, an underexplored direction, and propose algorithms that leverage side\ninformation with provable guarantees. Overall, these results position\nexploration-free allocation as a principled and efficient approach to\nmulti-group mean estimation, with potential applications in experimental\ndesign, personalization, and other domains requiring accurate multi-group\ninference."}
{"id": "2510.10375", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.10375", "abs": "https://arxiv.org/abs/2510.10375", "authors": ["Kenichi Satoh"], "title": "Applying non-negative matrix factorization with covariates to label matrix for classification", "comment": "2 figures, R package: nmfkc published in GitHub,\n  https://github.com/ksatohds/nmfkc", "summary": "Non-negative matrix factorization (NMF) is widely used for dimensionality\nreduction and interpretable analysis, but standard formulations are\nunsupervised and cannot directly exploit class labels. Existing supervised or\nsemi-supervised extensions usually incorporate labels only via penalties or\ngraph constraints, still requiring an external classifier. We propose\n\\textit{NMF-LAB} (Non-negative Matrix Factorization for Label Matrix), which\nredefines classification as the inverse problem of non-negative matrix\ntri-factorization (tri-NMF). Unlike joint NMF methods, which reconstruct both\nfeatures and labels, NMF-LAB directly factorizes the label matrix $Y$ as the\nobservation, while covariates $A$ are treated as given explanatory variables.\nThis yields a direct probabilistic mapping from covariates to labels,\ndistinguishing our method from label-matrix factorization approaches that\nmainly model label correlations or impute missing labels. Our inversion offers\ntwo key advantages: (i) class-membership probabilities are obtained directly\nfrom the factorization without a separate classifier, and (ii) covariates,\nincluding kernel-based similarities, can be seamlessly integrated to generalize\npredictions to unseen samples. In addition, unlabeled data can be encoded as\nuniform distributions, supporting semi-supervised learning. Experiments on\ndiverse datasets, from small-scale benchmarks to the large-scale MNIST dataset,\ndemonstrate that NMF-LAB achieves competitive predictive accuracy, robustness\nto noisy or incomplete labels, and scalability to high-dimensional problems,\nwhile preserving interpretability. By unifying regression and classification\nwithin the tri-NMF framework, NMF-LAB provides a novel, probabilistic, and\nscalable approach to modern classification tasks."}
{"id": "2510.10402", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.10402", "abs": "https://arxiv.org/abs/2510.10402", "authors": ["Jiachi Zhao", "Zehong Wang", "Yamei Liao", "Chuxu Zhang", "Yanfang Ye"], "title": "Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance", "comment": null, "summary": "Graph generation is a fundamental problem in graph learning with broad\napplications across Web-scale systems, knowledge graphs, and scientific domains\nsuch as drug and material discovery. Recent approaches leverage diffusion\nmodels for step-by-step generation, yet unconditional diffusion offers little\ncontrol over desired properties, often leading to unstable quality and\ndifficulty in incorporating new objectives. Inference-time guidance methods\nmitigate these issues by adjusting the sampling process without retraining, but\nthey remain inherently local, heuristic, and limited in controllability. To\novercome these limitations, we propose TreeDiff, a Monte Carlo Tree Search\n(MCTS) guided dual-space diffusion framework for controllable graph generation.\nTreeDiff is a plug-and-play inference-time method that expands the search space\nwhile keeping computation tractable. Specifically, TreeDiff introduces three\nkey designs to make it practical and scalable: (1) a macro-step expansion\nstrategy that groups multiple denoising updates into a single transition,\nreducing tree depth and enabling long-horizon exploration; (2) a dual-space\ndenoising mechanism that couples efficient latent-space denoising with\nlightweight discrete correction in graph space, ensuring both scalability and\nstructural fidelity; and (3) a dual-space verifier that predicts long-term\nrewards from partially denoised graphs, enabling early value estimation and\nremoving the need for full rollouts. Extensive experiments on 2D and 3D\nmolecular generation benchmarks, under both unconditional and conditional\nsettings, demonstrate that TreeDiff achieves state-of-the-art performance.\nNotably, TreeDiff exhibits favorable inference-time scaling: it continues to\nimprove with additional computation, while existing inference-time methods\nplateau early under limited resources."}
{"id": "2510.10425", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10425", "abs": "https://arxiv.org/abs/2510.10425", "authors": ["Sara Dragutinović", "Andrew M. Saxe", "Aaditya K. Singh"], "title": "Softmax $\\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent", "comment": null, "summary": "The remarkable ability of transformers to learn new concepts solely by\nreading examples within the input prompt, termed in-context learning (ICL), is\na crucial aspect of intelligent behavior. Here, we focus on understanding the\nlearning algorithm transformers use to learn from context. Existing theoretical\nwork, often based on simplifying assumptions, has primarily focused on linear\nself-attention and continuous regression tasks, finding transformers can learn\nin-context by gradient descent. Given that transformers are typically trained\non discrete and complex tasks, we bridge the gap from this existing work to the\nsetting of classification, with non-linear (importantly, softmax) activation.\nWe find that transformers still learn to do gradient descent in-context, though\non functionals in the kernel feature space and with a context-adaptive learning\nrate in the case of softmax transformer. These theoretical findings suggest a\ngreater adaptability to context for softmax attention, which we empirically\nverify and study through ablations. Overall, we hope this enhances theoretical\nunderstanding of in-context learning algorithms in more realistic settings,\npushes forward our intuitions and enables further theory bridging to larger\nmodels."}
{"id": "2510.10432", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10432", "abs": "https://arxiv.org/abs/2510.10432", "authors": ["Zhichen Zeng", "Mengyue Hang", "Xiaolong Liu", "Xiaoyi Liu", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Zhining Liu", "Siyang Yuan", "Chaofei Yang", "Yiqun Liu", "Hang Yin", "Jiyan Yang", "Hanghang Tong"], "title": "Hierarchical LoRA MoE for Efficient CTR Model Scaling", "comment": "13 pages, 9 figures", "summary": "Deep models have driven significant advances in click-through rate (CTR)\nprediction. While vertical scaling via layer stacking improves model\nexpressiveness, the layer-by-layer sequential computation poses challenges to\nefficient scaling. Conversely, horizontal scaling through Mixture of Experts\n(MoE) achieves efficient scaling by activating a small subset of experts in\nparallel, but flat MoE layers may struggle to capture the hierarchical\nstructure inherent in recommendation tasks. To push the Return-On-Investment\n(ROI) boundary, we explore the complementary strengths of both directions and\npropose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic\nscaling in a parameter-efficient manner. Specifically, HiLoMoE employs\nlightweight rank-1 experts for parameter-efficient horizontal scaling, and\nstacks multiple MoE layers with hierarchical routing to enable combinatorially\ndiverse expert compositions. Unlike conventional stacking, HiLoMoE routes based\non prior layer scores rather than outputs, allowing all layers to execute in\nparallel. A principled three-stage training framework ensures stable\noptimization and expert diversity. Experiments on four public datasets show\nthat HiLoMoE achieving better performance-efficiency tradeoff, achieving an\naverage AUC improvement of 0.20\\% in AUC and 18.5\\% reduction in FLOPs compared\nto the non-MoE baseline."}
{"id": "2510.10433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10433", "abs": "https://arxiv.org/abs/2510.10433", "authors": ["Zixiang Xu", "Menghui Zhou", "Jun Qi", "Xuanhan Fan", "Yun Yang", "Po Yang"], "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression", "comment": null, "summary": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in\naging populations, posing a significant and escalating burden on global\nhealthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful\ncomputational paradigm for modeling longitudinal AD data, existing frameworks\ndo not account for the time-varying nature of feature correlations. To address\nthis limitation, we propose a novel MTL framework, named Feature Similarity\nLaplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel\nFeature Similarity Laplacian (FSL) penalty that explicitly models the\ntime-varying relationships between features. By simultaneously considering\ntemporal smoothness among tasks and the dynamic correlations among features,\nour model enhances both predictive accuracy and biological interpretability. To\nsolve the non-smooth optimization problem arising from our proposed penalty\nterms, we adopt the Alternating Direction Method of Multipliers (ADMM)\nalgorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework\nachieves state-of-the-art performance, outperforming various baseline methods.\nThe implementation source can be found at https://github.com/huatxxx/MTL-FSL."}
{"id": "2510.10446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10446", "abs": "https://arxiv.org/abs/2510.10446", "authors": ["Masoud Makrehchi"], "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation", "comment": "10 pages", "summary": "We analyze a reversed-supervision strategy that searches over labelings of a\nlarge unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The\nsearch space is \\(2^n\\), and the resulting complexity remains exponential even\nunder large constant-factor speedups (e.g., quantum or massively parallel\nhardware). Consequently, arbitrarily fast -- but not exponentially faster --\ncomputation does not obviate the need for informative labels or priors. In\npractice, the machine learning pipeline still requires an initial human\ncontribution: specifying the objective, defining classes, and providing a seed\nset of representative annotations that inject inductive bias and align models\nwith task semantics. Synthetic labels from generative AI can partially\nsubstitute provided their quality is human-grade and anchored by a\nhuman-specified objective, seed supervision, and validation. In this view,\ngenerative models function as \\emph{label amplifiers}, leveraging small\nhuman-curated cores via active, semi-supervised, and self-training loops, while\nhumans retain oversight for calibration, drift detection, and failure auditing.\nThus, extreme computational speed reduces wall-clock time but not the\nfundamental supervision needs of learning; initial human (or human-grade) input\nremains necessary to ground the system in the intended task."}
{"id": "2510.10451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10451", "abs": "https://arxiv.org/abs/2510.10451", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "comment": "21 pages, 7 figures", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors."}
{"id": "2510.10465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10465", "abs": "https://arxiv.org/abs/2510.10465", "authors": ["Yi Ren", "Xinjie Yu"], "title": "LightSAE: Parameter-Efficient and Heterogeneity-Aware Embedding for IoT Multivariate Time Series Forecasting", "comment": "Submitted to IEEE IoT-J", "summary": "Modern Internet of Things (IoT) systems generate massive, heterogeneous\nmultivariate time series data. Accurate Multivariate Time Series Forecasting\n(MTSF) of such data is critical for numerous applications. However, existing\nmethods almost universally employ a shared embedding layer that processes all\nchannels identically, creating a representational bottleneck that obscures\nvaluable channel-specific information. To address this challenge, we introduce\na Shared-Auxiliary Embedding (SAE) framework that decomposes the embedding into\na shared base component capturing common patterns and channel-specific\nauxiliary components modeling unique deviations. Within this decomposition, we\n\\rev{empirically observe} that the auxiliary components tend to exhibit\nlow-rank and clustering characteristics, a structural pattern that is\nsignificantly less apparent when using purely independent embeddings.\nConsequently, we design LightSAE, a parameter-efficient embedding module that\noperationalizes these observed characteristics through low-rank factorization\nand a shared, gated component pool. Extensive experiments across 9 IoT-related\ndatasets and 4 backbone architectures demonstrate LightSAE's effectiveness,\nachieving MSE improvements of up to 22.8\\% with only 4.0\\% parameter increase."}
{"id": "2510.10467", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10467", "abs": "https://arxiv.org/abs/2510.10467", "authors": ["Gunho Park", "Jeongin Bae", "Beomseok Kwon", "Byeongwook Kim", "Se Jung Kwon", "Dongsoo Lee"], "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs", "comment": null, "summary": "The deployment of large language models (LLMs) is increasingly constrained by\nmemory and latency bottlenecks, motivating the need for quantization techniques\nthat flexibly balance accuracy and efficiency. Recent work has introduced\nmulti-precision models, which enable inference at multiple precisions within a\nsingle model depending on runtime constraints. To support such flexibility,\nquantized weights are often stored as bit-planes, where hardware efficiency\nimproves when the compute operates directly at the bit-plane level and\nactivates only the precision required by each request. In this work, we present\nAnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded\nQuantization (BCQ) that supports direct bit-plane operations. By representing\nweights as binary bit-planes with corresponding scale factors, AnyBCQ enables\nbit-plane-level computation and maps naturally to accelerator-friendly,\nbit-parallel arithmetic. Our progressive precision expansion mechanism\nincrementally refines scaling factors while reusing previously assigned binary\ncodes, yielding monotonic improvements in accuracy as additional bits are\nenabled. We further co-design a specialized kernel that exploits the BCQ\nstructure to support dynamic per-request precision selection with negligible\noverhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly\nnarrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains\ncompetitive at higher precision, and achieves throughput gains of up to 3.0x\nover half precision and 1.2x over state-of-the-art multi-precision methods. By\naligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a\npractical foundation for multi-precision LLM deployment across diverse\nservice-level objectives."}
{"id": "2510.10477", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10477", "abs": "https://arxiv.org/abs/2510.10477", "authors": ["Zhijian Zhou", "Liuhua Peng", "Xunye Tian", "Feng Liu"], "title": "Anchor-based Maximum Discrepancy for Relative Similarity Testing", "comment": null, "summary": "The relative similarity testing aims to determine which of the distributions,\nP or Q, is closer to an anchor distribution U. Existing kernel-based approaches\noften test the relative similarity with a fixed kernel in a manually specified\nalternative hypothesis, e.g., Q is closer to U than P. Although kernel\nselection is known to be important to kernel-based testing methods, the\nmanually specified hypothesis poses a significant challenge for kernel\nselection in relative similarity testing: Once the hypothesis is specified\nfirst, we can always find a kernel such that the hypothesis is rejected. This\nchallenge makes relative similarity testing ill-defined when we want to select\na good kernel after the hypothesis is specified. In this paper, we cope with\nthis challenge via learning a proper hypothesis and a kernel simultaneously,\ninstead of learning a kernel after manually specifying the hypothesis. We\npropose an anchor-based maximum discrepancy (AMD), which defines the relative\nsimilarity as the maximum discrepancy between the distances of (U, P) and (U,\nQ) in a space of deep kernels. Based on AMD, our testing incorporates two\nphases. In Phase I, we estimate the AMD over the deep kernel space and infer\nthe potential hypothesis. In Phase II, we assess the statistical significance\nof the potential hypothesis, where we propose a unified testing framework to\nderive thresholds for tests over different possible hypotheses from Phase I.\nLastly, we validate our method theoretically and demonstrate its effectiveness\nvia extensive experiments on benchmark datasets. Codes are publicly available\nat: https://github.com/zhijianzhouml/AMD."}
{"id": "2510.10480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10480", "abs": "https://arxiv.org/abs/2510.10480", "authors": ["Zishen Zhang", "Xiangzhe Kong", "Wenbing Huang", "Yang Liu"], "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders", "comment": null, "summary": "Designing protein binders targeting specific sites, which requires to\ngenerate realistic and functional interaction patterns, is a fundamental\nchallenge in drug discovery. Current structure-based generative models are\nlimited in generating nterfaces with sufficient rationality and\ninterpretability. In this paper, we propose Retrieval-Augmented Diffusion for\nAligned interface (RADiAnce), a new framework that leverages known interfaces\nto guide the design of novel binders. By unifying retrieval and generation in a\nshared contrastive latent space, our model efficiently identifies relevant\ninterfaces for a given binding site and seamlessly integrates them through a\nconditional latent diffusion generator, enabling cross-domain interface\ntransfer. Extensive exeriments show that RADiAnce significantly outperforms\nbaseline models across multiple metrics, including binding affinity and\nrecovery of geometries and interactions. Additional experimental results\nvalidate cross-domain generalization, demonstrating that retrieving interfaces\nfrom diverse domains, such as peptides, antibodies, and protein fragments,\nenhances the generation performance of binders for other domains. Our work\nestablishes a new paradigm for protein binder design that successfully bridges\nretrieval-based knowledge and generative AI, opening new possibilities for drug\ndiscovery."}
{"id": "2510.10483", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10483", "abs": "https://arxiv.org/abs/2510.10483", "authors": ["Narayan S Iyer", "Bivas Bhaumik", "Ram S Iyer", "Satyasaran Changdar"], "title": "Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations", "comment": null, "summary": "Partial differential equations (PDEs) provide a mathematical foundation for\nsimulating and understanding intricate behaviors in both physical sciences and\nengineering. With the growing capabilities of deep learning, data$-$driven\napproaches like Physics$-$Informed Neural Networks (PINNs) have been developed,\noffering a mesh$-$free, analytic type framework for efficiently solving PDEs\nacross a wide range of applications. However, traditional PINNs often struggle\nwith challenges such as limited precision, slow training dynamics, lack of\nlabeled data availability, and inadequate handling of multi$-$physics\ninteractions. To overcome these challenging issues of PINNs, we proposed a\nGradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically\nintroduces a gradient based pseudo point self$-$learning algorithm for solving\nPDEs. We tested the proposed method on three different types of PDE problems\nfrom various fields, each representing distinct scenarios. The effectiveness of\nthe proposed method is evident, as the PINN approach for solving the Burgers$'$\nequation attains a mean square error (MSE) on the order of $10^{-3}$, while the\ndiffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after\n12,500 iterations, with no further improvement as the iterations increase. In\ncontrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease,\ndemonstrating better generalization and reaching an MSE on the order of\n$10^{-5}$ after 18,500 iterations. Furthermore, the results show that the\nproposed purely semi$-$supervised gST$-$PINN consistently outperforms the\nstandard PINN method in all cases, even when solution of the PDEs are\nunavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and\ncan be effectively applied in scenarios prone to low accuracy and convergence\nissues, particularly in the absence of labeled data."}
{"id": "2510.10503", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10503", "abs": "https://arxiv.org/abs/2510.10503", "authors": ["Kanishkha Jaisankar", "Sunidhi Tandel"], "title": "Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving", "comment": null, "summary": "Motion planning in complex scenarios is a core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto generate trajectories, while recent approaches leverage large language\nmodels (LLMs) for decision-making. However, it remains unclear whether LLMs\ntruly capture human driving logic. We propose Align2Act, a motion planning\nframework that transforms instruction-tuned LLMs into interpretable planners\naligned with human behavior. We derive structured driving instructions based on\nhuman reasoning patterns (e.g., anticipate hazards, yield at intersections) and\ntraffic rules (e.g., stop at red lights, maintain lane boundaries). Our\nAlign2ActChain module guides step-by-step reasoning to produce both an\ninterpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with\nLoRA on one million scenarios from the nuPlan dataset, our method achieves an\nopen-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and\n66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or\nopen-loop settings, we demonstrate improved planning quality and human-likeness\non the real-world nuPlan closed-loop benchmark. Ablation studies confirm that\nstructured reasoning significantly improves performance over baseline LLM\nplanners."}
{"id": "2510.10510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10510", "abs": "https://arxiv.org/abs/2510.10510", "authors": ["Subhodip Panda", "Dhruv Tarsadiya", "Shashwat Sourav", "Prathosh A. P", "Sai Praneeth Karimireddy"], "title": "f-INE: A Hypothesis Testing Framework for Estimating Influence under Training Randomness", "comment": null, "summary": "Influence estimation methods promise to explain and debug machine learning by\nestimating the impact of individual samples on the final model. Yet, existing\nmethods collapse under training randomness: the same example may appear\ncritical in one run and irrelevant in the next. Such instability undermines\ntheir use in data curation or cleanup since it is unclear if we indeed\ndeleted/kept the correct datapoints. To overcome this, we introduce\n*f-influence* -- a new influence estimation framework grounded in hypothesis\ntesting that explicitly accounts for training randomness, and establish\ndesirable properties that make it suitable for reliable influence estimation.\nWe also design a highly efficient algorithm **f**-**IN**fluence **E**stimation\n(**f-INE**) that computes f-influence **in a single training run**. Finally, we\nscale up f-INE to estimate influence of instruction tuning data on Llama-3.1-8B\nand show it can reliably detect poisoned samples that steer model opinions,\ndemonstrating its utility for data cleanup and attributing model behavior."}
{"id": "2510.10513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10513", "abs": "https://arxiv.org/abs/2510.10513", "authors": ["Md Ibrahim Shikder Mahin", "Md Shamsul Arefin", "Md Tanvir Hasan"], "title": "A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets", "comment": null, "summary": "Healthcare research and development face significant obstacles due to data\nscarcity and stringent privacy regulations, such as HIPAA and the GDPR,\nrestricting access to essential real-world medical data. These limitations\nimpede innovation, delay robust AI model creation, and hinder advancements in\npatient-centered care. Synthetic data generation offers a transformative\nsolution by producing artificial datasets that emulate real data statistics\nwhile safeguarding patient privacy. We introduce a novel hybrid framework for\nhigh-fidelity healthcare data synthesis integrating five augmentation methods:\nnoise injection, interpolation, Gaussian Mixture Model (GMM) sampling,\nConditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a\nreinforcement learning-based dynamic weight selection mechanism. Its key\ninnovations include advanced calibration techniques -- moment matching, full\nhistogram matching, soft and adaptive soft histogram matching, and iterative\nrefinement -- that align marginal distributions and preserve joint feature\ndependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and\nKhulna Medical College cardiology datasets, our calibrated hybrid achieves\nWasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around\n0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores\nsurpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%,\nconfirming robust privacy protection. Downstream classifiers trained on\nsynthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable\nto models trained on real data. This scalable, privacy-preserving approach\nmatches state-of-the-art methods, sets new benchmarks for joint-distribution\nfidelity in healthcare, and supports sensitive AI applications."}
{"id": "2510.10530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10530", "abs": "https://arxiv.org/abs/2510.10530", "authors": ["Hanbing Liu", "Huaze Tang", "Yanru Wu", "Yang Li", "Xiao-Ping Zhang"], "title": "Reinforced Domain Selection for Continuous Domain Adaptation", "comment": null, "summary": "Continuous Domain Adaptation (CDA) effectively bridges significant domain\nshifts by progressively adapting from the source domain through intermediate\ndomains to the target domain. However, selecting intermediate domains without\nexplicit metadata remains a substantial challenge that has not been extensively\nexplored in existing studies. To tackle this issue, we propose a novel\nframework that combines reinforcement learning with feature disentanglement to\nconduct domain path selection in an unsupervised CDA setting. Our approach\nintroduces an innovative unsupervised reward mechanism that leverages the\ndistances between latent domain embeddings to facilitate the identification of\noptimal transfer paths. Furthermore, by disentangling features, our method\nfacilitates the calculation of unsupervised rewards using domain-specific\nfeatures and promotes domain adaptation by aligning domain-invariant features.\nThis integrated strategy is designed to simultaneously optimize transfer paths\nand target task performance, enhancing the effectiveness of domain adaptation\nprocesses. Extensive empirical evaluations on datasets such as Rotated MNIST\nand ADNI demonstrate substantial improvements in prediction accuracy and domain\nselection efficiency, establishing our method's superiority over traditional\nCDA approaches."}
{"id": "2510.10541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10541", "abs": "https://arxiv.org/abs/2510.10541", "authors": ["Zihan Chen", "Yiming Zhang", "Hengguang Zhou", "Zenghui Ding", "Yining Sun", "Cho-Jui Hsieh"], "title": "Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?", "comment": null, "summary": "Current benchmarks are inadequate for evaluating progress in reinforcement\nlearning (RL) for large language models (LLMs).Despite recent benchmark gains\nreported for RL, we find that training on these benchmarks' training sets\nachieves nearly the same performance as training directly on the test sets,\nsuggesting that the benchmarks cannot reliably separate further progress.To\nstudy this phenomenon, we introduce a diagnostic suite and the Oracle\nPerformance Gap (OPG) metric that quantifies the performance difference between\ntraining on the train split versus the test split of a benchmark. We further\nanalyze this phenomenon with stress tests and find that, despite strong\nbenchmark scores, existing RL methods struggle to generalize across\ndistribution shifts, varying levels of difficulty, and counterfactual\nscenarios: shortcomings that current benchmarks fail to reveal.We conclude that\ncurrent benchmarks are insufficient for evaluating generalization and propose\nthree core principles for designing more faithful benchmarks: sufficient\ndifficulty, balanced evaluation, and distributional robustness."}
{"id": "2510.10544", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10544", "abs": "https://arxiv.org/abs/2510.10544", "authors": ["Abdelkrim Zitouni", "Mehdi Hennequin", "Juba Agoun", "Ryan Horache", "Nadia Kabachi", "Omar Rivasplata"], "title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies", "comment": null, "summary": "We derive a novel PAC-Bayesian generalization bound for reinforcement\nlearning that explicitly accounts for Markov dependencies in the data, through\nthe chain's mixing time. This contributes to overcoming challenges in obtaining\ngeneralization guarantees for reinforcement learning, where the sequential\nnature of data breaks the independence assumptions underlying classical bounds.\nOur bound provides non-vacuous certificates for modern off-policy algorithms\nlike Soft Actor-Critic. We demonstrate the bound's practical utility through\nPB-SAC, a novel algorithm that optimizes the bound during training to guide\nexploration. Experiments across continuous control tasks show that our approach\nprovides meaningful confidence certificates while maintaining competitive\nperformance."}
{"id": "2510.10558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10558", "abs": "https://arxiv.org/abs/2510.10558", "authors": ["Weiming Zhao", "Xulong Wang", "Jun Qi", "Yun Yang", "Po Yang"], "title": "Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors", "comment": null, "summary": "Severity assessment of Parkinson's disease (PD) using wearable sensors offers\nan effective, objective basis for clinical management. However, general-purpose\ntime series models often lack pathological specificity in feature extraction,\nmaking it difficult to capture subtle signals highly correlated with\nPD.Furthermore, the temporal sparsity of PD symptoms causes key diagnostic\nfeatures to be easily \"diluted\" by traditional aggregation methods, further\ncomplicating assessment. To address these issues, we propose the Multi-scale\nFrequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances\nfeature specificity through a frequency decomposition module guided by medical\nprior knowledge. Furthermore, by introducing an attention-based multi-instance\nlearning (MIL) framework, the model can adaptively focus on the most\ndiagnostically valuable sparse segments.We comprehensively validated MFAM on\nboth the public PADS dataset for PD versus differential diagnosis (DD) binary\nclassification and a private dataset for four-class severity assessment.\nExperimental results demonstrate that MFAM outperforms general-purpose time\nseries models in handling complex clinical time series with specificity,\nproviding a promising solution for automated assessment of PD severity."}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness."}
{"id": "2510.10572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10572", "abs": "https://arxiv.org/abs/2510.10572", "authors": ["Byeongchan Lee"], "title": "Understanding Self-supervised Contrastive Learning through Supervised Objectives", "comment": "Accepted at TMLR 2025", "summary": "Self-supervised representation learning has achieved impressive empirical\nsuccess, yet its theoretical understanding remains limited. In this work, we\nprovide a theoretical perspective by formulating self-supervised representation\nlearning as an approximation to supervised representation learning objectives.\nBased on this formulation, we derive a loss function closely related to popular\ncontrastive losses such as InfoNCE, offering insight into their underlying\nprinciples. Our derivation naturally introduces the concepts of prototype\nrepresentation bias and a balanced contrastive loss, which help explain and\nimprove the behavior of self-supervised learning algorithms. We further show\nhow components of our theoretical framework correspond to established practices\nin contrastive learning. Finally, we empirically validate the effect of\nbalancing positive and negative pair interactions. All theoretical proofs are\nprovided in the appendix, and our code is included in the supplementary\nmaterial."}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers."}
{"id": "2510.10604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10604", "abs": "https://arxiv.org/abs/2510.10604", "authors": ["Yuheng Chen", "Dingkun Liu", "Xinyao Yang", "Xinping Xu", "Baicheng Chen", "Dongrui Wu"], "title": "FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation", "comment": null, "summary": "Brain-computer interfaces (BCIs) provide potential for applications ranging\nfrom medical rehabilitation to cognitive state assessment by establishing\ndirect communication pathways between the brain and external devices via\nelectroencephalography (EEG). However, EEG-based BCIs are severely constrained\nby data scarcity and significant inter-subject variability, which hinder the\ngeneralization and applicability of EEG decoding models in practical settings.\nTo address these challenges, we propose FusionGen, a novel EEG data generation\nframework based on disentangled representation learning and feature fusion. By\nintegrating features across trials through a feature matching fusion module and\ncombining them with a lightweight feature extraction and reconstruction\npipeline, FusionGen ensures both data diversity and trainability under limited\ndata constraints. Extensive experiments on multiple publicly available EEG\ndatasets demonstrate that FusionGen significantly outperforms existing\naugmentation techniques, yielding notable improvements in classification\naccuracy."}
{"id": "2510.10605", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10605", "abs": "https://arxiv.org/abs/2510.10605", "authors": ["MohammadHossein Bateni", "Hossein Esfandiari", "Samira HosseinGhorban", "Alireza Mirrokni", "Radin Shahdaei"], "title": "Budget Allocation for Unknown Value Functions in a Lipschitz Space", "comment": null, "summary": "Building learning models frequently requires evaluating numerous intermediate\nmodels. Examples include models considered during feature selection, model\nstructure search, and parameter tunings. The evaluation of an intermediate\nmodel influences subsequent model exploration decisions. Although prior\nknowledge can provide initial quality estimates, true performance is only\nrevealed after evaluation. In this work, we address the challenge of optimally\nallocating a bounded budget to explore the space of intermediate models. We\nformalize this as a general budget allocation problem over unknown-value\nfunctions within a Lipschitz space."}
{"id": "2510.10617", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10617", "abs": "https://arxiv.org/abs/2510.10617", "authors": ["Bahadur Yadav", "Sanjay Kumar Mohanty"], "title": "Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction", "comment": null, "summary": "Forecasting stock prices remains challenging due to the volatile and\nnon-linear nature of financial markets. Despite the promise of deep learning,\nissues such as mode collapse, unstable training, and difficulty in capturing\ntemporal and feature level correlations have limited the applications of GANs\nin this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that\nstrikes a balance between expressive power and simplicity. The model introduces\nkey innovations such as a temporal decoder with residual connections for\nprecise reconstruction, conditioning on static and dynamic covariates for\ncontextual learning, and a windowing mechanism to capture temporal dynamics.\nHere, the generator uses a dense encoder-decoder framework with residual GRU\nblocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN\nachieves superior forecasting accuracy and training stability, even in volatile\nmarkets. It consistently outperforms traditional GAN variants in forecasting\naccuracy and convergence stability under market conditions."}
{"id": "2510.10621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10621", "abs": "https://arxiv.org/abs/2510.10621", "authors": ["Hanbing Liu", "Yanru Wu", "Yang Li", "Ercan E. Kuruoglu", "Xuan Zhang"], "title": "SDG-L: A Semiparametric Deep Gaussian Process based Framework for Battery Capacity Prediction", "comment": null, "summary": "Lithium-ion batteries are becoming increasingly omnipresent in energy supply.\nHowever, the durability of energy storage using lithium-ion batteries is\nthreatened by their dropping capacity with the growing number of\ncharging/discharging cycles. An accurate capacity prediction is the key to\nensure system efficiency and reliability, where the exploitation of battery\nstate information in each cycle has been largely undervalued. In this paper, we\npropose a semiparametric deep Gaussian process regression framework named SDG-L\nto give predictions based on the modeling of time series battery state data. By\nintroducing an LSTM feature extractor, the SDG-L is specially designed to\nbetter utilize the auxiliary profiling information during charging/discharging\nprocess. In experimental studies based on NASA dataset, our proposed method\nobtains an average test MSE error of 1.2%. We also show that SDG-L achieves\nbetter performance compared to existing works and validate the framework using\nablation studies."}
{"id": "2510.10625", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10625", "abs": "https://arxiv.org/abs/2510.10625", "authors": ["Yuval Golbari", "Navve Wasserman", "Gal Vardi", "Michal Irani"], "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "comment": null, "summary": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."}
{"id": "2510.10634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10634", "abs": "https://arxiv.org/abs/2510.10634", "authors": ["Shaoning Li", "Le Zhuo", "Yusong Wang", "Mingyu Li", "Xinheng He", "Fandi Wu", "Hongsheng Li", "Pheng-Ann Heng"], "title": "ProteinAE: Protein Diffusion Autoencoders for Structure Encoding", "comment": null, "summary": "Developing effective representations of protein structures is essential for\nadvancing protein science, particularly for protein generative modeling.\nCurrent approaches often grapple with the complexities of the SE(3) manifold,\nrely on discrete tokenization, or the need for multiple training objectives,\nall of which can hinder the model optimization and generalization. We introduce\nProteinAE, a novel and streamlined protein diffusion autoencoder designed to\novercome these challenges by directly mapping protein backbone coordinates from\nE(3) into a continuous, compact latent space. ProteinAE employs a\nnon-equivariant Diffusion Transformer with a bottleneck design for efficient\ncompression and is trained end-to-end with a single flow matching objective,\nsubstantially simplifying the optimization pipeline. We demonstrate that\nProteinAE achieves state-of-the-art reconstruction quality, outperforming\nexisting autoencoders. The resulting latent space serves as a powerful\nfoundation for a latent diffusion model that bypasses the need for explicit\nequivariance. This enables efficient, high-quality structure generation that is\ncompetitive with leading structure-based approaches and significantly\noutperforms prior latent-based methods. Code is available at\nhttps://github.com/OnlyLoveKFC/ProteinAE_v1."}
{"id": "2510.10645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10645", "abs": "https://arxiv.org/abs/2510.10645", "authors": ["Michal Sadowski", "Maria Wyrzykowska", "Lukasz Sztukiewicz", "Tadija Radusinović", "Jan Rzymkowski", "Paweł Włodarczyk-Pruszyński", "Mikołaj Sacha", "Piotr Kozakowski", "Ruard van Workum", "Stanislaw Kamil Jastrzebski"], "title": "Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers", "comment": null, "summary": "Retrosynthesis is one of the domains transformed by the rise of generative\nmodels, and it is one where the problem of nonsensical or erroneous outputs\n(hallucinations) is particularly insidious: reliable assessment of synthetic\nplans is time-consuming, with automatic methods lacking. In this work, we\npresent RetroTrim, a retrosynthesis system that successfully avoids nonsensical\nplans on a set of challenging drug-like targets. Compared to common baselines\nin the field, our system is not only the sole method that succeeds in filtering\nout hallucinated reactions, but it also results in the highest number of\nhigh-quality paths overall. The key insight behind RetroTrim is the combination\nof diverse reaction scoring strategies, based on machine learning models and\nexisting chemical databases. We show that our scoring strategies capture\ndifferent classes of hallucinations by analyzing them on a dataset of labeled\nretrosynthetic intermediates. To measure the performance of retrosynthesis\nsystems, we propose a novel evaluation protocol for reactions and synthetic\npaths based on a structured review by expert chemists. Using this protocol, we\ncompare systems on a set of 32 novel targets, curated to reflect recent trends\nin drug structures. While the insights behind our methodology are broadly\napplicable to retrosynthesis, our focus is on targets in the drug-like domain.\nBy releasing our benchmark targets and the details of our evaluation protocol,\nwe hope to inspire further research into reliable retrosynthesis."}
{"id": "2510.10694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10694", "abs": "https://arxiv.org/abs/2510.10694", "authors": ["Ying-Kuan Tsai", "Vispi Karkaria", "Yi-Ping Chen", "Wei Chen"], "title": "Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning", "comment": "to be published in Journal of Mechanical Design", "summary": "Control Co-Design (CCD) integrates physical and control system design to\nimprove the performance of dynamic and autonomous systems. Despite advances in\nuncertainty-aware CCD methods, real-world uncertainties remain highly\nunpredictable. Multi-generation design addresses this challenge by considering\nthe full lifecycle of a product: data collected from each generation informs\nthe design of subsequent generations, enabling progressive improvements in\nrobustness and efficiency. Digital Twin (DT) technology further strengthens\nthis paradigm by creating virtual representations that evolve over the\nlifecycle through real-time sensing, model updating, and adaptive\nre-optimization. This paper presents a DT-enabled CCD framework that integrates\nDeep Reinforcement Learning (DRL) to jointly optimize physical design and\ncontroller. DRL accelerates real-time decision-making by allowing controllers\nto continuously learn from data and adapt to uncertain environments. Extending\nthis approach, the framework employs a multi-generation paradigm, where each\ncycle of deployment, operation, and redesign uses collected data to refine DT\nmodels, improve uncertainty quantification through quantile regression, and\ninform next-generation designs of both physical components and controllers. The\nframework is demonstrated on an active suspension system, where DT-enabled\nlearning from road conditions and driving behaviors yields smoother and more\nstable control trajectories. Results show that the method significantly\nenhances dynamic performance, robustness, and efficiency. Contributions of this\nwork include: (1) extending CCD into a lifecycle-oriented multi-generation\nframework, (2) leveraging DTs for continuous model updating and informed\ndesign, and (3) employing DRL to accelerate adaptive real-time decision-making."}
{"id": "2510.10695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10695", "abs": "https://arxiv.org/abs/2510.10695", "authors": ["Long Chen", "Huixin Bai", "Mingxin Wang", "Xiaohua Huang", "Ying Liu", "Jie Zhao", "Ziyu Guan"], "title": "Stock Prediction via a Dual Relation Fusion Network incorporating Static and Dynamic Relations", "comment": "11 pages", "summary": "Accurate modeling of inter-stock relationships is critical for stock price\nforecasting. However, existing methods predominantly focus on single-state\nrelationships, neglecting the essential complementarity between dynamic and\nstatic inter-stock relations. To solve this problem, we propose a Dual Relation\nFusion Network (DRFN) to capture the long-term relative stability of stock\nrelation structures while retaining the flexibility to respond to sudden market\nshifts. Our approach features a novel relative static relation component that\nmodels time-varying long-term patterns and incorporates overnight informational\ninfluences. We capture dynamic inter-stock relationships through distance-aware\nmechanisms, while evolving long-term structures via recurrent fusion of dynamic\nrelations from the prior day with the pre-defined static relations. Experiments\ndemonstrate that our method significantly outperforms the baselines across\ndifferent markets, with high sensitivity to the co-movement of relational\nstrength and stock price."}
{"id": "2510.10702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10702", "abs": "https://arxiv.org/abs/2510.10702", "authors": ["Usman Gani Joy", "Shahadat kabir", "Tasnim Niger"], "title": "Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh", "comment": null, "summary": "Accurate climate forecasting is vital for Bangladesh, a region highly\nsusceptible to climate change impacts on temperature and rainfall. Existing\nmodels often struggle to capture long-range dependencies and complex temporal\npatterns in climate data. This study introduces an advanced Long Short-Term\nMemory (LSTM) model integrated with an attention mechanism to enhance the\nprediction of temperature and rainfall dynamics. Utilizing comprehensive\ndatasets from 1901-2023, sourced from NASA's POWER Project for temperature and\nthe Humanitarian Data Exchange for rainfall, the model effectively captures\nseasonal and long-term trends. It outperforms baseline models, including\nXGBoost, Simple LSTM, and GRU, achieving a test MSE of 0.2411 (normalized\nunits), MAE of 0.3860 degrees C, R^2 of 0.9834, and NRMSE of 0.0370 for\ntemperature, and MSE of 1283.67 mm^2, MAE of 22.91 mm, R^2 of 0.9639, and NRMSE\nof 0.0354 for rainfall on monthly forecasts. The model demonstrates improved\nrobustness with only a 20 percent increase in MSE under simulated climate\ntrends (compared to an approximately 2.2-fold increase in baseline models\nwithout trend features) and a 50 percent degradation under regional variations\n(compared to an approximately 4.8-fold increase in baseline models without\nenhancements). These results highlight the model's ability to improve\nforecasting precision and offer potential insights into the physical processes\ngoverning climate variability in Bangladesh, supporting applications in\nclimate-sensitive sectors."}
{"id": "2510.10706", "categories": ["cs.LG", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.10706", "abs": "https://arxiv.org/abs/2510.10706", "authors": ["Mamoona Ghafoor", "Tatsuya Akutsu"], "title": "Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance", "comment": null, "summary": "The generation of trees with a specified tree edit distance has significant\napplications across various fields, including computational biology, structured\ndata analysis, and image processing. Recently, generative networks have been\nincreasingly employed to synthesize new data that closely resembles the\noriginal datasets. However, the appropriate size and depth of generative\nnetworks required to generate data with a specified tree edit distance remain\nunclear. In this paper, we theoretically establish the existence and\nconstruction of generative networks capable of producing trees similar to a\ngiven tree with respect to the tree edit distance. Specifically, for a given\nrooted, ordered, and vertex-labeled tree T of size n + 1 with labels from an\nalphabet \\Sigma, and a non-negative integer d, we prove that all rooted,\nordered, and vertex-labeled trees over \\Sigma with tree edit distance at most d\nfrom T can be generated using a ReLU-based generative network with size O(n^3 )\nand constant depth. The proposed networks were implemented and evaluated for\ngenerating trees with up to 21 nodes. Due to their deterministic architecture,\nthe networks successfully generated all valid trees within the specified tree\nedit distance. In contrast, state-of-the-art graph generative models GraphRNN\nand GraphGDP, which rely on non-deterministic mechanisms, produced\nsignificantly fewer valid trees, achieving validation rates of only up to 35%\nand 48%, respectively. These findings provide a theoretical foundation towards\nconstruction of compact generative models and open new directions for exact and\nvalid tree-structured data generation. An implementation of the proposed\nnetworks is available at https://github.com/MGANN-KU/TreeGen_ReLUNetworks."}
{"id": "2510.10730", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10730", "abs": "https://arxiv.org/abs/2510.10730", "authors": ["Jiazheng Sun", "Weixin Wang", "Pan Xu"], "title": "Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits", "comment": "40 pages, 1 figure", "summary": "We provide a unified algorithmic framework for ensemble sampling in nonlinear\ncontextual bandits and develop corresponding regret bounds for two most common\nnonlinear contextual bandit settings: Generalized Linear Ensemble Sampling\n(\\texttt{GLM-ES}) for generalized linear bandits and Neural Ensemble Sampling\n(\\texttt{Neural-ES}) for neural contextual bandits. Both methods maintain\nmultiple estimators for the reward model parameters via maximum likelihood\nestimation on randomly perturbed data. We prove high-probability frequentist\nregret bounds of $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ for \\texttt{GLM-ES}\nand $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ for \\texttt{Neural-ES}, where $d$ is\nthe dimension of feature vectors, $\\widetilde{d}$ is the effective dimension of\na neural tangent kernel matrix, and $T$ is the number of rounds. These regret\nbounds match the state-of-the-art results of randomized exploration algorithms\nin nonlinear contextual bandit settings. In the theoretical analysis, we\nintroduce techniques that address challenges specific to nonlinear models.\nPractically, we remove fixed-time horizon assumptions by developing anytime\nversions of our algorithms, suitable when $T$ is unknown. Finally, we\nempirically evaluate \\texttt{GLM-ES}, \\texttt{Neural-ES}, and their anytime\nvariants, demonstrating strong performance. Overall, our results establish\nensemble sampling as a provable and practical randomized exploration approach\nfor nonlinear contextual bandits."}
{"id": "2510.10739", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10739", "abs": "https://arxiv.org/abs/2510.10739", "authors": ["Shivani Shukla", "Himanshu Joshi"], "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications", "comment": "Peer-reviewed and accepted to the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) DynaFront 2025 Workshop\n  (https://sites.google.com/view/dynafrontneurips25)", "summary": "We introduce a general stochastic differential equation framework for\nmodelling multiobjective optimization dynamics in iterative Large Language\nModel (LLM) interactions. Our framework captures the inherent stochasticity of\nLLM responses through explicit diffusion terms and reveals systematic\ninterference patterns between competing objectives via an interference matrix\nformulation. We validate our theoretical framework using iterative code\ngeneration as a proof-of-concept application, analyzing 400 sessions across\nsecurity, efficiency, and functionality objectives. Our results demonstrate\nstrategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,\nand predictive accuracy achieving R2 = 0.74 for balanced approaches. This work\nproposes the feasibility of dynamical systems analysis for multi-objective LLM\ninteractions, with code generation serving as an initial validation domain."}
{"id": "2510.10764", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10764", "abs": "https://arxiv.org/abs/2510.10764", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior Efficiency", "comment": "6 pages, 3 figures, 1 table", "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively."}
{"id": "2510.10767", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10767", "abs": "https://arxiv.org/abs/2510.10767", "authors": ["Jiayuan Sheng", "Hanyang Zhao", "Haoxian Chen", "David D. Yao", "Wenpin Tang"], "title": "Understanding Sampler Stochasticity in Training Diffusion Models for RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nfine-tune diffusion models, but a key challenge arises from the mismatch\nbetween stochastic samplers used during training and deterministic samplers\nused during inference. In practice, models are fine-tuned using stochastic SDE\nsamplers to encourage exploration, while inference typically relies on\ndeterministic ODE samplers for efficiency and stability. This discrepancy\ninduces a reward gap, raising concerns about whether high-quality outputs can\nbe expected during inference. In this paper, we theoretically characterize this\nreward gap and provide non-vacuous bounds for general diffusion models, along\nwith sharper convergence rates for Variance Exploding (VE) and Variance\nPreserving (VP) Gaussian models. Methodologically, we adopt the generalized\ndenoising diffusion implicit models (gDDIM) framework to support arbitrarily\nhigh levels of stochasticity, preserving data marginals throughout.\nEmpirically, our findings through large-scale experiments on text-to-image\nmodels using denoising diffusion policy optimization (DDPO) and mixed group\nrelative policy optimization (MixGRPO) validate that reward gaps consistently\nnarrow over training, and ODE sampling quality improves when models are updated\nusing higher-stochasticity SDE training."}
{"id": "2510.10775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10775", "abs": "https://arxiv.org/abs/2510.10775", "authors": ["Amber Li", "Aruzhan Abil", "Juno Marques Oda"], "title": "Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction", "comment": null, "summary": "In financial markets, Graph Neural Networks have been successfully applied to\nmodeling relational data, effectively capturing nonlinear inter-stock\ndependencies. Yet, existing models often fail to efficiently propagate messages\nduring macroeconomic shocks. In this paper, we propose OmniGNN, an\nattention-based multi-relational dynamic GNN that integrates macroeconomic\ncontext via heterogeneous node and edge types for robust message passing.\nCentral to OmniGNN is a sector node acting as a global intermediary, enabling\nrapid shock propagation across the graph without relying on long-range\nmulti-hop diffusion. The model leverages Graph Attention Networks (GAT) to\nweigh neighbor contributions and employs Transformers to capture temporal\ndynamics across multiplex relations. Experiments show that OmniGNN outperforms\nexisting stock prediction models on public datasets, particularly demonstrating\nstrong robustness during the COVID-19 period."}
{"id": "2510.10777", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10777", "abs": "https://arxiv.org/abs/2510.10777", "authors": ["Andrey Veprikov", "Arman Bolatov", "Samuel Horváth", "Aleksandr Beznosikov", "Martin Takáč", "Slavomir Hanzely"], "title": "Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods", "comment": "22 pages, 2 figures, 8 tables", "summary": "Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent"}
{"id": "2510.10790", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10790", "abs": "https://arxiv.org/abs/2510.10790", "authors": ["Zhongju Yuan", "Geraint Wiggins", "Dick Botteldooren"], "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics", "comment": null, "summary": "Today's deep learning architectures are primarily based on perceptron models,\nwhich do not capture the oscillatory dynamics characteristic of biological\nneurons. Although oscillatory systems have recently gained attention for their\ncloser resemblance to neural behavior, they still fall short of modeling the\nintricate spatio-temporal interactions observed in natural neural circuits. In\nthis paper, we propose a bio-inspired oscillatory state system (BioOSS)\ndesigned to emulate the wave-like propagation dynamics critical to neural\nprocessing, particularly in the prefrontal cortex (PFC), where complex activity\npatterns emerge. BioOSS comprises two interacting populations of neurons: p\nneurons, which represent simplified membrane-potential-like units inspired by\npyramidal cells in cortical columns, and o neurons, which govern propagation\nvelocities and modulate the lateral spread of activity. Through local\ninteractions, these neurons produce wave-like propagation patterns. The model\nincorporates trainable parameters for damping and propagation speed, enabling\nflexible adaptation to task-specific spatio-temporal structures. We evaluate\nBioOSS on both synthetic and real-world tasks, demonstrating superior\nperformance and enhanced interpretability compared to alternative\narchitectures."}
{"id": "2510.10799", "categories": ["cs.LG", "physics.ao-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.10799", "abs": "https://arxiv.org/abs/2510.10799", "authors": ["Wanshu Nie", "Sujay V. Kumar", "Junyu Chen", "Long Zhao", "Olya Skulovich", "Jinwoong Yoo", "Justin Pflug", "Shahryar Khalique Ahmad", "Goutam Konapala"], "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage", "comment": null, "summary": "Recent advances in machine learning such as Long Short-Term Memory (LSTM)\nmodels and Transformers have been widely adopted in hydrological applications,\ndemonstrating impressive performance amongst deep learning models and\noutperforming physical models in various tasks. However, their superiority in\npredicting land surface states such as terrestrial water storage (TWS) that are\ndominated by many factors such as natural variability and human driven\nmodifications remains unclear. Here, using the open-access, globally\nrepresentative HydroGlobe dataset - comprising a baseline version derived\nsolely from a land surface model simulation and an advanced version\nincorporating multi-source remote sensing data assimilation - we show that\nlinear regression is a robust benchmark, outperforming the more complex LSTM\nand Temporal Fusion Transformer for TWS prediction. Our findings highlight the\nimportance of including traditional statistical models as benchmarks when\ndeveloping and evaluating deep learning models. Additionally, we emphasize the\ncritical need to establish globally representative benchmark datasets that\ncapture the combined impact of natural variability and human interventions."}
{"id": "2510.10803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10803", "abs": "https://arxiv.org/abs/2510.10803", "authors": ["Javier García-Sigüenza", "Mirco Nanni", "Faraón Llorens-Largo", "José F. Vicent"], "title": "PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning", "comment": null, "summary": "This work addresses the challenge of using a deep learning model to prune\ngraphs and the ability of this method to integrate explainability into\nspatio-temporal problems through a new approach. Instead of applying\nexplainability to the model's behavior, we seek to gain a better understanding\nof the problem itself. To this end, we propose a novel model that integrates an\noptimized pruning mechanism capable of removing nodes from the graph during the\ntraining process, rather than doing so as a separate procedure. This\nintegration allows the architecture to learn how to minimize prediction error\nwhile selecting the most relevant nodes. Thus, during training, the model\nsearches for the most relevant subset of nodes, obtaining the most important\nelements of the problem, facilitating its analysis. To evaluate the proposed\napproach, we used several widely used traffic datasets, comparing the accuracy\nobtained by pruning with the model and with other methods. The experiments\ndemonstrate that our method is capable of retaining a greater amount of\ninformation as the graph reduces in size compared to the other methods used.\nThese results highlight the potential of pruning as a tool for developing\nmodels capable of simplifying spatio-temporal problems, thereby obtaining their\nmost important elements."}
{"id": "2510.10807", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.10807", "abs": "https://arxiv.org/abs/2510.10807", "authors": ["Ali Atiah Alzahrani"], "title": "Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation", "comment": "Code available at: https://github.com/AliAtiah/MARCD", "summary": "We study whether regime-conditioned generative scenarios, coupled with a\nconvex CVaR allocator, improve portfolio decisions under regime shifts. We\nintroduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers\nlatent regimes via a Gaussian HMM, (ii) trains a diffusion model with a\ntail-weighted objective and a regime-specialized mixture-of-experts (MoE)\ndenoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios\ninto a turnover-aware CVaR epigraph quadratic program with explicit governance.\nIn strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD\noutperforms standard allocators and improves calibration relative to popular\ngenerators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains\nSharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent\nreduction, at comparable turnover; stationary block-bootstrap intervals\nindicate the Sharpe uplift is significant at 5 percent. We provide theory\nlinking tail-weighted diffusion to spectral-risk control of the\ndecision-relevant CVaR gap, oracle/consistency results for the regime-MoE\ndenoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD\noffers a reproducible bridge from tail-faithful scenario modeling to governed\nportfolio decisions with materially improved drawdown control."}
{"id": "2510.10810", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10810", "abs": "https://arxiv.org/abs/2510.10810", "authors": ["Omar Islam Laskar", "Fatemeh Ramezani Khozestani", "Ishika Nankani", "Sohrab Namazi Nia", "Senjuti Basu Roy", "Kaustubh Beedkar"], "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems", "comment": "Accepted at SIGMOD 2026", "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches."}
{"id": "2510.10849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10849", "abs": "https://arxiv.org/abs/2510.10849", "authors": ["Donald Loveland", "Yao-An Yang", "Danai Koutra"], "title": "Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion", "comment": null, "summary": "Learning on text-attributed graphs has motivated the use of Large Language\nModels (LLMs) for graph learning. However, most fusion strategies are applied\nuniformly across all nodes and attain only small overall performance gains. We\nargue this result stems from aggregate metrics that obscure when LLMs provide\nbenefit, inhibiting actionable signals for new strategies. In this work, we\nreframe LLM-GNN fusion around nodes where GNNs typically falter. We first show\nthat performance can significantly differ between GNNs and LLMs, with each\nexcelling on distinct structural patterns, such as local homophily. To leverage\nthis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- and\nContext-aware Embeddings), a framework that invokes an LLM to refine a GNN's\nprediction. GLANCE employs a lightweight router that, given inexpensive\nper-node signals, decides whether to query the LLM. Since the LLM calls are\nnon-differentiable, the router is trained with an advantage-based objective\nthat compares the utility of querying the LLM against relying solely on the\nGNN. Across multiple benchmarks, GLANCE achieves the best performance balance\nacross node subgroups, achieving significant gains on heterophilous nodes (up\nto $+13\\%$) while simultaneously achieving top overall performance. Our\nfindings highlight the value of adaptive, node-aware GNN-LLM architectures,\nwhere selectively invoking the LLM enables scalable deployment on large graphs\nwithout incurring high computational costs."}
{"id": "2510.10854", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10854", "abs": "https://arxiv.org/abs/2510.10854", "authors": ["Aadithya Srikanth", "Mudit Gaur", "Vaneet Aggarwal"], "title": "Discrete State Diffusion Models: A Sample Complexity Perspective", "comment": null, "summary": "Diffusion models have demonstrated remarkable performance in generating\nhigh-dimensional samples across domains such as vision, language, and the\nsciences. Although continuous-state diffusion models have been extensively\nstudied both empirically and theoretically, discrete-state diffusion models,\nessential for applications involving text, sequences, and combinatorial\nstructures, remain significantly less understood from a theoretical standpoint.\nIn particular, all existing analyses of discrete-state models assume score\nestimation error bounds without studying sample complexity results. In this\nwork, we present a principled theoretical framework for discrete-state\ndiffusion, providing the first sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$. Our structured decomposition of the\nscore estimation error into statistical, approximation, optimization, and\nclipping components offers critical insights into how discrete-state models can\nbe trained efficiently. This analysis addresses a fundamental gap in the\nliterature and establishes the theoretical tractability and practical relevance\nof discrete-state diffusion models."}
{"id": "2510.10862", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10862", "abs": "https://arxiv.org/abs/2510.10862", "authors": ["Samuel Yuan", "Divyanshu Saxena", "Jiayi Chen", "Nihal Sharma", "Aditya Akella"], "title": "A Joint Learning Approach to Hardware Caching and Prefetching", "comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."}
{"id": "2510.10864", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10864", "abs": "https://arxiv.org/abs/2510.10864", "authors": ["Shuaicheng Zhang", "Haohui Wang", "Junhong Lin", "Xiaojie Guo", "Yada Zhu", "Si Zhang", "Dongqi Fu", "Dawei Zhou"], "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations", "comment": null, "summary": "Graph heterophily, where connected nodes have different labels, has attracted\nsignificant interest recently. Most existing works adopt a simplified approach\n- using low-pass filters for homophilic graphs and high-pass filters for\nheterophilic graphs. However, we discover that the relationship between graph\nheterophily and spectral filters is more complex - the optimal filter response\nvaries across frequency components and does not follow a strict monotonic\ncorrelation with heterophily degree. This finding challenges conventional fixed\nfilter designs and suggests the need for adaptive filtering to preserve\nexpressiveness in graph embeddings. Formally, natural questions arise: Given a\nheterophilic graph G, how and to what extent will the varying heterophily\ndegree of G affect the performance of GNNs? How can we design adaptive filters\nto fit those varying heterophilic connections? Our theoretical analysis reveals\nthat the average frequency response of GNNs and graph heterophily degree do not\nfollow a strict monotonic correlation, necessitating adaptive graph filters to\nguarantee good generalization performance. Hence, we propose [METHOD NAME], a\nsimple yet powerful GNN, which extracts information across the heterophily\nspectrum and combines salient representations through adaptive mixing. [METHOD\nNAME]'s superior performance achieves up to 9.2% accuracy improvement over\nleading baselines across homophilic and heterophilic graphs."}
{"id": "2510.10902", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10902", "abs": "https://arxiv.org/abs/2510.10902", "authors": ["Mahmoud Abdelghafar", "Maryam Aliakbarpour", "Chris Jermaine"], "title": "Quantifying Information Disclosure During Gradient Descent Using Gradient Uniqueness", "comment": null, "summary": "Disclosing private information via publication of a machine learning model is\noften a concern. Intuitively, publishing a learned model should be less risky\nthan publishing a dataset. But how much risk is there? In this paper, we\npresent a principled disclosure metric called \\emph{gradient uniqueness} that\nis derived from an upper bound on the amount of information disclosure from\npublishing a learned model. Gradient uniqueness provides an intuitive way to\nperform privacy auditing. The mathematical derivation of gradient uniqueness is\ngeneral, and does not make any assumption on the model architecture, dataset\ntype, or the strategy of an attacker. We examine a simple defense based on\nmonitoring gradient uniqueness, and find that it achieves privacy comparable to\nclassical methods such as DP-SGD, while being substantially better in terms of\n(utility) testing accuracy."}
{"id": "2510.10915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10915", "abs": "https://arxiv.org/abs/2510.10915", "authors": ["Hanchang Cheng", "Weimin Mu", "Fan Liu", "Weilin Zhu", "Can Ma"], "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD."}
{"id": "2510.10925", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10925", "abs": "https://arxiv.org/abs/2510.10925", "authors": ["Hengyuan Zhang", "Shiping Yang", "Xiao Liang", "Chenming Shang", "Yuxuan Jiang", "Chaofan Tao", "Jing Xiong", "Hayden Kwok-Hay So", "Ruobing Xie", "Angel X. Chang", "Ngai Wong"], "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation", "comment": "19 pages, 10 figures", "summary": "Training student models on synthetic data generated by strong teacher models\nis a promising way to distilling the capabilities of teachers. However, recent\nstudies show that stronger models are not always optimal teachers, revealing a\nmismatch between teacher outputs and student learnability. To address this\nissue, we propose PerSyn (Personalized data Synthesis), a novel synthesis\nstrategy that operates under a new ``Route then Generate'' paradigm to create\ndata tailored to each student model, enabling it to learn more effectively.\nSpecifically, PerSyn first assigns each prompt to its optimal teacher via a\nquery-level router that jointly considers student learnability and teacher\nresponse quality. Each teacher then synthesizes data only for its assigned\nprompts, making the process more efficient than the conventional ``Generate\nthen Select'' paradigm, where all teachers must generate parallel responses for\nthe entire prompt set before constructing the final dataset. Extensive\nexperiments across different model families and scales demonstrate that PerSyn\nconsistently achieves superior or comparable performance to all baselines in\ninstruct tuning and math reasoning settings. Further analysis verifies the\neffectiveness of PerSyn and offers extra insights to propel future research."}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."}
{"id": "2510.10938", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10938", "abs": "https://arxiv.org/abs/2510.10938", "authors": ["Yuda Bi", "Ying Zhu", "Vince D Calhoun"], "title": "Redundancy as a Structural Information Principle for Learning and Generalization", "comment": null, "summary": "We present a theoretical framework that extends classical information theory\nto finite and structured systems by redefining redundancy as a fundamental\nproperty of information organization rather than inefficiency. In this\nframework, redundancy is expressed as a general family of informational\ndivergences that unifies multiple classical measures, such as mutual\ninformation, chi-squared dependence, and spectral redundancy, under a single\ngeometric principle. This reveals that these traditional quantities are not\nisolated heuristics but projections of a shared redundancy geometry. The theory\nfurther predicts that redundancy is bounded both above and below, giving rise\nto an optimal equilibrium that balances over-compression (loss of structure)\nand over-coupling (collapse). While classical communication theory favors\nminimal redundancy for transmission efficiency, finite and structured systems,\nsuch as those underlying real-world learning, achieve maximal stability and\ngeneralization near this equilibrium. Experiments with masked autoencoders are\nused to illustrate and verify this principle: the model exhibits a stable\nredundancy level where generalization peaks. Together, these results establish\nredundancy as a measurable and tunable quantity that bridges the asymptotic\nworld of communication and the finite world of learning."}
{"id": "2510.10952", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10952", "abs": "https://arxiv.org/abs/2510.10952", "authors": ["Xi Mao", "Zhendong Wang", "Jingyu Li", "Lingchao Mao", "Utibe Essien", "Hairong Wang", "Xuelei Sherry Ni"], "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant", "comment": null, "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling."}
{"id": "2510.10959", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10959", "abs": "https://arxiv.org/abs/2510.10959", "authors": ["Xiaoyun Zhang", "Xiaojian Yuan", "Di Huang", "Wang You", "Chen Hu", "Jingqing Ruan", "Kejiang Chen", "Xing Hu"], "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning", "comment": "16 pages, 4 figures", "summary": "Reasoning ability has become a defining capability of Large Language Models\n(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as\na key paradigm to enhance it. However, RLVR training often suffers from policy\nentropy collapse, where the policy becomes overly deterministic, hindering\nexploration and limiting reasoning performance. While entropy regularization is\na common remedy, its effectiveness is highly sensitive to the fixed\ncoefficient, making it unstable across tasks and models. In this work, we\nrevisit entropy regularization in RLVR and argue that its potential has been\nlargely underestimated. Our analysis shows that (i) tasks of varying difficulty\ndemand distinct exploration intensities, and (ii) balanced exploration may\nrequire the policy entropy to be maintained within a moderate range below its\ninitial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a\nframework that dynamically balances exploration and exploitation via three\ncomponents: difficulty-aware coefficient allocation, initial-anchored target\nentropy, and dynamic global coefficient adjustment. Experiments on multiple\nmathematical reasoning benchmarks show that AER consistently outperforms\nbaselines, improving both reasoning accuracy and exploration capability."}
{"id": "2510.10962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10962", "abs": "https://arxiv.org/abs/2510.10962", "authors": ["Wei Huang", "Yue Liao", "Yukang Chen", "Jianhui Liu", "Haoru Tan", "Si Liu", "Shiming Zhang", "Shuicheng Yan", "Xiaojuan Qi"], "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "comment": "15 pages, 13 figures", "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment."}
{"id": "2510.10963", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10963", "abs": "https://arxiv.org/abs/2510.10963", "authors": ["Zhuo Li", "Yuege Feng", "Dandan Guo", "Jinpeng Hu", "Anningzhe Gao", "Xiang Wan"], "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport", "comment": "EMNLP2025", "summary": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT"}
{"id": "2510.10964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10964", "abs": "https://arxiv.org/abs/2510.10964", "authors": ["Junhyuck Kim", "Ethan Ewer", "Taehong Moon", "Jongho Park", "Dimitris Papailiopoulos"], "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models", "comment": "20 pages, 12 figures", "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."}
{"id": "2510.10968", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10968", "abs": "https://arxiv.org/abs/2510.10968", "authors": ["Hongkai Zheng", "Austin Wang", "Zihui Wu", "Zhengyu Huang", "Ricardo Baptista", "Yisong Yue"], "title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors", "comment": null, "summary": "Derivative-free Bayesian inversion is an important task in many science and\nengineering applications, particularly when computing the forward model\nderivative is computationally and practically challenging. In this paper, we\nintroduce Blade, which can produce accurate and well-calibrated posteriors for\nBayesian inversion using an ensemble of interacting particles. Blade leverages\npowerful data-driven priors based on diffusion models, and can handle nonlinear\nforward models that permit only black-box access (i.e., derivative-free).\nTheoretically, we establish a non-asymptotic convergence analysis to\ncharacterize the effects of forward model and prior estimation errors.\nEmpirically, Blade achieves superior performance compared to existing\nderivative-free Bayesian inversion methods on various inverse problems,\nincluding challenging highly nonlinear fluid dynamics."}
{"id": "2510.10980", "categories": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH", "68T07, 62B11, 94A17, 53B12", "I.2.6; I.5.1; G.3; H.1.1"], "pdf": "https://arxiv.org/pdf/2510.10980", "abs": "https://arxiv.org/abs/2510.10980", "authors": ["Di Zhang"], "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation", "comment": "7 pages", "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms."}
{"id": "2510.10982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10982", "abs": "https://arxiv.org/abs/2510.10982", "authors": ["Zihan Wang", "Zhiyong Ma", "Zhongkui Ma", "Shuofeng Liu", "Akide Liu", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization", "comment": null, "summary": "Recent AI regulations call for data that remain useful for innovation while\nresistant to misuse, balancing utility with protection at the model level.\nExisting approaches either perturb data to make it unlearnable or retrain\nmodels to suppress transfer, but neither governs inference by unknown models,\nand both typically require control over training. We propose non-transferable\nexamples (NEs), a training-free and data-agnostic input-side usage-control\nmechanism. We recode inputs within a model-specific low-sensitivity subspace,\npreserving outputs for the authorized model while reducing performance on\nunauthorized models through subspace misalignment. We establish formal bounds\nthat guarantee utility for the authorized model and quantify deviation for\nunauthorized ones, with the Hoffman-Wielandt inequality linking degradation to\nspectral differences. Empirically, NEs retain performance on diverse vision\nbackbones and state-of-the-art vision-language models under common\npreprocessing, whereas non-target models collapse even with reconstruction\nattempts. These results establish NEs as a practical means to preserve intended\ndata utility while preventing unauthorized exploitation. Our project is\navailable at https://trusted-system-lab.github.io/model-specificity"}
{"id": "2510.11016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11016", "abs": "https://arxiv.org/abs/2510.11016", "authors": ["Ziyi Gao", "Yike Xu", "Jiahao Yuan", "Baokun Wang", "Jinyong Wen", "Xiaotong Lin", "Yun Liu", "Xing Fu", "Yu Cheng", "Yongchao Liu", "Weiqiang Wang", "Zhongle Xie"], "title": "Instruction-aware User Embedding via Synergistic Language and Representation Modeling", "comment": null, "summary": "User representation modeling has become increasingly crucial for personalized\napplications, yet existing approaches struggle with generalizability across\ndomains and sensitivity to noisy behavioral signals. We present InstructUE, an\ninstruction-aware user embedding foundation model that leverages large language\nmodels (LLMs) to generate general and instruction-aware user representations.\nInstructUE introduces a multi-encoder architecture with a lightweight adapter\nthat efficiently processes heterogeneous data from six different sources while\npreserving their structural characteristics. Additionally, it proposes a novel\ncontrastive-autoregressive training framework that bridges language and\nrepresentation spaces through a curated UserQA dataset. The\ncontrastive-autoregressive training framework simultaneously leverages\nautoregressive learning to capture domain knowledge in language space and\ncontrastive learning to align user-text embeddings in representation space,\nthereby enhancing the instruction-awareness and noise-robustness of user\nembeddings. Through extensive experiments on real-world applications, we\ndemonstrate that InstructUE significantly outperforms existing methods across\nmultiple domains including user prediction, marketing, and recommendation\nscenarios. Our results show that instruction-aware user modeling can\neffectively achieve instruction-guided denoising of user information in\nspecific scenarios, paving the way for more generalizable and robust user\nrepresentation learning."}
{"id": "2510.11018", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11018", "abs": "https://arxiv.org/abs/2510.11018", "authors": ["Pranav Ramesh", "Arjun Roy", "Deepak Ravikumar", "Kaushik Roy", "Gopalakrishnan Srinivasan"], "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness", "comment": null, "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods."}
{"id": "2510.11049", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11049", "abs": "https://arxiv.org/abs/2510.11049", "authors": ["Sonakshi Dua", "Gonzalo Mateos", "Sundeep Prabhakar Chepuri"], "title": "Conformal Inference for Time Series over Graphs", "comment": null, "summary": "Trustworthy decision making in networked, dynamic environments calls for\ninnovative uncertainty quantification substrates in predictive models for graph\ntime series. Existing conformal prediction (CP) methods have been applied\nseparately to multivariate time series and static graphs, but they either\nignore the underlying graph topology or neglect temporal dynamics. To bridge\nthis gap, here we develop a CP-based sequential prediction region framework\ntailored for graph time series. A key technical innovation is to leverage the\ngraph structure and thus capture pairwise dependencies across nodes, while\nproviding user-specified coverage guarantees on the predictive outcomes. We\nformally establish that our scheme yields an exponential shrinkage in the\nvolume of the ellipsoidal prediction set relative to its graph-agnostic\ncounterpart. Using real-world datasets, we demonstrate that the novel\nuncertainty quantification framework maintains desired empirical coverage while\nachieving markedly smaller (up to 80% reduction) prediction regions than\nexisting approaches."}
{"id": "2510.11057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11057", "abs": "https://arxiv.org/abs/2510.11057", "authors": ["Youngrok Park", "Hojung Jung", "Sangmin Bae", "Se-Young Yun"], "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models", "comment": "54 pages, 17 figures, 18 tables", "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks."}
{"id": "2510.11058", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11058", "abs": "https://arxiv.org/abs/2510.11058", "authors": ["I Chiu", "Yu-Tung Liu", "Kuan-Chen Wang", "Hung-Yu Wei", "Yu Tsao"], "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks", "comment": "5 pages, 2 figures", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems."}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs."}
{"id": "2510.11068", "categories": ["cs.LG", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.11068", "abs": "https://arxiv.org/abs/2510.11068", "authors": ["Xinyu Luo", "Jie Liu", "Kecheng Chen", "Junyi Yang", "Bo Ding", "Arindam Basu", "Haoliang Li"], "title": "Efficient Edge Test-Time Adaptation via Latent Feature Coordinate Correction", "comment": "Under review", "summary": "Edge devices face significant challenges due to limited computational\nresources and distribution shifts, making efficient and adaptable machine\nlearning essential. Existing test-time adaptation (TTA) methods often rely on\ngradient-based optimization or batch processing, which are inherently\nunsuitable for resource-constrained edge scenarios due to their reliance on\nbackpropagation and high computational demands. Gradient-free alternatives\naddress these issues but often suffer from limited learning capacity, lack\nflexibility, or impose architectural constraints. To overcome these\nlimitations, we propose a novel single-instance TTA method tailored for edge\ndevices (TED), which employs forward-only coordinate optimization in the\nprincipal subspace of latent using the covariance matrix adaptation evolution\nstrategy (CMA-ES). By updating a compact low-dimensional vector, TED not only\nenhances output confidence but also aligns the latent representation closer to\nthe source latent distribution within the latent principal subspace. This is\nachieved without backpropagation, keeping the model parameters frozen, and\nenabling efficient, forgetting-free adaptation with minimal memory and\ncomputational overhead. Experiments on image classification and keyword\nspotting tasks across the ImageNet and Google Speech Commands series datasets\ndemonstrate that TED achieves state-of-the-art performance while\n$\\textit{reducing computational complexity by up to 63 times}$, offering a\npractical and scalable solution for real-world edge applications. Furthermore,\nwe successfully $\\textit{deployed TED on the ZYNQ-7020 platform}$,\ndemonstrating its feasibility and effectiveness for resource-constrained edge\ndevices in real-world deployments."}
{"id": "2510.11084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11084", "abs": "https://arxiv.org/abs/2510.11084", "authors": ["Wonah Kim", "Jeonghyeon Park", "Dongsan Jun", "Jungkyu Han", "Sejin Chun"], "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series", "comment": "20 pages, 4 Figures,", "summary": "Disentangling complex causal relationships is important for accurate\ndetection of anomalies. In multivariate time series analysis, dynamic\ninteractions among data variables over time complicate the interpretation of\ncausal relationships. Traditional approaches assume statistical independence\nbetween variables in unsupervised settings, whereas recent methods capture\nfeature correlations through graph representation learning. However, their\nrepresentations fail to explicitly infer the causal relationships over\ndifferent time periods. To solve the problem, we propose Causally Disentangled\nRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and\nidentify their causal relationships in multivariate time series. First, we\ndesign the causal process as model input, the temporal heterogeneous graph, and\ncausal relationships. Second, our representation identifies causal\nrelationships over different time periods and disentangles latent variables to\ninfer the corresponding causal factors. Third, our experiments on real-world\ndatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms\nof accuracy and root cause analysis. Fourth, our model analysis validates\nhyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct\na case study to show how our approach assists human experts in diagnosing the\nroot causes of anomalies."}
{"id": "2510.11110", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11110", "abs": "https://arxiv.org/abs/2510.11110", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Min-Kyung Jung", "Dong-Joo Kim"], "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities", "comment": "9 pages, 2 figures", "summary": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability."}
{"id": "2510.11121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11121", "abs": "https://arxiv.org/abs/2510.11121", "authors": ["Rongjie Zhu", "Cong Zhang", "Zhiguang Cao"], "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "comment": null, "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."}
{"id": "2510.11128", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11128", "abs": "https://arxiv.org/abs/2510.11128", "authors": ["Qiyi Tong", "Olivia Nocentini", "Marta Lagomarsino", "Kuanqi Cai", "Marta Lorenzini", "Arash Ajoudani"], "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer", "comment": null, "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead."}
{"id": "2510.11133", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11133", "abs": "https://arxiv.org/abs/2510.11133", "authors": ["Yingnan Liu", "Rui Qiao", "Mong Li Lee", "Wynne Hsu"], "title": "Test-Time Adaptation by Causal Trimming", "comment": "Accepted to the Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025); Code is available at\n  https://github.com/NancyQuris/TACT", "summary": "Test-time adaptation aims to improve model robustness under distribution\nshifts by adapting models with access to unlabeled target samples. A primary\ncause of performance degradation under such shifts is the model's reliance on\nfeatures that lack a direct causal relationship with the prediction target. We\nintroduce Test-time Adaptation by Causal Trimming (TACT), a method that\nidentifies and removes non-causal components from representations for test\ndistributions. TACT applies data augmentations that preserve causal features\nwhile varying non-causal ones. By analyzing the changes in the representations\nusing Principal Component Analysis, TACT identifies the highest variance\ndirections associated with non-causal features. It trims the representations by\nremoving their projections on the identified directions, and uses the trimmed\nrepresentations for the predictions. During adaptation, TACT continuously\ntracks and refines these directions to get a better estimate of non-causal\nfeatures. We theoretically analyze the effectiveness of this approach and\nempirically validate TACT on real-world out-of-distribution benchmarks. TACT\nconsistently outperforms state-of-the-art methods by a significant margin."}
{"id": "2510.11140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11140", "abs": "https://arxiv.org/abs/2510.11140", "authors": ["Zhijian Zhou", "Xunye Tian", "Liuhua Peng", "Chao Lei", "Antonin Schrab", "Danica J. Sutherland", "Feng Liu"], "title": "DUAL: Learning Diverse Kernels for Aggregated Two-sample and Independence Testing", "comment": null, "summary": "To adapt kernel two-sample and independence testing to complex structured\ndata, aggregation of multiple kernels is frequently employed to boost testing\npower compared to single-kernel tests. However, we observe a phenomenon that\ndirectly maximizing multiple kernel-based statistics may result in highly\nsimilar kernels that capture highly overlapping information, limiting the\neffectiveness of aggregation. To address this, we propose an aggregated\nstatistic that explicitly incorporates kernel diversity based on the covariance\nbetween different kernels. Moreover, we identify a fundamental challenge: a\ntrade-off between the diversity among kernels and the test power of individual\nkernels, i.e., the selected kernels should be both effective and diverse. This\nmotivates a testing framework with selection inference, which leverages\ninformation from the training phase to select kernels with strong individual\nperformance from the learned diverse kernel pool. We provide rigorous\ntheoretical statements and proofs to show the consistency on the test power and\ncontrol of Type-I error, along with asymptotic analysis of the proposed\nstatistics. Lastly, we conducted extensive empirical experiments demonstrating\nthe superior performance of our proposed approach across various benchmarks for\nboth two-sample and independence testing."}
{"id": "2510.11141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11141", "abs": "https://arxiv.org/abs/2510.11141", "authors": ["Mohammad Karami", "Mostafa Jalali", "Fatemeh Ghassemi"], "title": "A Comprehensive Forecasting-Based Framework for Time Series Anomaly Detection: Benchmarking on the Numenta Anomaly Benchmark (NAB)", "comment": null, "summary": "Time series anomaly detection is critical for modern digital infrastructures,\nyet existing methods lack systematic cross-domain evaluation. We present a\ncomprehensive forecasting-based framework unifying classical methods\n(Holt-Winters, SARIMA) with deep learning architectures (LSTM, Informer) under\na common residual-based detection interface. Our modular pipeline integrates\npreprocessing (normalization, STL decomposition), four forecasting models, four\ndetection methods, and dual evaluation through forecasting metrics (MAE, RMSE,\nPCC) and detection metrics (Precision, Recall, F1, AUC). We conduct the first\ncomplete evaluation on the Numenta Anomaly Benchmark (58 datasets, 7\ncategories) with 232 model training runs and 464 detection evaluations\nachieving 100\\% success rate. LSTM achieves best performance (F1: 0.688,\nranking first or second on 81\\% of datasets) with exceptional correlation on\ncomplex patterns (PCC: 0.999). Informer provides competitive accuracy (F1:\n0.683) with 30\\% faster training. Classical methods achieve perfect predictions\non simple synthetic data with 60 lower cost but show 2-3 worse F1-scores on\nreal-world datasets. Forecasting quality dominates detection performance:\ndifferences between detection methods (F1: 0.621-0.688) are smaller than\nbetween forecasting models (F1: 0.344-0.688). Our findings provide\nevidence-based guidance: use LSTM for complex patterns, Informer for\nefficiency-critical deployments, and classical methods for simple periodic data\nwith resource constraints. The complete implementation and results establish\nbaselines for future forecasting-based anomaly detection research."}
{"id": "2510.11162", "categories": ["cs.LG", "cs.NE", "nlin.AO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.11162", "abs": "https://arxiv.org/abs/2510.11162", "authors": ["Roman A. Kononov", "Nikita A. Pospelov", "Konstantin V. Anokhin", "Vladimir V. Nekorkin", "Oleg V. Maslennikov"], "title": "Emergence of hybrid computational dynamics through reinforcement learning", "comment": "22 pages, 11 figures", "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems."}
{"id": "2510.11164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11164", "abs": "https://arxiv.org/abs/2510.11164", "authors": ["Ilaria Vascotto", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness", "comment": "Accepted at the European Workshop on Trustworthy Artificial\n  Intelligence (TRUST-AI), co-located within ECAI 2025", "summary": "The use of Artificial Intelligence (AI) models in real-world and high-risk\napplications has intensified the discussion about their trustworthiness and\nethical usage, from both a technical and a legislative perspective. The field\nof eXplainable Artificial Intelligence (XAI) addresses this challenge by\nproposing explanations that bring to light the decision-making processes of\ncomplex black-box models. Despite being an essential property, the robustness\nof explanations is often an overlooked aspect during development: only robust\nexplanation methods can increase the trust in the system as a whole. This paper\ninvestigates the role of robustness through the usage of a feature importance\naggregation derived from multiple models ($k$-nearest neighbours, random forest\nand neural networks). Preliminary results showcase the potential in increasing\nthe trustworthiness of the application, while leveraging multiple model's\npredictive power."}
{"id": "2510.11168", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11168", "abs": "https://arxiv.org/abs/2510.11168", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces", "comment": "Accepted to ICML 2025", "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy."}
{"id": "2510.11170", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11170", "abs": "https://arxiv.org/abs/2510.11170", "authors": ["Daniel Scalena", "Leonidas Zotos", "Elisabetta Fersini", "Malvina Nissim", "Ahmet Üstün"], "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling", "comment": null, "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling."}
{"id": "2510.11184", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11184", "abs": "https://arxiv.org/abs/2510.11184", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning."}
{"id": "2510.11188", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.11188", "abs": "https://arxiv.org/abs/2510.11188", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "title": "Protein as a Second Language for LLMs", "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18\n  pages, 9 figures total. Submitted to ICLR 2026 (under review)", "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels."}
{"id": "2510.11202", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11202", "abs": "https://arxiv.org/abs/2510.11202", "authors": ["Marco Pintore", "Giorgio Piras", "Angelo Sotgiu", "Maura Pintor", "Battista Biggio"], "title": "Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models", "comment": "Preprint", "summary": "To address the extremely concerning problem of software vulnerability, system\nsecurity is often entrusted to Machine Learning (ML) algorithms. Despite their\nnow established detection capabilities, such models are limited by design to\nflagging the entire input source code function as vulnerable, rather than\nprecisely localizing the concerned code lines. However, the detection\ngranularity is crucial to support human operators during software development,\nensuring that such predictions reflect the true code semantics to help debug,\nevaluate, and fix the detected vulnerabilities. To address this issue, recent\nwork made progress toward improving the detector's localization ability, thus\nnarrowing down the vulnerability detection \"window\" and providing more\nfine-grained predictions. Such approaches, however, implicitly disregard the\npresence of spurious correlations and biases in the data, which often\npredominantly influence the performance of ML algorithms. In this work, we\ninvestigate how detectors comply with this requirement by proposing an\nexplainability-based evaluation procedure. Our approach, defined as Detection\nAlignment (DA), quantifies the agreement between the input source code lines\nthat most influence the prediction and the actual localization of the\nvulnerability as per the ground truth. Through DA, which is model-agnostic and\nadaptable to different detection tasks, not limited to our use case, we analyze\nmultiple learning-based vulnerability detectors and datasets. As a result, we\nshow how the predictions of such models are consistently biased by\nnon-vulnerable lines, ultimately highlighting the high impact of biases and\nspurious correlations. The code is available at\nhttps://github.com/pralab/vuln-localization-eval."}
{"id": "2510.11209", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11209", "abs": "https://arxiv.org/abs/2510.11209", "authors": ["Nicola Alboré", "Gabriele Di Antonio", "Fabrizio Coccetti", "Andrea Gabrielli"], "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling", "comment": null, "summary": "We propose a new reservoir computing method for forecasting high-resolution\nspatiotemporal datasets. By combining multi-resolution inputs from coarser to\nfiner layers, our architecture better captures both local and global dynamics.\nApplied to Sea Surface Temperature data, it outperforms standard parallel\nreservoir models in long-term forecasting, demonstrating the effectiveness of\ncross-layers coupling in improving predictive accuracy. Finally, we show that\nthe optimal network dynamics in each layer become increasingly linear,\nrevealing the slow modes propagated to subsequent layers."}
{"id": "2510.11227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11227", "abs": "https://arxiv.org/abs/2510.11227", "authors": ["Ahmed Rashwan", "Keith Briggs", "Chris Budd", "Lisa Kreusser"], "title": "Enforcing convex constraints in Graph Neural Networks", "comment": null, "summary": "Many machine learning applications require outputs that satisfy complex,\ndynamic constraints. This task is particularly challenging in Graph Neural\nNetwork models due to the variable output sizes of graph-structured data. In\nthis paper, we introduce ProjNet, a Graph Neural Network framework which\nsatisfies input-dependant constraints. ProjNet combines a sparse vector\nclipping method with the Component-Averaged Dykstra (CAD) algorithm, an\niterative scheme for solving the best-approximation problem. We establish a\nconvergence result for CAD and develop a GPU-accelerated implementation capable\nof handling large-scale inputs efficiently. To enable end-to-end training, we\nintroduce a surrogate gradient for CAD that is both computationally efficient\nand better suited for optimization than the exact gradient. We validate ProjNet\non four classes of constrained optimisation problems: linear programming, two\nclasses of non-convex quadratic programs, and radio transmit power\noptimization, demonstrating its effectiveness across diverse problem settings."}
{"id": "2510.11234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11234", "abs": "https://arxiv.org/abs/2510.11234", "authors": ["Jegwang Ryu", "Minkyu Kim", "Seungjun Shin", "Hee Min Choi", "Dokwan Oh", "Jaeho Lee"], "title": "Neural Weight Compression for Language Models", "comment": null, "summary": "The efficient storage and transmission of language model weights is becoming\nincreasingly important, as their scale and adoption continue to grow. However,\nas our understanding of this new data modality is limited, designing a good\ncompression algorithm for language model weights heavily relies on manual,\ntrial-and-error approaches. In this paper, we propose a learned compression\nframework that trains neural codecs directly from pretrained language model\nweights. Unlike conventional data (e.g., images), language model weights pose\nunique challenges: the sizes and shapes of weight tensors vary significantly,\nand the reconstruction quality must be judged by downstream model predictions\nrather than na\\\"ive MSE loss. To address this, we introduce Neural Weight\nCompression (NWC), a novel autoencoder-based neural codec tailored to model\nweight compression. The proposed method inherits the advantages of\nautoencoder-based codecs while incorporating three technical components: (1)\ncolumn-wise tensor chunking and normalization; (2) an importance-aware training\nloss; (3) an inference-time error compensation mechanism guided by model\noutputs. Experiments on open-weight language models show that NWC achieves\ncompetitive or state-of-the-art accuracy-compression tradeoffs, with\nparticularly strong results at 4-6 bit precisions where accuracy remains nearly\non par with FP16 models."}
{"id": "2510.11245", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11245", "abs": "https://arxiv.org/abs/2510.11245", "authors": ["Leonardo Di Nino", "Gabriele D'Acunto", "Sergio Barbarossa", "Paolo Di Lorenzo"], "title": "Learning the Structure of Connection Graphs", "comment": null, "summary": "Connection graphs (CGs) extend traditional graph models by coupling network\ntopology with orthogonal transformations, enabling the representation of global\ngeometric consistency. They play a key role in applications such as\nsynchronization, Riemannian signal processing, and neural sheaf diffusion. In\nthis work, we address the inverse problem of learning CGs directly from\nobserved signals. We propose a principled framework based on maximum\npseudo-likelihood under a consistency assumption, which enforces spectral\nproperties linking the connection Laplacian to the underlying combinatorial\nLaplacian. Based on this formulation, we introduce the Structured Connection\nGraph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian\nmanifolds that jointly infers network topology, edge weights, and geometric\nstructure. Our experiments show that SCGL consistently outperforms existing\nbaselines in both topological recovery and geometric fidelity, while remaining\ncomputationally efficient."}
{"id": "2510.11250", "categories": ["cs.LG", "I.5.2"], "pdf": "https://arxiv.org/pdf/2510.11250", "abs": "https://arxiv.org/abs/2510.11250", "authors": ["Sujan Chakraborty", "Rahul Bordoloi", "Anindya Sengupta", "Olaf Wolkenhauer", "Saptarshi Bej"], "title": "FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware Optimization", "comment": null, "summary": "Graph-based learning is a cornerstone for analyzing structured data, with\nnode classification as a central task. However, in many real-world graphs,\nnodes lack informative feature vectors, leaving only neighborhood connectivity\nand class labels as available signals. In such cases, effective classification\nhinges on learning node embeddings that capture structural roles and\ntopological context. We introduce a fast semi-supervised embedding framework\nthat jointly optimizes three complementary objectives: (i) unsupervised\nstructure preservation via scalable modularity approximation, (ii) supervised\nregularization to minimize intra-class variance among labeled nodes, and (iii)\nsemi-supervised propagation that refines unlabeled nodes through\nrandom-walk-based label spreading with attention-weighted similarity. These\ncomponents are unified into a single iterative optimization scheme, yielding\nhigh-quality node embeddings. On standard benchmarks, our method consistently\nachieves classification accuracy at par with or superior to state-of-the-art\napproaches, while requiring significantly less computational cost."}
{"id": "2510.11257", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.11257", "abs": "https://arxiv.org/abs/2510.11257", "authors": ["Davide Borghini", "Davide Marchi", "Angelo Nardone", "Giordano Scerra", "Silvia Giulia Galfrè", "Alessandro Pingitore", "Giuseppe Prencipe", "Corrado Priami", "Alina Sîrbu"], "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction", "comment": "Presented in the Poster Session of Computational Intelligence methods\n  for Bioinformatics and Biostatistics (CIBB) 2025", "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase."}
{"id": "2510.11274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11274", "abs": "https://arxiv.org/abs/2510.11274", "authors": ["Jianzhe Zhao", "Hailin Zhu", "Yu Zhang", "Ziqi Chen", "Guibing Guo"], "title": "FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local Optimization in Heterogeneous Data Scenarios", "comment": null, "summary": "Federated efficient fine-tuning has emerged as an approach that leverages\ndistributed data and computational resources across nodes to address the\nchallenges of large-scale fine-tuning and privacy preservation. The Low-Rank\nAdaptation (LoRA) enables efficient fine-tuning of large-scale pre-trained\nmodels by introducing trainable low-rank matrices into weight updates.However,\nin heterogeneous data scenarios, client drift weakens the generalization of the\nglobal model, and local models often fail to meet the personalized needs of\nindividual clients.Moreover, existing federated LoRA efficient fine-tuning\ntechniques overlook fine-grained analysis of the tuning matrices. To address\nthis, we conducted preliminary experiments and found that different LoRA\nmatrices exhibit different sensitivity to changes in the direction and\nmagnitude of their vectors.We thus propose a fine-grained federated LoRA tuning\nmethod. By fine-tuning the more sensitive directional vectors in the A matrix,\nwhich encode shared knowledge, our method learns shared features more\neffectively across clients and enhances global generalization. Simultaneously,\nby fine-tuning the more sensitive magnitude vectors in the B matrix, which\nencode personalized knowledge, our method better captures personalized\nknowledge, enabling detailed adaptation to local data. The method uses a\npipeline combining global and local optimizers. Global optimization further\nimproves local models, achieving collaborative optimization between global and\nlocal levels. This improves both the generalization ability of the global model\nand the personalized adaptation of local models under heterogeneous data\nscenarios. Experiments on Databricks-Dolly-15k and Natural Instructions with\nLLaMA2-7B and Deepseek-7B confirm that our method improves global performance\nby 0.39% and local performance by 0.59%."}
{"id": "2510.11278", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11278", "abs": "https://arxiv.org/abs/2510.11278", "authors": ["Gareth Seneque", "Lap-Hang Ho", "Nafise Erfanian Saeedi", "Jeffrey Molendijk", "Ariel Kupermann", "Tim Elson"], "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models", "comment": "52 pages, 10 figures", "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability"}
{"id": "2510.11282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11282", "abs": "https://arxiv.org/abs/2510.11282", "authors": ["Ning Yang", "Hengyu Zhong", "Haijun Zhang", "Randall Berry"], "title": "Vision-LLMs for Spatiotemporal Traffic Forecasting", "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments."}
{"id": "2510.11283", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11283", "abs": "https://arxiv.org/abs/2510.11283", "authors": ["Antoine Mouchamps", "Arthur Malherbe", "Adrien Bolland", "Damien Ernst"], "title": "Gym-TORAX: Open-source software for integrating RL with plasma control simulators", "comment": null, "summary": "This paper presents Gym-TORAX, a Python package enabling the implementation\nof Reinforcement Learning (RL) environments for simulating plasma dynamics and\ncontrol in tokamaks. Users define succinctly a set of control actions and\nobservations, and a control objective from which Gym-TORAX creates a Gymnasium\nenvironment that wraps TORAX for simulating the plasma dynamics. The objective\nis formulated through rewards depending on the simulated state of the plasma\nand control action to optimize specific characteristics of the plasma, such as\nperformance and stability. The resulting environment instance is then\ncompatible with a wide range of RL algorithms and libraries and will facilitate\nRL research in plasma control. In its current version, one environment is\nreadily available, based on a ramp-up scenario of the International\nThermonuclear Experimental Reactor (ITER)."}
{"id": "2510.11292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11292", "abs": "https://arxiv.org/abs/2510.11292", "authors": ["Wenbo Wu", "Qingyi Si", "Xiurui Pan", "Ye Wang", "Jie Zhang"], "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences", "comment": null, "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."}
{"id": "2510.11335", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11335", "abs": "https://arxiv.org/abs/2510.11335", "authors": ["Mayank Nagda", "Phil Ostheimer", "Justus Arweiler", "Indra Jungjohann", "Jennifer Werner", "Dennis Wagner", "Aparna Muraleedharan", "Pouya Jafari", "Jochen Schmid", "Fabian Jirasek", "Jakob Burger", "Michael Bortz", "Hans Hasse", "Stephan Mandt", "Marius Kloft", "Sophie Fellenz"], "title": "DiffStyleTS: Diffusion Model for Style Transfer in Time Series", "comment": null, "summary": "Style transfer combines the content of one signal with the style of another.\nIt supports applications such as data augmentation and scenario simulation,\nhelping machine learning models generalize in data-scarce domains. While well\ndeveloped in vision and language, style transfer methods for time series data\nremain limited. We introduce DiffTSST, a diffusion-based framework that\ndisentangles a time series into content and style representations via\nconvolutional encoders and recombines them through a self-supervised\nattention-based diffusion process. At inference, encoders extract content and\nstyle from two distinct series, enabling conditional generation of novel\nsamples to achieve style transfer. We demonstrate both qualitatively and\nquantitatively that DiffTSST achieves effective style transfer. We further\nvalidate its real-world utility by showing that data augmentation with DiffTSST\nimproves anomaly detection in data-scarce regimes."}
{"id": "2510.11339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11339", "abs": "https://arxiv.org/abs/2510.11339", "authors": ["Xingtong Yu", "Ruijuan Liang", "Xinming Zhang", "Yuan Fang"], "title": "Event-Aware Prompt Learning for Dynamic Graphs", "comment": "Under review", "summary": "Real-world graph typically evolve via a series of events, modeling dynamic\ninteractions between objects across various domains. For dynamic graph\nlearning, dynamic graph neural networks (DGNNs) have emerged as popular\nsolutions. Recently, prompt learning methods have been explored on dynamic\ngraphs. However, existing methods generally focus on capturing the relationship\nbetween nodes and time, while overlooking the impact of historical events. In\nthis paper, we propose EVP, an event-aware dynamic graph prompt learning\nframework that can serve as a plug-in to existing methods, enhancing their\nability to leverage historical events knowledge. First, we extract a series of\nhistorical events for each node and introduce an event adaptation mechanism to\nalign the fine-grained characteristics of these events with downstream tasks.\nSecond, we propose an event aggregation mechanism to effectively integrate\nhistorical knowledge into node representations. Finally, we conduct extensive\nexperiments on four public datasets to evaluate and analyze EVP."}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training."}
{"id": "2510.11347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11347", "abs": "https://arxiv.org/abs/2510.11347", "authors": ["Etzion Harari", "Moshe Unger"], "title": "Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features."}
{"id": "2510.11354", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11354", "abs": "https://arxiv.org/abs/2510.11354", "authors": ["Xuan Tang", "Han Zhang", "Yuan Cao", "Difan Zou"], "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning Neural Networks", "comment": "71 pages, 12 figures, NeurIPS 2025", "summary": "Adam is a popular and widely used adaptive gradient method in deep learning,\nwhich has also received tremendous focus in theoretical research. However, most\nexisting theoretical work primarily analyzes its full-batch version, which\ndiffers fundamentally from the stochastic variant used in practice. Unlike SGD,\nstochastic Adam does not converge to its full-batch counterpart even with\ninfinitesimal learning rates. We present the first theoretical characterization\nof how batch size affects Adam's generalization, analyzing two-layer\nover-parameterized CNNs on image data. Our results reveal that while both Adam\nand AdamW with proper weight decay $\\lambda$ converge to poor test error\nsolutions, their mini-batch variants can achieve near-zero test error. We\nfurther prove Adam has a strictly smaller effective weight decay bound than\nAdamW, theoretically explaining why Adam requires more sensitive $\\lambda$\ntuning. Extensive experiments validate our findings, demonstrating the critical\nrole of batch size and weight decay in Adam's generalization performance."}
{"id": "2510.11390", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11390", "abs": "https://arxiv.org/abs/2510.11390", "authors": ["Razvan Marinescu", "Victoria-Elisabeth Gruber", "Diego Fajardo"], "title": "Medical Interpretability and Knowledge Maps of Large Language Models", "comment": "29 pages, 34 figures, 5 tables", "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied."}
{"id": "2510.11400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11400", "abs": "https://arxiv.org/abs/2510.11400", "authors": ["Kahou Tam", "Chunlin Tian", "Li Li", "Haikai Zhao", "ChengZhong Xu"], "title": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management", "comment": "Sensys 2024", "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines."}
{"id": "2510.11409", "categories": ["cs.LG", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11409", "abs": "https://arxiv.org/abs/2510.11409", "authors": ["Lucas Joos", "Daniel A. Keim", "Maximilian T. Fischer"], "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews", "comment": null, "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows."}
{"id": "2510.11442", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11442", "abs": "https://arxiv.org/abs/2510.11442", "authors": ["Xinyan Guan", "Yongfan Lai", "Jiarui Jin", "Jun Li", "Haoyu Wang", "Qinghao Zhao", "Deyun Zhang", "Shijia Geng", "Shenda Hong"], "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices", "comment": "24 pages, 5 figures, submitted to Nature Communications", "summary": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for\ncardiac diagnosis, providing comprehensive spatial coverage of the heart\nnecessary to detect conditions such as myocardial infarction (MI). However,\ntheir lack of portability limits continuous and large-scale use. Three-lead ECG\nsystems are widely used in wearable devices due to their simplicity and\nmobility, but they often fail to capture pathologies in unmeasured regions. To\naddress this, we propose WearECG, a Variational Autoencoder (VAE) method that\nreconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model\nincludes architectural improvements to better capture temporal and spatial\ndependencies in ECG signals. We evaluate generation quality using MSE, MAE, and\nFrechet Inception Distance (FID), and assess clinical validity via a Turing\ntest with expert cardiologists. To further validate diagnostic utility, we\nfine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label\nclassification task involving over 40 cardiac conditions, including six\ndifferent myocardial infarction locations, using both real and generated\nsignals. Experiments on the MIMIC dataset show that our method produces\nphysiologically realistic and diagnostically informative signals, with robust\nperformance in downstream tasks. This work demonstrates the potential of\ngenerative modeling for ECG reconstruction and its implications for scalable,\nlow-cost cardiac screening."}
{"id": "2510.11471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11471", "abs": "https://arxiv.org/abs/2510.11471", "authors": ["Sarthak Mittal", "Divyat Mahajan", "Guillaume Lajoie", "Mohammad Pezeshki"], "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers", "comment": null, "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation."}
{"id": "2510.11472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11472", "abs": "https://arxiv.org/abs/2510.11472", "authors": ["Yanjie Zhu", "Zhen Zhang", "Yunli Wang", "Zhiqiang Wang", "Yu Li", "Rufan Zhou", "Shiyang Wen", "Peng Jiang", "Chenhao Lin", "Jian Yang"], "title": "Differentiable Fast Top-K Selection for Large-Scale Recommendation", "comment": "12 pages, 5 figures", "summary": "Cascade ranking is a widely adopted paradigm in large-scale information\nretrieval systems for Top-K item selection. However, the Top-K operator is\nnon-differentiable, hindering end-to-end training. Existing methods include\nLearning-to-Rank approaches (e.g., LambdaLoss), which optimize ranking metrics\nlike NDCG and suffer from objective misalignment, and differentiable\nsorting-based methods (e.g., ARF, LCRON), which relax permutation matrices for\ndirect Top-K optimization but introduce gradient conflicts through matrix\naggregation. A promising alternative is to directly construct a differentiable\napproximation of the Top-K selection operator, bypassing the use of soft\npermutation matrices. However, even state-of-the-art differentiable Top-K\noperator (e.g., LapSum) require $O(n \\log n)$ complexity due to their\ndependence on sorting for solving the threshold. Thus, we propose DFTopK, a\nnovel differentiable Top-K operator achieving optimal $O(n)$ time complexity.\nBy relaxing normalization constraints, DFTopK admits a closed-form solution and\navoids sorting. DFTopK also avoids the gradient conflicts inherent in\ndifferentiable sorting-based methods. We evaluate DFTopK on both the public\nbenchmark RecFLow and an industrial system. Experimental results show that\nDFTopK significantly improves training efficiency while achieving superior\nperformance, which enables us to scale up training samples more efficiently. In\nthe online A/B test, DFTopK yielded a +1.77\\% revenue lift with the same\ncomputational budget compared to the baseline. To the best of our knowledge,\nthis work is the first to introduce differentiable Top-K operators into\nrecommendation systems and the first to achieve theoretically optimal\nlinear-time complexity for Top-K selection. We have open-sourced our\nimplementation to facilitate future research in both academia and industry."}
{"id": "2510.11484", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.11484", "abs": "https://arxiv.org/abs/2510.11484", "authors": ["Lion Mueller", "Alberto Garcia-Ortiz", "Ardalan Najafi", "Adam Fuks", "Lennart Bamberg"], "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware", "comment": "Submitted to IEEE Embedded Systems Letters", "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems."}
{"id": "2510.11495", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11495", "abs": "https://arxiv.org/abs/2510.11495", "authors": ["Nikolaos Tsilivis", "Eran Malach", "Karen Ullrich", "Julia Kempe"], "title": "How Reinforcement Learning After Next-Token Prediction Facilitates Learning", "comment": null, "summary": "Recent advances in reasoning domains with neural networks have primarily been\nenabled by a training recipe that optimizes Large Language Models, previously\ntrained to predict the next-token in a sequence, with reinforcement learning\nalgorithms. We introduce a framework to study the success of this paradigm, and\nwe theoretically expose the optimization mechanisms by which reinforcement\nlearning improves over next-token prediction in this setting. We study learning\nfrom mixture distributions of short and long ``chain-of-thought'' sequences\nencoding a single task. In particular, when the task consists of predicting the\nparity of $d$ bits and long sequences are rare, we show how reinforcement\nlearning after next-token prediction enables autoregressive transformers to\ngeneralize, whereas mere next-token prediction requires extreme statistical or\ncomputational resources to do so. We further explain how reinforcement learning\nleverages increased test-time computation, manifested in longer responses, to\nfacilitate this learning process. In a simplified setting, we theoretically\nprove that autoregressive linear models following this training recipe can\nefficiently learn to predict the parity of $d$ bits as long as the proportion\nof long demonstrations in the data mix is not exponentially small in the input\ndimension $d$. Finally, we demonstrate these same phenomena in other settings,\nincluding the post-training of Llama-series models on mixture variations of\ncommon mathematical reasoning benchmarks."}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling."}
{"id": "2510.11499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11499", "abs": "https://arxiv.org/abs/2510.11499", "authors": ["Xinsong Feng", "Leshu Tang", "Chenan Wang", "Haipeng Chen"], "title": "Offline Reinforcement Learning with Generative Trajectory Policies", "comment": "Preprint. Under review at ICLR 2026", "summary": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks."}
{"id": "2510.11501", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11501", "abs": "https://arxiv.org/abs/2510.11501", "authors": ["Emran Yasser Moustafa", "Ivana Dusparic"], "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing", "comment": "Accepted to IEEE ICAR 2025", "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors."}
{"id": "2510.11502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11502", "abs": "https://arxiv.org/abs/2510.11502", "authors": ["Alexis Ross", "Jacob Andreas"], "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors", "comment": null, "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests)."}
{"id": "2510.11505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11505", "abs": "https://arxiv.org/abs/2510.11505", "authors": ["Aleksei Rozanov", "Samikshya Subedi", "Vasudha Sharma", "Bryan C. Runck"], "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration in the U.S. Midwest", "comment": null, "summary": "Evapotranspiration (ET) plays a critical role in the land-atmosphere\ninteractions, yet its accurate quantification across various spatiotemporal\nscales remains a challenge. In situ measurement approaches, like eddy\ncovariance (EC) or weather station-based ET estimation, allow for measuring ET\nat a single location. Agricultural uses of ET require estimates for each field\nover broad areas, making it infeasible to deploy sensing systems at each\nlocation. This study integrates tree-based and knowledge-guided machine\nlearning (ML) techniques with multispectral remote sensing data, griddled\nmeteorology and EC data to upscale ET across the Midwest United States. We\ncompare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM -\nand a simple feed-forward artificial neural network in combination with\nfeatures engineered using knowledge-guided ML principles. Models were trained\nand tested on EC towers located in the Midwest of the United States using\nk-fold cross validation with k=5 and site-year, biome stratified train-test\nsplit to avoid data leakage. Results show that LightGBM with knowledge-guided\nfeatures outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE =\n8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance\nanalysis shows that knowledge-guided features were most important for\npredicting evapotranspiration. Using the best performing model, we provide a\ndata product at 500 m spatial and one-day temporal resolution for gridded ET\nfor the period of 2019-2024. Intercomparison between the new gridded product\nand state-level weather station-based ET estimates show best-in-class\ncorrespondence."}
{"id": "2510.11541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11541", "abs": "https://arxiv.org/abs/2510.11541", "authors": ["Yuchen Yan", "Zhihua Liu", "Hao Wang", "Weiming Li", "Xiaoshuai Hao"], "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN."}
{"id": "2510.11561", "categories": ["cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.11561", "abs": "https://arxiv.org/abs/2510.11561", "authors": ["Caglar Demir", "Alkid Baci", "N'Dah Jean Kouagou", "Leonie Nora Sieger", "Stefan Heindorf", "Simon Bin", "Lukas Blübaum", "Alexander Bigerl", "Axel-Cyrille Ngonga Ngomo"], "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in Python", "comment": null, "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn."}
{"id": "2510.11590", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11590", "abs": "https://arxiv.org/abs/2510.11590", "authors": ["Zihao Zhao", "Christopher Yeh", "Lingkai Kong", "Kai Wang"], "title": "Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization", "comment": null, "summary": "Decision-focused learning (DFL) integrates predictive modeling and\noptimization by training predictors to optimize the downstream decision target\nrather than merely minimizing prediction error. To date, existing DFL methods\ntypically rely on deterministic point predictions, which are often insufficient\nto capture the intrinsic stochasticity of real-world environments. To address\nthis challenge, we propose the first diffusion-based DFL approach, which trains\na diffusion model to represent the distribution of uncertain parameters and\noptimizes the decision by solving a stochastic optimization with samples drawn\nfrom the diffusion model. Our contributions are twofold. First, we formulate\ndiffusion DFL using the reparameterization trick, enabling end-to-end training\nthrough diffusion. While effective, it is memory and compute-intensive due to\nthe need to differentiate through the diffusion sampling process. Second, we\npropose a lightweight score function estimator that uses only several forward\ndiffusion passes and avoids backpropagation through the sampling. This follows\nfrom our results that backpropagating through stochastic optimization can be\napproximated by a weighted score function formulation. We empirically show that\nour diffusion DFL approach consistently outperforms strong baselines in\ndecision quality. The source code for all experiments is available at the\nproject repository: https://github.com/GT-KOALA/Diffusion_DFL."}
{"id": "2510.11616", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.11616", "abs": "https://arxiv.org/abs/2510.11616", "authors": ["Elliot L. Epstein", "Rose Wang", "Jaewon Choi", "Markus Pelger"], "title": "Attention Factors for Statistical Arbitrage", "comment": "Accepted to the 6th ACM International Conference on AI in Finance", "summary": "Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading."}
{"id": "2510.11653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11653", "abs": "https://arxiv.org/abs/2510.11653", "authors": ["Prasanna Mayilvahanan", "Ricardo Dominguez-Olmedo", "Thaddäus Wiedemer", "Wieland Brendel"], "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model", "comment": null, "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."}
{"id": "2510.11657", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11657", "abs": "https://arxiv.org/abs/2510.11657", "authors": ["Panos Tsimpos", "Youssef Marzouk"], "title": "An Eulerian Perspective on Straight-Line Sampling", "comment": null, "summary": "We study dynamic measure transport for generative modeling: specifically,\nflows induced by stochastic processes that bridge a specified source and target\ndistribution. The conditional expectation of the process' velocity defines an\nODE whose flow map achieves the desired transport. We ask \\emph{which processes\nproduce straight-line flows} -- i.e., flows whose pointwise acceleration\nvanishes and thus are exactly integrable with a first-order method? We provide\na concise PDE characterization of straightness as a balance between conditional\nacceleration and the divergence of a weighted covariance (Reynolds) tensor.\nUsing this lens, we fully characterize affine-in-time interpolants and show\nthat straightness occurs exactly under deterministic endpoint couplings. We\nalso derive necessary conditions that constrain flow geometry for general\nprocesses, offering broad guidance for designing transports that are easier to\nintegrate."}
{"id": "2510.11677", "categories": ["cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2510.11677", "abs": "https://arxiv.org/abs/2510.11677", "authors": ["Songrun He", "Linying Lv", "Asaf Manela", "Jimmy Wu"], "title": "Chronologically Consistent Generative AI", "comment": null, "summary": "We introduce a family of chronologically consistent, instruction-following\nlarge language models to eliminate lookahead bias. Each model is trained only\non data available before a clearly defined knowledge-cutoff date, ensuring\nstrict temporal separation from any post-cutoff data. The resulting framework\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\nmodel weights that guarantee replicability, and (iii) a conservative lower\nbound on forecast accuracy, isolating the share of predictability that survives\nonce training leakage is removed. Together, these features provide researchers\nwith an easy-to-use generative AI tool useful for a wide range of prediction\ntasks that is free of lookahead bias."}
{"id": "2510.11683", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11683", "abs": "https://arxiv.org/abs/2510.11683", "authors": ["Nianyi Lin", "Jiajie Zhang", "Lei Hou", "Juanzi Li"], "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models", "comment": null, "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks."}
{"id": "2510.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11686", "abs": "https://arxiv.org/abs/2510.11686", "authors": ["Jens Tuyls", "Dylan J. Foster", "Akshay Krishnamurthy", "Jordan T. Ash"], "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training", "comment": "Website and code: https://rep-exp.github.io", "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening."}
{"id": "2510.11691", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11691", "abs": "https://arxiv.org/abs/2510.11691", "authors": ["Taira Tsuchiya"], "title": "Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum Games", "comment": "29 pages, 2 figures", "summary": "In two-player zero-sum games, the learning dynamic based on optimistic Hedge\nachieves one of the best-known regret upper bounds among strongly-uncoupled\nlearning dynamics. With an appropriately chosen learning rate, the social and\nindividual regrets can be bounded by $O(\\log(mn))$ in terms of the numbers of\nactions $m$ and $n$ of the two players. This study investigates the optimality\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\nend, we begin by refining existing regret analysis and show that, in the\nstrongly-uncoupled setting where the opponent's number of actions is known,\nboth the social and individual regret bounds can be improved to $O(\\sqrt{\\log m\n\\log n})$. In this analysis, we express the regret upper bound as an\noptimization problem with respect to the learning rates and the coefficients of\ncertain negative terms, enabling refined analysis of the leading constants. We\nthen show that the existing social regret bound as well as these new social and\nindividual regret upper bounds cannot be further improved for optimistic Hedge\nby providing algorithm-dependent individual regret lower bounds. Importantly,\nthese social regret upper and lower bounds match exactly including the constant\nfactor in the leading term. Finally, building on these results, we improve the\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\ndynamic regret lower bounds that match the improved bounds."}
{"id": "2510.11696", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11696", "abs": "https://arxiv.org/abs/2510.11696", "authors": ["Wei Huang", "Yi Ge", "Shuai Yang", "Yicheng Xiao", "Huizi Mao", "Yujun Lin", "Hanrong Ye", "Sifei Liu", "Ka Chun Cheung", "Hongxu Yin", "Yao Lu", "Xiaojuan Qi", "Song Han", "Yukang Chen"], "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs", "comment": "Code is available at https://github.com/NVlabs/QeRL", "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs."}
{"id": "2510.11709", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11709", "abs": "https://arxiv.org/abs/2510.11709", "authors": ["Edward Stevinson", "Lucas Prieto", "Melih Barsbey", "Tolga Birdal"], "title": "Adversarial Attacks Leverage Interference Between Features in Superposition", "comment": null, "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."}
{"id": "2510.11711", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11711", "abs": "https://arxiv.org/abs/2510.11711", "authors": ["Sanghyeok Choi", "Sarthak Mittal", "Víctor Elvira", "Jinkyoo Park", "Nikolay Malkin"], "title": "Reinforced sequential Monte Carlo for amortised sampling", "comment": "code: https://github.com/hyeok9855/gfn-smc-jax", "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods."}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines."}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks."}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["João Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations."}
