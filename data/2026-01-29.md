<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 19]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 113]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: 提出一种新型位截断存储器，支持运行时任意位数截断，为近似应用提供灵活的质量-功耗权衡，在视频处理和深度学习应用中实现显著功耗节省。


<details>
  <summary>Details</summary>
Motivation: 现有位截断存储器需要为特定应用定制设计，缺乏灵活性。需要一种能适应不同近似应用、支持运行时任意位数截断的通用存储器，以优化边缘环境中的功耗效率。

Method: 开发具有完全适应灵活性的位截断存储器，支持运行时截断任意数据位数。将该存储器应用于两种数据密集型近似应用：视频处理和深度学习，包括亮度感知、内容感知和感兴趣区域感知三种视频应用场景。

Result: 相比现有技术，在视频应用中实现高达47.02%的功耗节省；在深度学习模型中（包括基准模型和剪枝轻量模型）实现高达51.69%的功耗节省，且实现成本低（仅2.89%的硅面积开销）。

Conclusion: 提出的位截断存储器具有完全适应灵活性，能有效支持多种近似应用的运行时质量-功耗权衡，显著提升功率效率，适合边缘环境部署。

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [2] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: 提出基于黄金比例的双环磨损均衡方法，无需硬件修改即可延长内存寿命


<details>
  <summary>Details</summary>
Motivation: 内存寿命延长对电子废物和可持续发展很重要，内存磨损不均衡是主要障碍，特别是新兴内存（如相变存储器）寿命更短，现有解决方案要么需要复杂硬件扩展，要么仅适用于特定程序结构

Method: 提出双环磨损均衡方法，灵感来自黄金比例和花瓣均匀接收阳光的自然规律，将内存建模为两个环，结合现有内存管理和垃圾回收技术

Result: 该方法具有确定性，能自动适应内存大小，无需硬件更改，且不会增加程序执行延迟

Conclusion: 双环磨损均衡提供了一种有效减少内存磨损、延长内存寿命的解决方案

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [3] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: STELLAR：首个利用结构相似性指导LLM生成SystemVerilog断言的框架，通过RTL块的AST结构指纹检索相关知识库，显著提升断言质量


<details>
  <summary>Details</summary>
Motivation: 手动编写SystemVerilog断言(SVA)过程缓慢且易出错，现有LLM方法要么从零生成断言，要么忽略硬件设计中的结构模式和专家编写的断言模式

Method: STELLAR将RTL块表示为AST结构指纹，从知识库中检索结构相关的(RTL, SVA)对，并将它们集成到结构引导的提示中指导LLM生成断言

Result: 实验表明STELLAR在语法正确性、风格对齐和功能正确性方面表现优异，突显了结构感知检索在工业形式验证中的潜力

Conclusion: 结构感知检索是工业形式验证的有前景方向，STELLAR框架通过利用结构相似性显著提升了LLM生成SystemVerilog断言的质量

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [4] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: DABench-LLM是首个针对数据流AI加速器的LLM训练基准测试框架，通过芯片内性能分析和芯片间可扩展性分析，在Cerebras、SambaNova、Graphcore等硬件上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的指数级增长超越了传统CPU/GPU架构的能力，而数据流AI加速器虽然前景广阔，但缺乏深入的性能分析和标准化基准测试方法。

Method: 开发DABench-LLM框架，结合芯片内性能分析和芯片间可扩展性分析，评估资源分配、负载均衡、资源效率等关键指标。

Result: 在Cerebras WSE-2、SambaNova RDU、Graphcore IPU三种商用数据流加速器上验证了框架的有效性，揭示了性能瓶颈并提供了具体优化策略。

Conclusion: DABench-LLM是首个针对数据流加速器的LLM训练基准测试框架，具有通用性和有效性，能帮助研究人员快速理解硬件行为并指导性能优化。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [5] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: 通过物理感知的硬件建模和重新训练，可以在存在显著非理想性的硅基模拟神经网络中完全恢复理想模型的推理精度，无需昂贵的校准和保守设计。


<details>
  <summary>Details</summary>
Motivation: 硅基模拟神经网络虽然物理上近似实现了理想神经网络模型，但存在非理想性（如电容串扰和位线压降）。传统方法通过昂贵的校准和保守设计来提高保真度，但成本高、扩展性差。本文旨在开发一种更高效的方法。

Method: 提出了一个物理感知的硬件感知模型，用于基于单晶体管浮栅存储单元的时间域向量矩阵乘法器。该模型明确考虑了电容串扰和位线压降两种主要非理想性，通过自适应时间槽离散化操作、并行处理激活模式、累积贡献来预测有效乘法器输出。使用16x16硅阵列测量校准模型，并改进了权重提取程序。

Result: 模型校准显示串扰是布局依赖且通常占主导地位。改进的权重提取程序使信噪比相比理想向量矩阵乘法器模型提高了一倍。通过在正向传播中使用硬件感知模型训练硅基模拟神经网络，可以在三种架构（自定义MLP、LeNet-5、VGG风格CNN）上完全恢复理想软件网络的准确率。

Conclusion: 通过物理感知的硬件建模和重新训练，可以完全恢复模拟神经网络的推理精度，无需昂贵的校准和保守设计。这为时间域模拟神经形态芯片建立了完整的设计到部署工作流程，具有更好的可扩展性和集成密度。

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [6] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC是一种基于生成式Transformer的近似电路设计模型，通过集成误差阈值到设计过程中，在误差约束下进一步减少6.4%的面积，同时速度快4.3倍。


<details>
  <summary>Details</summary>
Motivation: 针对容错应用，近似电路通过引入可控误差显著改善电路的性能、功耗和面积（PPA）。传统方法在平衡误差约束和PPA优化方面存在挑战，需要更智能的设计方法。

Method: 提出GTAC，一种基于生成式Transformer的模型，创新性地将误差阈值集成到设计过程中，利用近似计算和AI驱动的EDA原理。

Result: 与最先进方法相比，GTAC在误差率约束下进一步减少6.4%的面积，同时速度快4.3倍。

Conclusion: GTAC展示了生成式Transformer在近似电路设计中的有效性，为AI驱动的EDA提供了有前景的方向，能够更好地平衡误差约束和PPA优化。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [7] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: RAPID-Graph是一个协同设计的存内计算系统，通过算法、架构和设备级优化来加速全对最短路径计算，相比GPU集群和现有PIM加速器实现了显著的性能和能效提升。


<details>
  <summary>Details</summary>
Motivation: 全对最短路径计算在大规模图分析中面临数据移动的立方复杂度瓶颈，传统内存层次结构的带宽无法满足需求，需要新的计算范式来突破这一限制。

Method: 采用协同设计方法：算法层面引入递归感知分区器将图分解为顶点瓦片，支持完全原地计算；架构层面设计2.5D PIM堆栈，集成相变存储器计算芯片、逻辑芯片和高带宽暂存器；设备层面使用非易失性存储持久保存结果。

Result: 在2.45M节点的OGBN-Products数据集上，相比最先进的GPU集群快5.8倍，能效高1186倍；相比现有PIM加速器快8.3倍，能效高104倍；相比NVIDIA H100 GPU最高快42.8倍，节能392倍。

Conclusion: RAPID-Graph通过算法-架构-设备的协同设计，有效解决了全对最短路径计算的数据移动瓶颈，为大规模图分析提供了高性能、高能效的存内计算解决方案。

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [8] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: CHIME是一个基于chiplet的异构近内存加速器，用于边缘设备上的多模态大语言模型推理，通过结合M3D DRAM和RRAM芯片，显著提升了推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在边缘设备的部署，面临着严格的延迟和能耗约束，特别是多模态LLMs的高维视觉输入会生成大量token序列，导致KV缓存膨胀和数据移动开销巨大。

Method: CHIME采用chiplet异构架构，结合M3D DRAM（提供低延迟带宽用于注意力计算）和RRAM（提供高密度非易失存储用于权重），并通过协同设计的映射框架执行近数据融合内核，最小化跨chiplet流量。

Result: 在FastVLM和MobileVLM模型上，相比NVIDIA Jetson Orin NX边缘GPU，CHIME实现了最高54倍加速和246倍能效提升；相比最先进的PIM加速器FACIL，吞吐量提升69.2倍；相比纯M3D DRAM设计，能效提升7%，性能提升2.4倍。

Conclusion: CHIME通过异构内存架构和近数据计算，有效解决了边缘MLLM推理中的带宽和能耗瓶颈，为边缘设备上的高效多模态AI推理提供了有前景的解决方案。

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [9] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: KV缓存卸载技术通过将缓存存储在CPU DRAM中实现长上下文LLM推理，但PCIe带宽限制造成严重瓶颈。本文开发分析框架推导关键缓存-预填充令牌比例κ_crit，发现典型工作负载远超此阈值，99%延迟来自数据传输，GPU仅消耗28%额定TDP，提出硬件互连、模型架构和调度算法优化方案。


<details>
  <summary>Details</summary>
Motivation: KV缓存卸载技术虽然能支持长上下文LLM推理，但面临PCIe带宽限制的严重瓶颈问题。当前方法中数据传输开销巨大，导致GPU利用率低下，需要系统性优化解决方案。

Method: 开发分析框架推导关键缓存-预填充令牌比例κ_crit，通过实证表征分析延迟分布和GPU功耗，识别瓶颈所在，为优化提供理论基础。

Result: 发现典型工作负载远超κ_crit阈值，99%延迟来自数据传输，GPU仅消耗28%额定TDP，表明系统严重受限于内存带宽而非计算能力。

Conclusion: KV缓存卸载面临严重PCIe带宽瓶颈，需要从硬件互连（如更高带宽接口）、模型架构（减少缓存需求）和调度算法（优化数据传输）三方面进行协同优化。

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [10] [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911)
*Ilsun Chang*

Main category: cs.AR

TL;DR: 提出混合架构，在向量化执行基础上选择性将高影响原语卸载到GPU，通过仅传输键值和延迟物化减少数据移动，引入风险感知门控机制基于输入大小、传输成本等因素动态决策是否卸载。


<details>
  <summary>Details</summary>
Motivation: 现代OLAP系统通过存储计算分离和列式布局缓解了I/O瓶颈，但在执行层（特别是Top-K选择和连接探测）的CPU成本成为新的规模瓶颈，需要更高效的处理方案。

Method: 1) 混合架构：在现有向量化执行基础上选择性卸载高影响原语到GPU；2) 仅传输键值和指针，采用延迟物化减少数据移动；3) 引入风险感知门控机制，基于输入大小、传输成本、内核执行时间、后处理成本和候选集复杂度(K, M)动态决策是否卸载。

Result: 使用PostgreSQL微基准测试和GPU代理测量，相比始终开启的GPU卸载，门控卸载改善了尾部延迟(P95/P99)，证明了风险感知门控在OLAP执行层原语中的有效性。

Conclusion: 该工作将风险感知门控原则从优化器阶段的GPU辅助测量扩展到执行层OLAP原语，为混合CPU-GPU架构提供了有效的选择性卸载策略，平衡了性能收益和数据移动成本。

Abstract: Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.

</details>


### [11] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: 首次对LLM推理进行指令级故障注入研究，揭示模型架构、参数规模和任务复杂度对可靠性的影响


<details>
  <summary>Details</summary>
Motivation: LLM计算和内存需求高，GPU易受软错误影响，但现有研究主要关注通用应用或传统神经网络，缺乏对大规模LLM的系统性可靠性分析

Method: 采用指令级故障注入方法，从多个角度分析LLM推理的可靠性特征

Result: 揭示了模型架构、参数规模和任务复杂度对LLM可靠性的影响，提供了关于LLM可靠性的新见解

Conclusion: 研究填补了LLM可靠性分析空白，为设计更有效的容错机制提供依据

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [12] [PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator](https://arxiv.org/abs/2601.19920)
*Yuval Harary,Almog Sharoni,Esteban Garzón,Marco Lanuzza,Adam Teman,Leonid Yavits*

Main category: cs.AR

TL;DR: PiC-BNN是一个真正的端到端二进制神经网络加速器，使用汉明距离容错的内容可寻址存储器，无需全精度操作即可实现准确分类。


<details>
  <summary>Details</summary>
Motivation: 传统BNN虽然对线性层进行二值化，但仍需全精度实现其他层（如批归一化、softmax等），限制了面积和能耗优势，且需要全精度操作支持。

Method: 提出PiC-BNN，基于汉明距离容错的内容可寻址存储器，利用大数定律实现准确分类，无需全精度操作，实现真正的端到端二进制神经网络加速器。

Result: 在65nm工艺中实现，在MNIST数据集上达到95.2%准确率，在手势数据集上达到93.5%准确率，吞吐量560K推理/秒，能效703M推理/秒/W。

Conclusion: PiC-BNN通过汉明距离容错机制实现了真正的端到端二进制神经网络加速，消除了全精度操作需求，显著提升了能效和吞吐量。

Abstract: Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.

</details>


### [13] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: Bench4HLS：首个专门用于评估LLM生成HLS设计的综合基准测试框架，包含170个手动验证的案例，支持自动化编译、功能验证和PPA分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在RTL设计方面展现强大能力，HLS领域的应用逐渐增多但缺乏专门的评估框架。当前HLS与RTL研究比例从1:10提升到2:10，显示HLS应用增长趋势，需要系统化的基准测试方法来评估LLM在HLS设计中的表现。

Method: 构建包含170个手动起草和验证案例的Bench4HLS框架，涵盖从简单内核到复杂加速器的设计。支持自动化评估编译成功率、通过仿真的功能正确性、综合可行性/优化。关键创新是集成可插拔API，支持跨不同HLS工具链和架构的PPA（功耗、性能、面积）分析。

Result: 开发了首个专门针对LLM生成HLS设计的基准测试框架，已在Xilinx Vitis HLS上演示并验证了Catapult HLS的兼容性。提供了结构化、可扩展、即插即用的测试平台。

Conclusion: Bench4HLS为HLS工作流中LLM的基准测试建立了基础方法论，填补了该领域评估框架的空白，支持未来LLM在HLS设计中的系统化评估和比较。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [14] [Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification](https://arxiv.org/abs/2601.20061)
*Dhruv Parikh,Jebacyril Arockiaraj,Viktor Prasanna*

Main category: cs.AR

TL;DR: 本文提出了一种基于超维计算（HDC）的图像编码算法和端到端FPGA加速器，实现了高精度图像分类和显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 超维计算使用高维低精度向量进行学习推理，具有轻量级和噪声容忍特性，但在传统CPU/GPU上执行时存在维度高、稀疏性、数据移动频繁等问题，导致资源利用率低、内存瓶颈和实时性能受限。

Method: 1. 开发类似卷积神经网络的图像编码算法，将局部图像块映射到富含空间信息的超向量，然后通过HDC基本操作合并为全局表示；2. 设计端到端FPGA加速器，通过流水线架构在超向量维度和图像块集上实现并行计算。

Result: 图像编码器在MNIST上达到95.67%准确率，在Fashion-MNIST上达到85.14%，优于现有HDC图像编码器；FPGA实现实现0.09ms推理延迟，比最优CPU和GPU基线分别快1300倍和60倍。

Conclusion: 提出的HDC图像编码算法和FPGA加速器有效解决了传统处理器上的性能瓶颈，实现了高精度和实时推理，为边缘设备上的高效图像处理提供了可行方案。

Abstract: Hyperdimensional Computing (HDC) represents data using extremely high-dimensional, low-precision vectors, termed hypervectors (HVs), and performs learning and inference through lightweight, noise-tolerant operations. However, the high dimensionality, sparsity, and repeated data movement involved in HDC make these computations difficult to accelerate efficiently on conventional processors. As a result, executing core HDC operations: binding, permutation, bundling, and similarity search: on CPUs or GPUs often leads to suboptimal utilization, memory bottlenecks, and limits on real-time performance. In this paper, our contributions are two-fold. First, we develop an image-encoding algorithm that, similar in spirit to convolutional neural networks, maps local image patches to hypervectors enriched with spatial information. These patch-level hypervectors are then merged into a global representation using the fundamental HDC operations, enabling spatially sensitive and robust image encoding. This encoder achieves 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based image encoders. Second, we design an end-to-end accelerator that implements these compute operations on an FPGA through a pipelined architecture that exploits parallelism both across the hypervector dimensionality and across the set of image patches. Our Alveo U280 implementation delivers 0.09ms inference latency, achieving up to 1300x and 60x speedup over state-of-the-art CPU and GPU baselines, respectively.

</details>


### [15] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: 本文提出了一种新的多级优先级编码器设计范式，通过级联和组合技术扩展到三、四级，分析了复杂度与延迟的权衡，为硬件设计者提供架构选择建议。


<details>
  <summary>Details</summary>
Motivation: 传统优先级编码器在高位宽（如512位以上）时硬件复杂度高，限制了其在高速整数运算和内容可寻址存储器等关键应用中的使用。如果能降低复杂度，优先级编码器可以加速多种重要应用。

Method: 提出新的优先级编码器设计范式，将已有的两级结构推广到三、四级，采用级联和组合两种技术，并讨论进一步泛化。分析新设计和现有设计（传统单级、树形、递归、两级结构）在FPGA和ASIC实现中的复杂度和延迟特性。

Result: 两级架构在复杂度和延迟间提供平衡，复杂度降低约一半但延迟相应增加。更多级别收益递减，凸显复杂度与延迟的权衡。树形和递归设计通常更快，但比两级和多级结构更复杂。分析了不同输入长度下的设计特性，提供了基于设计优先级（复杂度或延迟）的架构选择建议。

Conclusion: 通过全面分析和比较各种优先级编码器架构，为硬件设计者提供了优先级编码器工具包，帮助根据具体输入长度、实现技术和设计重点（复杂度或延迟）选择最优设计。

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [16] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 该研究分析了NVIDIA数据中心GPU从2000年代中期至今的技术演进趋势，量化了计算性能、内存带宽、价格和功耗的增长速率，并评估了美国出口管制对AI芯片性能差距的影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在AI等关键领域的广泛应用，了解其技术发展轨迹对于预测未来科学研究的约束条件至关重要。特别是在美国实施AI芯片出口管制的背景下，分析GPU技术进展具有重要的现实意义。

Method: 研究构建了NVIDIA数据中心GPU的全面数据集，涵盖计算性能、价格等多方面特征。通过分析主要GPU特征的趋势，估算了每内存带宽、每美元和每瓦特的增长速率指标。

Result: FP16和FP32操作的倍增时间为1.44-1.69年，FP64为2.06-3.79年。内存大小和带宽增长较慢（3.32-3.53年倍增）。价格每5.1年翻倍，功耗每16年翻倍。出口管制可能将性能差距从23.6倍缩小到3.54倍。

Conclusion: GPU计算性能增长远快于内存带宽和价格增长，这可能导致内存墙问题。美国出口管制政策可能显著缩小国际AI芯片性能差距，对全球AI竞争格局产生重要影响。

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [17] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: SATA提出一种面向局部性的动态调度方案，通过重新排序操作数流和利用数据局部性，优化选择性注意力中稀疏访问模式的硬件实现效率。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制的二次方复杂度给硬件实现带来挑战，选择性token注意力虽然能减少计算量，但其稀疏访问模式会导致硬件利用率低下，需要专门的调度方案来优化。

Method: 提出SATA（局部性中心的动态调度方案），主动管理选择性Query-Key操作产生的稀疏访问模式，通过重新排序操作数流、利用数据局部性，实现中间Query/Key向量的早期获取和释放。

Result: 实验结果显示，该方法在基于选择性注意力的模型上，系统吞吐量提升最高达1.76倍，能效提升2.94倍，调度开销极小。

Conclusion: SATA通过高效的动态调度策略，显著提升了选择性注意力机制的硬件实现效率，为Transformer模型的高效硬件部署提供了有效解决方案。

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [18] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: VersaQ-3D是一个算法-架构协同设计框架，通过无校准的4位量化解决VGGT模型部署中的内存和计算挑战，同时设计专用加速器实现高效即时3D重建。


<details>
  <summary>Details</summary>
Motivation: VGGT虽然能实现无需逐场景优化的前馈3D重建，但其十亿参数规模导致高内存和计算需求，阻碍了设备端部署。现有LLM量化方法在VGGT上失效，且存在硬件挑战。

Method: 提出VersaQ-3D框架：算法上采用无校准、场景无关的4位量化，利用正交变换去相关特征并抑制异常值；架构上设计可重构加速器，支持BF16、INT8和INT4，采用统一脉动数据路径和两阶段重计算分块技术。

Result: 在W4A8配置下保持98-99%准确率，在W4A4配置下比现有方法提升1.61x-2.39x。加速器相比边缘GPU实现5.2x-10.8x加速，功耗低，支持高效即时3D重建。

Conclusion: VersaQ-3D成功解决了VGGT量化难题，通过算法-架构协同设计实现了高效、准确的设备端3D重建，为大规模视觉几何模型的部署提供了可行方案。

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>


### [19] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 本文提出针对扩散大语言模型采样阶段的NPU架构优化方案，通过专用指令集和内存层次设计，相比GPU实现2.53倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的采样阶段占推理延迟高达70%，主要由于词汇表级logits的内存读写、基于归约的token选择以及迭代掩码更新等操作，这些操作需要大量片上SRAM且涉及不规则内存访问，传统NPU难以高效处理。

Method: 识别出NPU架构必须专门优化的关键指令集，采用轻量级非GEMM向量原语、原地内存重用策略和分离的混合精度内存层次结构。

Result: 在等效纳米技术节点下，相比NVIDIA RTX A6000 GPU实现最高2.53倍加速，并开源了周期精确模拟和后综合RTL验证代码。

Conclusion: 针对dLLM采样阶段的专用NPU架构优化能显著提升推理效率，为解决扩散模型采样瓶颈提供了硬件解决方案。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [20] [Colored Markov Modulated Fluid Queues](https://arxiv.org/abs/2601.20537)
*Benny Van Houdt*

Main category: cs.PF

TL;DR: 提出了一种新的彩色马尔可夫调制流体队列（colored MMFQs）及其带流体跳跃的变体，通过引入颜色标记来追踪流体进入时的状态，增强了建模灵活性，解决了传统方法面临的维度灾难和状态空间爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫调制流体队列（MMFQs）虽然能有效分析计算机和通信系统的性能，但缺乏对流体历史状态的追踪能力。当需要分析某些事件发生时流体水平的状态时，传统MMFQs面临维度灾难和状态空间爆炸的挑战，限制了其建模灵活性。

Method: 通过引入"颜色"概念来扩展传统MMFQ框架，创建了彩色MMFQs和带流体跳跃的彩色MMFQs。颜色标记允许追踪流体进入时的状态，为系统提供了额外的记忆能力，从而能够记录特定事件发生时的流体水平。

Result: 提出的彩色MMFQ框架显著增强了建模灵活性，能够分析原本由于维度灾难或状态空间爆炸而难以处理的排队系统。这种扩展使得MMFQs能够应用于更广泛的系统性能分析场景。

Conclusion: 彩色MMFQs及其带跳跃的变体为系统性能建模提供了更强大的框架，通过引入颜色标记来追踪流体历史状态，解决了传统方法的局限性，为复杂排队系统的分析开辟了新途径。

Abstract: Markov-modulated fluid queues (MMFQs) are a powerful modeling framework for analyzing the performance of computer and communication systems. Their distinguishing feature is that the underlying Markov process evolves on a continuous state space, making them well suited to capture the dynamics of workloads, energy levels, and other performance-related quantities. Although classical MMFQs do not permit jumps in the fluid level, they can still be applied to analyze a wide range of jump processes.
  In this paper, we generalize the MMFQ framework in a new direction by introducing {\bf colored MMFQs} and {\bf colored MMFQs with fluid jumps}. This enriched framework provides an additional form of memory: the color of incoming fluid can be used to keep track of the fluid level when certain events took place. This capability greatly enhances modeling flexibility and enables the analysis of queueing systems that would otherwise be intractable due to the curse of dimensionality or state-space explosion.

</details>


### [21] [The Multiserver-Job Stochastic Recurrence Equation for Cloud Computing Performance Evaluation](https://arxiv.org/abs/2601.20653)
*Francois Baccelli,Diletta Olliaro,Marco Ajmone Marsan,Andrea Marin*

Main category: cs.PF

TL;DR: 提出基于随机递归方程和遍历理论的多服务器作业排队模型分析方法，证明了单调性和可分离性，开发了子完美采样算法和稳定性条件估计算法，支持GPU并行化并扩展到复杂系统。


<details>
  <summary>Details</summary>
Motivation: 研究多服务器作业排队模型（MJQM）在FCFS调度下的性能分析，传统方法难以处理一般独立到达和服务时间的复杂排队系统，需要开发高效的分析和仿真工具。

Method: 使用随机递归方程和遍历理论分析MJQM，证明系统的单调性和可分离性，应用Loynes定理的单调可分离扩展，开发子完美采样算法和稳定性条件估计算法，支持GPU并行化。

Result: 证明了MJQM的单调性和可分离性，定义了形式化的稳定性条件，实现了高效的子完美采样算法和稳定性估计算法，GPU并行化显著提升了性能评估效率，方法可扩展到资源类型化系统。

Conclusion: 提出的随机递归方程和遍历理论方法为多服务器作业排队模型提供了严格的分析框架，开发的算法工具能够高效评估系统性能，GPU并行化大幅提升计算效率，方法具有很好的扩展性。

Abstract: We study the Multiserver-Job Queuing Model (MJQM) with general independent arrivals and service times under FCFS scheduling, using stochastic recurrence equations (SREs) and ergodic theory. We prove the monotonicity and separability properties of the MJQM SRE, enabling the application of the monotone-separable extension of Loynes' theorem and the formal definition of the MJQM stability condition. Based on these results, we introduce and implement two algorithms: one for drawing sub-perfect samples (SPS) of the system's workload and the second one to estimate the system's stability condition given the statistics of the jobs' input stream. The SPS algorithm allows for a massive GPU parallelization, greatly improving the efficiency of performance metrics evaluation. We also show that this approach extends to more complex systems, including MJQMs with typed resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [22] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: 提出一种基于数据驱动的科学数据压缩方法Discontinuous DLS，利用局部时空子空间提高压缩效率，在保证误差边界的前提下显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据量快速增长带来存储和传输挑战，需要误差有界压缩技术来减少数据大小同时保证重建数据对科学分析的有效性。

Method: Discontinuous DLS压缩器利用数据结构的局部时空子空间，采用数据驱动方法而非数据无关压缩，在分布式计算环境中使用MPI实现。

Result: 相比最先进的误差有界压缩方法，在压缩比和重建精度方面表现优异，显著减少存储需求而不影响关键数据保真度。

Conclusion: Discontinuous DLS是高性能计算环境中大规模科学数据压缩的有前景方法，为现代科学模拟日益增长的数据需求提供稳健解决方案。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [23] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion是一个针对Diffusion Transformers的高效服务引擎，通过拓扑感知序列并行、Torus Attention和单边通信技术，解决了现有并行方法在通信模式、延迟和同步开销方面的限制，性能提升达1.35-1.77倍。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像和长视频生成需求的增长，单GPU推理效率低下，现有序列并行技术存在通信模式不优化、跨机器all-to-all操作延迟瓶颈、以及GPU双向通信同步开销大三个主要问题。

Method: StreamFusion包含三个关键技术：1) 考虑机器间和机器内带宽差异的拓扑感知序列并行；2) Torus Attention，一种允许跨机器all-to-all操作与计算重叠的新型SP技术；3) 最小化GPU发送-接收同步和计算开销的单边通信实现。

Result: 实验表明，StreamFusion平均性能优于现有最优方法1.35倍，最高可达1.77倍。

Conclusion: StreamFusion通过优化通信模式、重叠通信与计算、减少同步开销，显著提升了Diffusion Transformers的推理效率，为高分辨率图像和长视频生成提供了高效的服务引擎。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [24] [SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips](https://arxiv.org/abs/2601.20309)
*Jiahuan Yu,Mingtao Hu,Zichao Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: SuperInfer：基于Superchips（如NVIDIA GH200）的高性能LLM推理系统，通过RotaSched调度器和DuplexKV引擎解决KV缓存不足时的延迟SLO问题


<details>
  <summary>Details</summary>
Motivation: LLM服务面临严格的延迟SLO与有限的GPU内存容量之间的根本矛盾。当高请求率耗尽KV缓存预算时，现有LLM推理系统常遭受严重的队头阻塞。虽然先前工作探索了基于PCIe的卸载方案，但这些方法无法在高请求率下保持响应性，往往无法满足严格的TTFT和TBT SLO要求。

Method: SuperInfer针对采用NVLink-C2C紧密耦合GPU-CPU架构的Superchips设计，引入：1）RotaSched - 首个主动的、SLO感知的旋转调度器，通过旋转请求在Superchips上保持响应性；2）DuplexKV - 优化的旋转引擎，支持在NVLink-C2C上实现全双工传输。

Result: 在GH200上使用各种模型和数据集的评估显示，SuperInfer将TTFT SLO达成率提高了高达74.7%，同时保持与最先进系统相当的TBT和吞吐量。

Conclusion: SLO感知调度和内存协同设计释放了Superchips在响应式LLM服务中的全部潜力，SuperInfer展示了通过利用新兴Superchips架构特性来显著改善LLM推理系统响应性的有效方法。

Abstract: Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.

</details>


### [25] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出统一的多任务争用分类框架，通过表示变换、图结构建模和任务解耦机制，在高维系统环境中准确识别多种争用类型。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统环境中准确识别多种任务争用类型的挑战，为复杂计算环境的性能管理提供可靠技术方案。

Method: 1) 从高维指标序列构建系统状态表示，进行非线性变换提取跨维度动态特征；2) 引入基于图的建模机制捕捉指标间潜在依赖关系；3) 设计任务特定映射结构区分不同争用类型；4) 采用自适应多任务损失权重策略平衡共享和特定特征学习。

Result: 在公开系统跟踪数据集上的实验显示，在准确率、召回率、精确率和F1分数方面具有优势，对批量大小、训练样本规模和指标维度的敏感性分析进一步证实了模型的稳定性和适用性。

Conclusion: 基于高维指标的结构化表示和多任务分类能显著提高争用模式识别能力，为复杂计算环境的性能管理提供了可靠技术途径。

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [26] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT是一个分布式LLM优化框架，通过自动化复杂优化工作流程，使非专业团队能够进行模型压缩和调优，在GPU受限环境下实现2倍以上的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 企业LLM部署面临关键的可扩展性挑战：组织需要在有限的计算预算内系统化优化模型，但手动优化所需的专业知识和技能稀缺且难以获取。特别是在异构基础设施中管理GPU利用率，同时让具有多样化工作负载和有限LLM优化经验的团队能够高效部署模型。

Method: OptiKIT是一个分布式LLM优化框架，提供动态资源分配、分阶段流水线执行与自动清理、以及无缝企业集成。平台设计包括资源分配算法、流水线编排和集成模式，实现大规模、生产级的模型优化民主化。

Result: 在生产环境中，OptiKIT实现了超过2倍的GPU吞吐量提升，同时使应用团队能够在没有深厚LLM优化专业知识的情况下获得一致的性能改进。

Conclusion: OptiKIT成功解决了企业LLM部署中的可扩展性挑战，通过自动化优化工作流程使模型压缩和调优民主化。该系统已开源，以促进外部贡献和更广泛的可复现性。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [27] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: 提出用户空间调度框架USF和SCHED_COOP调度策略，通过在用户空间实现进程调度，减少操作系统调度器在过载情况下的干扰，提升并行应用性能。


<details>
  <summary>Details</summary>
Motivation: HPC与AI融合导致复杂并行应用增多，多个并行运行时共存给传统OS调度器带来压力。过载时OS调度器的周期性抢占会引入干扰，降低性能。

Method: 开发用户空间调度框架USF，完全在用户空间实现进程调度，无需特殊权限。提出SCHED_COOP协作调度策略，仅在线程阻塞时切换，避免不必要的抢占。通过扩展GNU C库和nOS-V运行时实现，支持多个运行时协调。

Result: 在过载多进程场景下性能提升最高达2.4倍，包括嵌套BLAS工作负载、多进程PyTorch推理（LLaMA-3）和分子动力学模拟。

Conclusion: USF和SCHED_COOP能有效减少调度干扰，解决锁持有者抢占、锁等待者抢占和可扩展性崩溃等问题，显著提升复杂并行应用的性能。

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [28] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap是一个编译器与运行时系统，通过通信块抽象实现单核内细粒度计算与通信重叠，相比现有粗粒度流级重叠方法，在多GPU工作负载上平均获得1.3倍加速，最高达4.7倍。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU工作负载中通信已成为首要瓶颈，现有分布式编译器主要在流级别重叠整个计算与通信内核，这种粗粒度方法导致额外内核启动、强制设备级同步，并在最慢的瓦片或内核延长通信尾部时产生大量空闲时间。

Method: 提出通信块抽象，将通信粒度与内核结构和后端机制解耦；作为Triton上的源到源编译器，给定本地Triton内核和块调度计划，执行转换以使计算与块可用性对齐；支持从现有分布式编译器移植、用户直接编写或从可重用模板实例化块级计划。

Result: 在多GPU工作负载上实现平均1.3倍端到端加速，最高可达4.7倍加速。

Conclusion: AutoOverlap通过细粒度重叠通信与计算，有效解决了大规模GPU工作负载中的通信瓶颈问题，相比现有粗粒度方法显著提升了性能。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [29] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece是一个针对多阶段AIGC工作流优化的分布式推理系统，通过RDMA通信和微服务架构显著提升吞吐量和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC系统在处理并发工作负载时面临吞吐量、资源利用率和可扩展性的效率瓶颈，需要更高效的分布式推理解决方案。

Method: 1) 将流水线分解为细粒度微服务；2) 利用单边RDMA通信减少节点间延迟和CPU开销；3) 采用双环形缓冲区设计解决RDMA内存访问死锁；4) 动态节点管理器根据实时负载弹性分配资源。

Result: 在Wan2.1图像到视频生成任务中，相比单体推理流水线，GPU资源消耗减少16倍，显著提升了系统效率和可扩展性。

Conclusion: OnePiece为生产环境AIGC提供了一个可扩展、容错且高效的分布式推理系统解决方案，通过RDMA优化和微服务架构解决了现有系统的关键瓶颈。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [30] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: 提出Agentic Fog (AF)模型，将雾节点表示为策略驱动的自主代理，通过基于共享内存和局部协调的p2p交互进行通信，在动态工作负载下实现低延迟和高效适应。


<details>
  <summary>Details</summary>
Motivation: 雾计算和边缘计算需要能够处理部分可观测性、严格延迟要求和动态变化工作负载的自适应控制方案。现有的Agentic AI工具由于计算成本高、随机性强和形式化分析能力差，不适用于基础设施级系统。

Method: 提出Agentic Fog (AF)通用模型，将雾节点表示为策略驱动的自主代理，通过p2p交互基于共享内存和局部协调进行通信。将系统目标分解为抽象策略指导，并将去中心化雾协调形式化为精确势博弈。

Result: AF框架在异步更新、有限理性最佳响应动态和节点故障下保证收敛和稳定。仿真显示AF系统比贪婪启发式算法和整数线性规划在动态条件下实现更低的平均延迟和更高效的适应能力。敏感性分析表明在不同内存和协调条件下都能实现最优性能。

Conclusion: Agentic Fog模型为雾计算和边缘计算提供了一种可形式化分析、保证收敛稳定的自适应控制框架，在动态工作负载下优于传统方法，具有实际应用价值。

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: 提出一种基于概率神经元的智能传感范式，通过概率决策实现数据采样的自主激活，显著提升能效


<details>
  <summary>Details</summary>
Motivation: 传统确定性采样决策存在信息丢失风险，需要一种能在数据采集过程中实现智能决策的方法，以显著提升能效

Method: 受自主神经系统启发，采用概率神经元(p-neuron)和模拟特征提取电路，实现微秒级响应时间的概率决策采样

Result: 在主动地震勘测数据验证中实现无损概率数据采集，归一化均方误差0.41%，系统活跃运行时间和生成样本数减少93%

Conclusion: 该概率传感范式突破了次采样率响应时间限制，实现了实时智能自主数据采样，在保持数据质量的同时显著提升能效

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [32] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: 提出Gap-K%方法，通过分析LLM预训练优化动态，利用top-1预测与目标token之间的对数概率差距，结合滑动窗口捕获局部相关性，实现更准确的预训练数据检测。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练语料库的不透明性引发隐私和版权问题，现有基于token似然的方法忽略了与模型top-1预测的差异以及相邻token之间的局部相关性。

Method: 基于LLM预训练优化动态，利用top-1预测token与目标token之间的对数概率差距，采用滑动窗口策略捕获局部相关性，减少token级波动。

Result: 在WikiMIA和MIMIR基准测试中，Gap-K%达到最先进性能，在不同模型大小和输入长度下均优于现有基线方法。

Conclusion: Gap-K%通过考虑预训练优化动态和局部相关性，为预训练数据检测提供了更有效的方法，解决了现有方法的局限性。

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [33] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS框架通过伪点压缩ML约束和两种选择器（边界规则和信息几何规则）在保持约束满足的同时实现可扩展的高质量聚类


<details>
  <summary>Details</summary>
Motivation: 现有成对约束聚类方法在数据可扩展性方面存在困难，特别是在量子或量子混合聚类等小众应用中，ML和CL约束增加了聚类问题的复杂性

Method: 提出PASS框架：1) 将ML约束压缩为伪点；2) 提供两种选择器：基于边界规则收集近边界点和所有检测到的CL违规，以及基于信息几何规则通过软分配后验的Fisher-Rao距离评分点，然后在简单预算下选择最高信息子集

Result: 在多样化基准测试中，PASS以显著低于精确或基于惩罚方法的成本获得竞争性SSE，并在先前方法失败的场景中保持有效性

Conclusion: PASS框架能够在保持ML和CL约束满足的同时实现可扩展的高质量聚类解决方案，解决了现有方法在数据可扩展性方面的挑战

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [34] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: 提出一种基于二阶信息近似生成共识权重的去中心化联邦学习聚合方法，解决数据和模型异构性问题，提高模型泛化能力并降低通信成本


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中，设备间的个体经验差异和交互程度不同导致数据和模型初始化异质性，造成本地模型参数差异大、收敛速度慢的问题

Method: 通过近似本地模型在本地数据集上的二阶信息生成共识权重，利用这些权重缩放邻居更新，然后聚合为全局邻居表示

Result: 在计算机视觉任务上的大量实验表明，该方法在降低通信成本的同时，显著提高了本地模型的泛化能力

Conclusion: 通过显式处理本地模型参数层面的证据可信度变化，提出了一种能够有效应对数据和模型异质性的鲁棒聚合方法

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [35] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: Oculomix：一种用于视网膜图像分析的层次化采样混合数据增强方法，通过保留患者特定属性来改进心血管事件预测


<details>
  <summary>Details</summary>
Motivation: 现有图像级混合数据增强方法（如CutMix、MixUp）在训练视网膜图像分析模型时会扰动患者特定的医学属性（如合并症、临床因素），因为这些方法只考虑图像和标签，而忽略了患者层次的结构信息。

Method: 提出Oculomix层次化采样策略，基于两个临床先验：1）同一时间点同一患者的图像共享相同属性；2）同一患者不同时间点的图像存在软时间趋势（发病率随时间增加）。该方法将混合空间约束到患者和检查层次，利用层次关系更好地保留患者特定特征。

Result: 在大型多民族人群（Alzeye）中进行5年主要不良心血管事件（MACE）预测验证，使用ViT模型。Oculomix在AUROC上比图像级CutMix和MixUp提升高达3%，表现一致更优。

Conclusion: Oculomix通过层次化采样混合增强方法有效保留了患者特定特征，在眼组学中显示出必要性和价值，为基于视网膜图像预测系统性疾病提供了更好的数据增强策略。

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [36] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,César David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: 提出一种数据流感知的连续流CNN架构设计方法，通过交错低数据率信号和共享硬件单元，在FPGA上实现接近100%的硬件利用率，从而在单FPGA上高效实现复杂CNN如MobileNet。


<details>
  <summary>Details</summary>
Motivation: 在FPGA上实现CNN时，池化层和步长大于1的卷积层会导致输出数据量减少，从而影响数据流并行实现的效率，造成硬件单元利用率低下。现有展开式实现主要关注全连接网络，而CNN在相同精度下需要更少计算量但面临数据率不匹配问题。

Method: 分析CNN的数据流特性，提出数据流感知的连续流CNN架构设计方法。通过交错低数据率信号、共享硬件单元以及适当的并行化策略，确保接近100%的硬件利用率，同时达到完全并行实现的吞吐量。

Result: 该方法显著减少了算术逻辑资源的使用，使得在单个FPGA上实现复杂CNN（如MobileNet）并保持高吞吐量成为可能。

Conclusion: 提出的数据流感知连续流CNN架构设计方法有效解决了FPGA上CNN实现时的硬件利用率问题，通过智能的数据流管理和资源共享，实现了高效、高吞吐量的CNN推理加速。

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [37] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: 论文通过几何和统计物理视角研究Transformer语言模型中多步推理能力的涌现，发现隐藏状态轨迹在隐式黎曼流形上的流动存在相变现象，有效维度在临界深度附近急剧下降，形成稳定的"概念盆地"和可重用的瞬态类对象。


<details>
  <summary>Details</summary>
Motivation: 研究深度Transformer语言模型中多步推理能力如何涌现，从几何和统计物理角度理解隐藏状态演化的内在机制，探索模型规模与推理能力之间的定量关系。

Method: 将隐藏状态轨迹视为隐式黎曼流形上的流动，分析激活的层间协方差谱，追踪与随机矩阵本体的偏差。引入基于稀疏性/局部化的序参数，将前向传播形式化为离散粗粒度映射，建立逻辑可分性与谱衰减的理论联系。

Result: 在1.5B-30B规模模型中发现相变现象：有效维度在临界归一化深度γ_c≈0.42处急剧下降，形成低熵区域，出现谱尾塌缩和可重用的瞬态类对象，验证了理论预测的签名特征。

Conclusion: Transformer语言模型中的多步推理能力通过几何相变机制涌现，形成稳定的概念盆地和可重用表示结构，为理解大规模语言模型的内部工作机制提供了新的理论框架。

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [38] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: 竞争驱动下的涌现专业化：通过竞争动态实现学习者群体的自发专业化分区，无需显式通信或多样性激励


<details>
  <summary>Details</summary>
Motivation: 研究在没有显式通信或多样性激励的情况下，学习者群体如何发展协调且多样化的行为。探索竞争是否足以诱导涌现的专业化现象。

Method: 提出NichePopulation算法，结合竞争排斥和生态位亲和度追踪机制。在六个真实世界领域（加密货币交易、商品价格、天气预报、太阳辐射、城市交通和空气质量）进行验证。

Result: 平均专业化指数达到0.75，效应量Cohen's d > 20。发现：1) 无生态位奖励时仍能实现SI > 0.30的专业化；2) 多样化群体通过方法级分工比同质基线表现提升26.5%；3) 比MARL基线方法（QMIX、MAPPO、IQL）表现好4.3倍且速度快4倍。

Conclusion: 竞争本身足以诱导涌现的专业化，学习者通过竞争动态自发形成针对不同环境机制的专业化分区，符合生态位理论。该方法在多个真实世界应用中表现出色，为群体智能中的专业化发展提供了新视角。

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [39] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel Grønhaug*

Main category: cs.LG

TL;DR: 研究21种分类器在表格数据上的后处理校准方法，发现Venn-Abers和Beta校准效果最好，而常用的Platt缩放和等渗回归可能降低现代表格模型的概率预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究模型无关的后处理校准方法在真实表格数据上的效果，特别关注具有分布无关有效性保证的共形预测和Venn方法，评估这些方法对现代表格模型概率预测的改进效果。

Method: 使用TabArena-v0.1套件中的二元分类任务，对21种分类器（包括线性模型、SVM、树集成模型和现代表格神经网络）进行随机分层五折交叉验证。在单独校准集上训练5种校准器（等渗回归、Platt缩放、Beta校准、Venn-Abers预测器和Pearsonify），然后在测试集上应用。使用对数损失、Brier分数等评分规则以及校准诊断指标进行评估。

Result: Venn-Abers预测器实现最大的平均对数损失减少，Beta校准紧随其后。Beta校准在最多任务中改进对数损失，而Venn-Abers显示出较少极端性能下降和稍多极端改进。常用的Platt缩放和等渗回归可能系统性地降低现代表格模型的评分性能。所有方法（除Pearsonify外）都略微提高准确率，但效果边际。

Conclusion: 校准效果因数据集和模型架构而异，没有单一方法占优。对于现代表格模型，Venn-Abers和Beta校准表现最佳，而传统方法如Platt缩放和等渗回归可能适得其反。校准主要影响概率预测质量，对分类性能影响有限。

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [40] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的视角，通过理论分析损失景观平坦度与标签噪声之间的关系，并提出了噪声补偿的锐度感知最小化(NCSAM)方法，利用SAM的扰动来缓解标签噪声的损害。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常包含错误或损坏的标注（如网络爬取数据），当前研究主要关注复杂的标签校正机制，而本文从理论角度分析损失景观平坦度与标签噪声的关系。

Method: 提出噪声补偿的锐度感知最小化(NCSAM)，利用SAM的扰动来补偿标签噪声的损害，理论证明精心模拟的标签噪声能协同提升泛化性能和标签噪声鲁棒性。

Result: 在多个基准数据集上的广泛实验结果表明，该方法在多样化任务上始终优于现有的最先进方法，测试准确率表现出与无噪声数据集相似的行为。

Conclusion: 通过理论分析和实验验证，证明了损失景观平坦度与标签噪声之间的重要关系，提出的NCSAM方法能有效提升在噪声标签下的学习性能。

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [41] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache：基于平均速度视角的训练免费缓存框架，通过缓存JVP构建区间平均速度，减少轨迹偏差和误差累积，在Flow Matching推理中实现4倍以上加速


<details>
  <summary>Details</summary>
Motivation: 现有缓存方法依赖瞬时速度信息，在高加速比下会导致严重的轨迹偏差和误差累积问题，需要更稳定的缓存策略来提升Flow Matching推理效率

Method: 1) 引入平均速度视角，利用缓存的Jacobian-向量乘积(JVP)从瞬时速度构建区间平均速度；2) 开发轨迹稳定性调度策略，采用预算约束下的峰值抑制最短路径来确定调度方案

Result: 在FLUX.1、Qwen-Image和HunyuanVideo上分别实现4.12倍、4.56倍和3.59倍加速，同时在生成质量上持续优于最先进的缓存基线方法

Conclusion: MeanCache为Flow Matching推理提供了简单有效的新视角，其稳定性驱动的加速方法有望在商业级生成模型中激发进一步探索

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [42] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: TCLA框架通过任务条件化潜在对齐，解决脑机接口中跨会话神经信号非平稳性问题，在有限数据下实现源会话知识向目标会话的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 植入式脑机接口面临跨会话神经活动非平稳性的重大挑战，解码器在一个会话上训练后往往无法泛化到后续会话。当新会话数据有限时，重新训练或适应解码器变得尤为困难。

Method: 提出任务条件化潜在对齐框架(TCLA)，基于自编码器架构：首先从数据充足的源会话学习神经动力学的低维表示；对于数据有限的目标会话，以任务条件化方式将目标潜在表示与源会话对齐，实现学习到的神经动力学的有效迁移。

Result: 在猕猴运动和眼动中心-外数据集上评估TCLA。相比仅使用目标会话数据训练的基线方法，TCLA在不同数据集和解码设置中一致提升解码性能，在运动数据集的y坐标速度解码中，决定系数提升高达0.386。

Conclusion: TCLA提供了从源会话向目标会话迁移知识的有效策略，能够在有限数据条件下实现更稳健的神经解码，为解决脑机接口中的跨会话非平稳性问题提供了有前景的解决方案。

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [43] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: 论文提出了NetCVR（净转化率）预测问题，引入首个大规模开源数据集CASCADE，并提出TESLA框架解决多阶段级联延迟反馈的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 传统转化率（CVR）无法完全反映推荐效果，因为它忽略了退款行为。NetCVR（点击后购买且不退款的概率）能更好捕捉真实用户满意度和商业价值，但面临多阶段级联延迟反馈、缺乏开源数据集和在线持续训练方案等挑战。

Method: 提出TESLA框架：采用CVR-退款率级联架构、分阶段去偏和延迟时间感知排序损失。基于对CASCADE数据集的分析，发现三个关键洞察：NetCVR具有强时序动态性、级联建模优于直接建模、延迟时间是重要特征。

Result: TESLA在CASCADE数据集上显著优于现有方法，在NetCVR预测上实现RI-AUC绝对提升12.41%、RI-PRAUC绝对提升14.94%。

Conclusion: NetCVR预测是工业推荐系统中的重要问题，需要专门的数据集和建模方法。提出的CASCADE数据集和TESLA框架为解决这一挑战提供了有效方案，代码和数据集已开源。

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [44] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 提出PIL方法，使用线性代理模型生成不可学习样本的扰动，显著降低计算成本，同时揭示不可学习样本通过诱导深度模型线性化来发挥作用


<details>
  <summary>Details</summary>
Motivation: 现有不可学习样本方法依赖深度神经网络作为代理模型生成扰动，计算成本高，需要更高效的方法

Method: 提出PIL方法，仅使用线性代理模型生成扰动，通过诱导深度模型线性化来创建不可学习样本

Result: PIL在计算时间大幅减少的同时，达到或优于现有基于代理模型的方法的性能，并分析了基于百分比的部分扰动特性

Conclusion: PIL为数据保护提供了实用方法，揭示了不可学习样本通过诱导模型线性化而有效的机制，为理解不可学习样本提供了新见解

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [45] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: BayPrAnoMeta：一种用于少样本工业图像异常检测的贝叶斯元学习方法，通过概率正态性建模和不确定性感知异常评分，在MVTec AD基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业图像异常检测面临极端类别不平衡和标记缺陷样本稀缺的挑战，特别是在少样本设置中。现有Proto-MAML方法依赖确定性类别原型和基于距离的适应，缺乏对不确定性的建模能力。

Method: 提出BayPrAnoMeta，将Proto-MAML中的原型替换为任务特定的概率正态性模型，使用Normal-Inverse-Wishart先验对正常支持嵌入建模，产生Student-t预测分布进行贝叶斯内环适应。还扩展到具有监督对比正则化的联邦元学习框架，并证明了非凸目标的收敛性。

Result: 在MVTec AD基准测试中，与MAML、Proto-MAML和基于PatchCore的方法相比，在少样本异常检测设置中取得了持续且显著的AUROC改进。

Conclusion: BayPrAnoMeta通过贝叶斯概率建模和不确定性感知异常评分，为极端少样本工业异常检测提供了更稳健的解决方案，并在联邦学习环境中具有实际应用价值。

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [46] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: 本文提出了一种改进的稀疏自编码器方法，用于多模态嵌入空间（如CLIP和CLAP）的分解，通过跨模态随机掩码和组稀疏正则化，解决了传统SAE在多模态场景中产生"分裂字典"的问题，提升了模态对齐和特征语义性。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器（SAEs）应用于多模态嵌入空间（如图像/文本的CLIP嵌入）时，往往学习到"分裂字典"，即大多数稀疏特征本质上是单模态的，只对单一模态数据激活。这限制了在多模态场景下的特征对齐和可解释性。

Method: 提出一种新的SAE方法：1）理论分析表明，对齐嵌入空间上存在分裂字典分解意味着存在具有更好模态对齐的非分裂字典；2）使用跨模态随机掩码和组稀疏正则化来促进多模态字典学习；3）应用于CLIP（图像/文本）和CLAP（音频/文本）嵌入。

Result: 相比标准SAE，该方法学习到更多模态的字典，减少了"死亡神经元"数量，提高了特征语义性。在CLIP和CLAP嵌入上的实验验证了方法的有效性。

Conclusion: 该方法改善了多模态概念对齐，能够提升跨模态任务的可解释性和控制能力，为多模态嵌入分解提供了更有效的解决方案。

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [47] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: 提出StructuralCFN架构，通过可微分结构先验建模特征间的组合关系，在保持可解释性的同时提升表格数据性能，参数量比标准深度模型小10-20倍。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在处理表格数据时难以同时达到梯度提升决策树的性能并保持科学可解释性，且通常将特征视为独立实体，忽略了表格数据固有的流形结构依赖关系。

Method: 提出StructuralCFN架构，通过可微分结构先验引入关系感知归纳偏置，使用可微分自适应门控将每个特征建模为其对应特征的数学组合，自动发现最优激活机制（如注意力式过滤与抑制极性），支持结构化知识集成。

Result: 在18个基准数据集上通过10折交叉验证评估，在科学和临床数据集（如血液输血、臭氧、WDBC）上显示出统计显著改进（p<0.05），参数量仅300-2500个，比标准深度基线小10-20倍。

Conclusion: StructuralCFN通过建模特征间的结构关系，在保持内在符号可解释性的同时提升了表格数据性能，能够以人类可读的数学表达式恢复数据流形的"规律"，为高风险领域的表格数据分析提供了新方法。

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [48] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: TONEL框架通过噪声感知投影模型学习任务特定嵌入，提升边缘设备上RAG在噪声环境下的鲁棒性和领域适应性


<details>
  <summary>Details</summary>
Motivation: 边缘设备上基于LLM的个性化虚拟助手面临效率瓶颈，特别是随着用户交互和更新数据的快速增长。虽然存内计算架构通过原位操作减少了数据移动，但易受环境噪声影响，在动态多领域边缘场景中检索精度下降成为关键问题。

Method: 提出任务导向的噪声鲁棒嵌入学习框架TONEL，采用噪声感知投影模型学习与存内计算硬件约束兼容的任务特定嵌入，实现在噪声条件下的准确检索。

Result: 在个性化基准测试上的广泛实验表明，该方法相对于强基线模型表现出有效性和实用性，特别是在任务特定的噪声场景中。

Conclusion: TONEL框架解决了边缘设备上RAG系统在噪声环境中的鲁棒性和领域适应性问题，为动态多领域边缘场景中的个性化虚拟助手提供了有效解决方案。

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [49] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO：基于狄利克雷过程高斯过程混合的贝叶斯优化方法，用于多区域优化问题


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化假设搜索空间具有均匀平滑性，但在多区域问题（如分子构象搜索、药物发现）中这一假设不成立。单一高斯过程要么过度平滑尖锐转变，要么在平滑区域产生噪声幻觉，导致不确定性校准错误。

Method: 提出RAMBO方法，使用狄利克雷过程混合高斯过程，在优化过程中自动发现潜在区域，每个区域用具有局部优化超参数的独立高斯过程建模。推导了折叠吉布斯采样以解析边缘化潜在函数进行高效推理，并引入自适应浓度参数调度实现从粗到细的区域发现。采集函数将不确定性分解为区域内和区域间分量。

Result: 在合成基准测试和实际应用中（包括分子构象优化、药物发现虚拟筛选、聚变反应堆设计）的实验表明，在多区域目标上相比最先进的基线方法有持续改进。

Conclusion: RAMBO通过自动发现和建模潜在区域，有效解决了多区域优化问题，在多个实际应用中表现出优越性能。

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [50] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: 提出SA-PEF方法，结合步前校正和部分误差反馈，解决联邦学习中非IID数据下梯度压缩导致的早期训练停滞问题，实现更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，带误差反馈的梯度压缩可以减少通信开销，但在非IID数据下，残差误差衰减缓慢，导致梯度不匹配和早期训练轮次进展停滞。

Method: 提出步前部分误差反馈（SA-PEF），结合步前校正和部分误差反馈。当步前系数α=0时恢复为EF，α=1时为步前EF（SAEF）。针对非凸目标和δ-压缩器，建立了二阶矩边界和残差递归分析。

Result: 理论分析表明SA-PEF在异构数据和部分客户端参与下能收敛到平稳点，收敛率与非凸Fed-SGD标准保证匹配，达到O((η,η₀TR)⁻¹)收敛到方差/异构性下界。实验显示SA-PEF比EF更快达到目标精度。

Conclusion: SA-PEF通过步前控制残差收缩解决了早期训练停滞问题，平衡了SAEF的快速预热和EF的长期稳定性，在多种架构和数据集上表现优于传统EF方法。

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [51] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: 开发并验证了用于转移性去势抵抗性前列腺癌（mCRPC）的180天死亡率风险模型，使用纵向数据比较了五种架构，GRU和RSF表现最佳，临床指标如BMI和收缩压最具预测性。


<details>
  <summary>Details</summary>
Motivation: mCRPC是一种高度侵袭性疾病，预后差且治疗反应异质性大，需要开发能够预测短期死亡风险的模型，以支持主动护理规划和临床决策。

Method: 使用两个III期队列的纵向数据（n=526和n=640），开发并外部验证了访问级别的180天死亡率风险模型。比较了五种候选架构：LSTM、GRU、Cox比例风险、随机生存森林和逻辑回归。仅使用可观察180天结果的访问进行标记，右删失病例被排除分析。

Result: GRU和RSF模型最初显示出高区分能力（C-index均为87%）。在外部验证中，GRU获得了更高的校准度（斜率：0.93；截距：0.07），PR-AUC达到0.87。临床影响分析显示真阳性中位预警时间为151.0天（假阳性为59.0天），每100次患者访问产生18.3次警报。BMI和收缩压被确定为最强的预测因子。

Conclusion: 纵向常规临床标志物可以估计mCRPC的短期死亡风险，支持在多个月窗口内进行主动护理规划。GRU模型在外部验证中表现出良好的校准和区分能力，具有临床实用性。

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [52] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出Domain Expansion框架解决多目标学习中潜在表示崩溃问题，通过正交池化机制为每个目标构建相互正交的子空间，防止梯度冲突并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多目标训练中，不同目标的梯度冲突会导致共享表示陷入妥协状态，无法为任何单一任务达到最优，即"潜在表示崩溃"问题。

Method: 提出Domain Expansion框架，使用新颖的正交池化机制构建潜在空间，为每个目标分配相互正交的子空间，避免表示冲突。

Result: 在ShapeNet、MPIIGaze和Rotated MNIST等多个基准测试中验证了方法有效性，特别是在分类与姿态/视线估计等多目标组合任务上表现优异。

Conclusion: 该方法不仅防止了表示崩溃，还创建了显式、可解释、可组合的潜在空间，其中概念可以直接操作，为多目标学习提供了新思路。

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [53] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 提出Distributional Sobolev Training方法，将分布强化学习扩展到连续状态-动作空间，不仅建模状态-动作值函数的分布，还建模其梯度的分布，以解决现有梯度正则化方法在随机/噪声环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度正则化值学习方法（如MAGE）在随机或噪声环境中表现不佳，限制了其应用范围。需要开发一种能够在随机环境中有效利用梯度信息的方法。

Method: 扩展连续状态-动作空间的分布强化学习，同时建模标量状态-动作值函数及其梯度的分布。采用基于样本的方法，使用条件变分自编码器（cVAE）实现一步世界模型，利用最大切片最大均值差异（MSMMD）实例化分布贝尔曼算子。

Result: 证明了Sobolev增强贝尔曼算子是收缩算子且具有唯一不动点，揭示了梯度感知强化学习中平滑性的基本权衡。在简单随机强化学习玩具问题和多个MuJoCo环境中验证了方法的有效性。

Conclusion: Distributional Sobolev Training方法通过同时建模值函数及其梯度的分布，成功解决了现有梯度正则化方法在随机环境中的局限性，为梯度感知强化学习提供了理论保证和实际应用价值。

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [54] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: 该研究扩展了热管微堆设计优化框架，采用PEARL算法进行多目标优化，同时最小化棒积分峰值因子和度电成本，评估了三种成本情景下的优化策略。


<details>
  <summary>Details</summary>
Motivation: 热管微堆具有紧凑、可运输和固有安全特性，适合偏远地区部署。先前研究仅关注最小化度电成本，本研究扩展为多目标优化，同时考虑安全性能（峰值因子）和经济性（度电成本）。

Method: 采用PEARL（Pareto Envelope Augmented with Reinforcement Learning）算法进行多目标优化，结合代理模型和强化学习，评估三种成本情景：高成本轴向和鼓反射体、低成本轴向反射体、低成本轴向和鼓反射体。

Result: 研究发现：降低固体慢化剂半径、栅距和鼓涂层角度，同时增加燃料高度可有效降低峰值因子；优化度电成本的四个关键策略包括：最小化昂贵轴向反射体贡献、减少控制鼓依赖、用石墨价格材料替代昂贵TRISO燃料、最大化燃料燃耗。

Conclusion: PEARL算法在权衡不同设计情景方面表现出潜力，但代理模型预测与全阶模拟仍存在差异。通过约束松弛和代理模型改进可进一步提升性能，这是持续研究的重点。

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [55] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: 该技术报告提出了量化感知蒸馏（QAD）方法，用于恢复NVFP4量化大型语言模型和视觉语言模型的精度，通过蒸馏全精度教师模型到量化学生模型，在复杂后训练流程中表现稳定且对数据质量要求低。


<details>
  <summary>Details</summary>
Motivation: 传统量化感知训练（QAT）在多阶段后训练流程（包括监督微调、强化学习和模型合并）中存在工程复杂性和训练不稳定问题，需要更稳定且对数据要求不高的量化精度恢复方法。

Method: 提出量化感知蒸馏（QAD），使用KL散度损失将全精度教师模型蒸馏到量化学生模型中，该方法对数据质量和覆盖范围具有鲁棒性，无需完整训练数据即可恢复精度。

Result: 在多个后训练模型（包括AceReason Nemotron、Nemotron 3 Nano、Nemotron Nano V2、Nemotron Nano V2 VL和Llama Nemotron Super v1）上评估，QAD能够一致地将精度恢复到接近BF16精度的水平。

Conclusion: QAD为量化大型语言模型和视觉语言模型提供了一种有效且稳定的精度恢复方法，特别适用于复杂的多阶段后训练流程，相比传统QAT具有更好的工程可行性和训练稳定性。

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [56] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出Decision Importance Transformer (DIT)框架，用于上下文强化学习，通过基于优势函数的加权最大似然估计，从次优离线数据中学习最优策略。


<details>
  <summary>Details</summary>
Motivation: Transformer在上下文学习方面表现出色，但将其应用于上下文强化学习时面临挑战：当离线数据来自次优行为策略时，标准自回归训练（模仿学习）会导致次优性能。需要一种方法能够从次优历史数据中学习最优策略。

Method: 提出DIT框架：1) 训练基于transformer的价值函数，估计次优轨迹行为策略的优势函数；2) 训练基于transformer的策略，使用加权最大似然估计损失，权重基于训练好的价值函数构建，引导次优策略向最优策略转变。

Result: 在老虎机和马尔可夫决策过程问题上进行广泛实验，结果显示DIT实现了优越性能，特别是在离线数据集包含次优历史数据时表现突出。

Conclusion: DIT框架成功地将actor-critic算法以上下文方式实现，能够有效从次优离线数据中学习，为上下文强化学习提供了有效的解决方案。

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [57] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: 提出基于强化学习的通用序列设计框架，用于6G极化码设计，支持多种信道条件和解码策略，可扩展到2048码长，在5G所有(N,K)配置中性能接近5G NR序列，在N=2048时比beta-expansion基线提升0.2dB。


<details>
  <summary>Details</summary>
Motivation: 为6G应用推进极化码设计，需要开发可扩展、适应不同信道条件和解码策略的通用序列设计方法，以支持标准化需求。

Method: 基于强化学习的通用序列设计框架，包含三个关键要素：(i) 基于极化码通用偏序特性的物理定律约束学习，(ii) 利用决策的弱长期影响限制前瞻评估，(iii) 联合多配置优化提高学习效率。

Result: 框架可扩展到2048码长，在5G所有(N,K)配置中性能与5G NR序列相当，在N=2048时比beta-expansion基线提升0.2dB增益。

Conclusion: 提出的强化学习框架为6G极化码设计提供了可扩展、通用的序列设计方法，通过物理定律约束学习和联合优化等关键技术实现了大规模学习，适合标准化应用。

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [58] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: 本文重新实现了Facebook Prophet模型于PyMC框架中，以克服原版在贝叶斯推断方法和建模灵活性上的限制，并详细分析了多种贝叶斯推断技术的实现与性能。


<details>
  <summary>Details</summary>
Motivation: 在使用Facebook Prophet进行贝叶斯推断时，发现其内置的推断方法有限（仅MAP估计和MCMC采样），且API设计不够灵活，难以实现自定义建模想法，因此需要更灵活的解决方案。

Method: 在PyMC中完全重新实现Prophet模型，支持扩展基础模型，并评估比较多种贝叶斯推断方法，包括完整MCMC技术、MAP估计和变分推断技术。

Result: 实现了基于PyMC的Prophet模型，能够应用多种贝叶斯推断技术，详细分析了采样方法、收敛诊断、预测指标以及计算效率，并识别了未来需要解决的问题。

Conclusion: PyMC重新实现的Prophet模型提供了更大的灵活性和更丰富的贝叶斯推断方法选择，为时间序列预测问题提供了更好的建模和推断框架。

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [59] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: 本文首次系统研究了扩散语言模型（DLM）在成员推理攻击（MIA）下的隐私泄露漏洞，提出SAMA攻击方法，相比基线AUC提升30%，揭示了DLM的显著隐私风险。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案，使用双向掩码标记预测，但其在成员推理攻击下的隐私泄露风险尚未得到充分探索。与自回归模型的单一固定预测模式不同，DLM的多种掩码配置指数级增加了攻击机会。

Method: 提出SAMA（子集聚合成员攻击）方法：1）采样渐进密度的掩码子集；2）应用基于符号的统计量处理重尾噪声；3）通过逆加权聚合优先处理稀疏掩码的干净信号；4）将稀疏记忆检测转化为鲁棒的投票机制。

Result: 在9个数据集上的实验表明：SAMA相比最佳基线实现30%的相对AUC提升，在低误报率下可达8倍改进。这揭示了DLM中先前未知的显著隐私漏洞。

Conclusion: 扩散语言模型存在严重的成员推理攻击漏洞，其多种掩码配置显著增加了攻击面。SAMA攻击方法有效利用了DLM的特性，表明需要开发针对性的隐私防御机制来保护这些模型。

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [60] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: 开发了一个用于脑磁图（MEG）的大型自回归模型，能够跨数据集和扫描仪进行长上下文预测，训练了超过500小时的MEG数据，实现了分钟级别的脑活动生成。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够处理长上下文、跨数据集和扫描仪的脑磁图生成模型。需要开发一个能够从脑活动数据中学习并生成长时间序列的模型，以更好地理解和模拟大脑活动。

Method: 1. 使用改进的SEANet风格向量量化器将多通道MEG数据压缩为扁平化的token流；2. 在Qwen2.5-VL骨干网络上从头训练，预测下一个脑token；3. 能够从长达一分钟的上下文中递归生成数分钟的MEG数据；4. 引入三个任务匹配测试来评估长时程生成质量。

Result: 1. 模型在跨数据集（CamCAN和Omega训练，MOUS测试）上表现出良好的泛化能力；2. 生成结果在长时间滚动中保持相对稳定；3. 正确上下文条件下的生成结果比交换提示的控制条件更接近真实脑活动。

Conclusion: 成功开发了一个能够跨数据集和扫描仪处理长上下文MEG数据的自回归生成模型，为脑活动建模和生成提供了新工具，代码已开源。

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [61] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的框架来理解自监督学习，从谱表示视角揭示了成功SSL算法的本质，为开发更高效的表示学习算法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在表示学习方面取得了显著进展，但缺乏清晰统一的理论理解。现有方法差异巨大，缺乏统一框架阻碍了表示学习的进一步发展，使得理论理解缺失、高效算法设计原则不明确、实际应用缺乏理论依据。

Method: 从谱表示视角对表示进行理论分析，揭示现有成功SSL算法的谱本质，建立统一的理解和分析框架。

Result: 提出了一个原则性的表示学习基础框架，揭示了SSL算法的谱本质，为理解和分析表示学习提供了统一视角。

Conclusion: 该框架不仅有助于理解现有SSL算法，还能启发开发更高效、易于使用的表示学习算法，为实际应用提供理论指导。

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [62] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出了一种更简单的方法来评估语言模型中的隐式规划能力，通过押韵诗歌生成和问答案例研究，发现从10亿参数模型开始就存在这种能力，并可通过向量操控影响中间token生成。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型在训练过程中表现出隐式规划行为，但现有评估方法复杂。本文旨在开发更简单、可扩展的技术来系统评估语言模型的隐式规划能力。

Method: 提出了简化的隐式规划评估方法，包括押韵诗歌生成和问答两个案例研究。通过在行末使用向量操控技术，影响中间token生成，从而控制最终的押韵词或答案。

Result: 方法可扩展到多种模型，发现隐式规划是普遍机制，从10亿参数模型开始就存在。通过向量操控可有效影响模型生成特定的押韵词（如"-ight"）或答案（如"whale"）。

Conclusion: 本文方法为广泛研究LLMs的隐式规划能力提供了直接途径，对AI安全和控制决策有重要启示，表明隐式规划能力存在于比先前认为更小的模型中。

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [63] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: 该论文建立了稀疏支持向量机（SSVM）的局部对偶理论，证明了SSVM是0/1损失SVM的对偶问题，并阐明了SSVM与hinge损失SVM和ramp损失SVM的理论关系。


<details>
  <summary>Details</summary>
Motivation: 稀疏支持向量机（SSVM）在优化中受到关注，但现有方法缺乏理论依据。论文旨在填补这一空白，为SSVM建立局部对偶理论，并解释其与hinge损失SVM和ramp损失SVM的关系。

Method: 开发了SSVM的局部对偶理论，证明了SSVM是0/1损失SVM的对偶问题。建立了线性表示定理，并分析了SSVM与hinge损失SVM、ramp损失SVM的收敛关系。

Result: 证明了SSVM是0/1损失SVM的对偶问题；在特定条件下，hinge损失SVM的全局解序列收敛到0/1损失SVM的局部解；0/1损失SVM的局部极小值也是ramp损失SVM的局部极小值。

Conclusion: SSVM提供了hinge损失SVM和ramp损失SVM的超参数选择指导，解释了为什么SSVM在先前实证研究中表现更好。通过真实数据集测试验证了SSVM的潜在优势。

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [64] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: 提出一种基于梯度影响的诊断方法，用于评估神经网络偏微分方程模拟器是否内化了物理对称性，超越了传统的前向传播等变性测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要测试神经网络前向传播的等变性，但无法判断模型是否真正内化了物理对称性。需要一种能直接评估学习动态是否耦合物理等效配置的诊断工具。

Method: 引入基于影响的诊断方法：测量参数更新在对称相关状态间的传播，定义为沿群轨道评估的损失梯度度量加权重叠。该方法探测学习损失景观的局部几何结构。

Result: 应用于自回归流体流动模拟器，发现轨道梯度相干性提供了学习在对称变换上泛化的机制，并指示训练何时选择对称兼容的盆地。

Conclusion: 开发了一种新技术，用于评估代理模型是否内化了已知解算子的对称性质，超越了传统等变性测试。

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [65] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: MAPLE是一种新的非线性降维方法，通过自监督学习和最大流形容量表示增强UMAP，能更好地处理高维数据中的复杂流形结构。


<details>
  <summary>Details</summary>
Motivation: UMAP在处理具有显著类内方差和弯曲流形结构的高维数据（如生物或图像数据）时，对复杂流形的建模能力有限，需要改进。

Method: 采用自监督学习方法，引入最大流形容量表示（MMCRs），通过压缩局部相似数据点间的方差同时放大不相似数据点间的方差，来解缠复杂流形。

Result: MAPLE相比UMAP能产生更清晰的视觉聚类分离和更精细的子聚类分辨率，同时保持相当的计算成本。

Conclusion: MAPLE通过改进流形建模，在保持计算效率的同时，为高维数据提供了比UMAP更好的降维和可视化效果。

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [66] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: NeuraLSP：一种新颖的神经预处理器，利用系统矩阵近零空间向量的左奇异子空间，通过压缩谱信息到固定低秩算子来解决现有方法中的秩膨胀问题，实现高达53%的加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的预处理器方法在提取连接结构时，需要将离散化系统矩阵聚合成图，这会导致秩膨胀和次优收敛率的问题。

Method: 提出NeuraLSP神经预处理器，结合新颖的损失度量，利用系统矩阵近零空间向量的左奇异子空间，将谱信息压缩到固定低秩算子中。

Result: 该方法具有理论保证和实证鲁棒性，能够抵抗秩膨胀问题，在多种偏微分方程家族上实现高达53%的加速。

Conclusion: NeuraLSP通过创新的损失函数和谱信息压缩方法，有效解决了现有神经预处理器中的秩膨胀问题，在理论和实验上都取得了显著进展。

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [67] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: 论文提出从因果视角重新审视OOD分类，引入基于必要性和充分性的因果有效性评估框架，替代传统域不变性假设，在挑战性域偏移下提升OOD性能。


<details>
  <summary>Details</summary>
Motivation: 现实分类任务中，测试分布常与训练数据显著不同，OOD泛化面临根本挑战。现有方法大多追求域不变表示，隐含假设不变性意味着可靠性，但跨域不变的特征不一定对预测具有因果有效性。

Method: 从因果视角重新审视OOD分类，提出基于必要性和充分性评估学习表示的因果有效性框架。引入显式的片段级框架，直接测量跨域的因果有效性，提供比单纯不变性更可靠的评估标准。

Result: 在多域基准测试上的实验表明，该方法在OOD性能上取得一致改进，特别是在挑战性域偏移下表现突出，凸显了因果评估对鲁棒泛化的价值。

Conclusion: 因果视角为OOD分类提供了更可靠的评估框架，因果有效性比单纯域不变性更能保证模型在分布偏移下的鲁棒泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [68] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: 在预测模型部署会改变数据分布的performative prediction中，当影响强度ρ>1时，计算ε-稳定点变得PPAD完全，与纳什均衡计算等价，即使ρ仅略大于1。


<details>
  <summary>Details</summary>
Motivation: 研究performative prediction中强影响机制（ρ>1）的计算复杂性，填补现有研究空白。现有工作已知弱影响（ρ<1）时简单重训练可线性收敛，但强影响机制的计算复杂性尚不清楚。

Method: 建立尖锐的相变理论，证明当ρ=1+O(ε)时，计算ε-稳定点是PPAD完全的。技术贡献包括将PPAD-hardness扩展到一般凸域，并研究战略分类的特殊情况。

Result: 发现计算ε-稳定点在ρ>1时是PPAD完全的，即使在线性分布偏移和二次损失函数的简单设置下也难解。战略分类中计算战略局部最优是PLS难的。

Conclusion: performative prediction在强影响机制下存在计算复杂性相变，从弱影响的可解到强影响的难解（PPAD完全），这对变分不等式复杂性有更广泛意义。

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [69] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 提出基于元认知的强化学习框架，通过内部可靠性信号评估、调节和恢复学习行为，使用价值预测误差稳定性驱动的元信任变量来调制学习动态。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒强化学习方法通常关注抑制不可靠经验或损坏奖励，但缺乏对自身学习过程可靠性的推理能力，导致要么对噪声过度反应变得过于保守，要么在不确定性累积时灾难性失败。

Method: 提出元认知强化学习框架，引入由价值预测误差稳定性驱动的元信任变量，通过故障安全调节和渐进信任恢复来调制学习动态，使智能体能够基于内部估计的可靠性信号评估、调节和恢复学习行为。

Result: 在具有奖励损坏的连续控制基准测试中，具备恢复能力的元认知控制相比强鲁棒基线实现了更高的平均回报，并显著减少了后期训练失败。

Conclusion: 元认知强化学习框架通过内部可靠性评估和调节机制，能够有效应对奖励损坏等不确定环境，提高学习稳定性和性能，减少灾难性失败。

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [70] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff提出一种无需重新训练的扩散模型正则化强度调节方法，通过在采样时动态调整对齐模型和参考后验的几何混合，实现不同正则化强度下的效果模拟。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对齐方法需要为不同正则化强度分别训练模型，计算成本极高。如何选择合适正则化强度（太强导致对齐有限，太弱导致奖励黑客）是一个核心难题。

Method: DeRaDiff：一种去噪时间重对齐方法，在采样时将反向步骤参考分布替换为对齐后验和参考后验的几何混合，通过单一可调参数λ实现实时控制。

Result: 实验表明，DeRaDiff能有效近似不同正则化强度下从头训练的对齐模型，在文本图像对齐和图像质量指标上表现一致，大幅降低计算成本。

Conclusion: DeRaDiff提供了一种高效搜索最优正则化强度的方法，无需昂贵的对齐扫描，显著减少计算开销，将语言模型的时间重对齐思想扩展到扩散模型。

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [71] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: 提出首个结合对偶预测的最小成本网络流算法，基于ε-松弛方法，在预测误差下提供理论保证，实验显示显著加速效果


<details>
  <summary>Details</summary>
Motivation: 机器学习预测已被证明能提升经典算法性能，但尚未有结合对偶预测的最小成本网络流算法。本文旨在填补这一空白，将预测融入经典算法以提升效率。

Method: 基于经典的ε-松弛最小成本流算法，加入对偶预测。提供无穷范数预测误差下的时间复杂度界，并证明PAC学习预测的样本复杂度界。

Result: 在交通网络和芯片逃逸布线两个应用上验证：学习固定预测和基于特征的神经网络模型预测。实验结果显示平均分别获得12.74倍和1.64倍加速。

Conclusion: 成功提出首个结合对偶预测的最小成本网络流算法，提供理论保证并在实际应用中验证了显著性能提升，为机器学习增强经典算法提供了新范例。

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [72] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: 提出一种新的MoE层参数化方法，通过动态平均场理论分析，实现超参数在不同规模模型间的可靠迁移，从5100万到20亿参数都有效。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE层虽然能解耦总参数和激活参数，但增加了训练复杂度：需要调优路由器权重超参数，且专家数量和规模等架构维度选择困难。需要一种廉价可靠的超参数选择方法。

Method: 提出新的MoE层参数化方法，基于动态平均场理论分析，在固定token预算下，通过参数化模型宽度、深度、专家数量和专家隐藏层大小等维度。

Result: 参数化方法实现了从5100万到20亿参数模型的可靠超参数迁移。从小模型短token训练中确定的超参数可以成功用于大模型长token训练，表现出良好的模型性能。

Conclusion: 提出的参数化方法为MoE模型的超参数选择提供了廉价可靠的解决方案，支持在不同规模模型间的超参数迁移，简化了MoE模型的训练和扩展。

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [73] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark框架通过关键状态动态分支实现策略感知的探索，在关键决策点选择性分支探索，以更少样本获得更好性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在长时程任务训练中面临高质量轨迹稀缺问题，通常盲目扩大采样规模并在中间步骤平均分配计算资源，导致大量计算浪费在平凡步骤上，且无法保证样本质量

Method: 提出Spark框架，在关键决策状态选择性分支进行资源高效探索，利用智能体内在决策信号激活自适应分支探索，减少对人类先验的依赖，实现精确资源分配

Result: 在多样化任务（如具身规划）实验中，Spark以显著更少的训练样本实现了更高的成功率，在未见场景中展现出强大的泛化能力

Conclusion: Spark框架通过策略感知的关键状态动态分支机制，实现了更高效的资源分配和探索策略，为长时程任务的强化学习训练提供了有效解决方案

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [74] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: 该论文推导了一个预测模型的会计恒等式，将准确性与常见公平性标准联系起来，表明准确性和公平性在二元预测中是互补关系而非权衡关系。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究通常将准确性和公平性视为权衡关系，但缺乏一个统一的数学框架来理解它们之间的内在联系。作者希望揭示准确性与各种公平性标准之间的根本关系。

Method: 推导了一个会计恒等式，将模型准确性（均方误差）与公平性标准（组内校准误差和组间误差不平衡）联系起来。该恒等式适用于全局校准模型，并扩展到二元和非二元预测任务。

Result: 恒等式显示：对于二元结果，总不公平预算等于模型的均方误差乘以跨结果类别的组流行度差异。准确性和公平性是互补的：提高准确性必然减少总不公平预算，反之亦然。实验验证了理论，并显示许多公平性干预措施主要在公平性违规之间进行替代。

Conclusion: 准确性和公平性在二元预测中应被视为互补而非权衡关系。该框架统一了现有的不可能性结果，并揭示了在非二元预测中额外结果信息如何缓解公平性不相容性。

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [75] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: 提出两个机器学习框架：快速参数化模型预测聚合曲线，生成模型学习订单级分布，用于EPEX SPOT日前市场的储能优化


<details>
  <summary>Details</summary>
Motivation: 解决电力市场日前交易中需求/供给曲线预测和储能优化的挑战，需要既快速可解释又全面准确的建模方法

Method: 1) 快速参数化模型：用最小/最大量和切比雪夫多项式表示弹性段，低维网格鲁棒表示；2) 生成模型：学习24小时订单级联合分布，生成合成订单场景

Result: 参数化模型适合日常使用，误差低且可解释；生成模型提供全面分析；基于预测优化储能策略，量化收益分布，发现价格压缩效应

Conclusion: 两种框架互补，参数化模型适合日常操作，生成模型适合深入分析，为电力市场储能优化提供有效工具

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [76] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow：一种零样本物理一致采样框架，使用预训练生成先验从稀疏观测中推断物理场，严格满足PDE约束，无需任务特定重训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在物理反问题中难以在不破坏生成先验或需要昂贵重训练的情况下强制执行硬物理约束，需要一种能同时满足严格物理一致性、观测保真度和预训练先验统计结构的采样机制。

Method: 提出ProFlow近端引导框架，采用严格的两步交替方案：1)终端优化步，通过近端最小化将流预测投影到物理和观测一致集的交集；2)插值步，将精炼状态映射回生成轨迹以保持与学习流概率路径的一致性。

Result: 在Poisson、Helmholtz、Darcy和粘性Burgers方程上的综合基准测试表明，ProFlow相比最先进的扩散和流基线方法，实现了更优的物理和观测一致性，以及更准确的分布统计。

Conclusion: ProFlow为零样本物理一致采样提供了有效框架，能够在不破坏预训练生成先验的情况下严格满足物理约束，在计算物理反问题中具有重要应用价值。

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [77] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出CGP方法，通过维护显式活动集和置信调整Lipschitz包络，为黑箱Lipschitz函数优化提供最优性证书和可测量进度保证


<details>
  <summary>Details</summary>
Motivation: 现有自适应离散化方法隐式避免次优区域，但缺乏显式最优性证书和可测量进度保证，需要一种能提供明确最优性证明的方法

Method: 提出CGP方法，维护显式活动集A_t，使用置信调整Lipschitz包络识别潜在最优点，任何在A_t外的点都被高概率证明是次优的；还开发了三个扩展：CGP-Adaptive在线学习L，CGP-TR通过信任区域扩展到高维，CGP-Hybrid在检测到局部平滑时切换到GP细化

Result: 在边缘条件和近最优维度α下，证明Vol(A_t)以受控速率收缩，获得样本复杂度$\tildeO(\varepsilon^{-(2+α)})$；在12个基准测试（d∈[2,100]）中，CGP变体匹配或超越强基线，同时通过证书体积提供原则性停止标准

Conclusion: CGP为黑箱Lipschitz函数优化提供了首个同时具备显式最优性证书、可测量进度保证和原则性停止标准的方法，其扩展版本在不同场景下均表现优异

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [78] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文证明了整流流模型在标准假设下达到样本复杂度$\tilde{O}(\varepsilon^{-2})$，优于流匹配模型的$O(\varepsilon^{-4})$，并与均值估计的最优速率匹配。


<details>
  <summary>Details</summary>
Motivation: 流基生成模型相比扩散模型显示出更高的效率，但现有理论分析未能充分解释其优越性能。整流流模型通过约束传输轨迹为线性路径来加速采样，但其样本复杂度的理论保证尚不明确。

Method: 研究整流流模型，该模型约束从基分布到数据分布的传输轨迹为线性。通过分析用于参数化速度场和数据分布的神经网络类，利用平方损失沿线性路径训练的特性，证明相关假设类具有严格控制的局部Rademacher复杂度。

Result: 在标准假设下，整流流模型达到样本复杂度$\tilde{O}(\varepsilon^{-2})$，这改进了流匹配模型已知的$O(\varepsilon^{-4})$界限，并与均值估计的最优速率匹配。

Conclusion: 整流流模型因其特殊的线性路径结构和平方损失训练方式，使得相关假设类具有良好控制的局部Rademacher复杂度，从而实现了最优的样本复杂度，为其实证性能提供了理论解释。

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [79] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本文提出HE-SNR指标，通过细粒度熵分析解决传统指标在指导LLM中期训练中的不足，特别针对软件工程任务优化。


<details>
  <summary>Details</summary>
Motivation: SWE-bench已成为评估LLM在复杂软件工程任务上的主要基准，但现有指标如困惑度(PPL)存在"长上下文税"问题，且与下游SWE性能相关性弱，无法有效指导中期训练。

Method: 1) 引入严格的数据过滤策略；2) 提出熵压缩假说，将智能重新定义为将不确定性结构化为低阶熵压缩状态的能力；3) 基于细粒度熵分析制定新指标HE-SNR(高熵信噪比)。

Result: 在工业级混合专家模型(MoE)上验证，涵盖不同上下文窗口(32K/128K)，该方法显示出卓越的鲁棒性和预测能力。

Conclusion: 本研究为优化LLM在复杂工程领域的潜在能力提供了理论基础和实用工具，通过HE-SNR指标填补了中期训练指导指标的空白。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [80] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: C2框架通过交叉学习块和约束感知损失增强决策变换器，提升自动出价性能


<details>
  <summary>Details</summary>
Motivation: 决策变换器在自动出价中能捕捉时序依赖，但存在两个关键局限：状态、动作和回报序列间的跨相关性建模不足，以及对最优/次优行为的学习不加区分

Method: 提出C2框架，包含两个核心创新：1) 通过交叉注意力机制的交叉学习块来加强序列间相关性建模；2) 结合预算和CPA约束的约束感知损失，用于选择性学习最优轨迹

Result: 在AuctionNet数据集上的离线评估显示，在不同预算设置下均获得持续性能提升（比最先进的GAVE高出3.23%）；消融研究验证了CLB和CL的互补协同作用

Conclusion: C2框架通过增强跨序列相关性建模和选择性学习机制，在自动出价任务中表现出优越性

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [81] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: 提出一种新框架，在时间顺序信息损坏或缺失时，同时恢复时间顺序并估计随机微分方程参数


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多动态过程（金融、健康、系统生物学）可用SDE建模，但现有参数估计方法依赖准确的时间戳序列。当时间顺序信息损坏、缺失或被隐藏（如隐私保护）时，现有方法失效

Method: 利用前向和后向过程的不对称性，推导得分匹配准则来推断观测对之间的正确时间顺序，通过排序过程恢复总顺序，然后使用最大似然估计从重建序列中估计SDE参数

Result: 在合成和真实数据集上进行广泛实验，证明该方法在时间顺序缺失情况下的有效性，扩展了参数估计的应用范围

Conclusion: 该框架能够同时恢复时间顺序并估计SDE参数，将参数估计扩展到时间顺序缺失的场景，拓宽了在敏感领域的应用

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [82] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: 提出δ-Adapter，一种轻量级、架构无关的后处理方法，用于提升已部署时间序列预测模型的准确性和不确定性校准，无需重新训练或修改主干模型。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列预测模型在架构改进上收益递减，存在一个关键但未被充分探索的机会：策略性地使用后处理。本文旨在解决时间序列预测中的"最后一公里"问题，即在无需重新训练或修改已部署主干模型的情况下提高准确性和不确定性。

Method: 提出δ-Adapter方法，在两个接口处学习微小、有界的模块：输入微调（对协变量进行软编辑）和输出残差校正。该方法提供局部下降保证、O(δ)漂移界限和组合稳定性。同时可作为特征选择器学习稀疏的、面向预测时长的输入掩码，并作为分布校准器引入分位数校准器和符合性校正器来提供校准的个性化区间。

Result: 在多种主干模型和数据集上的实验表明，δ-Adapter能够以可忽略的计算成本和无需接口更改的方式提高准确性和校准效果。

Conclusion: δ-Adapter为解决时间序列预测中的"最后一公里"问题提供了一种有效、轻量级的解决方案，能够在保持部署稳定性的同时提升模型性能，为后处理在预测系统中的应用开辟了新方向。

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [83] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: 该研究探讨了Transformer注意力层如何实现类似人类记忆的检索机制，提出关键词作为线索的假设，并识别出专门编码关键词的神经元，为可解释AI和机器遗忘提供新视角。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI在LLMs领域不断发展，但注意力层的具体作用仍未充分探索。监管压力要求提高透明度、问责制和隐私保护的机器遗忘能力，因此需要深入理解注意力层的记忆机制。

Method: 借鉴心理学和计算心理语言学的研究，将Transformer注意力机制与人类记忆的线索检索联系起来。基于编码特异性原则，提出关键词作为线索的假设，并通过实验识别注意力层中专门编码关键词的神经元。

Result: 提供了关键词作为线索假设的汇聚证据，成功分离出注意力层中专门编码和促进上下文定义关键词检索的神经元。这些关键词可以从识别的神经元中提取，并应用于下游任务如机器遗忘。

Conclusion: 注意力层实现了类似人类记忆的检索机制，关键词作为线索在其中起关键作用。识别出的关键词编码神经元为LLMs的可解释性和机器遗忘等应用提供了新的途径。

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [84] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: 提出一种基于学习的空间脉冲响应补偿方法，用于3D光声计算机断层扫描，通过将SIR污染数据映射到理想点状换能器数据，实现快速准确重建。


<details>
  <summary>Details</summary>
Motivation: 传统PACT重建方法面临两难：忽略换能器空间脉冲响应(SIR)的解析方法计算效率高但空间分辨率受损；考虑SIR的优化方法准确但计算成本高，特别是在3D应用中。需要一种既能保持计算效率又能提高重建准确性的方法。

Method: 提出学习型SIR补偿框架，在数据域操作：1) 将SIR污染的PACT测量数据映射到理想点状换能器补偿数据；2) 研究两种补偿模型：U-Net模型和物理启发的Deconv-Net模型；3) 开发快速解析训练数据生成程序；4) 补偿后数据可使用忽略SIR的高效重建方法。

Result: 虚拟成像研究验证了框架有效性：显示分辨率改善，对噪声变化、物体复杂性和声速异质性具有鲁棒性。应用于体内乳腺成像数据时，学习补偿模型揭示了被SIR伪影掩盖的精细结构。这是首次在3D PACT成像中展示学习型SIR补偿。

Conclusion: 该学习型SIR补偿框架成功解决了PACT重建中计算效率与准确性的权衡问题，为3D PACT成像提供了一种快速且准确的重建方法，能够显著改善图像分辨率并揭示被伪影掩盖的精细结构。

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [85] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Cheap2Rich是一个多尺度数据同化框架，通过结合快速低精度先验模型与可学习的可解释差异修正，从稀疏传感器数据重建高保真状态空间，应用于旋转爆震发动机等复杂多尺度系统。


<details>
  <summary>Details</summary>
Motivation: 解决计算廉价模型与复杂物理系统之间的sim2real差距是多尺度工程问题中的核心挑战，特别是当降阶模型通常只能捕捉主导动态时。需要一种能够从稀疏传感器数据重建高保真状态空间的方法。

Method: 提出Cheap2Rich多尺度数据同化框架，结合快速低精度先验模型与可学习的可解释差异修正。该方法能够从稀疏传感器历史数据重建高保真状态空间，特别适用于旋转爆震发动机等耦合爆震波传播、喷射器驱动非定常性、混合和刚性化学反应的复杂多尺度系统。

Result: 成功从稀疏测量重建了旋转爆震发动机的高保真状态，同时分离出与喷射器驱动效应相关的物理意义明确的差异动态。该方法为复杂多尺度系统的数据同化和系统识别提供了一个通用的多保真度框架。

Conclusion: Cheap2Rich框架能够实现快速设计探索、实时监测和控制，同时提供可解释的差异动态，为解决复杂多尺度系统中的sim2real差距问题提供了有效解决方案。代码已在GitHub上开源。

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [86] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: 论文提出基于同伴预测的LLM评估与后训练方法，利用博弈论激励兼容性，无需真实标签即可奖励诚实信息性回答，有效抵抗欺骗行为


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估和后训练依赖监督信号，但困难任务的强监督往往不可得，导致模型利用不完美监督产生欺骗性结果。博弈论中的激励兼容性研究未被充分利用，该领域专注于用弱监督获取诚实信息性回答

Method: 引入同伴预测方法，基于相互可预测性指标，无需真实标签即可奖励诚实信息性回答。该方法利用博弈论机制设计，确保激励兼容性，使模型有动机提供真实信息而非欺骗性回答

Result: 方法有效抵抗欺骗，理论保证和实证验证（模型参数达405B）。用同伴预测奖励训练8B模型可恢复因恶意微调导致的真实性下降。发现同伴预测存在逆缩放特性：专家与参与者能力差距越大，抗欺骗性越强，可在100倍规模差距下可靠评估强模型

Conclusion: 同伴预测方法为LLM评估和后训练提供了无需真实标签的可靠解决方案，特别适用于评估前沿模型。其逆缩放特性使其在专家-参与者能力差距大时表现更优，解决了LLM-as-a-Judge在面对强欺骗模型时的失效问题

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [87] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: 提出TRACE基准和READER模型，用于解决GMV预测中的延迟反馈问题，相比传统CVR预测更具挑战性


<details>
  <summary>Details</summary>
Motivation: 在线广告排序模型的目标正从转化率(CVR)等概率指标转向GMV等数值业务指标。GMV预测中的延迟反馈问题尚未被研究，且比CVR预测更具挑战性，因为GMV是连续目标，且单个点击可能导致多次购买

Method: 建立TRACE基准数据集，包含完整交易序列；提出READER模型，采用双分支架构，根据复购预测选择性激活专家参数，并动态校准回归目标以缓解标签不完整导致的低估问题

Result: READER在TRACE基准上优于基线方法，准确率提升2.19%。分析发现GMV标签分布快速演变，复购样本与单购样本分布差异显著

Conclusion: 该研究为GMV预测中的在线延迟反馈建模开辟了新方向，TRACE基准和洞察将促进该领域未来的研究和应用

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [88] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: 提出Window-Diffusion方法，通过窗口化token剪枝和缓存技术，在保持生成性能的同时实现高达99倍的扩散语言模型推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型推理时需要对整个序列进行注意力计算，存在大量冗余计算。现有块状扩散方法需要重新训练且更新顺序受限，难以直接应用于预训练模型。

Method: 提出Window-Diffusion方法：1）token级分析发现推理具有结构局部性；2）维护滑动计算窗口，将未解码token分为：在线计算的活跃token、KV状态缓存并定期刷新的缓冲token、窗口外剪枝的远场token；3）计算仅限于窗口内的活跃和缓冲token。

Result: 在LLaDA和Dream模型上实验，在相同计算预算下，实现了高达99倍的推理加速，同时基本保持了生成性能。

Conclusion: Window-Diffusion通过利用扩散语言模型推理的结构局部性，实现了高效的推理加速，为预训练扩散模型的部署提供了实用解决方案。

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [89] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: TABED：一种用于大型视觉语言模型的自适应批量集成草稿方法，通过动态集成多个草稿来加速推理，相比自回归解码获得1.74倍加速，比单草稿方法提升5%


<details>
  <summary>Details</summary>
Motivation: 推测解码在LLM推理加速中有效，但在大型视觉语言模型（LVLM）中尚未充分探索。现有方法在不同输入场景下表现不稳定，需要更鲁棒的加速方案。

Method: 提出测试时自适应批量集成草稿（TABED），通过批量推理获取多个草稿，利用推测解码设置中可用的历史真实值偏差进行动态集成，保持训练免费且通过参数共享最小化集成成本。

Result: 在11个数据集上评估，TABED相比自回归解码平均获得1.74倍的鲁棒墙钟时间加速，比单草稿方法提升5%，具有即插即用兼容性，可与高级验证和替代草稿方法集成。

Conclusion: TABED为LVLM提供了一种有效、鲁棒的推测解码加速方案，通过动态集成策略解决了场景特定性能波动问题，实现了显著的速度提升。

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [90] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs提出了一种新的神经网络架构，将网络权重参数化为时间的函数，允许空间表示随时间演化，相比传统PINNs在时间相关PDE求解中实现了4倍精度提升和10倍收敛加速。


<details>
  <summary>Details</summary>
Motivation: 传统空间-时间PINNs将时间作为输入，但在所有时间共享相同的网络权重，这导致相同的特征需要表示显著不同的动态特性。这种耦合会降低精度，并在同时强制PDE、边界和初始条件约束时可能破坏训练稳定性。

Method: 提出时间诱导神经网络(TINNs)，将网络权重参数化为时间的函数，允许有效的空间表示随时间演化，同时保持共享结构。该公式自然产生非线性最小二乘问题，使用Levenberg-Marquardt方法进行高效优化。

Result: 在各种时间相关PDE上的实验显示，相比PINNs和强基线方法，TINNs实现了高达4倍的精度提升和10倍的收敛加速。

Conclusion: TINNs通过将网络权重参数化为时间的函数，解决了传统PINNs在时间相关PDE求解中的局限性，显著提升了精度和训练效率，为物理信息神经网络在动态系统中的应用提供了更有效的架构。

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [91] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: 标准连续时间生成模型能否表示支持集为极度稀疏、全局约束的离散集合的分布？研究使用数独网格作为测试平台，发现随机采样优于确定性流，基于分数的采样器最可靠，DDPM式采样有效性最高。


<details>
  <summary>Details</summary>
Motivation: 探索标准连续时间生成模型（如流匹配和基于分数的模型）是否能够表示支持集为极度稀疏、全局约束的离散集合的分布，使用数独网格作为受控测试平台。

Method: 使用数独网格作为连续松弛空间的子集，训练流匹配和基于分数的模型沿着高斯概率路径，比较确定性（ODE）采样、随机（SDE）采样和从相同连续时间训练导出的DDPM式离散化方法。

Result: 无条件生成时，随机采样显著优于确定性流；基于分数的采样器在连续时间方法中最可靠；DDPM式祖先采样总体有效性最高。模型可重新用于引导生成：通过重复采样在固定线索下的完成情况，当约束满足时停止，模型可作为概率性数独求解器。

Conclusion: 经典扩散/流公式能够为非零概率质量分配给全局约束的组合结构，并可通过随机搜索用于约束满足，尽管样本效率远低于经典求解器和离散几何感知的扩散方法。

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [92] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: 提出基於多智能體Transformer的無監督異常檢測框架，用於識別自動駕駛中的安全關鍵場景，通過預測殘差建模正常駕駛並檢測偏差，在NGSIM數據集上驗證有效性。


<details>
  <summary>Details</summary>
Motivation: 自動駕駛中安全關鍵場景識別至關重要，但這類事件罕見使得監督標註不切實際。傳統基於規則的指標過於簡單，現有方法缺乏系統性驗證統計異常是否真正反映物理危險。

Method: 提出無監督異常檢測框架：1) 使用多智能體Transformer建模正常駕駛行為；2) 通過預測殘差測量偏差；3) 採用雙重評估方案：穩定性評估（Kendall等級相關係數和Jaccard指數）和物理對齊評估（與替代安全指標的相關性）。

Result: 在NGSIM數據集上驗證：最大殘差聚合器實現最高物理對齊同時保持穩定性；識別出388個被傳統方法遺漏的獨特異常，捕捉到如側向漂移下的反應制動等細微多智能體風險；將檢測到的異常聚類為四種可解釋風險類型。

Conclusion: 該框架有效識別自動駕駛中的安全關鍵場景，彌補了傳統方法的不足，為模擬和測試提供可操作的見解，並通過系統性驗證確保異常檢測的物理意義。

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [93] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP：利用LLM作为代理自动生成和优化数据处理策略的框架，无需人工干预或访问原始数据，通过迭代学习和加速技术实现高效数据处理。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据中常包含大量低质量样本，需要有效的数据处理。传统方法依赖人工分析和试错调整，成本高且在高隐私领域（如医疗）存在隐私风险。因此需要实现无需暴露原始数据的自动化数据处理。

Method: 提出LLM-AutoDP框架，利用LLM作为代理自动生成和优化数据处理策略。方法包括：生成多个候选策略，通过反馈信号和比较评估进行迭代优化；引入三种加速技术：分布保持采样（减少数据量同时保持分布完整性）、处理目标选择（使用二元分类器识别低质量样本进行重点处理）、缓存重用机制（重用先前处理结果减少冗余计算）。

Result: 使用该框架处理的数据训练的模型，相比未处理数据训练的模型获得超过80%的胜率；相比基于LLM代理的AutoML基线，获得约65%的胜率；加速技术将总搜索时间减少高达10倍。

Conclusion: LLM-AutoDP能够有效自动化数据处理策略的生成和优化，在保护隐私的同时显著提升模型性能，并通过加速技术实现高效搜索，为高隐私领域的LLM微调提供了实用解决方案。

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [94] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD是一种异构感知的联邦学习算法，通过参数引导的全局泛化聚合和局部去偏分类来解决联邦域泛化中的优化分歧和性能分歧问题，为参与和未见客户端获得最优全局模型。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习需要确保不同实体间的有效协作和隐私保护。新加入的客户端需要大量调整和额外训练来适应现有系统，因此在异构数据下将联邦学习模型泛化到未见客户端变得日益重要。论文强调了联邦域泛化中两个未解决的挑战性问题：优化分歧和性能分歧。

Method: 提出FedRD算法，采用异构感知的联邦学习方法，协同利用参数引导的全局泛化聚合和局部去偏分类来减少分歧。该方法旨在为参与和未见客户端获得最优全局模型。

Result: 在公共多域数据集上的大量实验表明，该方法在解决这一特定问题上相比竞争基线表现出显著的性能优势。

Conclusion: FedRD通过解决联邦域泛化中的优化分歧和性能分歧问题，能够有效处理异构联邦学习场景，为参与和未见客户端提供更好的模型泛化能力。

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [95] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion：结合散射变换和分层注意力机制的时间序列预测框架，通过多尺度特征提取和自适应增强，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临多时间尺度复杂依赖关系的挑战，需要能够同时捕捉局部和全局模式的方法。

Method: 包含四个核心组件：1）分层散射变换模块提取多尺度不变特征；2）尺度自适应特征增强模块动态调整特征重要性；3）多分辨率时间注意力机制学习不同时间范围的依赖关系；4）趋势-季节-残差分解引导的结构感知损失函数。

Result: 在七个基准数据集上的广泛实验表明，ScatterFusion优于其他常见方法，在不同预测时间范围内显著降低了误差指标。

Conclusion: ScatterFusion通过整合散射变换和分层注意力机制，为时间序列预测提供了一个强大的框架，能够有效处理多尺度时间依赖关系。

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [96] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer：一种结合自适应小波分解与跨尺度注意力机制的新型时间序列预测架构，通过动态选择小波基和分解层、跨尺度特征融合、频率感知注意力及分层预测网络，显著提升多变量时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要在多个时间尺度上捕捉模式，同时保持计算效率。现有方法在处理多尺度、非平稳时间序列时存在局限性，需要一种能够自适应处理不同频率成分并有效融合跨尺度信息的架构。

Method: 1. 自适应小波分解模块（AWDM）：根据信号特性动态选择最优小波基和分解层数
2. 跨尺度特征融合（CSFF）：通过可学习耦合矩阵捕捉不同频带间的相互作用
3. 频率感知多头注意力（FAMA）：根据频率选择性对注意力头进行加权
4. 分层预测网络（HPN）：在多个分辨率上生成预测后进行重构

Result: 在基准数据集上的广泛实验表明，AWGformer相比最先进方法取得了显著的平均改进，特别是在多尺度和非平稳时间序列上表现出色。理论分析提供了收敛保证，并建立了小波引导注意力与经典信号处理原理之间的联系。

Conclusion: AWGformer通过整合自适应小波分解和跨尺度注意力机制，为多变量时间序列预测提供了一种有效且理论完备的解决方案，特别适用于处理复杂的时间模式和多尺度特征。

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [97] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 论文提出Concept Component Analysis (ConCA)框架，通过线性解混从LLM表示中恢复概念的后验对数，解决了稀疏自编码器在理论上的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器(SAEs)在提取LLM可解释概念时缺乏理论基础，导致方法设计和评估标准存在困难。需要建立LLM表示与人类可解释概念之间的理论对应关系。

Method: 提出Concept Component Analysis (ConCA)框架，将LLM表示建模为概念后验对数的线性混合。具体实现稀疏ConCA变体，利用稀疏先验解决解混问题的病态性。

Result: 实现了12个稀疏ConCA变体，在多个LLM上成功提取了有意义的可解释概念，相比SAEs具有理论支持的优势。

Conclusion: ConCA为LLM概念提取提供了理论基础的框架，通过线性解混方法能够从LLM表示中恢复概念的后验分布，解决了SAEs的理论模糊性问题。

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [98] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: 本文对扩散映射技术进行了实践导向的综述，指出数据预处理、参数设置和成分选择对结果有显著影响，并展示了识别最相关成分的新方法。


<details>
  <summary>Details</summary>
Motivation: 扩散映射作为一种谱降维技术，能够揭示高维数据中的非线性子流形，在生物学、工程学和社会科学等领域广泛应用。然而，数据预处理、参数设置和成分选择对生成的流形有显著影响，但文献中对此缺乏全面讨论。

Method: 提供扩散映射技术的实践导向综述，说明常见陷阱，并展示最近引入的识别最相关成分的技术。

Result: 研究结果表明，前几个成分不一定是最相关的成分，这挑战了传统认知。

Conclusion: 扩散映射的应用需要仔细考虑数据预处理、参数设置和成分选择，新方法有助于更准确地识别数据中最相关的成分。

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [99] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: TimeCatcher：一种基于波动感知变分预测框架，针对非平稳时间序列的长期预测问题，通过变分编码器和波动感知增强机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级MLP模型在时间序列预测中依赖局部平稳性假设，在高度非平稳序列（特别是具有突发波动的序列，如网络流量监控）的长期预测中容易出错。

Method: TimeCatcher扩展线性架构，包含：1）变分编码器捕捉历史数据中的潜在动态模式；2）波动感知增强机制检测并放大显著的局部变化。

Result: 在交通、金融、能源和天气等9个真实世界数据集上的实验表明，TimeCatcher始终优于最先进的基线方法，在具有高波动性和突发波动的长期预测场景中改进尤为显著。

Conclusion: TimeCatcher通过变分编码和波动感知机制有效解决了非平稳时间序列的长期预测挑战，特别是在高波动性场景下表现出色。

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [100] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出基于强化学习的模型无关方法，生成满足个体公平、群体公平和混合公平约束的反事实解释，在三个基准数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释对于提升机器学习模型透明度至关重要，但需要确保相似个体和不同受保护群体获得相似且可行的补救措施，以实现可信和公平的决策。

Method: 将公平反事实生成建模为优化问题，提出基于强化学习的模型无关方法，同时满足个体公平（相似个体获得相似CFs）、群体公平（不同受保护群体间公平）和混合公平约束。

Result: 在三个基准数据集上评估，方法能有效确保个体和群体公平，同时保持生成反事实在邻近性和合理性方面的质量，并量化了不同层次公平的成本。

Conclusion: 该工作开启了关于混合公平及其在可解释人工智能中作用的更广泛讨论，提出了同时处理个体和群体公平的新方法，为公平反事实解释提供了有效解决方案。

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [101] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: 论文通过二元假设检验视角研究神经网络分类器的监督训练动态，发现泛化能力强的网络会逐渐与Neyman-Pearson最优决策规则对齐，KL散度单调改善并与错误率指数相关。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络分类器在监督训练过程中的动态特性，从二元假设检验的角度理解分类决策的形成机制，探索网络如何通过学习实现最优决策规则。

Method: 将分类建模为类别条件表示分布之间的二元检验，通过经验分析训练轨迹，研究网络决策规则与Neyman-Pearson最优决策规则的对齐过程。

Result: 泛化能力强的网络在训练过程中会逐渐与Neyman-Pearson最优决策规则对齐，KL散度单调改善，且这种改善与错误率指数相关。

Conclusion: 这一发现为理解神经网络训练动态提供了新视角，并可能为不同类别神经网络的训练和正则化策略提供理论依据和实用方法。

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [102] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: 提出一种半监督变分自编码器框架，通过相似性正则化将潜在变量与痴呆进展的临床或生物标志物对齐，用于分析阿尔茨海默病神经影像数据。


<details>
  <summary>Details</summary>
Motivation: 高维神经影像数据存在复杂的非线性关系，传统方法难以有效分析神经退行性疾病。需要一种能够将影像特征与疾病进展指标对齐的灵活框架。

Method: 提出半监督VAE框架，包含灵活的相似性正则化项，将选定的潜在变量与痴呆进展的临床或生物标志物对齐。使用ADNI的PET扫描数据，将第一个潜在维度与认知评分对齐。

Result: 监督潜在变量能生成不同认知障碍水平的平均重建图像。体素GLM分析显示海马等关键区域代谢降低，默认模式网络和中央执行网络受影响。其余潜在变量捕获仿射变换和强度变化等混杂因素。

Conclusion: 该框架能有效提取与阿尔茨海默病生物标志物对齐的疾病相关模式，为研究神经退行性进展提供可解释且适应性强的工具。

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [103] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CCMamba是首个基于Mamba的统一神经框架，用于组合复形上的学习，通过将多秩关联关系组织成结构化序列，用秩感知状态空间模型处理，实现线性时间内的自适应、定向和长程信息传播。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑深度学习方法大多依赖局部消息传递和注意力机制，存在二次复杂度且保持低维，限制了在更高阶组合复形中的可扩展性和秩感知信息聚合能力。

Method: 将消息传递重新表述为选择性状态空间建模问题，将多秩关联关系组织成结构化序列，通过秩感知状态空间模型处理，实现线性时间内的自适应、定向和长程信息传播。

Result: 在图形、超图和单纯形基准测试中，CCMamba始终优于现有方法，同时展现出改进的可扩展性和对深度的鲁棒性。

Conclusion: CCMamba为组合复形上的学习提供了一个高效、可扩展的统一框架，通过选择性状态空间建模克服了传统注意力机制的局限性，在保持理论表达力的同时实现了线性复杂度。

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [104] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: 提出一种基于深度能量的无监督集成学习方法，仅利用个体学习器的预测构建元学习器，无需标签数据或额外信息，在条件独立假设下有理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏真实标签或额外数据的情况下，如何有效结合多个学习器预测的挑战。这在评估个体分类器性能困难或信息有限的情况下尤为重要。

Method: 提出基于深度能量的方法，仅使用个体学习器的预测构建元学习器，能够捕捉学习器间的复杂依赖结构。该方法无需标签数据、学习器特征或问题特定信息。

Result: 在多种集成场景中表现出优越性能，包括具有挑战性的专家混合设置。在标准集成数据集和专门设计的测试数据集上均取得良好结果。

Conclusion: 无监督集成学习在数据稀缺或隐私敏感环境中具有重要潜力，能够有效利用集体智能，仅通过个体学习器的预测就能构建准确的元学习器。

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [105] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE是一种基于Group Relative Policy Optimization框架的LLM遗忘方法，通过内在奖励信号惩罚对禁止概念的提及，实现可验证的遗忘，相比现有方法显著减少token使用、提升流畅性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM在预训练中会无意记忆敏感或受版权保护的数据，这违反了GDPR和欧盟AI法案等法规要求。现有遗忘方法存在数据泄露、牺牲模型流畅性和鲁棒性、依赖昂贵外部奖励模型等问题。

Method: PURGE基于Group Relative Policy Optimization框架，将遗忘问题形式化为可验证任务。使用内在奖励信号惩罚对禁止概念的提及，实现安全一致的遗忘，无需依赖外部奖励模型。

Result: 相比SotA方法，PURGE将每个目标的token使用减少46倍，流畅性提升5.48%，对抗鲁棒性提升12.02%。在RWKU基准测试中，实现11%的遗忘效果，同时保留98%的原始效用。

Conclusion: 将LLM遗忘形式化为可验证任务，能够实现更可靠、高效和可扩展的遗忘，为遗忘研究提供了结合理论保证、改进安全性和实际部署效率的新方向。

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [106] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 提出AsylADMM算法，用于去中心化中位数和分位数估计，具有异步更新、内存高效（每节点仅需2个变量）和鲁棒性强的特点。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的去中心化学习需要通信高效、对数据损坏鲁棒且内存占用小的算法。现有gossip方法虽通信高效但鲁棒性不足，而ADMM方法虽能估计鲁棒的中位数但内存需求随节点度增长，不适用于内存受限场景。

Method: 提出AsylADMM算法，一种新颖的gossip算法，专为异步更新设计，每节点仅需维护两个变量。通过分析同步变体建立理论保证，并实证验证异步算法的快速收敛性。

Result: 算法支持分位数修剪、几何中位数估计和基于深度的修剪，其中分位数修剪在实证中优于现有基于秩的方法。同时提供了基于马尔可夫链理论的秩修剪新理论分析。

Conclusion: AsylADMM算法在去中心化中位数和分位数估计中实现了通信高效、内存轻量和鲁棒性的平衡，为资源受限边缘设备上的鲁棒学习提供了实用解决方案。

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [107] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出RARL框架，通过强化学习统一回归和排序任务，使用排名感知奖励和响应突变操作提升性能


<details>
  <summary>Details</summary>
Motivation: 传统方法难以建模序数回归和排序中的序数依赖关系，需要一种能够显式学习这些关系的框架

Method: 提出RARL框架：1) 统一目标函数协同整合回归和排序任务；2) 排名感知可验证奖励联合评估回归精度和排序准确性；3) 响应突变操作注入受控噪声增强探索

Result: 在三个不同基准测试上通过广泛实验验证了RARL的有效性

Conclusion: RARL框架成功解决了序数依赖建模问题，通过强化学习实现了回归和排序任务的协同改进

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [108] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出正则化GTD算法(R-GTD)，解决传统GTD在特征交互矩阵奇异时的不稳定问题，保证收敛到唯一解。


<details>
  <summary>Details</summary>
Motivation: 传统梯度时序差分(GTD)学习算法在离策略策略评估中广泛使用，但其收敛分析依赖于特征交互矩阵(FIM)非奇异的限制性假设。实际中FIM可能奇异，导致算法不稳定或性能下降。

Method: 通过重新表述均方投影贝尔曼误差(MSPBE)最小化问题，提出正则化优化目标，自然推导出正则化GTD算法(R-GTD)。该方法即使在FIM奇异时也能保证收敛到唯一解。

Result: 建立了理论收敛保证和显式误差界，并通过实证实验验证了所提方法的有效性。

Conclusion: R-GTD算法解决了传统GTD在奇异FIM情况下的稳定性问题，为离策略策略评估提供了更鲁棒的解决方案。

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [109] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo Müürsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: CoBA模型结合CNN、BiLSTM和注意力机制，利用5G毫米波信号分类低空无人机在授权与受限空域的飞行，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用和工业应用中的普及，确保低空飞行的安全性变得至关重要。在密集毫米波环境中，准确分类无人机是否在授权或受限空域具有挑战性，需要能够处理复杂传播和信号变化的模型。

Method: 提出CoBA深度学习模型，集成卷积神经网络(CNN)、双向长短期记忆网络(BiLSTM)和注意力机制，利用5G毫米波无线电测量数据，捕捉无人机信号的空间和时间模式。

Result: 实验使用TalTech的5G毫米波网络收集专用数据集，在控制环境下进行低空无人机飞行测试。CoBA模型在准确率上显著优于传统机器学习模型和指纹识别基准方法。

Conclusion: CoBA模型展示了可靠且受监管的无人机空域监控潜力，能够有效分类低空无人机在授权与受限空域的飞行操作。

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [110] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: 提出WFR-MFM方法，通过平均流框架实现单步生成，无需轨迹模拟，显著提升动态不平衡最优运输的推理速度


<details>
  <summary>Details</summary>
Motivation: 单细胞生物学中从有限观测重建动态演化是一个基本挑战，现有方法依赖推理时的轨迹模拟，导致推理成为可扩展应用的关键瓶颈

Method: 提出平均流框架，使用平均速度和质量增长场总结任意时间间隔的传输和质量增长动态，开发基于Wasserstein-Fisher-Rao几何的WFR-MFM方法

Result: 在合成和真实单细胞RNA测序数据上，WFR-MFM比现有基线方法推理速度快几个数量级，同时保持高预测准确性，能在数千条件下高效预测扰动响应

Conclusion: WFR-MFM通过平均流匹配框架解决了动态不平衡最优运输的推理瓶颈，实现了快速单步生成，为大规模单细胞生物学应用提供了高效工具

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [111] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: ACFormer：结合线性投影效率与卷积非线性特征提取能力的时间序列预测模型，通过系统感受野分析发现卷积层作为特征提取器，能有效捕捉高频成分并保持对非线性波动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有线性架构时间序列预测模型在捕捉全局趋势方面高效，但难以处理非线性信号；需要一种能同时兼顾线性投影效率和卷积非线性特征提取能力的解决方案。

Method: 1) 系统分析CNN模型的感受野，引入"个体感受野"揭示细粒度结构依赖；2) 提出ACFormer架构：共享压缩模块捕捉细粒度信息，门控注意力保持时间局部性，独立补丁扩展层重建变量特定时间模式。

Result: 在多个基准数据集上的实验表明，ACFormer持续达到最先进的性能，有效缓解了线性模型在捕捉高频成分方面的固有缺陷。

Conclusion: ACFormer成功调和了线性投影的效率与卷积的非线性特征提取能力，通过系统感受野分析为时间序列预测提供了新的架构设计见解，在保持对非线性波动鲁棒性的同时提升了预测性能。

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [112] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: DIVERSE框架通过FiLM层和CMA-ES算法探索深度神经网络的Rashomon集，无需重新训练即可生成预测行为多样但性能相当的不同模型变体。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过重新训练来探索Rashomon集（性能相当但预测行为不同的模型集合），但这种方法计算成本高昂。需要一种更高效的方法来系统性地探索这个模型多样性空间。

Method: 在预训练模型基础上添加特征线性调制（FiLM）层，使用协方差矩阵自适应进化策略（CMA-ES）在潜在调制空间中进行搜索，生成多样化的模型变体，无需重新训练或梯度访问。

Result: 在MNIST、PneumoniaMNIST和CIFAR-10数据集上，DIVERSE成功发现了多个高性能但功能不同的模型。相比重新训练基线，DIVERSE以更低的计算成本实现了相当的多样性。

Conclusion: DIVERSE提供了一种竞争性且高效的Rashomon集探索方法，使得构建既保持鲁棒性和性能又支持良好平衡的模型多样性集合变得可行。

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [113] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 首个虚拟传感器基础模型，能同时预测多种传感器信号，计算效率高，可扩展性强


<details>
  <summary>Details</summary>
Motivation: 现有虚拟传感器方法需要针对每个传感器定制模型，无法利用任务协同效应，缺乏统一基准；而现有时间序列基础模型计算昂贵且只能预测输入信号，不适用于虚拟传感器

Method: 提出首个虚拟传感器基础模型，统一模型能同时预测多种虚拟传感器，自动学习每个传感器相关输入信号，保持计算效率，参数数量几乎恒定

Result: 在标准基准和包含180亿样本的应用数据集上，相比基线方法实现了415倍计算时间减少和951倍内存需求减少，同时保持或提升预测质量，可扩展到数百个虚拟传感器

Conclusion: 该模型解决了现有虚拟传感器方法的局限性，实现了高效、可扩展的虚拟传感器预测，适用于大规模传感器网络的实际部署

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [114] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: 使用神经ODE和符号回归从噪声数据中建模复杂系统动力学并发现控制方程，展示了神经ODE的外推能力和符号回归的方程恢复能力。


<details>
  <summary>Details</summary>
Motivation: 准确建模复杂系统动力学并发现其控制微分方程对于加速科学发现至关重要，但传统方法面临噪声数据和有限观测的挑战。

Method: 使用两个阻尼振荡系统的噪声合成数据，探索神经ODE的外推能力，以及符号回归从噪声数据中恢复底层方程的能力。

Result: 1) 神经ODE能有效外推到新边界条件，前提是结果轨迹与训练数据具有动态相似性；2) 符号回归能从噪声真实数据成功恢复方程，但性能取决于输入变量的正确选择；3) 使用仅10%完整模拟数据训练的神经ODE生成的数据，符号回归恢复了三个控制方程中的两个，并对第三个获得了良好近似。

Conclusion: 使用神经ODE丰富有限数据并让符号回归推断物理定律，代表了科学发现的一种有前景的新方法，尽管完全恢复所有方程仍需未来工作。

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [115] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型记忆化检测方法，结合各向同性范数和各向异性对齐，无需去噪步骤即可高效检测训练数据复制


<details>
  <summary>Details</summary>
Motivation: 现有基于分数差范数的记忆化检测方法主要适用于各向同性对数概率分布假设，但在低噪声的各向异性区域效果有限，需要更有效的检测指标

Method: 通过分析各向异性区域发现记忆化样本在低噪声下具有强角度对齐特性，开发结合各向同性范数和各向异性对齐的检测指标，仅需两次条件/无条件前向传播

Result: 在Stable Diffusion v1.4和v2上的检测实验显示，新方法优于现有无需去噪的检测方法，速度比之前最佳方法快约5倍以上

Conclusion: 提出的检测指标能有效识别记忆化样本，并可用于基于该指标的提示词适应缓解策略，为扩散模型记忆化检测提供高效解决方案

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [116] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: 提出一个基于上下文感知的运行时监控框架，用于AI控制集成系统，通过上下文学习选择最适合当前环境的控制器，提高安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习控制器在复杂决策任务中表现出色，但在陌生环境中性能会急剧下降，存在安全隐患。传统集成方法通过平均或投票来提升鲁棒性，但会稀释各个控制器在不同操作上下文中的专业优势。

Method: 将安全AI控制集成设计重新定义为上下文监控问题：监控器持续观察系统上下文，选择最适合当前条件的控制器。将监控学习建模为上下文学习任务，采用上下文多臂老虎机技术。

Result: 在两个模拟自动驾驶场景中验证了框架，相比非上下文基线方法，在安全性和性能方面都有显著提升。

Conclusion: 提出的上下文感知监控框架能够识别和利用控制器在不同上下文中的专业优势，提供理论安全保证并更好地利用控制器多样性，为AI控制集成系统提供了更安全有效的解决方案。

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [117] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: MuRAL-CPD是一种新颖的半监督变化点检测方法，通过小波多分辨率分解和多轮主动学习结合用户反馈，优化超参数以匹配用户对"变化"的定义。


<details>
  <summary>Details</summary>
Motivation: 传统变化点检测方法多为无监督技术，缺乏对任务特定变化定义的适应性，且无法利用用户知识。需要一种能够结合用户反馈、提高准确性和可解释性的方法。

Method: 提出MuRAL-CPD方法：1) 使用小波基多分辨率分解在不同时间尺度检测变化；2) 将主动学习集成到多分辨率变化点检测算法中；3) 通过用户反馈迭代优化关键超参数，使模型对"变化"的定义与用户保持一致。

Result: 在多个真实世界数据集上的实验结果表明，MuRAL-CPD相比最先进方法具有显著效果，特别是在仅有最小监督可用的场景中表现优异。

Conclusion: MuRAL-CPD通过结合多分辨率分析和主动学习，成功解决了传统变化点检测方法缺乏适应性和无法利用用户知识的问题，提高了检测的准确性和可解释性。

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [118] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出PU RL蒸馏方法，让本地小模型无需人工标注或奖励模型，通过教师模型生成锚定响应，本地采样学生候选，进行锚定自排序来诱导偏好信号，实现完全本地训练。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、成本和延迟限制，本地部署小模型越来越普遍，但大多数实际管道停留在监督微调阶段，无法达到强化学习对齐阶段。RL对齐通常需要昂贵的人工偏好标注或依赖高质量奖励模型的大规模API使用和持续工程维护，这些都不适合本地设置。

Method: 提出正未标记RL蒸馏方法：1) 对每个提示查询教师模型一次获得锚定响应；2) 本地采样多个学生候选响应；3) 进行锚定条件自排序，诱导成对或列表偏好；4) 通过直接偏好优化或组相对策略优化实现完全本地训练循环。

Result: 理论分析证明该方法诱导的偏好信号是顺序一致的，并集中在接近最优的候选上，支持其偏好优化的稳定性。实验表明该方法在低成本设置下实现了持续强大的性能。

Conclusion: 该方法填补了本地小模型部署中RL对齐的空白，无需人工偏好标注或奖励模型，通过教师模型的偏好优化能力蒸馏，实现了高效、低成本的本地训练。

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [119] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: 提出一种学习显式最优传输映射的方法，为任意群体实例生成反事实解释，无需重新优化，最小化群体传输成本


<details>
  <summary>Details</summary>
Motivation: 现有群体反事实解释方法存在三个问题：(1) 仅针对固定群体优化，无法泛化到新成员；(2) 严格依赖强模型假设（如线性）以保证可解性；(3) 对反事实群体几何形变的控制较差

Method: 学习一个显式的最优传输映射，将任意群体实例发送到其反事实位置，无需重新优化，最小化群体的总传输成本。对于线性分类器，通过数学优化推导出群体反事实的函数表示

Result: 实验表明该方法能准确泛化，保持群体几何结构，相比基线方法仅产生可忽略的额外传输成本。即使无法利用模型线性性，也显著优于基线方法

Conclusion: 该方法通过学习显式最优传输映射，解决了现有群体反事实解释方法的局限性，实现了更好的泛化能力、几何保持性和可解释性

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [120] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: 本文提出纯利用学习(PEL)算法，证明在外生MDPs中无需探索即可获得有限样本遗憾界，颠覆了传统需要探索的认知。


<details>
  <summary>Details</summary>
Motivation: 外生MDPs在运营研究中很常见（如库存控制、能源存储），尽管经验表明贪婪方法效果很好，但现有理论仍依赖显式探索或表格假设。理论滞后于实践。

Method: 提出纯利用学习(PEL)算法，对于表格情况提供理论保证；对于大状态空间提出LSVI-PE线性近似方法。分析工具包括反事实轨迹和Bellman封闭特征传输。

Result: 表格情况下PEL达到$\widetilde{O}(H^2|Ξ|\sqrt{K})$遗憾界；LSVI-PE的遗憾与特征维度、外生状态空间和视野相关，独立于内生状态和动作空间。实验显示PEL优于基线方法。

Conclusion: 在外生MDPs中，纯利用就足够了，无需探索。这颠覆了传统需要探索的认知，为运营研究应用提供了理论基础。

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [121] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: LLM生成的参考文献列表在结构上与人类相似，但语义特征可被检测区分


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被用于整理参考文献，需要研究LLM生成的参考文献列表与人类撰写的列表是否可区分

Method: 构建了10,000篇论文的配对引用图（真实vs GPT-4o生成），使用结构特征和3072维标题/摘要嵌入，通过随机森林和图神经网络进行分析

Result: 仅结构特征难以区分GPT与真实引用（准确率约0.60），但语义嵌入显著提高可区分性（RF达0.83，GNN达93%）。结果在Claude Sonnet 4.5和不同嵌入模型上具有鲁棒性

Conclusion: LLM生成的参考文献在拓扑结构上接近人类引用，但留下可检测的语义指纹；检测和去偏应针对内容信号而非全局图结构

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [122] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN是一个自适应的Q学习框架，能够在非平稳环境中动态调整学习参数，适应奖励函数变化和动作空间扩展，防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在真实世界应用中面临非平稳环境的挑战，特别是当奖励函数变化或动作空间扩展时，传统方法需要完全重新训练，效率低下。

Method: MORPHIN框架整合了概念漂移检测机制，能够动态调整学习和探索超参数，适应奖励函数变化和动作空间扩展，同时保留先前的策略知识。

Result: 在Gridworld基准和交通信号控制模拟中，MORPHIN相比标准Q学习基线实现了更优的收敛速度和连续适应能力，学习效率提升高达1.7倍。

Conclusion: MORPHIN为强化学习在非平稳环境中的应用提供了有效的自适应解决方案，能够在动态变化中保持学习效率和策略稳定性。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [123] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 提出Cox-MT模型，结合半监督学习的Mean Teacher框架，利用标记和未标记数据训练ANN-based Cox模型，显著提升癌症预后预测性能


<details>
  <summary>Details</summary>
Motivation: 传统ANN-based Cox模型需要大量标记样本，但实际中标记数据有限，限制了模型性能。需要解决标记数据不足的问题

Method: 采用深度半监督学习(DSSL)方法，基于Mean Teacher框架开发单模态和多模态ANN-based Cox模型(Cox-MT)，利用标记和未标记数据进行训练

Result: 在TCGA数据上，单模态Cox-MT(RNA-seq或WSI)显著优于现有Cox-nnet模型；未标记样本增加时性能显著提升；多模态Cox-MT比单模态表现更好

Conclusion: Cox-MT模型能有效利用标记和未标记数据，相比仅用标记数据的现有ANN-based Cox模型显著提升预测准确性

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [124] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: 提出GUI-AiF框架，通过锚点奖励和区域奖励机制，解决GUI智能体在动态数据分布下的持续学习问题


<details>
  <summary>Details</summary>
Motivation: 数字环境（数据分布）不断变化，新的GUI数据随时间引入新的领域或分辨率，导致在静态环境中训练的智能体性能下降。现有方法无法在GUI分布随时间变化时保持稳定的基础定位

Method: 提出GUI-Anchoring in Flux (GUI-AiF)强化微调框架，包含两个新颖的奖励机制：Anchoring Point Reward in Flux (APR-iF)和Anchoring Region Reward in Flux (ARR-iF)，引导智能体适应变化的交互点和区域

Result: 大量实验表明GUI-AiF超越了最先进的基线方法

Conclusion: 建立了首个GUI智能体的持续学习框架，揭示了强化微调在持续GUI智能体中的未开发潜力

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [125] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia提出了一种Hessian引导的可微分量化感知训练框架，通过温度控制的softmax松弛替代硬舍入，结合张量级Hessian迹度量进行细粒度温度退火，有效提升极低比特LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，部署受限于内存瓶颈，需要极低比特量化。传统QAT方法从一开始就使用硬舍入和直通估计器，过早离散化优化空间，导致潜在权重和量化权重之间的梯度不匹配，阻碍量化模型的有效优化。

Method: Hestia框架：1) 用温度控制的softmax松弛替代刚性阶跃函数，保持训练早期梯度流动，同时逐步硬化量化；2) 利用张量级Hessian迹度量作为轻量级曲率信号，驱动细粒度温度退火，实现模型跨层的敏感度感知离散化。

Result: 在Llama-3.2上的评估显示，Hestia持续优于现有三元QAT基线，1B和3B模型的平均零样本性能分别提升5.39%和4.34%。结果表明Hessian引导的松弛有效恢复了表示能力，为1.58比特LLM建立了更鲁棒的训练路径。

Conclusion: Hestia通过Hessian引导的可微分量化框架，解决了传统QAT方法中梯度不匹配和过早离散化的问题，为极低比特大语言模型提供了更有效的训练方法，显著提升了量化模型的性能。

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [126] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: 提出了GraphAllocBench基准测试，用于评估多目标强化学习中的偏好条件策略学习，基于城市管理启发的图资源分配环境，并提出了两个新评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有PCPL基准测试主要局限于玩具任务和固定环境，缺乏真实性和可扩展性，需要更复杂、可扩展的基准来推动该领域发展。

Method: 构建了基于图资源分配沙盒环境CityPlannerEnv的GraphAllocBench基准，包含多样化目标函数、偏好条件和可扩展高维问题。提出了PNDS和OS两个新评估指标。

Result: GraphAllocBench揭示了现有MORL方法的局限性，为图神经网络等图方法在复杂组合分配任务中的应用铺平了道路。基准测试具有灵活性和可扩展性。

Conclusion: GraphAllocBench为PCPL研究提供了多功能、可扩展的基准测试，能够灵活变化目标、偏好和分配规则，推动该领域在复杂现实问题中的发展。

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [127] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: 该论文提出了一种在无限维函数空间中条件化扩散模型的方法，用于贝叶斯逆问题，通过Doob's h-变换和监督引导训练实现高效后验采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已扩展到无限维函数空间，可用于偏微分方程等逆问题。在贝叶斯逆问题中，目标是从通过条件化先验分布得到的后验分布中采样。虽然扩散模型提供了函数空间中的表达性先验，但如何条件化它们以从后验采样仍缺乏理论支持。

Method: 假设先验位于Cameron-Martin空间或相对于高斯测度绝对连续，证明了可以使用无限维Doob's h-变换条件化模型，后验得分分解为无条件得分和引导项。由于引导项难以处理，提出了监督引导训练的无仿真得分匹配目标，实现高效稳定的后验采样。

Result: 提供了函数空间中贝叶斯逆问题的数值示例，展示了该方法能够准确从后验分布中采样，是首个在函数空间中微调训练扩散模型以精确采样后验的方法。

Conclusion: 该工作首次提供了在函数空间中条件化扩散模型的理论框架和实用方法，通过监督引导训练实现了高效的后验采样，为函数空间中的贝叶斯逆问题提供了新的解决方案。

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [128] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出C^4方法解决离线强化学习中的分布偏移问题，通过分区缓冲采样和梯度校正惩罚来减少TD交叉协方差的有害影响，在OOD区域表现更好。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中分布偏移是核心挑战，稀缺数据或OOD主导的数据集会加剧此问题。标准平方误差目标会诱导有害的TD交叉协方差，在OOD区域放大，导致优化偏差和政策学习退化。

Method: 开发两种互补策略：1) 分区缓冲采样，将更新限制在局部重放分区，减弱不规则协方差效应，对齐更新方向；2) 显式梯度校正惩罚，在每次更新中消除协方差诱导的偏差。证明缓冲分区保持最大化目标的下界性质。

Result: 方法展现出更高的稳定性，在回报上比先前方法提升高达30%，特别是在小数据集和强调OOD区域的数据分割中表现优异。

Conclusion: C^4方法通过控制TD交叉协方差有效缓解离线强化学习中的分布偏移问题，在OOD区域减少过度保守性而不改变核心约束行为，易于集成到现有实现中。

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [129] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET-SG1是一个轻量级、稳定性导向的自回归回归模型，专为边缘和嵌入式AI系统的时间序列预测设计，通过线性行为空间编码、记忆锚定转换估计和确定性状态更新实现稳定长期预测。


<details>
  <summary>Details</summary>
Motivation: 针对边缘和嵌入式AI系统中时间序列预测的特殊需求，特别是需要保证长期预测稳定性（避免误差累积）的场景。传统RNN或Transformer模型在边缘设备上可能存在计算复杂、长期预测漂移等问题。

Method: 采用线性行为空间编码、记忆锚定转换估计和确定性状态更新机制。模型结构简单，参数少，支持定点算术运算，适合边缘设备部署。

Result: 在非平稳合成时间序列数据上，COMET-SG1在短期预测精度上与基线模型（MLP、LSTM、k近邻）相当，但在长期预测漂移方面显著减少。模型参数少，计算效率高。

Conclusion: COMET-SG1为边缘和嵌入式AI应用提供了一个实用且可解释的稳定自回归预测方法，特别适合需要长期预测稳定性的场景。

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [130] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: 提出基于距离的黑盒模型复制框架，将硬标签监督替换为到教师决策边界的带符号距离，将复制转化为平滑回归问题，提高边界几何恢复效率


<details>
  <summary>Details</summary>
Motivation: 实际部署的机器学习系统需要持续演化，但往往无法访问原始训练数据或模型内部。黑盒复制提供了一种实用的重构机制，但在硬标签输出限制下，复制变成了不连续的表面重建问题，严重限制了边界几何恢复效率

Method: 提出基于距离的蒸馏框架，用带符号距离替换硬标签监督；开发α控制的平滑和正则化方案，对诱导目标表面进行Hölder/Lipschitz控制；引入两种模型无关算法在仅标签访问下估计带符号距离

Result: 在合成问题和UCI基准测试中，相比硬标签基线，在保真度和泛化准确性方面表现出一致的改进，同时使距离输出能够作为黑盒副本的不确定性相关信号

Conclusion: 基于距离的复制框架通过将硬标签监督转化为平滑回归问题，有效解决了黑盒模型复制中的边界几何恢复问题，提高了复制效率和模型质量

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [131] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: 本文证明了多任务学习的统计极限，建立了比现有不可能定理更强的适应不可能性结果，表明即使每个任务有大量数据，多任务学习的困难也无法克服。


<details>
  <summary>Details</summary>
Motivation: 多任务学习框架在现代应用中取得了巨大成功，但arXiv:2006.15785的工作表明，在无法获取分布信息且每个任务样本量有限的情况下，仅基于样本聚合的算法无法保证最优风险。本文旨在超越这一"没有免费午餐"定理，深入理解多任务学习的统计极限。

Method: 本文通过建立更强的适应不可能性结果来研究多任务学习的统计极限。该方法超越了arXiv:2006.15785中的不可能定理，证明了即使每个任务有任意大的样本量，适应仍然是不可能的。

Result: 本文证明了多任务学习存在更强的适应不可能性结果，该结果适用于任意大的每个任务样本量。这一改进传达了一个重要信息：多任务学习的困难无法通过每个任务拥有丰富数据来克服。

Conclusion: 多任务学习的统计极限比先前认识的要更严格，即使每个任务有大量数据，适应仍然不可能。这一发现强调了多任务学习固有的困难，并提出了未来可能感兴趣的"最优适应性"概念。

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [132] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 本文首次分析了决策树作为二元分类器的主动学习标签复杂度，提出了在特定假设下实现多对数标签复杂度的理论框架，并设计了首个具有乘法误差保证的主动学习算法。


<details>
  <summary>Details</summary>
Motivation: 决策树是重要的机器学习模型，但对其主动学习标签复杂度的理论理解有限。现有研究缺乏对决策树分歧系数的分析，也没有能提供乘法误差保证的通用主动学习算法。

Method: 1. 首次分析决策树的分歧系数；2. 在两种自然假设下（根到叶路径查询不同特征维度、输入数据具有规则网格结构）分析标签复杂度；3. 设计首个具有(1+ε)近似保证的通用主动学习算法；4. 结合上述结果设计决策树专用主动学习算法。

Result: 1. 证明在特定假设下可实现多对数标签复杂度，放松假设则导致多项式复杂度；2. 提出的决策树主动学习算法在数据集大小上仅需多对数标签查询；3. 建立了标签复杂度下界，证明算法对误差容忍度ε的依赖接近最优。

Conclusion: 本文首次系统分析了决策树的主动学习标签复杂度理论，提出了在合理假设下实现高效主动学习的框架，设计的算法具有接近最优的理论保证，为决策树的主动学习提供了坚实的理论基础。

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [133] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 提出condPED-ANOVA框架，用于估计条件搜索空间中超参数的重要性，解决了传统方法无法处理条件超参数的问题。


<details>
  <summary>Details</summary>
Motivation: 原始PED-ANOVA虽然能快速估计超参数重要性，但假设搜索空间是固定的、无条件的，无法正确处理条件超参数（即某些超参数的存在或取值范围依赖于其他超参数的情况）。

Method: 提出条件超参数重要性定义，并推导出闭式估计器，能够准确反映条件激活和域变化。该方法专门针对条件搜索空间设计。

Result: 实验表明，现有HPI估计器的简单适配在条件设置下会产生误导性或无法解释的重要性估计，而condPED-ANOVA始终能提供反映底层条件结构的有意义的重要性估计。

Conclusion: condPED-ANOVA为条件搜索空间中的超参数重要性估计提供了一个原则性框架，能够正确处理条件依赖关系，提供准确且可解释的重要性评估。

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [134] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO是一种利用丰富文本反馈进行强化学习的新方法，通过自我蒸馏将失败尝试的反馈转化为密集学习信号，提高样本效率和最终准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习方法只使用标量结果奖励，存在严重的信用分配瓶颈。许多可验证环境实际上提供丰富的文本反馈（如运行时错误或评估解释），这些反馈可以解释失败原因，但现有方法未能充分利用这些信息。

Method: 提出自我蒸馏策略优化（SDPO），将标记化的反馈转换为密集学习信号，无需外部教师或显式奖励模型。SDPO将当前模型在反馈条件下的输出作为自我教师，将其反馈感知的下一个标记预测蒸馏回策略中，从而利用模型在上下文中回顾性识别自身错误的能力。

Result: 在科学推理、工具使用和LiveCodeBench v6的竞争性编程任务中，SDPO相比强基线方法提高了样本效率和最终准确性。即使在仅返回标量反馈的标准RLVR环境中，SDPO通过使用成功rollout作为失败尝试的隐式反馈也优于基线。在测试时对单个问题应用SDPO可加速困难二元奖励任务的发现，以3倍更少的尝试达到与最佳k采样或多轮对话相同的发现概率。

Conclusion: SDPO通过利用丰富文本反馈进行自我蒸馏，有效解决了强化学习中信用分配瓶颈问题，在多个可验证领域表现出优越性能，为利用环境反馈改进语言模型训练提供了有前景的方向。

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [135] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: SE-GNN解释可能完全与模型推理无关，现有忠实度指标无法检测这种退化解释，本文提出新指标可靠标记退化解释


<details>
  <summary>Details</summary>
Motivation: 现有自解释图神经网络（SE-GNNs）的解释可能存在误导性，但缺乏对其失败案例的系统分析。特别是解释可能与模型实际推理过程完全无关，这种退化解释可能被恶意利用来隐藏敏感属性的使用，也可能自然出现，需要可靠的审计方法。

Method: 识别SE-GNN解释的关键失败模式：解释可能与模型推理标签的方式完全无关。通过实证分析展示退化解释可以被恶意植入或自然出现，并引入一种新的忠实度指标来可靠地标记这些退化解释。

Result: 研究发现许多SE-GNNs可以在达到最优真实风险的同时产生退化解释，而大多数现有忠实度指标无法识别这些失败模式。新提出的忠实度指标在恶意和自然设置下都能可靠地将退化解释标记为不忠实。

Conclusion: SE-GNN解释存在严重退化问题，可能被恶意利用或自然出现，现有评估指标不足。提出的新忠实度指标能有效检测退化解释，为可靠审计提供工具。代码已开源。

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [136] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: 提出failure-prefix conditioning方法，通过在训练中暴露模型于失败推理轨迹的前缀，解决RLVR训练在饱和问题上的停滞问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了LLM的推理能力，但在问题饱和时训练会停滞。核心挑战是信息性失败的可访问性差——学习信号存在但标准rollout中很少遇到。

Method: 提出failure-prefix conditioning方法：不从原始问题开始，而是重新分配探索，将训练条件设置在从罕见错误推理轨迹派生的前缀上，使模型暴露于易失败状态。

Result: 该方法获得的性能提升相当于在中等难度问题上训练的效果，同时保持token效率。还能减少在误导性失败前缀下的性能下降，并通过迭代方法在性能平台期后获得额外提升。

Conclusion: failure-prefix conditioning为在饱和问题上扩展RLVR训练提供了有效途径，通过暴露模型于失败状态来解锁学习信号。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [137] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: 奖励模型（RMs）虽然从LLMs初始化，但其输出受基础模型价值倾向的显著影响，Llama RMs偏好"能动性"，Gemma RMs偏好"社群性"，这种差异持久存在且与偏好数据无关。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在LLMs与人类价值对齐中至关重要，但相比预训练和微调模型本身，其研究关注较少。RMs从LLMs初始化，继承了基础模型的表征，但这种影响的性质和程度尚未得到充分研究。

Method: 对10个领先的开源权重RMs进行综合研究，使用经过验证的心理语言学语料库，分析它们在人类价值多个维度上的差异。基于"Big Two"心理学轴（能动性vs社群性），比较Llama和Gemma基础模型的RMs。通过消融实验控制偏好数据来源和数量，追踪差异到指令微调和预训练模型的logits，并推导可用的隐式奖励分数。

Result: RMs表现出显著的基于基础模型的人类价值维度差异：Llama RMs对"能动性"有强烈偏好，而Gemma RMs对"社群性"有强烈偏好。这种差异在偏好数据和微调过程相同的情况下依然存在，且可追溯到基础模型的log-probability差异。隐式奖励分数也表现出相同的能动性/社群性差异。消融实验表明这种效应不仅可重复，而且异常持久。

Conclusion: 尽管RMs旨在代表人类偏好，但其输出受到所基于的预训练LLMs的显著影响。这项工作强调了预训练阶段安全和对齐工作的重要性，并明确指出开源开发者选择基础模型时，不仅要考虑性能，更要考虑价值取向。

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [138] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: 研究最小可嵌入维度(MED)用于嵌入集合成员关系(m个元素和其k元素子集)，推导了MED的紧界，并通过实验验证了不同距离/相似度度量下的结果。数值模拟显示MED与元素数量呈对数依赖关系，表明嵌入检索的限制主要来自学习能力而非几何约束。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入集合成员关系所需的最小维度，探讨嵌入检索系统的根本限制是几何约束还是学习能力问题，为未来算法设计提供指导。

Method: 理论推导最小可嵌入维度(MED)的紧界，并通过实验验证ℓ₂距离、内积和余弦相似度等不同度量下的结果。进行数值模拟，其中子集嵌入选择为包含元素的嵌入质心。

Result: 得到了MED的紧界，数值模拟显示MED与元素数量呈对数依赖关系，表明嵌入检索的限制主要来自学习能力挑战而非几何约束。

Conclusion: 嵌入检索的限制主要源于学习能力问题而非几何约束，这一发现为未来算法设计提供了重要指导方向。

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [139] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: PatchFormer：基于补丁的时间序列基础模型，通过分层掩码重建进行自监督预训练，使用轻量级适配器进行高效迁移，在24个基准数据集上实现最先进的零样本多步预测


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法通常需要领域特定的特征工程和大量标注数据，这限制了模型的泛化能力和应用范围。需要一种能够跨领域学习通用时间序列表示的基础模型。

Method: 1. 将时间序列分割成补丁（patches）；2. 使用分层掩码重建进行自监督预训练，包括动态掩码和鼓励局部准确性与全局一致性的目标；3. 跨领域知识蒸馏；4. 使用轻量级适配器进行高效迁移；5. 学习跨时间尺度的可聚合多尺度时间表示

Result: 在24个基准数据集（天气、能源、交通、金融、医疗）上实现最先进的零样本多步预测，相比强基线平均减少27.3%的均方误差，同时减少94%的任务特定训练数据需求。模型在1000亿数据点上呈现近似对数线性扩展，处理长度512序列比全序列Transformer快3.8倍

Conclusion: PatchFormer展示了时间序列基础模型的可行性，通过自监督预训练和高效适配器实现了强大的跨领域泛化能力，显著减少了领域特定数据和工程需求，为时间序列预测提供了通用解决方案

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [140] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair是一个单次训练框架，可在推荐系统中实现训练后公平性控制，无需为不同公平性要求重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法通常在训练时固定公平性要求，缺乏训练后的灵活性。现实场景中，不同利益相关者可能随时间变化提出不同的公平性要求，为每个新要求重新训练模型成本过高。

Method: 提出Cofair框架：1) 使用共享表示层和公平性条件适配器模块，生成针对不同公平性水平的用户嵌入；2) 引入用户级正则化项，确保用户在不同公平性水平上的单调公平性改进。

Result: 在多个数据集和骨干模型上的实验表明，Cofair能够提供不同水平的动态公平性，其公平性-准确性曲线与最先进基线相当或更好，且无需为每个新公平性要求重新训练。

Conclusion: Cofair解决了推荐系统中公平性控制的灵活性需求，通过单次训练即可支持多种公平性要求，为实际部署提供了实用解决方案。

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [141] [C3Box: A CLIP-based Class-Incremental Learning Toolbox](https://arxiv.org/abs/2601.20852)
*Hao Sun,Da-Wei Zhou*

Main category: cs.LG

TL;DR: C3Box是一个基于CLIP的类增量学习工具箱，整合了传统、ViT和CLIP-based方法，提供统一框架和标准化实验流程。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的类增量学习方法分散在不同代码库中，配置不一致，阻碍了公平比较、可复现性和实际应用。

Method: 开发模块化的Python工具箱C3Box，继承PyCIL的流线型设计，提供JSON配置和标准化执行流程，整合多种CIL方法到统一的CLIP框架中。

Result: C3Box提供了可靠的基准平台，支持低工程开销的可复现实验，依赖广泛使用的开源库，支持主流操作系统。

Conclusion: C3Box解决了CLIP-based CIL方法分散和配置不一致的问题，为持续学习研究提供了统一的基准平台和工具支持。

Abstract: Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.

</details>


### [142] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: 研究探索在VAE中集成Transformer架构，通过57个数据集实验发现：1) Transformer在潜在空间和解码器的位置会影响生成质量与多样性的权衡；2) Transformer连续块间存在高度相似性，解码器中输入输出近似线性关系


<details>
  <summary>Details</summary>
Motivation: 表格数据对生成模型仍具挑战性，传统基于多层感知机的VAE难以建模特征间关系，而Transformer的注意力机制更适合捕捉复杂特征交互。本研究旨在实证探索将Transformer集成到VAE不同组件中的影响。

Method: 在OpenML CC18套件的57个数据集上进行实验，系统研究将Transformer集成到VAE不同组件（编码器、潜在空间、解码器）中的效果，分析不同配置对模型性能的影响。

Result: 1) Transformer在潜在空间和解码器中的位置会导致生成质量（保真度）与多样性之间的权衡；2) 观察到Transformer所有组件中连续块之间存在高度相似性，特别是在解码器中，Transformer的输入输出关系近似线性。

Conclusion: Transformer可以增强VAE对表格数据的建模能力，但需要仔细考虑其在架构中的位置，因为这会显著影响生成质量与多样性的平衡。同时发现Transformer内部存在结构冗余，这为模型优化提供了可能。

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [143] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: 进化策略(ES)在LLM训练中能达到接近GRPO的性能，但存在严重的遗忘问题，限制了其在持续学习中的应用


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏部署后持续学习的能力，梯度算法需要大量内存，而进化策略作为无梯度替代方案在特定任务上表现良好，但需要全面评估其遗忘特性

Method: 对进化策略进行综合分析，评估其在增加更新步数时的遗忘曲线，与GRPO在数学和推理任务上比较，分析更新稀疏性和ℓ₂范数差异

Result: ES在计算预算相当的情况下能达到接近GRPO的性能，但伴随显著的先前能力遗忘；ES更新比GRPO更新稀疏性差且ℓ₂范数大几个数量级

Conclusion: 进化策略存在严重的遗忘问题，限制了其在在线训练模型中的应用，需要未来工作来缓解这些问题

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>
